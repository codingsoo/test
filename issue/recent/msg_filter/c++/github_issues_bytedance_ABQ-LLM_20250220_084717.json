[
  {
    "number": 4,
    "title": "2bit Packing logic post Bit Balance Strategy",
    "created_at": "2024-09-10T03:38:21Z",
    "closed_at": "2024-09-18T09:00:43Z",
    "labels": [],
    "url": "https://github.com/bytedance/ABQ-LLM/issues/4",
    "body": "I want some more understanding on how to pack the 2 bit weights after using bit balance strategy. \r\n\r\nWhile running blockwise quantization we have increased our range from 4 to 5 and values to {-2,-1,0,1,2}. After running the ABQ LLM for 2 bit, I will be having float weights which are good to quantize to 2 bits, but the problem here is with the packing logic. \r\n1.  How to represent the above range using 2 bits and pack them ?\r\n2.  If we use default 2 bit quantization, won't I lose the benefit of bit balance strategy ?",
    "comments_url": "https://api.github.com/repos/bytedance/ABQ-LLM/issues/4/comments",
    "author": "gdsaikrishna",
    "comments": [
      {
        "user": "lswzjuer",
        "created_at": "2024-09-11T02:40:30Z",
        "body": "After 2-bit balancing is done, the W3Ax kernel needs to be called for acceleration. Although the bit balancing strategy means that the weight will be degraded by one bit, ABQ's innovative Kernel implementation ensures the linearity of the acceleration gain, which means that the speed will only be slightly degraded."
      },
      {
        "user": "gdsaikrishna",
        "created_at": "2024-09-11T08:17:57Z",
        "body": "Ok, got it. \r\n1. What about the size of the model ? After 2 bit balancing is done, we save the weights in 3 bits? Which means the new 2 bit balance quantized model will be of same size as 3bit quantized model?\r\n"
      }
    ]
  }
]