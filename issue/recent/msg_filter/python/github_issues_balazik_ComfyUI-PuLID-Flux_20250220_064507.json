[
  {
    "number": 7,
    "title": " only one picture is generated each time",
    "created_at": "2024-10-02T05:44:57Z",
    "closed_at": "2024-10-02T20:02:34Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/7",
    "body": "Thank you for your great work!\r\nBut would you like to ask if only one picture is generated each time? If I generate two pictures, the following message will appear?\r\n\r\n# ComfyUI Error Report\r\n## Error Details\r\n- **Node Type:** SamplerCustomAdvanced\r\n- **Exception Type:** RuntimeError\r\n- **Exception Message:** Boolean value of Tensor with more than one value is ambiguous\r\n## Stack Trace\r\n```\r\n File \"C:\\Users\\ntzon\\Projects\\ComfyUI\\execution.py\", line 323, in execute\r\n output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^\r\n..........\r\n.............\r\n## Additional Context\r\n(Please add any additional context or steps to reproduce the error here)",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/7/comments",
    "author": "yangnt",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-02T12:26:25Z",
        "body": "Ah, I see by using `Repeat Latent Batch` (batch processing) I can replicate the error.\r\nI have never tested batch processing before. I'll try to edit it to make it work, but I don't promise anything. First, I have to find out what the sampler requires for batch processing."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-02T20:02:34Z",
        "body": "Funny thing is that I made some patches (not associated with this error) and now it automagically works. \r\nBut also I pulled the newest ComfyUI version. So it is possible that they fixed something.\r\n\r\nPull the new version and the new ComfyUI version and it should work.\r\nI tried:\r\n- Extra options: Batch count (with randomized seed)\r\n- EmptySD3LatentImage: batch_size\r\n- Repeat Latent Batch: amount \r\n\r\nAll variantion and batch generations worked for me.  If not I will reopen the issue.\r\n\r\n"
      }
    ]
  },
  {
    "number": 6,
    "title": "RuntimeError: expected scalar type Half but found BFloat16 when executing SamplerCustomAdvanced node",
    "created_at": "2024-10-02T04:49:31Z",
    "closed_at": "2024-10-02T18:34:44Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/6",
    "body": "ComfyUI_windows\\python_embeded\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2573, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: expected scalar type Half but found BFloat16",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/6/comments",
    "author": "wip163",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-02T12:11:45Z",
        "body": "Hi.  \r\n1. which FLUX.1-dev model did you use ? I mean the dtype (like F8_e4m3fn or F8_e5m2, checkpoint with backed F8 model etc.) If you don't know just paste here the link where you downloaded it. \r\n2. I need to know the HW (exect graphic card name) that you using (if cloud, then what type of pod you use)."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-02T18:37:19Z",
        "body": "Try to pull the new version (of ComfyUI-PuLID-Flux). It should check for bfloat16 support and set the dtype to float16 if not supported. And let me know if it helped."
      },
      {
        "user": "patientx",
        "created_at": "2024-10-02T20:08:12Z",
        "body": "RuntimeError: expected scalar type Half but found BFloat16\r\n\r\nthis is after the update, I am using weight type fp8_e4m3fn, with fp8 t5 and normal clip. I am also using a rx 6600 with zluda on windows."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-02T21:03:50Z",
        "body": "@patientx sorry, not waiting for you to confirm. Ok, changed the code so it detects what dtype will the original unet model use (manual cast) so If the model can run on the system also the pulid should. It should detect your special case running on ZLUDA. \r\n\r\nLet me know if it helped. (here: so I can better understand the needs for all platforms)."
      },
      {
        "user": "patientx",
        "created_at": "2024-10-03T19:48:21Z",
        "body": "> @patientx sorry, not waiting for you to confirm. Ok, changed the code so it detects what dtype will the original unet model use (manual cast) so If the model can run on the system also the pulid should. It should detect your special case running on ZLUDA.\r\n> \r\n> Let me know if it helped. (here: so I can better understand the needs for all platforms).\r\n\r\nNo problem, I can also confirm it now works with the latest version with amd + zluda combo. (mentioned it again because sometimes this causes specific problems). Thanks for the hard work !"
      },
      {
        "user": "hawkonetang",
        "created_at": "2024-10-18T14:39:05Z",
        "body": "Hello! I wonder if the graphics cards of series 20 cannot be used. The prompt is:  Custom sampler (advanced).\r\nexpected scalar type Half but found BFloat16 @balazik "
      },
      {
        "user": "ThisCodeIsMine",
        "created_at": "2024-10-26T16:19:23Z",
        "body": "I have the same error message. Is there anything you can do @balazik . I'm so close on making it work... :)\r\n```\r\n## System Information\r\n- **ComfyUI Version:** v0.2.4-6-g5281090\r\n- **Arguments:** ComfyUI\\main.py --windows-standalone-build\r\n- **OS:** nt\r\n- **Python Version:** 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\r\n- **Embedded Python:** true\r\n- **PyTorch Version:** 2.3.1+cu121\r\n## Devices\r\n\r\n- **Name:** cuda:0 NVIDIA GeForce RTX 2080 Ti : cudaMallocAsync\r\n  - **Type:** cuda\r\n  - **VRAM Total:** 11810832384\r\n  - **VRAM Free:** 1715076544\r\n  - **Torch VRAM Total:** 8589934592\r\n  - **Torch VRAM Free:** 14015936\r\n  \r\n  2024-10-26 17:58:12,152 - root - INFO - got prompt\r\n2024-10-26 17:58:12,194 - root - INFO - Using pytorch attention in VAE\r\n2024-10-26 17:58:12,195 - root - INFO - Using pytorch attention in VAE\r\n2024-10-26 17:58:12,742 - root - INFO - model weight dtype torch.bfloat16, manual cast: torch.float16\r\n2024-10-26 17:58:12,742 - root - INFO - model_type FLUX\r\n2024-10-26 17:58:17,249 - root - INFO - Requested to load FluxClipModel_\r\n2024-10-26 17:58:17,249 - root - INFO - Loading 1 new model\r\n2024-10-26 17:58:21,688 - root - INFO - loaded completely 0.0 3962.80810546875 True\r\n2024-10-26 17:58:24,062 - root - INFO - Loaded EVA02-CLIP-L-14-336 model config.\r\n2024-10-26 17:58:24,080 - root - INFO - Shape of rope freq: torch.Size([576, 64])\r\n2024-10-26 17:58:28,668 - root - INFO - Loading pretrained EVA02-CLIP-L-14-336 weights (eva_clip).\r\n2024-10-26 17:58:29,444 - root - INFO - incompatible_keys.missing_keys: ['visual.rope.freqs_cos', 'visual.rope.freqs_sin', 'visual.blocks.0.attn.rope.freqs_cos', 'visual.blocks.0.attn.rope.freqs_sin', 'visual.blocks.1.attn.rope.freqs_cos', 'visual.blocks.1.attn.rope.freqs_sin', 'visual.blocks.2.attn.rope.freqs_cos', 'visual.blocks.2.attn.rope.freqs_sin', 'visual.blocks.3.attn.rope.freqs_cos', 'visual.blocks.3.attn.rope.freqs_sin', 'visual.blocks.4.attn.rope.freqs_cos', 'visual.blocks.4.attn.rope.freqs_sin', 'visual.blocks.5.attn.rope.freqs_cos', 'visual.blocks.5.attn.rope.freqs_sin', 'visual.blocks.6.attn.rope.freqs_cos', 'visual.blocks.6.attn.rope.freqs_sin', 'visual.blocks.7.attn.rope.freqs_cos', 'visual.blocks.7.attn.rope.freqs_sin', 'visual.blocks.8.attn.rope.freqs_cos', 'visual.blocks.8.attn.rope.freqs_sin', 'visual.blocks.9.attn.rope.freqs_cos', 'visual.blocks.9.attn.rope.freqs_sin', 'visual.blocks.10.attn.rope.freqs_cos', 'visual.blocks.10.attn.rope.freqs_sin', 'visual.blocks.11.attn.rope.freqs_cos', 'visual.blocks.11.attn.rope.freqs_sin', 'visual.blocks.12.attn.rope.freqs_cos', 'visual.blocks.12.attn.rope.freqs_sin', 'visual.blocks.13.attn.rope.freqs_cos', 'visual.blocks.13.attn.rope.freqs_sin', 'visual.blocks.14.attn.rope.freqs_cos', 'visual.blocks.14.attn.rope.freqs_sin', 'visual.blocks.15.attn.rope.freqs_cos', 'visual.blocks.15.attn.rope.freqs_sin', 'visual.blocks.16.attn.rope.freqs_cos', 'visual.blocks.16.attn.rope.freqs_sin', 'visual.blocks.17.attn.rope.freqs_cos', 'visual.blocks.17.attn.rope.freqs_sin', 'visual.blocks.18.attn.rope.freqs_cos', 'visual.blocks.18.attn.rope.freqs_sin', 'visual.blocks.19.attn.rope.freqs_cos', 'visual.blocks.19.attn.rope.freqs_sin', 'visual.blocks.20.attn.rope.freqs_cos', 'visual.blocks.20.attn.rope.freqs_sin', 'visual.blocks.21.attn.rope.freqs_cos', 'visual.blocks.21.attn.rope.freqs_sin', 'visual.blocks.22.attn.rope.freqs_cos', 'visual.blocks.22.attn.rope.freqs_sin', 'visual.blocks.23.attn.rope.freqs_cos', 'visual.blocks.23.attn.rope.freqs_sin']\r\n2024-10-26 17:58:31,844 - root - INFO - Loading PuLID-Flux model.\r\n2024-10-26 17:58:39,170 - root - INFO - Requested to load Flux\r\n2024-10-26 17:58:39,170 - root - INFO - Loading 1 new model\r\n2024-10-26 17:58:47,553 - root - INFO - loaded completely 0.0 7276.8634033203125 True\r\n2024-10-26 17:58:48,340 - root - ERROR - !!! Exception during processing !!! expected scalar type Half but found BFloat16\r\n2024-10-26 17:58:48,345 - root - ERROR - Traceback (most recent call last):\r\n  File \"C:\\Program Files\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 323, in execute"
      },
      {
        "user": "ThisCodeIsMine",
        "created_at": "2024-10-28T07:56:49Z",
        "body": "> Hello! I wonder if the graphics cards of series 20 cannot be used. The prompt is: Custom sampler (advanced). expected scalar type Half but found BFloat16 @balazik\r\nDid you already found a solution? I get the same error with all updated files. Using a NVIDIA GeForce RTX 2080 Ti.\r\nI don't know what the problem could be but I'm sure there must be only a configuration that can be adapted. "
      }
    ]
  }
]