[
  {
    "number": 195,
    "title": "Support for PixArt models please\ud83d\ude4f",
    "created_at": "2025-01-04T15:29:15Z",
    "closed_at": "2025-01-08T15:39:37Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/195",
    "body": "He visto que ComfyUI ahora soporta de forma nativa estos modelos tanto Sigma como Alpha pero el modelo t5 que uso es un Q6 dada la poca capacidad de memoria que tengo, agradecer\u00eda infinitamente que incluyeras la configuraci\u00f3n para este CLIP por favor. Saludos desde la isla de Cuba amigo",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/195/comments",
    "author": "Mike257art",
    "comments": [
      {
        "user": "city96",
        "created_at": "2025-01-04T20:54:49Z",
        "body": "Sorry, forgot to add it after the PixArt PR got merged in comfy. Should be working now."
      }
    ]
  },
  {
    "number": 56,
    "title": "Error occurred when executing DualCLIPLoaderGGUF:",
    "created_at": "2024-08-21T14:15:20Z",
    "closed_at": "2024-08-21T22:53:26Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/56",
    "body": "I'm getting this errors message wen loading the dual clip Gguf loader, im using the t5-v1_1-xxl-encoder-Q5_K_M.gguf\r\n\r\nError occurred when executing DualCLIPLoaderGGUF:\r\n\r\nmodule 'comfy.sd' has no attribute 'load_text_encoder_state_dicts'\r\n\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 316, in execute\r\noutput_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 191, in get_output_data\r\nreturn_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 168, in _map_node_over_list\r\nprocess_inputs(input_dict, i)\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 157, in process_inputs\r\nresults.append(getattr(obj, func)(**inputs))\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 224, in load_clip\r\nreturn (self.load_patcher(clip_paths, clip_type, self.load_data(clip_paths)),)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 183, in load_patcher\r\nclip = comfy.sd.load_text_encoder_state_dicts(\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/56/comments",
    "author": "kakachiex2",
    "comments": [
      {
        "user": "a-ru2016",
        "created_at": "2024-08-21T14:19:06Z",
        "body": "Please update comfyUI."
      }
    ]
  },
  {
    "number": 18,
    "title": "12GB VRAM getting OOM with Q8_0, but not in Forge.",
    "created_at": "2024-08-15T23:32:44Z",
    "closed_at": "2024-08-16T04:25:56Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/18",
    "body": "Able to use Forge with default settings to run the Q8_0 variant on a 3060 w 12GB VRAM and 32GB sys RAM.\r\n\r\nBut with this node in Comfy I always get OOM trying to load the models up for inference. Tried --lowvram mode too.",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/18/comments",
    "author": "caustiq",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-08-16T02:37:29Z",
        "body": "Just pushed a patch that should fix this. Please test it!"
      },
      {
        "user": "NHLOCAL",
        "created_at": "2024-08-16T04:00:24Z",
        "body": "I also had the same problem with lowvram, now the problem is solved with the update push!\r\n\r\nBut a comment is still received when loading the model:\r\n```\r\nC:\\Users\\Public\\StableSwarmUI\\StableSwarmUI-master\\dlbackend\\comfy\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\dequant.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n  data = torch.tensor(tensor.data)\r\nC:\\Users\\Public\\StableSwarmUI\\StableSwarmUI-master\\dlbackend\\comfy\\ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\r\n```"
      },
      {
        "user": "caustiq",
        "created_at": "2024-08-16T07:49:32Z",
        "body": "edit: ran all night with no issues\r\n\r\n==\r\nI did get an OOM after about 20-30 gens of smooth sailing. \r\nI turned off Xorg and ran headless to free up a little more cause I was at 94-96% VRAM usage.\r\nBut it seems like Comfy will just use even more if it can, so still riding close to the edge.\r\nDunno if it's related to your changes so just thought I'd report that one OOM.\r\n\r\n"
      }
    ]
  }
]