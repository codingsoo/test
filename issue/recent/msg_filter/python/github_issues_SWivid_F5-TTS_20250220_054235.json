[
  {
    "number": 792,
    "title": "FileNotFoundError: [WinError 2]",
    "created_at": "2025-02-17T06:21:27Z",
    "closed_at": "2025-02-17T17:13:51Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/792",
    "body": "### Checks\n\n- [x] This template is only for usage issues encountered.\n- [x] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [x] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nwhen i click synthesize i get this error :(\n\n\n\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n### Steps to Reproduce\n\ni did everything the documentation said exactly and everything was going fine until i actually tried to run the synthesize button\n\n### \u2714\ufe0f Expected Behavior\n\nWorking, generating audio\n\n### \u274c Actual Behavior\n\nError, nothing happens",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/792/comments",
    "author": "SulaimanBasal",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:23:18Z",
        "body": "> FileNotFoundError: [WinError 2] The system cannot find the file specified\n\nmake sure you have fully upload the audio"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:29:37Z",
        "body": "you could probably provide more details as suggest by the issue template.\notherwise we could hardly help cuz no info"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:31:25Z",
        "body": "@SWivid yea i'm sorry about that, i'm a total noob at this... \n\nthis is the full message when i upload, will that help?\n\n\nC:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\nTraceback (most recent call last):\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n    response = await route_utils.call_process_api(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 2051, in process_api\n    result = await self.call_function(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 1598, in call_function\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\utils.py\", line 883, in wrapper\n    response = f(*args, **kwargs)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 241, in basic_tts\n    audio_out, spectrogram_path, ref_text_out = infer(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 131, in infer\n    ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, ref_text, show_info=show_info)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\utils_infer.py\", line 294, in preprocess_ref_audio_text\n    aseg = AudioSegment.from_file(ref_audio_orig)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\audio_segment.py\", line 728, in from_file\n    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py\", line 274, in mediainfo_json\n    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\subprocess.py\", line 971, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\subprocess.py\", line 1456, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n\n\n\n"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:36:24Z",
        "body": "@SWivid also when starting the gradio app  i get this before given the local link:\nStarting app...\nINFO: Could not find files for the given pattern(s).\n\nwould that be what's causing the issue?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:52:06Z",
        "body": "> C:\\Users\\SUL.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n\nmay try #739 \ninstall ffmpeg"
      }
    ]
  },
  {
    "number": 630,
    "title": "Checkpoint saving differences",
    "created_at": "2024-12-15T09:30:49Z",
    "closed_at": "2024-12-17T02:16:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/630",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\nWhen having `grad_accumulation_steps` set to a different value than `1`, the checkpoint saving is a bit unexpected:\r\n\r\nIn `trainer.py` for setting `save_per_updates`:\r\n\r\n```python\r\nif global_step % (self.save_per_updates * self.grad_accumulation_steps) == 0:\r\n    self.save_checkpoint(global_step)\r\n```\r\n\r\nfor setting `last_per_steps`:\r\n\r\n```python\r\nif global_step % self.last_per_steps == 0:\r\n    self.save_checkpoint(global_step, last=True)\r\n```\r\n\r\nConsequently, for my global batch-size of `19200*8` the value of `save_per_updates` needs to be divided by `8` to be comparable to the setting of `last_per_steps` and the overall variable `global_step` shown via `tqdm`.\r\n\r\nIs this intended ?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/630/comments",
    "author": "lumpidu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T10:11:43Z",
        "body": "yes, intended\r\nstep and update are different stuffs"
      }
    ]
  },
  {
    "number": 618,
    "title": "Numeric digits can only be pronounced using Chinese pronunciation, not English pronunciation.",
    "created_at": "2024-12-12T02:26:11Z",
    "closed_at": "2024-12-13T07:41:03Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/618",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\nWhen synthesizing numeric digits from an English text, these digits can only be pronounced using Chinese pronunciation, not English pronunciation. Date texts (e.g., Mar. 12, 2024) also have a similar problem.\n\n### 2. What is your suggested solution?\n\nSince I am using the API, I need a parameter like language=\"en\" and language=\"ch\" to identify how to pronounce the numeric digits.\n\n### 3. Additional context or comments\n\nno.\n\n### 4. Can you help us with this feature?\n\n- [ ] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/618/comments",
    "author": "likangkao",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-12T07:59:42Z",
        "body": "you need to transform the numeric digits into english words, it can works well"
      }
    ]
  },
  {
    "number": 597,
    "title": "\u751f\u6210\u7684\u8bed\u97f3\u5f00\u5934\u6709\u566a\u97f3\uff0c\u4eba\u751f\u6bd4\u8f83\u5feb\u3002",
    "created_at": "2024-12-06T10:50:35Z",
    "closed_at": "2024-12-06T12:16:55Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/597",
    "body": "### Checks\n\n- [ ] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [ ] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nRTX 3090\n\n### Steps to Reproduce\n\n1. \u6700\u65b0\u7684\u4ee3\u7801\r\n2. f5-tts_infer-cli \\\r\n--model \"F5-TTS\" \\\r\n--ref_audio \"test.wav\" \\\r\n--ref_text \"\u5927\u5bb6\u90fd\u51c6\u5907\u597d\u4e86\u5417\uff1f\u5927\u53ef\u5927\u53ef\uff0c\u54b1\u4eec\u5f00\u59cb\u4e0a\u8bfe\u5427\u3002\" \\\r\n--gen_text \"\u9762\u6761\u662f\u4e00\u79cd\u5f62\u72b6\u7ec6\u957f\u7684\u9762\u98df\u3002\u9762\u6761\u53ef\u4ee5\u5f88\u77ed\uff0c\u5982\u4f7f\u5934\u53d1\uff0c\u4e5f\u53ef\u4ee5\u53c8\u957f\u53c8\u7c97\uff0c\u5982\u835e\u9ea6\u9762\uff0c\u8fd8\u53ef\u4ee5\u540c\u6837\u957f\u4f46\u66f4\u7ec6\uff0c\u5982\u610f\u5927\u5229\u9762\uff0c\u6216\u6781\u7ec6\uff0c\u5982\u73bb\u7483\u9762\u3002\u65e0\u8bba\u662f\u54ea\u79cd\u5f62\u72b6\u548c\u5f62\u6001\u7684\u9762\u6761\uff0c\u90fd\u662f\u7528\u67d0\u79cd\u9762\u7c89\uff0c\u5c0f\u9ea6\u7c89\u3001\u5927\u7c73\u7c89\u3001\u5927\u8c46\u7c89\u3001\u8c4c\u8c46\u7c89\u7b49\uff09\u5236\u4f5c\u800c\u6210\uff0c\u4e0d\u52a0\u53d1\u9175\u5242\u3002\u9762\u6761\u5728\u4e16\u754c\u5404\u5730\u7684\u8bb8\u591a\u83dc\u7cfb\u4e2d\u90fd\u6709\u5236\u4f5c\uff0c\u4f46\u4e3b\u8981\u662f\u5728\u4e1c\u4e9a\u548c\u5730\u4e2d\u6d77\u83dc\u7cfb\u4e2d\u3002\u56db\u5343\u5e74\u524d\u5728\u4e2d\u56fd\u9752\u6d77\u7701\u53d1\u73b0\u7684\u9762\u6761\u5316\u77f3\u88ab\u8ba4\u4e3a\u662f\u4e16\u754c\u4e0a\u6700\u53e4\u8001\u7684\u9762\u6761\uff0c\u7814\u7a76\u8868\u660e\u6700\u65e9\u7684\u9762\u6761\u662f\u5728\u4e2d\u56fd\u65b0\u77f3\u5668\u65f6\u4ee3\u53d1\u5c55\u8d77\u6765\u7684\u3002\u9762\u6761\u6709\u65b0\u9c9c\u7684\u548c\u5e72\u7684\u4e24\u79cd\uff0c\u662f\u591a\u79cd\u83dc\u80b4\u7684\u57fa\u7840\u3002\u7279\u522b\u503c\u5f97\u4e00\u63d0\u7684\u662f\u6c64\u9762\uff0c\u5b83\u51e0\u4e4e\u5e7f\u6cdb\u5b58\u5728\u4e8e\u4e16\u754c\u5404\u5730\u7684\u6240\u6709\u6587\u5316\u4e2d\u3002\"\n\n### \u2714\ufe0f Expected Behavior\n\n\u80fd\u591f\u4fee\u590d\u8fd9\u4e2a\u95ee\u9898\n\n### \u274c Actual Behavior\n\n\u5f00\u5934\u6709\u566a\u97f3\u3002\u6709\u4e86\u4eba\u58f0\u540e \u901f\u5ea6\u6bd4\u8f83\u5feb\u3002\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/597/comments",
    "author": "dttlgotv",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T12:16:55Z",
        "body": "please review the **Checks**\r\n\r\nuse a better reference audio, (~10s good quality)"
      },
      {
        "user": "dttlgotv",
        "created_at": "2024-12-09T08:28:03Z",
        "body": "> please review the **Checks**\r\n> \r\n> use a better reference audio, (~10s good quality)\r\n\r\n\u591a\u8c22\uff0c\u6709\u6548\u679c\u3002\r\n\r\n\u5982\u679c\u6211\u7684\u751f\u6210\u6587\u672c\u662f\u8fd9\u4e2a\uff0c \u5408\u6210\u7684\u8bed\u97f3\u633a\u597d\u3002\r\n\r\n--gen_text \"\u9762\u6761\u662f\u4e00\u79cd\u5f62\u72b6\u7ec6\u957f\u7684\u9762\u98df\u3002\u9762\u6761\u53ef\u4ee5\u5f88\u77ed\uff0c\u5982\u4f7f\u5934\u53d1\uff0c\u4e5f\u53ef\u4ee5\u53c8\u957f\u53c8\u7c97\uff0c\u5982\u835e\u9ea6\u9762\uff0c\u8fd8\u53ef\u4ee5\u540c\u6837\u957f\u4f46\u66f4\u7ec6\uff0c\u5982\u610f\u5927\u5229\u9762\uff0c\u6216\u6781\u7ec6\uff0c\u5982\u73bb\u7483\u9762\u3002\u65e0\u8bba\u662f\u54ea\u79cd\u5f62\u72b6\u548c\u5f62\u6001\u7684\u9762\u6761\uff0c\u90fd\u662f\u7528\u67d0\u79cd\u9762\u7c89\uff0c\u5c0f\u9ea6\u7c89\u3001\u5927\u7c73\u7c89\u3001\u5927\u8c46\u7c89\u3001\u8c4c\u8c46\u7c89\u7b49\uff09\u5236\u4f5c\u800c\u6210\uff0c\u4e0d\u52a0\u53d1\u9175\u5242\u3002\"\r\n\r\n\u4f46\u662f\u5982\u679c\u6211\u7684\u751f\u6210\u6587\u672c\u662f\u4e0a\u9762\u6587\u672c\u7684\u4e24\u6b21\u91cd\u590d\uff08\u5982\u4e0b\uff09\uff0c\u5408\u6210\u7684\u58f0\u97f3\u5c31\u4e0d\u6e05\u6670\uff0c\u4e14\u6709\u5feb\u8fdb\u7684\u611f\u89c9\u3002\r\n--gen_text \"\u9762\u6761\u662f\u4e00\u79cd\u5f62\u72b6\u7ec6\u957f\u7684\u9762\u98df\u3002\u9762\u6761\u53ef\u4ee5\u5f88\u77ed\uff0c\u5982\u4f7f\u5934\u53d1\uff0c\u4e5f\u53ef\u4ee5\u53c8\u957f\u53c8\u7c97\uff0c\u5982\u835e\u9ea6\u9762\uff0c\u8fd8\u53ef\u4ee5\u540c\u6837\u957f\u4f46\u66f4\u7ec6\uff0c\u5982\u610f\u5927\u5229\u9762\uff0c\u6216\u6781\u7ec6\uff0c\u5982\u73bb\u7483\u9762\u3002\u65e0\u8bba\u662f\u54ea\u79cd\u5f62\u72b6\u548c\u5f62\u6001\u7684\u9762\u6761\uff0c\u90fd\u662f\u7528\u67d0\u79cd\u9762\u7c89\uff0c\u5c0f\u9ea6\u7c89\u3001\u5927\u7c73\u7c89\u3001\u5927\u8c46\u7c89\u3001\u8c4c\u8c46\u7c89\u7b49\uff09\u5236\u4f5c\u800c\u6210\uff0c\u4e0d\u52a0\u53d1\u9175\u5242\u3002\u9762\u6761\u662f\u4e00\u79cd\u5f62\u72b6\u7ec6\u957f\u7684\u9762\u98df\u3002\u9762\u6761\u53ef\u4ee5\u5f88\u77ed\uff0c\u5982\u4f7f\u5934\u53d1\uff0c\u4e5f\u53ef\u4ee5\u53c8\u957f\u53c8\u7c97\uff0c\u5982\u835e\u9ea6\u9762\uff0c\u8fd8\u53ef\u4ee5\u540c\u6837\u957f\u4f46\u66f4\u7ec6\uff0c\u5982\u610f\u5927\u5229\u9762\uff0c\u6216\u6781\u7ec6\uff0c\u5982\u73bb\u7483\u9762\u3002\u65e0\u8bba\u662f\u54ea\u79cd\u5f62\u72b6\u548c\u5f62\u6001\u7684\u9762\u6761\uff0c\u90fd\u662f\u7528\u67d0\u79cd\u9762\u7c89\uff0c\u5c0f\u9ea6\u7c89\u3001\u5927\u7c73\u7c89\u3001\u5927\u8c46\u7c89\u3001\u8c4c\u8c46\u7c89\u7b49\uff09\u5236\u4f5c\u800c\u6210\uff0c\u4e0d\u52a0\u53d1\u9175\u5242\u3002\"\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-09T12:34:06Z",
        "body": "\u957f\u7684\u6587\u672c\u4f1a\u88ab\u81ea\u52a8\u5207\u5206\uff0c\u6240\u4ee5\u53ef\u80fd\u4f1a\u5207\u5230\u4e0d\u7406\u60f3\u7684\u65ad\u53e5\u5bfc\u81f4\u8bed\u901f\u4e0d\u5747\u5300\u3002"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-09T13:03:29Z",
        "body": "\u662f\u6309\u7167\u6807\u70b9\u5207\u7684\uff0c\u4f46double\u4e4b\u540e\u53ef\u80fd\u4f1a\u5207\u5230\u9017\u53f7\uff0c\u5982\u679c\u6574\u53e5\u5f88\u957f"
      }
    ]
  },
  {
    "number": 546,
    "title": "Add functionality to the Transcription section",
    "created_at": "2024-11-28T04:53:46Z",
    "closed_at": "2024-12-13T07:49:04Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/546",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\nI can't upload a large volume of audio files in the \"Transcription data\" section to create a dataset through the audio file upload window, everything freezes. My file size is 20 GB\n\n### 2. What is your suggested solution?\n\nCan you add a window in \"Gradio\" in the \"Transcription data\" section where I can specify the path to the folder where the audio files are located, this will eliminate the freeze and make it possible to upload a large volume of files to create a dataset.\r\n\r\nThank you\n\n### 3. Additional context or comments\n\n_No response_\n\n### 4. Can you help us with this feature?\n\n- [ ] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/546/comments",
    "author": "nikish3",
    "comments": [
      {
        "user": "danielw97",
        "created_at": "2024-11-28T05:21:53Z",
        "body": "I don't have the interface in front of me at the moment, but there is an audio from path checkbox in the gradio training interface if memory serves.\r\nYou can then place your files into the dataset subdirectory of your dataset.\r\ni.e. if your dataset is called voice1, the directory would be voice1/dataset\r\nHope this helps a bit."
      }
    ]
  },
  {
    "number": 471,
    "title": "Moving storage, but f5 still load from old location",
    "created_at": "2024-11-15T23:48:34Z",
    "closed_at": "2024-11-16T02:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/471",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI cloned my f5 installation to a new drive (E:\\ -> Y:\\) and ran f5-tts_finetune-gradio from the new location.\r\nSeems the path for my data checkpoints is still mapped to E:\\. I need help to re-map, so I can delete the data on the source drive.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/471/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-16T01:36:20Z",
        "body": "`pip uninstall f5-tts`\r\nDo `pip install -e .` at new path."
      }
    ]
  },
  {
    "number": 459,
    "title": "load finetune data",
    "created_at": "2024-11-13T09:26:17Z",
    "closed_at": "2024-11-13T11:59:53Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/459",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\ntrain on 4 a800\n\n### Steps to Reproduce\n\nI encountered an issue when trying to load my fine-tuned checkpoint. During the load_state_dict() call, I received the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 151, in <module>\r\n    infer_tts()\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 72, in infer_tts\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 216, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, dtype=dtype, use_ema=use_ema)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 168, in load_checkpoint\r\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\r\n  File \"/remote-home1/ycyuan/conda/anaconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n    Missing key(s) in state_dict: \"transformer.time_embed.time_mlp.0.weight\", \"transformer.time_embed.time_mlp.0.bias\", \"transformer.time_embed.time_mlp.2.weight\", \"transformer.time_embed.time_mlp.2.bias\", \"transformer.text_embed.text_embed.weight\", \"transformer.text_embed.text_blocks.0.dwconv.weight\", \"transformer.text_embed.text_blocks.0.dwconv.bias\", \"transformer.text_embed.text_blocks.0.norm.weight\", \"transformer.text_embed.text_blocks.0.norm.bias\", \"transformer.text_embed.text_blocks.0.pwconv1.weight\", \"transformer.text_embed.text_blocks.0.pwconv1.bias\", \"transformer.text_embed.text_blocks.0.grn.gamma\", \"transformer.text_embed.text_blocks.0.grn.beta\", \"transformer.text_embed.text_blocks.0.pwconv2.weight\", \"transformer.text_embed.text_blocks.0.pwconv2.bias\", \"transformer.text_embed.text_blocks.1.dwconv.weight\", \"transformer.text_embed.text_blocks.1.dwconv.bias\", \"transformer.text_embed.text_blocks.1.norm.weight\", \"transformer.text_embed.text_blocks.1.norm.bias\"...\r\n    Unexpected key(s) in state_dict: \"module.transformer.time_embed.time_mlp.0.weight\", \"module.transformer.time_embed.time_mlp.0.bias\", \"module.transformer.time_embed.time_mlp.2.weight\", \"module.transformer.time_embed.time_mlp.2.bias\", \"module.transformer.text_embed.text_embed.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.bias\", \"module.transformer.text_embed.text_blocks.0.norm.weight\", \"module.transformer.text_embed.text_blocks.0.norm.bias\", \"module.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"module.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"module.transformer.text_embed.text_blocks.0.grn.gamma\", \"module.transformer.text_embed.text_blocks.0.grn.beta\",...\r\n```\r\nHowever, when I print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected:\r\n```\r\nOrderedDict([('transformer.time_embed.time_mlp.0.weight', tensor([[-0.0007, -0.0009, -0.0007,  ..., -0.0355,  0.0119,  0.0061], ...], device='cuda:3')),\r\n            ('transformer.time_embed.time_mlp.0.bias', tensor([ 0.0139,  0.0171,  0.0399,  ..., -0.0508,  0.0258, -0.0191], device='cuda:3'))]),....\r\n```\r\nHowever, when I attempt to load the checkpoint without any wrapping, the model loading fails due to the error above. If I wrap the model with DataParallel before loading the checkpoint (like this: model = torch.nn.DataParallel(model)), the loading works fine, but this leads to subsequent errors during inference.\r\n\r\nAdditionally, I have tried to remove the module prefix using the following function:\r\n```\r\ndef remove_module_prefix(state_dict):\r\n    new_state_dict = {}\r\n    for k, v in state_dict.items():\r\n        # Remove the 'module.' prefix\r\n        print(k)\r\n        new_k = k.replace('module.', '') if 'module.' in k else k\r\n        new_state_dict[new_k] = v\r\n    return new_state_dict\r\n```\r\nHowever, this approach does not work because, when I print the keys, they don't actually contain the module. prefix. Despite this, when loading the checkpoint, I still encounter a mismatch error regarding the module. prefix.\r\n\r\nI have not modified the checkpoint saving function. Here\u2019s the relevant code that I use to save checkpoints:\r\n```\r\ndef save_checkpoint(self, step, last=False):\r\n    self.accelerator.wait_for_everyone()\r\n    if self.is_main:\r\n        checkpoint = dict(\r\n            model_state_dict=self.accelerator.unwrap_model(self.model).state_dict(),\r\n            optimizer_state_dict=self.accelerator.unwrap_model(self.optimizer).state_dict(),\r\n            ema_model_state_dict=self.ema_model.state_dict(),\r\n            scheduler_state_dict=self.scheduler.state_dict(),\r\n            step=step,\r\n        )\r\n        if not os.path.exists(self.checkpoint_path):\r\n            os.makedirs(self.checkpoint_path)\r\n        rank = self.accelerator.process_index\r\n\r\n        if rank == 0:\r\n            if last:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_last.pt\")\r\n                print(f\"Saved last checkpoint at step {step}\")\r\n            else:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_{step}.pt\")\r\n```\r\n\r\nIs there a way to handle the mismatch of the module. prefix in the checkpoint file?\n\n### \u2714\ufe0f Expected Behavior\n\n_No response_\n\n### \u274c Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/459/comments",
    "author": "southwindyong",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-13T09:33:23Z",
        "body": "maybe you could provide the additional `File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\"` for more info to figure out the issue."
      },
      {
        "user": "southwindyong",
        "created_at": "2024-11-13T10:31:33Z",
        "body": "This is the file of ``/remote-home1/ycyuan/TTS/F5-TTS/inference.py``\r\n```\r\nimport os\r\nimport random\r\nimport json\r\nimport codecs\r\nimport numpy as np\r\nimport soundfile as sf\r\nimport tomli\r\nfrom pathlib import Path\r\nfrom f5_tts.infer.utils_infer import (\r\n    infer_process,\r\n    load_model,\r\n    load_vocoder,\r\n    preprocess_ref_audio_text,\r\n    remove_silence_for_generated_wav,\r\n)\r\nfrom f5_tts.model import DiT, UNetT\r\nimport re\r\n\r\ndef infer_tts():\r\n    # Directly specify the variables without argparse\r\n    print(\"\u5f00\u59cb TTS \u63a8\u7406\u8fc7\u7a0b...\")\r\n\r\n    config_path = Path(\"f5_tts\") / \"infer/examples/basic\" / \"basic.toml\"  \r\n    model = \"F5-TTS\"  # Choose \"F5-TTS\" or \"E2-TTS\"\r\n    ckpt_file = \"/remote-home1/ycyuan/TTS/F5-TTS/ckpts/F5TTS_Base/libritts_25s_1/model_last.pt\"  # Provide the path to checkpoint if necessary\r\n    vocab_file = \"\"  # Provide the vocab file path if necessary\r\n    text_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_text_2-6\"  # Text directory\r\n    audio_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_prompt_2_10s\"  # Audio directory\r\n    output_dir = \"./generate_data/libritts_25s_1\"  # Path to output folder\r\n    remove_silence = False  # Whether to remove silence from generated audio\r\n    vocoder_name = \"vocos\"  # Choose between \"vocos\" or \"bigvgan\"\r\n    load_vocoder_from_local = False  # Whether to load vocoder from local path\r\n    speed = 1.0  # Adjust audio generation speed (default is 1.0)\r\n\r\n    # Load configuration from the specified TOML file\r\n    print(f\"\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 {config_path}...\")\r\n    config = tomli.load(open(config_path, \"rb\"))\r\n    \r\n    # Use provided values or defaults from the config file\r\n    output_dir = output_dir if output_dir else config[\"output_dir\"]\r\n    model = model if model else config[\"model\"]\r\n    ckpt_file = ckpt_file if ckpt_file else \"\"\r\n    vocab_file = vocab_file if vocab_file else \"\"\r\n    remove_silence = remove_silence if remove_silence else config[\"remove_silence\"]\r\n    speed = speed\r\n    wave_path = Path(output_dir) / \"infer_cli_out.wav\"\r\n    \r\n    # Set vocoder local path based on the selected vocoder\r\n    print(f\"\u9009\u62e9\u7684 vocoder \u6a21\u578b: {vocoder_name}...\")\r\n    if vocoder_name == \"vocos\":\r\n        vocoder_local_path = \"../checkpoints/vocos-mel-24khz\"\r\n    elif vocoder_name == \"bigvgan\":\r\n        vocoder_local_path = \"../checkpoints/bigvgan_v2_24khz_100band_256x\"\r\n    \r\n    # Load the vocoder model\r\n    print(\"\u52a0\u8f7d vocoder \u6a21\u578b...\")\r\n    vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=load_vocoder_from_local, local_path=vocoder_local_path)\r\n\r\n    # Load the TTS model\r\n    print(f\"\u52a0\u8f7d TTS \u6a21\u578b: {model}...\")\r\n    if model == \"F5-TTS\":\r\n        model_cls = DiT\r\n        model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"F5-TTS\"\r\n            exp_name = \"F5TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    elif model == \"E2-TTS\":\r\n        model_cls = UNetT\r\n        model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"E2-TTS\"\r\n            exp_name = \"E2TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    \r\n    # Initialize the model\r\n    print(f\"\u4f7f\u7528 {model} \u6a21\u578b\u8fdb\u884c\u63a8\u7406...\")\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n    print(\"\u6a21\u578b\u52a0\u8f7d\u6210\u529f\u3002\")\r\n\r\n    # Load a random JSON file containing text\r\n    print(f\"\u4ece {text_dir} \u4e2d\u52a0\u8f7d\u968f\u673a JSON \u6587\u4ef6...\")\r\n    json_files = [f for f in os.listdir(text_dir) if f.endswith('.json')]\r\n    random_json_file = random.choice(json_files)\r\n    json_file_path = os.path.join(text_dir, random_json_file)\r\n\r\n    with open(json_file_path, 'r') as file:\r\n        json_data = json.load(file)\r\n        text = json_data[\"\u5b8c\u6574\u6587\u672c\"]\r\n        text_turns = json_data[\"\u6587\u4ef6\u6570\"]\r\n\r\n    # From the audio directory, select a random folder and corresponding audio file\r\n    print(f\"\u4ece {audio_dir} \u4e2d\u9009\u62e9\u4e00\u4e2a\u968f\u673a\u6587\u4ef6\u5939...\")\r\n    folders = [f for f in os.listdir(audio_dir) if os.path.isdir(os.path.join(audio_dir, f))]\r\n    while True:\r\n        random_folder = random.choice(folders)\r\n        folder_path = os.path.join(audio_dir, random_folder)\r\n        lab_files = [f for f in os.listdir(folder_path) if f.endswith('.lab')]\r\n        if not lab_files:\r\n            continue  # Skip folders without `.lab` files\r\n\r\n        random_lab_file = random.choice(lab_files)\r\n        lab_file_path = os.path.join(folder_path, random_lab_file)\r\n\r\n        # Read the corresponding `.lab` file to get prompt text\r\n        with open(lab_file_path, 'r') as lab_file:\r\n            prompt_text = lab_file.read().strip()\r\n        prompt_turns = len(prompt_text.splitlines())  # Number of lines = number of turns\r\n\r\n        if text_turns % 2 == prompt_turns % 2 and prompt_turns <= 4:  # Ensure matching turns and prompt_turns <= 4\r\n            break\r\n\r\n    # Choose the corresponding audio file\r\n    print(f\"\u9009\u62e9\u7684\u97f3\u9891\u6587\u4ef6: {random_lab_file}...\")\r\n    audio_file = random_lab_file.replace('.lab', '.wav')\r\n    ref_audio = os.path.join(folder_path, audio_file)\r\n    if not os.path.exists(ref_audio):\r\n        raise FileNotFoundError(f\"\u672a\u627e\u5230\u97f3\u9891\u6587\u4ef6 {ref_audio}\")\r\n\r\n    # Call the inference process\r\n    def main_process(ref_audio, text, gen_text, model_obj, mel_spec_type, remove_silence, speed):\r\n        print(f\"\u6b63\u5728\u751f\u6210\u97f3\u9891: {text}\")\r\n        generated_audio_segments = []\r\n        reg1 = r\"(?=\\[\\w+\\])\"\r\n        chunks = re.split(reg1, gen_text)\r\n        reg2 = r\"\\[(\\w+)\\]\"\r\n\r\n        # Process each chunk of the generated text\r\n        for text_chunk in chunks:\r\n            if not text_chunk.strip():\r\n                continue\r\n            match = re.match(reg2, text_chunk)\r\n            if match:\r\n                voice = match[1]\r\n            else:\r\n                print(\"\u6ca1\u6709\u627e\u5230\u8bed\u97f3\u6807\u7b7e\uff0c\u4f7f\u7528\u9ed8\u8ba4\u8bed\u97f3\u3002\")\r\n                voice = \"main\"\r\n            \r\n            print(f\"\u8bed\u97f3\u9009\u62e9: {voice}\")\r\n            gen_text = text_chunk.strip()\r\n            audio, final_sample_rate, spectrogram = infer_process(\r\n                ref_audio, text, gen_text, model_obj, vocoder, mel_spec_type=mel_spec_type, speed=speed\r\n            )\r\n            generated_audio_segments.append(audio)\r\n\r\n        if generated_audio_segments:\r\n            final_wave = np.concatenate(generated_audio_segments)\r\n\r\n            if not os.path.exists(output_dir):\r\n                os.makedirs(output_dir)\r\n\r\n            with open(wave_path, \"wb\") as f:\r\n                sf.write(f.name, final_wave, final_sample_rate)\r\n                if remove_silence:\r\n                    print(\"\u6b63\u5728\u53bb\u9664\u751f\u6210\u97f3\u9891\u4e2d\u7684\u9759\u97f3\u90e8\u5206...\")\r\n                    remove_silence_for_generated_wav(f.name)\r\n                print(f\"\u751f\u6210\u7684\u97f3\u9891\u5df2\u4fdd\u5b58\u5230 {f.name}\")\r\n\r\n    main_process(ref_audio, text, text, ema_model, vocoder_name, remove_silence, speed)\r\n\r\n# Call the infer_tts function to start the process\r\nif __name__ == \"__main__\":\r\n    infer_tts()\r\n```\r\nI think the text and prompt wav file is not important because it's all right"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-13T10:58:49Z",
        "body": "> print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected\r\n\r\nmaybe you have changed the code related with ema_model setup?\r\n\r\ntry do the removal of the module prefix using the function you got to `checkpoint['ema_model_state_dict']` see if works"
      }
    ]
  },
  {
    "number": 412,
    "title": "\u52a0\u8f7d\u897f\u73ed\u7259\u548c\u65e5\u8bed\u6a21\u578b\u7684\u65f6\u5019\uff0c\u62a5\u9519\uff1aRuntimeError: Error(s) in loading state_dict for CFM:",
    "created_at": "2024-11-07T02:20:30Z",
    "closed_at": "2024-11-07T05:19:54Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/412",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n\u52a0\u8f7dDiscussions\u4e2d\u63d0\u4f9b\u7684\u8bad\u7ec3\u597d\u7684\u65e5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u6a21\u578b\u7684\u65f6\u5019\u62a5\u9519\uff1a\r\n\u65e5\u8bed\u62a5\u9519\uff1a\r\nmodel :  /root/F5-TTS/ckpts/.../model_1108224.pt\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\n\u897f\u73ed\u7259\u8bed\u62a5\u9519\uff1a\r\nmodel :  /root/F5-TTS/ckpts/jpgallegoar/model_1200000.safetensors\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        Missing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\". ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/412/comments",
    "author": "liuhui881125",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-07T05:18:36Z",
        "body": "> RuntimeError: Error(s) in loading state_dict for CFM:\r\n> size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\nUse corresponding vocab.txt\r\n\r\n> RuntimeError: Error(s) in loading state_dict for CFM:\r\nMissing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\".\r\n\r\npull latest repo commit"
      }
    ]
  },
  {
    "number": 381,
    "title": "ValueError: Unknown scheme for proxy URL URL('socks://127.0.0.1:1089/')",
    "created_at": "2024-11-03T11:43:13Z",
    "closed_at": "2024-11-03T11:59:51Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/381",
    "body": "\r\n(f5)$ f5-tts_infer-gradio \r\nTraceback (most recent call last):\r\n  File \"/home/jk/miniconda3/envs/f5/bin/f5-tts_infer-gradio\", line 5, in <module>\r\n    from f5_tts.infer.infer_gradio import main\r\n  File \"/media/jk/000BCAF400087F74/F5-TTS/F5-TTS/src/f5_tts/infer/infer_gradio.py\", line 8, in <module>\r\n    import gradio as gr\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/__init__.py\", line 3, in <module>\r\n    import gradio._simple_templates\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/_simple_templates/__init__.py\", line 1, in <module>\r\n    from .simpledropdown import SimpleDropdown\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/_simple_templates/simpledropdown.py\", line 6, in <module>\r\n    from gradio.components.base import Component, FormComponent\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/components/__init__.py\", line 1, in <module>\r\n    from gradio.components.annotated_image import AnnotatedImage\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/components/annotated_image.py\", line 13, in <module>\r\n    from gradio import processing_utils, utils\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/processing_utils.py\", line 99, in <module>\r\n    sync_client = httpx.Client(transport=sync_transport)\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_client.py\", line 693, in __init__\r\n    proxy_map = self._get_proxy_map(proxies or proxy, allow_env_proxies)\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_client.py\", line 218, in _get_proxy_map\r\n    return {\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_client.py\", line 219, in <dictcomp>\r\n    key: None if url is None else Proxy(url=url)\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_config.py\", line 338, in __init__\r\n    raise ValueError(f\"Unknown scheme for proxy URL {url!r}\")\r\nValueError: Unknown scheme for proxy URL URL('socks://127.0.0.1:1089/')\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/381/comments",
    "author": "jakeytan",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-03T11:50:40Z",
        "body": "are you using system proxy?"
      },
      {
        "user": "jakeytan",
        "created_at": "2024-11-03T11:56:13Z",
        "body": "> are you using system proxy?\r\n\r\nyes ! qv2ray\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T11:58:14Z",
        "body": "so it's a network problem, try using a proxy that will not take over all system (e.g. not using tunnel mode)\r\nand if you could connect to hf directly, not using proxy."
      },
      {
        "user": "jakeytan",
        "created_at": "2024-11-03T11:59:59Z",
        "body": "huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T12:01:22Z",
        "body": "> huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\r\n\r\nyeh, so cannot connect directly, then try using a proxy that will not take over all system (e.g. not using tunnel mode)\r\nor you can manually download the ckpt files, and following instruction in readme to load the local ckpt file."
      }
    ]
  },
  {
    "number": 359,
    "title": "small update gradio finetune",
    "created_at": "2024-11-01T09:23:40Z",
    "closed_at": "2024-11-01T10:22:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/359",
    "body": "@SWivid just small fix stuff \r\nso now when audio stereo always get duraction mono and resample\r\nafter train take case stereo to mono and resample\r\nand just fix error speling the bf16",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/359/comments",
    "author": "lpscr",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T10:22:32Z",
        "body": "@lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification.\r\nJust remove the num_channel in eq is fine."
      }
    ]
  },
  {
    "number": 337,
    "title": "RuntimeError: Error(s) in loading state_dict for EMA",
    "created_at": "2024-10-30T17:01:15Z",
    "closed_at": "2024-10-31T01:26:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/337",
    "body": "\u6c42\u52a9\uff0c\u662f\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u4e0d\u5bf9\u5417\uff0c\u6211\u53ea\u6539\u4e86cache_path\u4e3a\u672c\u5730\u7684\u7edd\u5bf9\u8def\u5f84\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 161, in <module>\r\n    main()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 154, in main\r\n    trainer.train(\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 248, in train\r\n    start_step = self.load_checkpoint()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 168, in load_checkpoint\r\n    self.ema_model.load_state_dict(checkpoint[\"ema_model_state_dict\"])\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 512]) from checkpoint, the shape in current model is torch.Size([3413, 512]).\r\nTraceback (most recent call last):\r\n  File \"/data/env/f5-tts/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1168, in launch_command\r\n    simple_launcher(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 763, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/data/env/f5-tts/bin/python', 'src/f5_tts/train/finetune_cli.py', '--exp_name', 'F5TTS_Base', '--learning_rate', '1e-05', '--batch_size_per_gpu', '2400', '--batch_size_type', 'frame', '--max_samples', '64', '--grad_accumulation_steps', '1', '--max_grad_norm', '1', '--epochs', '18107', '--num_warmup_updates', '14', '--save_per_updates', '26', '--last_per_steps', '6', '--dataset_name', 'yn_1030', '--finetune', 'True', '--tokenizer', 'pinyin', '--log_samples', 'True', '--logger', 'wandb']' returned non-zero exit status 1.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/337/comments",
    "author": "russell-shu",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T00:51:44Z",
        "body": "vocab\u5927\u5c0f\u4e0d\u5bf9\uff0c\u5982\u679c\u60f3finetune\u7684\u8bdd\uff0c\u4e0d\u8981\u66f4\u6539vocab"
      }
    ]
  },
  {
    "number": 322,
    "title": "no such file f5-tts_infer-gradio",
    "created_at": "2024-10-30T01:20:47Z",
    "closed_at": "2024-10-30T11:16:59Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/322",
    "body": "# Launch a Gradio app (web interface)\r\nf5-tts_infer-gradio\r\nThere is no such file in the repo. cannot run it.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/322/comments",
    "author": "michaeltran33",
    "comments": [
      {
        "user": "justinjohn0306",
        "created_at": "2024-10-30T02:26:23Z",
        "body": "> # Launch a Gradio app (web interface)\r\n> f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n\r\ncd F5-TTS\r\npip install -e ."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T10:42:55Z",
        "body": "> > # Launch a Gradio app (web interface)\r\n> > f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n> \r\n> cd F5-TTS pip install -e .\r\nno such file regardless. Best to review the repo. \r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-10-30T10:46:01Z",
        "body": "Please note that this command is available when f5tts is installed as a pip package, not before.\r\nHope this helps a bit."
      }
    ]
  },
  {
    "number": 314,
    "title": "finetune-gradio \u8f6c\u5199\u7684\u65f6\u5019\u8017\u65f6\u5f88\u957f\uff0c\u5e76\u4e14\u6bcf\u6b21\u90fd\u4f1a\u5931\u8d25",
    "created_at": "2024-10-29T08:10:31Z",
    "closed_at": "2024-10-29T08:24:12Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/314",
    "body": "transcribe complete samples : 0\r\npath : /root/F5-TTS/src/f5_tts/../../data/my_spe_pinyin/wavs\r\nerror files : 6",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/314/comments",
    "author": "liuhui881125",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-29T08:14:38Z",
        "body": "check network to huggingface, or manully download `whisper-large-v3-turbo` ckpt files and put under `.cache/huggingface/hub`"
      }
    ]
  },
  {
    "number": 197,
    "title": "Minimal GPU Memory Requirements for Running the Model??",
    "created_at": "2024-10-21T07:10:33Z",
    "closed_at": "2024-10-21T11:24:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/197",
    "body": "I would like to know:\r\n\r\nWhat is the minimal GPU memory requirement for running this model smoothly?\r\nIs there a recommended batch size or other configurations (like precision or memory optimizations) I should use to reduce memory usage?\r\ni managed to change batch_size of trainer.py script, utils_infer.py still encountering cuda oom!! can you tell me the minimal requirements of GPU mmeory??",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/197/comments",
    "author": "sachin-seisei",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:05:56Z",
        "body": "the minimal GPU memory is like 2G (just loading F5/E2 TTS model, and not leverage ASR model to do transcription)\r\nbtw you were to do training or inference.\r\n"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:08:13Z",
        "body": "@SWivid so inference only i am doing as of now!! thats why i want to know in inference also why its showing cuda oom!!\r\nPS: i am not leveraging ASR even i am providing ref text also and not an empty string!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:15:49Z",
        "body": "which script are you using? gradio_app.py or inference-cli.py"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:16:16Z",
        "body": "inference-cli.py \r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:48:26Z",
        "body": "how many gpu mem do you have.\r\ni just tweak the `inference-cli.py` script, make sure it is not loading unneeded asr pipeline.\r\nit takes 1622M gpu mem for me running it"
      }
    ]
  },
  {
    "number": 192,
    "title": "RuntimeError: Input type (c10::Half) and bias type (float) should be the same",
    "created_at": "2024-10-21T01:57:10Z",
    "closed_at": "2024-10-21T17:55:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/192",
    "body": "I just started getting this error with the latest build:\r\n```\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/ttspod/speech/f5.py\", line 214, in infer_batch\r\n    generated_wave = self.vocos.decode(generated_mel_spec.cpu())\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n        ^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 308, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/192/comments",
    "author": "ajkessel",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T02:32:21Z",
        "body": "`generated = generated.to(torch.float32)` should be added as we done along with .half() for model and input\r\n```\r\n# Final result\r\ngenerated = generated.to(torch.float32)\r\ngenerated = generated[:, ref_audio_len:, :]\r\ngenerated_mel_spec = generated.permute(0, 2, 1)\r\ngenerated_wave = vocos.decode(generated_mel_spec.cpu())\r\n```"
      }
    ]
  },
  {
    "number": 190,
    "title": "Broken on MPS",
    "created_at": "2024-10-20T19:29:57Z",
    "closed_at": "2024-10-20T20:06:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/190",
    "body": "Not sure what happened, but looks like the app is broken on Macs at the moment.\r\n\r\nJust did a fresh install and the app itself runs, but the resulting audio is empty.\r\n\r\nAlso I am not sure if the logs are useful but pasting just in case:\r\n\r\n```\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n  warnings.warn(\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n\r\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\ngen_text 0 Reference text will be automatically transcribed with Whisper if not provided. For best results, keep your reference clips short\r\nBuilding prefix dict from the default dictionary ...\r\nLoading model from cache /Users/x/pinokio/cache/TMPDIR/jieba.cache\r\nLoading model cost 0.426 seconds.\r\nPrefix dict has been built successfully.\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:574: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\r\n  warnings.warn(warning.format(data.dtype))\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:577: RuntimeWarning: invalid value encountered in cast\r\n  data = data.astype(np.int16)\r\n```\r\n\r\nI am not completely sure but I don't remember seeing this many warning messages previously.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/190/comments",
    "author": "cocktailpeanut",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:35:37Z",
        "body": "a default fp16 inference setting was added.\r\nsee if the last commit works d3badb95cf1b97a61472d65d4787a35cddf9c908"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-20T20:05:25Z",
        "body": "> Could you share what the switch to fp16 means from end user's point of view (performance, etc.)? Appreciate it!\r\n\r\nA bit faster than using fp32, ~half graphics card usage (the %), and more environmentally friendly maybe lol\r\nCompared to a more aggressive int8 quantization, it can be seen as no performance (quality) penalty."
      }
    ]
  },
  {
    "number": 165,
    "title": "I get a mismatch error from using a finetune model while using the inference file",
    "created_at": "2024-10-18T17:51:27Z",
    "closed_at": "2024-10-18T20:22:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/165",
    "body": "I edit the inference-cli.py file to include the checkpoint path\r\n\r\n```\r\ndef load_model(repo_name, exp_name, model_cls, model_cfg, ckpt_step):\r\n    ckpt_path = f\"/home/user/F5-TTS/ckpts/Finetune/model_last.pt\" # .pt | \r\n.safetensors\r\n```\r\nI left this as it is\r\n\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Emilia_ZH_EN\", \"pinyin\")\r\n```\r\n\r\nThen I changed it after I got a message message\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Finetune\", \"pinyin\")\r\n```\r\n\r\nThen I use the inference file\r\n```\r\npython inference-cli.py \\\r\n--ref_audio \"/home/user/F5-TTS Test/Template/Input.wav\" \\\r\n--ref_text \"Robinson Industries, the world's leading scientific research and design factory. My dad runs the company. They mass produce his inventions. His motto, keep moving forward. It's what he does.\" \\\r\n--gen_text \"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring. Respect me and I'll nurture you; ignore me and you shall face the consequences.\"\r\n```\r\n\r\nThen I get this error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 392, in <module>\r\n    process(ref_audio, ref_text, gen_text, model, remove_silence)\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 374, in process\r\n    audio, spectragram = infer(ref_audio, ref_text, gen_text, model, remove_silence)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 346, in infer\r\n    return infer_batch((audio, sr), ref_text, gen_text_batches, model, remove_silence, cross_fade_duration)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 190, in infer_batch\r\n    ema_model = load_model(model, \"F5TTS_Base\", DiT, F5TTS_model_cfg, 1200000)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 148, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, use_ema = True)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/model/utils.py\", line 575, in load_checkpoint\r\n    ema_model.load_state_dict(checkpoint['ema_model_state_dict'])\r\n  File \"/home/user/F5-TTS/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\r\n    raise RuntimeError(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        Missing key(s) in state_dict: \"ema_model.transformer.text_embed.text_blocks.0.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.0.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.0.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.0.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.0.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.0.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.1.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.1.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.1.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.1.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.2.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.2.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.2.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.2.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.3.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.3.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.3.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.3.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.bias\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.bias\", \"ema_model.transformer.norm_out.linear.weight\", \"ema_model.transformer.norm_out.linear.bias\". \r\n        Unexpected key(s) in state_dict: \"ema_model.transformer.layers.0.1.g\", \"ema_model.transformer.layers.0.2.to_q.weight\", \"ema_model.transformer.layers.0.2.to_q.bias\", \"ema_model.transformer.layers.0.2.to_k.weight\", \"ema_model.transformer.layers.0.2.to_k.bias\", \"ema_model.transformer.layers.0.2.to_v.weight\", \"ema_model.transformer.layers.0.2.to_v.bias\", \"ema_model.transformer.layers.0.2.to_out.0.weight\", \"ema_model.transformer.layers.0.2.to_out.0.bias\", \"ema_model.transformer.layers.0.3.g\", \"ema_model.transformer.layers.0.4.ff.0.0.weight\", \"ema_model.transformer.layers.0.4.ff.0.0.bias\", \"ema_model.transformer.layers.0.4.ff.2.weight\", \"ema_model.transformer.layers.0.4.ff.2.bias\", \"ema_model.transformer.layers.1.1.g\", \"ema_model.transformer.layers.1.2.to_q.weight\", \"ema_model.transformer.layers.1.2.to_q.bias\", \"ema_model.transformer.layers.1.2.to_k.weight\", \"ema_model.transformer.layers.1.2.to_k.bias\", \"ema_model.transformer.layers.1.2.to_v.weight\", \"ema_model.transformer.layers.1.2.to_v.bias\", \"ema_model.transformer.layers.1.2.to_out.0.weight\", \"ema_model.transformer.layers.1.2.to_out.0.bias\", \"ema_model.transformer.layers.1.3.g\", \"ema_model.transformer.layers.1.4.ff.0.0.weight\", \"ema_model.transformer.layers.1.4.ff.0.0.bias\", \"ema_model.transformer.layers.1.4.ff.2.weight\", \"ema_model.transformer.layers.1.4.ff.2.bias\", \"ema_model.transformer.layers.2.1.g\", \"ema_model.transformer.layers.2.2.to_q.weight\", \"ema_model.transformer.layers.2.2.to_q.bias\", \"ema_model.transformer.layers.2.2.to_k.weight\", \"ema_model.transformer.layers.2.2.to_k.bias\", \"ema_model.transformer.layers.2.2.to_v.weight\", \"ema_model.transformer.layers.2.2.to_v.bias\", \"ema_model.transformer.layers.2.2.to_out.0.weight\", \"ema_model.transformer.layers.2.2.to_out.0.bias\", \"ema_model.transformer.layers.2.3.g\", \"ema_model.transformer.layers.2.4.ff.0.0.weight\", \"ema_model.transformer.layers.2.4.ff.0.0.bias\", \"ema_model.transformer.layers.2.4.ff.2.weight\", \"ema_model.transformer.layers.2.4.ff.2.bias\", \"ema_model.transformer.layers.3.1.g\", \"ema_model.transformer.layers.3.2.to_q.weight\", \"ema_model.transformer.layers.3.2.to_q.bias\", \"ema_model.transformer.layers.3.2.to_k.weight\", \"ema_model.transformer.layers.3.2.to_k.bias\", \"ema_model.transformer.layers.3.2.to_v.weight\", \"ema_model.transformer.layers.3.2.to_v.bias\", \"ema_model.transformer.layers.3.2.to_out.0.weight\", \"ema_model.transformer.layers.3.2.to_out.0.bias\", \"ema_model.transformer.layers.3.3.g\", \"ema_model.transformer.layers.3.4.ff.0.0.weight\", \"ema_model.transformer.layers.3.4.ff.0.0.bias\", \"ema_model.transformer.layers.3.4.ff.2.weight\", \"ema_model.transformer.layers.3.4.ff.2.bias\", \"ema_model.transformer.layers.4.1.g\", \"ema_model.transformer.layers.4.2.to_q.weight\", \"ema_model.transformer.layers.4.2.to_q.bias\", \"ema_model.transformer.layers.4.2.to_k.weight\", \"ema_model.transformer.layers.4.2.to_k.bias\", \"ema_model.transformer.layers.4.2.to_v.weight\", \"ema_model.transformer.layers.4.2.to_v.bias\", \"ema_model.transformer.layers.4.2.to_out.0.weight\", \"ema_model.transformer.layers.4.2.to_out.0.bias\", \"ema_model.transformer.layers.4.3.g\", \"ema_model.transformer.layers.4.4.ff.0.0.weight\", \"ema_model.transformer.layers.4.4.ff.0.0.bias\", \"ema_model.transformer.layers.4.4.ff.2.weight\", \"ema_model.transformer.layers.4.4.ff.2.bias\", \"ema_model.transformer.layers.5.1.g\", \"ema_model.transformer.layers.5.2.to_q.weight\", \"ema_model.transformer.layers.5.2.to_q.bias\", \"ema_model.transformer.layers.5.2.to_k.weight\", \"ema_model.transformer.layers.5.2.to_k.bias\", \"ema_model.transformer.layers.5.2.to_v.weight\", \"ema_model.transformer.layers.5.2.to_v.bias\", \"ema_model.transformer.layers.5.2.to_out.0.weight\", \"ema_model.transformer.layers.5.2.to_out.0.bias\", \"ema_model.transformer.layers.5.3.g\", \"ema_model.transformer.layers.5.4.ff.0.0.weight\", \"ema_model.transformer.layers.5.4.ff.0.0.bias\", \"ema_model.transformer.layers.5.4.ff.2.weight\", \"ema_model.transformer.layers.5.4.ff.2.bias\", \"ema_model.transformer.layers.6.1.g\", \"ema_model.transformer.layers.6.2.to_q.weight\", \"ema_model.transformer.layers.6.2.to_q.bias\", \"ema_model.transformer.layers.6.2.to_k.weight\", \"ema_model.transformer.layers.6.2.to_k.bias\", \"ema_model.transformer.layers.6.2.to_v.weight\", \"ema_model.transformer.layers.6.2.to_v.bias\", \"ema_model.transformer.layers.6.2.to_out.0.weight\", \"ema_model.transformer.layers.6.2.to_out.0.bias\", \"ema_model.transformer.layers.6.3.g\", \"ema_model.transformer.layers.6.4.ff.0.0.weight\", \"ema_model.transformer.layers.6.4.ff.0.0.bias\", \"ema_model.transformer.layers.6.4.ff.2.weight\", \"ema_model.transformer.layers.6.4.ff.2.bias\", \"ema_model.transformer.layers.7.1.g\", \"ema_model.transformer.layers.7.2.to_q.weight\", \"ema_model.transformer.layers.7.2.to_q.bias\", \"ema_model.transformer.layers.7.2.to_k.weight\", \"ema_model.transformer.layers.7.2.to_k.bias\", \"ema_model.transformer.layers.7.2.to_v.weight\", \"ema_model.transformer.layers.7.2.to_v.bias\", \"ema_model.transformer.layers.7.2.to_out.0.weight\", \"ema_model.transformer.layers.7.2.to_out.0.bias\", \"ema_model.transformer.layers.7.3.g\", \"ema_model.transformer.layers.7.4.ff.0.0.weight\", \"ema_model.transformer.layers.7.4.ff.0.0.bias\", \"ema_model.transformer.layers.7.4.ff.2.weight\", \"ema_model.transformer.layers.7.4.ff.2.bias\", \"ema_model.transformer.layers.8.1.g\", \"ema_model.transformer.layers.8.2.to_q.weight\", \"ema_model.transformer.layers.8.2.to_q.bias\", \"ema_model.transformer.layers.8.2.to_k.weight\", \"ema_model.transformer.layers.8.2.to_k.bias\", \"ema_model.transformer.layers.8.2.to_v.weight\", \"ema_model.transformer.layers.8.2.to_v.bias\", \"ema_model.transformer.layers.8.2.to_out.0.weight\", \"ema_model.transformer.layers.8.2.to_out.0.bias\", \"ema_model.transformer.layers.8.3.g\", \"ema_model.transformer.layers.8.4.ff.0.0.weight\", \"ema_model.transformer.layers.8.4.ff.0.0.bias\", \"ema_model.transformer.layers.8.4.ff.2.weight\", \"ema_model.transformer.layers.8.4.ff.2.bias\", \"ema_model.transformer.layers.9.1.g\", \"ema_model.transformer.layers.9.2.to_q.weight\", \"ema_model.transformer.layers.9.2.to_q.bias\", \"ema_model.transformer.layers.9.2.to_k.weight\", \"ema_model.transformer.layers.9.2.to_k.bias\", \"ema_model.transformer.layers.9.2.to_v.weight\", \"ema_model.transformer.layers.9.2.to_v.bias\", \"ema_model.transformer.layers.9.2.to_out.0.weight\", \"ema_model.transformer.layers.9.2.to_out.0.bias\", \"ema_model.transformer.layers.9.3.g\", \"ema_model.transformer.layers.9.4.ff.0.0.weight\", \"ema_model.transformer.layers.9.4.ff.0.0.bias\", \"ema_model.transformer.layers.9.4.ff.2.weight\", \"ema_model.transformer.layers.9.4.ff.2.bias\", \"ema_model.transformer.layers.10.1.g\", \"ema_model.transformer.layers.10.2.to_q.weight\", \"ema_model.transformer.layers.10.2.to_q.bias\", \"ema_model.transformer.layers.10.2.to_k.weight\", \"ema_model.transformer.layers.10.2.to_k.bias\", \"ema_model.transformer.layers.10.2.to_v.weight\", \"ema_model.transformer.layers.10.2.to_v.bias\", \"ema_model.transformer.layers.10.2.to_out.0.weight\", \"ema_model.transformer.layers.10.2.to_out.0.bias\", \"ema_model.transformer.layers.10.3.g\", \"ema_model.transformer.layers.10.4.ff.0.0.weight\", \"ema_model.transformer.layers.10.4.ff.0.0.bias\", \"ema_model.transformer.layers.10.4.ff.2.weight\", \"ema_model.transformer.layers.10.4.ff.2.bias\", \"ema_model.transformer.layers.11.1.g\", \"ema_model.transformer.layers.11.2.to_q.weight\", \"ema_model.transformer.layers.11.2.to_q.bias\", \"ema_model.transformer.layers.11.2.to_k.weight\", \"ema_model.transformer.layers.11.2.to_k.bias\", \"ema_model.transformer.layers.11.2.to_v.weight\", \"ema_model.transformer.layers.11.2.to_v.bias\", \"ema_model.transformer.layers.11.2.to_out.0.weight\", \"ema_model.transformer.layers.11.2.to_out.0.bias\", \"ema_model.transformer.layers.11.3.g\", \"ema_model.transformer.layers.11.4.ff.0.0.weight\", \"ema_model.transformer.layers.11.4.ff.0.0.bias\", \"ema_model.transformer.layers.11.4.ff.2.weight\", \"ema_model.transformer.layers.11.4.ff.2.bias\", \"ema_model.transformer.layers.12.0.weight\", \"ema_model.transformer.layers.12.1.g\", \"ema_model.transformer.layers.12.2.to_q.weight\", \"ema_model.transformer.layers.12.2.to_q.bias\", \"ema_model.transformer.layers.12.2.to_k.weight\", \"ema_model.transformer.layers.12.2.to_k.bias\", \"ema_model.transformer.layers.12.2.to_v.weight\", \"ema_model.transformer.layers.12.2.to_v.bias\", \"ema_model.transformer.layers.12.2.to_out.0.weight\", \"ema_model.transformer.layers.12.2.to_out.0.bias\", \"ema_model.transformer.layers.12.3.g\", \"ema_model.transformer.layers.12.4.ff.0.0.weight\", \"ema_model.transformer.layers.12.4.ff.0.0.bias\", \"ema_model.transformer.layers.12.4.ff.2.weight\", \"ema_model.transformer.layers.12.4.ff.2.bias\", \"ema_model.transformer.layers.13.0.weight\", \"ema_model.transformer.layers.13.1.g\", \"ema_model.transformer.layers.13.2.to_q.weight\", \"ema_model.transformer.layers.13.2.to_q.bias\", \"ema_model.transformer.layers.13.2.to_k.weight\", \"ema_model.transformer.layers.13.2.to_k.bias\", \"ema_model.transformer.layers.13.2.to_v.weight\", \"ema_model.transformer.layers.13.2.to_v.bias\", \"ema_model.transformer.layers.13.2.to_out.0.weight\", \"ema_model.transformer.layers.13.2.to_out.0.bias\", \"ema_model.transformer.layers.13.3.g\", \"ema_model.transformer.layers.13.4.ff.0.0.weight\", \"ema_model.transformer.layers.13.4.ff.0.0.bias\", \"ema_model.transformer.layers.13.4.ff.2.weight\", \"ema_model.transformer.layers.13.4.ff.2.bias\", \"ema_model.transformer.layers.14.0.weight\", \"ema_model.transformer.layers.14.1.g\", \"ema_model.transformer.layers.14.2.to_q.weight\", \"ema_model.transformer.layers.14.2.to_q.bias\", \"ema_model.transformer.layers.14.2.to_k.weight\", \"ema_model.transformer.layers.14.2.to_k.bias\", \"ema_model.transformer.layers.14.2.to_v.weight\", \"ema_model.transformer.layers.14.2.to_v.bias\", \"ema_model.transformer.layers.14.2.to_out.0.weight\", \"ema_model.transformer.layers.14.2.to_out.0.bias\", \"ema_model.transformer.layers.14.3.g\", \"ema_model.transformer.layers.14.4.ff.0.0.weight\", \"ema_model.transformer.layers.14.4.ff.0.0.bias\", \"ema_model.transformer.layers.14.4.ff.2.weight\", \"ema_model.transformer.layers.14.4.ff.2.bias\", \"ema_model.transformer.layers.15.0.weight\", \"ema_model.transformer.layers.15.1.g\", \"ema_model.transformer.layers.15.2.to_q.weight\", \"ema_model.transformer.layers.15.2.to_q.bias\", \"ema_model.transformer.layers.15.2.to_k.weight\", \"ema_model.transformer.layers.15.2.to_k.bias\", \"ema_model.transformer.layers.15.2.to_v.weight\", \"ema_model.transformer.layers.15.2.to_v.bias\", \"ema_model.transformer.layers.15.2.to_out.0.weight\", \"ema_model.transformer.layers.15.2.to_out.0.bias\", \"ema_model.transformer.layers.15.3.g\", \"ema_model.transformer.layers.15.4.ff.0.0.weight\", \"ema_model.transformer.layers.15.4.ff.0.0.bias\", \"ema_model.transformer.layers.15.4.ff.2.weight\", \"ema_model.transformer.layers.15.4.ff.2.bias\", \"ema_model.transformer.layers.16.0.weight\", \"ema_model.transformer.layers.16.1.g\", \"ema_model.transformer.layers.16.2.to_q.weight\", \"ema_model.transformer.layers.16.2.to_q.bias\", \"ema_model.transformer.layers.16.2.to_k.weight\", \"ema_model.transformer.layers.16.2.to_k.bias\", \"ema_model.transformer.layers.16.2.to_v.weight\", \"ema_model.transformer.layers.16.2.to_v.bias\", \"ema_model.transformer.layers.16.2.to_out.0.weight\", \"ema_model.transformer.layers.16.2.to_out.0.bias\", \"ema_model.transformer.layers.16.3.g\", \"ema_model.transformer.layers.16.4.ff.0.0.weight\", \"ema_model.transformer.layers.16.4.ff.0.0.bias\", \"ema_model.transformer.layers.16.4.ff.2.weight\", \"ema_model.transformer.layers.16.4.ff.2.bias\", \"ema_model.transformer.layers.17.0.weight\", \"ema_model.transformer.layers.17.1.g\", \"ema_model.transformer.layers.17.2.to_q.weight\", \"ema_model.transformer.layers.17.2.to_q.bias\", \"ema_model.transformer.layers.17.2.to_k.weight\", \"ema_model.transformer.layers.17.2.to_k.bias\", \"ema_model.transformer.layers.17.2.to_v.weight\", \"ema_model.transformer.layers.17.2.to_v.bias\", \"ema_model.transformer.layers.17.2.to_out.0.weight\", \"ema_model.transformer.layers.17.2.to_out.0.bias\", \"ema_model.transformer.layers.17.3.g\", \"ema_model.transformer.layers.17.4.ff.0.0.weight\", \"ema_model.transformer.layers.17.4.ff.0.0.bias\", \"ema_model.transformer.layers.17.4.ff.2.weight\", \"ema_model.transformer.layers.17.4.ff.2.bias\", \"ema_model.transformer.layers.18.0.weight\", \"ema_model.transformer.layers.18.1.g\", \"ema_model.transformer.layers.18.2.to_q.weight\", \"ema_model.transformer.layers.18.2.to_q.bias\", \"ema_model.transformer.layers.18.2.to_k.weight\", \"ema_model.transformer.layers.18.2.to_k.bias\", \"ema_model.transformer.layers.18.2.to_v.weight\", \"ema_model.transformer.layers.18.2.to_v.bias\", \"ema_model.transformer.layers.18.2.to_out.0.weight\", \"ema_model.transformer.layers.18.2.to_out.0.bias\", \"ema_model.transformer.layers.18.3.g\", \"ema_model.transformer.layers.18.4.ff.0.0.weight\", \"ema_model.transformer.layers.18.4.ff.0.0.bias\", \"ema_model.transformer.layers.18.4.ff.2.weight\", \"ema_model.transformer.layers.18.4.ff.2.bias\", \"ema_model.transformer.layers.19.0.weight\", \"ema_model.transformer.layers.19.1.g\", \"ema_model.transformer.layers.19.2.to_q.weight\", \"ema_model.transformer.layers.19.2.to_q.bias\", \"ema_model.transformer.layers.19.2.to_k.weight\", \"ema_model.transformer.layers.19.2.to_k.bias\", \"ema_model.transformer.layers.19.2.to_v.weight\", \"ema_model.transformer.layers.19.2.to_v.bias\", \"ema_model.transformer.layers.19.2.to_out.0.weight\", \"ema_model.transformer.layers.19.2.to_out.0.bias\", \"ema_model.transformer.layers.19.3.g\", \"ema_model.transformer.layers.19.4.ff.0.0.weight\", \"ema_model.transformer.layers.19.4.ff.0.0.bias\", \"ema_model.transformer.layers.19.4.ff.2.weight\", \"ema_model.transformer.layers.19.4.ff.2.bias\", \"ema_model.transformer.layers.20.0.weight\", \"ema_model.transformer.layers.20.1.g\", \"ema_model.transformer.layers.20.2.to_q.weight\", \"ema_model.transformer.layers.20.2.to_q.bias\", \"ema_model.transformer.layers.20.2.to_k.weight\", \"ema_model.transformer.layers.20.2.to_k.bias\", \"ema_model.transformer.layers.20.2.to_v.weight\", \"ema_model.transformer.layers.20.2.to_v.bias\", \"ema_model.transformer.layers.20.2.to_out.0.weight\", \"ema_model.transformer.layers.20.2.to_out.0.bias\", \"ema_model.transformer.layers.20.3.g\", \"ema_model.transformer.layers.20.4.ff.0.0.weight\", \"ema_model.transformer.layers.20.4.ff.0.0.bias\", \"ema_model.transformer.layers.20.4.ff.2.weight\", \"ema_model.transformer.layers.20.4.ff.2.bias\", \"ema_model.transformer.layers.21.0.weight\", \"ema_model.transformer.layers.21.1.g\", \"ema_model.transformer.layers.21.2.to_q.weight\", \"ema_model.transformer.layers.21.2.to_q.bias\", \"ema_model.transformer.layers.21.2.to_k.weight\", \"ema_model.transformer.layers.21.2.to_k.bias\", \"ema_model.transformer.layers.21.2.to_v.weight\", \"ema_model.transformer.layers.21.2.to_v.bias\", \"ema_model.transformer.layers.21.2.to_out.0.weight\", \"ema_model.transformer.layers.21.2.to_out.0.bias\", \"ema_model.transformer.layers.21.3.g\", \"ema_model.transformer.layers.21.4.ff.0.0.weight\", \"ema_model.transformer.layers.21.4.ff.0.0.bias\", \"ema_model.transformer.layers.21.4.ff.2.weight\", \"ema_model.transformer.layers.21.4.ff.2.bias\", \"ema_model.transformer.layers.22.0.weight\", \"ema_model.transformer.layers.22.1.g\", \"ema_model.transformer.layers.22.2.to_q.weight\", \"ema_model.transformer.layers.22.2.to_q.bias\", \"ema_model.transformer.layers.22.2.to_k.weight\", \"ema_model.transformer.layers.22.2.to_k.bias\", \"ema_model.transformer.layers.22.2.to_v.weight\", \"ema_model.transformer.layers.22.2.to_v.bias\", \"ema_model.transformer.layers.22.2.to_out.0.weight\", \"ema_model.transformer.layers.22.2.to_out.0.bias\", \"ema_model.transformer.layers.22.3.g\", \"ema_model.transformer.layers.22.4.ff.0.0.weight\", \"ema_model.transformer.layers.22.4.ff.0.0.bias\", \"ema_model.transformer.layers.22.4.ff.2.weight\", \"ema_model.transformer.layers.22.4.ff.2.bias\", \"ema_model.transformer.layers.23.0.weight\", \"ema_model.transformer.layers.23.1.g\", \"ema_model.transformer.layers.23.2.to_q.weight\", \"ema_model.transformer.layers.23.2.to_q.bias\", \"ema_model.transformer.layers.23.2.to_k.weight\", \"ema_model.transformer.layers.23.2.to_k.bias\", \"ema_model.transformer.layers.23.2.to_v.weight\", \"ema_model.transformer.layers.23.2.to_v.bias\", \"ema_model.transformer.layers.23.2.to_out.0.weight\", \"ema_model.transformer.layers.23.2.to_out.0.bias\", \"ema_model.transformer.layers.23.3.g\", \"ema_model.transformer.layers.23.4.ff.0.0.weight\", \"ema_model.transformer.layers.23.4.ff.0.0.bias\", \"ema_model.transformer.layers.23.4.ff.2.weight\", \"ema_model.transformer.layers.23.4.ff.2.bias\", \"ema_model.transformer.norm_out.g\". \r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n        size mismatch for ema_model.transformer.input_embed.proj.weight: copying a param with shape torch.Size([1024, 300]) from checkpoint, the shape in current model is torch.Size([1024, 712]).\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/165/comments",
    "author": "GUUser91",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-18T18:37:27Z",
        "body": "make sure you use corrrect model_type  also **F5TTS_Base** or **E2TTS_Base** this you finetune\r\n\r\n"
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T19:02:32Z",
        "body": "@lpscr\r\nI used the finetune_gradio.py file and selected E2TTS_Base for finetuning. Then I clicked on auto settings. Then I click on start training.\r\nI also used finetune-cli.py and the same thing happened."
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:10:36Z",
        "body": "@lpscr\r\nI used the finetune_gradio.py file again and finetuned with the F5TTS_Base model and now I don't have the error if I used this finetuned F5TTS_Base model. But I want to finetune with E2TTS_Base, can you try finetuning with E2TTS_Base on your end to see if it works?"
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-18T20:11:49Z",
        "body": "yes because you need to change in\r\nstart the interface-cli -m E2-TTS\r\n\r\n\r\nparser.add_argument(\r\n    \"-m\",\r\n    \"--model\",\r\n    help=\"F5-TTS | E2-TTS\",\r\n)"
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-18T20:13:30Z",
        "body": "what gpu you have and memory  ? because i working in auto setting , "
      }
    ]
  },
  {
    "number": 157,
    "title": "\u5408\u6210\u7684\u97f3\u9891\u7f3a\u5931\u548c\u5939\u6742\u53c2\u8003\u97f3\u9891\u5185\u5bb9",
    "created_at": "2024-10-18T10:11:30Z",
    "closed_at": "2024-10-20T19:22:30Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/157",
    "body": "\u9047\u5230\u7684\u51e0\u4e2a\u95ee\u9898\r\ngradio\u754c\u9762\u6d4b\u8bd5\r\n1 \u5408\u6210\u7684\u97f3\u9891\u4f1a\u5728\u5f00\u5934\u7f3a\u5931\u51e0\u4e2a\u5b57\u7684\u53d1\u97f3(\u53c2\u8003\u97f3\u98911s\u52303s\u8bd5\u4e86\u51e0\u4e2a)\r\n2 \u4ee5\u53ca\u5f00\u5934\u4f1a\u643a\u5e26\u4e00\u70b9\u53c2\u8003\u97f3\u9891\u7684\u5c3e\u90e8\r\n3 \u5408\u6210\u97f3\u9891\u4e2d\u95f4\u4f1a\u5939\u6742\u4e00\u4e9b\u53c2\u8003\u97f3\u9891\u7684\u5185\u5bb9\uff0825s\u53c2\u8003\u97f3\u9891+\u957f\u6587\u672c\uff09\r\n\u5982\u9898\u76843\u4e2a\u95ee\u9898\uff0c\u662f\u4ec0\u4e48\u539f\u56e0\u5462\uff0c\u6709\u6ca1\u6709\u4ec0\u4e48\u89e3\u51b3\u65b9\u5f0f",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/157/comments",
    "author": "Ustinianer",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:22:30Z",
        "body": "\u6211\u4eec\u5728README\u91cc\u6709\u8bf4\u660e\uff0cissue\u91cc\u4e5f\u6709\u7c7b\u4f3c\u7684\u89e3\u7b54\r\n\r\n1. \u592a\u77ed\u7684\u53c2\u8003\u97f3\u9891\u4f1a\u5bfc\u81f4\u7b80\u5355\u4f30\u7b97\u7684\u65f6\u957f\u4e0d\u51c6\uff0c\u5efa\u8bae\u5355\u72ec\u8bad\u7ec3\u4e00\u4e2a\u65f6\u957f\u9884\u6d4b\u6a21\u578b\u4ee5\u5e94\u5bf9\u8be5\u60c5\u51b5\r\n2. \u4e0d\u592a\u5efa\u8bae\u4f7f\u7528\u621b\u7136\u800c\u6b62\u7684\u53c2\u8003\u97f3\u9891\uff0c\u6700\u597d\u7559\u6709\u4e00\u4e9b\u7a7a\u767d\u9759\u97f3\u5728\u5c3e\u90e8\r\n3. \u8fc7\u957f\u7684\u53c2\u8003\u97f3\u9891\u5176\u5b9e\u4f1a\u88ab\u622a\u65ad\u523015\u79d2\uff0c\u53c2\u80032.\r\n\r\n\u4e00\u822c\u6765\u8bf4\u752810\u79d2\u5de6\u53f3\u7684\u53c2\u8003\u97f3\u9891\uff0c\u672b\u5c3e\u7559\u6709\u4e00\u5b9a\u7a7a\u767d\u4f59\u5730\u4f1a\u5f97\u5230\u6bd4\u8f83\u597d\u7684\u7ed3\u679c\r\n\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u5f88\u7c97\u7cd9\uff0c\u5e26\u6765\u4f7f\u7528\u4e0d\u4fbf\u656c\u8bf7\u89c1\u8c05~"
      }
    ]
  },
  {
    "number": 145,
    "title": "Run time error while fine tuning",
    "created_at": "2024-10-17T15:26:06Z",
    "closed_at": "2024-10-21T07:00:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/145",
    "body": "I am trying to fine tune the model. And I am stuck with this error \r\n\r\n>`Epoch 1/10:  23% 34/145 [00:24<01:21,  1.36step/s, loss=1.92, step=34]\r\nTraceback (most recent call last):\r\n  File \"/content/F5-TTS/train.py\", line 94, in <module>\r\n    main()\r\n  File \"/content/F5-TTS/train.py\", line 88, in main\r\n    trainer.train(train_dataset,\r\n  File \"/content/F5-TTS/model/trainer.py\", line 229, in train\r\n    loss, cond, pred = self.model(mel_spec, text=text_inputs, lens=mel_lengths, noise_scheduler=self.noise_scheduler)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 820, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/cfm.py\", line 273, in forward\r\n    pred = self.transformer(x = \u03c6, cond = cond, text = text, time = time, drop_audio_cond = drop_audio_cond, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 162, in forward\r\n    text_embed = self.text_embed(text, seq_len, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 57, in forward\r\n    text = self.text_embed(text) # b n -> b n d\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\", line 164, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2267, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)`\r\n\r\n\r\n\r\n\r\nthese are the inputs I am using.\r\n\r\ntarget_sample_rate = 24000\r\nn_mel_channels = 100\r\nhop_length = 256\r\n\r\ntokenizer = \"pinyin\" # 'pinyin', 'char', or 'custom'\r\ntokenizer_path = None # if tokenizer = 'custom', define the path to the tokenizer you want to use (should be vocab.txt)\r\ndataset_name = \"My_Dataset\"\r\n\r\n# -------------------------- Training Settings -------------------------- #\r\n\r\nexp_name = \"E2TTS_Base\"  # F5TTS_Base | E2TTS_Base\r\n\r\nlearning_rate = 5e-06\r\n\r\nbatch_size_per_gpu = 38400  # 8 GPUs, 8 * 38400 = 307200\r\nbatch_size_type = \"frame\"  # \"frame\" or \"sample\"\r\nmax_samples = 2  # max sequences per batch if use frame-wise batch_size. we set 32 for small models, 64 for base models\r\ngrad_accumulation_steps = 1  # note: updates = steps / grad_accumulation_steps\r\nmax_grad_norm = 1.\r\n\r\nepochs = 10  # use linear decay, thus epochs control the slope\r\nnum_warmup_updates = 20  # warmup steps\r\nsave_per_updates = 500  # save checkpoint per steps\r\nlast_per_steps = 5000  # save last checkpoint per steps\r\n\r\n\r\n\r\nIs this error something related to my dataset or my input? Can anyone help?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/145/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-19T02:12:00Z",
        "body": "'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\r\n\r\nmaybe you need to check the tensor's type"
      },
      {
        "user": "MilanaShhanukova",
        "created_at": "2024-10-19T19:48:36Z",
        "body": "@rasheed-aidetic I may guess that you have an empty text sample, check it out. If it is intended, you may add the type converter in TextEmbedding\r\n\r\n```\r\n        if text.dtype is not torch.long:\r\n            text = text.long()\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "number": 144,
    "title": "Shape mismatch error while fine tuning (non-singleton dimension 1)",
    "created_at": "2024-10-17T12:37:13Z",
    "closed_at": "2024-10-17T14:56:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/144",
    "body": "Hi This project is incredible guys. Great work.\r\n\r\nI am trying to finetune this model and facing an error on shape mismatch. Can anyone help?\r\n\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\r\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/content/F5-TTS/model/dataset.py\", line 117, in __getitem__\r\n    mel_spec = rearrange(mel_spec, '1 d t -> d t')\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\r\n    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\r\n    raise EinopsError(message + \"\\n {}\".format(e))\r\neinops.EinopsError:  Error while processing rearrange-reduction pattern \"1 d t -> d t\".\r\n Input tensor shape: torch.Size([2, 100, 1407]). Additional info: {}.\r\n Shape mismatch, 2 != 1`\r\n\r\nI am trying to fine tune the model with my own voice model. I have created the dataset using the prepare_csv_wavs.py script. Please let me know if I need to make any changes in the input section of train.py\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/144/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-17T12:57:14Z",
        "body": "i think the problem is you have stereo you need to  resample your audio to **mono** **24000 hz**\r\n\r\nrun this script please first make back up because replace your original files !\r\n\r\n```\r\nimport os\r\nimport glob\r\nfrom pydub import AudioSegment\r\n\r\ndef convert_wav_to_mono(folder_path):\r\n    wav_files = glob.glob(os.path.join(folder_path, '*.wav'))\r\n    for file_path in wav_files:\r\n        filename = os.path.basename(file_path)\r\n        audio = AudioSegment.from_wav(file_path)\r\n        mono_audio = audio.set_channels(1)\r\n        mono_audio = mono_audio.set_frame_rate(24000)\r\n        new_file_path = os.path.join(folder_path, f\"mono_{filename}\")\r\n        mono_audio.export(new_file_path, format=\"wav\")\r\n\r\nfolder_path = 'your_folder_path'\r\nconvert_wav_to_mono(folder_path)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 115,
    "title": "Problems In Fine Tuning",
    "created_at": "2024-10-16T05:56:32Z",
    "closed_at": "2024-10-16T08:46:30Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/115",
    "body": "Help, when fine-tuning, I have the following problems:\r\n\r\n RuntimeError: Error(s) in loading state_dict for EMA:\r\nsize mismatch for ema_model.transformer.text_embed.text_embed.weight:  copying a param with shape torch.Size([2546, 512]) from checkpoint, the shape in current model is torch.Size([259, 512])",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/115/comments",
    "author": "wen0320",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T06:07:43Z",
        "body": "remember to reuse the `vocab.txt` compatible with pretrained model\r\nyou could simply get it at `data/Emilia_ZH_EN_pinyin/vocab.txt`"
      }
    ]
  },
  {
    "number": 64,
    "title": "\u5305\u542b\u963f\u62c9\u4f2f\u6570\u5b57\u4f1a\u8f93\u51fa\u8bed\u97f3\u4f1a\u5f02\u5e38",
    "created_at": "2024-10-14T09:04:10Z",
    "closed_at": "2024-10-14T10:17:05Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/64",
    "body": "\u975e\u5e38\u68d2\u7684\u9879\u76ee\uff0c\u4f46\u6211\u53d1\u73b0\u4e86\u4e00\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u6587\u672c\u4e2d\u5305\u542b\u963f\u62c9\u4f2f\u6570\u5b57\u7684\u8bdd\uff0c\u8f93\u51fa\u7684\u8bed\u97f3\u4f1a\u5f02\u5e38\uff1f\u6240\u4ee5\u662f\u5fc5\u987b\u5c06\u963f\u62c9\u4f2f\u6570\u5b57\u9884\u5904\u7406\u6210\u5bf9\u5e94\u7684\u4e2d\u6587\u6216\u82f1\u6587\u5417\u3002",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/64/comments",
    "author": "alants56",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T09:46:15Z",
        "body": "\u6211\u4eec\u7684\u6587\u672c\u524d\u7aef\u8fd8\u6bd4\u8f83\u7b80\u964b\uff0c\u6211\u5efa\u8bae\u5c06\u963f\u62c9\u4f2f\u6570\u5b57\u8fdb\u884c\u9884\u5904\u7406\r\n\u867d\u7136\u6a21\u578b\u80fd\u591f\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5bf9\u963f\u62c9\u4f2f\u6570\u5b57\u8fdb\u884c\u751f\u6210\uff0c\u4f46\u662f\u8f6c\u5316\u6210\u4e2d\u6587\u6216\u82f1\u6587\u6765\u660e\u786e\u6307\u5b9a\u8bfb\u6cd5\u4f1a\u66f4\u7a33\u5b9a"
      }
    ]
  }
]