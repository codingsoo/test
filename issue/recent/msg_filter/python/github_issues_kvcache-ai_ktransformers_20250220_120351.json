[
  {
    "number": 321,
    "title": "NUMA\u6a21\u5f0f\u4e0d\u80fd\u5de5\u4f5c",
    "created_at": "2025-02-15T10:06:40Z",
    "closed_at": "2025-02-15T16:26:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/321",
    "body": "\u7cfb\u7edf\u914d\u7f6e\n  CPU: EPYC 7742 x2\n  DRAM: 64GB x 16\n  GPU: 3090 x 8\n  \u4f7f\u7528\u7684\u6a21\u578b: Ollama\u90a3\u8fb9\u62c9\u4e0b\u6765\u76848bit\u91cf\u5316\u7684gguf\n\n\u5982\u679c\u4e0d\u4f7f\u7528NUMA\u6a21\u5f0f, install.sh\u5b89\u88c5, \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\npython -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V3 --cpu_infer 64 --max_new_tokens 1000 --gguf_path /home/adonis/temp/ds_v3\n\n\u53ef\u4ee5\u6b63\u5e38\u5de5\u4f5c, \u7528\u9644\u52a0\u7684test\u6587\u4ef6\u91cc\u7684\u4fe1\u606f, \u8981\u6c42\u8fdb\u884c\u603b\u7ed3, \u6027\u80fd\u6570\u636e\u5982\u4e0b, \u53e6\u5916\u5728\u8fd9\u4e2a\u65f6\u5019cpu\u90a3\u8fb9\u7684\u5185\u5b58\u5360\u7528\u53ea\u670919G\n\nprompt eval count:    966 token(s)\nprompt eval duration: 17.906471252441406s\nprompt eval rate:     53.9469774017197 tokens/s\neval count:           1000 token(s)\neval duration:        222.02748823165894s\neval rate:            4.5039468219206285 tokens/s\n\n\u5982\u679c\u4f7f\u7528NUMA\u6a21\u5f0f, \u4e5f\u5c31\u662fexport NUMA=1; install.sh\u5b89\u88c5, \u4f7f\u7528\u4e0a\u8ff0\u547d\u4ee4\u542f\u52a8\n\u7a0b\u5e8f\u5728\u8f93\u51fa\u4e00\u7cfb\u5217\u52a0\u8f7d\u4fe1\u606f, \u76f4\u5230\u4ee5\u4e0b\u4fe1\u606f\n\nloading blk.46.ffn_norm.weight to cuda:0\nloading blk.47.attn_q_a_norm.weight to cuda:0\nloading blk.47.attn_kv_a_norm.weight to cuda:0\nloading blk.47.attn_kv_b.weight to cuda:0\n\n\u7136\u540e\u5c31killed\u4e86\n\n\u770b\u4e86\u4e00\u4e0bhtop\u4fe1\u606f, \u662f1T\u5185\u5b58\u90fd\u88ab\u5360\u6ee1\u4e86, \u7136\u540eswap\u7528\u5149(swap\u5c31\u914d\u7f6e\u4e868GB), \u7136\u540e\u5c31killed\u4e86\n\nbtw, \u975e\u5e38\u611f\u8c22\u4f5c\u8005\u7684\u5de5\u4f5c, \u975e\u5e38\u975e\u5e38\u68d2\u7684\u5de5\u4f5c!!!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/321/comments",
    "author": "adonishong",
    "comments": [
      {
        "user": "Radohead",
        "created_at": "2025-02-15T15:14:32Z",
        "body": "\u76ee\u524d\u67b6\u6784\u4e0b\u7528numa\u610f\u5473\u7740\u6709\u51e0\u8defcpu\uff0c\u5185\u5b58\u5c31\u590d\u5236\u51e0\u4efd\uff0c\u6240\u4ee52\u8defcpu\u8dd1q8 1t\u662f\u4e0d\u591f\u7684\u5927\u6982\u89811.5t"
      },
      {
        "user": "BiFangKNT",
        "created_at": "2025-02-15T16:06:17Z",
        "body": "\u8ba4\u771f\u770b\u6587\u6863\u554a\uff0c\u4f60\u90fd\u77e5\u9053numa\u4e86\uff0c\u96be\u9053\u4e0d\u77e5\u9053\u53cc\u8def\u4f1a\u628a\u6a21\u578b\u590d\u5236\u4e00\u4efd\u5417\n\n\u4e00\u4efd\u6a21\u578b\u5360713GB\uff0c\u4e24\u4efd\u5c31\u662f 713*2=1426GB \u554a"
      }
    ]
  },
  {
    "number": 82,
    "title": "Seg Fault on long replies",
    "created_at": "2024-09-10T14:44:09Z",
    "closed_at": "2024-09-11T13:43:47Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/82",
    "body": "I am trying to use this as a llama.cpp replacement as it is a lot faster.  I did modify the backend args file (`ktransformers/server/backend/args.py`) as max_new_tokens isn't an option for the sever:\r\n```\r\n-    max_new_tokens: int = Field(500, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n+    max_new_tokens: int = Field(2040, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n```\r\n\r\nto allow for longer responses as 500 tokens was causing a lot of my stuff to get cut off half way through.\r\nIt does generate some tokens (feels like about the 500 limit) and then hard crashes.  So maybe there is some other limit that I need to adjust?\r\n\r\nI am using DeekSeek-V2.5 in Q4_K_M, I did also try it with WizardLM 8x22B and the same thing happens.\r\n\r\nHardware: 1x 3090, Epyc 7402, 512 Gb Ram\r\n\r\n```\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../ktransformers.sh: line 12: 2152563 Segmentation fault      (core dumped) ktransformers --model_path /nvmes/models/DeepSeek-V2.5/ --gguf_path /nvmes/models/DeepSeek-V2.5-GGUF2/ --port 8081 --cpu_infer 32\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/82/comments",
    "author": "matthusby",
    "comments": [
      {
        "user": "matthusby",
        "created_at": "2024-09-11T13:43:47Z",
        "body": "Thank you, yeah changing the `cache_lens` param is what was causing my problem.  I guess my chat was at just the right length where the longer max tokens would trigger that error.\r\n\r\nFor reference I doubled it and now when I try to run DeepSeek-V2.5 I get a out of memory crash, but on Wizard 8x22 its working great.\r\n\r\nIt looks like there are plans to address some of this soon, so I will close this issue."
      }
    ]
  },
  {
    "number": 73,
    "title": "When the input token exceeds 4096, an error will occur.",
    "created_at": "2024-09-02T06:23:20Z",
    "closed_at": "2024-09-03T11:34:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/73",
    "body": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/fastapi/routing.py\", line 210, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 651, in forward\r\n    causal_mask = self._update_causal_mask(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1624, in _update_causal_mask\r\n    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\r\n                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nRuntimeError: The size of tensor a (4096) must match the size of tensor b (8122) at non-singleton dimension 3",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/73/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T09:30:43Z",
        "body": "> Hi, I haven\u2019t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?\r\n\r\nI compiled it using the most recent code, and my configs are: \r\n``` yaml\r\n- match:\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding\r\n  replace:\r\n    class: ktransformers.operators.RoPE.YarnRotaryEmbedding\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\.(?!.*self_attn\\\\.kv_b_proj).*$\"  # regular expression\r\n    class: torch.nn.Linear  # only match modules matching name and class simultaneously\r\n  replace:\r\n    class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n      generate_op: \"KLinearMarlin\"\r\n      prefill_op: \"KLinearTorch\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp$\"\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE\r\n  replace:\r\n    class: ktransformers.operators.experts.KDeepseekV2MoE     # mlp module with custom forward function\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp\\\\.experts$\"\r\n  replace:\r\n    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism\r\n    kwargs:\r\n      prefill_device: \"cuda\"\r\n      prefill_op: \"KExpertsTorch\"\r\n      generate_device: \"cpu\"\r\n      generate_op: \"KExpertsCPU\"\r\n      out_device: \"cuda\"\r\n  recursive: False # don't recursively inject submodules of this module\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.self_attn$\"\r\n  replace:\r\n    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model$\"\r\n  replace:\r\n    class: \"ktransformers.operators.models.KDeepseekV2Model\"\r\n    kwargs:\r\n      per_layer_prefill_intput_threshold: 0 # 0 is close layer wise prefill\r\n- match:\r\n    name: \"^model.embed_tokens\"\r\n  replace:\r\n    class: \"default\"\r\n    kwargs:\r\n      generate_device: \"cpu\"\r\n      prefill_device: \"cuda\"\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T10:05:26Z",
        "body": "I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release.\r\nIf you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens"
      }
    ]
  },
  {
    "number": 69,
    "title": "Missing pip packages flash_attn and wheel",
    "created_at": "2024-08-31T16:58:09Z",
    "closed_at": "2024-09-03T17:31:05Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/69",
    "body": "Might want to add those two packages to requirements file.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/69/comments",
    "author": "bitbottrap",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T03:17:24Z",
        "body": "Flash-attn is an optional package, and not all models require it. Additionally, if a precompiled package for flash-attn is not available, it needs to be compiled and installed, which can be very time-consuming. Therefore, flash-attn has not been added to the requirements.\r\n\r\nWe apologize for the omission of wheel package. We will include it in the next release. We use environments created with conda, which include wheel by default, so we did not encounter this issue."
      }
    ]
  },
  {
    "number": 4,
    "title": "Native windows support",
    "created_at": "2024-07-27T10:50:41Z",
    "closed_at": "2024-08-16T07:27:45Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/4",
    "body": "First of all thanks for your AI community contribution, it's a huge leap forward to use MoE models for consumer grade users with limited VRAM.\r\nWould it be possible to add native Windows support to KTransformers? I'd love to see the project become accessible to windows users as well.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/4/comments",
    "author": "DotNetDevlll",
    "comments": [
      {
        "user": "Atream",
        "created_at": "2024-08-12T03:18:17Z",
        "body": "You can try to install from source by running install.bat. Pre-built wheels will be released soon."
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T14:01:04Z",
        "body": ".\\install.bat \r\n\r\n```\r\nInstalling ktransformers\r\nProcessing c:\\users\\pc\\ktransformers\\ktransformers\\ktransformers\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [17 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\r\n          return hook(metadata_directory, config_settings)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\setuptools\\build_meta.py\", line 368, in prepare_metadata_for_build_wheel\r\n          self.run_setup()\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\setuptools\\build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 294, in <module>\r\n        File \"<string>\", line 132, in get_package_version\r\n        File \"<string>\", line 54, in get_cuda_bare_metal_version\r\n      TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\nInstallation completed successfully\r\n```"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-12T15:30:08Z",
        "body": "There might be a few possibilities here.\r\n\r\n1. CUDA is not installed on your machine.\r\n2. The CUDA environment variables are not active.\r\n3. The installed PyTorch is not for GPU, but for CPU.\r\n\r\nThere is a simple env check code\r\n```python\r\nimport torch\r\nimport subprocess\r\nfrom torch.utils.cpp_extension import CUDA_HOME\r\nprint(\"torch version is: \" + str(torch.__version__))\r\nprint(\"CUDA HOME is: \" + str(CUDA_HOME))\r\nraw_output = subprocess.check_output([str(CUDA_HOME) + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\r\nprint(\"nvcc version is : \" + raw_output)\r\n```\r\nThe output of my computer is:\r\n\r\n> torch version is: 2.4.0+**cu124**\r\n> CUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v12.5\r\n> nvcc version is : nvcc: NVIDIA (R) Cuda compiler driver\r\n> Copyright (c) 2005-2024 NVIDIA Corporation\r\n> Built on Wed_Apr_17_19:36:51_Pacific_Daylight_Time_2024\r\n> Cuda compilation tools, release 12.5, V12.5.40\r\n> Build cuda_12.5.r12.5/compiler.34177558_0\r\n\r\nCould you show me what the output is on your computer?"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T16:15:41Z",
        "body": "torch version is: 2.4.0+cpu\r\nCUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6\r\nnvcc version is : nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024\r\nCuda compilation tools, release 12.6, V12.6.20\r\nBuild cuda_12.6.r12.6/compiler.34431801_0\r\n\r\nthanks for your quick reply, after fixup some torch things ...\r\n\r\n```\r\npython -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\nERROR: The function received no value for the required argument: model_path\r\nUsage: local_chat.py MODEL_PATH <flags>\r\n  optional flags:        --optimize_rule_path | --gguf_path |\r\n                         --max_new_tokens | --cpu_infer\r\n\r\nFor detailed information on this command, run:\r\n  local_chat.py --help\r\n```"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-12T17:45:41Z",
        "body": "> torch version is: 2.4.0+cpu CUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6 nvcc version is : nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2024 NVIDIA Corporation Built on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024 Cuda compilation tools, release 12.6, V12.6.20 Build cuda_12.6.r12.6/compiler.34431801_0\r\n> \r\n> thanks for your quick reply, after fixup some torch things ...\r\n> \r\n> ```\r\n> python -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\n> ERROR: The function received no value for the required argument: model_path\r\n> Usage: local_chat.py MODEL_PATH <flags>\r\n>   optional flags:        --optimize_rule_path | --gguf_path |\r\n>                          --max_new_tokens | --cpu_infer\r\n> \r\n> For detailed information on this command, run:\r\n>   local_chat.py --help\r\n> ```\r\n\r\nPerhaps you can input --model_path instead of --model_name.\r\n```\r\npython -m ktransformers.local_chat --model_path Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\n```\r\n"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-13T12:54:23Z",
        "body": "Great, it works!\r\ni3900k+msi 4090 + ram 96GB\r\ngpu mem 6.3/24GB\r\nram 35GB maybe\r\n```\r\nChat: \u6570\u5b579.11\u548c9.9\u8c01\u5927\uff1f \r\n\u6570\u5b579.11\u548c9.9\u4e2d\uff0c9.11\u6bd49.9\u5c0f\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5c06\u4e24\u4e2a\u6570\u5b57\u8f6c\u6362\u4e3a\u5206\u6570\u6765\u66f4\u6e05\u695a\u5730\u770b\u5230\uff0c\u5176\u4e2d9.11\u4e3a911/1000\u548c9.9\u4e3a99/10\u3002\u6bd4\u8f83\u4e24\u4e2a\u5206\u6570\uff0c911/1000\u5c0f\u4e8e99/10\uff0c\u56e0\u6b649.11\u5c0f\u4e8e9.9\u3002\r\nprompt eval count:    31 token(s)\r\nprompt eval duration: 0.757000207901001s\r\nprompt eval rate:     40.95111160663528 tokens/s\r\neval count:           90 token(s)\r\neval duration:        6.651063442230225s\r\neval rate:            13.531670654132467 tokens/s\r\n```"
      }
    ]
  }
]