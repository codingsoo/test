[
  {
    "number": 67,
    "title": "Error: listen EACCES: permission denied 0.0.0.0:667",
    "created_at": "2024-11-05T10:31:09Z",
    "closed_at": "2024-11-05T22:31:33Z",
    "labels": [],
    "url": "https://github.com/raidendotai/cofounder/issues/67",
    "body": "I receive the following error when attempting to create a project.\r\nSteps to replicate:\r\n1. Clone repository\r\n2.  Run `npx @openinterface/cofounder`\r\n\r\nError:\r\n```\r\n{\r\n  build: {\r\n    'BACKEND:ASYNCAPI::DEFINE': [AsyncFunction: backendAsyncapiDefine],\r\n    'BACKEND:OPENAPI::DEFINE': [AsyncFunction: backendOpenapiDefine],\r\n    'BACKEND:SERVER::GENERATE': [AsyncFunction: backendServerGenerate],\r\n    'DB:POSTGRES::GENERATE': [AsyncFunction: dbPostgresGenerate],\r\n    'DB:SCHEMAS::GENERATE': [AsyncFunction: dbSchemasGenerate],\r\n    'DESIGNER:LAYOUTV1::VIEW:GENERATE': [AsyncFunction: designerLayoutv1ViewGenerate],\r\n    'DESIGNER:LAYOUTV1::VIEW:ITERATE': [AsyncFunction: designerLayoutv1ViewIterate],\r\n    'op:CONVERT::MARKDOWN:PDF': [AsyncFunction: opConvertMarkdownPdf],\r\n    'op:INDEXDB::QUERY': [AsyncFunction: opIndexdbQuery],\r\n    'op:LLM::GEN': [AsyncFunction: opLlmGen],\r\n    'op:LLM::VECTORIZE': [AsyncFunction: opLlmVectorize],\r\n    'op:LLM::VECTORIZE:CHUNK': [AsyncFunction: opLlmVectorizeChunk],\r\n    'op:LLM::DEBUG:SIMULATE': [AsyncFunction: opLlmDebugSimulate],\r\n    'op:PROJECT::STATE:UPDATE': [AsyncFunction: opProjectStateUpdate],\r\n    'op:PROJECT::STATE:LOAD': [AsyncFunction: opProjectStateLoad],\r\n    'op:PROJECT::STATE:SETUP': [AsyncFunction: opProjectStateSetup],\r\n    'op:PROJECT::STATE:EXPORT': [AsyncFunction: opProjectStateExport],\r\n    'op:RENDER::LAYOUT': [AsyncFunction: opRenderLayout],\r\n    'PM:BRD::ANALYSIS': [AsyncFunction: pmBrdAnalysis],\r\n    'PM:DRD::ANALYSIS': [AsyncFunction: pmDrdAnalysis],\r\n    'PM:FJMD::ANALYSIS': [AsyncFunction: pmFjmdAnalysis],\r\n    'PM:FRD::ANALYSIS': [AsyncFunction: pmFrdAnalysis],\r\n    'PM:PRD::ANALYSIS': [AsyncFunction: pmPrdAnalysis],\r\n    'PM:UXDMD::ANALYSIS': [AsyncFunction: pmUxdmdAnalysis],\r\n    'PM:UXSMD::ANALYSIS': [AsyncFunction: pmUxsmdAnalysis],\r\n    'SWARM:AUGMENT::BACKEND:EXTERNALAPIS': [AsyncFunction: swarmAugmentBackendExternalapis],\r\n    'SWARM:FIX::BACKEND': [AsyncFunction: swarmFixBackend],\r\n    'SWARM:FIX::WEBAPP': [AsyncFunction: swarmFixWebapp],\r\n    'SWARM:REVIEW::SERVER:MAIN': [AsyncFunction: swarmReviewServerMain],\r\n    'SWARM:REVIEW::WEBAPP:STORE': [AsyncFunction: swarmReviewWebappStore],\r\n    'SWARM:REVIEW::WEBAPP:ROOT': [AsyncFunction: swarmReviewWebappRoot],\r\n    'SWARM:REVIEW::WEBAPP:VIEW': [AsyncFunction: swarmReviewWebappView],\r\n    'UX:DATAMAP::STRUCTURE': [AsyncFunction: uxDatamapStructure],\r\n    'UX:DATAMAP::VIEWS': [AsyncFunction: uxDatamapViews],\r\n    'UX:DATAMAP::VIEWS:CHUNK': [AsyncFunction: uxDatamapViewsChunk],\r\n    'UX:SITEMAP::STRUCTURE': [AsyncFunction: uxSitemapStructure],\r\n    'UX:SITEMAP::VIEWS': [AsyncFunction: uxSitemapViews],\r\n    'UX:SITEMAP::VIEWS:NORMAL': [AsyncFunction: uxSitemapViewsNormal],\r\n    'UX:SITEMAP::VIEWS:SPECIAL': [AsyncFunction: uxSitemapViewsSpecial],\r\n    'UX:SITEMAP::VIEWS:SHARED': [AsyncFunction: uxSitemapViewsShared],\r\n    'WEBAPP:ROOT::GENERATE': [AsyncFunction: webappRootGenerate],\r\n    'WEBAPP:STORE::GENERATE': [AsyncFunction: webappStoreGenerate],\r\n    'WEBAPP:VIEW::GENERATE:MULTI': [AsyncFunction: webappViewGenerateMulti],\r\n    'WEBAPP:VIEW::GENERATE': [AsyncFunction: webappViewGenerate],\r\n    'WEBAPP:VIEW::ITERATE': [AsyncFunction: webappViewIterate]\r\n  }\r\n}\r\nbuild:DAG : parent node WEBAPP:STORE::GENERATE of node WEBAPP:ROOT::GENERATE not found in DAG - skipping dependency\r\nnode:events:495\r\n      throw er; // Unhandled 'error' event\r\n      ^\r\n\r\nError: listen EACCES: permission denied 0.0.0.0:667\r\n    at Server.setupListenHandle [as _listen2] (node:net:1800:21)\r\n    at listenInCluster (node:net:1865:12)\r\n    at Server.listen (node:net:1953:7)\r\n    at Function.listen (/home/user/Experiments/Cofounder/cofounder/api/node_modules/express/lib/application.js:635:24)\r\n    at file:///home/user/Experiments/Cofounder/cofounder/api/server.js:134:20\r\nEmitted 'error' event on Server instance at:\r\n    at emitErrorNT (node:net:1844:8)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {\r\n  code: 'EACCES',\r\n  errno: -13,\r\n  syscall: 'listen',\r\n  address: '0.0.0.0',\r\n  port: 667\r\n}\r\n\r\nNode.js v18.20.4\r\n[nodemon] app crashed - waiting for file changes before starting...\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/raidendotai/cofounder/issues/67/comments",
    "author": "zaakirio",
    "comments": [
      {
        "user": "suediedev",
        "created_at": "2024-11-05T18:43:55Z",
        "body": "Run the script with root"
      },
      {
        "user": "raidendotai",
        "created_at": "2024-11-05T22:31:33Z",
        "body": "ports fixed"
      }
    ]
  },
  {
    "number": 53,
    "title": "image exceeds 5 MB maximum",
    "created_at": "2024-11-04T07:41:41Z",
    "closed_at": "2024-11-05T22:31:54Z",
    "labels": [],
    "url": "https://github.com/raidendotai/cofounder/issues/53",
    "body": "Hi,\r\nI am wondering if anyone is getting this error while the Designer Analysis stage (designer/layoutv1 task analysis)?\r\n\r\n{\r\n  asyncretry_error: {\r\n    id: 'op:LLM::GEN',\r\n    error: BadRequestError: 400 {\r\n      status: 400,\r\n      headers: {\r\n        'cf-cache-status': 'DYNAMIC',\r\n        'cf-ray': '8dcd691aab942ead-MEL',\r\n        connection: 'keep-alive',\r\n        'content-length': '169',\r\n        'content-type': 'application/json',\r\n        date: 'Sun, 03 Nov 2024 15:31:25 GMT',\r\n        'request-id': 'req_01DpCHxH1cyZDKe4137651em',\r\n        server: 'cloudflare',\r\n        via: '1.1 google',\r\n        'x-robots-tag': 'none',\r\n        'x-should-retry': 'false'\r\n      },\r\n      request_id: 'req_01DpCHxH1cyZDKe4137651em',\r\n      error: {\r\n        type: 'error',\r\n        error: {\r\n          type: 'invalid_request_error',\r\n          message: 'messages.0.content.1.image.source.base64: image exceeds 5 MB maximum: 5327908 bytes > 5242880 bytes'\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n",
    "comments_url": "https://api.github.com/repos/raidendotai/cofounder/issues/53/comments",
    "author": "jroth55",
    "comments": [
      {
        "user": "raidendotai",
        "created_at": "2024-11-04T14:02:22Z",
        "body": "yes indeed - API was sending images bigger than claude limits.\r\nwaiting for new index checkpoint to complete (in ~1hr) and will push updated api\r\nthanks :)"
      },
      {
        "user": "QRMarketing",
        "created_at": "2024-11-05T05:15:55Z",
        "body": "Yes, same issue ... just loops on this error ....  Any work arounds?"
      }
    ]
  },
  {
    "number": 51,
    "title": "Markdown Document Output",
    "created_at": "2024-11-04T02:11:09Z",
    "closed_at": "2024-11-05T02:04:19Z",
    "labels": [],
    "url": "https://github.com/raidendotai/cofounder/issues/51",
    "body": "Great project!\r\n\r\nWould love if the generated markdown documents could be also written to the disk somewhere, perhaps in a parallel docs directory.",
    "comments_url": "https://api.github.com/repos/raidendotai/cofounder/issues/51/comments",
    "author": "mattdlong",
    "comments": [
      {
        "user": "raidendotai",
        "created_at": "2024-11-04T14:08:19Z",
        "body": "they're written to `cofounder/api/db/projects/{name}/{category}`"
      },
      {
        "user": "mattdlong",
        "created_at": "2024-11-05T02:04:19Z",
        "body": "Brilliant, thanks!"
      }
    ]
  },
  {
    "number": 50,
    "title": "OpenAI API not being used",
    "created_at": "2024-11-03T23:38:33Z",
    "closed_at": "2024-11-03T23:47:30Z",
    "labels": [],
    "url": "https://github.com/raidendotai/cofounder/issues/50",
    "body": "I've been getting consistent anthropic API rate limits after a single prompt that uses all my daily tokens since I'm still tier 1, however I put 20$ on my OpenAI API for gpt4o and I noticed that my usage hasn't gone up and I only used 0.01$ for OpenAI. However, I look at claude/anthropic and it's been BURNING all of my credits. I'm not sure if the Anthropic API rate limit error is a glitch and building the app is actually using claude. So far building this one app (still half-way due to rate limits) has cost me 22$ in API credits which I find ridiculous and seems like I'm getting charged for an API call just to receive the rate limit error. Anyone else getting this issue?",
    "comments_url": "https://api.github.com/repos/raidendotai/cofounder/issues/50/comments",
    "author": "BioCruz-sudo",
    "comments": [
      {
        "user": "hiddenhidden",
        "created_at": "2024-11-05T00:25:44Z",
        "body": "I'm encountering the same issue with Anthropic's API rate limits. Despite having a minimal usage with OpenAI (only $0.01 so far), I've hit my daily token limit with Anthropic after only a few interactions, which has effectively halted my work with Cofounder. I’ve reached out to Anthropic’s sales team, but I think there’s an opportunity here to improve API usage within Cofounder itself.\r\n\r\nWould it be possible to add an option to balance API usage between Anthropic and OpenAI? Perhaps you could prioritize OpenAI for tasks like embeddings and reserve Anthropic for specific inference cases where Claude might offer advantages. This would allow users to make the most of both platforms’ quotas and avoid hitting rate limits so quickly on Anthropic.\r\n\r\nA feature where users could set preferences or priorities for different LLM providers might make Cofounder more flexible and accessible, especially for those on lower API tiers. I will post here if I hear back from Anthropic Sales.\r\n\r\nThanks - "
      }
    ]
  },
  {
    "number": 10,
    "title": "Something about DAG",
    "created_at": "2024-10-31T10:56:34Z",
    "closed_at": "2024-11-05T22:31:20Z",
    "labels": [],
    "url": "https://github.com/raidendotai/cofounder/issues/10",
    "body": "While installing its giving me below 'error'. Now the dashboard doesnt seem to work correctly.\r\n\r\nbuild:DAG : parent node WEBAPP:STORE::GENERATE of node WEBAPP:ROOT::GENERATE not found in DAG - skipping dependency",
    "comments_url": "https://api.github.com/repos/raidendotai/cofounder/issues/10/comments",
    "author": "sandervdorth",
    "comments": [
      {
        "user": "rahmat-ullah",
        "created_at": "2024-10-31T19:09:45Z",
        "body": "build:DAG : parent node WEBAPP:STORE::GENERATE of node WEBAPP:ROOT::GENERATE not found in DAG - skipping dependency\r\n\r\nI got the same. "
      },
      {
        "user": "raidendotai",
        "created_at": "2024-11-04T16:56:28Z",
        "body": "likely debugging leftovers\r\nwill check - thanks"
      }
    ]
  },
  {
    "number": 2,
    "title": "Error `generated is empty` even after response is coming.",
    "created_at": "2024-10-04T04:15:00Z",
    "closed_at": "2024-10-04T13:50:15Z",
    "labels": [],
    "url": "https://github.com/raidendotai/cofounder/issues/2",
    "body": "I can see generation and streaming in nodes, but suddenly it stops and gives me the following error. I wait for multiple retries but it still same issue. I assume Tokens are getting exhausted and the code is not handling this. Any fix for it? \r\n\r\n```\r\nname\":\"Backend code\",\"desc\"\r\n{\r\n  asyncretry_error: {\r\n    id: 'BACKEND:SERVER::GENERATE',\r\n    error: Error: backend:server:generate error - generated is empty\r\n        at Object.backendServerGenerate [as BACKEND:SERVER::GENERATE] (./cofounder/api/system/functions/backend/server.js:314:9)\r\n        at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\r\n        at async ./cofounder/api/build.js:87:30\r\n        at async ./cofounder/api/build.js:84:25\r\n        at async ./cofounder/api/node_modules/p-queue/dist/index.js:187:36\r\n  }\r\n}\r\nlog:complete: node:op:LLM::GEN  {\"context\":{\"streams\":{},\"project\":\"myproject\",\"sequence\":{\"resume\":8},\"operation\":{\"key\":\"backend.server.main\",\"meta\":{\"name\":\"Backend code\",\"desc\"\r\n{\r\n  asyncretry_error: {\r\n    id: 'BACKEND:SERVER::GENERATE',\r\n    error: Error: backend:server:generate error - generated is empty\r\n        at Object.backendServerGenerate [as BACKEND:SERVER::GENERATE] (./cofounder/api/system/functions/backend/server.js:314:9)\r\n        at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\r\n        at async ./cofounder/api/build.js:87:30\r\n        at async ./cofounder/api/build.js:84:25\r\n        at async ./cofounder/api/node_modules/p-queue/dist/index.js:187:36\r\n  }\r\n}\r\n```",
    "comments_url": "https://api.github.com/repos/raidendotai/cofounder/issues/2/comments",
    "author": "rahulgurujala",
    "comments": [
      {
        "user": "raidendotai",
        "created_at": "2024-10-04T13:50:15Z",
        "body": "that code snippet handles exactly that and is already implement ,\r\nnamely retries with backoff in cases like\r\n- llm api problems ( rate limited , api cant be reached , ... )\r\n- instructions not followed by llm ( ie. empty code )\r\n- llm output not maching structure ( ie. code not written inside block )\r\n\r\nyou're most likely hitting output tokens limit of the model in your specific case,\r\nand it doesnt get to finish generating code\r\n\r\nfix would be using model with higher output tokens\r\n\r\ni'll use o1-mini with 32k as default in a lot of cases on next update, should fix most ^\r\n\r\nin the meantime , you can try digging into `cofounder/api/system/functions/backend/server.js` and specify desired models (same approach for any other inference pass)\r\n(also make sure `cofounder/api/.env` sets openai for LLMs , else everything defaults to `claude 3.5 sonnet` )"
      }
    ]
  }
]