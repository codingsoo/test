[
  {
    "number": 212,
    "title": "httpx.ConnectTimeout",
    "created_at": "2024-12-09T12:53:01Z",
    "closed_at": "2024-12-17T18:28:00Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/212",
    "body": "This kind of timeout situation occasionally occurs. How to solve this problem? Can the timeout period be modified or a retry mechanism be added?\r\n\r\n2024-12-09T12:44:45.254120Z [error    ] Error checking Langsmith access [langgraph_license.validation] api_revision=4da016a api_variant=local\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\r\n    yield\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 377, in handle_async_request\r\n    resp = await self._pool.handle_async_request(req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 216, in handle_async_request\r\n    raise exc from None\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 196, in handle_async_request\r\n    response = await connection.handle_async_request(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 99, in handle_async_request\r\n    raise exc\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 76, in handle_async_request\r\n    stream = await self._connect(request)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 122, in _connect\r\n    stream = await self._network_backend.connect_tcp(**kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 30, in connect_tcp\r\n    return await self._backend.connect_tcp(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 115, in connect_tcp\r\n    with map_exceptions(exc_map):\r\n         ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 158, in __exit__\r\n    self.gen.throw(value)\r\n  File \"/usr/local/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\r\n    raise to_exc(exc) from exc\r\nhttpcore.ConnectTimeout\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/site-packages/langgraph_license/validation.py\", line 56, in check_langsmith_access\r\n    res = await client.get(\r\n          ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1814, in get\r\n    return await self.request(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1585, in request\r\n    return await self.send(request, auth=auth, follow_redirects=follow_redirects)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1674, in send\r\n    response = await self._send_handling_auth(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1702, in _send_handling_auth\r\n    response = await self._send_handling_redirects(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1739, in _send_handling_redirects\r\n    response = await self._send_single_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_client.py\", line 1776, in _send_single_request\r\n    response = await transport.handle_async_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 376, in handle_async_request\r\n    with map_httpcore_exceptions():\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 158, in __exit__\r\n    self.gen.throw(value)\r\n  File \"/usr/local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\r\n    raise mapped_exc(message) from exc\r\nhttpx.ConnectTimeout\r\n",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/212/comments",
    "author": "xibudadao",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-12-17T18:28:00Z",
        "body": "This is an issue with LangGraph studio which has been flagged. Unfortunately it's not something we can deal with on the open canvas side."
      }
    ]
  },
  {
    "number": 211,
    "title": "how to disable trace or log upload to langchain platform",
    "created_at": "2024-12-08T14:47:26Z",
    "closed_at": "2024-12-17T21:25:22Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/211",
    "body": "For example, this information will be automatically uploaded to the platform, and I want to prohibit uploading because some of it may involve private data.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/211/comments",
    "author": "xibudadao",
    "comments": [
      {
        "user": "hinthornw",
        "created_at": "2024-12-08T22:13:46Z",
        "body": "You can set LANGSMITH_TRACING=false i believe"
      },
      {
        "user": "bracesproul",
        "created_at": "2024-12-17T21:25:22Z",
        "body": "Will's comment is correct! Just add `LANGSMITH_TRACING=false` as an environment variable"
      }
    ]
  },
  {
    "number": 204,
    "title": "How to use src/agent/open-canvas as the server side?",
    "created_at": "2024-11-27T12:31:33Z",
    "closed_at": "2024-12-14T01:47:37Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/204",
    "body": "What is the configuration in this directory in the project? Can it be used as an independent back-end service to provide functions similar to langgraph studio [memory-agent-js] ?",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/204/comments",
    "author": "honestAnt",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-12-04T17:10:27Z",
        "body": "Yes it can be! The `langgraph.json` file is the configuration file which langgraph studio/cloud uses to run the project"
      },
      {
        "user": "honestAnt",
        "created_at": "2024-12-14T01:47:37Z",
        "body": "OK, thanks for letting me know"
      }
    ]
  },
  {
    "number": 198,
    "title": "Replace LangSmith with LangFuse",
    "created_at": "2024-11-12T15:22:13Z",
    "closed_at": "2024-11-12T17:51:10Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/198",
    "body": "Hi, There is an option to replace the connection to LangSmith with LangFuse?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/198/comments",
    "author": "IsraelShok",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-11-12T17:51:10Z",
        "body": "Hey, this is not something we'd be looking to add to the main repo, however you're free to fork it and add it to your personal repo!"
      },
      {
        "user": "IsraelShok",
        "created_at": "2024-11-12T17:57:55Z",
        "body": "But can I disconnect langsmith?\r\nI can see that you create a Client that is in the package of lang chain.\r\nThere is a way to remove this creation?\r\nItâ€™s combined together and Iâ€™m trying to understand how the change can be made."
      },
      {
        "user": "bracesproul",
        "created_at": "2024-11-12T18:03:23Z",
        "body": "@IsraelShok yes you can! To prevent LangSmith from tracing, set the `LANGCHAIN_TRACING_V2` environment variable to `\"false\"`"
      },
      {
        "user": "IsraelShok",
        "created_at": "2024-11-12T18:05:27Z",
        "body": "And then I will not need the langchain api key? \r\nall I see itâ€™s used for creating a user for langsmith.\r\nSo what I missing here?\r\n"
      },
      {
        "user": "bracesproul",
        "created_at": "2024-11-12T18:17:08Z",
        "body": "@IsraelShok  If you set `LANGCHAIN_TRACING_V2` to `\"false\"` you can ignore all other LangChain API keys. You'll still need a LangSmith account to run it though since it runs on LangGraph Cloud. It requires logging in to run locally (free tier supported), and a Plus subscription to deploy to production."
      },
      {
        "user": "IsraelShok",
        "created_at": "2024-11-12T18:26:00Z",
        "body": "I run it on docker, itâ€™s still runs on langgraph cloud?\r\nI canâ€™t expose it to the cloud, there is a way to solve it?\r\n\r\nthanks a lot for your help!"
      }
    ]
  },
  {
    "number": 188,
    "title": "Python version of Open Canvas",
    "created_at": "2024-11-05T18:55:36Z",
    "closed_at": "2024-11-06T19:45:25Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/188",
    "body": "Are there any plans to port the Code to python?",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/188/comments",
    "author": "reidarkind",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-11-06T19:45:25Z",
        "body": "Hey @reidarkind this is being tracked by #39 \r\n\r\nIt's not on my roadmap, however I would love a community contribution!"
      }
    ]
  },
  {
    "number": 160,
    "title": "how can adding GEMINI api key",
    "created_at": "2024-10-29T02:23:19Z",
    "closed_at": "2024-10-29T19:03:12Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/160",
    "body": "I need plis, I don't know how to put it on",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/160/comments",
    "author": "bryansmithsantos",
    "comments": [
      {
        "user": "ahmad2b",
        "created_at": "2024-10-29T05:53:48Z",
        "body": "Hey @thx404, You can use Gemini by switching to the staging branch. Make sure you've added your Gemini API key to the .env file with the `GOOGLE_API_KEY` variable."
      },
      {
        "user": "bracesproul",
        "created_at": "2024-10-29T19:03:12Z",
        "body": "@thx404 What @ahmad2b said is true, however I've discovered an issue with streaming tool calls in the `@langchain/google-genai` package, so I've disabled the Gemini integration for now. I'm working on a fix in the LangChain.js repo though, and once that's done I'll reenable it in staging."
      }
    ]
  },
  {
    "number": 154,
    "title": "Improve the highlight to edit frontend UX",
    "created_at": "2024-10-28T18:39:27Z",
    "closed_at": "2025-02-13T17:03:34Z",
    "labels": [
      "tracking",
      "frontend"
    ],
    "url": "https://github.com/langchain-ai/open-canvas/issues/154",
    "body": "The highlight to edit feature can be a little janky. I would like to have the following improvements made to it:\n- [ ] Ensure the `Ask Open Canvas` input never shows up if text is highlighted outside of the code/markdown renderer.\n- [ ] Allow users to trigger it when they select using the keyboard (cmd + shift + arrow, etc.). Right now it is only triggered by selecting with a mouse.\n- [ ] Disable code string match highlighting. The code editor has a feature where you can select strings and it will highlight all occurrences in the file (like an IDE does). Let's disable this.\n- [ ] Improve the `Ask Open Canvas` UI in both the code and markdown renderers. (if possible, use the actions menu which pops up when you select text in the markdown editor. This is currently disabled because the `formattingToolbar={false}` prop is passed. Ideally we can overwrite the contents to contain an `Ask Open Canvas` input, and a `B` + `I` (bold/italic) button).\n- [ ] Improve the UX to include a smooth transition when you click `Ask Open Canvas` and it transitions into an input component.\n\nThese should all be done as separate PRs. If you want to take one on, please leave a reply and I will create a new issue for that task and assign it to you.",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/154/comments",
    "author": "bracesproul",
    "comments": [
      {
        "user": "ahmad2b",
        "created_at": "2024-11-05T13:45:56Z",
        "body": "Hey @bracesproul! ðŸ‘‹\r\n\r\nI've submitted PR #187 tackling the first UX improvement from your list - `ensuring AskOpenCanvas respects renderer boundaries`. The implementation is thorough with strict validation and clean state management.\r\n\r\nMoving forward with the remaining tasks.\r\n\r\nI'll keep the same pattern of isolated PRs for each task to maintain clean review cycles. Looking forward to your feedback on the current PR!"
      }
    ]
  },
  {
    "number": 152,
    "title": "Background job for generating titles of chats",
    "created_at": "2024-10-28T18:18:11Z",
    "closed_at": "2024-10-30T18:04:05Z",
    "labels": [
      "ai"
    ],
    "url": "https://github.com/langchain-ai/open-canvas/issues/152",
    "body": "The logic for getting the title of a chat history is to just take the input message and make that the title (truncated). This is _not_ ideal. My idea for the new flow should be as follows:\r\n\r\n1. Setup a subgraph (this way it can run in the background and not be blocking) the same way we setup & call the `reflection` graph (as a node inside the main graph which calls the subgraph via the LangGraph SDK). Add a new directory inside `./src/agent`, and add this graph to the `langgraph.json` file.\r\n2. This subgraph should only execute at the end of a run, if it was the first run in a thread (1 user message)\r\n3. It should take the user message, and the generated artifact, and response to message (or just response if no artifact was generated) and prompt the LLM to generate an extremely short title for this thread. Ideally, we never need to truncate these titles.\r\n4. Once generated, it should then call the LangGraph SDK to update the metadata on the thread with a new field `thread_title`.\r\n5. Update the client side code which gets the title for the thread to first look in the thread metadata for this field, and if it doesn't exist, fallback to the current logic.\r\n6. We should use `gpt-4o-mini` to generate this title. Do not use the configured model to do this.",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/152/comments",
    "author": "bracesproul",
    "comments": [
      {
        "user": "ahmad2b",
        "created_at": "2024-10-30T13:09:57Z",
        "body": "Hey @bracesproul, Thank you for the detailed spec!\r\n\r\nI've submitted a PR (#165) that implements the background jobs for generating chat titles as you suggested. It runs in the background, updates the thread `metadata` with a new `thread_title` field, and uses `gpt-4o-mini` to create concise titles.\r\n\r\nPlease check it out and let me know what you think!"
      }
    ]
  },
  {
    "number": 149,
    "title": "Add settings config for temperature, max tokens",
    "created_at": "2024-10-28T17:13:18Z",
    "closed_at": "2025-01-28T01:26:30Z",
    "labels": [
      "fullstack",
      "ai"
    ],
    "url": "https://github.com/langchain-ai/open-canvas/issues/149",
    "body": "Max tokens should have a dynamic slider based on the model selected and its token output range. Temperature should also be a slider, allowing 0-1 (or 0-2 if the model supports)\n\nIt should have a nice, non technical description describing what each field will do.\n\nThese fields should be passed through `configurable` like we what do with the model name.\n\nThe max tokens field should apply to all nodes which update/rewrite artifacts. It should not apply to the router or followup nodes.",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/149/comments",
    "author": "bracesproul",
    "comments": [
      {
        "user": "ahmad2b",
        "created_at": "2024-10-30T14:06:22Z",
        "body": "Hey @bracesproul,\r\n\r\nI appreciate your detailed suggestions! I'm eager to take on this issue and start working on adding the settings configuration for temperature and max tokens. It looks like this is tied to the model customization feature thatâ€™s currently in the staging branch.\r\n\r\nShould I proceed with creating the PR in the staging branch, or would you prefer I wait until itâ€™s released in the main branch?\r\n\r\nLooking forward to your guidance!\r\n\r\nSincerely,\r\nAhmad"
      },
      {
        "user": "bracesproul",
        "created_at": "2024-10-30T14:21:40Z",
        "body": "@ahmad2b planning on releasing staging today, so let's hold off until that gets merged in before working on this. I'll assign it to you in the meantime "
      },
      {
        "user": "ahmad2b",
        "created_at": "2024-10-30T15:08:49Z",
        "body": "Thanks for the update, @bracesproul! I appreciate the assignment. Iâ€™ll hold off until then. Iâ€™m excited to dive in once weâ€™re ready!"
      },
      {
        "user": "bracesproul",
        "created_at": "2024-10-31T00:47:01Z",
        "body": "@ahmad2b you're all set to start on this now"
      }
    ]
  },
  {
    "number": 141,
    "title": "Add Gemini 1.5 Flash to model options",
    "created_at": "2024-10-25T22:59:49Z",
    "closed_at": "2024-10-28T15:51:24Z",
    "labels": [
      "good first issue"
    ],
    "url": "https://github.com/langchain-ai/open-canvas/issues/141",
    "body": "This is only in the `staging` branch atm, so you'll need to checkout that branch first. Update the fields inside `constants.ts` to include Gemini 1.5 Flash. Install `@langchain/google-vertexai`, then test. It should be as simple as that!",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/141/comments",
    "author": "bracesproul",
    "comments": [
      {
        "user": "mjunaidca",
        "created_at": "2024-10-26T06:34:01Z",
        "body": "Hi @bracesproul!\r\n\r\nThis #142 PR adds Gemini 1.5 Flash to the model options in the staging branch as outlined in issue #141, with dependencies and authentication configured. Testing confirmed the model appears in the options, API calls authenticate successfully, and no errors were encountered.\r\n\r\nKindly review when you have a chance, and let me know if thereâ€™s anything else I can adjust. Thanks!"
      }
    ]
  },
  {
    "number": 134,
    "title": "Error Thread ID not found",
    "created_at": "2024-10-25T10:50:45Z",
    "closed_at": "2024-10-25T11:02:36Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/134",
    "body": "Getting an error \"Error Thread ID not found\" whenever I try generate something. Here's my terminal output.\r\n\r\n\r\n```\r\n POST /api/threads 500 in 449ms\r\nError in proxy\r\nTypeError: fetch failed\r\n    at node:internal/deps/undici/undici:13185:13\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\r\n    at async handleRequest (webpack-internal:///(rsc)/./src/app/api/[..._path]/route.ts:73:21)\r\n    at async /Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/compiled/next-server/app-route.runtime.dev.js:6:55038\r\n    at async ek.execute (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/compiled/next-server/app-route.runtime.dev.js:6:45808)\r\n    at async ek.handle (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/compiled/next-server/app-route.runtime.dev.js:6:56292)\r\n    at async doRender (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:1357:42)\r\n    at async cacheEntry.responseCache.get.routeKind (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:1567:40)\r\n    at async DevServer.renderToResponseWithComponentsImpl (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:1487:28)\r\n    at async DevServer.renderPageComponent (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:1911:24)\r\n    at async DevServer.renderToResponseImpl (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:1949:32)\r\n    at async DevServer.pipeImpl (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:916:25)\r\n    at async NextNodeServer.handleCatchallRenderRequest (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/next-server.js:272:17)\r\n    at async DevServer.handleRequestImpl (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/base-server.js:812:17)\r\n    at async /Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/dev/next-dev-server.js:339:20\r\n    at async Span.traceAsyncFn (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/trace/trace.js:154:20)\r\n    at async DevServer.handleRequest (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/dev/next-dev-server.js:336:24)\r\n    at async invokeRender (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/lib/router-server.js:173:21)\r\n    at async handleRequest (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/lib/router-server.js:350:24)\r\n    at async requestHandlerImpl (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/lib/router-server.js:374:13)\r\n    at async Server.requestListener (/Users/adamsardo/Developer/open-canvas-main/node_modules/next/dist/server/lib/start-server.js:141:13) {\r\n  [cause]: Error: connect ECONNREFUSED 127.0.0.1:53974\r\n      at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1606:16)\r\n      at TCPConnectWrap.callbackTrampoline (node:internal/async_hooks:130:17) {\r\n    errno: -61,\r\n    code: 'ECONNREFUSED',\r\n    syscall: 'connect',\r\n    address: '127.0.0.1',\r\n    port: 53974\r\n  }\r\n}\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/134/comments",
    "author": "adamsardo",
    "comments": [
      {
        "user": "girotte-tao",
        "created_at": "2025-01-17T07:58:42Z",
        "body": "Hello, same issue, can you kindly tell me how to solve it?\n"
      }
    ]
  },
  {
    "number": 112,
    "title": "yarn fail",
    "created_at": "2024-10-24T01:31:05Z",
    "closed_at": "2024-10-24T16:28:07Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/112",
    "body": "yarn install v1.22.22\r\n[1/4] Resolving packages...\r\n[2/4] Fetching packages...\r\n[3/4] Linking dependencies...\r\nwarning \" > @uiw/react-codemirror@4.23.5\" has unmet peer dependency \"@babel/runtime@>=7.11.0\".\r\nwarning \" > @uiw/react-codemirror@4.23.5\" has unmet peer dependency \"@codemirror/state@>=6.0.0\".\r\nwarning \" > @uiw/react-codemirror@4.23.5\" has unmet peer dependency \"@codemirror/theme-one-dark@>=6.0.0\".\r\nwarning \" > @uiw/react-codemirror@4.23.5\" has unmet peer dependency \"@codemirror/view@>=6.0.0\".\r\nwarning \" > @uiw/react-codemirror@4.23.5\" has unmet peer dependency \"codemirror@>=6.0.0\".\r\nwarning \"@uiw/react-codemirror > @uiw/codemirror-extensions-basic-setup@4.23.5\" has unmet peer dependency \"@codemirror/autocomplete@>=6.0.0\".\r\nwarning \"@uiw/react-codemirror > @uiw/codemirror-extensions-basic-setup@4.23.5\" has unmet peer dependency \"@codemirror/language@>=6.0.0\".\r\nwarning \"@uiw/react-codemirror > @uiw/codemirror-extensions-basic-setup@4.23.5\" has unmet peer dependency \"@codemirror/lint@>=6.0.0\".\r\nwarning \"@uiw/react-codemirror > @uiw/codemirror-extensions-basic-setup@4.23.5\" has unmet peer dependency \"@codemirror/search@>=6.0.0\".\r\nwarning \"@uiw/react-codemirror > @uiw/codemirror-extensions-basic-setup@4.23.5\" has unmet peer dependency \"@codemirror/view@>=6.0.0\".\r\nwarning \" > react-json-view@1.21.3\" has incorrect peer dependency \"react@^17.0.0 || ^16.3.0 || ^15.5.4\".\r\nwarning \" > react-json-view@1.21.3\" has incorrect peer dependency \"react-dom@^17.0.0 || ^16.3.0 || ^15.5.4\".\r\nwarning \"react-json-view > flux@4.0.4\" has incorrect peer dependency \"react@^15.0.2 || ^16.0.0 || ^17.0.0\".\r\ninfo There appears to be trouble with your network connection. Retrying...\r\n[4/4] Building fresh packages...\r\nDone in 72.59s.\r\nPS C:\\eines\\copia-canvas-gpt\\open-canvas>",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/112/comments",
    "author": "sontoriyama",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-10-24T16:28:07Z",
        "body": "```\r\ninfo There appears to be trouble with your network connection. Retrying...\r\n```\r\nyour network issue, not a bug."
      }
    ]
  },
  {
    "number": 111,
    "title": "cuando vi que pedis lo de Studio que solo se puede descargar en mac ya tenia media instalacion echa jaja",
    "created_at": "2024-10-24T00:31:28Z",
    "closed_at": "2024-10-24T18:51:19Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/111",
    "body": "en windows sacareis un docker aunque seaÂ¿? cuando creeis que podrÃ© usarlo en windows?  gracias, hacia dias que pensaba que habia de haver un open source del canvas de gpt pero solo encontrÃ© un proyecto de hace un aÃ±o con este nombre y pedia dependencias peligrosas que no me atrevÃ­ a instalar jeje",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/111/comments",
    "author": "sontoriyama",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-10-24T18:51:19Z",
        "body": "hey, so we are planning on adding better support for running on windows/linux, however it won't be ready for a couple weeks.\r\n\r\nIn the meantime you can still deploy on LangGraph Cloud, it only requires a LangSmith Plus account"
      }
    ]
  },
  {
    "number": 110,
    "title": "Docker version please!",
    "created_at": "2024-10-23T20:49:19Z",
    "closed_at": "2024-10-24T17:21:07Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/110",
    "body": "as per title",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/110/comments",
    "author": "gitwittidbit",
    "comments": [
      {
        "user": "che77a38",
        "created_at": "2024-10-24T01:30:05Z",
        "body": "I need it too"
      },
      {
        "user": "JerePlum99",
        "created_at": "2024-10-24T04:22:42Z",
        "body": "Yes even just docker scoped for the front end but integrated with the langgraph cloud api would help a lot. \n\nI have less front end experience so this and the Python backend would be huge!"
      },
      {
        "user": "bracesproul",
        "created_at": "2024-10-24T17:21:07Z",
        "body": "In the next coming weeks we're putting out better support for running LangGraph in docker on Windows/Linux. I'll update this repo with instructions on how to do that once it's released ðŸ™‚"
      }
    ]
  },
  {
    "number": 109,
    "title": "How to run OpenCanvas locally on Linux?",
    "created_at": "2024-10-23T17:15:48Z",
    "closed_at": "2024-10-24T17:28:03Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/109",
    "body": "Hi, great project!\r\nIf I understood correctly, OpenCanvas needs LangGraph, which as of now is available for local installation only on MacOS.\r\nI feel blind, but wasn't able to find an easy way to couple a local OpenCanvas to an LangGraph cloud API.\r\nPlease point me in the right direction.\r\nThank you.\r\nPiero",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/109/comments",
    "author": "PieBru",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-10-24T17:28:03Z",
        "body": "We're working on adding better support for running LangGraph Cloud apps on Windows/Linux in the coming weeks! I'll post an update in this repo & update the Readme once that's ready\r\n\r\nYou can also deploy Open Canvas on LangGraph Cloud which only requires a LangSmith Plus subscription."
      },
      {
        "user": "PieBru",
        "created_at": "2024-10-24T17:59:57Z",
        "body": "Great! Being a single dev, I will wait for the update. Thanks"
      }
    ]
  },
  {
    "number": 102,
    "title": "Update custom model selection to include `GPT 4o mini (Azure)` option",
    "created_at": "2024-10-23T04:26:36Z",
    "closed_at": "2024-11-05T22:59:12Z",
    "labels": [
      "fullstack"
    ],
    "url": "https://github.com/langchain-ai/open-canvas/issues/102",
    "body": "> **NOTE**: The customizable model feature is only in the `staging` branch at this time. You will need to checkout a branch off `staging` and open your PR against `staging` to implement this feature.\r\n\r\nUpdate the custom model dropdown options & util for converting model string to `modelName` and `modelProvider` to include `GPT 4o mini (Azure)` which uses the `@langchain/openai` package, but with Azure credentials. The util which returns `modelName` and `modelProvider` should be updated to also return the other init fields which are required to support calling the Azure OpenAI API via the `@langchain/openai` package.\r\n\r\nFinally, please update the `.env.example` file with the required credentials.",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/102/comments",
    "author": "petrusali",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-10-28T18:59:15Z",
        "body": "@petrusali now that we have the option for customizing the model (in staging branch atm), I've updated the issue title & description to include detailed steps showing how to add support for Azure OpenAI models! Let me know if I'm missing anything from your initial issue (not including Open Router support, LangChain's `@langchain/openai` class has support for calling the Azure API)."
      }
    ]
  },
  {
    "number": 99,
    "title": "Add support for Azure OpenAI",
    "created_at": "2024-10-22T09:30:20Z",
    "closed_at": "2024-10-22T14:22:44Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/99",
    "body": "Using:\r\n```\r\nAZURE_CLIENT_ID=...\r\nAZURE_CLIENT_SECRET=...\r\nAZURE_TENANT_ID=...\r\n```",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/99/comments",
    "author": "marcofiocco",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-10-22T14:22:39Z",
        "body": "Duplicate of #63"
      }
    ]
  },
  {
    "number": 98,
    "title": "Add support to use AWS services instead of Supabase for deployment in private environments",
    "created_at": "2024-10-22T09:28:29Z",
    "closed_at": "2024-10-24T22:14:16Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/98",
    "body": null,
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/98/comments",
    "author": "marcofiocco",
    "comments": [
      {
        "user": "bracesproul",
        "created_at": "2024-10-24T22:14:16Z",
        "body": "Hey, this is not something we're looking to implement at this time, so I'm going to be closing the issue as not planned."
      }
    ]
  },
  {
    "number": 73,
    "title": "Update runs to only ever use a single artifact",
    "created_at": "2024-10-19T22:09:34Z",
    "closed_at": "2024-10-21T23:02:33Z",
    "labels": [],
    "url": "https://github.com/langchain-ai/open-canvas/issues/73",
    "body": "At the moment, a single run can create as many artifacts as the user requests. We should update so it only ever generates a single artifact.\r\n\r\nIf we ship this before implementing thread history we don't need to worry about backwards compatibility around dealing with multiple artifacts.",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/73/comments",
    "author": "bracesproul",
    "comments": [
      {
        "user": "sid-newby",
        "created_at": "2024-10-20T05:44:15Z",
        "body": "We should update so it only ever generates a single artifact [at a time]. \r\nFor artifact saving, consider git.  Good solution if rehydrating later too. "
      },
      {
        "user": "bracesproul",
        "created_at": "2024-10-20T19:25:26Z",
        "body": "@sid-newby \r\n> We should update so it only ever generates a single artifact [at a time]. For artifact saving, consider git. Good solution if rehydrating later too.\r\n\r\nI like this idea, and yeah I think I'm going to update it today to only ever have a single artifact per conversation (this will include #21 so you can go back and fourth between your edits). However I'm probably going to keep our existing artifact storage system in place (stored in thread state, uses the LangGraph checkpointer which will persist since it's hosted on LangGraph cloud) since it doesn't require any additional setup/config work on the dev end."
      }
    ]
  },
  {
    "number": 55,
    "title": "Make markdown renderer extend all the way to the bottom",
    "created_at": "2024-10-18T23:40:06Z",
    "closed_at": "2024-10-19T20:29:14Z",
    "labels": [
      "good first issue",
      "frontend"
    ],
    "url": "https://github.com/langchain-ai/open-canvas/issues/55",
    "body": "atm there is some padding between the bottom/right side of the screen and the markdown editor. Update it to extend all the way to the bottom & left sides of the screen and remove the border on the bottom/left sides.",
    "comments_url": "https://api.github.com/repos/langchain-ai/open-canvas/issues/55/comments",
    "author": "bracesproul",
    "comments": [
      {
        "user": "ahmad2b",
        "created_at": "2024-10-19T05:57:27Z",
        "body": "I have created a pull request (#67 ) that addresses this issue by extending the markdown renderer to the bottom and left sides. Please review it when you have a chance."
      }
    ]
  }
]