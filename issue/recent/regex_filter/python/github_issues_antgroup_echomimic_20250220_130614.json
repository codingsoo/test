[
  {
    "number": 207,
    "title": "针对新版本 diffusers 和 huggingface_hub 版本兼容性是否存在问题",
    "created_at": "2024-12-18T11:45:23Z",
    "closed_at": "2024-12-27T10:50:28Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/207",
    "body": "Traceback (most recent call last):\r\n  File \"/content/drive/MyDrive/Colab Notebooks/echomimic-main/infer_audio2vid.py\", line 21, in <module>\r\n    from diffusers import AutoencoderKL, DDIMScheduler\r\n  File \"/usr/local/lib/python3.10/dist-packages/diffusers/__init__.py\", line 5, in <module>\r\n    from .utils import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/__init__.py\", line 38, in <module>\r\n    from .dynamic_modules_utils import get_class_from_dynamic_module\r\n  File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/dynamic_modules_utils.py\", line 28, in <module>\r\n    from huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info\r\nImportError: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py)",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/207/comments",
    "author": "JhonWeak777",
    "comments": [
      {
        "user": "shuqian8421",
        "created_at": "2024-12-23T16:39:30Z",
        "body": "我也遇到了这个问题。\r\n我的解决办法，供您参考：\r\n将文件/usr/local/lib/python3.10/dist-packages/diffusers/utils/dynamic_modules_utils.py的第28行注释掉或删除\r\n```\r\n# from huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info\r\n```\r\n就不会再报此项错误。"
      },
      {
        "user": "thundax-lyp",
        "created_at": "2024-12-24T02:11:45Z",
        "body": "> Traceback (most recent call last): File \"/content/drive/MyDrive/Colab Notebooks/echomimic-main/infer_audio2vid.py\", line 21, in from diffusers import AutoencoderKL, DDIMScheduler File \"/usr/local/lib/python3.10/dist-packages/diffusers/**init**.py\", line 5, in from .utils import ( File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/**init**.py\", line 38, in from .dynamic_modules_utils import get_class_from_dynamic_module File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/dynamic_modules_utils.py\", line 28, in from huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info ImportError: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/**init**.py)\r\n\r\n降 huggingface_hub 版本"
      },
      {
        "user": "AuYuHui",
        "created_at": "2025-01-09T04:15:58Z",
        "body": "> > Traceback (most recent call last): File \"/content/drive/MyDrive/Colab Notebooks/echomimic-main/infer_audio2vid.py\", line 21, in from diffusers import AutoencoderKL, DDIMScheduler File \"/usr/local/lib/python3.10/dist-packages/diffusers/**init**.py\", line 5, in from .utils import ( File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/**init**.py\", line 38, in from .dynamic_modules_utils import get_class_from_dynamic_module File \"/usr/local/lib/python3.10/dist-packages/diffusers/utils/dynamic_modules_utils.py\", line 28, in from huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info ImportError: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/**init**.py)\r\n> \r\n> 降 huggingface_hub 版本\r\n\r\n降低到什么版本？"
      }
    ]
  },
  {
    "number": 141,
    "title": "CUDA out of memory. :( no hugging face no colab ",
    "created_at": "2024-08-18T19:15:14Z",
    "closed_at": "2024-08-21T07:53:17Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/141",
    "body": "“First, thank you for sharing this wonderful repository. \r\n-How to avoid cuda out of memory many repos has an option  to set batch size how to set it in echomimic?\r\n**Could you please create a lite version for users with lower-end graphic cards?** \r\n-Additionally, Hugging Face closed the session before EchoMimic completed due to GPU time limits. \r\n-I tried using Google Colab but encountered a size limit while downloading the pretrained model, as there wasn’t enough space.”\r\n",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/141/comments",
    "author": "medalawi",
    "comments": [
      {
        "user": "nitinmukesh",
        "created_at": "2024-08-18T19:22:33Z",
        "body": "duration of audio?"
      },
      {
        "user": "medalawi",
        "created_at": "2024-08-18T19:29:17Z",
        "body": "@nitinmukesh  Cuda out of memory show in terminal before launch host  run in browser\r\nI tried huggingface with 2sec got GPU abort time limit"
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-09-21T09:31:03Z",
        "body": "I am not sure why CUDA OOM error.\r\n\r\nWhat is you PC specification? GPU / VRAM / RAM.\r\n\r\nI just now converted 56s audio on RTX 4060 8 GB VRAM, it took a lot of time but no OOM issues"
      }
    ]
  },
  {
    "number": 133,
    "title": "可以设置人物头部不晃动吗",
    "created_at": "2024-08-14T10:19:57Z",
    "closed_at": "2024-09-11T02:25:37Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/133",
    "body": "您好，请问如何设置人物头部不晃动？",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/133/comments",
    "author": "ppx0001",
    "comments": [
      {
        "user": "lymhust",
        "created_at": "2024-09-11T02:25:13Z",
        "body": "用pose的版本，录制一个只眨眼，不晃头的driver"
      }
    ]
  },
  {
    "number": 122,
    "title": "有哪位知道如何才能不裁剪",
    "created_at": "2024-08-05T23:51:26Z",
    "closed_at": "2024-09-11T02:28:15Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/122",
    "body": "有哪位知道如何才能不裁剪。现在只显示了头部",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/122/comments",
    "author": "wsy6566",
    "comments": [
      {
        "user": "JimWang151",
        "created_at": "2024-08-07T00:33:06Z",
        "body": "调整face_crop就可以。"
      },
      {
        "user": "rahuljustbaat",
        "created_at": "2024-08-14T12:01:00Z",
        "body": "set Facecrop Dilation Ratio to 1"
      }
    ]
  },
  {
    "number": 113,
    "title": "diffusers版本问题",
    "created_at": "2024-07-31T12:22:09Z",
    "closed_at": "2024-09-03T13:48:22Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/113",
    "body": "我使用 diffusers==0.26.1 报错:cannot import name ‘PositionNet‘ from ‘diffusers.models.embeddings\r\n\r\n降级到 diffusers==0.25.1 报错:cannot import name 'CaptionProjection' from 'diffusers.models.embeddings' (/root/miniconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py) \r\n\r\n我猜测可能是我其他依赖有版本问题.是否可以提供一下各个依赖包的版本",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/113/comments",
    "author": "yeohx",
    "comments": [
      {
        "user": "XingWang1234",
        "created_at": "2024-07-31T12:38:15Z",
        "body": "`requirements.txt` requires  diffusers=0.24.0, I installed this version, and it works"
      },
      {
        "user": "yeohx",
        "created_at": "2024-08-01T06:07:03Z",
        "body": "> `requirements.txt` requires diffusers=0.24.0, I installed this version, and it works\r\n\r\n请问一下,你的centos和python是什么版本的?"
      },
      {
        "user": "XingWang1234",
        "created_at": "2024-08-01T06:10:59Z",
        "body": "> > `requirements.txt` requires diffusers=0.24.0, I installed this version, and it works\r\n> \r\n> 请问一下,你的centos和python是什么版本的?\r\n\r\nmy OS is: PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\r\npython==3.10"
      }
    ]
  },
  {
    "number": 109,
    "title": "Full body talking video?",
    "created_at": "2024-07-30T08:05:32Z",
    "closed_at": "2024-09-11T02:30:55Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/109",
    "body": "\r\nThanks for your great work!  May I ask do you have any plans on something  like \"full body talking video\"  instead of talking head?",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/109/comments",
    "author": "moxiegushi",
    "comments": [
      {
        "user": "oisilener1982",
        "created_at": "2024-08-01T01:31:35Z",
        "body": "i also need this"
      },
      {
        "user": "lymhust",
        "created_at": "2024-09-11T02:30:52Z",
        "body": "next work echomimic-v2 will focus on halfbody generation."
      },
      {
        "user": "oisilener1982",
        "created_at": "2024-09-11T16:12:22Z",
        "body": "> next work echomimic-v2 will focus on halfbody generation.\r\n\r\nWill this be released this year?"
      }
    ]
  },
  {
    "number": 108,
    "title": "执行python -u infer_audio2vid.py 报错:No module named 'cv2'",
    "created_at": "2024-07-29T13:59:43Z",
    "closed_at": "2024-09-03T13:50:21Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/108",
    "body": "python -u webgui.py --server_port=3000\r\nTraceback (most recent call last):\r\n  File \"/root/EchoMimic/webgui.py\", line 12, in <module>\r\n    import cv2\r\nModuleNotFoundError: No module named 'cv2'\r\n(base) [root@iZrj9btw2sw63gaduxfvbgZ EchoMimic]# python -u infer_audio2vid.py\r\nTraceback (most recent call last):\r\n  File \"/root/EchoMimic/infer_audio2vid.py\", line 18, in <module>\r\n    import cv2\r\nModuleNotFoundError: No module named 'cv2'\r\n\r\npython 版本 3.10.0\r\ncentos7",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/108/comments",
    "author": "yeohx",
    "comments": [
      {
        "user": "yiyijiu92",
        "created_at": "2024-08-20T03:59:10Z",
        "body": "same problem +1"
      },
      {
        "user": "yeohx",
        "created_at": "2024-09-03T13:50:21Z",
        "body": "选择有显卡的服务器,依赖重新安装.注意安装显卡驱动. 注意版本"
      }
    ]
  },
  {
    "number": 100,
    "title": "Create webgui_a2v_acc.py",
    "created_at": "2024-07-27T18:08:20Z",
    "closed_at": "2024-07-28T08:36:44Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/pull/100",
    "body": "Add WebUI for audio to video accelerated model",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/100/comments",
    "author": "newgenai79",
    "comments": [
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-28T08:33:04Z",
        "body": "This can be deleted. Some more changes needed."
      }
    ]
  },
  {
    "number": 93,
    "title": "如果video_length超过face_mask_tensor的长度会崩溃",
    "created_at": "2024-07-26T09:22:38Z",
    "closed_at": "2024-07-26T14:01:11Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/93",
    "body": "infra_ audio2vid acc. py中，如果video_length超过face_mask_tensor的长度，会报错。因为在推理过程中pose_latents_cond = torch.cat([face_locator_tensor[:, :, c] for c in new_context]).to(device)的c会越界，从而导致崩溃。",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/93/comments",
    "author": "XeoOuYang",
    "comments": [
      {
        "user": "XeoOuYang",
        "created_at": "2024-07-26T09:23:32Z",
        "body": "acc与之前区别是arg.L默认1200，而原来是240的差异"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T14:01:11Z",
        "body": "多谢，这个问题在后续的版本会修复"
      }
    ]
  },
  {
    "number": 89,
    "title": "Release Dataset?",
    "created_at": "2024-07-26T06:02:37Z",
    "closed_at": "2024-07-26T14:01:35Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/89",
    "body": "Nice work! Do you have any plan to release training datasets?",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/89/comments",
    "author": "JZArray",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T14:01:35Z",
        "body": "We are considering this issue."
      }
    ]
  },
  {
    "number": 87,
    "title": "如何使用指定GPU进行推理",
    "created_at": "2024-07-25T08:43:42Z",
    "closed_at": "2024-07-26T02:47:13Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/87",
    "body": "现在是使用GPU0，如果我想用GPU2，如何设置",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/87/comments",
    "author": "ppx0001",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T02:47:13Z",
        "body": "export CUDA_VISIBLE_DEVICES=\"2\""
      }
    ]
  },
  {
    "number": 86,
    "title": "如何使用多GPU推理",
    "created_at": "2024-07-25T07:18:38Z",
    "closed_at": "2024-07-25T07:54:49Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/86",
    "body": "现在只能使用GPU0进行推理，还有多个空闲GPU",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/86/comments",
    "author": "ppx0001",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-25T07:54:47Z",
        "body": "多卡、流式输出在现行的算法框架下都是比较难改造的点。。。要实现这类工程改造需要从源头对算法框架进行重构，难度比较大，也很难保证是否能实现"
      },
      {
        "user": "FucK5t4r",
        "created_at": "2024-11-29T10:59:59Z",
        "body": "> 现在只能使用GPU0进行推理，还有多个空闲GPU\r\n\r\n可以简单在推理的时候加一个mapredece多线程手动分配卡，因为echomimic不涉及自回归推理，所以理论实现起来也不复杂\r\n"
      }
    ]
  },
  {
    "number": 85,
    "title": "发现两个问题",
    "created_at": "2024-07-24T16:23:57Z",
    "closed_at": "2024-07-25T07:55:57Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/85",
    "body": "问题1. 我合成的视频长度是1分钟,但是到50秒的时候,画面就不动了.这个哪里设置呢?\r\n问题2.  我提供的图片长宽都超过512,但是最终的视频分辨率大小是512X512,视频分辨率请问怎么调整呢?",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/85/comments",
    "author": "nicevlhjfihtqngkldl",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-25T07:55:55Z",
        "body": "1. L参数的设置问题，现在默认最大1200帧，也就是50s左右\r\n2. 现在模型只能对512有比较好的效果，所以做了强制resize，调整分辨率之后效果可能不会太好"
      },
      {
        "user": "xiao-keeplearning",
        "created_at": "2024-07-25T13:41:07Z",
        "body": "你好，问下这个L参数可以调大么，调大该参数会不会导致效果变差"
      },
      {
        "user": "ldavis9000aws",
        "created_at": "2024-08-06T06:06:24Z",
        "body": "I'd also like to know if the L parameter can be increased."
      }
    ]
  },
  {
    "number": 84,
    "title": "音频与关键点作为条件驱动问题",
    "created_at": "2024-07-24T09:12:37Z",
    "closed_at": "2024-07-26T02:41:21Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/84",
    "body": "本论文是否支持，将音频与嘴部关键点序列共同作为条件进行驱动生成视频？  考虑到音频内在的嘴部运动与提供嘴部关键点序列可能不匹配，导致合成视频的嘴部运动混乱。",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/84/comments",
    "author": "gobigrassland",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T02:41:21Z",
        "body": "如果同时输入，pose的驱动会占主导。"
      }
    ]
  },
  {
    "number": 80,
    "title": "音频驱动有加速吗？",
    "created_at": "2024-07-24T03:29:38Z",
    "closed_at": "2024-07-26T02:53:35Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/80",
    "body": null,
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/80/comments",
    "author": "yanglibo0512",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T02:53:35Z",
        "body": "已经开源了"
      },
      {
        "user": "liushh39",
        "created_at": "2024-08-16T06:24:59Z",
        "body": "怎么加速呀"
      }
    ]
  },
  {
    "number": 78,
    "title": "Accelerated Version of infer_audio2vid.py with Poor Results After Adjustments",
    "created_at": "2024-07-24T02:07:07Z",
    "closed_at": "2024-07-26T02:54:38Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/78",
    "body": "Hello,\r\n\r\nI've noticed that there is an official accelerated version for infer_audio2vid_pose.py, but not for infer_audio2vid.py. The acceleration seems to be achieved primarily by adjusting the step and CFG hyperparameters. I attempted to replicate the approach used in infer_audio2vid_pose to modify infer_audio2vid and loaded the accelerated (acc) model version. However, the results were unsatisfactory.\r\n\r\nCould you please provide some insight into why this might be happening? Are there specific considerations or additional modifications required for accelerating infer_audio2vid that are not covered by simply adjusting steps and CFG parameters?\r\n\r\nAny guidance on how to properly accelerate infer_audio2vid while maintaining good performance would be greatly appreciated.\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/78/comments",
    "author": "pia-ai",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T02:54:38Z",
        "body": "An official accelerated version for infer_audio2vid.py is released now."
      }
    ]
  },
  {
    "number": 77,
    "title": "卡会限制吗？",
    "created_at": "2024-07-24T02:05:06Z",
    "closed_at": "2024-07-25T09:12:09Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/77",
    "body": "目前手头只有一个3090的英伟达的卡，看着需要的配置挺高的，有稍微低配的版本吗",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/77/comments",
    "author": "PengChaoJay",
    "comments": [
      {
        "user": "oisilener1982",
        "created_at": "2024-07-25T00:01:50Z",
        "body": "Echomimic runs fine with my RTX 3080"
      },
      {
        "user": "PengChaoJay",
        "created_at": "2024-07-25T09:12:04Z",
        "body": "ok，我知道啦，我回头试试"
      }
    ]
  },
  {
    "number": 74,
    "title": "生成的视频清晰度问题",
    "created_at": "2024-07-23T06:02:17Z",
    "closed_at": "2024-07-26T03:13:17Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/74",
    "body": "上传一张非面部局部图片，最终生成的视频清晰度很差，请问下需要调整什么参数控制一下",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/74/comments",
    "author": "sunjinguo",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:13:17Z",
        "body": "可以使用最新的脚本，增加了face crop，清晰度会有改善。"
      }
    ]
  },
  {
    "number": 73,
    "title": "执行demo脚本报错RuntimeError: Failed to import transformers.models.clip.image_processing_clip",
    "created_at": "2024-07-22T15:06:58Z",
    "closed_at": "2024-07-26T03:08:18Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/73",
    "body": "(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic> python -u infer_audio2vid.py     \r\nC:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nC:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1567, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\models\\clip\\image_processing_clip.py\", line 21, in <module>\r\n    from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\image_processing_utils.py\", line 21, in <module>\r\n    from .image_transforms import center_crop, normalize, rescale\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\image_transforms.py\", line 22, in <module>\r\n    from .image_utils import (\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\image_utils.py\", line 58, in <module>\r\n    from torchvision.transforms import InterpolationMode\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torchvision\\__init__.py\", line 6, in <module>\r\n    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torchvision\\datasets\\__init__.py\", line 1, in <module>\r\n    from ._optical_flow import FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torchvision\\datasets\\_optical_flow.py\", line 12, in <module>\r\n    from ..io.image import _read_png_16\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torchvision\\io\\__init__.py\", line 36, in <module>\r\n    from .video import read_video, read_video_timestamps, write_video\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torchvision\\io\\video.py\", line 16, in <module>\r\n    import av\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\av\\__init__.py\", line 23, in <module>\r\n    _delvewheel_patch_1_5_1()\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\av\\__init__.py\", line 20, in _delvewheel_patch_1_5_1\r\n    raise OSError('Error loading {}; {}'.format(lib, ctypes.FormatError()))\r\nOSError: Error loading libogg-0-f34e1352e804ae274f494f49de171881.dll; 操作成功完成。\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"infer_audio2vid.py\", line 28, in <module>\r\n    from src.pipelines.pipeline_echo_mimic import Audio2VideoPipeline\r\n  File \"E:\\code\\Live_Portrait\\EchoMimic\\src\\pipelines\\pipeline_echo_mimic.py\", line 32, in <module>\r\n    from transformers import CLIPImageProcessor\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1558, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1557, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1569, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.clip.image_processing_clip because of the following error (look up to see its traceback):\r\nError loading libogg-0-f34e1352e804ae274f494f49de171881.dll; 操作成功完成。",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/73/comments",
    "author": "seekme-ai",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:08:18Z",
        "body": "检查下库是不是成功安装了"
      }
    ]
  },
  {
    "number": 72,
    "title": "Is it possible to batch process target mask .pkl files to reduce VRAM usage?",
    "created_at": "2024-07-22T15:04:31Z",
    "closed_at": "2024-07-26T03:12:40Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/72",
    "body": "I am using a driver video with ~2000 frames. Currently in `infer_audio2vid_pose_acc.py` all 2000 .pkl files in pose_dir are loaded at once and when `face_locator` is run I get a `torch.cuda.OutOfMemoryError: CUDA out of memory`. Is it possible to batch process the .pkl files and concatenate the resulting videos to reduce VRAM requirement for large videos?\r\n\r\n``` infer_audio2vid_pose_acc.py\r\nfor index in range(len(os.listdir(pose_dir))):\r\n    tgt_musk_path = os.path.join(pose_dir, f\"{index}.pkl\")\r\n\r\n    with open(tgt_musk_path, \"rb\") as f:\r\n        tgt_kpts = pickle.load(f)\r\n    tgt_musk = visualizer.draw_landmarks((args.W, args.H), tgt_kpts)\r\n    tgt_musk_pil = Image.fromarray(np.array(tgt_musk).astype(np.uint8)).convert('RGB')\r\n    pose_list.append(torch.Tensor(np.array(tgt_musk_pil)).to(dtype=weight_dtype, device=\"cuda\").permute(2,0,1) / 255.0)\r\nface_mask_tensor = torch.stack(pose_list, dim=1).unsqueeze(0)\r\n\r\n\r\nvideo = pipe(\r\n    ref_image_pil,\r\n    audio_path,\r\n    face_mask_tensor,\r\n    width,\r\n    height,\r\n    args.L,\r\n    args.steps,\r\n    args.cfg,\r\n    generator=generator,\r\n    audio_sample_rate=args.sample_rate,\r\n    context_frames=12,\r\n    fps=final_fps,\r\n    context_overlap=3\r\n).videos\r\n\r\nfinal_length = min(video.shape[2], face_mask_tensor.shape[2])\r\nvideo = torch.cat([video[:, :, :final_length, :, :], face_mask_tensor[:, :, :final_length, :, :].detach().cpu()], dim=-1)\r\n```\r\n\r\n``` pipeline_echo_mimic_pose_acc.py\r\nface_locator_tensor = self.face_locator(face_mask_tensor)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/72/comments",
    "author": "Sidd065",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:12:41Z",
        "body": "It is possible to do that. I can give you some suggestions.\r\n(1) Split the 2000 frames to small subsets, for instance 250 frames per subset.\r\n(2) initialize variables subset by subset, such as landmarks, latents and audios.\r\n(3) call pipe on each subset.\r\n(4) concat the results as the final result."
      }
    ]
  },
  {
    "number": 71,
    "title": "Poor LipSync Quality at lower resolution.",
    "created_at": "2024-07-22T10:38:26Z",
    "closed_at": "2024-07-23T05:42:57Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/71",
    "body": "If i decrease the resolution to 256x256, there is no lip movement. I want to run at lower resolution for faster inference.",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/71/comments",
    "author": "345ishaan",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-23T05:42:56Z",
        "body": "we only train the model on 512*512, so it works bad on low resolution :("
      }
    ]
  },
  {
    "number": 69,
    "title": "麻烦问下，微调代码有时间计划开源么？感觉使用新的非训练的图片，效果没那么好。",
    "created_at": "2024-07-22T06:13:01Z",
    "closed_at": "2024-07-26T03:06:02Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/69",
    "body": "如题",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/69/comments",
    "author": "myhostone1990",
    "comments": [
      {
        "user": "nssai001",
        "created_at": "2024-07-22T08:17:54Z",
        "body": "我用的图片做成的视频，总是感觉视频中的人物与原图不是太像是什么原因"
      },
      {
        "user": "myhostone1990",
        "created_at": "2024-07-23T08:44:11Z",
        "body": "> 我用的图片做成的视频，总是感觉视频中的人物与原图不是太像是什么原因\r\n\r\n确实，我也有这种问题。能微调应该就会好很多。"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:06:02Z",
        "body": "训练代码的开源需要看后续中稿情况了～感谢关注。"
      }
    ]
  },
  {
    "number": 68,
    "title": "请问pytorch版本什么要求，安装后报错",
    "created_at": "2024-07-21T16:34:38Z",
    "closed_at": "2024-07-22T15:05:55Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/68",
    "body": "(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic> python -c \"import torch; print(torch.__version__)\"\r\n2.2.2+cpu\r\n(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic> \r\n(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic>\r\n(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic>\r\n(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic>\r\n(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic>\r\n(echomimic) PS E:\\code\\Live_Portrait\\EchoMimic>   python -u infer_audio2vid.py                                                             \r\nC:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nC:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nC:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nCannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with:\r\n```\r\npip install accelerate\r\n```\r\n.\r\nTraceback (most recent call last):\r\n  File \"infer_audio2vid.py\", line 257, in <module>\r\n    main()\r\n  File \"infer_audio2vid.py\", line 111, in main\r\n    vae = AutoencoderKL.from_pretrained(\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1152, in to\r\n    return self._apply(convert)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 802, in _apply\r\n    module._apply(fn)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 802, in _apply\r\n    module._apply(fn)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 825, in _apply\r\n    param_applied = fn(param)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1150, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n  File \"C:\\Users\\seekm\\anaconda3\\envs\\echomimic\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 293, in _lazy_init\r\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\nAssertionError: Torch not compiled with CUDA enabled\r\n",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/68/comments",
    "author": "seekme-ai",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-22T02:17:34Z",
        "body": "这个貌似跟版本没关系，跟安装torch时的cuda环境变量关系可能更大"
      },
      {
        "user": "seekme-ai",
        "created_at": "2024-07-22T09:36:22Z",
        "body": "请问有修改建议么，cuda是之前安装好的12.4版本，环境变量没有做额外配置"
      },
      {
        "user": "seekme-ai",
        "created_at": "2024-07-22T15:06:11Z",
        "body": "cuda没安装好导致的，已解决"
      }
    ]
  },
  {
    "number": 66,
    "title": "测试了最新的更新，项目说速度提升10X以上，测试结果，基本上无任何提升",
    "created_at": "2024-07-21T03:36:40Z",
    "closed_at": "2024-07-22T07:00:02Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/66",
    "body": "测试了最新的更新，项目说速度提升10X以上，测试结果，基本上无任何提升，大家有测试结果吗？",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/66/comments",
    "author": "tgdtu",
    "comments": [
      {
        "user": "lymhust",
        "created_at": "2024-07-21T03:39:49Z",
        "body": "测试代码发出来看一下，或者加微信群进群讨论一下。"
      },
      {
        "user": "tgdtu",
        "created_at": "2024-07-21T03:40:18Z",
        "body": "邮件已收到，谢谢！冶专涂国栋。"
      },
      {
        "user": "tgdtu",
        "created_at": "2024-07-21T03:44:48Z",
        "body": "@lymhust 群怎么加？\r\n"
      },
      {
        "user": "lymhust",
        "created_at": "2024-07-21T03:47:28Z",
        "body": "Git readme主页，有个微信图标，点开，扫右边小助手的二维码，让他拉你进讨论群。"
      },
      {
        "user": "tgdtu",
        "created_at": "2024-07-21T04:17:28Z",
        "body": "@lymhust 加了，要等统一处理，谢谢\r\n"
      }
    ]
  },
  {
    "number": 63,
    "title": "如何平衡latent上的mse loss 和 pixel空间的spatial loss",
    "created_at": "2024-07-19T14:28:40Z",
    "closed_at": "2024-07-26T03:07:06Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/63",
    "body": "论文提到了一个权重, 关于这两部分的loss权重, 想咨询下是否有经验可以借鉴.  另外,在pixel空间的spatial loss, 在vgg perception loss 和 L2 loss的权重是否有相关的配置.",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/63/comments",
    "author": "XuankeShi",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:07:06Z",
        "body": "这个我们后续会根据paper投稿情况给出详细细节。"
      }
    ]
  },
  {
    "number": 62,
    "title": "Audio driven acceleration not possible?",
    "created_at": "2024-07-19T07:52:19Z",
    "closed_at": "2024-07-26T03:19:04Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/62",
    "body": "Hey, I tried to run accelerated models, it did not run for pose script, my installations are recent. But I am eager to run it for audio driven inference. Can you tell us by when would it be released? ",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/62/comments",
    "author": "kashishnaqvi101",
    "comments": [
      {
        "user": "theZaX",
        "created_at": "2024-07-21T00:58:36Z",
        "body": "Same this would be excellent for us"
      },
      {
        "user": "oisilener1982",
        "created_at": "2024-07-21T22:39:03Z",
        "body": "I also need this one. Hope devs would be able to accelerate Audio Driven"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:19:04Z",
        "body": "Audio accelerated models and pipes are released. Please try it."
      }
    ]
  },
  {
    "number": 59,
    "title": "Motion Sync not working",
    "created_at": "2024-07-17T15:54:01Z",
    "closed_at": "2024-07-26T03:15:43Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/59",
    "body": "Windows 10, conda. I tried audio2video script before and it worked. Now that I updated your program to try \"motion sync\" nothing happen. I tried multiple times the motion sync script:\r\n```\r\n(echomimic) S:\\AIprograms\\EchoMimic>  python -u demo_motion_sync.py\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1721230965.082872   31692 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n288\r\n```\r\nin my root folder EchoMimic created folder named \"d\" it's the same name as source image. I can see some \".pkl\" files but that's it. Overall there is 288 pkl files in \"d\" folder. So it matches the numer in conda \"288\". But still it didn't rendered video.\r\n\r\nThis is my pip list:\r\n```\r\nPackage                   Version\r\n------------------------- ------------\r\nabsl-py                   2.1.0\r\naccelerate                0.32.1\r\naiofiles                  23.2.1\r\naltair                    5.3.0\r\nannotated-types           0.7.0\r\nantlr4-python3-runtime    4.9.3\r\nanyio                     4.4.0\r\nasttokens                 2.4.1\r\nattrs                     23.2.0\r\nav                        11.0.0\r\nbackcall                  0.2.0\r\nBrotli                    1.0.9\r\ncertifi                   2024.7.4\r\ncffi                      1.16.0\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncolorama                  0.4.6\r\ncontourpy                 1.1.1\r\ncycler                    0.12.1\r\ndecorator                 4.4.2\r\ndiffusers                 0.24.0\r\ndnspython                 2.6.1\r\neinops                    0.4.1\r\nemail_validator           2.2.0\r\nexceptiongroup            1.2.2\r\nexecuting                 2.0.1\r\nfacenet-pytorch           2.5.0\r\nfastapi                   0.111.1\r\nfastapi-cli               0.0.4\r\nffmpeg-python             0.2.0\r\nffmpy                     0.3.2\r\nfilelock                  3.13.1\r\nflatbuffers               24.3.25\r\nfonttools                 4.53.1\r\nfsspec                    2024.6.1\r\nfuture                    1.0.0\r\ngmpy2                     2.1.2\r\ngradio                    4.38.1\r\ngradio_client             1.1.0\r\nh11                       0.14.0\r\nhttpcore                  1.0.5\r\nhttptools                 0.6.1\r\nhttpx                     0.27.0\r\nhuggingface-hub           0.23.4\r\nidna                      3.7\r\nimageio                   2.34.2\r\nimageio-ffmpeg            0.5.1\r\nimportlib_metadata        8.0.0\r\nimportlib_resources       6.4.0\r\nintel-openmp              2021.4.0\r\nipython                   8.12.3\r\njax                       0.4.13\r\njedi                      0.19.1\r\nJinja2                    3.1.4\r\njsonschema                4.23.0\r\njsonschema-specifications 2023.12.1\r\nkiwisolver                1.4.5\r\nlazy_loader               0.4\r\nlightning-utilities       0.11.3.post0\r\nmarkdown-it-py            3.0.0\r\nMarkupSafe                2.1.3\r\nmatplotlib                3.7.5\r\nmatplotlib-inline         0.1.7\r\nmdurl                     0.1.2\r\nmediapipe                 0.10.11\r\nmkl                       2021.4.0\r\nmkl-fft                   1.3.8\r\nmkl-random                1.2.4\r\nmkl-service               2.4.0\r\nml-dtypes                 0.2.0\r\nmoviepy                   1.0.3\r\nmpmath                    1.3.0\r\nnetworkx                  3.1\r\nnumpy                     1.24.3\r\nomegaconf                 2.3.0\r\nopencv-contrib-python     4.10.0.84\r\nopencv-python             4.10.0.84\r\nopt-einsum                3.3.0\r\norjson                    3.10.6\r\npackaging                 24.1\r\npandas                    2.0.3\r\nparso                     0.8.4\r\npickleshare               0.7.5\r\npillow                    10.3.0\r\npip                       24.0\r\npkgutil_resolve_name      1.3.10\r\nproglog                   0.1.10\r\nprompt_toolkit            3.0.47\r\nprotobuf                  3.20.3\r\npsutil                    6.0.0\r\npure-eval                 0.2.2\r\npycparser                 2.22\r\npydantic                  2.8.2\r\npydantic_core             2.20.1\r\npydub                     0.25.1\r\nPygments                  2.18.0\r\npyparsing                 3.1.2\r\nPySocks                   1.7.1\r\npython-dateutil           2.9.0.post0\r\npython-dotenv             1.0.1\r\npython-multipart          0.0.9\r\npytz                      2024.1\r\nPyWavelets                1.4.1\r\nPyYAML                    6.0.1\r\nreferencing               0.35.1\r\nregex                     2024.5.15\r\nrequests                  2.32.2\r\nrich                      13.7.1\r\nrpds-py                   0.19.0\r\nruff                      0.5.2\r\nsafetensors               0.4.3\r\nscikit-image              0.21.0\r\nscipy                     1.10.1\r\nsemantic-version          2.10.0\r\nsetuptools                69.5.1\r\nshellingham               1.5.4\r\nsix                       1.16.0\r\nsniffio                   1.3.1\r\nsounddevice               0.4.7\r\nstack-data                0.6.3\r\nstarlette                 0.37.2\r\nsympy                     1.12\r\ntbb                       2021.13.0\r\ntifffile                  2023.7.10\r\ntokenizers                0.19.1\r\ntomlkit                   0.12.0\r\ntoolz                     0.12.1\r\ntorch                     2.0.1\r\ntorchaudio                2.0.2\r\ntorchmetrics              1.4.0.post0\r\ntorchtyping               0.1.4\r\ntorchvision               0.15.2\r\ntqdm                      4.66.4\r\ntraitlets                 5.14.3\r\ntransformers              4.42.3\r\ntypeguard                 4.3.0\r\ntyper                     0.12.3\r\ntyping_extensions         4.11.0\r\ntzdata                    2024.1\r\nurllib3                   2.2.2\r\nuvicorn                   0.30.1\r\nwatchfiles                0.22.0\r\nwcwidth                   0.2.13\r\nwebsockets                11.0.3\r\nwheel                     0.43.0\r\nwin-inet-pton             1.1.0\r\nzipp                      3.19.2\r\n```",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/59/comments",
    "author": "A-2-H",
    "comments": [
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-18T09:18:45Z",
        "body": "It seems the problem is in \r\nEchoMimic\\src\\pipelines\\pipeline_echo_mimic_pose_acc.py\r\n\r\n```\r\n@torch.no_grad()\r\n    def __call__(\r\n\r\n..................\r\n\r\nprint(\"23: with self.progress_bar\")\r\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\r\n            print(f\"23.1: total:{total}, num_inference_steps:{num_inference_steps}\")\r\n\r\n```\r\n\r\nOutput\r\n\r\n```\r\n(echomimic) C:\\tut\\EchoMimic>python -u infer_audio2vid_pose_acc.py\r\nC:\\Users\\nitin\\miniconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nC:\\Users\\nitin\\miniconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\n\r\n23: with self.progress_bar\r\n\r\n(echomimic) C:\\tut\\EchoMimic>\r\n```\r\n\r\nIt is not going inside **with** statement.\r\n\r\nIf developers can help how to debug I am ready to help."
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-18T09:36:56Z",
        "body": "Some more logs. Not going inside with statement\r\n```\r\n\r\n(echomimic) C:\\tut\\EchoMimic>python -u infer_audio2vid_pose_acc.py\r\nC:\\Users\\nitin\\miniconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\nC:\\Users\\nitin\\miniconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\r\n  torch.utils._pytree._register_pytree_node(\r\n1: Initializing pipeline\r\n2: Pipeline initialized\r\n7: Getting execution device\r\nvideo in 24 FPS, audio idx in 50FPS\r\n15: Preparing latents\r\n17: latents shape:torch.Size([1, 4, 160, 64, 64]), video_length:160\r\n20: face_locator_tensor\r\n10: Preparing extra step kwargs\r\n11: Extra step kwargs prepared\r\n21: extra_step_kwargs\r\n22: denoising loop\r\n23: with self.progress_bar\r\nref_image_latents shape: torch.Size([1, 4, 64, 64])\r\nface_mask_tensor shape: torch.Size([1, 3, 240, 512, 512])\r\nface_locator_tensor shape: torch.Size([1, 320, 240, 64, 64])\r\nself.progress_bar: <bound method DiffusionPipeline.progress_bar of AudioPose2VideoPipeline {\r\n  \"_class_name\": \"AudioPose2VideoPipeline\",\r\n  \"_diffusers_version\": \"0.24.0\",\r\n  \"audio_guider\": [\r\n    \"src.models.whisper.audio2feature\",\r\n    \"Audio2Feature\"\r\n  ],\r\n  \"denoising_unet\": [\r\n    \"src.models.unet_3d_echo\",\r\n    \"EchoUNet3DConditionModel\"\r\n  ],\r\n  \"face_locator\": [\r\n    \"src.models.face_locator\",\r\n    \"FaceLocator\"\r\n  ],\r\n  \"image_proj_model\": [\r\n    null,\r\n    null\r\n  ],\r\n  \"reference_unet\": [\r\n    \"src.models.unet_2d_condition\",\r\n    \"UNet2DConditionModel\"\r\n  ],\r\n  \"scheduler\": [\r\n    \"diffusers\",\r\n    \"DDIMScheduler\"\r\n  ],\r\n  \"text_encoder\": [\r\n    null,\r\n    null\r\n  ],\r\n  \"tokenizer\": [\r\n    null,\r\n    null\r\n  ],\r\n  \"vae\": [\r\n    \"diffusers\",\r\n    \"AutoencoderKL\"\r\n  ]\r\n}\r\n>\r\n\r\n(echomimic) C:\\tut\\EchoMimic>\r\n```\r\n\r\n\r\nCode\r\n```\r\n\r\n        print(\"23: with self.progress_bar\")\r\n        print(\"ref_image_latents shape:\", ref_image_latents.shape)\r\n        print(\"face_mask_tensor shape:\", face_mask_tensor.shape)\r\n        print(\"face_locator_tensor shape:\", face_locator_tensor.shape)\r\n        print(\"self.progress_bar:\", self.progress_bar)\r\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\r\n            print(\"Inside with statement\")\r\n\r\n```"
      },
      {
        "user": "DoItEric",
        "created_at": "2024-07-20T03:54:03Z",
        "body": "maybe Motion Sync just for help people extract motion pkl file from video?\r\nyou'll get new dir to save pkl files with run this script.\r\nand you can run audio2vid_pose to sync it well , right?"
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-20T17:36:16Z",
        "body": "> maybe Motion Sync just for help people extract motion pkl file from video? you'll get new dir to save pkl files with run this script. and you can run audio2vid_pose to sync it well , right?\r\n\r\nWe are just trying to inference sample provided in this repo which is not working.\r\n\r\nCreating pickle using our own video is working fine."
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:15:43Z",
        "body": "motion sync only provides pkl files for each frame. It is a pre-process for the driven video (if you have your own driven video and ref image, you should run it before calling infer to generate video).\r\nNow, try the new script released. Motion sync is done online in the infer process. No need to run it individually. "
      }
    ]
  },
  {
    "number": 58,
    "title": "video 2 video ",
    "created_at": "2024-07-17T10:38:53Z",
    "closed_at": "2024-07-17T14:38:53Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/58",
    "body": "is it possible to give video as an input ?\r\n",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/58/comments",
    "author": "prajwal3232",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-17T14:38:53Z",
        "body": "I assume that the inputs are (image, audio, driven-video). The pipeline is under development and will be released soon."
      }
    ]
  },
  {
    "number": 56,
    "title": "About FID/FVD metric",
    "created_at": "2024-07-17T02:30:52Z",
    "closed_at": "2024-07-17T04:50:16Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/56",
    "body": "Thanks for your great work!\r\nI have a question about the model evaluation. Many related works report FID/FVD metrics for their models, but they often lack specific details about the evaluation process. I'd like to know whether you calculated FID/FVD on the training split or the test split, and how many images/videos you generated when computing these metrics. If you could provide more specific evaluation details, it would be helpful for fair comparisons between models.",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/56/comments",
    "author": "Vincent-luo",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-17T04:50:16Z",
        "body": "We will consider adding the details to our paper. Thanks for your advise."
      }
    ]
  },
  {
    "number": 55,
    "title": "why can inference without landmark",
    "created_at": "2024-07-16T14:06:38Z",
    "closed_at": "2024-07-16T15:10:01Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/55",
    "body": "Thanks for you great work. I found that the training is with random landmark input, but in inference, it can only input audio. can you introduce how to acchieve without degard result.",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/55/comments",
    "author": "renrenzsbbb",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-16T15:10:01Z",
        "body": "Thank you for the interest. During training, the pose is randomly dropped, which leads to some audio-only cases. It is the reason why it works with only audio during inference."
      }
    ]
  },
  {
    "number": 53,
    "title": "能否提供多卡推理",
    "created_at": "2024-07-16T08:15:05Z",
    "closed_at": "2024-07-16T15:10:36Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/53",
    "body": "能否提供自适应多卡推理功能，我在四卡机器上运行，只使用了第一张显卡，速度不够理想",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/53/comments",
    "author": "jinghong-6",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-16T11:04:45Z",
        "body": "抱歉这个与现有算法基础有冲突，改造量较大，短期内可能很难支持"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-16T15:10:36Z",
        "body": "加速的模型马上就要发布了，速度会有明显提升，预计加速10倍"
      }
    ]
  },
  {
    "number": 52,
    "title": "能不能提供提供一个驱动完整图片的功能？或者提供参数只驱动头部区域运动，现在512*512的边缘会运动，使用者无法合成原大小视频",
    "created_at": "2024-07-16T07:31:55Z",
    "closed_at": "2024-07-26T03:16:21Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/52",
    "body": "能不能提供提供一个驱动完整图片的功能？或者提供参数只驱动头部区域运动，现在512*512的边缘会运动，使用者无法合回原大小的视频，无法对齐，会有明显分隔",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/52/comments",
    "author": "sunbin728",
    "comments": [
      {
        "user": "Galleons2029",
        "created_at": "2024-07-18T07:08:36Z",
        "body": "同问，调整过多组参数仍然没有demo里的那种效果  貌似是因为面部锁定的缘故？"
      },
      {
        "user": "cindylaii",
        "created_at": "2024-07-21T03:42:53Z",
        "body": "同問。臉的效果非常好，但是跑完切到只剩下臉，不符合需求。\r\n有什麼方法能連身體也一起產出到video裡？"
      },
      {
        "user": "cindylaii",
        "created_at": "2024-07-21T06:01:15Z",
        "body": "> 同問。臉的效果非常好，但是跑完切到只剩下臉，不符合需求。 有什麼方法能連身體也一起產出到video裡？\r\n\r\n自問自答：Facecrop Dilation Ratio設成1，出來的效果比較好一些，至少沒有裁得很小"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-26T03:16:21Z",
        "body": "这个已经在开发了～有较好的结果我们会放出来～"
      }
    ]
  },
  {
    "number": 50,
    "title": "add pose in webgui",
    "created_at": "2024-07-16T02:45:13Z",
    "closed_at": "2024-07-16T07:24:07Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/pull/50",
    "body": null,
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/50/comments",
    "author": "Robin021",
    "comments": [
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-16T07:25:19Z",
        "body": "@Robin021 \r\n\r\nThanks for your contribution.\r\n\r\nPlease could you update\r\n\r\ndefault='0.0.0.0'\r\n\r\nTO\r\n\r\ndefault='127.0.0.1'"
      }
    ]
  },
  {
    "number": 42,
    "title": "这个项目目前最多能推理多长时间的视频呢？ ",
    "created_at": "2024-07-15T12:38:15Z",
    "closed_at": "2024-07-15T13:50:05Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/42",
    "body": null,
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/42/comments",
    "author": "beibidesr",
    "comments": [
      {
        "user": "oisilener1982",
        "created_at": "2024-07-15T13:17:23Z",
        "body": "issue #33 \r\n"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-15T13:50:05Z",
        "body": "If u use gradio, you can set the length according to issue33. Or you adjust the args.L in the scripts. The length equals args.L / args.fps."
      }
    ]
  },
  {
    "number": 33,
    "title": "The image stops moving after 48 seconds",
    "created_at": "2024-07-12T18:22:52Z",
    "closed_at": "2024-07-13T07:19:16Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/33",
    "body": "Is the maximum length only 48 seconds? If it exceeds 48 seconds, the image will stop moving.",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/33/comments",
    "author": "TryingToDoBetter25",
    "comments": [
      {
        "user": "oisilener1982",
        "created_at": "2024-07-13T03:15:00Z",
        "body": "Not sure if this will solve your problem but you can try to use gradio and adjust the length"
      },
      {
        "user": "TryingToDoBetter25",
        "created_at": "2024-07-13T07:19:11Z",
        "body": "> Not sure if this will solve your problem but you can try to use gradio and adjust the length\r\n\r\nIt's working,thanks a lot"
      }
    ]
  },
  {
    "number": 32,
    "title": "\"OpenSSL appears to be unavailable\" Error",
    "created_at": "2024-07-12T16:38:14Z",
    "closed_at": "2024-07-14T04:54:42Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/32",
    "body": "After applying the command\r\n`conda create -n echomimic python=3.8`\r\n\r\nI get the following error in the command prompt window:\r\n```\r\nCollecting package metadata (current_repodata.json): failed\r\n\r\nCondaSSLError: OpenSSL appears to be unavailable on this machine. OpenSSL is required to\r\ndownload and install packages.\r\n\r\nException: HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/main/win-64/current_repodata.json (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))\r\n```\r\n\r\nAny idea how to solve this please?",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/32/comments",
    "author": "TomiTom1234",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-13T13:27:38Z",
        "body": "The computer is not accessed to anaconda, maybe you can change the source of conda"
      }
    ]
  },
  {
    "number": 30,
    "title": "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: ",
    "created_at": "2024-07-12T14:36:16Z",
    "closed_at": "2024-07-14T04:54:15Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/30",
    "body": null,
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/30/comments",
    "author": "BoragoCode",
    "comments": [
      {
        "user": "stella0909",
        "created_at": "2024-07-13T13:18:55Z",
        "body": "同样的问题但是不影响使用。"
      },
      {
        "user": "yuange250",
        "created_at": "2024-07-13T13:41:03Z",
        "body": "嗯嗯，正常的，因为referencenet没有用到正常Unet2D的全部参数"
      }
    ]
  },
  {
    "number": 25,
    "title": "能否支持全身图片推理，和视频推理 这样就很厉害了",
    "created_at": "2024-07-12T01:47:04Z",
    "closed_at": "2024-07-12T09:22:05Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/25",
    "body": null,
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/25/comments",
    "author": "anstonjie",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-12T09:14:43Z",
        "body": "这个还是很难做的，最近有新工作比如mimicmotion在video2video上有了比较好的进步，还能看到一点希望，全身的audio2video还是有点难的"
      }
    ]
  },
  {
    "number": 24,
    "title": "supports Russian speech ",
    "created_at": "2024-07-11T15:48:36Z",
    "closed_at": "2024-07-14T04:54:25Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/24",
    "body": "Hello and thanks for this wonderful work\r\n\r\nPlease tell me, before installing I would like to know if this works with Russian voice acting?\r\n\r\nPS: sorry for my english",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/24/comments",
    "author": "enjoykian",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-12T09:17:38Z",
        "body": "We indeed have not trained our model with Russian data, but algorithms like lipsync generally have strong cross-language generalization capabilities. For instance, while the performance on Japanese may not be as good as on Chinese or English, it can still achieve certain effects, so it's worth giving it a try. And your English is quite good :)"
      }
    ]
  },
  {
    "number": 21,
    "title": "How can I solve this error?",
    "created_at": "2024-07-11T10:43:31Z",
    "closed_at": "2024-07-12T09:21:19Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/21",
    "body": "Traceback (most recent call last):\r\n  File \"/home/tom/fssd/EchoMimic/infer_audio2vid.py\", line 23, in <module>\r\n    from src.models.unet_2d_condition import UNet2DConditionModel\r\n  File \"/home/tom/fssd/EchoMimic/src/models/unet_2d_condition.py\", line 18, in <module>\r\n    from diffusers.models.embeddings import (\r\nImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings' (/opt/conda/lib/python3.10/site-packages/diffusers/models/embeddings.py)",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/21/comments",
    "author": "z497947607",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T10:48:37Z",
        "body": "pip install diffusers==0.24.0"
      },
      {
        "user": "z497947607",
        "created_at": "2024-07-11T10:53:16Z",
        "body": "谢谢，但是又出现了ffmpeg的报错，是因为我放的路径不对吗\r\nTraceback (most recent call last):\r\n  File \"/home/tom/fssd/EchoMimic/infer_audio2vid.py\", line 25, in <module>\r\n    from src.models.whisper.audio2feature import load_audio_model\r\n  File \"/home/tom/fssd/EchoMimic/src/models/whisper/audio2feature.py\", line 2, in <module>\r\n    from .whisper import load_model\r\n  File \"/home/tom/fssd/EchoMimic/src/models/whisper/whisper/__init__.py\", line 11, in <module>\r\n    from .audio import load_audio, log_mel_spectrogram, pad_or_trim\r\n  File \"/home/tom/fssd/EchoMimic/src/models/whisper/whisper/audio.py\", line 5, in <module>\r\n    import ffmpeg\r\nModuleNotFoundError: No module named 'ffmpeg'"
      },
      {
        "user": "yuange250",
        "created_at": "2024-07-11T10:55:09Z",
        "body": "没事，直接把infer_audio2vid.py里的\r\nffmpeg_path = os.getenv('FFMPEG_PATH')\r\n改成你放的目录吧"
      },
      {
        "user": "yuange250",
        "created_at": "2024-07-11T10:56:03Z",
        "body": "注意有时候tar解压文件夹容易多解压出一层文件夹"
      },
      {
        "user": "z497947607",
        "created_at": "2024-07-11T11:04:54Z",
        "body": "谢谢，已经解决了，原来是压缩了两遍，第一次解压出来是个tar压缩包，还需要在解压一次"
      }
    ]
  },
  {
    "number": 20,
    "title": "运行    python -u infer_audio2vid.py 出现错误 ImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings' (",
    "created_at": "2024-07-11T10:41:31Z",
    "closed_at": "2024-07-14T04:53:30Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/20",
    "body": "Traceback (most recent call last):\r\n  File \"E:\\ai3\\EchoMimic\\infer_audio2vid.py\", line 23, in <module>\r\n    from src.models.unet_2d_condition import UNet2DConditionModel\r\n  File \"E:\\ai3\\EchoMimic\\src\\models\\unet_2d_condition.py\", line 18, in <module>\r\n    from diffusers.models.embeddings import (\r\nImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings' (E:\\ai3\\EchoMimic\\venv\\Lib\\site-packages\\diffusers\\models\\embeddings.py)\r\n",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/20/comments",
    "author": "anstonjie",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T10:42:17Z",
        "body": "pip install diffusers==0.24.0"
      },
      {
        "user": "anstonjie",
        "created_at": "2024-07-11T11:24:18Z",
        "body": "thank you"
      }
    ]
  },
  {
    "number": 19,
    "title": "Is there anyone who set up the enviroment successfully?",
    "created_at": "2024-07-11T10:05:58Z",
    "closed_at": "2024-07-12T07:20:11Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/19",
    "body": "```\r\nconda create -n echomimic python=3.8\r\nconda activate echomimic\r\npip install -r requirements.txt\r\n```\r\nThe above code does not works well for me, and the following errors is given to me,  anyone can help me, or give a docker img to others. \r\n```\r\n\r\nraceback (most recent call last):\r\n  File \"/ssdcache/jtmeng/miniconda3/envs/echomimic/lib/python3.8/site-packages/diffusers/utils/import_utils.py\", line 808, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/ssdcache/jtmeng/miniconda3/envs/echomimic/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/ssdcache/jtmeng/miniconda3/envs/echomimic/lib/python3.8/site-packages/diffusers/models/autoencoders/__init__.py\", line 1, in <module>\r\n    from .autoencoder_asym_kl import AsymmetricAutoencoderKL\r\n  File \"/ssdcache/jtmeng/miniconda3/envs/echomimic/lib/python3.8/site-packages/diffusers/models/autoencoders/autoencoder_asym_kl.py\", line 21, in <module>\r\n    from ..modeling_outputs import AutoencoderKLOutput\r\n  File \"/ssdcache/jtmeng/miniconda3/envs/echomimic/lib/python3.8/site-packages/diffusers/models/modeling_outputs.py\", line 7, in <module>\r\n    class AutoencoderKLOutput(BaseOutput):\r\n  File \"/ssdcache/jtmeng/miniconda3/envs/echomimic/lib/python3.8/site-packages/diffusers/utils/outputs.py\", line 61, in __init_subclass__\r\n    import torch.utils._pytree\r\nModuleNotFoundError: No module named 'torch.utils._pytree'\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/19/comments",
    "author": "Mengjintao",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T10:15:52Z",
        "body": "这种一般是conda已经有了现成的torch和diffusers版本，跟新装的包不适配，可以试试重装下：\r\npip install torch==2.0.1\r\npip install torchvision==0.15.2\r\npip install diffusers==0.24.0"
      },
      {
        "user": "Mengjintao",
        "created_at": "2024-07-11T11:28:07Z",
        "body": "It works great!  Thanks for your help. "
      }
    ]
  },
  {
    "number": 18,
    "title": "可以在mac电脑M芯片上面跑吗 后期有考虑支持吗",
    "created_at": "2024-07-11T09:20:01Z",
    "closed_at": "2024-07-12T09:21:30Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/18",
    "body": "可以在mac电脑M芯片上面跑吗 后期有考虑支持吗",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/18/comments",
    "author": "kevenfeng",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T09:25:26Z",
        "body": "这个有点难的，现在还是非常依赖GPU的算力。我们也在努力优化效率，但纯CPU的推理速度还是太慢了。。。"
      },
      {
        "user": "div-wang",
        "created_at": "2024-07-11T11:40:48Z",
        "body": "去autodl开个机器跑呗，也不贵"
      },
      {
        "user": "kevenfeng",
        "created_at": "2024-07-13T03:01:08Z",
        "body": "谢谢回复。"
      }
    ]
  },
  {
    "number": 17,
    "title": "照片可以为全身照吗",
    "created_at": "2024-07-11T08:24:42Z",
    "closed_at": "2024-07-11T08:56:48Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/17",
    "body": "刚体验了下，照片为半身照的，效果还行，echomimic支持照片为全身照的吗？",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/17/comments",
    "author": "chenhongwu127",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T08:46:36Z",
        "body": "直接把全身图扔给SD这个还不太行。。。。要不可以自己调整下裁剪代码加一个贴回的操作？"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-11T08:56:41Z",
        "body": "目前只支持人脸区域的输入，所以全身图需要调整下，crop人脸区域输入"
      }
    ]
  },
  {
    "number": 16,
    "title": "Train code Release?",
    "created_at": "2024-07-11T08:21:15Z",
    "closed_at": "2024-07-11T08:58:39Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/16",
    "body": "Congrats on this great work!! When will the train code be released? Thanks!",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/16/comments",
    "author": "zdyshine",
    "comments": [
      {
        "user": "JoeFannie",
        "created_at": "2024-07-11T08:58:39Z",
        "body": "We are considering to release the training code. However, we are occupied with many other TODOs such as inference acceleration, paper writing and so on. The training code release is not on the top. It may not be released soon."
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-11T17:48:04Z",
        "body": "@JoeFannie \r\n\r\nThe results are just amazing. Any possibility to prioritize training release so we can train in other languages. It will be helpful to use with other languages."
      }
    ]
  },
  {
    "number": 14,
    "title": "Is this suitable for lipsync?",
    "created_at": "2024-07-11T02:39:24Z",
    "closed_at": "2024-07-11T09:00:31Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/14",
    "body": "Hi, this project seems to be animating a reference image with driving audio and/or face pose. Is this suitable for lipsync?\r\n\r\nAlso, this seems to be using video diffusion methods, how do you make it to generate such a long video, without running into resource constraints? Is this using Conv method instead of temporal attention for handling the temporal dimension?",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/14/comments",
    "author": "xiankgx",
    "comments": [
      {
        "user": "huanggou666",
        "created_at": "2024-07-11T06:17:18Z",
        "body": "it   renders kind of slow, don't think people can use it for lipsync, 5 second audio on RTX 4090 , takes 30/30 [03:37<00:00,  7.25s/it]"
      },
      {
        "user": "yuange250",
        "created_at": "2024-07-11T08:11:53Z",
        "body": "Indeed, the current algorithm's efficiency severely limits its practical application and future prospects, we are trying to optimize the efficiency of this algorithm. At the same time, one of  purpose on open sourcing is to gather collective efforts to improve the practical usability of this technology."
      },
      {
        "user": "xiankgx",
        "created_at": "2024-07-11T08:49:38Z",
        "body": "It will be slow because this is based on video diffusion model. Not looking at realtime prediction. But it is suitable for lip sync? Do you have a pipeline for that? \r\n\r\nHow can we split the audio feature to only generate one frame, rather than complete video to do frame-by-frame processing?"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-11T09:00:28Z",
        "body": "It is a common limitation for SD models for its slow inference. We are trying to accelerate it by step distillation techniques. It is possible that we release a fast version of EchoMimic in this month."
      }
    ]
  },
  {
    "number": 13,
    "title": "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel",
    "created_at": "2024-07-11T02:38:25Z",
    "closed_at": "2024-07-11T09:00:53Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/13",
    "body": "The following warning appears during runtime，Does this matter？\r\n\r\nSome weights of the model checkpoint were not used when initializing UNet2DConditionModel: \r\n ['down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight, down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight, down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight, down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight, down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight, up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight, up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight, up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight, up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight, up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight, up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight, up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight, up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight, up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias, mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, mid_block.attentions.0.transformer_blocks.0.norm2.weight, mid_block.attentions.0.transformer_blocks.0.norm2.bias, conv_norm_out.weight, conv_norm_out.bias, conv_out.weight, conv_out.bias']\r\n[0, 0, 1342, 1342]",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/13/comments",
    "author": "TryingToDoBetter25",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T03:44:00Z",
        "body": " doesn‘t matter 😁"
      }
    ]
  },
  {
    "number": 12,
    "title": "Please add colab OR HF demo",
    "created_at": "2024-07-10T20:36:53Z",
    "closed_at": "2024-07-11T09:01:09Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/12",
    "body": "need this demo page to check the example script",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/12/comments",
    "author": "s9anus98a",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-11T02:24:45Z",
        "body": "we are constructing hugging face demo, we will inform you as the demo is finshied :)"
      }
    ]
  },
  {
    "number": 10,
    "title": "ImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings' ",
    "created_at": "2024-07-10T16:02:33Z",
    "closed_at": "2024-07-11T09:01:40Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/10",
    "body": "(echomimic) C:\\ALLWEBUI\\EchoMimic\\EchoMimic>python -u infer_audio2vid.py\r\nTraceback (most recent call last):\r\n  File \"infer_audio2vid.py\", line 23, in <module>\r\n    from src.models.unet_2d_condition import UNet2DConditionModel\r\n  File \"C:\\ALLWEBUI\\EchoMimic\\EchoMimic\\src\\models\\unet_2d_condition.py\", line 18, in <module>\r\n    from diffusers.models.embeddings import (\r\nImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings' (C:\\Users\\admin\\.conda\\envs\\echomimic\\lib\\site-packages\\diffusers\\models\\embeddings.py)",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/10/comments",
    "author": "huanggou666",
    "comments": [
      {
        "user": "farhanyosuf11",
        "created_at": "2024-07-10T16:51:22Z",
        "body": "I fixed it by installing diffusers=0.24.0"
      },
      {
        "user": "huanggou666",
        "created_at": "2024-07-11T06:19:05Z",
        "body": "> I fixed it by installing diffusers=0.24.0\r\n\r\nthanks problem solved "
      }
    ]
  },
  {
    "number": 7,
    "title": "Will publish a gradio app that has all the features you presented in paper?",
    "created_at": "2024-07-10T03:52:57Z",
    "closed_at": "2024-07-11T09:04:38Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/7",
    "body": "that would be great",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/7/comments",
    "author": "FurkanGozukara",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-10T05:20:53Z",
        "body": "We are considering developing a ComfyUI node, and we will inform you as soon as the development is completed.：）\r\n"
      },
      {
        "user": "FurkanGozukara",
        "created_at": "2024-07-10T08:00:49Z",
        "body": "@yuange250 comfyui is being very limited audience. please make gradio as well ty"
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-07-10T16:54:28Z",
        "body": "Yes please gradio based UI"
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-11T09:04:38Z",
        "body": "We are working on a hf demo via gradio. The corresponding code can be released if it is necessary."
      },
      {
        "user": "FurkanGozukara",
        "created_at": "2024-07-11T10:14:12Z",
        "body": "> We are working on a hf demo via gradio. The corresponding code can be released if it is necessary.\r\n\r\nCloneable hf demo would work. Just add all features ty "
      }
    ]
  },
  {
    "number": 6,
    "title": "The paper link is invalid",
    "created_at": "2024-07-10T03:00:02Z",
    "closed_at": "2024-07-11T09:05:22Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/6",
    "body": null,
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/6/comments",
    "author": "FacePoluke",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-10T03:02:36Z",
        "body": "Sorry... We upload the paper to arxiv yesterday, but the website neads time to processing..."
      },
      {
        "user": "JoeFannie",
        "created_at": "2024-07-11T09:05:22Z",
        "body": "It will be published at around 0:00 GMT 2024/07/12."
      }
    ]
  },
  {
    "number": 3,
    "title": "ImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings'",
    "created_at": "2024-07-09T18:03:12Z",
    "closed_at": "2024-07-10T03:34:00Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/3",
    "body": "(echomimic) D:\\AI\\EchoMimic>python -u infer_audio2vid.py\r\nTraceback (most recent call last):\r\n  File \"infer_audio2vid.py\", line 23, in <module>\r\n    from src.models.unet_2d_condition import UNet2DConditionModel\r\n  File \"D:\\AI\\EchoMimic\\src\\models\\unet_2d_condition.py\", line 18, in <module>\r\n    from diffusers.models.embeddings import (\r\nImportError: cannot import name 'PositionNet' from 'diffusers.models.embeddings' (C:\\Users\\Renel\\anaconda3\\envs\\echomimic\\lib\\site-packages\\diffusers\\models\\embeddings.py)",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/3/comments",
    "author": "oisilener1982",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-10T01:37:56Z",
        "body": "Upgrading the 'diffusers' to higher version may solve the problem, try:\r\npip install diffusers==0.24.0"
      },
      {
        "user": "oisilener1982",
        "created_at": "2024-07-10T03:33:56Z",
        "body": "thanks it solved the issue"
      },
      {
        "user": "lonngxiang",
        "created_at": "2024-07-11T04:04:57Z",
        "body": "一样错误"
      }
    ]
  },
  {
    "number": 2,
    "title": "Benchmarks",
    "created_at": "2024-07-09T16:06:24Z",
    "closed_at": "2024-07-10T07:10:35Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/2",
    "body": "what is the inference time per frame on a low/mid level card?",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/2/comments",
    "author": "opchronatron",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-10T01:53:23Z",
        "body": "We only test the algo on V100(16G) and 4090D(24G), which may be defined as low/mid GPU. On v100,  the generation of a  10s/24fps(240 frame) video may cost about 8 minute(including model init), on 4090D it may cost about 6-7 minute. \r\nHovever, the time consumption is a biased statistical measure. It is not only affected by GPU performance but also significantly influenced by CPU, memory, and disk I/O, among other factors."
      }
    ]
  },
  {
    "number": 1,
    "title": "作者拉微信群，赶紧的",
    "created_at": "2024-07-09T12:04:43Z",
    "closed_at": "2024-07-10T07:10:17Z",
    "labels": [],
    "url": "https://github.com/antgroup/echomimic/issues/1",
    "body": "如题。",
    "comments_url": "https://api.github.com/repos/antgroup/echomimic/issues/1/comments",
    "author": "jeffreyhaoau",
    "comments": [
      {
        "user": "yuange250",
        "created_at": "2024-07-09T12:05:45Z",
        "body": "主页readme就有群，欢迎入群～"
      }
    ]
  }
]