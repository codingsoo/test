[
  {
    "number": 390,
    "title": "fix: update install scripts to use quotes for zsh compatibility",
    "created_at": "2024-11-23T12:14:09Z",
    "closed_at": "2024-12-16T20:50:23Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/390",
    "body": "Adding quotes to the installation script makes it compatible with zsh shells, while still working with any other shell. Example: `pip install -U \"ell-ai[all]\"`. Basically the problem is that the square brackets have special meaning in ZSH as they're used for pattern matching, so the shell is trying to interpret `[all]` as a pattern rather than passing it literally to pip.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/390/comments",
    "author": "MrPolymath",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-12-16T20:50:20Z",
        "body": "Great fix!"
      }
    ]
  },
  {
    "number": 375,
    "title": "fix getsource for jupyter context",
    "created_at": "2024-11-13T21:25:28Z",
    "closed_at": "2024-11-14T17:23:41Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/375",
    "body": "Don't know if this creates any other issues but the problem seemed to be that by default, getsource is looking for the source filename which in jupyter doesn't exist.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/375/comments",
    "author": "chrisgoddard",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-11-14T17:23:36Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 369,
    "title": "Update tool_usage.rst",
    "created_at": "2024-11-09T06:46:48Z",
    "closed_at": "2024-12-20T07:51:19Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/369",
    "body": "Fix This Issue\r\n\r\n> openai.BadRequestError: Error code: 400 - {'error': {'message': 'operation error Bedrock Runtime: Converse, https response error StatusCode: 400, RequestID: 23c8f6d0-f0bd-4bad-a54f-2dce41bc10ff, ValidationException: A conversation must alternate between user and assistant roles. Make sure the conversation alternates between user and assistant roles and try again.', 'code': '-4003'}}",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/369/comments",
    "author": "leeight",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-11-20T20:46:45Z",
        "body": "is there anyway you can modify bedrock to merge assistant/user messages in the provider itself"
      }
    ]
  },
  {
    "number": 365,
    "title": "fix  sync",
    "created_at": "2024-11-06T11:02:20Z",
    "closed_at": "2024-11-11T00:52:53Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/365",
    "body": null,
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/365/comments",
    "author": "chenxingqiang",
    "comments": [
      {
        "user": "darkacorn",
        "created_at": "2024-11-09T18:02:18Z",
        "body": "Copyright (c) 2024 William Guss\r\nCopyright (c) 2024 Xingqiang Chen\r\n\r\nbreaking licence .. renameing stuff and pushing that .. \r\nyou got balls\r\nlol CLEAN UP YOUR CODEBASE .. for the sake of god .. "
      },
      {
        "user": "MadcowD",
        "created_at": "2024-11-11T00:49:40Z",
        "body": "waht????"
      }
    ]
  },
  {
    "number": 364,
    "title": "examples += `wikipedia_mini_rag.py` for educational LLM+Wikipedia integration",
    "created_at": "2024-11-04T14:15:14Z",
    "closed_at": "2024-11-05T16:58:20Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/364",
    "body": "* Introduced `wikipedia_mini_rag.py` to showcase integration of LLM agents with Wikipedia using `lynx`.\r\n* Key functionalities:\r\n  * `search_wikipedia`: tool to return search results and links.\r\n  * `wikipedia_page_content`: fetches page content.\r\n  * `search_wikipedia_and_suggest_page_to_read`: AI suggests Wikipedia page based on query.\r\n  * `answer_query_by_reading_wikipedia_page`: AI reads page content to answer queries.\r\n* Implements verbosity via command-line flags `-v` and `-vv`.\r\n* Example positioned in `examples/` to guide educational purposes.\r\n* Integrates LLM workflows using `ell` library, illustrating refined AI-agent interactions with tools.\r\n\r\nAdditionally gives users educational task (and link to solution if someone seeking spoilers).",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/364/comments",
    "author": "gwpl",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-11-05T16:58:30Z",
        "body": "TY~!!!!"
      }
    ]
  },
  {
    "number": 363,
    "title": "feat(examples): add `llm_lottery.py` for demonstrating LLM tool integration",
    "created_at": "2024-11-04T12:20:36Z",
    "closed_at": "2024-11-05T17:00:47Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/363",
    "body": "\r\n\r\n* Introduce `llm_lottery.py`, a new example script leveraging LLM with predefined tools.\r\n* Tools introduced:\r\n  * `buy_lottery_ticket`: Purchases a lottery ticket.\r\n  * `check_lottery_result`: Validates if purchased ticket wins.\r\n* Implements `play_lottery`, enhancing LLM actions with GPT-4o-mini model.\r\n* Included `loop_llm_and_tools` for iterative task resolution until completion.\r\n* Integrated `argparse` for verbosity, showcasing scalable CLI with `-v`, `-vv` flags.\r\n* Establishes script execution with `main()` for elucidated tool dynamics.\r\n* Illustrates LLM capabilities in procedural, educational lottery gameplay context.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/363/comments",
    "author": "gwpl",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-11-05T16:59:53Z",
        "body": "Thank you looks good!"
      }
    ]
  },
  {
    "number": 343,
    "title": "Separate storage, studio dependencies",
    "created_at": "2024-10-29T00:56:13Z",
    "closed_at": "2024-11-11T00:50:00Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/343",
    "body": "Tried making this as unobtrusive as possible :)\r\n\r\nStudio and storage are separated and are available as optional installs.\r\n\r\n## Simple\r\n### ell[all]\r\n- Includes anthropic, groq, storage with sqlite, and studio (no change, same as what we have today -- this is the recommended install in the docs)\r\n \r\n\r\n### ell\r\n- Installs `ell` without studio, no tracing/storage support, and the OpenAI provider\r\n\r\n\r\n## Complex \r\nAdvanced users can specify extras to control what is installed and what isn't:\r\n\r\n### ell[sqlite]\r\n- Adds storage via sqlite\r\n- Does not include studio\r\n\r\n### ell[studio, postgres]\r\n- Adds ell studio and tracing using postgres as a backend\r\n\r\n\r\nFixes #284 ",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/343/comments",
    "author": "alex-dixon",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-10-29T01:10:01Z",
        "body": "@alex-dixon  Would you make a PR to update the docs for this? It's great :) \r\n\r\nSome requests:\r\n\r\nI cannot think of a world where you would want sqlite but not studio? I think something like:\r\n```\r\npip install ell-ai[all]\r\npip install ell-ai[studio] #defaults to sqlite\r\npip install ell-ai[studio, postgres]\r\n```\r\nwould be acceptable.\r\n\r\nAnd can you make sure that if they try and run the studio command or init the store without the dependency they get a really helpful error message like:\r\n```\r\nell.init(store='./logdir')\r\n# ImportErorr(You need to do pip install ell-ai[all] or pip install ell-ai[studio]) \r\n```\r\nbasically i want it to me idiotproof.\r\n\r\nAlso it should be so that if you try and run `ell-studio ...` and pip install ell-ai[studio]` is not installed then it says `ImportError(You need to do pip install ell-ai[all] or pip install ell-ai[studio])`\r\n"
      },
      {
        "user": "alex-dixon",
        "created_at": "2024-10-30T15:10:52Z",
        "body": "@MadcowD \r\n\r\n> I cannot think of a world where you would want sqlite but not studio?\r\n\r\n\r\nin pythonland it could be ‚Äúmy program uses a fast api version that doesn‚Äôt work with ell studio (but i still want to write traces and have versioning)‚Äù. The same goes for any dependency that is required to run studio but not write traces.\r\n\r\nI still need to update the error messages but docs are ready. \r\n\r\nAny thoughts on linking to docs in the error message in addition to including an install command?"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-10-31T17:02:10Z",
        "body": "Linking docs is a good idea :) "
      }
    ]
  },
  {
    "number": 328,
    "title": "Update `instructor_ex.py` - fix typo",
    "created_at": "2024-10-22T19:36:48Z",
    "closed_at": "2024-10-24T12:59:17Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/328",
    "body": "Fix minor typos in `instructor_ex.py` docstring\r\n\r\nAnd thanks for making `ell`, great tool for prompt engineering ‚ù§Ô∏è !",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/328/comments",
    "author": "CharlieJCJ",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-10-24T12:59:25Z",
        "body": "THANK YOU!"
      }
    ]
  },
  {
    "number": 313,
    "title": "Exa AI documentation",
    "created_at": "2024-10-16T00:01:10Z",
    "closed_at": "2024-10-20T12:50:33Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/313",
    "body": "Added python file and README for exa ai integration with ell. Let me know how I can improve my code!",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/313/comments",
    "author": "master-senses",
    "comments": [
      {
        "user": "darkacorn",
        "created_at": "2024-10-16T00:03:35Z",
        "body": "1 thing otherswise LGTM \r\nmaybe group it to a subfolder it so the readme is bound to that example  "
      },
      {
        "user": "master-senses",
        "created_at": "2024-10-16T02:02:56Z",
        "body": "should be in exa subfolder now"
      },
      {
        "user": "darkacorn",
        "created_at": "2024-10-16T03:08:04Z",
        "body": "@MadcowD lgtm - alex may want to take a look at it .. but its a nice example "
      },
      {
        "user": "reecelikesramen",
        "created_at": "2024-10-17T16:28:36Z",
        "body": "+1 good example"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-10-20T12:50:42Z",
        "body": "Awesome :)))"
      }
    ]
  },
  {
    "number": 296,
    "title": "Feature: configure autocommit model",
    "created_at": "2024-10-09T05:44:27Z",
    "closed_at": "2024-10-09T17:39:09Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/296",
    "body": "I don't use OpenAI stuff really so I wanted Claude 3 Haiku to write my autocommits.\r\n\r\nMain changes:\r\n- config param for autocommit_model\r\n- fixed diff in `write_commit_message_for_diff`\r\n  - the diff variable was a generator, so you'd get this for the value of diff in a fstring: `<generator object unified_diff at 0x10aef4d40>`\r\n  - fixed by joining lines of diff with newline\r\n- `write_commit_message_for_diff` max tokens 500\r\n  - should be a reasonable limit, just needed to set this because of the Anthropic client requirement\r\n- changed `write_commit_message_for_diff` prompt, got more consistent behavior out of Claude 3 Haiku this way\r\n  - I just followed some of Anthropic's guidelines. GPT 4o mini didn't change its behavior much, you can check the difference for two tests in `tests/test_autocommit_model.py`",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/296/comments",
    "author": "reecelikesramen",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-10-09T17:38:11Z",
        "body": "I see interesting :) "
      },
      {
        "user": "MadcowD",
        "created_at": "2024-10-09T17:39:11Z",
        "body": "Thank you for this!"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-10-09T17:39:11Z",
        "body": "Thank you for this!"
      }
    ]
  },
  {
    "number": 294,
    "title": "Is there a way to call openai api in parallel?",
    "created_at": "2024-10-09T01:24:52Z",
    "closed_at": "2024-10-26T22:56:41Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/294",
    "body": null,
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/294/comments",
    "author": "kcui23",
    "comments": [
      {
        "user": "alex-dixon",
        "created_at": "2024-10-26T20:20:33Z",
        "body": "Yes. You should be able to call LMPs in parallel the same as you would for any Python function "
      }
    ]
  },
  {
    "number": 292,
    "title": "Missing index.html in ell-studio when installing from Git Repository",
    "created_at": "2024-10-07T07:44:43Z",
    "closed_at": "2024-10-07T20:30:50Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/292",
    "body": "I'm installing direct from the git URL to catch up on some of the latest commits, but this breaks ell-studio with an Internal Server Error:\r\n\r\n```\r\nFile \"/usr/local/lib/python3.12/site-packages/starlette/responses.py\", line 330, in __call__\r\n\traise RuntimeError(f\"File at path {self.path} does not exist.\")\r\n\tRuntimeError: File at path /usr/local/lib/python3.12/site-packages/ell/studio/static/index.html does not exist.\r\n```\r\n\r\nNot sure if there's a difference between between the Pypi package and the Git code, I can't see an index.html in the repo.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/292/comments",
    "author": "btimothy-har",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-10-07T20:30:51Z",
        "body": "If you're trying to run in development mode you need to use `--dev` when launching ell studio and run `npm install && npm run start` in the `ell-studio` directory to run the front end. Otherwise if you just want to install and run from git without modifying studio do `python3 build.py` and you can run normally."
      }
    ]
  },
  {
    "number": 287,
    "title": "Fix BV rendering",
    "created_at": "2024-10-05T22:29:10Z",
    "closed_at": "2024-10-14T15:30:11Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/287",
    "body": "```\r\n# <BV>\r\nbound_local = \"prompt\"\r\n# </BV>\r\n```\r\nis appearing in the promopts",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/287/comments",
    "author": "MadcowD",
    "comments": [
      {
        "user": "reecelikesramen",
        "created_at": "2024-10-14T02:28:51Z",
        "body": "closure.py\r\n```py\r\n104    dsrc = _clean_src(dirty_src_without_func)\r\n105\r\n106    # Format the sorce and dsrc soruce using Black\r\n107    source = _format_source(source)\r\n108    dsrc = _format_source(dsrc)\r\n```\r\nhappens on 108 when you use black to format the `dsrc` cause black adds spaces after # comments"
      },
      {
        "user": "reecelikesramen",
        "created_at": "2024-10-14T02:31:47Z",
        "body": "This is probably an issue with <BmV> and <LMP> tags as well, right?\r\nI would just change to add spaces by the comment tags in LMPSourceView.js from ell-studio\r\n```js\r\nreturn [\r\n  {\r\n    name: \"boundedVariable\",\r\n    startTag: \"# <BV>\",\r\n    endTag: \"# </BV>\",\r\n    wrapper: ({ children, key, content }) => {\r\n      return <>{children}</>\r\n    },\r\n  },\r\n  {\r\n    name: \"boundedMutableVariable\",\r\n    startTag: \"# <BmV>\",\r\n    endTag: \"# </BmV>\",\r\n    wrapper: mutableBVWrapper,\r\n  },\r\n  {\r\n    name: \"usedLMP\",\r\n    startTag: \"# <LMP>\",\r\n    endTag: \"# </LMP>\",\r\n    wrapper: ({ children, selectedInvocation, content }) => {\r\n      return (\r\n        <UsedLMPWrapper uses={uses} selectedInvocation={selectedInvocation} content={content}>\r\n          {children}\r\n        </UsedLMPWrapper>\r\n      )\r\n    },\r\n  },\r\n]\r\n```\r\n\r\nI'll test it and open a PR"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-10-14T15:30:23Z",
        "body": "@reecelikesramen is the best"
      }
    ]
  },
  {
    "number": 284,
    "title": "Incopatibility with sqlmodel and fastapi",
    "created_at": "2024-10-04T16:32:52Z",
    "closed_at": "2024-11-11T00:50:01Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/284",
    "body": "Hi, loved ell, but I cant use it on my project unfortunately. \r\nHere are my dependencies\r\nrequires-python = \">=3.10\"\r\ndependencies = [\r\n    \"asyncpg>=0.29.0\",\r\n    \"black>=24.8.0\",\r\n    \"fastapi[standard]>=0.115.0\",\r\n    \"numpy>=2.1.1\",\r\n    \"pandas>=2.2.3\",\r\n    \"psycopg2-binary>=2.9.9\",\r\n    \"pydantic-settings>=2.5.2\",\r\n    \"python-dotenv>=1.0.1\",\r\n    \"scrapy>=2.11.2\",\r\n    \"sqlmodel>=0.0.22\",\r\n]\r\n\r\nwhen i tried to install ell-ai got this:\r\n\r\n No solution found when resolving dependencies for split (python_full_version == '3.10.*'):\r\n  ‚ï∞‚îÄ‚ñ∂ Because only the following versions of ell-ai are available:\r\n          ell-ai==0.0.1\r\n          ell-ai==0.0.2\r\n          ell-ai==0.0.3\r\n          ell-ai==0.0.4\r\n          ell-ai==0.0.5\r\n          ell-ai==0.0.6\r\n          ell-ai==0.0.7\r\n          ell-ai==0.0.8\r\n          ell-ai==0.0.9\r\n          ell-ai==0.0.10\r\n          ell-ai==0.0.11\r\n          ell-ai==0.0.12\r\n          ell-ai==0.0.13\r\n      and all of:\r\n          ell-ai<=0.0.7\r\n          ell-ai>=0.0.9\r\n      depend on sqlmodel>=0.0.21,<0.0.22, we can conclude that all of:\r\n          ell-ai<0.0.8\r\n          ell-ai>0.0.8\r\n      depend on sqlmodel>=0.0.21,<0.0.22.\r\n      And because ell-ai==0.0.8 was yanked (reason: Regression fixed in 0.0.9), we can conclude that all\r\n      versions of ell-ai depend on sqlmodel>=0.0.21,<0.0.22.\r\n      And because your project depends on ell-ai and sqlmodel>=0.0.22, we can conclude that your project's\r\n      requirements are unsatisfiable.\r\n\r\ntried to delete fastapi and used the one that comes with ell on install but even when it work to create a basic api, whein i tried to connect db is broken and not connect no matter what i tried.\r\n\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\n\r\nuninstalling ell-ai and reinstalled fastapi resolved the connection issues. So I cannot use ell on my project unfortunately. \r\n\r\n\r\n\r\n\r\n      \r\n      ",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/284/comments",
    "author": "jdgaravito",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-10-04T23:52:21Z",
        "body": "kk i need to deal with this sorry"
      },
      {
        "user": "alex-dixon",
        "created_at": "2024-11-11T13:23:57Z",
        "body": "@jdgaravito This should be addressed in the next release. \r\n\r\nsqlmodel is still required for tracking at the moment. \r\n\r\nTo install with no fast api or sqlmodel (no tracking support). \r\n\r\npip install ell-ai\r\n\r\nTo install with no fast-api but for tracking with SQLite (which requires sqlmodel):\r\n\r\npip install ell-ai[sqlite]"
      },
      {
        "user": "jdgaravito",
        "created_at": "2024-11-11T14:11:00Z",
        "body": "Thank you, looking forward to test it. "
      }
    ]
  },
  {
    "number": 283,
    "title": "removed hardcoded tool choice in openai provider in favour of a \"get\"‚Ä¶",
    "created_at": "2024-10-04T11:54:05Z",
    "closed_at": "2024-10-04T19:51:25Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/283",
    "body": "‚Ä¶ that defaults to \"auto\"\r\n\r\nThis addresses issue I was facing described in #227 ",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/283/comments",
    "author": "JTCorrin",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-10-04T19:51:25Z",
        "body": "Beautiful :)_ "
      }
    ]
  },
  {
    "number": 264,
    "title": "Can I pass the model as a LMP call override?",
    "created_at": "2024-09-29T15:56:34Z",
    "closed_at": "2024-09-29T17:18:31Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/264",
    "body": "OMG - Ell is fantastic! It's super obvious to use, and much simpler then Langchain! \r\n\r\nThe only thing I haven't been able to figure out is how to select the model at call time (as opposed to passing it to ```ell.simple``` or ```ell.complex```. I'd expect an override just like we have for api_params on any LMP call. \r\n\r\nI want this because I have a lot of model fine tunes which I select at runtime, and loading it into the decorator feels abusive. Another use case would be a program that wants to pass the model name on the command line.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/264/comments",
    "author": "idvorkin",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-29T17:18:24Z",
        "body": "Yes! You actually use the api_params override to do it.\r\n\r\n`my_lmp(api_params={\"model\": \"another_model\"})`"
      },
      {
        "user": "CharlieJCJ",
        "created_at": "2024-10-22T21:56:46Z",
        "body": "Hi @MadcowD, A followup question on this, is there a way to override the model choice / config globally (i.e. for all ell calls)? "
      }
    ]
  },
  {
    "number": 259,
    "title": "Improve ell-studio CLI UX.",
    "created_at": "2024-09-28T00:16:28Z",
    "closed_at": "2024-09-28T17:53:36Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/259",
    "body": "**Issue**\r\nCurrently, running `ell-studio` with no args fails unexpectedly with an unhandled `TypeError`:\r\n\r\n```\r\nom:ell kw$ uvx --from ell-ai[all] ell-studio\r\nTraceback (most recent call last):\r\n  File \"/Users/kw/.cache/uv/archive-v0/yRhKWbo2kdkMVmJ2BWBWg/bin/ell-studio\", line 10, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/Users/kw/.cache/uv/archive-v0/yRhKWbo2kdkMVmJ2BWBWg/lib/python3.12/site-packages/ell/studio/__main__.py\", line 44, in main\r\n    db_path = os.path.join(args.storage_dir)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen posixpath>\", line 76, in join\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\nom:ell kw$\r\n```\r\n\r\n**Changes**\r\n  * Fix the default behavior (no args passed) to align with the documented default of cwd (via `ell.studio.config.Config`).\r\n  * Port usage of `os.path` to `pathlib.Path`.\r\n  * Port usage of `print()` to the `logger` module.\r\n  * Add a `--open` flag to automatically open the UI in a browser on launch.\r\n  * Add a `-v`/`--verbose` flag to change the log level from INFO -> DEBUG.\r\n\r\n**Testing**\r\n```\r\n$ uv run --no-project --with toml --with pytest build.py\r\n...\r\n$ uvx poetry run ell-studio --open -v\r\n...\r\n$ uvx poetry run ell-studio --storage-dir=./ell_logs --open -v\r\n```\r\nand verified UI was functional.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/259/comments",
    "author": "kwlzn",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-28T17:53:55Z",
        "body": "Amazing!"
      }
    ]
  },
  {
    "number": 253,
    "title": "AttributeError: 'Groq' object has no attribute 'beta' when using ELL with Groq client",
    "created_at": "2024-09-26T19:19:41Z",
    "closed_at": "2024-09-28T18:01:34Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/253",
    "body": "**Description:**\r\n\r\nI'm encountering an `AttributeError` when trying to use the ELL library with the Groq client. The error occurs when invoking a function decorated with `@ell.complex`.\r\n\r\n**Steps to Reproduce:**\r\n\r\n1. Set up the environment variable `GROQ_API_KEY` with a valid API key.\r\n2. Run the following minimal code:\r\n\r\n```python\r\n# minimal_error_reproduction.py\r\n\r\nimport os\r\nimport ell\r\nimport groq\r\n\r\n# Initialize ELL with verbose logging\r\nell.init(verbose=True)\r\n\r\n# Retrieve Groq API key from environment variable\r\ngroq_api_key = os.getenv('GROQ_API_KEY')\r\nif not groq_api_key:\r\n    raise EnvironmentError(\"GROQ_API_KEY environment variable not set.\")\r\n\r\n# Initialize Groq client\r\nclient = groq.Groq(api_key=groq_api_key)\r\n\r\n# Register the Groq client with ELL\r\nell.models.groq.register(client)\r\n\r\n# Define a simple Pydantic model for structured output\r\nfrom pydantic import BaseModel\r\n\r\nclass SimpleResponse(BaseModel):\r\n    message: str\r\n\r\n# Use @ell.complex with the Groq model\r\n@ell.complex(model=\"llama3-8b-8192\", response_format=SimpleResponse)\r\ndef test_function():\r\n    return \"Return a JSON object with a 'message' field containing 'Hello, world!'\"\r\n\r\n# Invoke the function\r\nresponse_message = test_function()\r\nprint(response_message.parsed)\r\n```\r\n\r\n**Expected Behavior:**\r\n\r\nThe `test_function` should return a parsed `SimpleResponse` object containing the message \"Hello, world!\".\r\n\r\n**Actual Behavior:**\r\n\r\nAn `AttributeError` is raised:\r\n\r\n```\r\nAttributeError: 'Groq' object has no attribute 'beta'\r\n```\r\n\r\n**Full Traceback:**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"minimal_error_reproduction.py\", line 36, in <module>\r\n    response_message = test_function()\r\n  File \".../ell/lmp/_track.py\", line 64, in tracked_func\r\n    return func_to_track(*fn_args, **fn_kwargs, _invocation_origin=invocation_id)[0]\r\n  File \".../ell/lmp/complex.py\", line 68, in model_call\r\n    (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)\r\n  File \".../ell/provider.py\", line 121, in call\r\n    call = self.provider_call_function(ell_call.client, final_api_call_params)\r\n  File \".../ell/providers/openai.py\", line 25, in provider_call_function\r\n    return client.beta.chat.completions.parse\r\nAttributeError: 'Groq' object has no attribute 'beta'\r\n```\r\n\r\n**Environment:**\r\n\r\n- Python version: 3.11\r\n- ELL version: 0.0.12\r\n- Groq version: 0.11.0\r\n\r\n\r\n**Additional Information:**\r\n\r\nIt appears that the ELL library is attempting to access the `beta` attribute on the Groq client, which doesn't exist. This might indicate a compatibility issue or a misconfiguration in how ELL interacts with the Groq client.\r\n\r\n---\r\n\r\n**Request:**\r\n\r\nCould you please look into this issue? Let me know if additional information is needed to diagnose the problem.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/253/comments",
    "author": "aandresalvarez",
    "comments": [
      {
        "user": "aandresalvarez",
        "created_at": "2024-09-26T21:18:58Z",
        "body": "\r\ngroq has support for response_format:\r\n\r\nfrom groq import Groq\r\n\r\nclient = Groq()\r\ncompletion = client.chat.completions.create(\r\n    model=\"llama3-8b-8192\",\r\n    messages=[\r\n        {\r\n            \"role\": \"system\",\r\n            \"content\": \"asdf\"\r\n        },\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"\"\r\n        }\r\n    ],\r\n    temperature=1,\r\n    max_tokens=1024,\r\n    top_p=1,\r\n    stream=False,\r\n    response_format={\"type\": \"json_object\"},\r\n    stop=None,\r\n)\r\n\r\nprint(completion.choices[0].message)\r\n"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-09-27T19:47:34Z",
        "body": "Can you upgrade your ell? Or what version are you on?"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-09-28T18:01:34Z",
        "body": "```\r\nfrom groq import Groq\r\n\r\nclient = Groq()\r\ncompletion = client.chat.completions.create(\r\nmodel=\"llama3-8b-8192\",\r\nmessages=[\r\n{\r\n\"role\": \"system\",\r\n\"content\": \"Please respond in json\"\r\n},\r\n{\r\n\"role\": \"user\",\r\n\"content\": \"\"\r\n}\r\n],\r\ntemperature=1,\r\nmax_tokens=1024,\r\ntop_p=1,\r\nstream=False,\r\nresponse_format={\"type\": \"json_object\"},\r\nstop=None,\r\n)\r\n\r\nprint(completion.choices[0].message)\r\n```\r\nworks for me on 0.0.12!"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-09-28T18:01:49Z",
        "body": "```\r\npip install --upgrade ell-ai\r\n```"
      },
      {
        "user": "aandresalvarez",
        "created_at": "2024-09-28T20:49:01Z",
        "body": "  I will try. Thanks for taking the time to review! "
      },
      {
        "user": "idvorkin",
        "created_at": "2024-10-20T14:51:45Z",
        "body": "This issue still exists if you try to pass a complex response format - e.g. \r\n\r\n```python\r\n@ell.complex(\r\n        model=\"llama-3.2-90b-vision-preview\",\r\n        response_format=ImageRecognitionResult,\r\n)  \r\n```\r\n\r\n```bash\r\n  File \"/Users/idvorkin/gits/nlp/.venv/lib/python3.12/site-packages/ell/lmp/_track.py\", line 118, in tracked_func\r\n    else func_to_track(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )\r\n         ‚îÇ              ‚îÇ                           ‚îÇ                ‚îî {}\r\n         ‚îÇ              ‚îÇ                           ‚îî 'invocation-3c24dedfd205432dd20590e9e132ad34'\r\n         ‚îÇ              ‚îî (<PIL.WebPImagePlugin.WebPImageFile image mode=RGB size=2000x1093 at 0x127FC60C0>,)\r\n         ‚îî <function prompt_recognize at 0x127fc89a0>\r\n  File \"/Users/idvorkin/gits/nlp/.venv/lib/python3.12/site-packages/ell/lmp/complex.py\", line 68, in model_call\r\n    (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)\r\n                                           ‚îÇ        ‚îÇ    ‚îÇ                   ‚îÇ                          ‚îÇ          ‚îî False\r\n                                           ‚îÇ        ‚îÇ    ‚îÇ                   ‚îÇ                          ‚îî <function model_usage_logger_post_intermediate.<locals>.log_stream_chunk at 0x1330fdd00>\r\n                                           ‚îÇ        ‚îÇ    ‚îÇ                   ‚îî 'invocation-3c24dedfd205432dd20590e9e132ad34'\r\n                                           ‚îÇ        ‚îÇ    ‚îî EllCallParams(model='llama-3.2-90b-vision-preview', messages=[Message(role='system', content=[ContentBlock(text=\r\n                                           ‚îÇ        ‚îÇ          You are ...\r\n                                           ‚îÇ        ‚îî <function Provider.call at 0x124971da0>\r\n                                           ‚îî <ell.providers.groq.GroqProvider object at 0x124c4ede0>\r\n  File \"/Users/idvorkin/gits/nlp/.venv/lib/python3.12/site-packages/ell/provider.py\", line 121, in call\r\n    call = self.provider_call_function(ell_call.client, final_api_call_params)\r\n           ‚îÇ    ‚îÇ                      ‚îÇ        ‚îÇ       ‚îî {'response_format': <class '__main__.ImageRecognitionResult'>, 'model': 'llama-3.2-90b-vision-preview', 'messages': [{'role':...\r\n           ‚îÇ    ‚îÇ                      ‚îÇ        ‚îî <groq.Groq object at 0x127f581a0>\r\n           ‚îÇ    ‚îÇ                      ‚îî EllCallParams(model='llama-3.2-90b-vision-preview', messages=[Message(role='system', content=[ContentBlock(text=\r\n           ‚îÇ    ‚îÇ                            You are ...\r\n           ‚îÇ    ‚îî <function OpenAIProvider.provider_call_function at 0x124c47e20>\r\n           ‚îî <ell.providers.groq.GroqProvider object at 0x124c4ede0>\r\n  File \"/Users/idvorkin/gits/nlp/.venv/lib/python3.12/site-packages/ell/providers/openai.py\", line 25, in provider_call_function\r\n    return client.beta.chat.completions.parse\r\n           ‚îî <groq.Groq object at 0x127f581a0>\r\n\r\nAttributeError: 'Groq' object has no attribute 'beta'\r\n```\r\n\r\nIs this expected? If so, could we throw a more descriptive error? "
      }
    ]
  },
  {
    "number": 230,
    "title": "No support for multiple tool calls in one go",
    "created_at": "2024-09-24T06:58:38Z",
    "closed_at": "2024-09-25T08:38:26Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/230",
    "body": "Sometimes the llm can call two functions at once. This is not supported now and raises an error that the length of the tool call results are > 1.\r\n\r\nError produced:\r\n```\r\n  File \"/Users/.../lib/python3.12/site-packages/ell/providers/openai.py\", line 72, in translate_to_provider\r\n    assert len(tool_results) == 1, \"Message should only have one tool result\"\r\n```\r\n\r\nCode to reproduce\r\n```\r\nimport ell\r\n\r\n\r\n@ell.tool()\r\ndef get_user_name():\r\n    return \"Isac\"\r\n\r\n\r\n@ell.tool()\r\ndef get_ice_cream_flavors():\r\n    return [\"Vanilla\", \"Strawberry\", \"Coconut\"]\r\n\r\n\r\n@ell.complex(model=\"gpt-4o\", tools=[get_user_name, get_ice_cream_flavors])\r\ndef f(message_history: list[ell.Message]) -> list[ell.Message]:\r\n    return [\r\n        ell.system(\r\n            \"You are a helpful assistant that greets the user and asks them what ice cream flavor they want. Call both tools immediately and then greet the user\"\r\n        )\r\n    ] + message_history\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ell.init(\"playground/ell-logs/\")\r\n    messages = []\r\n    while True:\r\n        message = f(messages)\r\n        messages.append(message)\r\n\r\n        print(\"message.tool_calls:\", message.tool_calls)\r\n        if message.tool_calls:\r\n            tool_call_response = message.call_tools_and_collect_as_message(\r\n                parallel=True, max_workers=2\r\n            )\r\n            print(\"tool_call_response:\", tool_call_response)\r\n            messages.append(tool_call_response)\r\n        else:\r\n            break\r\n\r\n    print(messages)\r\n```",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/230/comments",
    "author": "isacarnekvist",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-24T18:19:45Z",
        "body": "On it"
      }
    ]
  },
  {
    "number": 228,
    "title": "0.0.7 breaks ollama example",
    "created_at": "2024-09-23T20:28:44Z",
    "closed_at": "2024-09-23T21:04:44Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/228",
    "body": " % python -m unittest tests.test_greeting\r\n\r\nUsing model: llama3.1:latest\r\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢\r\n‚îÇ   assistant: \"Hello, Alice! It's lovely to connect with you\r\n‚îÇ              today! How's your day going so far? I'm here to listen,\r\n‚îÇ              help or just chat - whatever makes you feel happy and\r\n‚îÇ              supported!\"\r\n‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\r\n\r\n\r\n======================================================================\r\nERROR: test_hello_custom (tests.test_greeting.TestGreeting.test_hello_custom)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"//ell-main/artificial_brain_agent/tests/test_greeting.py\", line 25, in test_hello_custom\r\n    greeting = hello_world(name)\r\n               ^^^^^^^^^^^^^^^^^\r\n  File \"//venv/lib/python3.12/site-packages/ell/lmp/_track.py\", line 122, in tracked_func\r\n    prompt_tokens=usage.get(\"prompt_tokens\", 0)\r\n                  ^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'get'\r\n\r\n\r\n\r\n\r\n\r\nc/o @patchthecode",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/228/comments",
    "author": "alex-dixon",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-23T21:04:44Z",
        "body": "fixed"
      }
    ]
  },
  {
    "number": 206,
    "title": "Support URLs for images.",
    "created_at": "2024-09-22T09:43:14Z",
    "closed_at": "2024-09-23T08:09:13Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/206",
    "body": "Hello. Added support for image URLs. Also extended docs to show usage. Thanks.",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/206/comments",
    "author": "klvnptr",
    "comments": [
      {
        "user": "alex-dixon",
        "created_at": "2024-09-22T14:20:44Z",
        "body": "Thanks for this! I‚Äôve been thinking about images in general and how best to handle them. If some providers don‚Äôt support image URLs that makes this all the more interesting üòÖ\r\n\r\nI‚Äôm guessing we‚Äôll want 1 Image type and add an optional image url key to it. This means the pil image key would become optional also. This is the same pattern as an ell content block ‚Äî it‚Äôs polymorphic and all keys are optional. \r\n\r\nwe‚Äôd also want eg anthropic to work when an image url is supplied instead of erroring out. In the anthropic provider, would it be possible to read the url into a PIL image?\r\n\r\nFor any provider that does support image URLs I believe that‚Äôs much more efficient in terms of tokens etc. Knowing some providers don‚Äôt support them makes it trickier to standardize on though. "
      },
      {
        "user": "klvnptr",
        "created_at": "2024-09-22T18:15:21Z",
        "body": "hey @alex-dixon thanks for the suggestions. could you please check this? if it fits your ideas, i clean it up and write the docs."
      },
      {
        "user": "alex-dixon",
        "created_at": "2024-09-22T20:55:30Z",
        "body": "@klvnptr looks great! exactly what i had in mind. I can see after your latest it's probably easier for providers like anthropic to interop if Image has a get_pil or get_bytes or similar that either returns the pil image or reads the image from the URL and constructs it. \r\n\r\n@MadcowD will need to weigh in and merge from here"
      }
    ]
  },
  {
    "number": 203,
    "title": "make \"chat-based\" example work ell_complex.rst",
    "created_at": "2024-09-21T21:25:36Z",
    "closed_at": "2024-09-25T08:41:37Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/203",
    "body": "Fixing errors of \"Message\" not defined or:\r\n\r\n```\r\nTypeError: 'Message' object is not subscriptable\r\n```",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/203/comments",
    "author": "gwpl",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-23T03:08:13Z",
        "body": "Actually it should be\r\nprint(\"Bot:\", response.text)\r\nCan you make that change I'll merge"
      }
    ]
  },
  {
    "number": 202,
    "title": "opening studio programmatically",
    "created_at": "2024-09-21T18:37:17Z",
    "closed_at": "2024-09-23T02:33:19Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/202",
    "body": "hi, great library, thank you!\r\n\r\nis there a way to programmatically open the studio?\r\n\r\ne.g.\r\n\r\n```\r\nfrom ell import studio\r\n\r\nstudio.open()\r\n\r\n```\r\n\r\nthe `ell-studio` command doesnt work for me (i suspect something to do with using `uv` for installation)",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/202/comments",
    "author": "colinricardo",
    "comments": [
      {
        "user": "colinricardo",
        "created_at": "2024-09-22T11:09:25Z",
        "body": "update, with `uv` we can do: \r\n\r\n`uv run python -m ell.studio --storage ./logdir`\r\n\r\n"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-09-23T02:33:19Z",
        "body": "You can also run \r\n```\r\npython3 -m ell.studio --storage ./logdir\r\n```\r\nIf you want to post your error with the original `ell-studio` please do I'd look forward to it in another issue :) "
      }
    ]
  },
  {
    "number": 196,
    "title": "Added support for AWS Bedrock",
    "created_at": "2024-09-20T07:52:52Z",
    "closed_at": "2024-09-30T07:24:24Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/196",
    "body": "Add support for AWS Bedrock ",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/196/comments",
    "author": "brnaba-aws",
    "comments": [
      {
        "user": "brnaba-aws",
        "created_at": "2024-09-20T14:59:15Z",
        "body": "See #197 "
      },
      {
        "user": "massi-ang",
        "created_at": "2024-09-23T08:38:40Z",
        "body": "Working on it."
      },
      {
        "user": "brnaba-aws",
        "created_at": "2024-09-23T15:59:51Z",
        "body": "See #226"
      },
      {
        "user": "brnaba-aws",
        "created_at": "2024-09-30T07:24:22Z",
        "body": "fixed in #226 "
      }
    ]
  },
  {
    "number": 183,
    "title": "Support OpenAI JSON mode",
    "created_at": "2024-09-19T17:10:39Z",
    "closed_at": "2024-09-25T22:07:22Z",
    "labels": [
      "LM Functionality"
    ],
    "url": "https://github.com/MadcowD/ell/issues/183",
    "body": "Many API-compatible models, such as those from DeepSeek, support the JSON Mode feature of the OpenAI API. However, if JSON Mode is not activated in the OpenAI client, the responses are returned as JSON enclosed in markdown quotes. This makes it difficult to work with the raw JSON data directly.\r\n\r\nTo enhance compatibility with these models, I propose adding an option to activate JSON Mode in the OpenAI client, such as by using a parameter like: response_format={'type': 'json_object'}",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/183/comments",
    "author": "olafgeibig",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-19T17:30:41Z",
        "body": "Gotcha"
      }
    ]
  },
  {
    "number": 168,
    "title": "Docs http/s redirect ",
    "created_at": "2024-09-16T14:51:29Z",
    "closed_at": "2024-09-18T20:27:41Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/168",
    "body": "The folks having trouble accessing the docs as university may have been hitting this?\r\n\r\ndocs.ell.so goes to http://\r\n\r\nshould redirect http to https",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/168/comments",
    "author": "alex-dixon",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-16T19:22:21Z",
        "body": "weird i'm not getting that. good to know!"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-09-17T18:11:45Z",
        "body": "@alex-dixon any idea how to fix this?"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-09-18T20:27:41Z",
        "body": "Fidxded"
      }
    ]
  },
  {
    "number": 160,
    "title": "Fixing host not being passed to uvicorn for ell-studio",
    "created_at": "2024-09-12T21:11:42Z",
    "closed_at": "2024-09-13T04:50:10Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/160",
    "body": "Looks like when the websocket code was added support for the host flag was dropped",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/160/comments",
    "author": "murmus",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-13T04:50:15Z",
        "body": "ty~!!"
      }
    ]
  },
  {
    "number": 159,
    "title": "fix(ell_complex.rst.txt): Correct chat_bot function return value hand‚Ä¶",
    "created_at": "2024-09-12T14:03:50Z",
    "closed_at": "2024-09-12T15:42:56Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/159",
    "body": "‚Ä¶ling",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/159/comments",
    "author": "notdp",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-12T15:43:05Z",
        "body": "tysm ‚ù§Ô∏è‚ù§Ô∏è"
      }
    ]
  },
  {
    "number": 155,
    "title": "Fix #154 : UnboundLocalError after setting autocommit to False",
    "created_at": "2024-09-12T01:42:53Z",
    "closed_at": "2024-09-13T04:50:34Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/155",
    "body": null,
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/155/comments",
    "author": "ninehills",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-09-13T04:50:34Z",
        "body": "i accidently solved this after seeing your branch :)) "
      }
    ]
  },
  {
    "number": 109,
    "title": "use datetime with timezone on backend (#108)",
    "created_at": "2024-08-05T15:06:26Z",
    "closed_at": "2024-08-06T05:23:47Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/109",
    "body": null,
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/109/comments",
    "author": "alex-dixon",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-08-05T17:45:01Z",
        "body": "i believe you will have ot change the front end too. I do stuff like: `{date}` + Z thorughout the code can you double check this?"
      },
      {
        "user": "alex-dixon",
        "created_at": "2024-08-06T05:08:07Z",
        "body": "@MadcowD should be ready"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-08-06T05:23:55Z",
        "body": "Woo üçæ "
      }
    ]
  },
  {
    "number": 103,
    "title": "add structured example",
    "created_at": "2024-08-04T15:06:25Z",
    "closed_at": "2024-08-04T23:43:04Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/103",
    "body": null,
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/103/comments",
    "author": "alex-dixon",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-08-04T23:41:03Z",
        "body": "Oh wow this is interesting :) "
      }
    ]
  },
  {
    "number": 14,
    "title": "fix build hook",
    "created_at": "2024-07-31T19:29:16Z",
    "closed_at": "2025-01-18T20:08:52Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/14",
    "body": "`poetry version -s` uses the value in `pyproject.toml` `tool.version` but with dynamic versioning (allowing us to pass in the version to the package itself) means it will key off of a git tag. Having git tags is nice in its own right, but then it also rewrites the default behavior of `poetry version` so explicitly looking in `pyproject.toml` and updating the git version before build/install.\r\n",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/14/comments",
    "author": "chrisaddy",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-07-31T19:31:47Z",
        "body": "will this overwrite tags if i build twice?"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-08-04T02:50:00Z",
        "body": "@chrisaddy ?"
      }
    ]
  },
  {
    "number": 11,
    "title": "expand python support",
    "created_at": "2024-07-30T23:23:51Z",
    "closed_at": "2024-07-30T23:40:11Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/pull/11",
    "body": null,
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/11/comments",
    "author": "chrisaddy",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-07-30T23:40:11Z",
        "body": ":( "
      }
    ]
  },
  {
    "number": 3,
    "title": "Cannot import FilesystemSerializer",
    "created_at": "2024-07-27T01:02:09Z",
    "closed_at": "2024-07-27T21:45:02Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/3",
    "body": "```\r\nell/examples/serialization_example.py\r\nTraceback (most recent call last):\r\n  File \"d:\\dev\\ell\\examples\\serialization_example.py\", line 7, in <module>\r\n    from ell.stores.jsonl import FilesystemSerializer\r\nImportError: cannot import name 'FilesystemSerializer' from 'ell.stores.jsonl' (D:\\dev\\ell\\ell\\src\\ell\\stores\\jsonl.py)\r\n```",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/3/comments",
    "author": "tianyaohu",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-07-27T18:29:08Z",
        "body": "Yep, we use SQLiteStore now. Sorry for all the dust I'm gonna do some clean up today!"
      },
      {
        "user": "tianyaohu",
        "created_at": "2024-07-27T20:09:53Z",
        "body": "Just pulled the latest update from main.\r\na826283 (HEAD -> main, origin/main, origin/HEAD) kind of working setup.py\r\n\r\nNow most of the examples are complaining about ell.stores.jsonl\r\n```\r\n(ell_lab) D:\\dev\\ell>D:/anaconda/envs/ell_lab/python.exe d:/dev/ell/examples/lexical_closure_example.py\r\nTraceback (most recent call last):\r\n  File \"d:\\dev\\ell\\examples\\lexical_closure_example.py\", line 1, in <module>\r\n    from ell.util.closure import lexically_closured_source\r\nModuleNotFoundError: No module named 'ell.util.closure'\r\n\r\n\r\n#jokes, hello world all shows this.\r\n(ell_lab) D:\\dev\\ell>D:/anaconda/envs/ell_lab/python.exe d:/dev/ell/examples/multilmp.py\r\nTraceback (most recent call last):\r\n  File \"d:\\dev\\ell\\examples\\multilmp.py\", line 4, in <module>\r\n    from ell.stores.sql import SQLiteStore\r\nModuleNotFoundError: No module named 'ell.stores.sql'\r\n```"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-07-27T20:27:56Z",
        "body": "on it"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-07-27T20:29:00Z",
        "body": "we swtiched off of poetry:\r\n```\r\npip uninstall ell\r\npip install -e .\r\n```\r\nin the main directory"
      }
    ]
  },
  {
    "number": 1,
    "title": "Model is not registered",
    "created_at": "2024-07-26T19:34:10Z",
    "closed_at": "2024-07-26T19:49:56Z",
    "labels": [],
    "url": "https://github.com/MadcowD/ell/issues/1",
    "body": "### Description\r\n\r\nclient_example.py works. It uses \"gpt-4o\", and correctly produce output in terminal and ell.db\r\n\r\nAll other examples does not work, showing same error, despite having key in C:\\\\User\\\\.oaikey\r\n`    raise ValueError(f\"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.\")\r\nValueError: No client found for model 'gpt-4o'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.`\r\n\r\nTested with my own script and it is not the openai lib not working.]\r\n\r\n### Steps to Reproduce\r\n\r\n1. ```- pip install dill\r\n- pip install sqlmodel\r\n- pip install cattrs\r\n- pip install uvicorn\r\n- pip install fastapi```\r\n2.   sl-XXXXXXXXXXXXXXXXXXXXX > C:\\\\User\\\\.oaikey\r\n\r\n### Expected Behavior\r\n\r\nAll examples should produce the nice looking output\r\n\r\n### Actual Behavior\r\n\r\n(ell_lab) D:\\dev\\ell>D:/anaconda/envs/ell_lab/python.exe d:/dev/ell/examples/joke.py\r\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\r\n‚ïë come_up_with_a_premise_for_a_joke_about(minecraf..) # (notimple...)\r\n‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\r\n‚ïë Prompt:\r\n‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢\r\n‚îÇ      system: You are an incredibly talented comedian. Come up with a premise for a joke about topic\r\n‚îÇ\r\n‚îÇ        user: come up with a premise for a joke about minecraft\r\nModel 'gpt-4o' is not registered. Falling back to OpenAI client from environment variables.\r\nTraceback (most recent call last):\r\n  File \"d:\\dev\\ell\\examples\\joke.py\", line 30, in <module>\r\n    joke(\"minecraft\") # <The joke>\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"D:\\dev\\ell\\ell\\src\\ell\\decorators.py\", line 190, in wrapper\r\n    else fn(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\dev\\ell\\ell\\src\\ell\\decorators.py\", line 142, in wrapper\r\n    res = fn(*fn_args, **fn_kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"d:\\dev\\ell\\examples\\joke.py\", line 24, in joke\r\n    return f\"Act out a full joke. Make your script {get_random_length()} words long. Here's the premise: {come_up_with_a_premise_for_a_joke_about(topic)}\"\r\n                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\dev\\ell\\ell\\src\\ell\\decorators.py\", line 190, in wrapper\r\n    else fn(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\dev\\ell\\ell\\src\\ell\\decorators.py\", line 148, in wrapper\r\n    tracked_str = _run_lm(**_invocation_kwargs, _invocation_origin=_invocation_origin, _logging_color=color)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\dev\\ell\\ell\\src\\ell\\decorators.py\", line 77, in _run_lm\r\n    raise ValueError(f\"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.\")\r\nValueError: No client found for model 'gpt-4o'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.\r\n\r\n\r\n###  My own Working Test Script:\r\n```\r\nimport os\r\n\r\n# Get the home directory path\r\nhome_directory = os.path.expanduser(\"~\")\r\n\r\n# Construct the full path to the .oaikey file\r\noaikey_path = os.path.join(home_directory, '.oaikey')\r\n\r\n# Read the API key from the file\r\ntry:\r\n    with open(oaikey_path, 'r') as file:\r\n        api_key = file.read().strip()\r\nexcept FileNotFoundError:\r\n    raise Exception(f\"API key file not found at {oaikey_path}. Please ensure it exists and the path is correct.\")\r\n\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(\r\n  api_key=api_key,\r\n)\r\n# Use the OpenAI API with the correct model name\r\n\r\ncompletion = client.chat.completions.create(\r\n  model=\"gpt-4o-mini\",\r\n  messages=[\r\n    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair. Use less than 100 tokens\"},\r\n    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\r\n  ]\r\n)\r\n\r\n\r\nprint(completion.choices[0].message)\r\n```",
    "comments_url": "https://api.github.com/repos/MadcowD/ell/issues/1/comments",
    "author": "tianyaohu",
    "comments": [
      {
        "user": "MadcowD",
        "created_at": "2024-07-26T19:42:19Z",
        "body": "You need to set your environment api key variable for the other examples to work:\r\n```\r\nimport os\r\nos.environ['OPENAI_API_KEY'] = 'sk-...'\r\n```\r\nOr do it in Windows wherever that is"
      },
      {
        "user": "MadcowD",
        "created_at": "2024-07-26T19:49:56Z",
        "body": "- [ ] Make the erorr message more clear so that you actually go and and the API key!"
      },
      {
        "user": "tianyaohu",
        "created_at": "2024-07-26T19:52:02Z",
        "body": "Windows needs to have OPENAI_API_KEY env\r\nfrom openai official doc.\r\nopen cmd terminal\r\n```\r\nsetx OPENAI_API_KEY \"your-api-key-here\"\r\n#verify\r\necho %OPENAI_API_KEY%\r\n```\r\nReopen your IDE VScode  or cursor"
      }
    ]
  }
]