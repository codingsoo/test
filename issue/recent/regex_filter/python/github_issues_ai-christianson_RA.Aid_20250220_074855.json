[
  {
    "number": 79,
    "title": "o3-mini Bug 'temperature' is not supported with this model.\" with Tool Calls",
    "created_at": "2025-02-01T21:20:27Z",
    "closed_at": "2025-02-02T23:57:29Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/ai-christianson/RA.Aid/issues/79",
    "body": "Got this error call, not sure which tool it was trying to use, but my args included: `--provider \"openai\" --model \"o3-mini\"`\n```\n╭──────────────────────────────────────── ❌ Tool Error ────────────────────────────────────────╮\n│ Error executing code: Error code: 400 - {'error': {'message': \"Unsupported parameter:         │\n│ 'temperature' is not supported with this model.\", 'type': 'invalid_request_error', 'param':   │\n│ 'temperature', 'code': 'unsupported_parameter'}}                                              │\n╰────────────────────────────────────────────────────────────────────────────────────────────\n```",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/79/comments",
    "author": "ariel-frischer",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-02-01T21:42:33Z",
        "body": "We have the new `supports_temperature` model parameter so theoretically this should have been fixed.\n\nIt was working on my machine but clearly there's a bug somewhere."
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-02-02T14:48:39Z",
        "body": "@ariel-frischer what commit are you testing on? I just tested on the latest commit `c14fad6d1410272cdea537b637bbd8331b7a0ee7` and got this:\n\n```\n(ra-aid) ~/workspace/ra-aid master $ ra-aid --provider openai --model o3-mini -m 'run the test suite'                                                                        \n/home/user/workspace/ra-aid/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\n╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│🔎 Research Stage                                                                                                                                                             │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭───────────────────────────────────────────────────────────────────────────── 📊 Project Status ──────────────────────────────────────────────────────────────────────────────╮\n│ Existing project with 108 file(s)                                                                                                                                            │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭────────────────────────────────────────────────────────────────────────────────── 🐚 Shell ──────────────────────────────────────────────────────────────────────────────────╮\n│ pytest --maxfail=1 --disable-warnings -q                                                                                                                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\nExecute this command? (y=yes, n=no, c=enable cowboy mode for session) [y/n/c] (y): c\n\n Yippee ki yay! 🤠\n\n\n........................................................................................................................................................................ [ 55%]\n.....................................................................................................................................                                    [100%]\n301 passed, 7 warnings in 3.04s\n\n╭───────────────────────────────────────────────────────────────────────────── ✅ Task Completed ──────────────────────────────────────────────────────────────────────────────╮\n│ All tests passed successfully; no further actions required.                                                                                                                  │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```"
      },
      {
        "user": "ariel-frischer",
        "created_at": "2025-02-02T21:49:37Z",
        "body": "I do think it was the latest commit but not 100% sure on that, could have been an older one, I was running swe-benchmarks. The weird thing I noticed is that it was on some tool call, but not on most? I guess we can close until I see this again. I'm gonna be avoiding o3-mini for now, it just doesn't seem to work well in this agentic system. But I will use it for aider coding portion."
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-02-02T21:51:23Z",
        "body": "> I do think it was the latest commit but not 100% sure on that, could have been an older one, I was running swe-benchmarks. The weird thing I noticed is that it was on some tool call, but not on most? I guess we can close until I see this again. I'm gonna be avoiding o3-mini for now, it just doesn't seem to work well in this agentic system. But I will use it for aider coding portion.\n\nExpert tool by chance?"
      },
      {
        "user": "ariel-frischer",
        "created_at": "2025-02-02T21:53:54Z",
        "body": "That was the error message, so there is no indicator it was expert tool. So I really don't know."
      },
      {
        "user": "ariel-frischer",
        "created_at": "2025-02-02T21:54:07Z",
        "body": "If it was expert tool it should have been o1."
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-02-02T23:45:53Z",
        "body": "I got this just now:\n\n```\n~/workspace/browser-use main $ ~/workspace/ra-aid/.venv/bin/ra-aid --provider openai --model o3-mini -m 'is there a way for users of this library to disable creation of new tabs (i.e. reuse existing tabs if possible)?'\n/home/user/workspace/ra-aid/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\n╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│🔎 Research Stage                                                                                                                                                             │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭───────────────────────────────────────────────────────────────────────────── 📊 Project Status ──────────────────────────────────────────────────────────────────────────────╮\n│ Existing project with 145 file(s)                                                                                                                                            │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭────────────────────────────────────────────────────────────────────────────── 🤔 Expert Query ───────────────────────────────────────────────────────────────────────────────╮\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n│ ┃                                                                                 Question                                                                                 ┃ │\n│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │\n│                                                                                                                                                                              │\n│ Could you please clarify what exactly you are asking? Do you want to know if there's a configuration option to disable the creation of new tabs and have the library reuse   │\n│ existing tabs, or are you looking for guidance on implementing such a behavior?                                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─────────────────────────────────────────────────────────────────────────────── ❌ Tool Error ────────────────────────────────────────────────────────────────────────────────╮\n│ Error executing code: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'temperature' is not supported with this model.\", 'type': 'invalid_request_error',     │\n│ 'param': 'temperature', 'code': 'unsupported_parameter'}}                                                                                                                    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n```"
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-02-02T23:53:31Z",
        "body": "Fixed this in the latest commit. Will build a minor release since this bug would affect everyone on the current version."
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-02-02T23:57:29Z",
        "body": "Will close the issue for now since I think it's fixed. @ariel-frischer if it pops back up, we can reopen this."
      }
    ]
  },
  {
    "number": 64,
    "title": "FIX prevent duplicate files to add to the context.",
    "created_at": "2025-01-29T03:46:35Z",
    "closed_at": "2025-01-29T19:48:02Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/64",
    "body": "I found the list of files can include duplicates during long iterations.\r\nThis change is to ensure there are unique files only.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/64/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-29T19:47:57Z",
        "body": "Awesome, I def had this problem before. Thanks for the contribution :+1: \r\n\r\nAlso, nice to see the \"All checks have passed.\" PR workflow working!"
      }
    ]
  },
  {
    "number": 59,
    "title": "REFACTOR handle user defined test cmd into a class",
    "created_at": "2025-01-27T01:50:27Z",
    "closed_at": "2025-01-27T13:41:02Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/59",
    "body": "Refactor from a series of functions into a class for extensibility. Two PRs, #59 and #60, are options to refactor those functions. This PR is straight forward refactored, while #60 applies the strategy pattern.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/59/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-27T13:40:59Z",
        "body": "Thanks for the contribution :+1: "
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-01-27T13:43:38Z",
        "body": "KISS approach is good :smile: "
      }
    ]
  },
  {
    "number": 58,
    "title": "FEAT Run tests during Github CICD",
    "created_at": "2025-01-25T23:55:59Z",
    "closed_at": "2025-01-27T13:40:21Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/58",
    "body": "Tests now work during PR creation. Fixes #54 ",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/58/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-27T13:40:17Z",
        "body": "Thanks for the contribution! :+1: "
      }
    ]
  },
  {
    "number": 53,
    "title": "Added Provider/Model Override Arguments to Seperate Research/Planner Configurations",
    "created_at": "2025-01-24T22:32:17Z",
    "closed_at": "2025-01-24T23:00:47Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/53",
    "body": "# Added Provider/Model Override Arguments\r\n\r\n## Changes\r\n\r\n- Added new CLI arguments to override provider/model for specific agent types:\r\n  - `--research-provider` and `--research-model` for research agent\r\n  - `--planner-provider` and `--planner-model` for planning agent\r\n  - Updated README.md to document new arguments\r\n  - These override the default provider/model settings when specified\r\n\r\n- Added more debug logging\r\n- Added tests\r\n- Added uv.lock",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/53/comments",
    "author": "ariel-frischer",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-24T23:00:44Z",
        "body": "Thanks for the contribution :+1: "
      },
      {
        "user": "ariel-frischer",
        "created_at": "2025-01-25T00:26:19Z",
        "body": "Whoops, after fixing the merge conflicts it seems some tests started failing. Related to `temperature` settings expected. Will need to fix that, and for the future github-ci to run all tests would be great."
      }
    ]
  },
  {
    "number": 51,
    "title": "Integrate litellm to retrieve model token limits",
    "created_at": "2025-01-23T19:44:08Z",
    "closed_at": "2025-01-23T19:48:30Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/51",
    "body": "- Use litellm to get `max_input_tokens` by default.\r\n- Fallback to using our LLM dict for missing/new models like `deepseek-reasoner`.\r\n- fix(agent_utils.py): rename max_tokens to max_input_tokens for clarity in state_modifier function\r\n- fix(models_tokens.py): update deepseek-reasoner token limit to 64000 for accuracy\r\n- test(agent_utils.py): add tests for litellm integration and fallback logic in get_model_token_limit function",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/51/comments",
    "author": "ariel-frischer",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-23T19:48:25Z",
        "body": "Looks good. Thank you for the contribution :+1: "
      }
    ]
  },
  {
    "number": 50,
    "title": "Add Deepseek Provider Support and Custom Deepseek Reasoner Chat Model",
    "created_at": "2025-01-22T02:14:35Z",
    "closed_at": "2025-01-22T12:21:10Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/50",
    "body": "- Add Deepseek Provider Support with fallback to OpenAIChat model if model is not the `reasoner` model.\r\n- Add custom `ChatDeepseekReasoner` chat model which forcefully merges messages until they follow strict alternation policy enforced by deepseek API. It also removes unsupported model params.\r\n- Refactor `ra_aid/llm.py` for readability + less code duplication.\r\n- Add `DeepSeekStrategy`\r\n- Add/refactor llm.py tests\r\n- Update README.md with correct deepseek model names.\r\n\r\nFixes: #48 ",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/50/comments",
    "author": "ariel-frischer",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-22T12:21:04Z",
        "body": "Looks good! Thanks for the contribution :+1: "
      }
    ]
  },
  {
    "number": 48,
    "title": "Support for Deepseek R1",
    "created_at": "2025-01-21T06:35:23Z",
    "closed_at": "2025-01-22T12:21:11Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/issues/48",
    "body": "Deepseek R1 seems hyped right now, doing well on many benchmarks and is quite cheap compared to alternatives like o1 and sonnet 3.5. This model seems like a game changer but its throwing errors in `ra-aid` agents due to `invalid_request_error`. It doesn't support \"successive user or assistant messages\". Lets try to get it working if possible.\n\nFull error logs:\n\n```\n2025-01-20 22:27:35,761 - ra_aid.ra_aid.agent_utils - ERROR - Research agent failed: {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': {'__kind': 'OK', 'data': '{\"error\":{\"message\":\"deepseek-reasoner does not support successive user or assistant messages (messages[1] and messages[2] in your input). You should interleave the user/assistant messages in the message sequence.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":\"invalid_request_error\"}}'}, 'provider_name': 'DeepSeek'}}\nTraceback (most recent call last):\n  File \"/home/ari/repos/RA.Aid/ra_aid/agent_utils.py\", line 363, in run_research_agent\n    return run_agent_with_retry(agent, prompt, run_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/ra_aid/agent_utils.py\", line 696, in run_agent_with_retry\n    for chunk in agent.stream(\n  File \"/home/ari/repos/RA.Aid/ra_aid/agents/ciayn_agent.py\", line 321, in stream\n    response = self.model.invoke([SystemMessage(\"Execute efficiently yet completely as a fully autonomous agent.\")] + full_history)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\n    self.generate_prompt(\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 790, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 647, in generate\n    raise e\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 637, in generate\n    self._generate_with_cache(\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 855, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 791, in _generate\n    return self._create_chat_result(response, generation_info)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 825, in _create_chat_result\n    raise ValueError(response_dict.get(\"error\"))\nValueError: {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': {'__kind': 'OK', 'data': '{\"error\":{\"message\":\"deepseek-reasoner does not support successive user or assistant messages (messages[1] and messages[2] in your input). You should interleave the user/assistant messages in the message sequence.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":\"invalid_request_error\"}}'}, 'provider_name': 'DeepSeek'}}\nTraceback (most recent call last):\n  File \"/home/ari/repos/RA.Aid/.venv/bin/ra-aid\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/ari/repos/RA.Aid/ra_aid/__main__.py\", line 345, in main\n    run_research_agent(\n  File \"/home/ari/repos/RA.Aid/ra_aid/agent_utils.py\", line 363, in run_research_agent\n    return run_agent_with_retry(agent, prompt, run_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/ra_aid/agent_utils.py\", line 696, in run_agent_with_retry\n    for chunk in agent.stream(\n  File \"/home/ari/repos/RA.Aid/ra_aid/agents/ciayn_agent.py\", line 321, in stream\n    response = self.model.invoke([SystemMessage(\"Execute efficiently yet completely as a fully autonomous agent.\")] + full_history)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\n    self.generate_prompt(\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 790, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 647, in generate\n    raise e\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 637, in generate\n    self._generate_with_cache(\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 855, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 791, in _generate\n    return self._create_chat_result(response, generation_info)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ari/repos/RA.Aid/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 825, in _create_chat_result\n    raise ValueError(response_dict.get(\"error\"))\nValueError: {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': {'__kind': 'OK', 'data': '{\"error\":{\"message\":\"deepseek-reasoner does not support successive user or assistant messages (messages[1] and messages[2] in your input). You should interleave the user/assistant messages in the message sequence.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":\"invalid_request_error\"}}'}, 'provider_name': 'DeepSeek'}}\n```",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/48/comments",
    "author": "ariel-frischer",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-21T16:27:04Z",
        "body": "👍 would love to support this model. \n\nWhat provider are you using for the model? Open router? Deepseek API?"
      },
      {
        "user": "ariel-frischer",
        "created_at": "2025-01-21T16:29:08Z",
        "body": "> 👍 would love to support this model. \n> \n> What provider are you using for the model? Open router? Deepseek API?\n\nI'm using openrouter."
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-01-21T16:43:43Z",
        "body": "Have you tried with the Deepseek API directly? Sometimes I find that to work better. "
      },
      {
        "user": "ariel-frischer",
        "created_at": "2025-01-21T16:47:53Z",
        "body": "Haven't tried that yet. I can sign up and test that out later today. I'm also thinking about having these reasoner models only be used in planner agent rather than the research agent. Seems overkill for research agent and would probably be very slow."
      }
    ]
  },
  {
    "number": 44,
    "title": "FIX Issue 42: Error with Levenshtein",
    "created_at": "2025-01-18T21:54:41Z",
    "closed_at": "2025-01-21T16:46:15Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/44",
    "body": "Fixes #42 \r\n\r\nThere was also an error seen during git push which needed to be fixed, hence the second commit.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/44/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-20T19:38:06Z",
        "body": "Looks good thank you for the contribution!"
      }
    ]
  },
  {
    "number": 43,
    "title": "Add Aider config File Argument Support",
    "created_at": "2025-01-18T17:43:47Z",
    "closed_at": "2025-01-21T18:17:59Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/43",
    "body": "# Add Aider Config File Argument Support\r\n\r\n## Changes\r\n- Added `--aider-config` argument to specify custom aider configuration file path\r\n- Enhanced programmer tool to include aider config in commands\r\n- Added unit tests for aider config flag functionality\r\n- Added pytest-mock to development requirements\r\n",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/43/comments",
    "author": "ariel-frischer",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-21T16:48:19Z",
        "body": "Looks good but we have a conflict now after merging the other PRs."
      },
      {
        "user": "ai-christianson",
        "created_at": "2025-01-21T18:17:55Z",
        "body": "Thanks for the contribution :+1: \r\n\r\nBtw, I added you and @leonj1 as collaborators on the project, so you all can review/merge work if you want."
      }
    ]
  },
  {
    "number": 37,
    "title": "Changes",
    "created_at": "2025-01-09T13:43:50Z",
    "closed_at": "2025-01-09T14:46:32Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/37",
    "body": "- Adds a LLM call and a regex to extract correct tool usages from incorrect llm responses\r\n- Adds support for `.aiderignore` files for directory listing\r\n- Fixes an issues where `ripgrep` was called with invalid file types. E.g. the filetype for `rust` in `ripgrep` is `rust` not `rs`",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/37/comments",
    "author": "terhechte",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-09T14:46:28Z",
        "body": ":+1: Thanks for the contribution! Tried it out with `qwen/qwen-2.5-coder-32b-instruct` and it worked pretty well."
      }
    ]
  },
  {
    "number": 34,
    "title": "Adding Gemini API due to openrouter's limitations.",
    "created_at": "2025-01-04T07:40:55Z",
    "closed_at": "2025-01-04T12:19:05Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/34",
    "body": "Pull Request: Add Gemini Support and Test Improvements\r\nSummary\r\nThis PR adds Google's Gemini AI model support and includes minor fixes to the OpenAI model default naming.\r\nKey Changes\r\nAdded Gemini provider support in ra_aid/llm.py:\r\nNew ChatGoogleGenerativeAI integration\r\nEnvironment variable handling for GEMINI_API_KEY and EXPERT_GEMINI_API_KEY\r\nFixed OpenAI model naming:\r\nUpdated default model name to use o1 instead of previous naming\r\nEnhanced test coverage in tests/ra_aid/test_llm.py:\r\nAdded Gemini initialization tests\r\nAdded test cases for expert LLM initialization\r\nImproved token estimation tests\r\nAdded Gemini validation strategy in provider_strategy.py\r\nTesting\r\nAdded comprehensive test coverage for Gemini provider initialization\r\nEnhanced test cases for expert LLM configuration\r\nIncluded validation tests for Gemini API keys\r\nThis is a focused change that expands the supported LLM providers while maintaining the existing architecture.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/34/comments",
    "author": "arthrod",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-04T12:18:57Z",
        "body": "Thanks for the contribution 👍!"
      }
    ]
  },
  {
    "number": 31,
    "title": "Add Ollama Support via Litellm for Local LLMs (Fixes #20)",
    "created_at": "2025-01-02T21:37:00Z",
    "closed_at": "2025-01-02T21:41:02Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/31",
    "body": "# Pull Request Description\n\n## Overview\n\nThis pull request addresses issue #20, which requests support for local LLMs using `litellm` and `ollama`. The enhancements made in this PR introduce the capability to utilize local LLMs within the existing structure, expanding the functionality of the coding assistant.\n\n**Fixes #20**\n\n## Summary of Changes\n\n1. **Dependency Addition**: \n   - Integrated `litellm` into the project dependencies by updating `pyproject.toml`. This ensures that the necessary package is available for use with Ollama.\n\n2. **New Strategy Class**: \n   - Implemented the `OllamaStrategy` class in `provider_strategy.py`. This new class handles the specifics of invoking `ollama`, allowing for better modularity and clarity in the codebase.\n\n3. **LLM Provider Handling**:\n   - Updated the `llm.py` file to include the new `OllamaStrategy`. This modification enables the assistant to recognize and properly instantiate this strategy when prompted by user input.\n\n4. **Provider Factory Update**:\n   - Enhanced the provider factory to ensure seamless integration of the new Ollama strategy. This addition guarantees that the assistant correctly routes LLM requests to the appropriate implementation based on the user's selection.\n\n## Background\n\nPrior to these changes, the project primarily supported LLMs from providers such as OpenAI, Anthropic, and OpenRouter. The proposal to add `litellm` and `ollama` was driven by the need for local LLM support, which would allow users to leverage local resources rather than relying solely on cloud-based models.\n\n## Considerations\n\n- **Environment Variables**: It is crucial to note that certain features (like Expert Tools and Web Research) remain disabled due to the lack of required environment variables. Users should set `EXPERT_OPENAI_API_KEY` for expert tools and `TAVILY_API_KEY` for web research to ensure full functionality.\n  \n- **Testing**: While modifications have been made to incorporate local LLM support, thorough testing is essential to validate the integration of `ollama` and confirm that it works as expected in conjunction with other providers.\n\n## Next Steps\n\n1. Review the changes for integration completeness.\n2. Test the new capabilities introduced with local LLMs for expected behavior.\n3. Set the necessary environment variables to unlock all features as discussed.\n\nBy implementing these changes, we aim to significantly enhance the coding assistant's versatility and cater to users who prefer a local LLM setup with `litellm` and `ollama`.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/31/comments",
    "author": "minimalProviderAgentMarket",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-02T21:40:54Z",
        "body": "Thank you for the contribution @agentmarketbot!"
      }
    ]
  },
  {
    "number": 29,
    "title": "Add default support for OpenAI models",
    "created_at": "2024-12-31T20:46:07Z",
    "closed_at": "2025-01-01T14:34:51Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/29",
    "body": "Small change to will help users that want to rely on OpenAI. Defaults provider to `openai` if the user has set up the Open AI API key. It also makes a default OpenAI model.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/29/comments",
    "author": "guillermocreus",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-01T14:34:49Z",
        "body": "Thanks for the contribution :+1: "
      }
    ]
  },
  {
    "number": 27,
    "title": "FEAT add function to check for dependencies on startup",
    "created_at": "2024-12-31T18:52:58Z",
    "closed_at": "2025-01-01T14:35:41Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/27",
    "body": "Added function to check for dependencies such as 'ripgrep'.\r\nThis was implemented using an interface to make it extensible and making it a way to add dependencies in the future.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/27/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2025-01-01T14:35:37Z",
        "body": "Good idea, thanks for the contribution! :+1: "
      }
    ]
  },
  {
    "number": 26,
    "title": "Separate env vars for OPENAI_API_KEY so openai can be used for expert and openai-compatible for main model",
    "created_at": "2024-12-30T22:45:17Z",
    "closed_at": "2025-02-18T02:55:17Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/ai-christianson/RA.Aid/issues/26",
    "body": "If we are using an openai-compatible endpoint for the main agent model, we currently cannot use openai for the expert model because both are trying to use the `OPENAI_API_KEY` env var.\r\n\r\nA possible solution is to use `OPENAI_COMPATIBLE_API_KEY` as the env var for openai-compatible providers.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/26/comments",
    "author": "ai-christianson",
    "comments": [
      {
        "user": "leonj1",
        "created_at": "2024-12-31T01:51:24Z",
        "body": "How about a dedicated EXPERT_OPENAI_API_KEY ?"
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-31T19:51:58Z",
        "body": "Good point :smile:. We already have that capability in there. It's a bit confusing from a UX perspective in that it falls back, unless you are using openai-compatible for the main model, then you don't want fallback if you're actually using openai."
      }
    ]
  },
  {
    "number": 25,
    "title": "FIX SyntaxError: f-string expression part cannot include a backslash",
    "created_at": "2024-12-30T05:10:22Z",
    "closed_at": "2024-12-30T13:45:21Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/25",
    "body": "1. Fix broken unit tests\r\n2. Establish pre-commit hook to run tests prior to git push\r\n3. FIx the reported error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/codespace/.python/current/bin/ra-aid\", line 5, in <module>\r\n    from ra_aid.__main__ import main\r\n  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/ra_aid/__init__.py\", line 5, in <module>\r\n    from .agent_utils import run_agent_with_retry\r\n  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/ra_aid/agent_utils.py\", line 14, in <module>\r\n    from ra_aid.agents.ciayn_agent import CiaynAgent\r\n  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/ra_aid/agents/ciayn_agent.py\", line 133\r\n    Output **ONLY THE CODE** and **NO MARKDOWN BACKTICKS**\"\"\"\r\n                                                             ^\r\nSyntaxError: f-string expression part cannot include a backslash \r\n```",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/25/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-30T13:44:58Z",
        "body": "Awesome thanks for the contribution :+1: "
      }
    ]
  },
  {
    "number": 23,
    "title": "macOS compatibility",
    "created_at": "2024-12-29T17:33:21Z",
    "closed_at": "2024-12-30T17:38:07Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/23",
    "body": "macOS Compatibility",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/23/comments",
    "author": "willbonde",
    "comments": [
      {
        "user": "willbonde",
        "created_at": "2024-12-29T21:44:09Z",
        "body": "Added filtering out of the mystery control character appearing in the tests on macOS. While the test failed (albeit to no \"real\" issue), it could be a hindrance to other devs"
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-29T22:49:34Z",
        "body": "Left some comments. This one is tricky since it requires both a mac and a linux to fully test.\r\n\r\nWe'll need some conditionals on a couple of them, e.g. branching depending on `script --version`.\r\n\r\nFeel free to address, or I can just merge and add the conditionals on master.\r\n\r\nThanks for the PR though! :+1: "
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-30T16:57:12Z",
        "body": "Awesome thanks for the contribution! :+1: \r\n\r\nI think it has a minor conflict then I can merge it."
      }
    ]
  },
  {
    "number": 20,
    "title": "ollama support using litellm",
    "created_at": "2024-12-27T18:50:16Z",
    "closed_at": "2025-01-02T21:41:03Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/issues/20",
    "body": "Hi,\r\nIs it possible to support local LLMs using litellm and ollama? Thank you.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/20/comments",
    "author": "ck-amrahd",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-27T19:07:38Z",
        "body": "Local LLMs are currently supported using the openai-compatible provider. This should work with litellm, but we don't have litellm or ollama built-in at this point."
      },
      {
        "user": "leonj1",
        "created_at": "2024-12-28T00:27:40Z",
        "body": "Does this change need to happen if it is working as-is today using the openai-compatible argument?"
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-28T16:54:46Z",
        "body": "> Does this change need to happen if it is working as-is today using the openai-compatible argument?\r\n\r\nRight, that's kind of how I saw it initially --we should be 100% compatible with litellm as-is. I think the only thing we could potentially add is running our own litellm proxy internally, but I'm not sure how significant of a UX improvement that'd be."
      }
    ]
  },
  {
    "number": 19,
    "title": "If Anthropic key is not set in environment an error is raised even if another provider/model is specified",
    "created_at": "2024-12-27T09:51:19Z",
    "closed_at": "2024-12-28T21:53:58Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/issues/19",
    "body": "1. validate_environment Function:\r\n\r\nThe validate_environment function in env.py is designed to check for the presence of necessary API keys before the command-line arguments are fully processed.\r\n\r\nIt starts with the assumption that you might be using Anthropic because it checks for the ANTHROPIC_API_KEY first.\r\n\r\nif provider in PROVIDER_CONFIGS:\r\n    config = PROVIDER_CONFIGS[provider]\r\n    if config.base_required and not os.environ.get(config.key_name):\r\n        missing.append(f'{config.key_name} environment variable is not set')\r\n\r\nPython\r\nIn this code, if provider is set to anthropic and ANTHROPIC_API_KEY is not found, it will add the error message to the missing list.\r\n\r\nThe code does not take into consideration the values of --provider and --model specified on the command line at this stage. It's only checking environment variables based on a hardcoded priority (Anthropic first).\r\n\r\nLater in the function, it checks if there are any missing keys and, if so, exits with an error:\r\n\r\nif missing:\r\n    print_error(\"Missing required dependencies:\")\r\n    for item in missing:\r\n        print_error(f\"- {item}\")\r\n    sys.exit(1)\r\n\r\nPython\r\n2. Order of Operations:\r\n\r\nThe core issue is the order in which things happen:\r\n\r\nEnvironment variable validation (validate_environment) happens before full command-line argument parsing.\r\n\r\nThe default provider is assumed to be Anthropic unless modified later by command-line arguments.\r\n\r\n3. Impact:\r\n\r\nEven if I provide --provider openai or --provider openrouter on the command line, the validate_environment function will still check for ANTHROPIC_API_KEY first and raise an error if it's not found.\r\n\r\nIn essence, the current code has an implicit dependency on the ANTHROPIC_API_KEY being set, even when you intend to use another provider.\r\n\r\nProposed Solutions (as mentioned in the issue draft):\r\n\r\nPrioritize Command-line Arguments: Modify validate_environment to respect the --provider flag first, if it's provided. Only check for ANTHROPIC_API_KEY if --provider is not set or is explicitly set to anthropic.\r\n\r\nEnvironment Variable Defaults: Implement the RA_AID_DEFAULT_PROVIDER and RA_AID_DEFAULT_MODEL environment variables as I described earlier. This would change the default behavior and only require ANTHROPIC_API_KEY if the user doesn't set defaults and doesn't use the --provider flag.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/19/comments",
    "author": "zzt108",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-27T13:14:40Z",
        "body": "Which version/commit is this?"
      },
      {
        "user": "zzt108",
        "created_at": "2024-12-27T15:09:22Z",
        "body": "ra-aid                    0.10.1\r\n\r\nI installed this morning with PIP"
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-27T19:24:09Z",
        "body": "Testing this out locally:\r\n\r\n```\r\n(ratest-2024-12-17) ~/tmp/ratest-2024-12-17 $ printenv | cut -d= -f1 | grep API_KEY\r\nOPENAI_API_KEY\r\n(ratest-2024-12-17) ~/tmp/ratest-2024-12-17 $ ra-aid --version                     \r\nra-aid 0.10.1\r\n(ratest-2024-12-17) ~/tmp/ratest-2024-12-17 $ ra-aid --provider openai --model gpt-4o --chat\r\n╭─────────────────────────────────────────────────────────────────────────── Web Research Disabled ────────────────────────────────────────────────────────────────────────────╮\r\n│ Web research disabled due to missing configuration:                                                                                                                          │\r\n│ - TAVILY_API_KEY environment variable is not set                                                                                                                             │\r\n│ Set the required environment variables to enable web research.                                                                                                               │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│🚀 Chat Mode                                                                                                                                                                  │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭─────────────────────────────────────────────────────────────────────────── 💭 Question for Human ────────────────────────────────────────────────────────────────────────────╮\r\n│ What would you like help with?                                                                                                                                               │\r\n│                                                                                                                                                                              │\r\n│ Multiline input is supported; use Ctrl+D to submit. Use Ctrl+C to exit the program.                                                                                          │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n\r\n>                                                                                                                                                                               \r\n\r\n 👋 Bye!\r\n\r\n(ratest-2024-12-17) ~/tmp/ratest-2024-12-17 $ ra-aid --provider openai --model gpt-4o -m 'write a hello world in c++'\r\n╭─────────────────────────────────────────────────────────────────────────── Web Research Disabled ────────────────────────────────────────────────────────────────────────────╮\r\n│ Web research disabled due to missing configuration:                                                                                                                          │\r\n│ - TAVILY_API_KEY environment variable is not set                                                                                                                             │\r\n│ Set the required environment variables to enable web research.                                                                                                               │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│🔎 Research Stage                                                                                                                                                             │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭───────────────────────────────────────────────────────────────────────────── ✅ Task Completed ──────────────────────────────────────────────────────────────────────────────╮\r\n│ Here's a simple \"Hello, World!\" program in C++:                                                                                                                              │\r\n│                                                                                                                                                                              │\r\n│                                                                                                                                                                              │\r\n│  #include <iostream>                                                                                                                                                         │\r\n│                                                                                                                                                                              │\r\n│  int main() {                                                                                                                                                                │\r\n│      std::cout << \"Hello, World!\" << std::endl;                                                                                                                              │\r\n│      return 0;                                                                                                                                                               │\r\n│  }                                                                                                                                                                           │\r\n│                                                                                                                                                                              │\r\n│                                                                                                                                                                              │\r\n│ This program includes the iostream library, which allows for input and output operations. The main function outputs \"Hello, World!\" to the console and returns 0 to indicate │\r\n│ successful execution.                                                                                                                                                        │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n^C2024-12-27 14:21:36,464 - ra_aid.ra_aid.agent_utils - ERROR - Research agent failed: Interrupt requested\r\nTraceback (most recent call last):\r\n  File \"/home/user/tmp/ratest-2024-12-17/.venv/lib/python3.12/site-packages/ra_aid/agent_utils.py\", line 167, in run_research_agent\r\n    return run_agent_with_retry(agent, prompt, run_config)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/tmp/ratest-2024-12-17/.venv/lib/python3.12/site-packages/ra_aid/agent_utils.py\", line 472, in run_agent_with_retry\r\n    check_interrupt()\r\n  File \"/home/user/tmp/ratest-2024-12-17/.venv/lib/python3.12/site-packages/ra_aid/agent_utils.py\", line 447, in check_interrupt\r\n    raise AgentInterrupt(\"Interrupt requested\")\r\nra_aid.exceptions.AgentInterrupt: Interrupt requested\r\n\r\n 👋 Bye!\r\n\r\n```\r\n\r\nCan you give it a try with the `--verbose` flag and paste the full log?"
      },
      {
        "user": "zzt108",
        "created_at": "2024-12-28T13:06:31Z",
        "body": "I redeployed on my home PC with the exact same scripts I used yesterday on my work PC. It works now with 0.10.3\r\n\r\nPS C:\\Git\\ra-aid> .\\run-ra-aid.ps1\r\nAPI Keys found: OPENAI_API_KEY OPENROUTER_API_KEY\r\nra-aid version: ra-aid 0.10.3\r\n╭───────────────────────────────────────────────────────────────────────────────────────── Web Research Disabled ─────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ Web research disabled due to missing configuration:                                                                                                                                                     │\r\n│ - TAVILY_API_KEY environment variable is not set                                                                                                                                                        │\r\n│ Set the required environment variables to enable web research.                                                                                                                                          │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│🚀 Chat Mode                                                                                                                                                                                             │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭───────────────────────────────────────────────────────────────────────────────────────── 💭 Question for Human ─────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ What would you like help with?                                                                                                                                                                          │\r\n│                                                                                                                                                                                                         │\r\n│ Multiline input is supported; use Ctrl+D to submit. Use Ctrl+C to exit the program.                                                                                                                     │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n\r\n>\r\n\r\n 👋 Bye!\r\n\r\nPS C:\\Git\\ra-aid> ra-aid --provider openai --model gpt-4o -m 'write a hello world in c++'\r\n╭───────────────────────────────────────────────────────────────────────────────────────── Web Research Disabled ─────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ Web research disabled due to missing configuration:                                                                                                                                                     │\r\n│ - TAVILY_API_KEY environment variable is not set                                                                                                                                                        │\r\n│ Set the required environment variables to enable web research.                                                                                                                                          │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│🔎 Research Stage                                                                                                                                                                                        │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭─────────────────────────────────────────────────────────────────────────────────────────── ✅ Task Completed ───────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ Here's a simple \"Hello, World!\" program in C++:                                                                                                                                                         │\r\n│                                                                                                                                                                                                         │\r\n│                                                                                                                                                                                                         │\r\n│  #include <iostream>                                                                                                                                                                                    │\r\n│                                                                                                                                                                                                         │\r\n│  int main() {                                                                                                                                                                                           │\r\n│      std::cout << \"Hello, World!\" << std::endl;                                                                                                                                                         │\r\n│      return 0;                                                                                                                                                                                          │\r\n│  }                                                                                                                                                                                                      │\r\n│                                                                                                                                                                                                         │\r\n│                                                                                                                                                                                                         │\r\n│ This program includes the iostream library, which allows for input and output operations. The main function is the entry point of the program, and it uses std::cout to print \"Hello, World!\" to the    │\r\n│ console. The std::endl is used to insert a newline character and flush the output buffer. Finally, the program returns 0, indicating successful execution.                                              │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭───────────────────────────────────────────────────────────────────────────────────────────── 🤖 Assistant ──────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ I've provided a simple \"Hello, World!\" program in C++.                                                                                                                                                  │\r\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nPS C:\\Git\\ra-aid> "
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-28T21:54:34Z",
        "body": "Thanks for the contribution :+1: "
      }
    ]
  },
  {
    "number": 10,
    "title": "FEAT add verbose logging",
    "created_at": "2024-12-25T23:41:46Z",
    "closed_at": "2024-12-26T02:40:58Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/10",
    "body": "Fixes #8\r\nAdded \"--verbose\" flag",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/10/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-26T02:40:52Z",
        "body": "👍 great contribution.\r\n\r\nWe will refine and tweak specific log statements over time but this is a great start and something we needed. "
      }
    ]
  },
  {
    "number": 5,
    "title": "FEAT support future aider flags",
    "created_at": "2024-12-23T15:56:45Z",
    "closed_at": "2024-12-24T00:39:12Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/5",
    "body": "Since Aider is constantly evolving and adding more flags, here is a proposal to allow the user to add desired flags without this code having to evolve lock step with Aider. \r\n\r\nHere is an example of some recent args added to aider that are currently halting Ra.Aid because they are prompting for user input:\r\n* --no-show-release-notes\r\n* --no-detect-urls\r\n\r\nI would have written a unit test for this, but where this function is being invoked is unclear. I suspect it's internal to Langgraph. Even if this merge request isn't merged, the intent is clear, allowing this project to avoid maintaining the aider arguments.\r\n",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/5/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-23T18:20:24Z",
        "body": "The idea makes sense and I think we should add it.\r\n\r\nWith your PR as-is, we're making the aider flags a parameter on the tool, which means that the agent will be deciding which flags to add, not the user.\r\n\r\nIn order for the user to have control over the flags, we'd want to allow it to be set via env var or CLI arguments and then add it to `_global_config` in `__main__`. Since it could be confusing to define CLI flags using CLI flags, we may want to go with the env var approach, e.g.:\r\n\r\n```\r\nAIDER_FLAGS=\"...\"\r\nra-aid --chat # starts RA.Aid and any time aider is called, it adds the additional AIDER_FLAGS.\r\n```"
      },
      {
        "user": "leonj1",
        "created_at": "2024-12-24T00:11:04Z",
        "body": "Figured it out. As requested, the tool is now fetching the values from the env var and parsing them. The change includes multiple scenarios, and the implementation is wrapped in a try-catch as a precaution."
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-24T00:16:24Z",
        "body": "Looks good to me 👍\r\n\r\nI can merge it assuming you're done making any edits, lmk."
      },
      {
        "user": "leonj1",
        "created_at": "2024-12-24T00:27:59Z",
        "body": "I'm done now.\r\nUpdated the README for completeness.\r\nThanks for this tool!\r\nI'm looking forward to showing it off.\r\nThe next PR will be about adding up all the tokens and cost usage (since I was surprised by the bill :) )."
      },
      {
        "user": "ai-christianson",
        "created_at": "2024-12-24T00:39:05Z",
        "body": "Thanks for the PR!\r\n\r\nAnd yeah token profiling/cost tracking would be nice. \r\n\r\nIt's sort of in the \"make it work\" phase. There's definitely opportunities to optimize token usage."
      }
    ]
  },
  {
    "number": 4,
    "title": "Add support for Ollama",
    "created_at": "2024-12-22T15:05:44Z",
    "closed_at": "2024-12-22T19:25:28Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/issues/4",
    "body": "This project is great, but I ran up my Claude bill during one session. Could Ollama support be added for local usage and keep costs down? A stretch goal could be to start with Ollama, but fall back to the current implementation when its unable to solve the problem.",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/4/comments",
    "author": "leonj1",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-22T17:08:52Z",
        "body": "I hear you about running up the claude bill. It should already support any openai-compatible endpoints including ollama. You would have to set --provider to openai-compatible and set OPENAI_API_BASE in the environment.\r\n\r\nIn my testing, I haven't been able to get any agent other than claude to work well, however. I'm not sure if it has to do with langgraph create_react_agent having specialized prompts for claude or not.\r\n\r\nMy hope is that over the next year or so, open models will catch up and become much more capable of driving agents like this."
      },
      {
        "user": "leonj1",
        "created_at": "2024-12-22T19:25:28Z",
        "body": "I see. Yea. Local Ollama is struggling. Thanks!"
      }
    ]
  },
  {
    "number": 2,
    "title": "docs: update README.md",
    "created_at": "2024-12-13T14:45:05Z",
    "closed_at": "2024-12-13T14:55:44Z",
    "labels": [],
    "url": "https://github.com/ai-christianson/RA.Aid/pull/2",
    "body": "minor fix",
    "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/2/comments",
    "author": "eltociear",
    "comments": [
      {
        "user": "ai-christianson",
        "created_at": "2024-12-13T14:55:50Z",
        "body": "Merged. Thank you!"
      }
    ]
  }
]