[
  {
    "number": 992,
    "title": "多线程推理报错",
    "created_at": "2025-02-18T11:58:09Z",
    "closed_at": "2025-02-20T08:47:27Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/992",
    "body": "Traceback (most recent call last):\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/cli/model.py\", line 113, in llm_job\n    for i in self.llm.inference(text=text.to(self.device),\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 56, in generator_context\n    response = gen.send(request)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/llm/llm.py\", line 326, in inference\n    top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/llm/llm.py\", line 150, in sampling_ids\n    top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/utils/common.py\", line 110, in ras_sampling\n    top_ids = nucleus_sampling(weighted_scores, top_p=top_p, top_k=top_k)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/utils/common.py\", line 131, in nucleus_sampling\n    top_ids = indices[prob.multinomial(1, replacement=True)]\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\nException in thread Thread-11 (llm_job):\nTraceback (most recent call last):\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/cli/model.py\", line 113, in llm_job\n    for i in self.llm.inference(text=text.to(self.device),\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 56, in generator_context\n    response = gen.send(request)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/llm/llm.py\", line 326, in inference\n    top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/llm/llm.py\", line 150, in sampling_ids\n    top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/utils/common.py\", line 110, in ras_sampling\n    top_ids = nucleus_sampling(weighted_scores, top_p=top_p, top_k=top_k)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/utils/common.py\", line 131, in nucleus_sampling\n    top_ids = indices[prob.multinomial(1, replacement=True)]\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\nException in thread Thread-10 (llm_job):\nTraceback (most recent call last):\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/cli/model.py\", line 113, in llm_job\n    for i in self.llm.inference(text=text.to(self.device),\n  File \"/home/omadmin/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 56, in generator_context\n    response = gen.send(request)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/llm/llm.py\", line 326, in inference\n    top_ids = self.sampling_ids(logp.squeeze(dim=0), out_tokens, sampling, ignore_eos=True if i < min_len else False).item()\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/llm/llm.py\", line 150, in sampling_ids\n    top_ids = self.sampling(weighted_scores, decoded_tokens, sampling)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/utils/common.py\", line 110, in ras_sampling\n    top_ids = nucleus_sampling(weighted_scores, top_p=top_p, top_k=top_k)\n  File \"/home/omadmin/zhengyi/CosyVoice/cosyvoice/utils/common.py\", line 131, in nucleus_sampling\n    top_ids = indices[prob.multinomial(1, replacement=True)]\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/992/comments",
    "author": "scutfrank",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-18T13:35:32Z",
        "body": "看上去跟多线程应该是没有关系"
      }
    ]
  },
  {
    "number": 986,
    "title": "CosyVoice2 flow模型，非流式推理和流式推理，模型加载有何不同？",
    "created_at": "2025-02-18T02:36:53Z",
    "closed_at": "2025-02-19T02:29:28Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/986",
    "body": "CosyVoicd2的  causal-attention flow 模型，其流式推理和非流式推理有何不同？ 模型加载时设置 flow_model.encoder.static_chunk_size 和 flow_model.decoder.estimator.static_chunk_size ，是因为流式推理需要的吧？如果非流式，这两个值应该设置为啥值呢？0还是-1？\n    另外 utils/mask.py 182行 代码里，关于static_chunk_size 这个参数注释是否错误了？ 应该是“if it is  NOT greater than 0, \n or,  if use_dynamic_chunk is true, this parameter will be ignored” ?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/986/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-18T13:38:30Z",
        "body": "static_chunk_size如果是0或负值会被忽略，注释应该没有错误。非流式的话static_chunk_size可以设置为0或-1都可以"
      }
    ]
  },
  {
    "number": 972,
    "title": "关于CosyVoice2-0.5B模型及笑声语气支持的一些问题",
    "created_at": "2025-02-13T09:58:46Z",
    "closed_at": "2025-02-14T03:15:22Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/972",
    "body": "您好，感谢您们的出色工作！在使用过程中，有两个问题想请教：\n\nCosyVoice2-0.5B模型是否可以拆分？ \n  - 目前模型较大，如果能够拆分，初始化速度是否会比1.0版本更快？另外，在Instruct模式下生成长句时，角色一致性偶尔会有些不稳定，我主要希望使用Clone功能。\n\ninference_zero_shot是否支持语气词和笑声？ \n  - 如果目前还不支持，是否可以通过微调包含笑声的音频或语气词数据来增强对笑声或语气的表现能力？\n \n期待您的解答，非常感谢！ 😊\n\n\n\n\n\n\n\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/972/comments",
    "author": "laishujie",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-13T12:17:05Z",
        "body": "1.拆分什么意思？不了解\n2. 支持微调，准备好相应表现力数据即可"
      },
      {
        "user": "laishujie",
        "created_at": "2025-02-14T02:41:43Z",
        "body": "> 1.拆分什么意思？不了解\n\nCosyVoice2-0.5B模型融合了指令生成（Instruct）和零样本文本到语音合成（Zero-shot TTS）的能力。是否可以保持CosyVoice1.0模型的规范一样，拆分为0.5b和0.5b-Instruct. 就这意思。"
      },
      {
        "user": "aluminumbox",
        "created_at": "2025-02-14T02:51:23Z",
        "body": "> > 1.拆分什么意思？不了解\n> \n> CosyVoice2-0.5B模型融合了指令生成（Instruct）和零样本文本到语音合成（Zero-shot TTS）的能力。是否可以保持CosyVoice1.0模型的规范一样，拆分为0.5b和0.5b-Instruct. 就这意思。\n\n考虑到大模型能力涌现，官方没有拆分计划，个人可以尝试"
      }
    ]
  },
  {
    "number": 965,
    "title": "Update requirements.txt",
    "created_at": "2025-02-11T13:38:49Z",
    "closed_at": "2025-02-12T13:03:42Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/965",
    "body": "I updated to always use the latest version of the libraries, cleaned up the order, and added pyarrow to fix errors. It worked fine.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/965/comments",
    "author": "hwangsihu",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-12T13:03:42Z",
        "body": "thank you, but we fixed the library version because we do not want to introduce problems related with version, and our current cuda version in alibaba is cuda12.1"
      }
    ]
  },
  {
    "number": 963,
    "title": "请问CosyVoice2的Speech Tokenizer是因果的吗？支持流式音频输入吗？",
    "created_at": "2025-02-11T10:26:34Z",
    "closed_at": "2025-02-18T11:44:11Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/963",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/963/comments",
    "author": "zzy1026",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-12T13:05:41Z",
        "body": "不支持"
      }
    ]
  },
  {
    "number": 955,
    "title": "中文数字读错",
    "created_at": "2025-02-10T09:16:03Z",
    "closed_at": "2025-02-11T09:39:52Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/955",
    "body": "\"探讨了超过200项前沿科技项目\"模型生成的音频中读成了2-0-0,而不是两百，有什么解决办法吗？\n另外发现改成200多项时，是正常的两百",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/955/comments",
    "author": "Sunnycl",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-11T02:40:14Z",
        "body": "自行处理一下前端"
      },
      {
        "user": "Sunnycl",
        "created_at": "2025-02-11T07:52:51Z",
        "body": "> 自行处理一下前端\n\n但是我有的需要是中文读法，有的需要是播报的读法，没法直接规则转换，有什么前端处理的好方法吗？"
      },
      {
        "user": "Sunnycl",
        "created_at": "2025-02-11T09:39:52Z",
        "body": "> 自行处理一下前端\n\n加了一个细化的规则解决了，感谢"
      }
    ]
  },
  {
    "number": 948,
    "title": "HIFI-GAN 鉴别器模型",
    "created_at": "2025-02-08T06:43:24Z",
    "closed_at": "2025-02-12T06:57:51Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/948",
    "body": "hi，感谢团队的工作。请问有计划开源hifigan 的鉴别器预训练权重吗？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/948/comments",
    "author": "Wentao795",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-11T02:41:04Z",
        "body": "没有计划，Hifigan训练并不麻烦，可以自己复现"
      }
    ]
  },
  {
    "number": 947,
    "title": "dpo微调",
    "created_at": "2025-02-07T09:52:58Z",
    "closed_at": "2025-02-17T06:24:59Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/947",
    "body": "目前的做法是instruct模型的基础上指令微调了一版，当作policy模型，然后用instruct+text去推理token，现在想用llm生成logits，可是会出现token和logits维度对不上所以算不了dpo loss；有没有做过的同学可以交流一下，或者官方有相关代码吗",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/947/comments",
    "author": "yunpeili",
    "comments": [
      {
        "user": "FlynnFlag",
        "created_at": "2025-02-08T01:24:58Z",
        "body": "我在用cosyvoice1做，我是自己重写了推理代码，让llm.inference输出每一个token和总词汇表的概率，再计算每个token的概率。LLM推理改成随机采样。其他不变"
      },
      {
        "user": "yunpeili",
        "created_at": "2025-02-08T02:17:12Z",
        "body": "> 我在用cosyvoice1做，我是自己重写了推理代码，让llm.inference输出每一个token和总词汇表的概率，再计算每个token的概率。LLM推理改成随机采样。其他不变\n\n是推理token的时候同时返回logp的意思吗？我也是cosyvoice1，之前也是改了llm.inference，但算logits不知道该不该让llm.forward"
      },
      {
        "user": "FlynnFlag",
        "created_at": "2025-02-09T08:44:48Z",
        "body": "> > 我在用cosyvoice1做，我是自己重写了推理代码，让llm.inference输出每一个token和总词汇表的概率，再计算每个token的概率。LLM推理改成随机采样。其他不变\n> \n> 是推理token的时候同时返回logp的意思吗？我也是cosyvoice1，之前也是改了llm.inference，但算logits不知道该不该让llm.forward\n\n我是先改了inference 随机生成同样文本不同音频的数据，然后打分得到多个正负偏好，等造完再改llm的forward，直接输入造的数据的logp当reference model的部分来计算loss，还没做好，不知道可不可行"
      }
    ]
  },
  {
    "number": 945,
    "title": "cosyvoice2 llm training support？",
    "created_at": "2025-02-06T07:42:11Z",
    "closed_at": "2025-02-11T13:25:10Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/945",
    "body": "I noticed that flow training code was updated at dev/lyuxiang.lx branch two weeks ago.  Is there any plan to update llm training code like cosyvoice1？ If so, about when？\nThanks！",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/945/comments",
    "author": "kiraYuukiAsuna",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-02-07T08:25:33Z",
        "body": "later, maybe in this month"
      }
    ]
  },
  {
    "number": 942,
    "title": "No module named 'matcha.models'; 'matcha' is not a package",
    "created_at": "2025-02-05T14:19:03Z",
    "closed_at": "2025-02-08T00:28:16Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/942",
    "body": "run `python webui.py`\n```\nTraceback (most recent call last):\n  File \"D:\\sdk\\miniconda3\\envs\\cosyvoice2\\Lib\\pydoc.py\", line 457, in safeimport\n    module = __import__(path)\n             ^^^^^^^^^^^^^^^^\n  File \"e:\\Projects\\CosyVoice\\cosyvoice\\flow\\flow_matching.py\", line 17, in <module>\n    from matcha.models.components.flow_matching import BASECFM\nModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package\n```",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/942/comments",
    "author": "Cupcc",
    "comments": [
      {
        "user": "zhihei2623",
        "created_at": "2025-02-05T18:12:56Z",
        "body": "我直接重新搞了个虚拟环境就好啦。"
      },
      {
        "user": "hwangsihu",
        "created_at": "2025-02-06T07:24:02Z",
        "body": "Try `git submodule update --init --recursive` in CosyVoice Folder"
      },
      {
        "user": "Cupcc",
        "created_at": "2025-02-08T00:28:08Z",
        "body": "> Try `git submodule update --init --recursive` in CosyVoice Folder\n\nthanks, I solved it by installing `third_party/matcha-TTS`"
      }
    ]
  },
  {
    "number": 912,
    "title": "请问训练from scratch时满足条件的 batch_size是什么？",
    "created_at": "2025-01-21T08:06:53Z",
    "closed_at": "2025-01-24T02:44:39Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/912",
    "body": "请问配置文件里 4卡，12000帧的batch配置，是经过试验调优后的最佳batch 配置吗？ 总batch的值 [卡数 X 单卡上的帧数] 应该在什么值范围内，是一个合理的配置？ 因为可能有的环境硬件条件有限，没办法支持这样大的batch配置。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/912/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-23T16:45:19Z",
        "body": "暂时没有，但经验是batch size不能太小"
      }
    ]
  },
  {
    "number": 911,
    "title": "hi，有计划开源一下hifigan 训练的鉴别器吗？",
    "created_at": "2025-01-21T06:52:18Z",
    "closed_at": "2025-01-21T07:51:43Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/911",
    "body": "hi，感谢团队的工作。我在自己微调hift模型的时候发现鉴别器非常难训练，请问有计划开源下这个鉴别器模型吗？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/911/comments",
    "author": "Wentao795",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-21T07:27:36Z",
        "body": "已经有Hift得训练，但是torch2.3.1不支持pitch提取，后面会更新一下这个代码"
      }
    ]
  },
  {
    "number": 898,
    "title": "Error when start a webui. server",
    "created_at": "2025-01-17T09:49:50Z",
    "closed_at": "2025-01-20T03:03:23Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/898",
    "body": "**Describe the bug**\nwhen I start webui server \n\npython webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M\n\nI got this \n\n File \"/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/hyperpyyaml/core.py\", line 778, in recursive_update\n    raise KeyError(f\"Override '{k}' not found in: {[key for key in d.keys()]}\")\nKeyError: \"Override 'qwen_pretrain_path' not found in: ['__set_seed1', '__set_seed2', '__set_seed3', '__set_seed4', 'sample_rate', 'text_encoder_input_size', 'llm_input_size', 'llm_output_size', 'spk_embed_dim', 'llm', 'flow', 'hift', 'parquet_opener', 'get_tokenizer', 'allowed_special', 'tokenize', 'filter', 'resample', 'feat_extractor', 'compute_fbank', 'parse_embedding', 'shuffle', 'sort', 'batch', 'padding', 'data_pipeline', 'train_conf']\"\n\nDuring handling of the above exception, another exception occurred:\n\nI guess there is no module ,but  I looked the pretrain dir, the files are there.\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/898/comments",
    "author": "skytraveler",
    "comments": [
      {
        "user": "skytraveler",
        "created_at": "2025-01-17T10:01:00Z",
        "body": "/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n2025-01-17 17:59:17,309 INFO input frame rate=50\nTraceback (most recent call last):\n  File \"/Users/li/ai/CosyVoice/webui.py\", line 188, in <module>\n    cosyvoice = CosyVoice(args.model_dir)\n  File \"/Users/li/ai/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 37, in __init__\n    self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'],\n  File \"/Users/li/ai/CosyVoice/cosyvoice/cli/frontend.py\", line 52, in __init__\n    self.campplus_session = onnxruntime.InferenceSession(campplus_model, sess_options=option, providers=[\"CPUExecutionProvider\"])\n  File \"/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 419, in __init__\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\n  File \"/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 472, in _create_inference_session\n    sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidProtobuf: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Load model from pretrained_models/CosyVoice-300M/campplus.onnx failed:Protobuf parsing failed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/li/ai/CosyVoice/webui.py\", line 191, in <module>\n    cosyvoice = CosyVoice2(args.model_dir)\n  File \"/Users/li/ai/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 135, in __init__\n    configs = load_hyperpyyaml(f, overrides={'qwen_pretrain_path': os.path.join(model_dir, 'CosyVoice-BlankEN')})\n  File \"/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/hyperpyyaml/core.py\", line 157, in load_hyperpyyaml\n    yaml_stream = resolve_references(yaml_stream, overrides, overrides_must_match)\n  File \"/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/hyperpyyaml/core.py\", line 316, in resolve_references\n    recursive_update(preview, overrides, must_match=overrides_must_match)\n  File \"/Users/li/miniconda3/envs/cosyvoice/lib/python3.10/site-packages/hyperpyyaml/core.py\", line 778, in recursive_update\n    raise KeyError(f\"Override '{k}' not found in: {[key for key in d.keys()]}\")\nKeyError: \"Override 'qwen_pretrain_path' not found in: ['__set_seed1', '__set_seed2', '__set_seed3', '__set_seed4', 'sample_rate', 'text_encoder_input_size', 'llm_input_size', 'llm_output_size', 'spk_embed_dim', 'llm', 'flow', 'hift', 'parquet_opener', 'get_tokenizer', 'allowed_special', 'tokenize', 'filter', 'resample', 'feat_extractor', 'compute_fbank', 'parse_embedding', 'shuffle', 'sort', 'batch', 'padding', 'data_pipeline', 'train_conf']\"\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/li/ai/CosyVoice/webui.py\", line 193, in <module>\n    raise TypeError('no valid model_type!')\nTypeError: no valid model_type!"
      },
      {
        "user": "aluminumbox",
        "created_at": "2025-01-17T10:44:01Z",
        "body": "模型下载不完整"
      },
      {
        "user": "skytraveler",
        "created_at": "2025-01-20T03:03:21Z",
        "body": "thank you. I redownloaded it .and the problem solved!"
      }
    ]
  },
  {
    "number": 881,
    "title": "instruct指令合成语音，会出现掉字",
    "created_at": "2025-01-14T10:16:22Z",
    "closed_at": "2025-01-15T10:26:33Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/881",
    "body": "**Describe the bug**\r\ninstruct指令合成语音，会出现掉字\r\n\r\n**To Reproduce**\r\n```\r\n    for i, j in enumerate(cosyvoice.inference_instruct2('武汉话<|endofprompt|>收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。',# 这碗热干面蛮好吃。哎呀，咋个这么倒霉哦！\r\n        '用武汉话说这句话', # 通过语言控制语音合成\r\n        prompt_speech_16k, stream=False)):\r\n```\r\n合成的音频中把指令武汉话读出来了，并且前面“收到好友”几个字掉了\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/881/comments",
    "author": "wwfcnu",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-14T13:05:39Z",
        "body": "去掉'武汉话<|endofprompt|>，看readme里的具体用法"
      }
    ]
  },
  {
    "number": 869,
    "title": "NoneType object has no attribute set_input_shape",
    "created_at": "2025-01-10T10:08:01Z",
    "closed_at": "2025-02-11T02:24:43Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/869",
    "body": "更新了今天（2025-01-10）最新的代码与模型（CosyVoice2-0.5B），我运行的是多线程方式。\r\n\r\n```python\r\nimport multiprocessing as mp\r\nfrom cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\r\n\r\ndef worker_process():\r\n···\r\n    model_id = \"pretrained_models/CosyVoice2-0.5B\"\r\n    CosyVoiceModel = CosyVoice2(model_id, load_jit=False, load_trt=True, fp16=True)\r\n····\r\n\r\nprocesses = []\r\n    for _ in range(3):\r\n        p = mp.Process(target=worker_process)\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\n\r\n在运行 `CosyVoiceModel.inference_cross_lingual(tts_text, prompt_speech_16k)` 后报错如下\r\n```\r\n<class 'AttributeError'>:'NoneType' object has no attribute 'set_input_shape'\r\n```\r\n\r\n感谢。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/869/comments",
    "author": "cpken",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-10T16:16:26Z",
        "body": "改成了在线导出tensorrt模型，先启动一个线程，导出成功后，再启动其他线程"
      },
      {
        "user": "cpken",
        "created_at": "2025-01-11T08:49:10Z",
        "body": "解决方案\r\n\r\n```python\r\nimport threading\r\nimport multiprocessing as mp\r\nfrom funasr import AutoModel\r\nfrom cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\r\nimport asyncio\r\nimport os\r\nfrom loguru import logger\r\n\r\nasync def main(pid, logger):\r\n    pass\r\n\r\ndef export_tensorrt_model():\r\n    \"\"\"导出 TensorRT 模型的代码\"\"\"\r\n    global SenseVoiceModel\r\n    global CosyVoiceModel\r\n    SenseVoiceModel = AutoModel()\r\n    CosyVoiceModel = CosyVoice2(model_id, load_jit=False, load_trt=False, fp16=False)\r\n\r\ndef worker_process():\r\n    # 启动一个线程来导出 TensorRT 模型\r\n    export_thread = threading.Thread(target=export_tensorrt_model)\r\n    export_thread.start()\r\n    export_thread.join()  # 等待导出完成\r\n    asyncio.run(main(os.getpid(), logger))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    processes = []\r\n    for _ in range(SUB_PROCESS):\r\n        p = mp.Process(target=worker_process)\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\n\r\n但是我如果这样子写，依然会报错\r\n```python\r\nCosyVoiceModel = CosyVoice2(model_id, load_jit=False, load_trt=True, fp16=False)\r\n```\r\n\r\n不加载 load_trt 不会报错：\r\n```python\r\nCosyVoiceModel = CosyVoice2(model_id, load_jit=False, load_trt=False, fp16=True)\r\n```\r\n"
      },
      {
        "user": "cpken",
        "created_at": "2025-01-11T09:00:32Z",
        "body": "> 改成了在线导出tensorrt模型，先启动一个线程，导出成功后，再启动其他线程\r\n\r\n尝试改了代码，是 load_trt=True 的问题导致的，并且我在测试的时候，多线程没有一次成功的。\r\n```\r\n<class 'AttributeError'>:'NoneType' object has no attribute 'set_input_shape'\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-02-11T01:59:57Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      }
    ]
  },
  {
    "number": 866,
    "title": "怎样才能控制说话的情绪",
    "created_at": "2025-01-10T06:18:34Z",
    "closed_at": "2025-01-10T06:40:17Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/866",
    "body": "```\r\ncosyvoice = CosyVoice2(\r\n    \"pretrained_models/CosyVoice2-0.5B\", load_jit=False, load_trt=False, fp16=False\r\n)\r\n\r\n# zero_shot usage\r\nprompt_speech_16k = load_wav(\"my_voice.wav\", 16000)\r\nprompt_text = \"这是一款性价比超高的旗舰手机,大容量电池,快充,高清拍照,性能拉满,今天直播间限时优惠,快来下单吧\"\r\nfor i, j in enumerate(\r\n    cosyvoice.inference_zero_shot(\r\n        \"这是一款性价比超高的旗舰手机,大容量电池,快充,高清拍照,性能拉满,今天直播间限时优惠,快来下单吧[HAPPY]\",\r\n        prompt_text,\r\n        prompt_speech_16k,\r\n        stream=False,\r\n        text_frontend=True\r\n    )\r\n):\r\n    torchaudio.save(\r\n        \"tests/daihuo-cn-happy.wav\", j[\"tts_speech\"], cosyvoice.sample_rate\r\n    )\r\n```\r\n比如我要兴奋地说这句话，我看到cosyvoice/tokenizer/tokenizer.py里面有```HAPPY```，应该是可以控制的吧，但是不知道在哪里加上去",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/866/comments",
    "author": "wzr0108",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-10T06:39:34Z",
        "body": "看demo里的示例用法"
      }
    ]
  },
  {
    "number": 842,
    "title": "support online onnx to trt conversion",
    "created_at": "2025-01-07T09:23:32Z",
    "closed_at": "2025-01-08T03:21:42Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/842",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/842/comments",
    "author": "Vinkle-hzt",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-08T03:20:15Z",
        "body": "看上去是可行的，先merge到dev，后续统一修改好格式后再merge到main"
      }
    ]
  },
  {
    "number": 827,
    "title": "关于README中的Basic Usage",
    "created_at": "2025-01-03T04:10:29Z",
    "closed_at": "2025-01-04T05:41:01Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/827",
    "body": "我觉得Basic Usage那部分描述有些让人晕头转向了。既然说了推荐使用CosyVoice2-0.5B，而且CosyVoice2示例代码中CosyVoice2-0.5B是针对inference_zero_shot、inference_cross_lingual、inference_instruct2的，\r\n而描述中又说“ For zero_shot/cross_lingual inference, please use CosyVoice-300M model. For sft inference, please use CosyVoice-300M-SFT model. For instruct inference, please use CosyVoice-300M-Instruct model.”，看上去又让用cosyvoice1几个模型才能很好支持zero_shot/cross_lingual inference，sft inference, For instruct inference.\r\n\r\n总之，这部分描述让人一头雾水。\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/827/comments",
    "author": "cn-knight",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-04T05:41:01Z",
        "body": "已更新"
      }
    ]
  },
  {
    "number": 823,
    "title": "咱们CosyVoice2的25hz，大概啥时候能发布呢？",
    "created_at": "2025-01-02T09:07:44Z",
    "closed_at": "2025-01-03T03:35:24Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/823",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/823/comments",
    "author": "0xCAFEBABE0",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2025-01-02T14:06:33Z",
        "body": "这个就是25hz"
      }
    ]
  },
  {
    "number": 807,
    "title": "新版的的模型能支持调整音量、调整语调、调整多音字、调整语速等功能吗？还有最终语音对应的字幕文件",
    "created_at": "2024-12-30T03:37:48Z",
    "closed_at": "2024-12-30T03:46:10Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/807",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/807/comments",
    "author": "hjj-lmx",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-30T03:46:10Z",
        "body": "看readme，语音音量调整可以自行sox实现"
      }
    ]
  },
  {
    "number": 802,
    "title": "CosyVoice2, 调用 FastAPI 的 `inference_zero_shot` 接口后，生成音频的音色变粗了",
    "created_at": "2024-12-29T05:19:14Z",
    "closed_at": "2025-01-09T13:36:02Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/802",
    "body": "**Describe the bug**\r\nCosyVoice1 是正常的，但是CosyVoice2 生成的音频中，人的音色变粗。prompt 音频和 1 中使用的是一模一样的。\r\n\r\n**Expected behavior**\r\nNormal voice quality. \r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Ubuntu 22.04\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/802/comments",
    "author": "AspadaX",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-29T09:25:01Z",
        "body": "先看直接python里参照readme里命令生成是不是也是这样"
      },
      {
        "user": "hit-xpf",
        "created_at": "2024-12-29T10:20:31Z",
        "body": "`torchaudio.save(output_wav, tts_speech, sample_rate)`\r\n保存的时候，受sample_rate影响，我的输出也是对不上（用16000的sample_rate）。\r\n不知道是不是前一步对返回的response的音频字节流的处理有问题。。。"
      },
      {
        "user": "AspadaX",
        "created_at": "2024-12-29T14:12:41Z",
        "body": "> 先看直接python里参照readme里命令生成是不是也是这样\r\n\r\n我这里看了一下 2 和 1 的模型文件夹里面的 `cosyvoice.yaml`。里面的 sample rate 2是 24000，1是 22050。然后`CosyVoice2` class 在 init 的时候会加载配置文件里面写的 sample rate？是因为这个原因吗？"
      },
      {
        "user": "JiajunDou",
        "created_at": "2024-12-30T04:38:38Z",
        "body": "> 先看直接python里参照readme里命令生成是不是也是这样\r\n\r\n用readme里面是正常的"
      },
      {
        "user": "EnochX",
        "created_at": "2024-12-31T03:37:59Z",
        "body": "我也有这个问题，同样的prompt音频，cosyvoice2，inference_zero_shot生成的音色很粗，相似度比cosyvoice1差很多。想要试一下官方效果，modelscope studio又一直跑不通"
      },
      {
        "user": "hit-xpf",
        "created_at": "2024-12-31T03:41:25Z",
        "body": "@hit-xpf \r\n输出保存的 `sample_rete` 用 22050 大致可以。还没有试 24000"
      },
      {
        "user": "EnochX",
        "created_at": "2024-12-31T03:52:41Z",
        "body": "确实是sample_rate的问题，改成24000就好了"
      },
      {
        "user": "cpken",
        "created_at": "2025-01-02T10:35:56Z",
        "body": "保存音频的时候，要用prompt音频的 sample_rete，才不会导致导出音频变差。"
      },
      {
        "user": "CodePothunter",
        "created_at": "2025-01-09T05:00:44Z",
        "body": "得保存成22050或者24000"
      },
      {
        "user": "AspadaX",
        "created_at": "2025-01-09T13:36:02Z",
        "body": "> 确实是sample_rate的问题，改成24000就好了\r\n\r\n是的，修改成 24000 后声音正常。我这里 prompt 音频是 44K 左右的 sample rate，不过并不妨碍。\r\n\r\n我这里测试后发现，sample rate 越高，声音越细，反之亦然。之前 CosyVoice 1 的时候用的是 22050，这个采样率在 2 里面就会让声音变粗。"
      }
    ]
  },
  {
    "number": 787,
    "title": "关于CosyVoice2的SFT训练",
    "created_at": "2024-12-25T09:54:07Z",
    "closed_at": "2024-12-29T11:53:10Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/787",
    "body": "您好，\r\n      我们在CosyVoice1的 Flow-Matching的 SFT训练并没有得到理想的效果，SFT训练后的模型还不如baseline 的CER更低。在学习CosyVoice2论文中，第2.7 Multi-Speaker Fine-tuning， 第2.8  Reinforcement Learning for SFT 都提到了目标音色的精调训练过程，但是没有明确的说明这是针对 TextSpeech LM的，还是flow-matching的，还是说两者都涉及？  在2.7章节描述中，加tag的操作好像是 说的TextSpeech LM，但是TextSpeech LM根本就不关心音色，连speaker-embedding都没有用到。但是说它是面型flow模型的，那么相当于给flow模型增加了一个条件，在推理时增加不一致，也不像。 所以有点糊涂，能否明确一下，论文中这两个SFT章节，主要说的是哪个部分呢?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/787/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-27T08:15:28Z",
        "body": "音色精调实现了吗"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-27T08:27:09Z",
        "body": "> 音色精调实现了吗\r\n\r\n没有，flow模型的SFT【Speaker 都是base模型见过的，但是这部分语音质量最高】，训练后发现效果反而差了。可能LLM部分的SFT对效果有显著提升。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-27T08:51:10Z",
        "body": "我感觉cosyvoice2对音色的抽取效果，没有F5好"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-27T08:56:16Z",
        "body": "> 我感觉cosyvoice2对音色的抽取效果，没有F5好\r\n\r\n不好说。对比模型效果之前，得先看两者训练环境，数据量，数据质量是否有可比性的。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-27T11:02:31Z",
        "body": "肯定是各自释放出的权重，cosyvoice没有训练脚本和数据\r\n\r\n另外cosyvoice2里面的人物信息，应该是用固定的模型（cam++）抽取的，不太确定这个是不是固定死的"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-27T13:23:23Z",
        "body": "cosyvoice 有示例训练脚本，数据你自己准备，可以复现的。   \r\ncosyvoice使用complus 声纹提取器做音色特征抽取，对于SFT，可以是设置死的；对于zeroshot，你来什么prompt就提取什么complus向量，没所谓限制死的。    \r\n可以看一下他们释放的论文以及工程代码。\r\n"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-12-29T09:31:41Z",
        "body": "sft只是针对某个人的效果更好，大数据集上cer变高是正常的"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-30T02:10:14Z",
        "body": "@JohnHerry complus是固定的onnx模型，应该也不支持prompt？我理解prompt是在LLM阶段加限定的。我的意思是，是否是complus没有参与训练，直接就是一个onnx固定特征提取用于其他模块训练的。如果是这样，camplus音色效果不好，不太确定是否能实现微调其他模块提升音色。"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-30T02:23:24Z",
        "body": "> @JohnHerry complus是固定的onnx模型，应该也不支持prompt？我理解prompt是在LLM阶段加限定的。我的意思是，是否是complus没有参与训练，直接就是一个onnx固定特征提取用于其他模块训练的。如果是这样，camplus音色效果不好，不太确定是否能实现微调其他模块提升音色。\r\n\r\ncomplus模块自身不参与CosyVoice训练，参与训练的是complus生成的声纹特征。所以你把它当成跟提取梅尔谱一样的特征提取器就行了，不管是对训练样本，还是zero-shot时给的prompt音频，都是一样的。 如果你是在基础模型上做SFT训练，而做zero-shot合成，那肯定音色不好的。你要想zero-shot好，那就直接用基座模型，不要用SFT[少量精标音色]的微调训练。    \r\ncomplus 自身就是一个声纹提取器，它输出是一个192维度的固定长度的张量，本身就是起到一个音色标记的作用，并不负责音色的完美复现的。所以跟它无关【你说的camplus音色效果不好，这个不准确，应该跟它没有关系】。 你要想提高音色相似度，你就要提升你的CosyVoice_Flow模型，以及后续的Vocoder的音色复原效果。它们是真正语义token转化为声学的关键部分。 主要是在flow模型。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-31T03:27:48Z",
        "body": "我用的就是0.5b的v2模型，cross language 音色不贴合，不是zeroshot。\r\n这个有啥办法优化吗"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-12-31T03:31:05Z",
        "body": "> 我用的就是0.5b的v2模型，cross language 音色不贴合，不是zeroshot。 这个有啥办法优化吗\r\n\r\n把调用代码与prompt音频提供一下"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-31T03:44:17Z",
        "body": "\r\n```\r\na = self.model.inference_cross_lingual(\r\n                text,\r\n                prompt_speech_16k,\r\n                stream=stream,\r\n            )\r\n```\r\n\r\n主要是这个，音色不是说不像，主要是和F5对比起来，没有那么贴合\r\n\r\n"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-31T07:32:15Z",
        "body": "没做过跨语言的尝试，但是看他们开源代码， inference_cross_lingual 本质上就是zero_shot,  并且不提供 Prompt Text和Promt Speech，相当于零条件的zero Prompt zero shot，甚至连模型的In-Context Learning的能力都没办法用到，所以效果不太理想是可以想见的。   \r\n 第一你只能用base model不要用精调模型；第二如果base模型还是效果不好，那也没别的办法了。增大模型尺寸容量，增加训练数据【特别是音色类型覆盖度】，多训练一些步骤，可能会好一些。  另外如果训练的时候，能够强调让模型在只有 声纹prompt，不包含文本prompt和语音promt上的合成效果，跟真实目标之间的损失，有可能会有所帮助。但是我觉得大概率帮助不会很大。因为我觉得声纹，192维向量，主要作用还是“区分”， 而不是“蕴含”； 也就是它是区分不同音色的效果更大一些，而并不是包含了当前声纹代表音色的所有声学特征，所以指望这个东西能非常理想的复刻你的外语音色，可能性不大。 而声纹本质来源于分类，所以在当前模型下，大概率模型会用一个非常靠近你的声纹音色的某个其它训练样本中见过的音色，来合成你的声音，不太可能非常像的。 只是个人见解，如有错误请指正。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-31T07:34:35Z",
        "body": "prompt_speech 是肯定用到了的。\r\n不太确定为什么没用到prompt text"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-31T07:37:12Z",
        "body": "您把它用到prompt speech的例子代码贴出来？ 它的front_end里边，cross_lingual的时候，是把prompt text， prompt mel，以及两个长度，都del掉了的。你的prompt speech只能用来提取声纹了。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-31T08:22:29Z",
        "body": "没有深究cosyvoice2原理代码，从前端接口来看，F5会用到promptspeech和prompttext，cosyvoice2甚至没传入text肯定没用到，speech用到了哪一层不太确定，以及为什么不需要text存疑"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-31T09:42:38Z",
        "body": "cosyvoice2，在一般的同语种 Prompt的合成的时候，也是可以支持Prompt Speech 和Prompt Text的。只是跨语种的Prompt语音，它对应的文本语言，可能不在CosyVoice2支持语言种类范围内啊？怎么输入呢？  你这跨语种的，而且Prompt包含的语种不是模型支持范围内的话，我估计F5那个也不敢用。谁知道用这个Prompt的东西会引导生成啥不稳定的结果出来啊。关键是跨语种。    \r\n而且跟相关同学了解到，声纹提取，其实也是跟语种有一定关系的。我们不清楚这个预训练的complus是不是支持各种跨语种，或者只是支持中英日韩等少量大语种，而你用孟加拉、祖鲁 等等这些小语种的语音，可能也是有影响的。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-31T10:55:11Z",
        "body": "tts 和 cloning 两码事"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-31T12:38:55Z",
        "body": "> tts 和 cloning 两码事\r\n\r\n哪句话提到克隆了？ ....  跳跃太大，理解不能。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2025-01-02T04:05:09Z",
        "body": "无法交流。。"
      },
      {
        "user": "LzyloveRila",
        "created_at": "2025-01-06T11:18:22Z",
        "body": "这个Mono把我看乐了，源码都不愿意看，纯杠"
      },
      {
        "user": "CrossLee1",
        "created_at": "2025-02-06T09:45:37Z",
        "body": "@JohnHerry hello，请问下这个问题你们解决了么，是 finetune llm，还是 finetune flow matching 效果好呢"
      },
      {
        "user": "CHIHHSIANGLI",
        "created_at": "2025-02-06T09:49:32Z",
        "body": "这个Mono就不是做算法的  你们交流不了"
      }
    ]
  },
  {
    "number": 786,
    "title": "Voice cloning of Chinglish issue",
    "created_at": "2024-12-25T09:14:23Z",
    "closed_at": "2024-12-26T08:13:56Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/786",
    "body": "Occasionally, there is the issue of Chinglish. Is there a way to address it?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/786/comments",
    "author": "MonolithFoundation",
    "comments": [
      {
        "user": "darkacorn",
        "created_at": "2024-12-26T07:54:02Z",
        "body": "finetuneing on more english - natualy its dominant chinese - but nothing that can not be fixed with more data"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-12-26T08:13:12Z",
        "body": "听君一席如听一席"
      }
    ]
  },
  {
    "number": 779,
    "title": "CosyVoice2 模型配置文件更新",
    "created_at": "2024-12-24T07:17:14Z",
    "closed_at": "2024-12-24T09:56:31Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/779",
    "body": "**Is your feature request related to a problem? Please describe.**\r\n看论文里CosyVoice2应该是24Khz的，但是现在的example配置里还是22Khz呢，建议更新一下配置文件，以免混淆。\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/779/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "cpken",
        "created_at": "2024-12-24T08:05:40Z",
        "body": "因为 2.0 的训练脚本还没有提供出来，你看到的是 1.0 的训练脚本。"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-24T09:56:31Z",
        "body": "收到，谢谢！ 持续学习中!"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-12-31T09:55:15Z",
        "body": "> 因为 2.0 的训练脚本还没有提供出来，你看到的是 1.0 的训练脚本。\r\n\r\n请教一下，就flow模型而言， CosyVoice2 和 CosyVoice1的训练配置，和训练脚本应该没有太大变化吧？"
      }
    ]
  },
  {
    "number": 776,
    "title": "CosyVoice2.0支持微调吗？",
    "created_at": "2024-12-24T02:10:26Z",
    "closed_at": "2025-01-02T01:30:07Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/776",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/776/comments",
    "author": "wjddd",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-29T09:38:21Z",
        "body": "支持，1月底更新"
      },
      {
        "user": "wjddd",
        "created_at": "2025-01-02T01:30:07Z",
        "body": "> 支持，1月底更新\r\n\r\n好的，谢谢！"
      },
      {
        "user": "Kingdroper",
        "created_at": "2025-01-20T09:26:08Z",
        "body": "> 支持，1月底更新\n\nHi, It's the end of January, when will you update"
      },
      {
        "user": "Downupanddownup",
        "created_at": "2025-01-24T01:41:48Z",
        "body": "感觉得年后了呀"
      },
      {
        "user": "aluminumbox",
        "created_at": "2025-01-24T03:09:26Z",
        "body": "dev/lyuxiang.lx分支已经更新cosyvoice2.0 flow训练以及flow推理cache代码，由于这个更新影响其他服务，因此会年后测试好后再merge main"
      },
      {
        "user": "zw76859420",
        "created_at": "2025-02-12T08:02:44Z",
        "body": "期待"
      }
    ]
  },
  {
    "number": 749,
    "title": "fix the bug of constantlr and deespeed",
    "created_at": "2024-12-19T06:34:35Z",
    "closed_at": "2025-01-07T02:52:48Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/749",
    "body": "修复使用deepspeed训练，且scheduler为constantlr时的bug：\r\n1. 原有代码传入的是conf/ds_stage2.json中的optimizer，并未传入yaml文件中的optimizer。\r\n2. constantlr会报关于scheduler_conf和warmup_steps的KeyError\r\n\r\n在yaml中设置lr为0.00001，\r\n修复bug之前的log：\r\n`TypeError: __init__() got an unexpected keyword argument 'warmup_steps'`\r\n`2024-12-19 11:25:00,526 DEBUG TRAIN Batch 0/100 loss 3.138866 acc 0.303122 lr 0.00100000 grad_norm 0.286094 rank 0`\r\n修复bug之后的log：\r\n`2024-12-19 06:20:32,664 DEBUG TRAIN Batch 0/100 loss 3.581383 acc 0.204409 lr 0.00001000 grad_norm 1.757465 rank 0`\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/749/comments",
    "author": "Shengqiang-Li",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-29T09:18:36Z",
        "body": "这里是因为deepspeed optimizer效率更高，因此传入的是none，但确实需要根据yaml里配置调整一下ds的optimizer配置"
      },
      {
        "user": "Shengqiang-Li",
        "created_at": "2024-12-29T12:35:10Z",
        "body": "> 这里是因为deepspeed optimizer效率更高，因此传入的是none，但确实需要根据yaml里配置调整一下ds的optimizer配置\r\n\r\n了解，那这块的optimizer就不改了。"
      }
    ]
  },
  {
    "number": 743,
    "title": "[Bug] Tags < and > are removed during inference when text_frontend=True",
    "created_at": "2024-12-17T22:45:17Z",
    "closed_at": "2025-01-08T10:06:22Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/743",
    "body": "### **Bug Report**\r\n\r\n#### **Description**\r\nWhen setting `text_frontend=True` (or leaving it as the default), the `<` and `>` tags are removed from the text during inference.\r\n\r\nFor example:\r\n```text\r\nINFO synthesis text 这也strong太strong离谱了吧！\r\n```\r\n---\r\n\r\n#### **To Reproduce**\r\n\r\n```python\r\nfrom cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\r\nfrom cosyvoice.utils.file_utils import load_wav\r\nimport torchaudio\r\n\r\n# Initialize the CosyVoice2 model\r\ncosyvoice = CosyVoice2(\r\n    'pretrained_models/CosyVoice2-0.5B',\r\n    load_jit=True,\r\n    load_onnx=False,\r\n    load_trt=False\r\n)\r\n\r\naudio_file_path = 'audio/48k.wav'\r\nprompt_speech_16k = load_wav(audio_file_path, 16000)\r\n\r\nfor i, j in enumerate(cosyvoice.inference_cross_lingual(\r\n    '这也<strong>太</strong>离谱了吧！',\r\n    prompt_speech_16k,\r\n    stream=False\r\n)):\r\n    torchaudio.save(\r\n        'fine_grained_control_{}.wav'.format(i),\r\n        j['tts_speech'],\r\n        cosyvoice.sample_rate\r\n    )\r\n```\r\n\r\n---\r\n\r\n#### **Expected Behavior**\r\nThe inference should preserve the `<` and `>` tags as part of the input text.\r\n\r\n```text\r\n这也<strong>太</strong>离谱了吧！\r\n```\r\n\r\n---\r\n\r\n#### **Actual Behavior**\r\nThe tags `<` and `>` are removed, resulting in:\r\n```text\r\n这也strong太strong离谱了吧\r\n```\r\n\r\n---\r\n\r\n#### **Environment**\r\nPlease provide details about your environment:\r\n\r\n- **Operating System**: WSL2\r\n- **Python Version**: Python3.10 on `conda`",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/743/comments",
    "author": "youngercloud",
    "comments": [
      {
        "user": "darkacorn",
        "created_at": "2024-12-18T11:11:22Z",
        "body": "interesting .. maybe just a display thing as <laughter></laughter> does work - on cross lingual that is"
      },
      {
        "user": "youngercloud",
        "created_at": "2024-12-20T04:28:32Z",
        "body": "@darkacorn I reinstalled the Cosyvoice2 step by step on a new Ubuntu-based machine, and I got the log below.\r\n\r\n```\r\n2024-12-20 15:17:49,499 INFO synthesis text 这也<strong>太</strong>。\r\n```\r\n\r\nIn terms of actual inference, the model uses the post-processed string, which indicates an incorrect string, and outputs the wrong audio.\r\n\r\nThe reason of re-installing is that I wanted to see if `WeTextProcessing` works properly, and it works fine for the above sentence.\r\n\r\nAnyway, I will set the `text_frontend` to `False` and leave this issue open for a while to see if someone encounters a similar issue."
      }
    ]
  },
  {
    "number": 731,
    "title": "can MBP M3 Pro run this cosyvoice2? can not install requirements.txt.",
    "created_at": "2024-12-16T09:42:13Z",
    "closed_at": "2024-12-17T05:55:55Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/731",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/731/comments",
    "author": "hildazzz",
    "comments": [
      {
        "user": "darkacorn",
        "created_at": "2024-12-16T16:58:11Z",
        "body": "how about you fill the form out ^^ - not just spam an empty report template"
      },
      {
        "user": "hildazzz",
        "created_at": "2024-12-17T05:55:53Z",
        "body": "after commenting out some lines in requiremens.txt, the reference of v2 works well, while the v1 model can not work, as issue #724.\r\nI will close this issue. "
      }
    ]
  },
  {
    "number": 712,
    "title": "能否在语音合成的同时出srt字幕，还是得合成完后再whisper识别出来呢？",
    "created_at": "2024-12-13T03:20:07Z",
    "closed_at": "2024-12-14T09:24:13Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/712",
    "body": "能否在语音合成的同时出srt字幕，还是得合成完后再whisper识别出来呢？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/712/comments",
    "author": "bk111",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-14T09:24:13Z",
        "body": "合成的内容和文本是对应的，可以自行处理"
      },
      {
        "user": "Lennon-cheng",
        "created_at": "2024-12-24T11:11:14Z",
        "body": "@aluminumbox  TTS 后得到每个字对应的时间戳。这个是 只能后处理 whisper 识别后对应文本，还是说，模型哪个地方可以吐出时间戳？"
      }
    ]
  },
  {
    "number": 711,
    "title": "transformer is not installed, please install it if you want to use related modules",
    "created_at": "2024-12-13T02:14:21Z",
    "closed_at": "2024-12-14T09:24:21Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/711",
    "body": "2024-12-12 17:47:19,464 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\r\n2024-12-12 17:47:19,465 - modelscope - INFO - Loading ast index from /home/dengz/.cache/modelscope/ast_indexer\r\n2024-12-12 17:47:19,616 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 c6929b755ba51ba218327722832189d6 and a total number of 980 components indexed\r\ntransformer is not installed, please install it if you want to use related modules\r\n\r\n\r\n\r\ntransformer  是  transformers 包没有安装吗？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/711/comments",
    "author": "bk111",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-14T09:24:21Z",
        "body": "因为不需要"
      }
    ]
  },
  {
    "number": 710,
    "title": "fix(bug).when generating text that contains only punctuation marks or…",
    "created_at": "2024-12-12T08:50:55Z",
    "closed_at": "2024-12-30T02:55:56Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/710",
    "body": "fix(bug).when generating text that contains only punctuation marks or whitespace characters, the CPU usage reaches 100%, and the process crashes.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/710/comments",
    "author": "0xCAFEBABE0",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-29T09:22:27Z",
        "body": "这个检测很好，但是我会考虑直接把这个检测放到frontend.py里，如果全是标点则返回空list，后面就不会进入model.py里的tts方法"
      },
      {
        "user": "0xCAFEBABE0",
        "created_at": "2024-12-30T02:50:38Z",
        "body": "> 这个检测很好，但是我会考虑直接把这个检测放到frontend.py里，如果全是标点则返回空list，后面就不会进入model.py里的tts方法\r\n\r\n@aluminumbox 好主意，已更新代码。"
      }
    ]
  },
  {
    "number": 706,
    "title": "多个请求的时候gpu利用率100%卡死",
    "created_at": "2024-12-10T09:25:03Z",
    "closed_at": "2024-12-30T09:47:02Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/706",
    "body": "大佬好，ubuntu20.04  4090,按照文档部署后，写了个demo，同时请求超过5个后，出现gpu利用率100%，卡死不释放，服务也假死了，日志有打印：\r\n/root/miniconda3/envs/cv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../third_party/nvfuser/csrc/parser.cpp:3777.)                                                                                                                                                                              | 0/1 [00:00<?, ?it/s]\r\n  return forward_call(*args, **kwargs) ",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/706/comments",
    "author": "xuejiale929",
    "comments": [
      {
        "user": "xuejiale929",
        "created_at": "2024-12-10T09:25:15Z",
        "body": "补充：/root/miniconda3/envs/cv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() profile_node %359 : int[] = prim::profile_ivalue[profile_failed=\"varying profile values\"](%357)\r\n does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.85s/it]\r\n  return forward_call(*args, **kwargs)\r\n"
      },
      {
        "user": "xuejiale929",
        "created_at": "2024-12-10T09:26:19Z",
        "body": "cuda11.8,驱动550.135"
      },
      {
        "user": "ganfengbao",
        "created_at": "2024-12-11T06:53:50Z",
        "body": "+1 遇到了同样的问题，两个流式推理直接就卡死了，利用webui做测试的时候"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-12-30T09:47:02Z",
        "body": "不要用webui并发推理，runtime里的grpc我是试过可以并发没问题的，其他的部分并发不是官方保证，如果一定要用gradio提供服务，那么需要自己研究一下"
      }
    ]
  },
  {
    "number": 698,
    "title": "如何禁止ttsfrd 打印 tn xxx to xxx 的log",
    "created_at": "2024-12-05T06:29:38Z",
    "closed_at": "2024-12-05T09:23:43Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/698",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/698/comments",
    "author": "vra",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-05T08:32:40Z",
        "body": "没有办法"
      }
    ]
  },
  {
    "number": 692,
    "title": "模型支持商用么",
    "created_at": "2024-12-03T07:13:07Z",
    "closed_at": "2025-02-05T02:38:16Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/692",
    "body": "除了仓库有个开源协议，没有看到模型相关的描述",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/692/comments",
    "author": "the-loki",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-03T09:42:03Z",
        "body": "可以，也适用于模型"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-03T02:00:25Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      }
    ]
  },
  {
    "number": 690,
    "title": "如何开启LOG日志，启动无反应",
    "created_at": "2024-12-03T02:33:47Z",
    "closed_at": "2024-12-03T16:18:53Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/690",
    "body": "如何开启LOG日志，启动无反应，也不报错，也无任何信息输出， 搜索文件也未看到log文件。\r\n\r\n求大佬指导，感谢！",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/690/comments",
    "author": "iloveuaa",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-12-03T09:40:17Z",
        "body": "terminal里应该有日志"
      },
      {
        "user": "iloveuaa",
        "created_at": "2024-12-03T14:35:20Z",
        "body": "> terminal里应该有日志\r\n\r\n然而是没有的，不知道怎么搞了。\r\n5600x 4060TI 16G"
      }
    ]
  },
  {
    "number": 686,
    "title": "分段文本连续调用CosyVoice-300M的tts推理，音频后段电音越来越严重。",
    "created_at": "2024-11-29T05:18:09Z",
    "closed_at": "2024-12-02T03:04:20Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/686",
    "body": "我在主进程挂载了CosyVoice-300M模型，把长文本进行切片，把切片文本输入给TTS方法推理，最后把生成的音频进行合并。前面的音频听起来很正常，后续电音越来越严重，我猜测是模型挂载后存在缓存造成影响。如果是的话，有没有方法定期清理缓存数据避免这种电音。\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/686/comments",
    "author": "ZzMLvzZ-792998470",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-30T02:54:56Z",
        "body": "应该跟缓存没关系，确定是不是切了短句后，每一句分开推理。torch.cuda.empty_cache清楚缓存"
      },
      {
        "user": "ZzMLvzZ-792998470",
        "created_at": "2024-11-30T10:03:44Z",
        "body": "> 应该跟缓存没关系，确定是不是切了短句后，每一句分开推理。torch.cuda.empty_cache清楚缓存\r\n\r\n我是这么实现的，分成单句，遍历走单独推理，每次执行为使用toch.cuda.empty_cache()，但是问题还是存在。\r\n```\r\n# for text in tts_split:\r\n    while index < len(tts_split):\r\n        text = tts_split[index]\r\n        origin = origin_split[index]\r\n\r\n        ret, content = extract_special(text, '#<', '>#')\r\n        subtitle_content = origin if not ret else ''\r\n        # tts处理并生成音频数据\r\n        json_data = {\r\n            'content': subtitle_content,\r\n            'start': -1,\r\n            'end': -1\r\n        }\r\n\r\n        # 单句推理\r\n        audio_data, duration = tts_warp(ret, content, text, model_input, language)\r\n        audio_data_list.append(audio_data)\r\n\r\n        # -------------------------------------\r\n        json_data['start'] = startTime\r\n        json_data['end'] = startTime + duration\r\n        subtitle_list.append(json_data)\r\n        startTime += duration\r\n        # -------------------------------------\r\n\r\n        torch.cuda.empty_cache()\r\n        index += 1\r\n\r\n```"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-12-02T02:59:16Z",
        "body": "不清楚，如果每一句都这样的话，那是模型问题。如果只是后面的句子这样，前面句子正常，那应该是调用有问题"
      },
      {
        "user": "ZzMLvzZ-792998470",
        "created_at": "2024-12-02T03:04:17Z",
        "body": "> 不清楚，如果每一句都这样的话，那是模型问题。如果只是后面的句子这样，前面句子正常，那应该是调用有问题\r\n\r\n好吧 我再研究一下。感谢回复"
      }
    ]
  },
  {
    "number": 685,
    "title": "ERROR: ttsfrd-0.3.6-cp38-cp38-linux_x86_64.whl is not a supported wheel on this platform.",
    "created_at": "2024-11-29T02:14:17Z",
    "closed_at": "2025-01-15T01:58:43Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/685",
    "body": " ttsfrd-0.3.6-cp38-cp38-linux_x86_64.whl 有python3.10的版本吗",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/685/comments",
    "author": "LIUKAI0815",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-30T02:55:02Z",
        "body": "没有"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-31T02:00:02Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-15T01:58:43Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "number": 679,
    "title": "in China读的时候in和China中间总多点东西?",
    "created_at": "2024-11-26T06:05:50Z",
    "closed_at": "2025-01-10T02:02:27Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/679",
    "body": "in China这个是要做啥格式化处理吗？用预训练英文女的模型读，in China中间会多一个音，in china小写这个没有多音。但是不管大写还是小写放到 a pride of Shenyang in China 这句话里都会多个音，这是什么问题？\r\n以上测试是在webui.py测试的，用的模型是300M。\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/679/comments",
    "author": "slash130",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-26T13:55:19Z",
        "body": "可以自己debug看看frontend.py里tokenize的结果，试试 a pride of Shenyang和in China分别tokenize然后拼起来"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-27T02:00:10Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-10T02:02:27Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "number": 675,
    "title": "【VC模式】VC模式下的流式推理，已经是分段返回，但合成音频时仍然只有最后一段",
    "created_at": "2024-11-24T15:17:35Z",
    "closed_at": "2024-11-25T07:43:57Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/675",
    "body": "VC模式下的流式推理，已经是分段返回，但合成音频时仍然只有最后一段，下面是我的调用代码\r\n\r\n\r\n`cosyvoice = CosyVoice(\"tts_weight/CosyVoice-300M-25Hz\")\r\nprompt_speech_16k = load_wav('girl_cut.wav', 16000)\r\nsource_speech_16k = load_wav('AP.wav', 16000)\r\n\r\naudio_tts = []\r\nfor i, j in enumerate(cosyvoice.inference_vc(source_speech_16k, prompt_speech_16k, stream=False)):\r\n    audio_tts.append(j[\"tts_speech\"])\r\nfull_tts = torch.cat(audio_tts, dim=1)\r\ntorchaudio.save(\"vc_{}.wav\".format(2222), j[\"tts_speech\"], 22050)`",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/675/comments",
    "author": "wang-TJ-20",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-25T06:14:21Z",
        "body": "你这里stream=False"
      },
      {
        "user": "wang-TJ-20",
        "created_at": "2024-11-25T07:43:35Z",
        "body": "@aluminumbox 嗯嗯，解决啦，我代码有个bug"
      }
    ]
  },
  {
    "number": 673,
    "title": "Question",
    "created_at": "2024-11-24T02:00:16Z",
    "closed_at": "2024-11-24T03:17:22Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/673",
    "body": "Is this able to change the language of vocals? Like a singer?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/673/comments",
    "author": "dillfrescott",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-24T03:10:29Z",
        "body": "no"
      },
      {
        "user": "dillfrescott",
        "created_at": "2024-11-24T03:17:15Z",
        "body": "Okay"
      }
    ]
  },
  {
    "number": 637,
    "title": "最新版本 文本正则 丢字，官方例子下",
    "created_at": "2024-11-11T10:27:42Z",
    "closed_at": "2024-11-14T03:34:38Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/637",
    "body": "**Describe the bug**\r\n版本：\r\ncommit 6d22d0b76f7daf01324f8f569167c110af7bcb1e (HEAD -> main, origin/main, origin/HEAD)\r\nMerge: 3914b54 487701c\r\nAuthor: Xiang Lyu <lyuxiang.lx@alibaba-inc.com>\r\nDate:   Tue Nov 5 09:36:51 2024 +0800\r\n\r\n    Merge pull request #618 from FunAudioLLM/dev/lyuxiang.lx\r\n    \r\n    Dev/lyuxiang.lx\r\n\r\n\r\n\r\n运行代码： \r\n\r\n\r\nfrom cosyvoice.cli.cosyvoice import CosyVoice\r\nfrom cosyvoice.utils.file_utils import load_wav\r\nimport torchaudio\r\n\r\ncosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-SFT', load_jit=True, load_onnx=False, fp16=True)\r\ncosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-25Hz') \r\n\r\nprompt_speech_16k = load_wav('zero_shot_prompt.wav', 16000)\r\nfor i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '希望你以后能够做的比我还好呦。', prompt_speech_16k, stream=False)):\r\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], 22050)\r\n\r\n\r\n\r\n\r\n\r\n\r\n实际合成音频，\"如花儿般绽放\" 被 修改为： \"如花般绽放\"  少了一个 \"儿\"\r\n\r\n**日志： **\r\n\r\n(cosyvoice) os@zrway:~/.wtt/CosyVoice$ python test.py \r\n2024-11-11 10:06:41,326 - modelscope - INFO - PyTorch version 2.0.1 Found.\r\n2024-11-11 10:06:41,326 - modelscope - INFO - Loading ast index from /home/os/.cache/modelscope/ast_indexer\r\n2024-11-11 10:06:41,406 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 092a6171a3704eb030d97bb4a88f4275 and a total number of 980 components indexed\r\ntransformer is not installed, please install it if you want to use related modules\r\nfailed to import ttsfrd, use WeTextProcessing instead\r\n/home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\r\n  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\r\n2024-11-11 10:06:47,177 INFO input frame rate=50\r\n2024-11-11 10:06:49.025035641 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\r\n2024-11-11 10:06:49.025050272 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\r\n2024-11-11 10:06:49,813 WETEXT INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\r\n2024-11-11 10:06:49,813 INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\r\n2024-11-11 10:06:49,813 WETEXT INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\r\n2024-11-11 10:06:49,813 INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\r\n2024-11-11 10:06:49,813 WETEXT INFO skip building fst for zh_normalizer ...\r\n2024-11-11 10:06:49,813 INFO skip building fst for zh_normalizer ...\r\n2024-11-11 10:06:50,160 WETEXT INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\r\n2024-11-11 10:06:50,160 INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\r\n2024-11-11 10:06:50,160 WETEXT INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\r\n2024-11-11 10:06:50,160 INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\r\n2024-11-11 10:06:50,160 WETEXT INFO skip building fst for en_normalizer ...\r\n2024-11-11 10:06:50,160 INFO skip building fst for en_normalizer ...\r\n['中文女', '中文男', '日语男', '粤语女', '英文女', '英文男', '韩语女']\r\n2024-11-11 10:06:56,493 INFO input frame rate=25\r\n2024-11-11 10:06:57.617058157 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\r\n2024-11-11 10:06:57.617073624 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\r\n2024-11-11 10:06:57,792 WETEXT INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\r\n2024-11-11 10:06:57,792 WETEXT INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\r\n2024-11-11 10:06:57,792 INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\r\n2024-11-11 10:06:57,792 WETEXT INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\r\n2024-11-11 10:06:57,792 WETEXT INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\r\n2024-11-11 10:06:57,792 INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\r\n2024-11-11 10:06:57,792 WETEXT INFO skip building fst for zh_normalizer ...\r\n2024-11-11 10:06:57,792 WETEXT INFO skip building fst for zh_normalizer ...\r\n2024-11-11 10:06:57,792 INFO skip building fst for zh_normalizer ...\r\n2024-11-11 10:06:58,139 WETEXT INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\r\n2024-11-11 10:06:58,139 WETEXT INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\r\n2024-11-11 10:06:58,139 INFO found existing fst: /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\r\n2024-11-11 10:06:58,139 WETEXT INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\r\n2024-11-11 10:06:58,139 WETEXT INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\r\n2024-11-11 10:06:58,139 INFO                     /home/os/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\r\n2024-11-11 10:06:58,139 WETEXT INFO skip building fst for en_normalizer ...\r\n2024-11-11 10:06:58,139 WETEXT INFO skip building fst for en_normalizer ...\r\n2024-11-11 10:06:58,139 INFO skip building fst for en_normalizer ...\r\n  0%|                                                                                                                                                                  | 0/1 [00:00<?, ?it/s]2024-11-11 10:07:02,346 INFO synthesis text 收到好友从寄来的生日礼物,那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐,笑容如花般绽放。\r\n2024-11-11 10:07:11,595 INFO yield speech len 10.669569160997732, rtf 0.8668492293295423\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.60s/it]\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/637/comments",
    "author": "wantt",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-13T05:42:54Z",
        "body": "这个是wetextprocessing的原因，有空我们可以换一个例子，但前端问题不是这个repo的重点"
      },
      {
        "user": "wantt",
        "created_at": "2024-11-28T03:00:40Z",
        "body": "之前的版本也是 wetextprocessing 不丢字，最新版本持续发现丢字问题，比如： \r\n\r\n你是不是就想逗我玩啊?  -->  你是不是就想逗我玩?\r\n\r\n"
      }
    ]
  },
  {
    "number": 627,
    "title": "finetune 训练卡死",
    "created_at": "2024-11-07T10:18:00Z",
    "closed_at": "2024-11-08T05:56:04Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/627",
    "body": "正在finetune一个flow模块， 用6卡环境做的， 训练数据分到 156个不同的 块加载文件，每个文件包含的样本数小于等于1000个。\r\n训练到第一个epoch结束的时候，在 executor.py :47\r\n```\r\nwith model_context():\r\n   .... 执行完毕\r\n   这里卡死 hang住\r\n```    \r\n整个训练流程堵死了。 请问这种情况怎么处理呢？ 是否是 两个步骤forward对应一个backword的设置【accum_grad=2】，如果卡上的batch数不是2的整数倍，就会引发这种现象呢？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/627/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-08T01:32:57Z",
        "body": "数据极端的不均衡会导致卡死，可以试试train_dataset里设置partition=False试试，代码应该不需要修改"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-11-08T05:56:04Z",
        "body": "> 数据极端的不均衡会导致卡死，可以试试train_dataset里设置partition=False试试，代码应该不需要修改\r\n\r\n感谢，下修了训练的 prefetch 参数和timeout参数，现在可以正常运行了。 prefetch 100-> 1,   timeout 6000 -> 60"
      },
      {
        "user": "JohnHerry",
        "created_at": "2025-02-07T09:09:04Z",
        "body": "> 数据极端的不均衡会导致卡死，可以试试train_dataset里设置partition=False试试，代码应该不需要修改\n\n多卡训练时，设置partition=False, 岂不是所有卡看到的数据都是一样的？这样就失去ddp的意义了吧？"
      },
      {
        "user": "zw76859420",
        "created_at": "2025-02-12T08:11:47Z",
        "body": "> > 数据极端的不均衡会导致卡死，可以试试train_dataset里设置partition=False试试，代码应该不需要修改\n> \n> 感谢，下修了训练的 prefetch 参数和timeout参数，现在可以正常运行了。 prefetch 100-> 1, timeout 6000 -> 60\n\n这里看着有点像是IO问题，避免使用网盘DDP训练"
      },
      {
        "user": "JohnHerry",
        "created_at": "2025-02-12T08:23:45Z",
        "body": "不是IO层面的问题。 因为CosyVoice的IterableDataset， fix_length dynamic mini-batch的配置，可能会导致各个卡上分到的训练数据量不一致，导致分派数据多的卡对应的训练进程，等待数据少的卡对应的训练进程 同步梯度，而卡死。用开源的默认训练脚本的参数就不会卡死，因为这里最多等待60秒，就自己超时退出，从而开始下一个新epoch了。 我的训练卡死时因为误解了这个超时值的含义，把它调的太大了。"
      }
    ]
  },
  {
    "number": 616,
    "title": "modify the output sampling rate to 16000hz",
    "created_at": "2024-11-04T07:34:49Z",
    "closed_at": "2024-11-10T08:17:46Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/616",
    "body": "I tried to modify the output sampling rate to 16000hz, but the audio sounds clunky, how should I correctly output 16000Hz.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/616/comments",
    "author": "yanxp1227",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-05T04:53:29Z",
        "body": "use torchaudio.resample(22050, 16000)"
      },
      {
        "user": "yanxp1227",
        "created_at": "2024-11-05T09:15:00Z",
        "body": "> use torchaudio.resample(22050, 16000)\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 606,
    "title": "LOAD WAV AUTO CUTTER",
    "created_at": "2024-10-28T11:48:01Z",
    "closed_at": "2024-11-04T14:39:34Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/606",
    "body": "```\r\ndef load_wav(wav, target_sr):\r\n    speech, sample_rate = torchaudio.load(wav)\r\n    speech = speech.mean(dim=0, keepdim=True)\r\n    \r\n \r\n    if sample_rate != target_sr:\r\n        assert sample_rate > target_sr, f'wav sample rate {sample_rate} must be greater than {target_sr}'\r\n        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)(speech)\r\n    \r\n    \r\n    max_length = target_sr * 30  \r\n    if speech.size(1) > max_length:\r\n        speech = speech[:, :max_length]\r\n    \r\n    return speech\r\n```\r\n\r\n\r\nHello! When audio files exceed 30 seconds, they trigger an error. Automatically trimming any excess beyond 30 seconds would simplify the process. Instead of performing operations on the audio, saving it, and then changing the path name (which is time-consuming), this approach would be more efficient.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/606/comments",
    "author": "osmankrblt",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-04T03:27:46Z",
        "body": "we want the user to process the audio, simply truncate it may lead to undesired result, for example prompt wav and audio mismatch"
      },
      {
        "user": "osmankrblt",
        "created_at": "2024-11-04T14:39:33Z",
        "body": "Thank you for answer."
      }
    ]
  },
  {
    "number": 601,
    "title": "how does llm model prosody?",
    "created_at": "2024-10-28T01:58:56Z",
    "closed_at": "2024-11-04T04:02:16Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/601",
    "body": "\r\nWhich of the LLM inputs, speech_token or embedding, contains prosody information? Isn’t the speech_feat input for flow_matching supposed to carry prosody information? Does the LLM only focus on reconstructing semantic content, while flow_matching handles the reconstruction of prosody and timbre?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/601/comments",
    "author": "yangyazhou97",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-04T03:32:11Z",
        "body": "llm focus on semantic and prosody, llm also influence timbre, flow matching focus on timbre and audio quality"
      },
      {
        "user": "yangyazhou97",
        "created_at": "2024-11-04T04:02:14Z",
        "body": "Thanks a lot"
      }
    ]
  },
  {
    "number": 598,
    "title": "Confirmation of License?",
    "created_at": "2024-10-27T07:22:49Z",
    "closed_at": "2024-11-04T10:06:54Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/598",
    "body": "So code is Apache-2.0 license but does the same apply to models are they free to use for commercial purposes?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/598/comments",
    "author": "lukaLLM",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-11-04T03:33:01Z",
        "body": "yes model is also opensource"
      },
      {
        "user": "lukaLLM",
        "created_at": "2024-11-04T10:06:54Z",
        "body": "Thank you for your confirmation!"
      }
    ]
  },
  {
    "number": 594,
    "title": "“啊啊啊啊” 发音发不出来",
    "created_at": "2024-10-25T11:28:56Z",
    "closed_at": "2024-10-26T11:21:28Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/594",
    "body": "文本中有“啊”话，“啊”字发音发不出来，但少了口字旁的“阿”就没问题。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/594/comments",
    "author": "liuhuang31",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-26T11:19:10Z",
        "body": "把具体代码以及Prompt音频贴上来"
      },
      {
        "user": "liuhuang31",
        "created_at": "2024-10-26T11:21:17Z",
        "body": "> 把具体代码以及Prompt音频贴上来\r\n\r\n前端问题，wenetprocess会把“啊”字抹掉，换成ttsfrd就行了，谢谢回复～"
      }
    ]
  },
  {
    "number": 593,
    "title": "During single-machine multi-GPU training, the memory changes abnormally",
    "created_at": "2024-10-25T03:49:53Z",
    "closed_at": "2024-10-31T09:09:29Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/593",
    "body": "**Describe the bug**\r\nDuring single-machine multi-GPU training, the memory changes abnormally. \r\nTraining with 2 docker containers, which are bound to 1 Gpus and 2 GPUs \r\nbatchsize=2，single-machine 1 gpu is used for training, GPU memory is 26483 MIB\r\nbatchsize=2，single-machine 2 gpus is used for training, GPU 0 memory is 38335MB、GPU 1 memory is 45733MB\r\n\r\n\r\nUse the default Settings，cosyvoice.fromscratch.yaml，batch_type: 'static'，batchsize=2\r\n\r\ntrain.sh：\r\n\r\nstage=5\r\nstop_stage=5\r\npretrained_model_dir=./pretrained_models/CosyVoice-300M\r\n#export CUDA_VISIBLE_DEVICES=\"2\"\r\n#num_gpus=$(echo $CUDA_VISIBLE_DEVICES | awk -F \",\" '{print NF}')\r\nnum_gpus=2\r\njob_id=1986\r\ndist_backend=\"nccl\"\r\nnum_workers=2\r\nprefetch=100\r\ntrain_engine=torch_ddp\r\nif [ ${stage} -le 5 ] && [ ${stop_stage} -ge 5 ]; then\r\n  echo \"Run train. We only support llm traning for now. If your want to train from scratch, please use conf/cosyvoice.fromscratch.yaml\"\r\n  if [ $train_engine == 'deepspeed' ]; then\r\n    echo \"Notice deepspeed has its own optimizer config. Modify conf/ds_stage2.json if necessary\"\r\n  fi\r\n  # cat examples/libritts/cosyvoice/data/{train-clean-100,train-clean-360,train-other-500}/parquet/data.list > examples/libritts/cosyvoice/data/train.data.list\r\n  # cat examples/libritts/cosyvoice/data/{dev-clean,dev-other}/parquet/data.list > examples/libritts/cosyvoice/data/dev.data.list\r\n  for model in flow; do\r\n    torchrun --nnodes=1 --nproc_per_node=$num_gpus \\\r\n        --rdzv_id=$job_id --rdzv_backend=\"c10d\" --rdzv_endpoint=\"localhost:0\" \\\r\n      train.py \\\r\n      --train_engine $train_engine \\\r\n      --config examples/libritts-test/cosyvoice/conf/cosyvoice.fromscratch.yaml \\\r\n      --train_data examples/libritts-test/cosyvoice/data/train.data.list \\\r\n      --cv_data examples/libritts-test/cosyvoice/data/dev.data.list \\\r\n      --model $model \\\r\n      --model_dir `pwd`/exp/cosyvoice/$model/$train_engine \\\r\n      --tensorboard_dir `pwd`/tensorboard/cosyvoice/$model/$train_engine \\\r\n      --ddp.dist_backend $dist_backend \\\r\n      --num_workers ${num_workers} \\\r\n      --prefetch ${prefetch} \\\r\n      --pin_memory \\\r\n      --deepspeed_config examples/libritts/cosyvoice/conf/ds_stage2.json \\\r\n      --deepspeed.save_states model+optimizer\r\n  done\r\nfi\r\n\r\nWhy does memory get much larger when training on multiple Gpus？\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/593/comments",
    "author": "shenlou11",
    "comments": [
      {
        "user": "shenlou11",
        "created_at": "2024-10-25T03:54:51Z",
        "body": "When trained to epoch=2, the memory of both Gpus is 40G+"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-10-25T08:11:29Z",
        "body": "40g memory useage is normal, because each gpu each worker consumes memory"
      },
      {
        "user": "shenlou11",
        "created_at": "2024-10-25T08:53:57Z",
        "body": "> 40g memory useage is normal, because each gpu each worker consumes memory40g内存使用是正常的，因为每个gpu每个worker都消耗内存\r\n\r\nWhy in the same batchsize same num_workers situation, 1gpu, memory is 20G+? 2 gpu, each card memory is 40G+"
      },
      {
        "user": "shenlou11",
        "created_at": "2024-10-25T09:01:10Z",
        "body": "> > 40g memory useage is normal, because each gpu each worker consumes memory40g内存使用是正常的，因为每个gpu每个worker都消耗内存\r\n> \r\n> Why in the same batchsize same num_workers situation, 1gpu, memory is 20G+? 2 gpu, each card, the memory is 40G+\r\n\r\nNormally it should be 2 Gpus, each card  the memory is 20G+ ？\r\n"
      }
    ]
  },
  {
    "number": 526,
    "title": "ModuleNotFoundError: No module named 'onnxruntime'",
    "created_at": "2024-10-20T17:00:00Z",
    "closed_at": "2024-10-29T12:31:13Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/526",
    "body": "requirement.txt里面的包太老了。很多已经无法获取，如何解决？\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/526/comments",
    "author": "dandiago",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-22T03:26:31Z",
        "body": "可以更换版本，requirements里写死只是为了方便复现"
      }
    ]
  },
  {
    "number": 524,
    "title": "fade_in_out error when steam = True",
    "created_at": "2024-10-18T23:29:59Z",
    "closed_at": "2024-10-22T06:01:56Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/524",
    "body": "**Efficient tensor handling**: You're using .cpu() and .to(device) to move tensors between devices (CPU/GPU), which is fine for this operation. \r\n\r\n**Ensure** that this moving between devices doesn't introduce unnecessary overhead if used within large loops.\r\n\r\n**Clone operation**: The clone() method is a good choice here to avoid in-place modifications to the input fade_in_mel.\r\n\r\n**Window length check**: Ensure that the window’s length matches or exceeds the required overlap length.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/524/comments",
    "author": "RoyRonik",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-22T06:01:57Z",
        "body": "the change is not intended"
      }
    ]
  },
  {
    "number": 521,
    "title": "termination time solved",
    "created_at": "2024-10-18T16:56:44Z",
    "closed_at": "2024-10-22T05:59:38Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/521",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/521/comments",
    "author": "ARVINDH-CT06",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-22T05:59:38Z",
        "body": "we can add a switch button on webui to change from ZH to EN, but simply change the webui to EN is not intended"
      }
    ]
  },
  {
    "number": 518,
    "title": "Resolving GPU Timeout Issue During LLM Training",
    "created_at": "2024-10-18T16:40:07Z",
    "closed_at": "2024-10-22T05:56:35Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/518",
    "body": "This solution addresses the \"GPU communication timed out\" error encountered during the training of a large language model (LLM). The updated code includes gradient accumulation, mixed precision training (FP16), and batch size optimization to manage GPU memory usage and reduce operation times. Additionally, recommendations are provided for adjusting system timeout settings (TDR) to prevent GPU timeouts, ensuring a more stable and efficient training process. The solution focuses on optimizing model training without compromising performance or accuracy.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/518/comments",
    "author": "ARVINDH-CT06",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-22T05:56:35Z",
        "body": "I believe this is a false PR, I didn't see any relation with the repo"
      }
    ]
  },
  {
    "number": 512,
    "title": "Update bug_report.md",
    "created_at": "2024-10-18T09:46:11Z",
    "closed_at": "2024-10-25T05:34:44Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/512",
    "body": "bug_report in python has been cleared ",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/512/comments",
    "author": "Kalpana-Salva",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-22T05:55:11Z",
        "body": "what does the change mean? why change * to **? I didn't see any problem in bug_report issue template"
      }
    ]
  },
  {
    "number": 503,
    "title": "安装ttsfrd后出现libre2.so.11找不到的情况",
    "created_at": "2024-10-16T14:46:48Z",
    "closed_at": "2024-10-23T04:12:53Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/503",
    "body": "我是py3.10，上网找了3.10版本的ttsfrd进行pip install之后发现libre2.so.11: cannot open shared object file: No such file or directory。请问这个如何解决呢？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/503/comments",
    "author": "czydfj",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-16T15:48:04Z",
        "body": "不清楚，建议使用wetextprocessing有人维护"
      }
    ]
  },
  {
    "number": 486,
    "title": "12秒的参考音频时卡着一直无法生成出音频",
    "created_at": "2024-10-13T01:52:40Z",
    "closed_at": "2024-10-16T14:47:06Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/486",
    "body": "想请问一下zero shot模式下输入参考音频的时候，是否有时长的限制呢？\r\n\r\n我试了一下6秒的参考音频时可以很快输出结果的，但是加到12秒，它就一直卡着不动，十几分钟了。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/486/comments",
    "author": "czydfj",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-16T05:20:00Z",
        "body": "把具体调用代码以及prompt音频贴上来"
      }
    ]
  },
  {
    "number": 475,
    "title": "最后生成的语音只能是22050采样率吗？我改成16000，生成的语音很奇怪",
    "created_at": "2024-10-11T06:05:33Z",
    "closed_at": "2024-10-16T05:25:21Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/475",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/475/comments",
    "author": "hjj-lmx",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-16T05:25:21Z",
        "body": "因为返回结果就是22050采样率"
      }
    ]
  },
  {
    "number": 449,
    "title": "部分标点符号会导致语音生成中断",
    "created_at": "2024-09-29T03:24:43Z",
    "closed_at": "2024-09-29T07:26:19Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/449",
    "body": "```\r\nfrom cosyvoice.cli.cosyvoice import CosyVoice\r\nimport os\r\nimport torch\r\nimport gc\r\nimport random\r\nimport io\r\nimport soundfile as sf\r\nimport librosa\r\nimport torchaudio\r\n\r\n# os.environ[\"PYTHONPATH\"] = \"/workspace/third_party/Matcha-TTS\" # export PYTHONPATH=/workspace/third_party/Matcha-TTS\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    content = \"“明天会更好！”他说道\"\r\n    sr = 22050\r\n\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(7)\r\n    torch.cuda.empty_cache()\r\n    gc.collect()\r\n\r\n    #####################\r\n    cosyvoice = CosyVoice('/workspace/CosyVoice-300M-SFT')\r\n    #####################\r\n\r\n    speaker = random.sample(['中文男', '中文女'], 1)[0]\r\n    output = cosyvoice.inference_sft(content, speaker)\r\n\r\n    audio_tensor = output['tts_speech']\r\n\r\n    audio_np = audio_tensor.squeeze().cpu().numpy() \r\n    duration = audio_tensor.size(1) / sr\r\n\r\n    byte_io = io.BytesIO()\r\n    sf.write(byte_io, audio_np, sr, format='WAV')\r\n    byte_data = byte_io.getvalue()\r\n    audio, _ = librosa.load(io.BytesIO(byte_data), sr=sr)\r\n\r\n    audio_tensor, sr = torchaudio.load(io.BytesIO(byte_data))\r\n    torchaudio.save(f\"output.wav\", audio_tensor, sr)\r\n    \r\n\r\n    print(\"saved ...\")\r\n\r\n    byte_io.close()\r\n```\r\n\r\n感叹号之后的的内容就无法生成了\r\n\r\n另外，我发现如果我使用 `os.environ[\"PYTHONPATH\"] = \"/workspace/third_party/Matcha-TTS\"`代码就无法识别`PYTHONPATH`这个环境变量，但是我在外部使用`export PYTHONPATH=/workspace/third_party/Matcha-TTS`就可以识别，为什么呢？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/449/comments",
    "author": "Timaos123",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-29T07:26:14Z",
        "body": "更新代码，无法复现丢句情况，看一下日志是否是前段后句子丢失。\r\nsys.path.append('{}/third_party/Matcha-TTS'.format(ROOT_DIR))"
      }
    ]
  },
  {
    "number": 448,
    "title": "fix nucleus_sampling",
    "created_at": "2024-09-28T14:31:52Z",
    "closed_at": "2024-10-16T05:35:33Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/448",
    "body": "When the first value of `prob` is greater than 0.8 and its `ids` is `speech_token_size` (4096), it causes `prob.multinomial` to always select this value, ultimately resulting in an infinite loop (`cosyvoice/llm/llm.py` sampling_ids() method). \r\n\r\n**Fix:** Ensure that `prob` always has at least two values for `multinomial`.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/448/comments",
    "author": "zhuzizyf",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-16T05:36:16Z",
        "body": "这确实是个问题，但本质应该是看为什么llm给出了这样的结果，直接规则跳过可能会给出不正常的合成结果"
      }
    ]
  },
  {
    "number": 447,
    "title": "inference_cross_lingual about - ",
    "created_at": "2024-09-28T11:45:18Z",
    "closed_at": "2024-11-13T02:34:28Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/447",
    "body": "- reads to mnius in chinese!\r\n- 语音-文本基础模型\r\n读作：语音减文本基础模型。。。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/447/comments",
    "author": "jacksonjack001",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-29T01:37:07Z",
        "body": "you can remove the '-',  because in training data, '-' read as minus in most cases"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-29T02:38:10Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-13T02:34:28Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "number": 446,
    "title": "Is the flow model trained with half-precision works well?",
    "created_at": "2024-09-27T11:13:28Z",
    "closed_at": "2024-10-10T01:19:55Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/446",
    "body": "In the pretained opensource stack, the llm support half inference but not flow model.  If I train the flow with deepspeed and get a half-precision float model, will it works well?  will it get good speech quality?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/446/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-29T01:36:09Z",
        "body": "not amp trained, we will update cosyvoice-300m and coysvoice-300m-25hz with amp training later"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-10-10T01:19:55Z",
        "body": "ok， thank you for the help"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-10-10T01:26:32Z",
        "body": "> ok， thank you for the help\r\n by the way,  what I mean is to train the base model from scratch. not to fine tune on the released pretraines.\r\n\r\n"
      }
    ]
  },
  {
    "number": 444,
    "title": "[Fix] remove the speaker embedding in the llm inputs during instruct fine-tuning",
    "created_at": "2024-09-27T06:42:48Z",
    "closed_at": "2024-11-16T15:11:10Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/444",
    "body": "1. 问题：在训练阶段，LLM输入序列中有speaker embedding。在指令微调模型推理时LLM输入序列没有speaker embedding。因此对于指令微调模型而言，训练和推理存在mismatch，效果不理想。\r\n2. 解决方案：LLM增加instruct_finetuning的参数，缺省值为False。当该参数为True时，训练阶段也会去掉LLM输入序列中的speaker embedding，从而保证指令微调模型可以正常训练和推理。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/444/comments",
    "author": "Shengqiang-Li",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-16T05:39:25Z",
        "body": "在训练instruct时，llm保留embedding其实也会有效果，只是我们之前对比实验最后开源的是去掉embedding的版本。在instruct finetune时希望用户多多自己探索，我们的也不一定是最佳方案。因此这个Pr会保留，但暂时不考虑合并"
      }
    ]
  },
  {
    "number": 443,
    "title": "Misreading words, missing sentences or repeating reading happens very often.",
    "created_at": "2024-09-27T05:00:39Z",
    "closed_at": "2024-11-13T02:34:30Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/443",
    "body": "Misreading words, missing sentences or repeating reading happens very often.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/443/comments",
    "author": "jdola",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-29T02:04:09Z",
        "body": "please provide an exact example of the bug"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-29T02:38:12Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-13T02:34:30Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "number": 441,
    "title": "模型里面的llm是否支持单字流式输入进行编码",
    "created_at": "2024-09-27T02:30:32Z",
    "closed_at": "2024-11-14T02:35:05Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/441",
    "body": "你好咨询下，目前模型里的llm模块是一段文本输入进行编码，然后送入flow进行音频合成，想问下是否支持流式单字输入进行编码，等单字编码的token数满足self.token_min_hop_len，然后送入flow进行音频合成",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/441/comments",
    "author": "wang-TJ-20",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-29T01:35:19Z",
        "body": "no, we only support sentence level stream input"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-29T02:38:13Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-14T02:35:04Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      },
      {
        "user": "Hkaisense",
        "created_at": "2024-12-06T03:01:03Z",
        "body": "我看阿里云的官网应用是支持大模型生成的单字直接输入的开源版本不支持吗？"
      }
    ]
  },
  {
    "number": 439,
    "title": "Length limit of 25Hz s3tokenizer",
    "created_at": "2024-09-26T08:30:56Z",
    "closed_at": "2024-10-11T07:30:33Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/439",
    "body": "It seems now it downsamples mel 4x first then feeding into transformer, and transformer's pos emb limit is still 1500, which would be 60 seconds of audio.\r\n\r\nso max length of tokenizer becomes 60s. am i right?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/439/comments",
    "author": "sphmel",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-29T01:33:11Z",
        "body": "yes"
      }
    ]
  },
  {
    "number": 438,
    "title": "语音克隆时，300M 或 300HZ 对两个字的文本生成空语音",
    "created_at": "2024-09-26T08:30:40Z",
    "closed_at": "2024-10-20T05:17:21Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/438",
    "body": "现象：生成的语音是空的，或者说没有能识别的人声。\r\n功能：语音克隆\r\n输入文本：好的、收到、测试  等这些两个字的短文本\r\n复现概率：目前看跟上传的要模仿的音频有关，差不多有50%的概率能复现",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/438/comments",
    "author": "m358807551",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-10-16T05:32:25Z",
        "body": "把具体代码和prompt音频贴上来"
      },
      {
        "user": "m358807551",
        "created_at": "2024-10-20T05:17:21Z",
        "body": "最近测试没有复现，先 close了。"
      }
    ]
  },
  {
    "number": 433,
    "title": "why the length of the final chunk changes even when using the same input (streaming mode)?",
    "created_at": "2024-09-25T09:35:43Z",
    "closed_at": "2024-11-15T02:42:47Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/433",
    "body": "Hi, I repeated the streaming generation several times with the same input but I found that the length of the final yielded chunk changes every time. As you can see below, the  yield speech len of the final chunk across the three generated samples are different, i.e., 0.8475s, 2.3104s, and 2.5890s, respectively. Does anyone know why this happened? \r\n\r\n```shell\r\n\r\n100%|██████████| 1/1 [00:16<00:00, 16.68s/it]\r\n100%|██████████| 1/1 [00:16<00:00, 16.68s/it]\r\n\r\n2024-09-24 10:06:23,630 INFO synthesis text 我是通义实验室语音团队全新推出的生成式语音大模型,提供舒适自然的语音合成能力、\r\n2024-09-24 10:06:23,630 INFO prompt speech len 3.0875\r\n2024-09-24 10:06:25,734 INFO yield speech len 1.7647 | cost 2.1032 | rtf 1.1918 | init_delay 2.1025\r\n2024-09-24 10:06:27,046 INFO yield speech len 1.9969 | cost 1.3117 | rtf 0.6568 | init_delay 2.1025\r\n2024-09-24 10:06:28,371 INFO yield speech len 1.9969 | cost 1.3250 | rtf 0.6635 | init_delay 2.1025\r\n2024-09-24 10:06:29,508 INFO yield speech len 1.9969 | cost 1.1365 | rtf 0.5692 | init_delay 2.1025\r\n2024-09-24 10:06:30,058 INFO yield speech len 0.8475 | cost 0.5502 | rtf 0.6492 | init_delay 6.4269\r\n\r\n100%|██████████| 1/1 [00:06<00:00,  6.51s/it]\r\n100%|██████████| 1/1 [00:06<00:00,  6.51s/it]\r\n\r\n2024-09-24 10:06:30,161 INFO synthesis text 我是通义实验室语音团队全新推出的生成式语音大模型,提供舒适自然的语音合成能力、\r\n2024-09-24 10:06:30,161 INFO prompt speech len 3.0875\r\n2024-09-24 10:06:32,260 INFO yield speech len 1.7647 | cost 2.0989 | rtf 1.1894 | init_delay 2.0981\r\n2024-09-24 10:06:33,566 INFO yield speech len 1.9969 | cost 1.3063 | rtf 0.6542 | init_delay 2.0981\r\n2024-09-24 10:06:34,890 INFO yield speech len 1.9969 | cost 1.3240 | rtf 0.6630 | init_delay 2.0981\r\n2024-09-24 10:06:56,889 INFO yield speech len 2.3104 | cost 21.9980 | rtf 9.5214 | init_delay 26.7272\r\n\r\n100%|██████████| 1/1 [00:26<00:00, 26.81s/it]\r\n100%|██████████| 1/1 [00:26<00:00, 26.81s/it]\r\n\r\n2024-09-24 10:06:56,995 INFO synthesis text 我是通义实验室语音团队全新推出的生成式语音大模型,提供舒适自然的语音合成能力、\r\n2024-09-24 10:06:56,995 INFO prompt speech len 3.0875\r\n2024-09-24 10:06:59,463 INFO yield speech len 1.7647 | cost 2.4683 | rtf 1.3987 | init_delay 2.4677\r\n2024-09-24 10:07:00,469 INFO yield speech len 1.9969 | cost 1.0056 | rtf 0.5036 | init_delay 2.4677\r\n2024-09-24 10:07:01,680 INFO yield speech len 1.9969 | cost 1.2103 | rtf 0.6061 | init_delay 2.4677\r\n2024-09-24 10:07:13,595 INFO yield speech len 2.5890 | cost 11.9148 | rtf 4.6020 | init_delay 16.5990\r\n```",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/433/comments",
    "author": "huskyachao",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-26T02:06:33Z",
        "body": "there is random function in llm sampling"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-26T02:32:03Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-15T02:42:47Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "number": 429,
    "title": "vioce merge",
    "created_at": "2024-09-24T06:42:49Z",
    "closed_at": "2024-11-09T02:28:59Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/429",
    "body": "There is a demonstration of the fusion of male and female timbres in the example. How do you achieve natural timbre fusion?\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/429/comments",
    "author": "CuiRobert",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-24T07:15:55Z",
        "body": "well we select these embedding fusion manually"
      },
      {
        "user": "CuiRobert",
        "created_at": "2024-09-24T07:26:03Z",
        "body": "> well we select these embedding fusion manually\r\n\r\nthx! Do you use zero-shot or sft for fusion?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-25T02:37:27Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-09T02:28:59Z",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "number": 423,
    "title": "fastapi: client.py  Always reporting errors  ValueError: buffer size must be a multiple of element size",
    "created_at": "2024-09-23T07:27:18Z",
    "closed_at": "2024-09-25T07:16:52Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/423",
    "body": "**Describe the bug**\r\nwhen i use fastapi run the client.py,  Always reporting errors\r\nTraceback (most recent call last):\r\n  File \"client.py\", line 98, in <module>\r\n    main()\r\n  File \"client.py\", line 59, in main\r\n    tts_speech = torch.from_numpy(np.array(np.frombuffer(tts_audio, dtype=np.int16))).unsqueeze(dim=0)\r\nValueError: buffer size must be a multiple of element size\r\n\r\nthe client.py code :\r\n\r\n    tts_audio = b''\r\n    for r in response.iter_content(chunk_size=16000):\r\n        tts_audio += r\r\n    tts_speech = torch.from_numpy(np.array(np.frombuffer(tts_audio, dtype=np.int16))).unsqueeze(dim=0)\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/423/comments",
    "author": "likuangyu01",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-23T15:18:46Z",
        "body": "how did you run the code, are you using readme example?"
      },
      {
        "user": "likuangyu01",
        "created_at": "2024-09-24T10:06:33Z",
        "body": "> how did you run the code, are you using readme example?\r\n\r\n已经解决了，在client.py中加了一点判断的代码\r\n        tts_audio = b''\r\n        for r in response.iter_content(chunk_size=16000):\r\n            tts_audio += r\r\n\r\n        # 检查字节长度\r\n        if len(tts_audio) % 2 != 0:\r\n            tts_audio = tts_audio[:-1]  # 去掉最后一个字节"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-09-24T10:16:41Z",
        "body": "返回奇数个字节应该是有bug，你是官方推理示例吗？"
      },
      {
        "user": "likuangyu01",
        "created_at": "2024-09-25T07:15:41Z",
        "body": "> 返回奇数个字节应该是有bug，你是官方推理示例吗？\r\n\r\n是的，我用的是官方的推理示例， bug原因我找到了，是因为 没有配置 export MODEL_DIR=/path/CosyVoice/pretrained_models/CosyVoice-300M-SFT， 当client.py 发送请求后，server.py 运行model_output = cosyvoice.inference_sft(tts_text, spk_id)这段代码时，cosyvoice未被定义引起的bug，原因是if __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--port',\r\n                        type=int,\r\n                        default=50000)\r\n    parser.add_argument('--model_dir',\r\n                        type=str,\r\n                        default='iic/CosyVoice-300M',\r\n                        help='local path or modelscope repo id')\r\n    args = parser.parse_args()\r\n    cosyvoice = CosyVoice(args.model_dir)  , cosyvoice 被定义在if __name__ == '__main__':条件之下，当client.py 发送请求调用async def inference_sft(tts_text: str = Form(), spk_id: str = Form()): 函数时，它并不能找到cosyvoice 对象，解决办法就是在直接在server.py文件里定义 cosyvoice = CosyVoice(\"/root/CosyVoice/pretrained_models/CosyVoice-300M\") 为全局变量就可以了\r\n"
      }
    ]
  },
  {
    "number": 409,
    "title": "About instant-feature-extraction training",
    "created_at": "2024-09-20T02:07:26Z",
    "closed_at": "2024-09-27T08:40:21Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/409",
    "body": "We are tring to make instant-feature-extraction training of flow model, because the feature extraction is so costly on large amount of samples.  we can make instant-feature-extraction and feature cache to replace the offline-feature extraction.   \r\nthe problem is about the pretrained S3  speech token extractor.  it is an ONNX model, we can write a dataloader function about S3 speech token extraction on CPU, but it is too slow. while using GPU onnx session  in multi-card distribution training, it will always report some kind of exceptions. When we use a global onnxruntime-gpu session in all dataloaders,  report like  \r\n `disk.__verify_params_across_processes(process_group, tensors, loger)   NCCL error in:  ../torch.cscr/distributed/c10d/NCCLUtils.hpp:275, invalid usage, This usually reflects invalid usage of NCCL library. Last error: Duplicate GPU dected: rank3 and rank0 both on CUDA device 34000`   \r\nwhen we create a independent onnxruntime-gpu sessiion in each dataloader instance,  it will report:   \r\n`E:onnxruntime:Default, cuda_call.cc:118  CudaCall] CUDA failure 3: initialization error; GPU=0; hostname=XXX; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc; line=422; expr=cudaSetDevice(GetDeviceId()) `   \r\nour code about speechtoken extraction is from the cosyvoice example tools code.    \r\nSo, is there any suggestion to make stable instant-feature-extraction traning on distributed training of cosyvoide flow?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/409/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-20T08:33:53Z",
        "body": "dataloader is multiprocessing, I think onnx do not support multi-process inference using one instance"
      }
    ]
  },
  {
    "number": 405,
    "title": "Checkpoints for LibriTTS experiments from paper?",
    "created_at": "2024-09-18T17:34:10Z",
    "closed_at": "2024-09-19T03:29:02Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/405",
    "body": "Hey there! Been testing this model out for a while now and have been loving the results.\r\n\r\nIs there a chance that you would be able to release checkpoints for the LibriTTS experiments performed in the paper?\r\n\r\nI've been looking to deploy the model on edge devices using English as its only language, but the RTF is unfortunately too high with the standard multi-lingual model (approx. 1.7-2.0). This is a shame since the quality of the model far outperforms anything else I've tested (FishSpeech, XTTSv2, OpenVoice, etc.)\r\n\r\nIt'd be really helpful for users if the checkpoints for the experiments (particularly Exp-3-LibriTTS from the paper) were released. It seems like the tiny model would be very performant while still having good enough accuracy, and it'd help the paper have even more reach and impact if users could access these results\r\n\r\nHope it'd be possible for you guys to release these, many thanks",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/405/comments",
    "author": "lcflint",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-19T01:48:49Z",
        "body": "we will release a 25hz model maybe in this month, the rtf should reduce by half. I think checkpoints form libritts is not suitable for deployment because its training data is far not enough."
      },
      {
        "user": "lcflint",
        "created_at": "2024-09-19T03:29:02Z",
        "body": "That sounds good, thanks for the heads up"
      }
    ]
  },
  {
    "number": 400,
    "title": "sft 中文男 声音突然会变小",
    "created_at": "2024-09-14T08:57:42Z",
    "closed_at": "2024-10-17T05:35:10Z",
    "labels": [
      "stale"
    ],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/400",
    "body": "使用sft生成长文本音频时，有一段音频生成的声音会很小，请问该如何解决？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/400/comments",
    "author": "xipingL",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-14T16:38:31Z",
        "body": "you can try sox wav normalization, adjust the wav to a uniform energy"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-15T02:37:17Z",
        "body": "This issue is stale because it has been open for 30 days with no activity."
      }
    ]
  },
  {
    "number": 356,
    "title": "Implemented fast processing of extract_embedding",
    "created_at": "2024-09-05T11:34:51Z",
    "closed_at": "2024-09-18T08:16:46Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/356",
    "body": "because we run this model on the processor. We can run parallel processing of the dataset.\r\nIn my experiment, I accelerated the preprocessing of the dataset from 2 hours to 30 minutes.\r\n\r\nMoreover, we work with dicts, so it doesn't matter to us in what order the data is processed.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/356/comments",
    "author": "MiXaiLL76",
    "comments": [
      {
        "user": "MiXaiLL76",
        "created_at": "2024-09-05T11:42:54Z",
        "body": "```bash\r\nflake8 --max-line-length 150 --ignore B006,B008,B905,C408,E402,E741,W503,W504 --exclude ./third_party/,./runtime/python/grpc/cosyvoice_pb2*py\r\n./cosyvoice/utils/scheduler.py:92:75: E231 missing whitespace after ','\r\n./cosyvoice/utils/train_utils.py:133:9: F811 redefinition of unused 'scheduler' from line 127\r\n```\r\n\r\nI have a newer version of lintner, but I don't think that's a problem."
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-09-06T01:46:08Z",
        "body": "yes, use multi thread can increase the throughput, but I don't think using queue is a good idea. you can use threadpool.map, this can make the code more clear. for example \r\n\r\n、、、\r\n    with Pool(processes=num_workers) as pool:\r\n        predictions = pool.map(single_job, tasks)\r\n、、、"
      },
      {
        "user": "MiXaiLL76",
        "created_at": "2024-09-06T08:09:08Z",
        "body": "> yes, use multi thread can increase the throughput, but I don't think using queue is a good idea. you can use threadpool.map, this can make the code more clear. for example\r\n> \r\n> 、、、 with Pool(processes=num_workers) as pool: predictions = pool.map(single_job, tasks) 、、、\r\n\r\nThe implementation of concurrent.futures looks not bad, I thought that onnx can't work in this way."
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-09-06T08:31:21Z",
        "body": "麻烦按意见修改一下，改为了merge到dev/lyuxiang.lx，我这边测过后再会和几个新的修改统一merge到main，谢谢 @MiXaiLL76 "
      }
    ]
  },
  {
    "number": 351,
    "title": "Make the FastAPI server available",
    "created_at": "2024-09-04T02:31:56Z",
    "closed_at": "2024-09-05T05:37:17Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/351",
    "body": "Correct the use of generator in response function",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/351/comments",
    "author": "onthia",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-05T05:37:18Z",
        "body": "close this pr as we have already change fastapi to stream response, will merge soon"
      }
    ]
  },
  {
    "number": 347,
    "title": "export onnx",
    "created_at": "2024-09-03T03:14:47Z",
    "closed_at": "2024-09-03T03:25:15Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/347",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/347/comments",
    "author": "hexisyztem",
    "comments": [
      {
        "user": "vonchenplus",
        "created_at": "2024-11-18T01:54:28Z",
        "body": "@hexisyztem  为什么移除了trt的导出方案？"
      },
      {
        "user": "hexisyztem",
        "created_at": "2024-11-18T02:04:00Z",
        "body": "> @hexisyztem 为什么移除了trt的导出方案？\r\n\r\n有计算精度损失，不太能作为通用方案对外发布。"
      }
    ]
  },
  {
    "number": 345,
    "title": "修正了在cpu下跑的错误",
    "created_at": "2024-09-01T13:23:45Z",
    "closed_at": "2024-09-05T05:36:58Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/345",
    "body": "1、webui.py 增加了load_jit的配置\r\n2、model.py增加了cpu模式下采用float的方式",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/345/comments",
    "author": "szsteven008",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-09-05T05:36:58Z",
        "body": "close this pr as we have already fixed in, will merge it soon"
      },
      {
        "user": "czydfj",
        "created_at": "2024-09-29T09:55:35Z",
        "body": "@aluminumbox \r\n\r\nHello！I meet the following error when I run the [webui.py] of the latest branch:\r\nTraceback (most recent call last):\r\n  File \"/data/home/ghostchen/miniconda3/envs/cosyvoice/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"/data/home/ghostchen/miniconda3/envs/cosyvoice/lib/python3.8/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/data/home/ghostchen/CosyVoice/cosyvoice/cli/model.py\", line 84, in llm_job\r\n    for i in self.llm.inference(text=text.to(self.device),\r\n  File \"/data/home/ghostchen/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\r\n    response = gen.send(None)\r\n  File \"/data/home/ghostchen/CosyVoice/cosyvoice/llm/llm.py\", line 172, in inference\r\n    text, text_len = self.encode(text, text_len)\r\n  File \"/data/home/ghostchen/CosyVoice/cosyvoice/llm/llm.py\", line 75, in encode\r\n    encoder_out, encoder_mask = self.text_encoder(text, text_lengths, decoding_chunk_size=1, num_decoding_left_chunks=-1)\r\n  File \"/data/home/ghostchen/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\nRuntimeError: The following operation failed in the TorchScript interpreter.\r\nTraceback of TorchScript, serialized code (most recent call last):\r\n  File \"code/__torch__/cosyvoice/transformer/encoder/___torch_mangle_5.py\", line 22, in forward\r\n    masks = torch.bitwise_not(torch.unsqueeze(mask, 1))\r\n    embed = self.embed\r\n    _0 = torch.add(torch.matmul(xs, CONSTANTS.c0), CONSTANTS.c1)\r\n                   ~~~~~~~~~~~~ <--- HERE\r\n    input = torch.layer_norm(_0, [1024], CONSTANTS.c2, CONSTANTS.c3)\r\n    pos_enc = embed.pos_enc\r\n\r\nTraceback of TorchScript, original code (most recent call last):\r\nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'"
      },
      {
        "user": "jacksonzjh",
        "created_at": "2024-10-14T06:21:33Z",
        "body": "Traceback (most recent call last):\r\n  File \"webui.py\", line 188, in <module>\r\n    cosyvoice = CosyVoice(args.model_dir, args.load_jit)\r\n  File \"/usr/bin/MetaMonAIoT/TTS/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 45, in __init__\r\n    self.model.load_jit('{}/llm.text_encoder.fp16.zip'.format(model_dir),\r\nTypeError: load_jit() takes 3 positional arguments but 4 were given\r\n(cosyvoice) -bash-4.2# \r\n"
      }
    ]
  },
  {
    "number": 315,
    "title": "Duoren",
    "created_at": "2024-08-23T07:04:24Z",
    "closed_at": "2024-08-26T07:34:22Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/315",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/315/comments",
    "author": "AssassinQuin",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-26T07:34:22Z",
        "body": "this is a auto mix speaker script? This is not the main feature of this repo, more like a downstream task. We will close this PR. thank you for your interest"
      }
    ]
  },
  {
    "number": 314,
    "title": "训练一种新的语言该怎么设置如下内容？",
    "created_at": "2024-08-22T10:52:23Z",
    "closed_at": "2024-08-27T06:10:20Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/314",
    "body": "感谢大佬的工作，如果我要训练法语，下面的language 需要修改吗？ 期待回复\r\nget_tokenizer: !name:whisper.tokenizer.get_tokenizer\r\n    multilingual: True\r\n    num_languages: 100\r\n    language: 'en'\r\n    task: 'transcribe'",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/314/comments",
    "author": "Wentao795",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-26T07:14:13Z",
        "body": "no, whisper has french language label, check whisper.tokenizer.get_tokenizer detail and add <|fr|>"
      }
    ]
  },
  {
    "number": 311,
    "title": "报错信息：pydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha'",
    "created_at": "2024-08-22T08:19:42Z",
    "closed_at": "2024-08-23T03:03:49Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/311",
    "body": "系统：MacOS-12.6.2\r\n运行过程中报错信息如下：\r\npydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha'\r\n\r\n是PYTHONPATH环境变量引入错误么？\r\n通过命令echo $PYTHONPATH查看，third_party/Matcha-TTS已加入环境变量，为什么还是报错。。。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/311/comments",
    "author": "yufengzhixing",
    "comments": [
      {
        "user": "LAM-0706",
        "created_at": "2024-08-22T20:12:38Z",
        "body": "同样的问题 File \"/opt/miniconda3/envs/cosyvoice/lib/python3.8/pydoc.py\", line 358, in safeimport\r\n    raise ErrorDuringImport(path, sys.exc_info())\r\npydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha'\r\n"
      },
      {
        "user": "yufengzhixing",
        "created_at": "2024-08-23T02:45:53Z",
        "body": "自问自答一波，我没有按说明文档安装，python改成3.9版本，然后直接安装matcha-tts，其他与文档一致，成功了"
      },
      {
        "user": "NorthhhFishSoup",
        "created_at": "2024-09-03T12:35:24Z",
        "body": "你是不是没从git下载，你检查一下third_party下面的Matcha-TTS是不是空的"
      }
    ]
  },
  {
    "number": 291,
    "title": "About “instruct inference”",
    "created_at": "2024-08-15T09:27:06Z",
    "closed_at": "2024-08-26T09:52:08Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/291",
    "body": "I see that will del spk_embedding, spk_embedding may only use for \"flow_embedding\";then, when instruct inference, how to control Timbre？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/291/comments",
    "author": "sunnnnnnnny",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-16T02:01:09Z",
        "body": "you can try control timbre using prompt audio in instruct mode, but we cannot assure its quality"
      }
    ]
  },
  {
    "number": 288,
    "title": "<break time=\\\"1s\\\">",
    "created_at": "2024-08-15T02:30:37Z",
    "closed_at": "2024-08-16T02:37:06Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/288",
    "body": "Does the mark \"<break time=\\\"1s\\\">\" work?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/288/comments",
    "author": "dgg8411",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-16T02:37:06Z",
        "body": "no"
      }
    ]
  },
  {
    "number": 283,
    "title": "man，how do i make it work？",
    "created_at": "2024-08-14T03:28:51Z",
    "closed_at": "2024-08-14T05:55:42Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/283",
    "body": "(cosyvoice) gcf@gcf:/mnt/d/python/CosyVoice$ python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M\r\n2024-08-14 11:26:02,976 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\r\n2024-08-14 11:26:02,980 - modelscope - INFO - Loading ast index from /home/gcf/.cache/modelscope/ast_indexer\r\n2024-08-14 11:26:02,997 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 71036b8baf6ba232175cc7c60065032e and a total number of 980 components indexed\r\nfailed to import ttsfrd, use WeTextProcessing instead\r\n/home/gcf/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/torch/_jit_internal.py:726: FutureWarning: ignore(True) has been deprecated. TorchScript will now drop the function call on compilation. Use torch.jit.unused now. {}\r\n  warnings.warn(\r\n/home/gcf/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\r\n  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\r\n2024-08-14 11:26:07,496 INFO input frame rate=50\r\nTraceback (most recent call last):\r\n  File \"webui.py\", line 190, in <module>\r\n    cosyvoice = CosyVoice(args.model_dir)\r\n  File \"/mnt/d/python/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 30, in __init__\r\n    self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'],\r\n  File \"/mnt/d/python/CosyVoice/cosyvoice/cli/frontend.py\", line 52, in __init__\r\n    self.campplus_session = onnxruntime.InferenceSession(campplus_model, sess_options=option, providers=[\"CPUExecutionProvider\"])\r\n  File \"/home/gcf/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 419, in __init__\r\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n  File \"/home/gcf/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 460, in _create_inference_session\r\n    sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidProtobuf: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Load model from pretrained_models/CosyVoice-300M/campplus.onnx failed:Protobuf parsing failed.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/283/comments",
    "author": "askie",
    "comments": [
      {
        "user": "zaiji100",
        "created_at": "2024-09-05T12:41:29Z",
        "body": "这是怎么解决的呀？同样的问题，按照 README 一步一步来的"
      },
      {
        "user": "blair319",
        "created_at": "2025-01-08T06:45:38Z",
        "body": "这个解决了？我现在也遇到了这个问题，我已经安装 git lfs了"
      }
    ]
  },
  {
    "number": 276,
    "title": "batch size for multi-lingual model",
    "created_at": "2024-08-12T03:54:31Z",
    "closed_at": "2024-08-13T06:20:23Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/276",
    "body": "hi, I want to train a 300M LLM following the config in the CosyVoice paper. I wonder the how to set \"max_frames_in_batch\" and \"accum_grad\" in the config.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/276/comments",
    "author": "zhangyike",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-12T16:09:31Z",
        "body": "set max_frames_in_batch according to your gpu memory, bigger max_frames_in_batch helps the training process stable"
      },
      {
        "user": "zhangyike",
        "created_at": "2024-08-13T04:01:16Z",
        "body": "> set max_frames_in_batch according to your gpu memory, bigger max_frames_in_batch helps the training process stable\r\n\r\nThanks. Do I need to set 'accum_grad' larger than 1? Or what is the preferred value for accum_grad when 64 V100 are used?"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-08-13T04:03:25Z",
        "body": "> > set max_frames_in_batch according to your gpu memory, bigger max_frames_in_batch helps the training process stable\r\n> \r\n> Thanks. Do I need to set 'accum_grad' larger than 1? Or what is the preferred value for accum_grad when 64 V100 are used?\r\n\r\nwell accum_grad larger also makes training more stable. but you have 64 v100, I think accum_grad is not so important"
      }
    ]
  },
  {
    "number": 272,
    "title": "字幕怎么生成？",
    "created_at": "2024-08-09T12:44:44Z",
    "closed_at": "2024-08-10T01:01:11Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/272",
    "body": "请问可以同时生成字幕吗？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/272/comments",
    "author": "Heey731",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-10T01:01:11Z",
        "body": "you can generate sentence level timestamp by calculating synthesis audio length"
      }
    ]
  },
  {
    "number": 259,
    "title": "请问下他一次最多可以转换多少字",
    "created_at": "2024-08-07T03:07:40Z",
    "closed_at": "2024-08-10T01:10:07Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/259",
    "body": "请问下他一次最多可以转换多少字",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/259/comments",
    "author": "anstonjie",
    "comments": [
      {
        "user": "anstonjie",
        "created_at": "2024-08-07T03:33:45Z",
        "body": "  File \"/home/CosyVoice/cosyvoice/transformer/encoder_layer.py\", line 93, in forward\r\n    x_att, new_att_cache = self.self_attn(x, x, x, mask, pos_emb=pos_emb, cache=att_cache)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/cosyvoice/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/CosyVoice/cosyvoice/transformer/attention.py\", line 323, in forward\r\n    scores = (matrix_ac + matrix_bd) / math.sqrt(\r\n              ~~~~~~~~~~^~~~~~~~~~~\r\nRuntimeError: The size of tensor a (5002) must match the size of tensor b (2) at non-singleton dimension 3\r\n\r\n字数多了转换报这个错误"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-08-10T01:10:07Z",
        "body": "max embedding length is 5000, so approximately 100s max synthesis length"
      }
    ]
  },
  {
    "number": 254,
    "title": "跨语种的应该怎么传参，例如粤语",
    "created_at": "2024-08-06T10:51:48Z",
    "closed_at": "2024-08-10T01:12:01Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/254",
    "body": "需要传入粤语音频作为样例吗？\r\ntext应该怎么传入",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/254/comments",
    "author": "hjj-lmx",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-10T01:12:02Z",
        "body": "start it with <|yue|>"
      }
    ]
  },
  {
    "number": 252,
    "title": "tips to improve flow training",
    "created_at": "2024-08-06T04:56:04Z",
    "closed_at": "2024-08-10T01:15:25Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/252",
    "body": "Hi, \r\n\r\nI am training flow from the scratch using higher number of mel bins, \r\nit works, but the sound quality is not as good as the pretrained model, any tips to train flow model for improved quality of sound?\r\nAlso, I found out that after a certain number of epochs, the loss explodes.\r\nThanks.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/252/comments",
    "author": "taalua",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-10T01:15:25Z",
        "body": "use more data, reduce lr if the loss explodes"
      }
    ]
  },
  {
    "number": 246,
    "title": "How to get the speaker embedding duration training of base model?",
    "created_at": "2024-08-05T02:55:45Z",
    "closed_at": "2024-08-12T01:27:37Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/246",
    "body": "Hi, I read in the paper that a x-vector encoder was pretrained to extract the speaker vector v.  \r\nthere are two parts where the vector is used:  the LLM model and the flow-matching model.  \r\nthen how did the speaker embedding generated for use in the two model training? \r\n maybe 1)  During training, I get the x-vector with the current target speech mel as input.    `v = x_vec(speech_target)`  \r\n maybe 2)  During training, I get the x-vector with a random speech sample from the same speaker of current target speech as input.   `v = x_vec(speech_x)   WHEN speaker_of(speech_x) == speaker_of(speech_target)`  \r\n maybe 3)  During preprocessing, I get the average of all speechs' x-vector embedding under the same speaker to get a  static， **fixed** speaker embeding  for **all samples of the same speaker**. and then that value will be used during training and inference to label the speaker:   `v = average( x_vec(all_speechs_of_current_speaker))`    \r\nwhich one is the truth?  ",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/246/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-10T01:20:02Z",
        "body": "during training, we use xvec from the current speech, but actually you can also use random xvec for the spk, our code tried to do it by concate all utt embedding into spk embedding, but it takes too much disk space. Random xvec during training should improve zero shot inference performance"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-08-12T01:27:37Z",
        "body": "Thanks,  We had tried TorToiseTTS with speaker condition from the current speech, but the result is terrible, while the case to use random uttrance from the same speaker get accessable result. may be our dataset is too small,  we had less then 10k hours dataset."
      }
    ]
  },
  {
    "number": 232,
    "title": "求问：不知能否控制暂停？",
    "created_at": "2024-07-31T14:15:40Z",
    "closed_at": "2024-08-02T07:18:40Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/232",
    "body": "怎么控制暂停呢？（不是 breath）\r\n以及，能否控制暂停大致时长？\r\n文档里没找到。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/232/comments",
    "author": "aetherwu",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-08-01T16:40:33Z",
        "body": "well the instruction mode is relatively limited, there is no such symbol like <pause> yet"
      }
    ]
  },
  {
    "number": 222,
    "title": "runtime/python/server.py 运行后，调用api/inference/zero-shot，报错",
    "created_at": "2024-07-29T09:09:44Z",
    "closed_at": "2024-07-29T09:45:50Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/222",
    "body": "ERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/UD/CosyVoice/runtime/python/fastapi/server.py\", line 75, in zeroShot\r\n    output = app.cosyvoice.inference_zero_shot(tts, prompt, prompt_speech_16k)\r\n  File \"/usr/local/UD/CosyVoice/runtime/python/fastapi/../../../cosyvoice/cli/cosyvoice.py\", line 59, in inference_zero_shot\r\n    model_input = self.frontend.frontend_zero_shot(i, prompt_text, prompt_speech_16k)\r\n  File \"/usr/local/UD/CosyVoice/runtime/python/fastapi/../../../cosyvoice/cli/frontend.py\", line 142, in frontend_zero_shot\r\n    speech_token, speech_token_len = self._extract_speech_token(prompt_speech_16k)\r\n  File \"/usr/local/UD/CosyVoice/runtime/python/fastapi/../../../cosyvoice/cli/frontend.py\", line 79, in _extract_speech_token\r\n    speech_token = self.speech_tokenizer_session.run(None, {self.speech_tokenizer_session.get_inputs()[0].name: feat.detach().cpu().numpy(),\r\n  File \"/home/vipuser/anaconda3/envs/cosyvoice_test/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 220, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running Add node. Name:'/Add_2' Status Message: /Add_2: right operand cannot broadcast on dim 0 LeftShape: {1,2891,1280}, RightShape: {1500,1280}",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/222/comments",
    "author": "hjj-lmx",
    "comments": [
      {
        "user": "987661913",
        "created_at": "2024-10-08T10:39:30Z",
        "body": "I also met you, how did you solve it?"
      },
      {
        "user": "zjxcc",
        "created_at": "2024-11-27T09:28:31Z",
        "body": "@hjj-lmx\r\nI also met you, how did you solve it?\r\n"
      }
    ]
  },
  {
    "number": 219,
    "title": "Why there are different configs between cosyvoice.yaml and cosyvoice.fromscratch.yaml",
    "created_at": "2024-07-29T02:41:33Z",
    "closed_at": "2024-07-30T03:17:00Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/219",
    "body": "I found in cosyvoice.yaml , the llm num_blocks is 14, while that config in cosyvoice.fromscratch.yaml Half reduced, it seems that it will get a smaller model from cosyvoice.fromscratch.yaml. Is that means the cosyvoice.fromscratch.yaml is used to train the smaller model while the cosyvoice.yaml is used to train the cosyvoide-base?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/219/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-29T23:58:14Z",
        "body": "cosyvoice.fromscratch.yaml is intended for train on libritts 960h, so it has smaller model size"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-07-30T01:31:31Z",
        "body": "> cosyvoice.fromscratch.yaml is intended for train on libritts 960h, so it has smaller model size\r\n\r\nThank you very much.  I see in the paper that Cosyvoice-tiny takes 12 layers, but in the `cosyvoice.fromscrath.yaml`, its `num_blocks` is only 7,  it is even smaller then in the paper."
      }
    ]
  },
  {
    "number": 208,
    "title": "使用spk_embeding还是utt_embedding?",
    "created_at": "2024-07-25T08:20:49Z",
    "closed_at": "2024-07-25T08:59:03Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/208",
    "body": "在微调时，提供了两种方式，spk_embedding和utt_embedding，这两种embeding对模型效果有多大影响？建议使用哪一种？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/208/comments",
    "author": "yzliu90",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-25T08:59:03Z",
        "body": "when sft, you are using the specific speaker, so use spk_embedding because you will use it in sft inference mode"
      }
    ]
  },
  {
    "number": 205,
    "title": "大标题之间使用传统方法增加停顿？",
    "created_at": "2024-07-25T06:39:31Z",
    "closed_at": "2024-07-25T09:01:22Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/205",
    "body": "一、国内要闻\r\n今天是...\r\n二、国际要闻\r\n今天...\r\n\r\n\r\n\r\n对于第二个大标题与前文之间停顿较小的问题，有没有什么方法可以解决\r\n比如使用正则化的方法，强行在第二个标题开始前增加停顿",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/205/comments",
    "author": "LayBrick",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-25T09:01:22Z",
        "body": "there is no way to fix it using model yet, you can try add 0.2s blank at each sentence start"
      }
    ]
  },
  {
    "number": 204,
    "title": "请问 extract_speech_token 这一步是否可以加速呢？",
    "created_at": "2024-07-24T09:48:10Z",
    "closed_at": "2024-07-24T09:57:10Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/204",
    "body": "想要微调，但是抽特征的速度有点慢。\r\n请问有没有建议的加速方法呢？\r\n\r\n目前好像写死了 batch=1 推断，这块儿是否建议修改呢？\r\n或者转成 tensorrt 之类的，不知道是否会有加速呢",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/204/comments",
    "author": "TinaChen95",
    "comments": [
      {
        "user": "JunityZhan",
        "created_at": "2024-07-29T17:43:57Z",
        "body": "请问你解决了吗？speech tokenizer可以调batchsize更大吗？"
      },
      {
        "user": "csu-roycheng",
        "created_at": "2024-08-22T01:43:21Z",
        "body": "> 请问你解决了吗？speech tokenizer可以调batchsize更大吗？\r\n\r\n有可能是 onnx 没用 GPU 推理的问题"
      },
      {
        "user": "r666ay",
        "created_at": "2024-08-23T10:36:34Z",
        "body": "用gpu的话, 10h能提取960h的speech tokens(以LibriTTS为例). 但是onnx相当吃cpu, 在12cpu上同时跑2个extractor speech token的脚本, cpu利用率就100%. 建议用gpu和更多的cpu来处理."
      },
      {
        "user": "pengzhendong",
        "created_at": "2024-09-10T11:26:12Z",
        "body": "> 请问你解决了吗？speech tokenizer可以调batchsize更大吗？\r\n\r\n``` python\r\nimport onnx\r\nimport os\r\nimport struct\r\n\r\nfrom argparse import ArgumentParser\r\n\r\n\r\ndef rebatch(infile, outfile, batch_size):\r\n    model = onnx.load(infile)\r\n    graph = model.graph\r\n\r\n    # Change batch size in input, output and value_info\r\n    for tensor in list(graph.input) + list(graph.value_info) + list(graph.output):\r\n        tensor.type.tensor_type.shape.dim[0].dim_param = batch_size\r\n\r\n    # Set dynamic batch size in reshapes (-1)\r\n    for node in  graph.node:\r\n        if node.op_type != 'Reshape':\r\n            continue\r\n        for init in graph.initializer:\r\n            # node.input[1] is expected to be a reshape\r\n            if init.name != node.input[1]:\r\n                continue\r\n            # Shape is stored as a list of ints\r\n            if len(init.int64_data) > 0:\r\n                # This overwrites bias nodes' reshape shape but should be fine\r\n                init.int64_data[0] = -1\r\n            # Shape is stored as bytes\r\n            elif len(init.raw_data) > 0:\r\n                shape = bytearray(init.raw_data)\r\n                struct.pack_into('q', shape, 0, -1)\r\n                init.raw_data = bytes(shape)\r\n\r\n    onnx.save(model, outfile)\r\n\r\nif __name__ == '__main__':\r\n    parser = ArgumentParser('Replace batch size with \\'N\\'')\r\n    parser.add_argument('infile')\r\n    parser.add_argument('outfile')\r\n    args = parser.parse_args()\r\n\r\n    rebatch(args.infile, args.outfile, 'N')\r\n```"
      }
    ]
  },
  {
    "number": 197,
    "title": "increment training bug",
    "created_at": "2024-07-23T09:49:36Z",
    "closed_at": "2024-07-25T09:09:19Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/197",
    "body": "**Describe the bug**\r\nWhen I start training, it produce error like this\r\n’‘’\r\nraise ImportError(\"There is no such entity as %s\" % callable_string)\r\n  ImportError: There is no such entity as matcha.utils.audio.mel_spectrogram\r\n‘’‘\r\n\r\nIn fact, the error code originates from this line.\r\n\r\n‘’‘\r\nwith open(args.config, 'r') as f:\r\n        configs = load_hyperpyyaml(f, overrides=override_dict)\r\n’‘’‘\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/197/comments",
    "author": "QianguoS",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-23T14:52:24Z",
        "body": "check faq.md, export PYTHONPATH"
      }
    ]
  },
  {
    "number": 196,
    "title": "执行cosyvoice/bin/train.py进行音色训练报错",
    "created_at": "2024-07-23T09:02:01Z",
    "closed_at": "2024-07-25T09:09:30Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/196",
    "body": "**Describe the bug**\r\n\r\n```\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\r\n [WARNING]  using untested triton version (2.0.0), only 1.0.0 is known to be compatible\r\nTraceback (most recent call last):\r\n  File \"cosyvoice/bin/train.py\", line 29, in <module>\r\n    from cosyvoice.utils.executor import Executor\r\nModuleNotFoundError: No module named 'cosyvoice'\r\n```\r\n\r\n**To Reproduce**\r\n\r\ncosyvoice/bin/train.py 我已经把参数写好了\r\n\r\n```\r\ndef get_args():\r\n    model_path = '/CosyVoice/pretrained_models/CosyVoice-300M/'\r\n    config_path = '/pretrained_models/CosyVoice-300M/configuration.json'\r\n    train_data_path = '/CosyVoice/train_input/qrcode_wuwei1/'\r\n    #cv_data_path = '/CosyVoice/'\r\n    model_dir = '/CosyVoice/save_model_dir'\r\n```\r\n\r\n运行\r\n```\r\npython cosyvoice/bin/train.py\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/196/comments",
    "author": "cpken",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-23T14:52:44Z",
        "body": "run . ./path.sh"
      }
    ]
  },
  {
    "number": 195,
    "title": "TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
    "created_at": "2024-07-23T08:02:44Z",
    "closed_at": "2024-07-23T13:10:34Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/195",
    "body": "When trying to run train:\r\n\r\n```bash\r\nOriginal Traceback (most recent call last):\r\n    File \"/home/raph/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n      data = fetcher.fetch(index)\r\n    File \"/home/raph/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 41, in fetch\r\n      data = next(self.dataset_iter)\r\n    File \"/home/raph/repos/CosyVoice/cosyvoice/dataset/processor.py\", line 322, in padding\r\n      for sample in data:\r\n    File \"/home/raph/repos/CosyVoice/cosyvoice/dataset/processor.py\", line 283, in dynamic_batch\r\n      for sample in data:\r\n    File \"/home/raph/repos/CosyVoice/cosyvoice/dataset/processor.py\", line 237, in sort\r\n      for sample in data:\r\n    File \"/home/raph/repos/CosyVoice/cosyvoice/dataset/processor.py\", line 209, in shuffle\r\n      for sample in data:\r\n    File \"/home/raph/repos/CosyVoice/cosyvoice/dataset/processor.py\", line 172, in parse_embedding\r\n      sample['spk_embedding'] = torch.tensor(sample['spk_embedding'], dtype=torch.float32)\r\nTypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\r\n```",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/195/comments",
    "author": "rlenain",
    "comments": [
      {
        "user": "inconnu1111",
        "created_at": "2024-07-29T08:40:24Z",
        "body": "你怎么解决的啊"
      },
      {
        "user": "Dinxin",
        "created_at": "2024-10-16T08:30:51Z",
        "body": "大佬是如何解决的呢？\r\n"
      }
    ]
  },
  {
    "number": 194,
    "title": "Extract Continuous Features Before VQ",
    "created_at": "2024-07-23T07:54:07Z",
    "closed_at": "2024-07-25T09:09:38Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/194",
    "body": "We would like to request a new feature to extract and access the continuous features before the Vector Quantization (VQ) layer.\r\n\r\nCurrent needs:\r\n1. Access to continuous feature representations before VQ\r\n2. A simple method to extract these features\r\n3. Option to save these features for later use\r\n\r\nThis feature would be helpful for:\r\n- Analyzing intermediate model representations\r\n- Conducting more granular feature experiments\r\n- Comparing features before and after VQ\r\n\r\nPlease let us know if this feature is feasible and if you need any additional information.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/194/comments",
    "author": "liutaocode",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-23T14:54:15Z",
        "body": "you mean provide a onnx file which generates continuous feathre? I am sorry but this is not part of the opensource plan yet"
      }
    ]
  },
  {
    "number": 190,
    "title": "TypeError: padding() missing 1 required positional argument: 'use_spk_embedding'",
    "created_at": "2024-07-22T15:07:30Z",
    "closed_at": "2024-07-23T08:02:03Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/190",
    "body": "**Describe the bug**\r\nWhen running `bash run.sh` in the training recipe (under `examples/libritts/cosyvoice`), I get the following error: \r\n\r\n``` \r\nTypeError: padding() missing 1 required positional argument: 'use_spk_embedding'\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/190/comments",
    "author": "rlenain",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-23T01:45:27Z",
        "body": "use latest yaml, it has use_spk_embedding in cosyvoice.yaml"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-09-05T02:30:32Z",
        "body": "> use latest yaml, it has use_spk_embedding in cosyvoice.yaml\r\n\r\nin the latest config, use_spk_embedding is false, is that means when training flow model from scratch,  we shell use the spk_embedding of current utterance instead of  a random one from the utter speaker?"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-09-05T02:32:51Z",
        "body": "no, set use_spk_embedding to false when train base model. set to true if you want to train a sft model for a specific speaker"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-09-05T02:34:45Z",
        "body": "> no, set use_spk_embedding to false when train base model. set to true if you want to train a sft model for a specific speaker\r\n\r\nYes, I means the base model of flow-matching.  which need the spk_embedding as condition."
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-09-05T02:35:49Z",
        "body": "there are too much samples, If I can use a random spk_embedding from the same speaker, the computation of preprocessing will save a lot."
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-09-05T02:38:25Z",
        "body": "> > no, set use_spk_embedding to false when train base model. set to true if you want to train a sft model for a specific speaker\r\n> \r\n> Yes, I means the base model of flow-matching. which need the spk_embedding as condition.\r\n\r\nno, when train base model, use utt_embedding which is extracted from current utterance. only when sft training, use spk_embedding"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-09-05T02:43:39Z",
        "body": "> > > no, set use_spk_embedding to false when train base model. set to true if you want to train a sft model for a specific speaker\r\n> > \r\n> > \r\n> > Yes, I means the base model of flow-matching. which need the spk_embedding as condition.\r\n> \r\n> no, when train base model, use utt_embedding which is extracted from current utterance. only when sft training, use spk_embedding\r\n\r\nget it, thanks a lot !"
      }
    ]
  },
  {
    "number": 187,
    "title": "Instruct合成和跨语种合成的一些疑问",
    "created_at": "2024-07-22T10:52:04Z",
    "closed_at": "2024-07-25T09:13:25Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/187",
    "body": "首先感谢开源你们的工作，我有以下几个疑问：\r\n1、观察到在跨语种合成时，在text前加入语种标签，如<|zh|>，可以通过zero-shot的方式，进行跨语种合成，其实这就是sft的合成方式吧？\r\n2、instruct合成时，llm的输入不再使用embedding，是有什么特别考虑吗？如果用了xvector，能够实现风格迁移？\r\n3、微调之后，观察到zero_shot合成音比sft合成音，音量小很多，这是为什么？\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/187/comments",
    "author": "yzliu90",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-23T15:01:47Z",
        "body": "1. yes\r\n2. embedding will influence instruct in our experiment\r\n3. during sft, use spk embedding for the sft speaker. I have no clue for the volume issue either"
      },
      {
        "user": "yzliu90",
        "created_at": "2024-07-24T03:03:48Z",
        "body": "感谢回复！有几个问题请在确认一下。\r\n1、也就是说，在instruct时，如果使用embeding，会影响instruct的指导意义，其实是embeding同时携带了情感或风格信息，影响了风格迁移？\r\n2、如果embeding接偶完美，仅保留说话人信息，理论上可以通过类似于跨语种<|language|>类似的标签实现风格的迁移？\r\n3、在微调时，提供了两种方式，spk_embedding和utt_embedding，这两种embeding对模型效果有多大影响？建议使用哪一种？\r\n感谢回复！"
      }
    ]
  },
  {
    "number": 171,
    "title": "HTTPS无法播放生成的音频",
    "created_at": "2024-07-18T07:44:35Z",
    "closed_at": "2024-07-19T09:32:43Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/171",
    "body": "如果服务通过nginx解析成https的话，点击生成视频的时候，会导致音频文件无法显示，无法播放，请问如何解决https的问题？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/171/comments",
    "author": "linxianzhong0128",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-19T01:42:02Z",
        "body": "well this is related with nginx proxy, we have no experience in this"
      }
    ]
  },
  {
    "number": 170,
    "title": "Some Question about ffn timecost",
    "created_at": "2024-07-18T07:21:41Z",
    "closed_at": "2024-10-11T03:39:58Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/170",
    "body": "**Describe the bug**\r\n----\r\nI found that in the TransformerEncoder, the time consumption of the attention (attn) part is similar to that of the feed-forward network (ffn) part. However, in autoregressive models, the attn part needs to involve all time steps of the key and value in the calculations, whereas the ffn part only needs to focus on the current time step. Therefore, the time consumption of this part is relatively unusual; the time consumption of the ffn part should be significantly different from that of the attn part.\r\n我发现 TransformerEncoder 中 attn 部分耗时跟 ffn 部分耗时接近，但是在自回归中，因为 attn 部分需要将 kv 全部时间序参与计算，但是 ffn 部分只需要关注当前时间序，因此这个部分的耗时比较异常，ffn 的耗时应当和 attn 部分存在显著差异。\r\n\r\n**To Reproduce**\r\nAdd time logging and execute torch.cuda.synchronize() to ensure CUDA synchronization.\r\n添加时间日志，并且执行 torch.cuda.synchronize() 确保cuda同步\r\n```python\r\nclass TransformerEncoderLayer(nn.Module):\r\n    \"\"\"Encoder layer module.\r\n    Args:\r\n        size (int): Input dimension.\r\n        self_attn (torch.nn.Module): Self-attention module instance.\r\n            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention`\r\n            instance can be used as the argument.\r\n        feed_forward (torch.nn.Module): Feed-forward module instance.\r\n            `PositionwiseFeedForward`, instance can be used as the argument.\r\n        dropout_rate (float): Dropout rate.\r\n        normalize_before (bool):\r\n            True: use layer_norm before each sub-block.\r\n            False: to use layer_norm after each sub-block.\r\n    \"\"\"\r\n    def __init__(\r\n        self,\r\n        size: int,\r\n        self_attn: torch.nn.Module,\r\n        feed_forward: torch.nn.Module,\r\n        dropout_rate: float,\r\n        normalize_before: bool = True,\r\n    ):\r\n        \"\"\"Construct an EncoderLayer object.\"\"\"\r\n        super().__init__()\r\n        self.self_attn = self_attn\r\n        self.feed_forward = feed_forward\r\n        self.norm1 = nn.LayerNorm(size, eps=1e-5)\r\n        self.norm2 = nn.LayerNorm(size, eps=1e-5)\r\n        self.dropout = nn.Dropout(dropout_rate)\r\n        self.size = size\r\n        self.normalize_before = normalize_before\r\n\r\n    def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        mask: torch.Tensor,\r\n        pos_emb: torch.Tensor,\r\n        mask_pad: torch.Tensor = torch.ones((0, 0, 0), dtype=torch.bool),\r\n        att_cache: torch.Tensor = torch.zeros((0, 0, 0, 0)),\r\n        cnn_cache: torch.Tensor = torch.zeros((0, 0, 0, 0)),\r\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\r\n        \"\"\"Compute encoded features.\r\n\r\n        Args:\r\n            x (torch.Tensor): (#batch, time, size)\r\n            mask (torch.Tensor): Mask tensor for the input (#batch, time，time),\r\n                (0, 0, 0) means fake mask.\r\n            pos_emb (torch.Tensor): just for interface compatibility\r\n                to ConformerEncoderLayer\r\n            mask_pad (torch.Tensor): does not used in transformer layer,\r\n                just for unified api with conformer.\r\n            att_cache (torch.Tensor): Cache tensor of the KEY & VALUE\r\n                (#batch=1, head, cache_t1, d_k * 2), head * d_k == size.\r\n            cnn_cache (torch.Tensor): Convolution cache in conformer layer\r\n                (#batch=1, size, cache_t2), not used here, it's for interface\r\n                compatibility to ConformerEncoderLayer.\r\n        Returns:\r\n            torch.Tensor: Output tensor (#batch, time, size).\r\n            torch.Tensor: Mask tensor (#batch, time, time).\r\n            torch.Tensor: att_cache tensor,\r\n                (#batch=1, head, cache_t1 + time, d_k * 2).\r\n            torch.Tensor: cnn_cahce tensor (#batch=1, size, cache_t2).\r\n        \"\"\"\r\n        torch.cuda.synchronize() # cuda tongbu\r\n        time1 = time.perf_counter()\r\n        residual = x\r\n        if self.normalize_before:\r\n            x = self.norm1(x)\r\n        x_att, new_att_cache = self.self_attn(x, x, x, mask, pos_emb=pos_emb, cache=att_cache)\r\n\r\n        x = residual + self.dropout(x_att)\r\n        if not self.normalize_before:\r\n            x = self.norm1(x)\r\n\r\n        residual = x\r\n        if self.normalize_before:\r\n            x = self.norm2(x)\r\n        torch.cuda.synchronize()\r\n        time2 = time.perf_counter()\r\n        x = residual + self.dropout(self.feed_forward(x))\r\n        if not self.normalize_before:\r\n            x = self.norm2(x)\r\n\r\n        torch.cuda.synchronize()\r\n        time3 = time.perf_counter() #benchmark\r\n\r\n        print(x.shape, att_cache.shape)\r\n        print(\"time cost: \", time2-time1, time3-time2)\r\n\r\n        fake_cnn_cache = torch.zeros((0, 0, 0), dtype=x.dtype, device=x.device)\r\n        return x, mask, new_att_cache, fake_cnn_cache\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/170/comments",
    "author": "hexisyztem",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-19T01:41:33Z",
        "body": "during inference, there is att_cache, I believe the ffn layer will not recalculate the history part, you can check the detail in self.self_attn(x, x, x, mask, pos_emb=pos_emb, cache=att_cache)"
      }
    ]
  },
  {
    "number": 161,
    "title": "ttsfrd 是不是可以提供py3.9等版本的wheel",
    "created_at": "2024-07-17T08:56:03Z",
    "closed_at": "2024-07-18T03:44:44Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/161",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/161/comments",
    "author": "marching-on",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-17T09:09:55Z",
        "body": "no, there is no plan to update ttsfrd, dont wait for it"
      }
    ]
  },
  {
    "number": 155,
    "title": "I use 3070 8gb , the inference time suddenly increase when text is longer, not linear",
    "created_at": "2024-07-16T15:32:10Z",
    "closed_at": "2024-07-18T03:46:06Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/155",
    "body": "15字 - 4s, 30字-8秒, 50字-40s\r\n50字 suddenly increase to 40s.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/155/comments",
    "author": "linyu0219",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-17T04:55:08Z",
        "body": "do not use long sentence, first in our training, we do not have so many long sentence, second it will increase training time as att_cache is longer"
      }
    ]
  },
  {
    "number": 151,
    "title": "inference_sft 与 inference_zero_shot",
    "created_at": "2024-07-16T08:38:18Z",
    "closed_at": "2024-07-18T03:47:20Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/151",
    "body": "请问如何理解这两种推理方式的差异呢？\r\n\r\n个人理解，zeroshot采用续写模式，而sft则是直接从0开始生成。\r\n\r\n尝试了一下微调模型，并且微调后的模型在zeroshot模式下效果优于zeroshot，这是否符合预期呢？\r\n理论上，zeroshot模式提供更多上下文，是否总是这种推断模式效果会更好？\r\n\r\n什么情况下，应当使用zeroshot，什么情况应当使用sft模式呢？\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/151/comments",
    "author": "TinaChen95",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-17T04:58:42Z",
        "body": "zero shot mode also use the embedding from the prompt audio, but sft mode use global speaker embedding from many utterance"
      }
    ]
  },
  {
    "number": 143,
    "title": "where is the BPE tokenizer model used in CosyVoice",
    "created_at": "2024-07-15T12:36:17Z",
    "closed_at": "2024-07-17T03:48:09Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/143",
    "body": "I read from paper that CosyVocie use BEP to tokenize text inputs. it has a 51886 dict size, but I did not find where to get the BPE model, where can I get it ?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/143/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-16T01:48:40Z",
        "body": "see cosyvoice.yaml for tokenizer"
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-07-16T03:47:10Z",
        "body": "> see cosyvoice.yaml for tokenizer\r\n\r\nYes, I  had seen in cosyvoice.yaml that it use whisper installed tokenizer, but the whisper multilingual tokenzier contains only 50256 tokens in its .tiktoken file, while in CosyVoice yaml config,  the \"text_token_size\" is set as 51866,  there are more tokens in cosyvoice,  So I had thought that you have your self-version BPE."
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-07-17T03:48:09Z",
        "body": "Get it, the extra tokens are from whisper."
      }
    ]
  },
  {
    "number": 142,
    "title": "训练flow模型",
    "created_at": "2024-07-15T12:13:23Z",
    "closed_at": "2024-07-18T03:48:16Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/142",
    "body": "请问从头训练flow模型，需要多少数据量和step。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/142/comments",
    "author": "WangGewu",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-16T01:48:14Z",
        "body": "our flow model is trained using the same data with llm, according to report it is 170k hour. you can try train from scratch on libritts, but if you want a well-trained flow model, we suggest the more data the better"
      }
    ]
  },
  {
    "number": 131,
    "title": "是否可以将一些配置暴露出来？",
    "created_at": "2024-07-14T09:02:12Z",
    "closed_at": "2024-07-15T10:14:38Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/131",
    "body": "我在本地部署了我们的Cosy Voice，体验下来非常棒，我已经有一些使用它的小创意了。再次感谢项目组的辛苦付出。\r\n\r\n我的环境：\r\n我使用的一个我的一个旧电脑，CPU i7 8核心，一个AMD显卡 2G显存，内存16G，操作系统为Centos7。\r\n\r\n1、关于CPU资源使用：我发现在使用的过程中，并没有把我的CPU资源给占满，而且我也没有发现配置进程数量的地方（可能是我没找到），是否可以默认将输入的文本按照CPU的核心数然后切分，然后进行推理（当时可能要保持最小token数量）。我不确定这样是否可以加速生成。\r\n2、关于GPU资源：我本地默认有一个AMD的显卡，但是我发现在使用过程中，默认并没有使用GPU，不确定是否我操作的不对。不知道这个是否可以配置\r\n3、关于启动webui.py的demo 页面：我按照README.md的说明操作之后（而且README.md上缺少对硬件资源的需求，导致我第一次部署的时候，低估了需要的资源，这个是否也可以补充？？），发现启动webui.py 会缺少很多依赖，是否可以打一个全量的requirement.txt\r\n4、关于模型文件：我们的模型文件都有点大，不知道后面是否进行压缩，通过git clone下拉压缩文件然后本地自己解压即可\r\n\r\n再次感谢项目组所有同学的付出，和开源精神。\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/131/comments",
    "author": "zhanbei1",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-15T10:08:44Z",
        "body": "we only provide the basic inference implementation. \r\n1. you need to use multiprocess in order to use multiple cpu, or python will continue to use only 1 gpu due to gil, unless your core library like numpy it already have multiple gpu support\r\n2. maybe your gpu is not supported, cuda toolkit is only supported on Nvidia\r\n3. the requirements.txt contains the full package needed to run the repo. we may add some hardware resource requirements later\r\n4. I don't think zip will reduce the file size significantly because it is already pt format. We want the resource listed on repo for a clearer view"
      },
      {
        "user": "zhanbei1",
        "created_at": "2024-07-15T10:14:38Z",
        "body": "> we only provide the basic inference implementation.\r\n> \r\n> 1. you need to use multiprocess in order to use multiple cpu, or python will continue to use only 1 gpu due to gil, unless your core library like numpy it already have multiple gpu support\r\n> 2. maybe your gpu is not supported, cuda toolkit is only supported on Nvidia\r\n> 3. the requirements.txt contains the full package needed to run the repo. we may add some hardware resource requirements later\r\n> 4. I don't think zip will reduce the file size significantly because it is already pt format. We want the resource listed on repo for a clearer view\r\n\r\nOK ，Let me learn more about this project. The issue is closed"
      }
    ]
  },
  {
    "number": 130,
    "title": "Streaming support?",
    "created_at": "2024-07-14T01:36:16Z",
    "closed_at": "2024-07-15T15:35:56Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/130",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThe cosyvoice models are amazing but it would be great to have streaming support for real time generation.\r\n\r\n**Describe the solution you'd like**\r\nStreaming support for Cosyvoice models\r\n\r\n**Describe alternatives you've considered**\r\nI don’t believe there is another alternative for real time generation with Cosyvoice models. \r\n\r\n**Additional context**\r\nI don’t have any other context\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/130/comments",
    "author": "johnwick123f",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-15T10:02:12Z",
        "body": "not yet, we are working on fixing bug yet, also the released flow matching model is trained in non-causal way, we may do streaming support later"
      },
      {
        "user": "johnwick123f",
        "created_at": "2024-07-15T15:35:56Z",
        "body": "Alright thanks for answering, great project btw!"
      },
      {
        "user": "lucasjinreal",
        "created_at": "2024-08-30T14:27:38Z",
        "body": "Is the streaming can be supported now?"
      }
    ]
  },
  {
    "number": 129,
    "title": "Fix run.sh train",
    "created_at": "2024-07-13T22:23:52Z",
    "closed_at": "2024-07-15T02:06:32Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/129",
    "body": "run.sh when train llm, not generate data/train.data.list and data/dev.data.list successfully (My env is wsl2+ubuntu22.04).",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/129/comments",
    "author": "cacard",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-15T02:05:20Z",
        "body": "well, in this case, user should check why some data.list is not generated successfully, instead of hiding this problem. Sorry, but this PR is not suitable for merge. You can check why some data.list is not generated, whether it is reproducible."
      }
    ]
  },
  {
    "number": 126,
    "title": "使用 SFT 时，声音忽大忽小",
    "created_at": "2024-07-13T11:25:51Z",
    "closed_at": "2024-07-14T02:14:54Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/126",
    "body": "使用 sft 使用加载音频，声音忽大忽小，\r\n使用零文本模型，使用加载音频，结尾句后总有 叮 的声音",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/126/comments",
    "author": "AssassinQuin",
    "comments": [
      {
        "user": "traddo",
        "created_at": "2024-07-13T12:44:42Z",
        "body": "你可以用这段代码达到均衡音量的目的：\r\n\r\n```\r\n\r\nimport pyloudnorm as pyln\r\nimport soundfile as sf\r\n\r\ndef _norm_loudness(wav_path):\r\n    data, rate = sf.read(wav_path)\r\n    meter = pyln.Meter(rate) # create BS.1770 meter\r\n    loudness = meter.integrated_loudness(data) # measure loudness\r\n    normalized_audio = pyln.normalize.loudness(data, loudness, -12.0)\r\n    sf.write(wav_path, normalized_audio, 22050)\r\n    return wav_path\r\n\r\n```"
      },
      {
        "user": "AssassinQuin",
        "created_at": "2024-07-14T02:14:52Z",
        "body": "好的，已解决，谢谢"
      },
      {
        "user": "xipingL",
        "created_at": "2024-09-13T01:55:23Z",
        "body": "我也遇到了这个问题 中文男  有这个问题 使用上面的方法也没解决，请问有什么好的方法吗？"
      }
    ]
  },
  {
    "number": 119,
    "title": "非常棒的项目，想问一下都支持什么情绪和语气呀？",
    "created_at": "2024-07-12T09:34:00Z",
    "closed_at": "2024-09-20T12:29:18Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/119",
    "body": "能否列举一下支持的情绪和语气，比如我使用sad的时候出现的不是悲伤的语气，而是读出sad。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/119/comments",
    "author": "daoliangai",
    "comments": [
      {
        "user": "LeesimEverglow",
        "created_at": "2024-07-12T09:53:28Z",
        "body": "需要使用自然语言控制模式，在instruct输入框里输入emotion"
      },
      {
        "user": "LeesimEverglow",
        "created_at": "2024-07-12T09:58:32Z",
        "body": "如果使用接口模式，依赖模型库CosyVoice-300M-Instruct \r\n\r\n需要使用\r\ndef inference_instruct(self, tts_text, spk_id, instruct_text)\r\n\r\n在instruct_text 传入情绪 "
      }
    ]
  },
  {
    "number": 117,
    "title": "bug反馈，1.语言标签不生效 2.输出会截掉句号后面的内容",
    "created_at": "2024-07-12T08:25:29Z",
    "closed_at": "2024-07-15T11:13:35Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/117",
    "body": "暂时遇到两个问题：\r\n1.zero-shot的时候，当promt音频是口音比较重的普通话时，输出直接就是粤语了。开头指定了<|zh|>也没用。我的promt语音就是周立齐语音包\r\n2.输入文字只TTS生成到第一个句号之前的。后面不继续生成了。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/117/comments",
    "author": "Simon-chai",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-12T09:08:03Z",
        "body": "1. this is not bug, the model have never seen such case when prompt audio is corner case\r\n2. you can split it into short sentence manually. systhesis speech separately if possible."
      },
      {
        "user": "Simon-chai",
        "created_at": "2024-07-15T11:05:58Z",
        "body": "> 1. this is not bug, the model have never seen such case when prompt audio is corner case\r\n\r\nBut the model actually know something. The accent of 周立齐 sound like Cantonese indeed. But to my opinion,promt should play a role of tips,should not be so determinative,at least should not more determinative than input text. "
      },
      {
        "user": "Simon-chai",
        "created_at": "2024-07-15T11:13:35Z",
        "body": "> 2\\. you can split it into short sentence manually. systhesis speech separately if possible.\r\nAs long as I make my input only contain comma,everything is fine.But I think CosyVoice can do better. Maybe one day you can controll pause duration or emotion by punctuation,so this will be a problem in that day. Just a little advice. BTW,the CosyVoice surprive me in it's sound likeness!"
      }
    ]
  },
  {
    "number": 116,
    "title": "inference_zero_shot不支持英文吗",
    "created_at": "2024-07-12T08:11:08Z",
    "closed_at": "2024-07-17T01:43:01Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/116",
    "body": "rt,调用cosyvoice.inference_zero_shot接口，文本为英文时无法生成吗？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/116/comments",
    "author": "kli017",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-12T09:06:30Z",
        "body": "it can generate english, see readme"
      }
    ]
  },
  {
    "number": 109,
    "title": "会上传flow训练部分代码吗？",
    "created_at": "2024-07-11T11:11:33Z",
    "closed_at": "2024-07-11T11:26:32Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/109",
    "body": "仓库里没有看到flow训练的代码，现有的flow部分训练相关代码错误很多，一些基本的训练部分都是错误的，请问有计划上传训练flow部分的代码吗",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/109/comments",
    "author": "hongwen-sun",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T11:26:32Z",
        "body": "its already in the repo, just change llm to flow in run.sh stage 5, it is already verified"
      },
      {
        "user": "hongwen-sun",
        "created_at": "2024-07-11T12:06:36Z",
        "body": "#79 那这个地方后续还会更新吗"
      }
    ]
  },
  {
    "number": 108,
    "title": "Is the FA tool used to refine ASR result opensourced?",
    "created_at": "2024-07-11T10:41:31Z",
    "closed_at": "2024-07-11T10:47:32Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/108",
    "body": "Hi, I read in the paper that when collecting the huge training data,  you are using a FA tool to refine speech ASR results, to drop bad samples. is this tool available?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/108/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T10:47:32Z",
        "body": "no, it is not part of the opensource"
      }
    ]
  },
  {
    "number": 107,
    "title": "请问训练部分的代码计划上传吗？",
    "created_at": "2024-07-11T10:22:15Z",
    "closed_at": "2024-07-11T10:47:55Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/107",
    "body": "仓库里没有看到训练部分的代码，现有的部分训练相关代码错误很多，一些基本的训练部分都是错误的，请问有计划上传训练部分的代码吗",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/107/comments",
    "author": "hongwen-sun",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T10:47:55Z",
        "body": "see example/run.sh, it is already verified"
      }
    ]
  },
  {
    "number": 106,
    "title": "接口服务允许跨域请求，接入其他服务，比如ollama或者其他大模型服务",
    "created_at": "2024-07-11T10:12:01Z",
    "closed_at": "2024-07-12T02:03:45Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/106",
    "body": "接口服务允许跨域请求，接入其他服务，比如ollama或者其他大模型服务\r\n\r\n跨域资源共享 (CORS) 是一种机制，它允许在不同域中的客户端 Web 应用程序与其他域中的资源进行交互。  这在现代 Web 开发中非常有用，因为复杂的应用程序经常引用其客户端代码中的第三方 API 和资源。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/106/comments",
    "author": "v3ucn",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T13:40:14Z",
        "body": "add me 铝箱 in Dingding group to discuss details"
      }
    ]
  },
  {
    "number": 104,
    "title": "What is the benefit to introduce the masked mel condition into the flow modeling?",
    "created_at": "2024-07-11T09:38:46Z",
    "closed_at": "2024-07-11T13:36:26Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/104",
    "body": "Hi,  I had read in the paper that there are four condition signals to train the flow module.  the speaker embddding, the S3 tokens,  the masked mel and the intermidieate state on the density path.  I do not understand what is the masked mel condition used to?  the flow model is not autoregressively to generate mels one by one, Why there need a masked mel condition as training input?  and what is the masked mel from during inference?  Thanks.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/104/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T10:49:46Z",
        "body": "mel is used during zero shot inference, prompt mel will be used as  mel condition"
      }
    ]
  },
  {
    "number": 102,
    "title": "How sft on a new voice with English?",
    "created_at": "2024-07-11T09:12:56Z",
    "closed_at": "2024-07-11T13:36:40Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/102",
    "body": "Hi, I want finetune the model with a English speaker's voice, and want make it work in other launguage as well.\r\n\r\nI tried zero shot, it's good, but cross langual is not good, is there any suggestions on how to comes up a new voice with my data? (so that I don't need prompt, and this model can produce extremly same voice like that spkear, for all launguages)",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/102/comments",
    "author": "MonolithFoundation",
    "comments": [
      {
        "user": "MonolithFoundation",
        "created_at": "2024-07-11T09:13:29Z",
        "body": "BTW, my voice just a bout 80 samples ( each of them are about 10-30seconds)."
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T10:50:48Z",
        "body": "prepare at least 1h for each speaker"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-07-11T11:18:15Z",
        "body": "oh didn't have that much data\n\n\n\n---- Replied Message ----\n| From | Xiang ***@***.***> |\n| Date | 07/11/2024 18:51 |\n| To | ***@***.***> |\n| Cc | ***@***.***>***@***.***> |\n| Subject | Re: [FunAudioLLM/CosyVoice] How sft on a new voice with English? (Issue #102) |\n\nprepare at least 1h for each speaker\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>"
      }
    ]
  },
  {
    "number": 99,
    "title": "Is the LID seen during the training of base model?",
    "created_at": "2024-07-11T07:32:05Z",
    "closed_at": "2024-07-11T10:51:58Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/99",
    "body": "Hi, I see in the paper, as to zero-shot in-context learning part.  a LID (language identity) is needed for cross lingual  generation. is the LID token seen during the training process of the LLM base model? what is the position  in training input sequence?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/99/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T10:51:58Z",
        "body": "lid is at sentence start during training"
      }
    ]
  },
  {
    "number": 98,
    "title": "现在ttsfrd的最新版本只支持python3.8吗？",
    "created_at": "2024-07-11T06:51:48Z",
    "closed_at": "2024-07-11T07:08:28Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/98",
    "body": "我使用的是python3.10，但是最多只能安装到0.2.1的版本的ttsfrd，想问一下最新的ttsfrd是仅支持python3.8吗？还是在哪里可以下载？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/98/comments",
    "author": "hwhyyds",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T07:08:28Z",
        "body": "yes, you can ignore ttsfrd and use wetextprocessing instead"
      }
    ]
  },
  {
    "number": 97,
    "title": "supervised fine-tuning Flow and HIFT",
    "created_at": "2024-07-11T06:38:47Z",
    "closed_at": "2024-07-11T13:36:55Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/97",
    "body": "It seems that supervised fine-tuning (SFT) for flow or HIFT modules uses ground-truth data. Specifically, speech tokens are derived from a speech tokenizer rather than from an SFTed LLM for flow SFT, and mel features are derived from real waveforms rather than from an SFTed flow for HIFT SFT. I wonder if only LLM SFT is necessary, or if flow/HIFT SFT with ground-truth data provides satisfactory results?\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/97/comments",
    "author": "zhangyike",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T07:09:09Z",
        "body": "we also support flow sft, but not hift sft, just change llm to flow in run.sh stage 5"
      },
      {
        "user": "zhangyike",
        "created_at": "2024-07-11T08:05:13Z",
        "body": "> we also support flow sft, but not hift sft, just change llm to flow in run.sh stage 5\r\n\r\nMy question is inputs for flow sft are speech tokens generated from speech tokenizer rather than from the SFTed llm outputs. \r\nFor example, given a training sample consisting of a wave and the corresponding text, speech_tokens can be obtained from the following ways:\r\nspeech_tokens1 = speech_tokenizer(wave)\r\nspeech_tokens2 = SFTed_LLM(text)\r\nThe released demo uses speech_tokens1 for flow SFT, but I think  using speech_tokens2 may be better."
      },
      {
        "user": "zhangyike",
        "created_at": "2024-07-11T08:11:55Z",
        "body": "The config in cosyvoice.yaml is proper to SFT?  max_frames_in_batch seems too small and max_epoch seem too large. Are these recommanded?  \r\nbatch: !name:cosyvoice.dataset.processor.batch\r\n    max_frames_in_batch: 2000\r\ntrain_conf:\r\n    max_epoch: 200\r\n`"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T11:29:44Z",
        "body": "> The config in cosyvoice.yaml is proper to SFT? max_frames_in_batch seems too small and max_epoch seem too large. Are these recommanded? batch: !name:cosyvoice.dataset.processor.batch max_frames_in_batch: 2000 train_conf: max_epoch: 200 `\r\n\r\ncheck cosyvoice.yaml change some hyperparameter according to annotation. you can increase max_frames if your cuda memory is enough"
      }
    ]
  },
  {
    "number": 96,
    "title": "Instruct模型音色不固定",
    "created_at": "2024-07-11T03:59:46Z",
    "closed_at": "2024-07-11T06:21:03Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/96",
    "body": "使用Instruct模型使用同一个Instruct文本对不同文本进行推理时，有时会有音色发生明显变化的情况，请问这个问题有解决办法吗？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/96/comments",
    "author": "fengpings",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T06:21:00Z",
        "body": "no, this is the drawback of instruct model"
      },
      {
        "user": "yuyun2000",
        "created_at": "2024-07-12T10:03:52Z",
        "body": "我觉得你可以尝试用指令模型生成想要的声音后，然后用他的克隆模型持续生成这个声音？"
      }
    ]
  },
  {
    "number": 94,
    "title": "如何训练定制的spk2info.pt？",
    "created_at": "2024-07-11T03:05:13Z",
    "closed_at": "2024-07-11T11:06:41Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/94",
    "body": "如题\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/94/comments",
    "author": "traddo",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T06:19:18Z",
        "body": "train your own sft model, save spk embedding in spk2info.pt"
      },
      {
        "user": "traddo",
        "created_at": "2024-07-11T07:37:22Z",
        "body": "> train your own sft model, save spk embedding in spk2info.pt\r\n\r\n直接用原厂模型是否可以提取speaker embedding?\r\n"
      },
      {
        "user": "zhangyike",
        "created_at": "2024-07-11T10:52:24Z",
        "body": "> train your own sft model, save spk embedding in spk2info.pt\r\n\r\nhow to get a single spk embedding from spk2embedding.pt when the spk has many utts?"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-11T11:06:41Z",
        "body": "> > train your own sft model, save spk embedding in spk2info.pt\r\n> \r\n> how to get a single spk embedding from spk2embedding.pt when the spk has many utts?\r\n\r\nsimple mean, check tools/prepare_embedding.py for detail"
      },
      {
        "user": "zhangyike",
        "created_at": "2024-07-11T11:22:09Z",
        "body": "> > > train your own sft model, save spk embedding in spk2info.pt\r\n> > \r\n> > \r\n> > how to get a single spk embedding from spk2embedding.pt when the spk has many utts?\r\n> \r\n> simple mean, check tools/prepare_embedding.py for detail\r\n\r\nI cannot find  tools/prepare_embedding.py. I guess the \"spk2embedding.pt\" should regenerated as a single vector for each spk by average embeddings of the same spk by modifying \"tool/extract_embeddings.py\""
      },
      {
        "user": "traddo",
        "created_at": "2024-07-11T11:26:34Z",
        "body": "> > > > train your own sft model, save spk embedding in spk2info.pt\r\n> > > \r\n> > > \r\n> > > how to get a single spk embedding from spk2embedding.pt when the spk has many utts?\r\n> > \r\n> > \r\n> > simple mean, check tools/prepare_embedding.py for detail\r\n> \r\n> I cannot find tools/prepare_embedding.py. I guess the \"spk2embedding.pt\" should regenerated as a single vector for each spk by average embeddings of the same spk by modifying \"tool/extract_embeddings.py\"\r\n\r\nlook examples/libritts/cosyvoice/run.sh, inference.py"
      }
    ]
  },
  {
    "number": 90,
    "title": "WeTextProcessing PyPI installation error on macOS",
    "created_at": "2024-07-10T12:39:07Z",
    "closed_at": "2024-07-10T14:38:31Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/90",
    "body": "**Describe the bug**\r\n```\r\nDownloading WeTextProcessing-1.0.3-py3-none-any.whl (2.0 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 2.8 MB/s eta 0:00:00\r\nBuilding wheels for collected packages: pynini\r\n  Building wheel for pynini (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × python setup.py bdist_wheel did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [55 lines of output]\r\n..........\r\n      extensions/_pywrapfst.cpp:1291:10: fatal error: 'fst/util.h' file not found\r\n      #include <fst/util.h>\r\n               ^~~~~~~~~~~~\r\n      1 error generated.\r\n      error: command '/usr/bin/gcc' failed with exit code 1\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for pynini\r\n  Running setup.py clean for pynini\r\nFailed to build pynini\r\nERROR: Could not build wheels for pynini, which is required to install pyproject.toml-based projects\r\n```\r\n\r\n**Desktop (please complete the following information):**\r\n - OS:  macOS Sonoma 14.5\r\n\r\n**Solution**\r\nUse `conda install -c conda-forge pynini==2.1.5` to install pynini first, then pip install WeTextProcessing==1.0.3",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/90/comments",
    "author": "XXXXRT666",
    "comments": [
      {
        "user": "conkeur",
        "created_at": "2024-08-13T15:42:40Z",
        "body": "> **Describe the bug**\r\n> \r\n> ```\r\n> Downloading WeTextProcessing-1.0.3-py3-none-any.whl (2.0 MB)\r\n>    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 2.8 MB/s eta 0:00:00\r\n> Building wheels for collected packages: pynini\r\n>   Building wheel for pynini (setup.py) ... error\r\n>   error: subprocess-exited-with-error\r\n>   \r\n>   × python setup.py bdist_wheel did not run successfully.\r\n>   │ exit code: 1\r\n>   ╰─> [55 lines of output]\r\n> ..........\r\n>       extensions/_pywrapfst.cpp:1291:10: fatal error: 'fst/util.h' file not found\r\n>       #include <fst/util.h>\r\n>                ^~~~~~~~~~~~\r\n>       1 error generated.\r\n>       error: command '/usr/bin/gcc' failed with exit code 1\r\n>       [end of output]\r\n>   \r\n>   note: This error originates from a subprocess, and is likely not a problem with pip.\r\n>   ERROR: Failed building wheel for pynini\r\n>   Running setup.py clean for pynini\r\n> Failed to build pynini\r\n> ERROR: Could not build wheels for pynini, which is required to install pyproject.toml-based projects\r\n> ```\r\n> \r\n> **Desktop (please complete the following information):**\r\n> \r\n> * OS:  macOS Sonoma 14.5\r\n> \r\n> **Solution** Use `conda install -c conda-forge pynini==2.1.5` to install pynini first, then pip install WeTextProcessing==1.0.3\r\n\r\ndid not work for me \r\nconda list shows pynini installed ,but pip list doesn't.\r\nand pip install pynini shows\r\n  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD \"-IC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\include\" \"-IC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.7.2\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\" /EHsc /Tpextensions/_pywrapfst.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/_pywrapfst.obj -std=c++17 -Wno-register -Wno-deprecated-declarations -Wno-unused-function -Wno-unused-local-typedefs -funsigned-char\r\n      cl: 命令行 error D8021 :无效的数值参数“/Wno-register”\r\n      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for pynini\r\n  Running setup.py clean for pynini\r\nFailed to build pynini\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pynini)"
      },
      {
        "user": "nijanthan0",
        "created_at": "2025-02-11T02:42:07Z",
        "body": "Hi @conkeur , Did you solved this issue?"
      }
    ]
  },
  {
    "number": 88,
    "title": "Will FSQ and LFQ better when used in LLM based TTS?",
    "created_at": "2024-07-10T09:59:16Z",
    "closed_at": "2024-07-10T12:39:57Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/88",
    "body": "We are  following the TorToiseTTS like  LLM  projects.  Both TorToise and CosyVoice choosing the VQ as quantizer as I had read in papers,  while it is said that FSQ or LFQ makes less dead code.  Are there any proves that LFQ and FSQ do help LLM based tts training and inference?  Thanks for information about it, and we appreciate for more detailed explanation.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/88/comments",
    "author": "JohnHerry",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T11:28:56Z",
        "body": "@ZhihaoDU maybe can answer this question"
      },
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-10T11:41:09Z",
        "body": "According to my experiments, FSQ or LFQ indeed make less dead codes than the VQ, almost no dead codes. However, I haven't found an evidence that less dead code means better performance. Up to my knowledge, this is still an open question."
      },
      {
        "user": "JohnHerry",
        "created_at": "2024-07-10T12:39:57Z",
        "body": "> According to my experiments, FSQ or LFQ indeed make less dead codes than the VQ, almost no dead codes. However, I haven't found an evidence that less dead code means better performance. Up to my knowledge, this is still an open question.\r\n\r\nThank you very much.  \r\n"
      }
    ]
  },
  {
    "number": 84,
    "title": "关于Instruction方面的问题",
    "created_at": "2024-07-10T07:06:05Z",
    "closed_at": "2024-07-10T15:13:50Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/84",
    "body": "代码中这一部分有一个spk_id，请问用了这一个是不是说话人音色相当于被固定了？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/84/comments",
    "author": "wkingqq",
    "comments": [
      {
        "user": "hoyden",
        "created_at": "2024-07-10T09:56:50Z",
        "body": "Instruction model里的spk_id应该没有实际生效，主要还是靠instruction来控制"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T11:32:08Z",
        "body": "we will use spk id in instruct mode, but the instruct text can influence the spk identity, even change it"
      },
      {
        "user": "wkingqq",
        "created_at": "2024-07-11T02:30:41Z",
        "body": "> Instruction model里的spk_id应该没有实际生效，主要还是靠instruction来控制\r\n\r\n我测试的时候发现spk_id用英文男很容易出现性别混乱的问题"
      }
    ]
  },
  {
    "number": 83,
    "title": "克隆的声音电音很严重怎么优化？",
    "created_at": "2024-07-10T06:21:41Z",
    "closed_at": "2024-07-10T15:13:33Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/83",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/83/comments",
    "author": "hsdjkfnsfc",
    "comments": [
      {
        "user": "ScottishFold007",
        "created_at": "2024-07-10T06:30:02Z",
        "body": "很可能是你给的参考音频不行"
      }
    ]
  },
  {
    "number": 80,
    "title": "Insert pauses between sentences for a more natural flow",
    "created_at": "2024-07-10T04:14:02Z",
    "closed_at": "2024-07-15T02:07:47Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/80",
    "body": "Currently, after sentence segmentation, there is no pause between the voices of adjacent sentences, which sounds rather odd.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/80/comments",
    "author": "eshoyuan",
    "comments": [
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-10T12:29:09Z",
        "body": "Thanks for your contribution, I think 10000 samples is little longer, maybe 20~40ms is good enough. Could you kindly revise the PR to add 40 ms pause? "
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T14:53:40Z",
        "body": "please add 铝箱 in Dingding group to discuss details"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-15T02:07:48Z",
        "body": "we will add silence frame in model.py as the code is more concise in this way"
      }
    ]
  },
  {
    "number": 78,
    "title": "如果用WeTextProcessing的话，像 [laughter] 这种特殊符号还可以识别出来吗",
    "created_at": "2024-07-10T02:31:29Z",
    "closed_at": "2024-07-10T15:12:40Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/78",
    "body": "想问一下：如果用WeTextProcessing的话，像 [laughter] 这种特殊符号还可以识别出来吗？感谢！",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/78/comments",
    "author": "KevinWang676",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T03:06:30Z",
        "body": "yes it will. you can try more example to see if there is any bug"
      },
      {
        "user": "KevinWang676",
        "created_at": "2024-07-10T03:16:56Z",
        "body": "@aluminumbox Thanks for your reply. I tried to include [laughter] in my texts, however, the model could not recognize this symbol [laughter] and could not generate laughs. Can you help us resolve this issue? Thanks!"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T03:35:02Z",
        "body": "> @aluminumbox Thanks for your reply. I tried to include [laughter] in my texts, however, the model could not recognize this symbol [laughter] and could not generate laughs. Can you help us resolve this issue? Thanks!\r\n\r\nprint normalized text in your code, check if [laughter] is kept. also remember use cosyvoice-300m-instruct model"
      },
      {
        "user": "KevinWang676",
        "created_at": "2024-07-10T07:14:47Z",
        "body": "@aluminumbox I see. So, only cosyvoice-300m-instruct supports fine-grained control like [laughter] and other models like cosyvoice-300m do not. Am I right? Thanks!"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T15:12:40Z",
        "body": "> @aluminumbox I see. So, only cosyvoice-300m-instruct supports fine-grained control like [laughter] and other models like cosyvoice-300m do not. Am I right? Thanks!\r\n\r\nyes, check readme to learn how to use the 3 models"
      }
    ]
  },
  {
    "number": 73,
    "title": "对数字的朗读效果不好",
    "created_at": "2024-07-09T13:14:51Z",
    "closed_at": "2024-07-09T15:57:27Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/73",
    "body": "**Describe the bug**\r\n包含较多数字，如12345678或者2022年12月29日，的句子，朗读效果差。\r\n\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/73/comments",
    "author": "haochange",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T15:21:54Z",
        "body": "this is due to frontend process. we will use wetextprocessing to replace it, or you can convert it to character manually. these cases cannot be numerated, so we will not focus on fixing these bug"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T15:57:27Z",
        "body": "we have used wetextprocessing to replace ttsfrd, update the code and install requirements to see whether it fix the bug"
      }
    ]
  },
  {
    "number": 72,
    "title": "docker 版本运行，一个模型能支持4种模式？",
    "created_at": "2024-07-09T11:41:31Z",
    "closed_at": "2024-07-09T15:57:36Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/72",
    "body": "看运行只有CosyVoice-300M基础模型，下面client使用能分别使用sft|zero_shot|cross_lingual|instruct 4中模型？我看前面文档有写CosyVoice-300M-sft是tts，CosyVoice-300M直接是克隆，CosyVoice-300M-instruct 是加入语态控制\r\ndocker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c \"cd /opt/CosyVoice/CosyVoice/runtime/python && python3 server.py --port 50000 --max_conc 4 --model_dir iic/CosyVoice-300M && sleep infinity\"\r\n\r\n\r\npython3 client.py --port 50000 --mode <sft|zero_shot|cross_lingual|instruct>",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/72/comments",
    "author": "lonngxiang",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T15:20:36Z",
        "body": "no, the mode should be compatible with loaded model. for sft, please use cosyvoice-300m-sft, for zeroshot and cross lingual, use cosyvoice-300m, for instruct, use cosyvoice-300m-instruct. the command is just an example. please see readme for details"
      }
    ]
  },
  {
    "number": 71,
    "title": "如很改成 ip运行",
    "created_at": "2024-07-09T08:36:35Z",
    "closed_at": "2024-07-10T15:11:07Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/71",
    "body": " python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M   如何修改启动的host , 默认是本机访问，如很改成IP 访问",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/71/comments",
    "author": "zkxmaster",
    "comments": [
      {
        "user": "cacard",
        "created_at": "2024-07-09T09:48:18Z",
        "body": "你为啥改IP？默认就能通过localhost或127.0.0.1或你的IP访问。"
      },
      {
        "user": "xxxzsgxxx",
        "created_at": "2024-07-09T10:24:21Z",
        "body": "> 你为啥改IP？默认就能通过localhost或127.0.0.1或你的IP访问。\r\n\r\nlocalhost和127.0.0.1可以，外网IP地址是不行的。"
      },
      {
        "user": "lilongwei5054",
        "created_at": "2024-07-09T11:18:44Z",
        "body": "> 你为啥改IP？默认就能通过localhost或127.0.0.1或你的IP访问。\r\n\r\n因为很多调试大模型的设备是专用的设备，一般配备高性能显卡，放在机房，不是自己的工作电脑，而我们一般是通过xshell远程调试的，因此无法通过localhost访问，所以希望在自己的工作电脑上能通过局域网类似192.168.x.x 来访问。"
      },
      {
        "user": "hongdangshao",
        "created_at": "2024-07-09T12:42:41Z",
        "body": "demo.launch(server_name=\"0.0.0.0\",...)"
      },
      {
        "user": "xxxzsgxxx",
        "created_at": "2024-07-09T16:27:23Z",
        "body": "> demo.launch(server_name=\"0.0.0.0\",...)\r\n\r\n我也碰到了这个问题，demo.launch添加可以工作。\r\n\r\n可以patch一下：\r\n\r\n```\r\ncat host.patch\r\n169c169\r\n<     demo.launch(server_name=args.host,server_port=args.port)\r\n---\r\n>     demo.launch(server_port=args.port)\r\n180,185d179\r\n<     parser.add_argument('--host',\r\n<                     type=str,\r\n<                     default='0.0.0.0',\r\n<                     help='Specify the IP address to listen on. Default is 0.0.0.0.')\r\n<\r\n```"
      },
      {
        "user": "hongdangshao",
        "created_at": "2024-07-09T16:27:57Z",
        "body": "I have got it!Thanks's."
      },
      {
        "user": "shrekzhao",
        "created_at": "2025-01-18T00:21:57Z",
        "body": "not use"
      }
    ]
  },
  {
    "number": 70,
    "title": "Supervised speech tokenizer",
    "created_at": "2024-07-09T06:13:34Z",
    "closed_at": "2024-07-11T01:04:54Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/70",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThe paper includes methods about \"Supervised speech tokenizer\", but the repository does not contain the training and reconstruction test code for this part.\r\n\r\n**Describe the solution you'd like**\r\nDid you perhaps forget to open-source this part of the code? Will you open-source this implementation in the future?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/70/comments",
    "author": "LqNoob",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T06:21:00Z",
        "body": "we have no plan to opensource speech tokenizer, we only release its onnx file"
      },
      {
        "user": "LqNoob",
        "created_at": "2024-07-09T06:48:15Z",
        "body": "> we have no plan to opensource speech tokenizer, we only release its onnx file\r\n\r\nThis message is quite regrettable, but thank you for your quick reply!\r\n\r\nHowever, I have another question I would like to consult with you about. When you were conducting experiments with the speech tokenizer, how did you determine that \"using ASR as backbone and having the tokenizer provide only semantic information (without acoustic information)\" was feasible?\r\n\r\nThank you！"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T11:04:25Z",
        "body": "> > we have no plan to opensource speech tokenizer, we only release its onnx file\r\n> \r\n> This message is quite regrettable, but thank you for your quick reply!\r\n> \r\n> However, I have another question I would like to consult with you about. When you were conducting experiments with the speech tokenizer, how did you determine that \"using ASR as backbone and having the tokenizer provide only semantic information (without acoustic information)\" was feasible?\r\n> \r\n> Thank you！\r\n\r\nwe did some experiement and verified that such token can recover original mel info. @ZhihaoDU can tell you more details"
      },
      {
        "user": "LqNoob",
        "created_at": "2024-07-10T01:09:45Z",
        "body": "> > we have no plan to opensource speech tokenizer, we only release its onnx file\r\n> \r\n> This message is quite regrettable, but thank you for your quick reply!\r\n> \r\n> However, I have another question I would like to consult with you about. When you were conducting experiments with the speech tokenizer, how did you determine that \"using ASR as backbone and having the tokenizer provide only semantic information (without acoustic information)\" was feasible?\r\n> \r\n> Thank you！\r\n\r\n@ZhihaoDU Could you provide some detailed and specific information? I would be very grateful."
      },
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-10T12:14:28Z",
        "body": "> > > we have no plan to opensource speech tokenizer, we only release its onnx file\r\n> > \r\n> > \r\n> > This message is quite regrettable, but thank you for your quick reply!\r\n> > However, I have another question I would like to consult with you about. When you were conducting experiments with the speech tokenizer, how did you determine that \"using ASR as backbone and having the tokenizer provide only semantic information (without acoustic information)\" was feasible?\r\n> > Thank you！\r\n> \r\n> @ZhihaoDU Could you provide some detailed and specific information? I would be very grateful.\r\n\r\nThanks  for your attention. I believe, we have not claimed there is only semantic information (without any acoustic information) in the S3 tokens. In fact, decoupling semantic and acoustic information is still an open problem to my best knowledge. What we can do is modeling semantic information in the token as more as possible. That is also the motivation, we employ the ASR task to learn the tokenizer in a supervised manner. We think such training process can maintain more semantic information into the tokens than self-supervised or unsupervised methods, such as Encoder, SoundStream, HuBert, wave2bert and so on."
      },
      {
        "user": "LqNoob",
        "created_at": "2024-07-11T01:04:54Z",
        "body": "\r\n@ZhihaoDU \r\nThank you for your explanation of the motivation behind the speech tokenizer in the paper; it has been very helpful to me.\r\nI will close this issues."
      }
    ]
  },
  {
    "number": 67,
    "title": "模型微调，执行extract_speech_token出现报错",
    "created_at": "2024-07-09T03:34:58Z",
    "closed_at": "2024-07-09T04:00:32Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/67",
    "body": "运行tools/extract_speech_token.py出现以下报错：\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"extract_speech_token.py\", line 64, in <module>\r\n    main(args)\r\n  File \"extract_speech_token.py\", line 54, in main\r\n    speech_token = ort_session.run(None, {ort_session.get_inputs()[0].name: feat.detach().cpu().numpy(),\r\n  File \"/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 220, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: feats for the following indices\r\n index: 0 Got: 2 Expected: 1\r\n Please fix either the inputs or the model.\r\n ```\r\n 请问这个是模型的问题还是数据的问题？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/67/comments",
    "author": "wjddd",
    "comments": [
      {
        "user": "wjddd",
        "created_at": "2024-07-09T04:00:32Z",
        "body": "已解决，训练音频要求为单声道。"
      },
      {
        "user": "clearlove-civ6",
        "created_at": "2024-08-19T07:44:41Z",
        "body": "感谢"
      }
    ]
  },
  {
    "number": 63,
    "title": "微调时，训练 loss 不断降低，评估 loss 不断升高？",
    "created_at": "2024-07-09T02:03:22Z",
    "closed_at": "2024-07-09T14:04:17Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/63",
    "body": "按照 libritts 训练几个中文说话人的模型，结果发现训练 loss 不断降低，但是评估 loss 在不断升高，但是推理时合成的音频又都正常，这是什么原因？\r\n第二个问题，例子里写的是微调 200 轮，那实际上 10 小时以内的数据规模，大概需要多少轮？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/63/comments",
    "author": "zhusy09",
    "comments": [
      {
        "user": "ScottishFold007",
        "created_at": "2024-07-09T06:01:23Z",
        "body": "> 按照 libritts 训练几个中文说话人的模型，结果发现训练 loss 不断降低，但是评估 loss 在不断升高，但是推理时合成的音频又都正常，这是什么原因？ 第二个问题，例子里写的是微调 200 轮，那实际上 10 小时以内的数据规模，大概需要多少轮？\r\n典型的过拟合现象，数据搞多些吧"
      },
      {
        "user": "jiangxiaobai222",
        "created_at": "2024-07-09T06:16:11Z",
        "body": "> 按照 libritts 训练几个中文说话人的模型，结果发现训练 loss 不断降低，但是评估 loss 在不断升高，但是推理时合成的音频又都正常，这是什么原因？ 第二个问题，例子里写的是微调 200 轮，那实际上 10 小时以内的数据规模，大概需要多少轮？\r\n\r\n感觉是过拟合了，也许轮数少一些会好。\r\n\r\n同时您提到的“例子里写的是微调200轮”，请问这个例子指什么，麻烦了\r\n是指config文件吗？\r\n```\r\n# train conf\r\ntrain_conf:\r\n    optim: adam\r\n    optim_conf:\r\n        lr: 0.001\r\n    scheduler: warmuplr\r\n    scheduler_conf:\r\n        warmup_steps: 2500\r\n    max_epoch: 200\r\n    grad_clip: 5\r\n    accum_grad: 2\r\n    log_interval: 100\r\n    save_per_step: -1\r\n```\r\n"
      },
      {
        "user": "zhusy09",
        "created_at": "2024-07-09T06:25:04Z",
        "body": "对，就是这个。"
      },
      {
        "user": "zhusy09",
        "created_at": "2024-07-09T14:04:14Z",
        "body": "目前参照 fish speech 项目，把 学习率降低了，epoch 也降低了，总算正常了。"
      },
      {
        "user": "AssassinQuin",
        "created_at": "2024-07-10T03:02:08Z",
        "body": "> 目前参照 fish speech 项目，把 学习率降低了，epoch 也降低了，总算正常了。\r\n\r\n请问你数据规模多大？参数怎么调的呢？"
      },
      {
        "user": "zhusy09",
        "created_at": "2024-07-10T14:13:47Z",
        "body": "大概十几个小时，五六个人。学习率尽量小点，1e-5左右，epoch先弄个5，再慢慢增加看看。"
      },
      {
        "user": "AssassinQuin",
        "created_at": "2024-07-10T23:39:55Z",
        "body": "好的谢谢🙏"
      },
      {
        "user": "sunnnnnnnny",
        "created_at": "2024-08-08T05:57:43Z",
        "body": "对于微调的数据量要求是多少呢？用10几分钟的数据去微调，效果没什么提升"
      }
    ]
  },
  {
    "number": 62,
    "title": "更换默认ttsfrd为WeTextProcessing，修复半角句号结尾或者文本中没有标点会导致合成失败：RuntimeError: torch.cat(): expected a non-empty list of T…",
    "created_at": "2024-07-09T00:18:22Z",
    "closed_at": "2024-07-09T15:26:14Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/62",
    "body": "…ensors\r\n\r\ntext='小明因为感冒,鼻子不通,讲话总带着齉音.'\r\n  File \"/usr/local/data/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 62, in inference_zero_shot\r\n    return {'tts_speech': torch.concat(tts_speeches, dim=1)}\r\nRuntimeError: torch.cat(): expected a non-empty list of Tensors\r\n\r\n原因为self.frontend.text_normalize(tts_text, split=True)返回为空",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/62/comments",
    "author": "passerbya",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T07:13:38Z",
        "body": "nice job, we will try it later and merge it if possible"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T08:52:37Z",
        "body": "hello, please add me in Dingding group. My name is 铝箱. or wechat name AluminumBox. Please add me to discuss further details."
      }
    ]
  },
  {
    "number": 60,
    "title": "Support MPS Backend",
    "created_at": "2024-07-08T15:53:19Z",
    "closed_at": "2024-07-10T14:48:51Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/60",
    "body": "Add MPS support; now you can use –device to select MPS, CUDA, or CPU.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/60/comments",
    "author": "XXXXRT666",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T00:19:45Z",
        "body": "can we detect mps in code rather than specify device argument, then we can change only one file model.py line 22."
      },
      {
        "user": "XXXXRT666",
        "created_at": "2024-07-09T01:07:06Z",
        "body": "Yes, I kept the default setting just because I haven't tested whether it's faster or not on a larger vram machine. On my M2 chip Mac I suppose it's may slower than CPU (No exact timing)"
      },
      {
        "user": "XXXXRT666",
        "created_at": "2024-07-09T01:08:18Z",
        "body": "I will test it later on a machine with larger vram"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T15:25:14Z",
        "body": "> Yes, I kept the default setting just because I haven't tested whether it's faster or not on a larger vram machine. On my M2 chip Mac I suppose it's may slower than CPU (No exact timing)\r\n\r\nhello, please add me in Dingding group. My name is 铝箱.  Please add me to discuss further details."
      },
      {
        "user": "XXXXRT666",
        "created_at": "2024-07-09T16:58:20Z",
        "body": "> hello, please add me in Dingding group. My name is 铝箱. Please add me to discuss further details.\r\n\r\nI have enter the DingTalk group, but I think private messaging is not allowed.\r\n\r\n> Yes, I kept the default setting just because I haven't tested whether it's faster or not on a larger vram machine. On my M2 chip Mac I suppose it's may slower than CPU (No exact timing)\r\n\r\nAnd for MPS inference, it takes 100s in average for default text in webUI, while it only takes 37s in average for CPU. Also MPS have unsolved memory leaks issue, the memory increases by 8 GB after inferencing a sentence. So I set it a second choice, the default device is CUDA for Nvidia GPU and CPU for others.\r\n\r\n\r\n"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T01:18:06Z",
        "body": "> > hello, please add me in Dingding group. My name is 铝箱. Please add me to discuss further details.\r\n> \r\n> I have enter the DingTalk group, but I think private messaging is not allowed.\r\n> \r\n> > Yes, I kept the default setting just because I haven't tested whether it's faster or not on a larger vram machine. On my M2 chip Mac I suppose it's may slower than CPU (No exact timing)\r\n> \r\n> And for MPS inference, it takes 100s in average for default text in webUI, while it only takes 37s in average for CPU. Also MPS have unsolved memory leaks issue, the memory increases by 8 GB after inferencing a sentence. So I set it a second choice, the default device is CUDA for Nvidia GPU and CPU for others.\r\n\r\n@XXXXRT666 please add me now, I have removed the restriction."
      }
    ]
  },
  {
    "number": 57,
    "title": "Error happened during zero shot TTS for text that does not end with punctuation",
    "created_at": "2024-07-08T11:38:51Z",
    "closed_at": "2024-07-09T16:01:37Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/57",
    "body": "In Zero Shot mode，if the Prompt text or Target text does not end with punctuation (eg '.' or '?' or '!' ), the following error message will happen：\r\n\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 22, in <module>\r\n    output = cosyvoice.inference_zero_shot(target_text, prompt_text, prompt_speech_16k)\r\n  File \"/root/autodl-tmp/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 63, in inference_zero_shot\r\n    return {'tts_speech': torch.concat(tts_speeches, dim=1)}\r\nRuntimeError: torch.cat(): expected a non-empty list of Tensors.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/57/comments",
    "author": "LwLiu-2012",
    "comments": [
      {
        "user": "v3ucn",
        "created_at": "2024-07-08T12:27:55Z",
        "body": "推理文本和参考文本的结尾必须用标点符号结尾，否则就报错"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T16:01:37Z",
        "body": "the code is now updated, it will add puncatation by default at sentence end."
      }
    ]
  },
  {
    "number": 52,
    "title": "Zero Shot Inference Error",
    "created_at": "2024-07-08T09:57:25Z",
    "closed_at": "2024-07-09T16:04:44Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/52",
    "body": "when do zero_shot_inference, i uploaded a chinese mp3, it throws:\r\nWhen I trim the audio length to shorter, the error is gone. \r\n2024-07-08 17:52:16,887 DEBUG Used_phis: defaultdict(<class 'set'>,\r\n            {State(pc_initial=0 nstack_initial=0): set(),\r\n             State(pc_initial=186 nstack_initial=0): set(),\r\n             State(pc_initial=190 nstack_initial=0): set()})\r\n2024-07-08 17:52:16,888 DEBUG defmap: {}\r\n2024-07-08 17:52:16,888 DEBUG phismap: defaultdict(<class 'set'>, {})\r\n2024-07-08 17:52:16,888 DEBUG changing phismap: defaultdict(<class 'set'>, {})\r\n2024-07-08 17:52:16,888 DEBUG keep phismap: {}\r\n2024-07-08 17:52:16,888 DEBUG new_out: defaultdict(<class 'dict'>, {})\r\n2024-07-08 17:52:16,888 DEBUG ----------------------DONE Prune PHIs-----------------------\r\n2024-07-08 17:52:16,888 DEBUG block_infos State(pc_initial=0 nstack_initial=0):\r\nAdaptBlockInfo(insts=((0, {}), (2, {}), (4, {'res': '$x4.0'}), (6, {'res': '$const6.1'}), (8, {'index': '$const6.1', 'target': '$x4.0', 'res': '$8binary_subscr.2'}), (10, {}), (12, {}), (14, {}), (16, {}), (18, {'res': '$x18.3'}), (20, {'res': '$const20.4'}), (22, {'index': '$const20.4', 'target': '$x18.3', 'res': '$22binary_subscr.5'}), (24, {}), (26, {}), (28, {}), (30, {}), (32, {'op': '+', 'lhs': '$8binary_subscr.2', 'rhs': '$22binary_subscr.5', 'res': '$binop_add32.6'}), (34, {}), (36, {'res': '$const36.7'}), (38, {'res': '$x38.8'}), (40, {'res': '$const40.9'}), (42, {'index': '$const40.9', 'target': '$x38.8', 'res': '$42binary_subscr.10'}), (44, {}), (46, {}), (48, {}), (50, {}), (52, {'op': '*', 'lhs': '$const36.7', 'rhs': '$42binary_subscr.10', 'res': '$binop_mul52.11'}), (54, {}), (56, {'op': '-', 'lhs': '$binop_add32.6', 'rhs': '$binop_mul52.11', 'res': '$binop_sub56.12'}), (58, {}), (60, {'value': '$binop_sub56.12'}), (62, {'res': '$x62.13'}), (64, {'res': '$const64.14'}), (66, {'index': '$const64.14', 'target': '$x62.13', 'res': '$66binary_subscr.15'}), (68, {}), (70, {}), (72, {}), (74, {}), (76, {'res': '$x76.16'}), (78, {'res': '$const78.17'}), (80, {'index': '$const78.17', 'target': '$x76.16', 'res': '$80binary_subscr.18'}), (82, {}), (84, {}), (86, {}), (88, {}), (90, {'op': '-', 'lhs': '$66binary_subscr.15', 'rhs': '$80binary_subscr.18', 'res': '$binop_sub90.19'}), (92, {}), (94, {'res': '$const94.20'}), (96, {'op': '/', 'lhs': '$binop_sub90.19', 'rhs': '$const94.20', 'res': '$binop_truediv96.21'}), (98, {}), (100, {'value': '$binop_truediv96.21'}), (102, {'idx': 0, 'res': '$102load_global.22'}), (104, {}), (106, {}), (108, {}), (110, {}), (112, {}), (114, {'item': '$102load_global.22', 'res': '$114load_attr.24'}), (116, {}), (118, {}), (120, {}), (122, {}), (124, {'res': '$b124.25'}), (126, {}), (128, {}), (130, {'func': '$114load_attr.24', 'args': ['$b124.25'], 'kw_names': None, 'res': '$130call.26'}), (132, {}), (134, {}), (136, {}), (138, {}), (140, {'idx': 0, 'res': '$140load_global.27'}), (142, {}), (144, {}), (146, {}), (148, {}), (150, {}), (152, {'item': '$140load_global.27', 'res': '$152load_attr.29'}), (154, {}), (156, {}), (158, {}), (160, {}), (162, {'res': '$a162.30'}), (164, {}), (166, {}), (168, {'func': '$152load_attr.29', 'args': ['$a162.30'], 'kw_names': None, 'res': '$168call.31'}), (170, {}), (172, {}), (174, {}), (176, {}), (178, {'lhs': '$130call.26', 'rhs': '$168call.31', 'res': '$178compare_op.32'}), (180, {}), (182, {}), (184, {'pred': '$178compare_op.32'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={186: (), 190: ()})\r\n2024-07-08 17:52:16,889 DEBUG block_infos State(pc_initial=186 nstack_initial=0):\r\nAdaptBlockInfo(insts=((186, {'res': '$const186.0'}), (188, {'retval': '$const186.0', 'castval': '$188return_value.1'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})\r\n2024-07-08 17:52:16,889 DEBUG block_infos State(pc_initial=190 nstack_initial=0):\r\nAdaptBlockInfo(insts=((190, {'res': '$b190.0'}), (192, {'value': '$b190.0', 'res': '$192unary_negative.1'}), (194, {'res': '$a194.2'}), (196, {'op': '/', 'lhs': '$192unary_negative.1', 'rhs': '$a194.2', 'res': '$binop_truediv196.3'}), (198, {}), (200, {'retval': '$binop_truediv196.3', 'castval': '$200return_value.4'})), outgoing_phis={}, blockstack=(), active_try_block=None, outgoing_edgepushed={})\r\n2024-07-08 17:52:16,890 DEBUG label 0:\r\n    x = arg(0, name=x)                       ['x']\r\n    $const6.1 = const(int, 1)                ['$const6.1']\r\n    $8binary_subscr.2 = getitem(value=x, index=$const6.1, fn=<built-in function getitem>) ['$8binary_subscr.2', '$const6.1', 'x']\r\n    $const20.4 = const(int, -1)              ['$const20.4']\r\n    $22binary_subscr.5 = getitem(value=x, index=$const20.4, fn=<built-in function getitem>) ['$22binary_subscr.5', '$const20.4', 'x']\r\n    $binop_add32.6 = $8binary_subscr.2 + $22binary_subscr.5 ['$22binary_subscr.5', '$8binary_subscr.2', '$binop_add32.6']\r\n    $const36.7 = const(int, 2)               ['$const36.7']\r\n    $const40.9 = const(int, 0)               ['$const40.9']\r\n    $42binary_subscr.10 = getitem(value=x, index=$const40.9, fn=<built-in function getitem>) ['$42binary_subscr.10', '$const40.9', 'x']\r\n    $binop_mul52.11 = $const36.7 * $42binary_subscr.10 ['$42binary_subscr.10', '$binop_mul52.11', '$const36.7']\r\n    a = $binop_add32.6 - $binop_mul52.11     ['$binop_add32.6', '$binop_mul52.11', 'a']\r\n    $const64.14 = const(int, 1)              ['$const64.14']\r\n    $66binary_subscr.15 = getitem(value=x, index=$const64.14, fn=<built-in function getitem>) ['$66binary_subscr.15', '$const64.14', 'x']\r\n    $const78.17 = const(int, -1)             ['$const78.17']\r\n    $80binary_subscr.18 = getitem(value=x, index=$const78.17, fn=<built-in function getitem>) ['$80binary_subscr.18', '$const78.17', 'x']\r\n    $binop_sub90.19 = $66binary_subscr.15 - $80binary_subscr.18 ['$66binary_subscr.15', '$80binary_subscr.18', '$binop_sub90.19']\r\n    $const94.20 = const(int, 2)              ['$const94.20']\r\n    b = $binop_sub90.19 / $const94.20        ['$binop_sub90.19', '$const94.20', 'b']\r\n    $102load_global.22 = global(np: <module 'numpy' from 'C:\\\\Users\\\\sekkitshi\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\cosyvoice\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'>) ['$102load_global.22']\r\n    $114load_attr.24 = getattr(value=$102load_global.22, attr=abs) ['$102load_global.22', '$114load_attr.24']\r\n    $130call.26 = call $114load_attr.24(b, func=$114load_attr.24, args=[Var(b, pitch.py:429)], kws=(), vararg=None, varkwarg=None, target=None) ['$114load_attr.24', '$130call.26', 'b']\r\n    $140load_global.27 = global(np: <module 'numpy' from 'C:\\\\Users\\\\sekkitshi\\\\AppData\\\\Local\\\\miniconda3\\\\envs\\\\cosyvoice\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'>) ['$140load_global.27']\r\n    $152load_attr.29 = getattr(value=$140load_global.27, attr=abs) ['$140load_global.27', '$152load_attr.29']\r\n    $168call.31 = call $152load_attr.29(a, func=$152load_attr.29, args=[Var(a, pitch.py:428)], kws=(), vararg=None, varkwarg=None, target=None) ['$152load_attr.29', '$168call.31', 'a']\r\n    $178compare_op.32 = $130call.26 >= $168call.31 ['$130call.26', '$168call.31', '$178compare_op.32']\r\n    bool184 = global(bool: <class 'bool'>)   ['bool184']\r\n    $184pred = call bool184($178compare_op.32, func=bool184, args=(Var($178compare_op.32, pitch.py:431),), kws=(), vararg=None, varkwarg=None, target=None) ['$178compare_op.32', '$184pred', 'bool184']\r\n    branch $184pred, 186, 190                ['$184pred']\r\nlabel 186:\r\n    $const186.0 = const(int, 0)              ['$const186.0']\r\n    $188return_value.1 = cast(value=$const186.0) ['$188return_value.1', '$const186.0']\r\n    return $188return_value.1                ['$188return_value.1']\r\nlabel 190:\r\n    $192unary_negative.1 = unary(fn=<built-in function neg>, value=b) ['$192unary_negative.1', 'b']\r\n    $binop_truediv196.3 = $192unary_negative.1 / a ['$192unary_negative.1', '$binop_truediv196.3', 'a']\r\n    $200return_value.4 = cast(value=$binop_truediv196.3) ['$200return_value.4', '$binop_truediv196.3']\r\n    return $200return_value.4                ['$200return_value.4']\r\n\r\n2024-07-08 17:52:17.2848084 [E:onnxruntime:, sequential_executor.cc:514 onnxruntime::ExecuteKernel] Non-zero status code returned while running Add node. Name:'/Add_2' Status Message: D:\\a\\_work\\1\\s\\onnxruntime\\core/providers/cpu/math/element_wise_ops.h:560 onnxruntime::BroadcastIterator::Append axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 1500 by 1932\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\gradio\\queueing.py\", line 521, in process_events\r\n    response = await route_utils.call_process_api(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\gradio\\blocks.py\", line 1945, in process_api\r\n    result = await self.call_function(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\gradio\\blocks.py\", line 1513, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2177, in run_sync_in_worker_thread\r\n    return await future\r\n           ^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 859, in run\r\n    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\gradio\\utils.py\", line 831, in wrapper\r\n    response = f(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^\r\n  File \"X:\\tmp\\CosyVoice_For_Windows-pack\\webui.py\", line 126, in generate_audio\r\n    output = cosyvoice.inference_zero_shot(tts_text, prompt_text, prompt_speech_16k)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"X:\\tmp\\CosyVoice_For_Windows-pack\\cosyvoice\\cli\\cosyvoice.py\", line 63, in inference_zero_shot\r\n    model_input = self.frontend.frontend_zero_shot(i, prompt_text, prompt_speech_16k)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"X:\\tmp\\CosyVoice_For_Windows-pack\\cosyvoice\\cli\\frontend.py\", line 120, in frontend_zero_shot\r\n    speech_token, speech_token_len = self._extract_speech_token(prompt_speech_16k)\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"X:\\tmp\\CosyVoice_For_Windows-pack\\cosyvoice\\cli\\frontend.py\", line 66, in _extract_speech_token\r\n    speech_token = self.speech_tokenizer_session.run(None, {self.speech_tokenizer_session.get_inputs()[0].name: feat.detach().cpu().numpy(),\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\sekkitshi\\AppData\\Local\\miniconda3\\envs\\cosyvoice\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py\", line 220, in run    return self._sess.run(output_names, input_feed, run_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nonnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Add node. Name:'/Add_2' Status Message: D:\\a\\_work\\1\\s\\onnxruntime\\core/providers/cpu/math/element_wise_ops.h:560 onnxruntime::BroadcastIterator::Append axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 1500 by 1932",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/52/comments",
    "author": "sekkit",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T16:04:44Z",
        "body": "make sure prompt audio less than 30s"
      }
    ]
  },
  {
    "number": 49,
    "title": "Inference performance is relatively poor and speech synthesis is slow",
    "created_at": "2024-07-08T09:31:03Z",
    "closed_at": "2024-07-10T15:09:12Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/49",
    "body": "**Describe the bug**\r\n\r\nTest on a RTX 4090(24GB) server, found out it is very slow to inference.\r\n\r\nModel: CosyVoice-300M-SFT\r\nInput: 虽然技术路线和《Her》有所差别，但从直观效果来看，也算得上是给网友们带来了新的玩具。\r\nTake time: 5.51s\r\n\r\nIs it possible to make reasoning faster, please?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/49/comments",
    "author": "iflamed",
    "comments": [
      {
        "user": "xwang0415",
        "created_at": "2024-07-09T09:24:21Z",
        "body": "可以通过frontend.text_normalize处做合成文本切分，实现短句流式推理，不过还是耗时较长，主要在llm和flow，期待优化方案。\r\n\r\n另外借楼咨询下tiny模型是否开源计划？"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T16:06:07Z",
        "body": "we are currently working on fixing bugs, we may try to increase inference speed later. we also used wetextprocessing to replace ttsfrd."
      },
      {
        "user": "hanswang73",
        "created_at": "2024-11-26T04:38:47Z",
        "body": "> we are currently working on fixing bugs, we may try to increase inference speed later. we also used wetextprocessing to replace ttsfrd.\r\n\r\n推理加速的新版本，今年出的来吗？"
      }
    ]
  },
  {
    "number": 46,
    "title": "为何跨语种复制不需要prompt text",
    "created_at": "2024-07-08T08:49:54Z",
    "closed_at": "2024-07-09T16:07:54Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/46",
    "body": "为何跨语种复制不需要prompt text",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/46/comments",
    "author": "sekkit",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T08:53:40Z",
        "body": "check inference code for more details"
      }
    ]
  },
  {
    "number": 45,
    "title": "webdui里，如果输入文本里，没有。等结束符号，会无法生成音频。",
    "created_at": "2024-07-08T08:41:42Z",
    "closed_at": "2024-07-08T08:46:36Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/45",
    "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/45/comments",
    "author": "dogvane",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T08:46:36Z",
        "body": "this is frontend logic, please process it by yourself, or make sure add punctation at sentence end"
      }
    ]
  },
  {
    "number": 44,
    "title": "Any plans for SSML support?",
    "created_at": "2024-07-08T08:01:07Z",
    "closed_at": "2024-07-08T08:47:39Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/44",
    "body": null,
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/44/comments",
    "author": "wjddd",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T08:47:39Z",
        "body": "this is business logic, you can implement it by yourself, we do not have plans to suport it, but we may support more symbols like <laugnter> <angry> <sad>"
      }
    ]
  },
  {
    "number": 43,
    "title": "ttsfrd安装起来比较麻烦，能不能换成WeTextProcessing，也是一个比较好用的TN库。",
    "created_at": "2024-07-08T07:49:34Z",
    "closed_at": "2024-07-09T16:08:14Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/43",
    "body": "ttsfrd安装起来比较麻烦，能不能换成WeTextProcessing，也是一个比较好用的TN库。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/43/comments",
    "author": "lidingke",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T08:51:29Z",
        "body": "we will replace it later"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T16:08:14Z",
        "body": "update code, we have used wetextprocessing to replace ttsfrd"
      },
      {
        "user": "sirius-ai",
        "created_at": "2024-07-10T07:58:22Z",
        "body": "while use wetextprocessing instead none before，the inference time cost more ~20%."
      }
    ]
  },
  {
    "number": 40,
    "title": "Missing information of **spk_id**",
    "created_at": "2024-07-08T06:45:50Z",
    "closed_at": "2024-07-08T08:14:14Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/40",
    "body": "Thanks for the great work of your team, \r\n\r\nI had a problem when using\r\n`output = cosyvoice.inference_instruct('在面对挑战时，他展现了非凡的<strong>勇气</strong>与<strong>智慧</strong>。', '中文男', 'Theo \\'Crimson\\', is a fiery, passionate rebel leader. Fights with fervor for justice, but struggles with impulsiveness.')\r\n`\r\nthere is a **spk_id** parameter， In addition to the \"中文男\" in the text, can you tell me other parameters that can be used?\r\nThank you for your help😊",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/40/comments",
    "author": "TheCutestCat",
    "comments": [
      {
        "user": "Lixi20",
        "created_at": "2024-07-08T07:01:06Z",
        "body": "Literal['中文女', '中文男', '英文女', '英文男', '日语男', '粤语女', '韩语女'] Default: \"中文女\""
      }
    ]
  },
  {
    "number": 39,
    "title": "请问如何微调，训练自己的语音模型",
    "created_at": "2024-07-08T03:32:43Z",
    "closed_at": "2024-07-09T16:09:05Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/39",
    "body": "oneshot的效果不稳定，我看project页面说可以支持finetuning，但是在git上没有看到有关finetuning和介绍\r\n\r\n是否会提供相关代码或者文档？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/39/comments",
    "author": "zergtant",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T05:32:14Z",
        "body": "see example/libritts/run.sh"
      },
      {
        "user": "zergtant",
        "created_at": "2024-07-09T00:37:53Z",
        "body": "好的，我研究下，谢谢"
      },
      {
        "user": "chenxu126",
        "created_at": "2024-07-12T05:42:58Z",
        "body": "这个run.sh是基础训练么？"
      }
    ]
  },
  {
    "number": 38,
    "title": "speaker",
    "created_at": "2024-07-08T03:18:26Z",
    "closed_at": "2024-07-10T15:08:20Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/38",
    "body": "可以识别音频里面的speaker吗。有没有借口？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/38/comments",
    "author": "Lixi20",
    "comments": [
      {
        "user": "Lixi20",
        "created_at": "2024-07-08T03:18:41Z",
        "body": "> 可以识别音频里面的speaker吗。有没有接口？\r\n\r\n"
      },
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-10T12:31:56Z",
        "body": "我想你要找的是SenseVoice或者3D-speaker。CosyVoice是生成模型，不能识别说话人。"
      }
    ]
  },
  {
    "number": 37,
    "title": "如何训练一个新的语言",
    "created_at": "2024-07-08T02:57:53Z",
    "closed_at": "2024-07-10T15:08:10Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/37",
    "body": "我想基于这个模型结构训练其他的语言，想请教一下：\r\n1. 训练的数据格式要求是怎样的，有参考示例吗？\r\n2. 预训练的代码可以在哪参考？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/37/comments",
    "author": "kwaihua",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T03:07:20Z",
        "body": "Check run.sh in examples/libritts, you can prepare your own data for sft or training from scratch. you can start with llm training."
      }
    ]
  },
  {
    "number": 36,
    "title": "Optimize model loading and reasoning",
    "created_at": "2024-07-08T02:17:41Z",
    "closed_at": "2024-07-09T15:51:57Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/36",
    "body": "Optimize model loading and reasoning",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/36/comments",
    "author": "v3ucn",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T03:05:51Z",
        "body": "Thank you for your suggestions, we will consider some of them, for example empty cache. but in inference, we specify the kwargs in llm/flow/hift because we want a more clear view of the inputs of each module. The code you provided is easier to read, but maybe harder to debug. We will update our code according to your suggestions, but not the whole PR. "
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-09T15:51:57Z",
        "body": "@v3ucn thank you for your PR. we ignored the refactor part because we want a more detailed view of how cosyvoice model works, we keep the torch.cuda.empty_cache code as it can release cuda memory release after every inference. Thank you for your effor."
      }
    ]
  },
  {
    "number": 35,
    "title": "ubuntu执行代码出现问题",
    "created_at": "2024-07-08T01:41:17Z",
    "closed_at": "2024-07-10T15:08:01Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/35",
    "body": "pydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package报这个错误怎么办\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/35/comments",
    "author": "zhiwanxia",
    "comments": [
      {
        "user": "zhiwanxia",
        "created_at": "2024-07-08T01:42:12Z",
        "body": "pydoc.ErrorDuringImport: problem in cosyvoice.hifigan.generator - ModuleNotFoundError: No module named 'academicodec'还会出现这个问题\r\n"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T01:46:47Z",
        "body": "check faq.md, export pythonpath before run python"
      },
      {
        "user": "zhiwanxia",
        "created_at": "2024-07-08T01:54:02Z",
        "body": "> check faq.md, export pythonpath before run python\r\nexport pythonpath之后会出现这个\r\npydoc.ErrorDuringImport: problem in cosyvoice.hifigan.generator - ModuleNotFoundError: No module named 'academicodec'怎么办？\r\n"
      },
      {
        "user": "Hwangkop",
        "created_at": "2024-07-08T02:04:21Z",
        "body": "did you execute this command ?  ```git submodule update --init --recursive```"
      },
      {
        "user": "zhiwanxia",
        "created_at": "2024-07-08T02:11:08Z",
        "body": "> did you execute this command ? `git submodule update --init --recursive`\r\nyes,but still don't work"
      },
      {
        "user": "ldxbxl",
        "created_at": "2024-07-08T05:25:48Z",
        "body": "> > did you execute this command ? `git submodule update --init --recursive`\r\n> > yes,but still don't work\r\n\r\n我也遇到了相同的报错ModuleNotFoundError: No module named 'academicodec'，我已将matcha-tts的代码放到文件夹下面了然后就出现了这个错误"
      },
      {
        "user": "yangcunning1",
        "created_at": "2024-07-08T09:48:36Z",
        "body": "> pydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package报这个错误怎么办\r\n\r\n我是这么解决的，下载Matcha-TTS，并把Matcha-TTS放到third_party目录下"
      },
      {
        "user": "ldxbxl",
        "created_at": "2024-07-08T14:29:23Z",
        "body": "> > pydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package报这个错误怎么办\r\n> \r\n> 我是这么解决的，下载Matcha-TTS，并把Matcha-TTS放到third_party目录下\r\n\r\nModuleNotFoundError: No module named 'academicodec'我下载并放到third_party目录下是这样报错的"
      },
      {
        "user": "ldxbxl",
        "created_at": "2024-07-08T14:42:08Z",
        "body": "> > pydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package报这个错误怎么办\r\n> \r\n> 我是这么解决的，下载Matcha-TTS，并把Matcha-TTS放到third_party目录下\r\n\r\n请问放到third_party目录下还需要在按照Matcha-TTS的readme安装Matcha-TTS的包吗？还有是不是需要python版本大于等于3.9\r\n"
      },
      {
        "user": "yangcunning1",
        "created_at": "2024-07-08T14:48:22Z",
        "body": "直接放入就行，不需要安装，按照git上的安装指导，安装的python3.8\r\n\r\n\r\n\r\n---原始邮件---\r\n发件人: ***@***.***&gt;\r\n发送时间: 2024年7月8日(周一) 晚上10:42\r\n收件人: ***@***.***&gt;;\r\n抄送: ***@***.******@***.***&gt;;\r\n主题: Re: [FunAudioLLM/CosyVoice] ubuntu执行代码出现问题 (Issue #35)\r\n\r\n\r\n\r\n\r\n   \r\npydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package报这个错误怎么办\r\n  \r\n我是这么解决的，下载Matcha-TTS，并把Matcha-TTS放到third_party目录下\r\n  \r\n请问放到third_party目录下还需要在按照Matcha-TTS的readme安装Matcha-TTS的包吗？还有是不是需要python版本大于等于3.9\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you commented.Message ID: ***@***.***&gt;"
      },
      {
        "user": "13140438775",
        "created_at": "2024-08-07T10:59:43Z",
        "body": "> 直接放入就行，不需要安装，按照git上的安装指导，安装的python3.8\r\n> […](#)\r\n> ---原始邮件--- 发件人: ***@***.***&gt; 发送时间: 2024年7月8日(周一) 晚上10:42 收件人: ***@***.***&gt;; 抄送: ***@***.******@***.***&gt;; 主题: Re: [FunAudioLLM/CosyVoice] ubuntu执行代码出现问题 (Issue #35) pydoc.ErrorDuringImport: problem in cosyvoice.flow.flow_matching - ModuleNotFoundError: No module named 'matcha.models'; 'matcha' is not a package报这个错误怎么办 我是这么解决的，下载Matcha-TTS，并把Matcha-TTS放到third_party目录下 请问放到third_party目录下还需要在按照Matcha-TTS的readme安装Matcha-TTS的包吗？还有是不是需要python版本大于等于3.9 — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***&gt;\r\n\r\n用python3.10不可以吗？"
      }
    ]
  },
  {
    "number": 33,
    "title": "FileNotFoundError: [Errno 2] No such file or directory: '***/CosyVoice-main/examples/libritts/cosyvoice/data/test-clean/parquet/utt2parquet_000000000.json'",
    "created_at": "2024-07-07T17:58:35Z",
    "closed_at": "2024-07-10T15:07:43Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/33",
    "body": "echo \"Run inference. Please make sure utt in tts_text is in prompt_data\"\r\n怎么准备inference的文件呀？\r\nFileNotFoundError: [Errno 2] No such file or directory: '/***/CosyVoice-main/examples/libritts/cosyvoice/data/test-clean/parquet/utt2parquet_000000000.json'\r\n这个是什么问题\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/33/comments",
    "author": "zhaojingxin123",
    "comments": [
      {
        "user": "zhaojingxin123",
        "created_at": "2024-07-07T18:08:22Z",
        "body": "这个问题是在stage 4 inference出现的\r\n"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-08T01:46:22Z",
        "body": "really, have you executed stage 1-3? what do you have in data/test-clean/parquet directory?"
      },
      {
        "user": "huskyachao",
        "created_at": "2024-07-23T03:14:43Z",
        "body": "您好，请问你解决了吗  我也遇到了同样的问题 "
      }
    ]
  },
  {
    "number": 30,
    "title": "中文 省略号 bug",
    "created_at": "2024-07-07T07:09:34Z",
    "closed_at": "2024-07-08T11:08:37Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/30",
    "body": "例子：\r\n“少爷你听话，你不能出去，让小楚去，让他去吧……”\r\n\r\n此句子中没有         pounc = ['。', '？', '！', '；', '：', '.', '?', '!', ';'] 标点符号，会导致没有字符输出，合成会失败\r\n\r\ncode：cosyvoice/utils/frontend_utils.py\r\n    if lang == \"zh\":\r\n        pounc = ['。', '？', '！', '；', '：', '.', '?', '!', ';']\r\n    else:\r\n        pounc = ['.', '?', '!', ';', ':']\r\n    if comma_split:\r\n        pounc.extend(['，', ','])\r\n    st = 0\r\n    utts = []\r\n    for i, c in enumerate(text):\r\n        if c in pounc:\r\n            if len(text[st: i]) > 0:\r\n                utts.append(text[st: i] + c)\r\n            if i + 1 < len(text) and text[i + 1] in ['\"', '”']:\r\n                tmp = utts.pop(-1)\r\n                utts.append(tmp + text[i + 1])\r\n                st = i + 2\r\n            else:\r\n                st = i + 1\r\n                \r\n             ",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/30/comments",
    "author": "AssassinQuin",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T08:13:44Z",
        "body": "This is due to text frontend error. Please modify it by yourself because we cannot consider all such cases in the code."
      }
    ]
  },
  {
    "number": 29,
    "title": "docs: update README.md",
    "created_at": "2024-07-07T03:22:23Z",
    "closed_at": "2024-07-07T08:22:26Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/29",
    "body": "recommand -> recommend",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/29/comments",
    "author": "eltociear",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T08:22:07Z",
        "body": "Thank you very much for the PR. We have closed it due to merge conflict with current branch and we have solved it according to your suggestion. Hope you can try more with our repo and give more suggestions."
      }
    ]
  },
  {
    "number": 26,
    "title": "about pretrained_models directory",
    "created_at": "2024-07-06T14:37:45Z",
    "closed_at": "2024-07-07T08:22:37Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/pull/26",
    "body": "If the pre-trained model has already been downloaded to the pretrained_models directory, then the model should be loaded from the local directory instead of being re-downloaded when loading the model. If the model is downloaded again in the user's cache directory, it will cause unnecessary hard disk space waste.",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/26/comments",
    "author": "v3ucn",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T08:22:37Z",
        "body": "Thank you very much for the PR. We have closed it due to merge conflict with current branch and we have solved it according to your suggestion. Hope you can try more with our repo and give more suggestions."
      }
    ]
  },
  {
    "number": 25,
    "title": "能提供一份阿里云的镜像吗，现在太难下载原始镜像了",
    "created_at": "2024-07-06T14:07:34Z",
    "closed_at": "2024-07-10T15:06:26Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/25",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/25/comments",
    "author": "xs818818",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T01:23:26Z",
        "body": "我们这个dockerfile是从dockerhub原始镜像编译过来的，是为了让用户能够自己增加自己的业务逻辑，可以试试开个vpn，先把基础镜像下载好"
      }
    ]
  },
  {
    "number": 24,
    "title": "有没有什么技巧可以生成中英文混读？",
    "created_at": "2024-07-06T13:53:34Z",
    "closed_at": "2024-07-10T15:06:18Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/24",
    "body": "单单中文，和英文都很好。不过试了一下混读，感觉不太好。有什么技巧么？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/24/comments",
    "author": "fxsome",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T01:26:28Z",
        "body": "看看日志里，是不是前端对于中英文混读处理的有问题，如果是的话，那么可以手动调整一下中英文混合输入，毕竟前端规则无法穷举"
      },
      {
        "user": "ben-8878",
        "created_at": "2024-07-12T09:02:21Z",
        "body": "1. 中英混合，某个英文单词会读错，看tn处理是对的。\r\n2. 中英混合，英文单词和英文单词间会有相对较长的停顿，整体觉得会比较突兀，比如欢迎收看HI FOUR T发布会，FOUR和T之间会停顿0.5s,可感觉衔接不自然。\r\n有什么应对办法吗"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-12T09:04:41Z",
        "body": "1.  try change FOUR to four or Four\r\n2.  no easy solution yet"
      },
      {
        "user": "ben-8878",
        "created_at": "2024-07-12T09:09:46Z",
        "body": "> 1. try change FOUR to four or Four\r\n好多了，谢谢哈哈哈哈\r\n> 2. no easy solution yet\r\n\r\n"
      }
    ]
  },
  {
    "number": 22,
    "title": "pydoc.ErrorDuringImport: problem in cosyvoice.hifigan.generator - ModuleNotFoundError: No module named 'academicodec'",
    "created_at": "2024-07-06T11:49:28Z",
    "closed_at": "2024-07-06T14:07:44Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/22",
    "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/22/comments",
    "author": "xs818818",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-06T12:28:41Z",
        "body": "check readme ,remember export pythonpath and git submodule download third_party modules"
      }
    ]
  },
  {
    "number": 21,
    "title": "ttsfrd apple metal cpu(arm64) support",
    "created_at": "2024-07-06T10:03:50Z",
    "closed_at": "2024-07-09T01:40:08Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/21",
    "body": "**Is your feature request related to a problem? Please describe.** \r\nWanna run cosyvoice on macbook metal cpu\r\n\r\n**Describe the solution you'd like** \r\nprovide `ttsfrd-0.3.6-cp38-cp38-linux_arm64.whl`\r\n\r\n**Describe alternatives you've considered**\r\n-\r\n\r\n**Additional context**\r\n-\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/21/comments",
    "author": "songlairui",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-06T12:29:11Z",
        "body": "we will add wetextprocessing to replace ttsfrd when ttsfrd is unavaliable"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T08:25:14Z",
        "body": "please update the repo, ttsfrd is now optional during inference"
      }
    ]
  },
  {
    "number": 19,
    "title": "安装环境时，提示python 3.8版本太低",
    "created_at": "2024-07-06T08:48:40Z",
    "closed_at": "2024-07-10T15:06:05Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/19",
    "body": "ERROR: Ignored the following versions that require a different python version: 3.2 Requires-Python >=3.9; 3.2.1 Requires-Python >=3.9; 3.2rc0 Requires-Python >=3.9; 3.3 Requires-Python >=3.10; 3.3rc0 Requires-Python >=3.10; 3.8.0 Requires-Python >=3.9; 3.8.0rc1 Requires-Python >=3.9; 3.8.1 Requires-Python >=3.9; 3.8.2 Requires-Python >=3.9; 3.8.3 Requires-Python >=3.9; 3.8.4 Requires-Python >=3.9; 3.9.0 Requires-Python >=3.9; 3.9.0rc2 Requires-Python >=3.9; 3.9.1 Requires-Python >=3.9\r\nERROR: Could not find a version that satisfies the requirement onnxruntime-gpu==1.16.0 (from versions: none)\r\nERROR: No matching distribution found for onnxruntime-gpu==1.16.0",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/19/comments",
    "author": "qiangduscu",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-06T12:31:01Z",
        "body": "Really? are you using linux or windows system?"
      },
      {
        "user": "threadflow",
        "created_at": "2024-07-07T04:09:17Z",
        "body": "Try to use conda create -n cosyvoice python=3.10, seems work."
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T05:52:06Z",
        "body": "I believe you are using MacOS. Please change onnxruntime-gpu==1.16 to onnxruntime in requirements.txt"
      }
    ]
  },
  {
    "number": 16,
    "title": "speech-to-speech voice cloning feature",
    "created_at": "2024-07-06T06:15:14Z",
    "closed_at": "2024-07-10T15:04:53Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/16",
    "body": "Does this project support speech-to-speech voice cloning like sovits-svc? If not, will you guys support this in the future?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/16/comments",
    "author": "NeoBerekov",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-06T12:33:45Z",
        "body": "Maybe you mean voice conversion? You can use our zero_shot tts to clone voices already."
      }
    ]
  },
  {
    "number": 15,
    "title": "支持流式的TTS输出吗",
    "created_at": "2024-07-06T04:31:44Z",
    "closed_at": "2024-07-10T15:04:34Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/15",
    "body": "很棒的工作，有像fish-speech那种支持流式的tts输出吗，加快数字人说话的反应时间。",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/15/comments",
    "author": "rximg",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-06T12:34:29Z",
        "body": "hmm we only provided basic train and inference, we may consider streaming output if we have extra hands"
      },
      {
        "user": "chenxu126",
        "created_at": "2024-07-12T04:35:11Z",
        "body": "哈哈，看出来意思就是不可能"
      },
      {
        "user": "yuyun2000",
        "created_at": "2024-07-12T10:06:58Z",
        "body": "> 哈哈，看出来意思就是不可能\r\n\r\n你那里看出来不可能了...人家都说了人手不够"
      }
    ]
  },
  {
    "number": 12,
    "title": "english short words infer error",
    "created_at": "2024-07-05T10:21:13Z",
    "closed_at": "2024-07-09T02:09:49Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/12",
    "body": "run inference code as shown below: \r\n\r\n```\r\nref_path=\"female01.wav\"\r\ncosyvoice = CosyVoice(\"pretrained_models/CosyVoice-300M\")\r\n# zero_shot usage\r\nprompt_speech_16k = load_wav(ref_path, 16000)\r\noutput = cosyvoice.inference_cross_lingual(\r\n    \"hello world\",\r\n    prompt_speech_16k,\r\n)\r\n```\r\n\r\n\r\nencounter error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"my_local_infer.py\", line 18, in <module>\r\n    output = cosyvoice.inference_cross_lingual(\r\n  File \"/data/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 75, in inference_cross_lingual\r\n    return {'tts_speech': torch.concat(tts_speeches, dim=1)}\r\nRuntimeError: torch.cat(): expected a non-empty list of Tensors\r\n```",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/12/comments",
    "author": "marson666",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-05T10:42:59Z",
        "body": "please add punctation at sentence end, for example 'hello world.'"
      }
    ]
  },
  {
    "number": 11,
    "title": "后续会支持Windows么？",
    "created_at": "2024-07-05T10:10:38Z",
    "closed_at": "2024-07-07T08:58:53Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/11",
    "body": "目前看到限制python3.8 linux平台",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/11/comments",
    "author": "CyberWon",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-05T10:42:24Z",
        "body": "有docker支持，windows镜像部署即可，这个是python的运行环境"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-07T08:26:31Z",
        "body": "check #31  for windows usage"
      },
      {
        "user": "CyberWon",
        "created_at": "2024-07-07T08:59:11Z",
        "body": "刘悦大佬出马，哈哈哈。"
      }
    ]
  },
  {
    "number": 10,
    "title": "是否支持其他语种的合成",
    "created_at": "2024-07-05T09:10:44Z",
    "closed_at": "2024-07-10T15:03:52Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/10",
    "body": "从readme上看貌似只支持中文和英文的tts。其他语种的是否支持呢？如果支持的话，要怎样设置指定的语种呢？",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/10/comments",
    "author": "ZeeChao",
    "comments": [
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-05T09:17:11Z",
        "body": "支持中英日粤韩五个语种，可以在要合成文本前面加上Language ID，比如\r\n中文：<|zh|>\r\n英文：<|en|>\r\n日文：<|jp|>\r\n粤语：<|yue|>\r\n韩语：<|ko|>"
      },
      {
        "user": "abc8350712",
        "created_at": "2024-07-08T12:03:00Z",
        "body": "`cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M')\r\n\r\nprompt_speech_16k = load_wav('dzq_ref.wav', 16000)\r\n\r\noutput = cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '这是一段参考音频，请录制我的声音', prompt_speech_16k)\r\n\r\ntorchaudio.save('zero_shot.wav', output['tts_speech'], 22050)`\r\n\r\n@ZhihaoDU 请问我这个该怎么改呢？我想指定中文，而不是粤语"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T15:03:52Z",
        "body": "> `cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M')\r\n> \r\n> prompt_speech_16k = load_wav('dzq_ref.wav', 16000)\r\n> \r\n> output = cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '这是一段参考音频，请录制我的声音', prompt_speech_16k)\r\n> \r\n> torchaudio.save('zero_shot.wav', output['tts_speech'], 22050)`\r\n> \r\n> @ZhihaoDU 请问我这个该怎么改呢？我想指定中文，而不是粤语\r\n\r\nuse <|zh|> for chinese, see readme"
      }
    ]
  },
  {
    "number": 9,
    "title": "docker版本镜像可以提供公开下载的地址吗",
    "created_at": "2024-07-05T09:00:29Z",
    "closed_at": "2024-07-10T15:03:05Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/9",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/9/comments",
    "author": "lonngxiang",
    "comments": [
      {
        "user": "aluminumbox",
        "created_at": "2024-07-05T10:43:37Z",
        "body": "not yet, we only provide dockerfile and you can build it yourself, because you may want to add your own code in server.py"
      },
      {
        "user": "DoiiarX",
        "created_at": "2024-09-23T12:38:04Z",
        "body": "好想吐槽啊。这dockerfile也太糟糕了。按教程来都走不通。只能走本地安装的路。"
      }
    ]
  },
  {
    "number": 8,
    "title": "用预置语音生成（sft）inference会漏句",
    "created_at": "2024-07-05T08:39:25Z",
    "closed_at": "2024-07-10T15:02:55Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/8",
    "body": "用预置语音生成（sft）inference会漏句，音色：中文女\r\n输入：\r\n那有哪些美剧是不太适合学英语的呢？我来给大家举几个例子吧。第一个《破产姐妹》，我不知道为什么总有人推荐这一部，我先声明一下，我真的很喜欢很喜欢破产姐妹，它真的很下饭。我大学有一段时间就是天天去食堂打包吃的，然后回到宿舍，我就边看边吃，甚至听到她那个片头曲，我就会很有食欲，但是我真的真的没有办法用它来学英语。**一个是语速太快了**；第二全是开车的台词，你说是生活中、考试试中哪儿会用到？所以我觉得破产姐妹下饭必备，学英语还是算了。\r\n\r\n其中,  \"一个是语速太快了\"没有合成直接被跳过了",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/8/comments",
    "author": "zzu-xcy",
    "comments": [
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-05T09:15:47Z",
        "body": "建议可以尝试适当拆句之后再合成，比如这一段话拆成两段分别合成试试看"
      },
      {
        "user": "Liujingxiu23",
        "created_at": "2024-07-05T10:32:52Z",
        "body": "@ZhihaoDU 请教下 SFT(比如使用5小时单发音人数据)，会导致鲁棒性下降，比如 重复、插入、替换错误的增多吗？如果不会，是通过何种方式进行SFT的呢。比如设置学习率、batchsize有什么技巧吗"
      },
      {
        "user": "Pydataman",
        "created_at": "2024-07-07T01:08:41Z",
        "body": "mark"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-10T15:02:55Z",
        "body": "> @ZhihaoDU 请教下 SFT(比如使用5小时单发音人数据)，会导致鲁棒性下降，比如 重复、插入、替换错误的增多吗？如果不会，是通过何种方式进行SFT的呢。比如设置学习率、batchsize有什么技巧吗\r\n\r\ncheck example/libritts for sft training example, it will futher improve model performance on specific speaker"
      }
    ]
  },
  {
    "number": 7,
    "title": "ImportError: cannot import name 'Annotated' from 'pydantic.typing'",
    "created_at": "2024-07-05T08:38:19Z",
    "closed_at": "2024-07-05T08:58:43Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/7",
    "body": "example.py is copied from official example. \r\n\r\n2024-07-05 16:29:50,876 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\r\n2024-07-05 16:29:50,876 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\r\n2024-07-05 16:29:52,012 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 51af4c199ce0493bf05f6e6a4c460b07 and a total number of 980 components indexed\r\ntransformer is not installed, please install it if you want to use related modules\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 1, in <module>\r\n    from cosyvoice.cli.cosyvoice import CosyVoice\r\n  File \"/mnt/bigclass/project/cosyvoice/CosyVoice/cosyvoice/cli/cosyvoice.py\", line 18, in <module>\r\n    from cosyvoice.cli.frontend import CosyVoiceFrontEnd\r\n  File \"/mnt/bigclass/project/cosyvoice/CosyVoice/cosyvoice/cli/frontend.py\", line 23, in <module>\r\n    import inflect\r\n  File \"/mnt/bigclass/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/inflect/__init__.py\", line 77, in <module>\r\n    from pydantic.typing import Annotated\r\nImportError: cannot import name 'Annotated' from 'pydantic.typing' (/mnt/bigclass/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/pydantic/typing.py)\r\n\r\nI update pydantic.typing from 2.7.0 to 2.8.3, still ...\r\nNeed I add transformers to requirements.txt?",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/7/comments",
    "author": "neuxys",
    "comments": [
      {
        "user": "DarkLand-Chen",
        "created_at": "2024-07-05T08:48:30Z",
        "body": "File \"/mnt/bigclass/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/inflect/init.py\", line 77  修改  from pydantic.typing import Annotated  改成 from typing_extensions import Annotated"
      },
      {
        "user": "neuxys",
        "created_at": "2024-07-05T08:57:48Z",
        "body": "> File \"/mnt/bigclass/anaconda3/envs/cosyvoice/lib/python3.8/site-packages/inflect/init.py\", line 77 修改 from pydantic.typing import Annotated 改成 from typing_extensions import Annotated\r\n\r\nYou are my idol, problem solved."
      }
    ]
  },
  {
    "number": 4,
    "title": "哈哈哈的声音合成不出来",
    "created_at": "2024-07-05T06:18:13Z",
    "closed_at": "2024-07-10T15:00:45Z",
    "labels": [],
    "url": "https://github.com/FunAudioLLM/CosyVoice/issues/4",
    "body": "略显生硬",
    "comments_url": "https://api.github.com/repos/FunAudioLLM/CosyVoice/issues/4/comments",
    "author": "MonolithFoundation",
    "comments": [
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-05T07:12:14Z",
        "body": "试试”哈哈“两个字，以及多个不同的seed看看。另外笑声等富语言推荐使用instruct模型"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-07-05T07:53:46Z",
        "body": "都不行 效果很差\n\n\n\n---- 回复的原邮件 ----\n| 发件人 | Zhihao ***@***.***> |\n| 日期 | 2024年07月05日 15:12 |\n| 收件人 | ***@***.***> |\n| 抄送至 | ***@***.***>***@***.***> |\n| 主题 | Re: [FunAudioLLM/CosyVoice] 哈哈哈的声音合成不出来 (Issue #4) |\n\n试试”哈哈“两个字，以及多个不同的seed看看。另外笑声等富语言推荐使用instruct模型\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>"
      },
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-05T08:07:34Z",
        "body": "我简单试了一下，选预置音色生成、中文女，这句话效果还是可以呀\r\n\r\n哈哈，那真是太好了，很高兴能够帮助您的孩子学习英语。\r\n\r\n你再试试？\r\n"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-07-05T09:14:09Z",
        "body": "还是比较生硬。\r\n\r\n在语气词这块，相较于ChatTTS还是差蛮多。但是音色更好一点。请问有计划改进一下吗？\r\n\r\n这里说的笑指的是笑声，不是念哈哈两个字，这样听起来就非常生硬，不知道CosyVoice是否有考虑加一些类似于笑、嗯、等的一些语气声\r\n\r\n如果是Instruct模型的话，是否能提供一个完整的强调模版呢？ 用instruct 哈哈哈也是念字。\r\n\r\n此外，Instruct我设置中文男，出来的事女音"
      },
      {
        "user": "ZhihaoDU",
        "created_at": "2024-07-05T09:33:22Z",
        "body": "使用instruct模型是这样设置的，\r\n单个笑声：那位喜剧演员真有才，[laughter]一开口就让全场观众爆笑。\r\n笑着说：他搞的一个恶作剧，让大家\\<laughter\\>忍俊不禁\\</laughter\\>。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-07-05T10:14:26Z",
        "body": "有更全的这种语气配置吗，可以列在readme里面，不然大家都不知道如何用\n\n\n\n---- 回复的原邮件 ----\n| 发件人 | Zhihao ***@***.***> |\n| 日期 | 2024年07月05日 17:33 |\n| 收件人 | ***@***.***> |\n| 抄送至 | ***@***.***>***@***.***> |\n| 主题 | Re: [FunAudioLLM/CosyVoice] 哈哈哈的声音合成不出来 (Issue #4) |\n\n使用instruct模型是这样设置的，\n单个笑声：那位喜剧演员真有才，[laughter]一开口就让全场观众爆笑。\n笑着说：他搞的一个恶作剧，让大家忍俊不禁。\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>"
      },
      {
        "user": "aluminumbox",
        "created_at": "2024-07-05T10:45:17Z",
        "body": "> 有更全的这种语气配置吗，可以列在readme里面，不然大家都不知道如何用\r\n> […](#)\r\n> ---- 回复的原邮件 ---- | 发件人 | Zhihao ***@***.***> | | 日期 | 2024年07月05日 17:33 | | 收件人 | ***@***.***> | | 抄送至 | ***@***.***>***@***.***> | | 主题 | Re: [FunAudioLLM/CosyVoice] 哈哈哈的声音合成不出来 (Issue #4) | 使用instruct模型是这样设置的， 单个笑声：那位喜剧演员真有才，[laughter]一开口就让全场观众爆笑。 笑着说：他搞的一个恶作剧，让大家忍俊不禁。 — Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***>\r\n\r\n后续我们再readme里会列出来"
      },
      {
        "user": "lukecq1231",
        "created_at": "2024-07-08T02:14:27Z",
        "body": "目前支持四种细粒度标签，分别是插入笑声、换气、边笑边说、重读。下面是具体的示例：\r\n1. Well that’s kind of scary [laughter].\r\n2. I don’t think I over eat yeah [breath] and um I do exercise regularly.\r\n3. Well that pretty much covers \\<laughter>the subject\\</laughter> well thanks for calling me.\r\n4. The team’s \\<strong>unity\\</strong> and \\<strong>resilience\\</strong> helped them win the championship.\r\n\r\n"
      },
      {
        "user": "lukecq1231",
        "created_at": "2024-07-08T02:23:05Z",
        "body": "> 还是比较生硬。\r\n> \r\n> 在语气词这块，相较于ChatTTS还是差蛮多。但是音色更好一点。请问有计划改进一下吗？\r\n> \r\n> 这里说的笑指的是笑声，不是念哈哈两个字，这样听起来就非常生硬，不知道CosyVoice是否有考虑加一些类似于笑、嗯、等的一些语气声\r\n> \r\n> 如果是Instruct模型的话，是否能提供一个完整的强调模版呢？ 用instruct 哈哈哈也是念字。\r\n> \r\n> 此外，Instruct我设置中文男，出来的事女音\r\n\r\n因为Instruct模型的LM模型没有使用Speaker Embedding，所以没法通过只给Speaker Embedding的方式确定特定的音色，从而会导致就算给定的是中文男的Speaker Embedding，也会有一定概率出来女的声音。 如果想要Instruct的模型出比较稳定的音色，建议用续写的方式来使用，也就是LM模型给audio prompt和对应的text prompt，同时flow matching模型也给audio prompt。"
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-07-11T08:32:21Z",
        "body": "那这个就很蛋疼啊，\r\n\r\n首先只有instruct支持这种音调，然后又不能指定音色。\r\n\r\n我需要sft那个男音，就那个音色还可以，应该怎么做到"
      }
    ]
  }
]