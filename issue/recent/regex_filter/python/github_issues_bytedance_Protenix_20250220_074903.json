[
  {
    "number": 45,
    "title": "Protein ubiquitination",
    "created_at": "2025-01-04T20:42:54Z",
    "closed_at": "2025-01-06T20:09:47Z",
    "labels": [],
    "url": "https://github.com/bytedance/Protenix/issues/45",
    "body": "Hi, I'm modeling a ubiquitin-labeled protein and need help introducing a covalent bond between the Nε of Lysine 13 in my target protein and the carbonyl carbon of Glycine 76 in ubiquitin's C-terminus.\r\n\r\nI ran Protenix (web server) with the following parameters, but the resulting structures are not covalently linked. I'm puzzled about the results and would greatly appreciate any guidance on modeling the ubiquitinated state. Thank you!\r\n\r\n\"root\":{4 items\r\n\"name\":string\"protenix_job_ubK13Cpten_covalent\"\r\n\"sequences\":[2 items\r\n0:{1 item\r\n\"proteinChain\":{3 items\r\n\"count\":int1\r\n\"sequence\":string\"MTAIIKEIVSRNKRRYQEDGFDLDLTYIYPNIIAMGFPAERLEGVYRNNIDDVVRFLDSKHKNHYKIYNLCAERHYDTAKFNCRVAQYPFEDHNPPQLELIKPFCEDLDQWLSEDDNHVAAIHCKAGKGRTGVMICAYLLHRGKFLKAQEALDFYGEVRTRDKKGVTIPSQRRYVYYYSYLLKNHLDYRPVALLFHKMMFETIPMFSGGTCNPQFVVCQLKVKIYSSNSGPTRREDKFMYFEFPQPLPVCGDIKVEFFHKQNKMLKKDKMFHFWVNTFFIPGPEETSEKVENGSLCDQEIDSICSIERADNDKEYLVLTLTKNDLDKANKDKANRYFSPNFKVKLYFTKTVEEPSNPEASSSTSVTPDVSDNEPDHYRYSDTTDSDPENEPFDEDQHTQITKV\"\r\n\"modifications\":[]0 items\r\n}\r\n}\r\n1:{1 item\r\n\"proteinChain\":{3 items\r\n\"count\":int1\r\n\"sequence\":string\"MQIFVKTLTGKTITLEVEPSDTIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG\"\r\n\"modifications\":[]0 items\r\n}\r\n}\r\n]\r\n\"covalent_bonds\":[1 item\r\n0:{8 items\r\n\"entity1\":int1\r\n\"position1\":int13\r\n\"entity2\":int2\r\n\"position2\":int76\r\n\"copy1\":int1\r\n\"copy2\":int1\r\n\"atom1\":string\"NZ\"\r\n\"atom2\":string\"C\"\r\n}\r\n]\r\n\"model_seeds\":[1 item\r\n0:int92892\r\n]\r\n}",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/45/comments",
    "author": "jonleebiochem",
    "comments": [
      {
        "user": "cloverzizi",
        "created_at": "2025-01-06T02:15:05Z",
        "body": "Hi, @jonleebiochem \r\nUnfortunately, our current model's input features do not include polymer-polymer bonds, which means we can't specify ubiquitination structures through `covalent_bonds`. In this scenario, specifying `covalent_bonds` will only assist in removing the leaving atom from residues, but the model may not predict the specified atoms within covalent distance. We plan to optimize for this situation in the future."
      },
      {
        "user": "jonleebiochem",
        "created_at": "2025-01-06T20:09:40Z",
        "body": "Hi @cloverzizi, thank you so much for the clarification! I’ll look into alternative ways to model ubiquitination for now. Thanks again for considering adding this feature in the future—it’ll be super useful and handy for chemical biologists."
      }
    ]
  },
  {
    "number": 35,
    "title": "Error when only predicting ligand",
    "created_at": "2024-12-08T15:34:57Z",
    "closed_at": "2024-12-17T12:52:40Z",
    "labels": [],
    "url": "https://github.com/bytedance/Protenix/issues/35",
    "body": "Hi! I wanna only predict the ligand structure, but proteinix raised errors:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/workspace/runner/inference.py\", line 213, in main\r\n    runner.dumper.dump(\r\n  File \"/workspace/runner/dumper.py\", line 74, in dump\r\n    self.dump_predictions(\r\n  File \"/workspace/runner/dumper.py\", line 107, in dump_predictions\r\n    self._save_structure(\r\n  File \"/workspace/runner/dumper.py\", line 135, in _save_structure\r\n    save_structure_cif(\r\n  File \"/workspace/protenix/data/utils.py\", line 181, in save_structure_cif\r\n    save_atoms_to_cif(\r\n  File \"/workspace/protenix/data/utils.py\", line 154, in save_atoms_to_cif\r\n    cifwriter.save_to_cif(\r\n  File \"/workspace/protenix/data/utils.py\", line 288, in save_to_cif\r\n    block_dict.update(self._get_entity_poly_and_entity_poly_seq_block())\r\n  File \"/workspace/protenix/data/utils.py\", line 260, in _get_entity_poly_and_entity_poly_seq_block\r\n    \"entity_poly\": pdbx.CIFCategory(entity_poly),\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 327, in __init__\r\n    columns = {\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 328, in <dictcomp>\r\n    key: CIFColumn(col) if not isinstance(col, CIFColumn) else col\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 138, in __init__\r\n    data = CIFData(data, str)\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 66, in __init__\r\n    self._array = _arrayfy(array)\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 1061, in _arrayfy\r\n    raise ValueError(\"Array must contain at least one element\")\r\nValueError: Array must contain at least one element\r\n```\r\n\r\nThe json file is:\r\n```\r\n[\r\n    {\r\n        \"sequences\": [\r\n            {\r\n                \"ligand\": {\r\n                    \"ligand\": \"COc1cc(OC)ccc1/C=C/N(C(=O)C)C\",\r\n                    \"count\": 1\r\n                }\r\n            }\r\n        ],\r\n        \"modelSeeds\": [],\r\n        \"assembly_id\": \"1\",\r\n        \"name\": \"LIG_1\"\r\n    }\r\n]\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/35/comments",
    "author": "v-shaoningli",
    "comments": [
      {
        "user": "cloverzizi",
        "created_at": "2024-12-12T03:05:45Z",
        "body": "Hi Shaoning, \r\n\r\nThis issue has been resolved in the recent code update. The task results without polymer can now be saved normally. \r\nThank you for the feedback :D\r\n"
      },
      {
        "user": "v-shaoningli",
        "created_at": "2024-12-12T08:12:44Z",
        "body": "Thanks for the update!"
      }
    ]
  },
  {
    "number": 31,
    "title": "How to do the atom permutation for ligand prediciton?",
    "created_at": "2024-11-26T10:01:30Z",
    "closed_at": "2024-12-18T14:40:24Z",
    "labels": [],
    "url": "https://github.com/bytedance/Protenix/issues/31",
    "body": "Hi.\r\n\r\nI noticed that Protenix implemented **atom permutation.**\r\nMy question is this: I used Protenix to predict a protein-ligand complex. Now I want to perform atom permutation on some symmetric atoms of the ligand. How can I use your implementation of atom permutation to accomplish this?\r\nCould you provide some examples?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/31/comments",
    "author": "fuxuliu",
    "comments": [
      {
        "user": "zhangyuxuann",
        "created_at": "2024-12-17T13:29:16Z",
        "body": "@fuxuliu Do you mean you alreadly have the ground truth, then you want to perform atom permutation? cc @cloverzizi "
      },
      {
        "user": "fuxuliu",
        "created_at": "2024-12-17T14:19:19Z",
        "body": "@zhangyuxuann yes."
      },
      {
        "user": "cloverzizi",
        "created_at": "2024-12-18T03:12:41Z",
        "body": "Hi @fuxuliu , I've prepared a demo for performing atom permutation on the 'PCG' ligand within PDB ID: 7PZB for your reference. You can execute it by downloading the 7PZB CIF file and using functions from Protenix.\r\n\r\n```\r\nimport copy\r\n\r\nimport biotite.structure.io.pdbx as pdbx\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom protenix.data.parser import AddAtomArrayAnnot\r\nfrom protenix.utils.permutation import atom_permutation\r\n\r\nmmcif_file = \"7pzb.cif\"\r\n\r\nwith open(mmcif_file, \"rt\") as f:\r\n    cif_file = pdbx.CIFFile.read(f)\r\n\r\n\r\natom_array = pdbx.get_structure(\r\n    cif_file, model=1, altloc=\"first\", extra_fields=[\"label_asym_id\"]\r\n)\r\n\r\nlig_true = atom_array[\r\n    atom_array.label_asym_id == \"G\"\r\n]  # use chain \"G\" (ligand ) as an example\r\n\r\nlig_true = lig_true[\r\n    ~np.isin(lig_true.element, [\"H\", \"D\"])\r\n]  # ignore H, hydrogen is not considered in permutations\r\n\r\n\r\nlig_pred = copy.deepcopy(lig_true)  # your pred lig\r\natom_perm_list = AddAtomArrayAnnot.add_res_perm(lig_pred)  # find symmetry atoms\r\n\r\n\r\ntrue_coord = torch.tensor(lig_true.coord)\r\ntruc_coord_mask = torch.ones(true_coord.shape[0])\r\npred_coord = torch.tensor(lig_pred.coord)\r\n\r\n# Each residue has a unique ID in the structure (chain ID + residue ID)\r\n# In this example, there is only one ligand, so its Res ID is used directly\r\nref_space_uid = torch.tensor(lig_true.res_id.astype(int))\r\n\r\npermuted_pred_dict, atom_perm_log_dict, atom_perm_pred_indices = atom_permutation.run(\r\n    pred_coord=pred_coord,\r\n    true_coord=true_coord,\r\n    true_coord_mask=truc_coord_mask,\r\n    ref_space_uid=ref_space_uid,\r\n    atom_perm_list=atom_perm_list,\r\n    permute_label=False,\r\n)\r\n\r\n\r\npermuted_pred_coord = permuted_pred_dict[\"coordinate\"]\r\n\r\nprint(permuted_pred_coord)  # N_atoms, 3\r\nprint(atom_perm_log_dict)  # logging info for the permutation\r\nprint(atom_perm_pred_indices)  # permutation indices\r\n```\r\n\r\n"
      },
      {
        "user": "fuxuliu",
        "created_at": "2024-12-18T03:15:59Z",
        "body": "@cloverzizi Thanks for the demo, I will try it later."
      }
    ]
  },
  {
    "number": 28,
    "title": "Issues with cuda cusparse and cuda_runtime.api",
    "created_at": "2024-11-15T21:38:30Z",
    "closed_at": "2024-12-17T12:57:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/bytedance/Protenix/issues/28",
    "body": "Hi, \r\n\r\nThank you for this amazing work. I wanted to set up a conda environment to run inference with the model you provided but I keep running into issues. I've followed the steps in the Dockerfile and also in #5 but I keep getting the following error. \r\n\r\n```\r\nbash inference_demo.sh \r\n[2024-11-15 16:31:57,365] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.\r\n [WARNING]  async_io: please install the libaio-dev package with apt\r\n [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\r\n [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\r\n [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\r\n [WARNING]  using untested triton version (2.3.1), only 1.0.0 is known to be compatible\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/build.ninja...\r\nBuilding extension module fastfold_layer_norm_cuda...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n[1/3] /home/alper/miniconda3/envs/protenix/bin/x86_64-conda-linux-gnu-c++ -MMD -MF layer_norm_cuda.o.d -DTORCH_EXTENSION_NAME=fastfold_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/TH -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/THC -isystem /home/alper/miniconda3/envs/protenix/include -isystem /home/alper/miniconda3/envs/protenix/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel/layer_norm_cuda.cpp -o layer_norm_cuda.o \r\nFAILED: layer_norm_cuda.o \r\n/home/alper/miniconda3/envs/protenix/bin/x86_64-conda-linux-gnu-c++ -MMD -MF layer_norm_cuda.o.d -DTORCH_EXTENSION_NAME=fastfold_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/TH -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/THC -isystem /home/alper/miniconda3/envs/protenix/include -isystem /home/alper/miniconda3/envs/protenix/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel/layer_norm_cuda.cpp -o layer_norm_cuda.o \r\nIn file included from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/c10/cuda/CUDAGraphsC10Utils.h:3,\r\n                 from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/c10/cuda/CUDACachingAllocator.h:4,\r\n                 from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/c10/cuda/impl/CUDAGuardImpl.h:8,\r\n                 from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/c10/cuda/CUDAGuard.h:7,\r\n                 from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel/layer_norm_cuda.cpp:15:\r\n/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/c10/cuda/CUDAStream.h:6:10: fatal error: cuda_runtime_api.h: No such file or directory\r\n    6 | #include <cuda_runtime_api.h>\r\n      |          ^~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n[2/3] /home/alper/miniconda3/envs/protenix/bin/nvcc --generate-dependencies-with-compile --dependency-output layer_norm_cuda_kernel.cuda.o.d -ccbin /home/alper/miniconda3/envs/protenix/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=fastfold_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/TH -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/THC -isystem /home/alper/miniconda3/envs/protenix/include -isystem /home/alper/miniconda3/envs/protenix/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++17 -maxrregcount=50 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -c /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel/layer_norm_cuda_kernel.cu -o layer_norm_cuda_kernel.cuda.o \r\nFAILED: layer_norm_cuda_kernel.cuda.o \r\n/home/alper/miniconda3/envs/protenix/bin/nvcc --generate-dependencies-with-compile --dependency-output layer_norm_cuda_kernel.cuda.o.d -ccbin /home/alper/miniconda3/envs/protenix/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=fastfold_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/TH -isystem /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/THC -isystem /home/alper/miniconda3/envs/protenix/include -isystem /home/alper/miniconda3/envs/protenix/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++17 -maxrregcount=50 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -c /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel/layer_norm_cuda_kernel.cu -o layer_norm_cuda_kernel.cuda.o \r\nnvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\r\nIn file included from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/ATen/cuda/CUDAContext.h:3,\r\n                 from /home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/kernel/layer_norm_cuda_kernel.cu:26:\r\n/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/include/ATen/cuda/CUDAContextLight.h:7:10: fatal error: cusparse.h: No such file or directory\r\n    7 | #include <cusparse.h>\r\n      |          ^~~~~~~~~~~~\r\ncompilation terminated.\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/layer_norm.py\", line 28, in <module>\r\n    fastfold_layer_norm_cuda = importlib.import_module(\"fastfold_layer_norm_cuda\")\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'fastfold_layer_norm_cuda'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2107, in _run_ninja_build\r\n    subprocess.run(\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/subprocess.py\", line 526, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/alper/Documents/packages/Protenix/runner/inference.py\", line 32, in <module>\r\n    from protenix.model.protenix import Protenix\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/protenix.py\", line 31, in <module>\r\n    from protenix.openfold_local.model.primitives import LayerNorm\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/openfold_local/model/primitives.py\", line 41, in <module>\r\n    from protenix.model.layer_norm.layer_norm import FusedLayerNorm\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/__init__.py\", line 16, in <module>\r\n    from .layer_norm import FusedLayerNorm\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/layer_norm.py\", line 33, in <module>\r\n    fastfold_layer_norm_cuda = compile(\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/protenix/model/layer_norm/torch_ext_compile.py\", line 23, in compile\r\n    return load(\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1309, in load\r\n    return _jit_compile(\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1719, in _jit_compile\r\n    _write_ninja_file_and_build_library(\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1832, in _write_ninja_file_and_build_library\r\n    _run_ninja_build(\r\n  File \"/home/alper/miniconda3/envs/protenix/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2123, in _run_ninja_build\r\n    raise RuntimeError(message) from e\r\nRuntimeError: Error building extension 'fastfold_layer_norm_cuda'\r\n```\r\n\r\nAll the necessary cuda stuff is installed and are in the conda environment but the compiler can't seem to be able to find them. \r\n\r\nThanks, \r\nAlper",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/28/comments",
    "author": "celalp",
    "comments": [
      {
        "user": "yangyanpinghpc",
        "created_at": "2024-11-18T04:02:38Z",
        "body": "1. verify whether you have installed CUDA.\r\n2. check whether the CUDA_HOME environment variable has been set. ( export CUDA_HOME=/usr/local/cuda )"
      }
    ]
  },
  {
    "number": 24,
    "title": "Licensing Question",
    "created_at": "2024-11-14T19:41:11Z",
    "closed_at": "2024-11-15T10:18:10Z",
    "labels": [],
    "url": "https://github.com/bytedance/Protenix/issues/24",
    "body": "Why is using the code and weights locally restricted to noncommercial use, even when the webserver has no such noncommercial restriction? Thanks in advance.",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/24/comments",
    "author": "jaketanderson",
    "comments": [
      {
        "user": "zhangyuxuann",
        "created_at": "2024-11-15T05:49:40Z",
        "body": "@jaketanderson Due to our limited GPU quota and the need for more GPUs to train a better model, we are exploring commercial income opportunities. This is why we have chosen to use a non-commercial license for our code and model weights. However, to allow more people to experience and test our model, our server operates under a more permissive license."
      },
      {
        "user": "jaketanderson",
        "created_at": "2024-11-15T10:17:46Z",
        "body": "@zhangyuxuann that makes sense, thank you for explaining!"
      }
    ]
  },
  {
    "number": 23,
    "title": "Using AF3 weights with Protenix",
    "created_at": "2024-11-14T17:56:25Z",
    "closed_at": "2024-12-17T12:58:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/bytedance/Protenix/issues/23",
    "body": "Is there a way to use the recently released AF3 weights using Protenix so that we can do a head-to-head comparison on specific use cases? OpenFold has a similar implementation to use AF2 weights with their PyTorch OpenFold model.",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/23/comments",
    "author": "abhinavb22",
    "comments": [
      {
        "user": "zhangyuxuann",
        "created_at": "2024-11-15T04:50:10Z",
        "body": "Hi @abhinavb22, our model is released before the AF3 inference code is available, currently we do not support do a head-to-head comparison with released AF3. **Additionally, due to the limitations of the AF3-related license, this functionality will not be supported in the future.**"
      }
    ]
  },
  {
    "number": 13,
    "title": "How to make mmcif_bioassembly.pkl.gz data",
    "created_at": "2024-11-13T03:13:46Z",
    "closed_at": "2025-02-06T05:56:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/bytedance/Protenix/issues/13",
    "body": "Hi.\r\n\r\nI noticed that you processed the RAW data of **mmcif/mmcif bioassembly** into a file in **pkl.gz format**, which contains relevant data of mmcif/mmcif bioassembly. Such as atom array data in **AtomArray** format and **TokenArray** token array data, such as **AtomArray contains more meta info about the target.**\r\n\r\nNow I want to process a raw data of an mmcif/mmcif bioassembly into a file of the same format, so that I can conduct some experiments on your pipeline. Could you please provide a relevant script or tool to realize such processing and conversion?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/13/comments",
    "author": "fuxuliu",
    "comments": [
      {
        "user": "willx-y",
        "created_at": "2024-11-14T13:31:20Z",
        "body": "We used some seperate scripts and distributed pipelines to create bioassembly dict, so there is no single script ready to release yet. \r\nWe may need some extra work to prepare this, and there is no time expectation for this yet."
      }
    ]
  },
  {
    "number": 9,
    "title": "How is masking implemented for padded structures?",
    "created_at": "2024-11-11T18:27:34Z",
    "closed_at": "2024-11-14T13:41:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/bytedance/Protenix/issues/9",
    "body": "Hi,\r\n\r\nAmazing work on the reproduction!\r\n\r\nI was wondering how masking is implemented in attention. In the standard_multihead_attention method for instance, there is no mask to account for residues that might be padded during training. Having to mask things all the time was annoying in my repo and I am really curious how you got around this issue.\r\n\r\ndef standard_multihead_attention(\r\n        self,\r\n        a: torch.Tensor,\r\n        s: torch.Tensor,\r\n        z: torch.Tensor,\r\n        inplace_safe: bool = False,\r\n    ) -> torch.Tensor:\r\n        \"\"\"Used by Algorithm 7/20\r\n\r\n        Args:\r\n            a (torch.Tensor): the single feature aggregate per-atom representation\r\n                [..., N_token, c_a]\r\n            s (torch.Tensor): single embedding\r\n                [..., N_token, c_s]\r\n            z (torch.Tensor): pair embedding, used for computing pair bias.\r\n                [..., N_token, N_token, c_z]\r\n            inplace_safe (bool): Whether it is safe to use inplace operations. Defaults to False.\r\n\r\n        Returns:\r\n            torch.Tensor: the updated a from AttentionPairBias\r\n                [..., N_token, c_a]\r\n        \"\"\"\r\n\r\nArda\r\n",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/9/comments",
    "author": "ardagoreci",
    "comments": [
      {
        "user": "v-shaoningli",
        "created_at": "2024-11-12T02:40:26Z",
        "body": "In AF3, the batch size is `1` (per GPU) so we do not use mask. As for its sampling, it generates **same** 48 protein so we do not need mask as well."
      },
      {
        "user": "zhangyuxuann",
        "created_at": "2024-11-12T02:42:19Z",
        "body": "Hi @ardagoreci, due to GPU memory limitations, the batch size during different training stages (AF2, AF3) is set to 1, allowing for simplification of some batch-level masks. However, using masks remains a more elegant approach, and we may consider incorporating this in future versions."
      }
    ]
  },
  {
    "number": 2,
    "title": "RuntimeError: DataLoader worker (pid 572) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
    "created_at": "2024-11-09T11:30:14Z",
    "closed_at": "2024-11-14T13:39:58Z",
    "labels": [
      "documentation"
    ],
    "url": "https://github.com/bytedance/Protenix/issues/2",
    "body": "Hi, could I get an idea of how much memory is required?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/2/comments",
    "author": "yipy0005",
    "comments": [
      {
        "user": "willx-y",
        "created_at": "2024-11-09T12:16:04Z",
        "body": "For initial training stage training, each dataloader worker costs about 15~18GB memory.\r\nI think the simple work around could be using less dataloader workers with --data.num_dl_workers x, the default value is 16."
      },
      {
        "user": "yipy0005",
        "created_at": "2024-11-10T11:21:49Z",
        "body": "Thanks for replying to my issue! I have one GPU (RTX 4090) and I just wanted to run inference. I tried `--data.num_dl_worker 1`, it didn't resolve the issue."
      },
      {
        "user": "nimrod-bio",
        "created_at": "2024-11-10T13:31:03Z",
        "body": "the default docker shared memory for interprocess communication is very low\r\nyou can bypass it by mounting the host shared memory:  add `-v /dev/shm:/dev/shm` to the docker run command"
      },
      {
        "user": "zhangyuxuann",
        "created_at": "2024-11-11T03:05:23Z",
        "body": "> the default docker shared memory for interprocess communication is very low you can bypass it by mounting the host shared memory: add `-v /dev/shm:/dev/shm` to the docker run command\r\n\r\n@yipy0005  can you try the method proposed by @nimrod-bio?  Thanks to @nimrod-bio, we will update the README later\r\n```bash\r\ndocker run --gpus all -it -v $(pwd):/workspace -v /dev/shm:/dev/shm ai4s-cn-beijing.cr.volces.com/infra/protenix:v0.0.1 /bin/bash\r\n```"
      },
      {
        "user": "v-shaoningli",
        "created_at": "2024-11-11T05:01:21Z",
        "body": "It is due to the limited shared memory in docker. \r\nYou can check the memory by `df -lh | grep shm` or manually set  ```bash\r\n    docker run --gpus all -it --shm-size=128g -v $(pwd):/workspace ai4s-cn-beijing.cr.volces.com/infra/protenix:v0.0.1 /bin/bash\r\n    ```"
      }
    ]
  },
  {
    "number": 1,
    "title": "Having issues running \"bash inference_demo.sh\"",
    "created_at": "2024-11-09T11:14:18Z",
    "closed_at": "2024-11-09T11:21:42Z",
    "labels": [],
    "url": "https://github.com/bytedance/Protenix/issues/1",
    "body": "The following message occurred when I was running `bash inference_demo.sh` in the docker image:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/runner/inference.py\", line 30, in <module>\r\n    from configs.configs_base import configs as configs_base\r\nModuleNotFoundError: No module named 'configs'\r\n```\r\n\r\nAppreciate if I could get some help to resolve this. It would be a shame to be unable to run such a good repository.",
    "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/1/comments",
    "author": "yipy0005",
    "comments": [
      {
        "user": "yipy0005",
        "created_at": "2024-11-09T11:21:42Z",
        "body": "A quick solution would be to copy the folders `configs` and `protenix` into the folder `runner`."
      },
      {
        "user": "zhangyuxuann",
        "created_at": "2024-11-09T12:22:14Z",
        "body": "Run `pip install -e .` before training and inference."
      }
    ]
  }
]