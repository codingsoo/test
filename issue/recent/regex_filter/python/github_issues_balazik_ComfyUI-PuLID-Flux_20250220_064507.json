[
  {
    "number": 71,
    "title": "RuntimeError: expected scalar type Half but found BFloat16 after all necessary update",
    "created_at": "2024-12-03T14:30:32Z",
    "closed_at": "2024-12-10T16:00:33Z",
    "labels": [],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/71",
    "body": "Traceback (most recent call last):\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_c\r\nb)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy_extras\\nodes_custom_sampler.py\", line 633, in sample\r\n    samples = guider.sample(noise.generate_noise(latent), latent_image, sampler, sigmas, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed\r\n=noise.seed)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 740, in sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 719, in inner_sample\r\n    samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\custom_nodes\\ComfyUI_smZNodes\\smZNodes.py\", line 101, in KSAMPLER_sample\r\n    return orig_fn(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\custom_nodes\\ComfyUI-TiledDiffusion\\utils.py\", line 34, in KSAMPLER_sample\r\n    return orig_fn(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 624, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\k_diffusion\\sampling.py\", line 1058, in sample_deis\r\n    denoised = model(x_cur, t_cur * s_in, **extra_args)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 299, in __call__\r\n    out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 706, in __call__\r\n    return self.predict_noise(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 709, in predict_noise\r\n    return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_option\r\ns, seed=seed)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\custom_nodes\\ComfyUI_smZNodes\\smZNodes.py\", line 176, in sampling_function\r\n    out = orig_fn(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 279, in sampling_function\r\n    out = calc_cond_batch(model, conds, x, timestep, model_options)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\samplers.py\", line 228, in calc_cond_batch\r\n    output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\custom_nodes\\ComfyUI-Advanced-ControlNet\\adv_control\\utils.py\", line 69, in apply_model_uncond_cleanup_wrapper\r\n    return orig_apply_model(self, *args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\model_base.py\", line 144, in apply_model\r\n    model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\comfy\\ldm\\flux\\model.py\", line 159, in forward\r\n    out = self.forward_orig(img, img_ids, context, txt_ids, timestep, y, guidance, control)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\custom_nodes\\ComfyUI-PuLID-Flux-Enhanced\\pulidflux.py\", line 136, in forward_orig\r\n    img = img + node_data['weight'] * self.pulid_ca[ca_idx](node_data['embedding'], img)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\custom_nodes\\ComfyUI-PuLID-Flux-Enhanced\\encoders_flux.py\", line 53, in forward\r\n    latents = self.norm2(latents)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 202, in forward\r\n    return F.layer_norm(\r\n  File \"C:\\pinokio\\api\\comfy.git\\app\\env\\lib\\site-packages\\torch\\nn\\functional.py\", line 2576, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: expected scalar type Half but found BFloat16``",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/71/comments",
    "author": "bregsma",
    "comments": [
      {
        "user": "caixiaoxi",
        "created_at": "2024-12-04T02:26:14Z",
        "body": "t4\r\n\r\ngot prompt\r\nRequested to load FluxClipModel_\r\nLoading 1 new model\r\nloaded completely 0.0 4777.53759765625 True\r\nloaded completely 11721.902138519286 12150.458068847656 True\r\n  0%|                                                                                                                                        | 0/8 [00:00<?, ?it/s]\r\n!!! Exception during processing !!! expected scalar type Half but found BFloat16\r\nTraceback (most recent call last):\r\n  File \"/workspace/ComfyUI/execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/workspace/ComfyUI/execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/nodes.py\", line 1466, in sample\r\n    return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/nodes.py\", line 1433, in common_ksampler\r\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/sample.py\", line 43, in sample\r\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 855, in sample\r\n    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 753, in sample\r\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 740, in sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 719, in inner_sample\r\n    samples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 624, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/k_diffusion/sampling.py\", line 155, in sample_euler\r\n    denoised = model(x, sigma_hat * s_in, **extra_args)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 299, in __call__\r\n    out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 706, in __call__\r\n    return self.predict_noise(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 709, in predict_noise\r\n    return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 279, in sampling_function\r\n    out = calc_cond_batch(model, conds, x, timestep, model_options)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/samplers.py\", line 228, in calc_cond_batch\r\n    output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/model_base.py\", line 145, in apply_model\r\n    model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/comfy/ldm/flux/model.py\", line 184, in forward\r\n    out = self.forward_orig(img, img_ids, context, txt_ids, timestep, y, guidance, control, transformer_options)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-PuLID-Flux-Enhanced/pulidflux.py\", line 136, in forward_orig\r\n    img = img + node_data['weight'] * self.pulid_ca[ca_idx](node_data['embedding'], img)\r\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-PuLID-Flux-Enhanced/encoders_flux.py\", line 53, in forward\r\n    latents = self.norm2(latents)\r\n              ^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/modules/normalization.py\", line 217, in forward\r\n    return F.layer_norm(\r\n           ^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/comfyui_env/lib/python3.12/site-packages/torch/nn/functional.py\", line 2900, in layer_norm\r\n    return torch.layer_norm(\r\n           ^^^^^^^^^^^^^^^^^\r\nRuntimeError: expected scalar type Half but found BFloat16"
      },
      {
        "user": "zhangp365",
        "created_at": "2024-12-08T01:42:02Z",
        "body": "not this repo, \"File \"/workspace/ComfyUI/custom_nodes**/**_ComfyUI-PuLID-Flux-Enhanced*_***/encoders_flux.py\", line 53, in forward\"\r\n"
      },
      {
        "user": "Zeusaus",
        "created_at": "2024-12-08T10:08:37Z",
        "body": "!!! Exception during processing !!! expected scalar type Half but found BFloat16\r\nTraceback (most recent call last):\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\execution.py\", line 324, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\execution.py\", line 199, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\execution.py\", line 170, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\execution.py\", line 159, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy_extras\\nodes_custom_sampler.py\", line 633, in sample\r\n    samples = guider.sample(noise.generate_noise(latent), latent_image, sampler, sigmas, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=noise.seed)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 904, in sample\r\n    output = executor.execute(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\patcher_extension.py\", line 110, in execute\r\n    return self.original(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 873, in outer_sample\r\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 857, in inner_sample\r\n    samples = executor.execute(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\patcher_extension.py\", line 110, in execute\r\n    return self.original(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 714, in sample\r\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 155, in sample_euler\r\n    denoised = model(x, sigma_hat * s_in, **extra_args)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 384, in __call__\r\n    out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 839, in __call__\r\n    return self.predict_noise(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 842, in predict_noise\r\n    return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 364, in sampling_function\r\n    out = calc_cond_batch(model, conds, x, timestep, model_options)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 200, in calc_cond_batch\r\n    return executor.execute(model, conds, x_in, timestep, model_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\patcher_extension.py\", line 110, in execute\r\n    return self.original(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\samplers.py\", line 313, in _calc_cond_batch\r\n    output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\model_base.py\", line 128, in apply_model\r\n    return comfy.patcher_extension.WrapperExecutor.new_class_executor(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\patcher_extension.py\", line 110, in execute\r\n    return self.original(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\model_base.py\", line 157, in _apply_model\r\n    model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\comfy\\ldm\\flux\\model.py\", line 184, in forward\r\n    out = self.forward_orig(img, img_ids, context, txt_ids, timestep, y, guidance, control, transformer_options)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\custom_nodes\\ComfyUI-PuLID-Flux-Enhanced\\pulidflux.py\", line 136, in forward_orig\r\n    img = img + node_data['weight'] * self.pulid_ca[ca_idx](node_data['embedding'], img)\r\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\ComfyUI\\custom_nodes\\ComfyUI-PuLID-Flux-Enhanced\\encoders_flux.py\", line 53, in forward\r\n    latents = self.norm2(latents)\r\n              ^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 217, in forward\r\n    return F.layer_norm(\r\n           ^^^^^^^^^^^^^\r\n  File \"N:\\Program Files (x86)\\AI SD\\ComfyUI\\python_embeded\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2900, in layer_norm\r\n    return torch.layer_norm(\r\n           ^^^^^^^^^^^^^^^^^\r\nRuntimeError: expected scalar type Half but found BFloat16"
      },
      {
        "user": "bregsma",
        "created_at": "2024-12-10T16:00:29Z",
        "body": "found the fix: I just uninstall PulID Flux Enhance."
      },
      {
        "user": "Zeusaus",
        "created_at": "2024-12-14T07:13:52Z",
        "body": "> found the fix: I just uninstall PulID Flux Enhance.\r\n\r\nThank you. Fix worked for me."
      }
    ]
  },
  {
    "number": 63,
    "title": "无法与fill模型一起运作。",
    "created_at": "2024-11-25T04:23:35Z",
    "closed_at": "2024-12-08T15:51:06Z",
    "labels": [],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/63",
    "body": "可能fill模型的架构不一样。",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/63/comments",
    "author": "Windecay",
    "comments": [
      {
        "user": "Oo121oO",
        "created_at": "2024-12-19T06:42:11Z",
        "body": "I think so. Not compatible with Fill model. "
      },
      {
        "user": "Windecay",
        "created_at": "2024-12-19T06:45:58Z",
        "body": "> I think so. Not compatible with Fill model.\r\n\r\nI found a way to work with the fill model that requires twice the weight（above 1.5）.\r\n"
      }
    ]
  },
  {
    "number": 29,
    "title": "Apply pulid flux still applies to the model after disconnecting the link.",
    "created_at": "2024-10-12T18:01:46Z",
    "closed_at": "2024-10-21T23:40:54Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/29",
    "body": "Why did it apply patches to the checkpoint model? If the weight is 0 or the node is skipped, the face analysis module and EVA clip will still be loaded.",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/29/comments",
    "author": "Windecay",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-12T18:32:24Z",
        "body": "Hi,\r\n\r\n- Weight has nothing to do with how ComfyUI executes the node graph (loading of models, etc.). Node connections do!\r\n- This is an early Alpha version (I have a newer version in the works, with masking and ModelPatcher, but I can’t publish it because it’s not finished).\r\n- About the model patches, see the Known issues section of the readme.txt.\r\n\r\n**Patching:** The problem is that ComfyUI currently doesn’t have the right hooks for applying PuLIDAttention embeddings to the model output, so I needed to improvise. This is why I patch the underlying model directly. When I finish the new version of this node, it will hopefully patch and unpatch the model depending on the connection graph."
      },
      {
        "user": "Windecay",
        "created_at": "2024-10-13T04:36:45Z",
        "body": "Thank you very much!"
      }
    ]
  },
  {
    "number": 23,
    "title": "Loading pretrained EVA02-CLIP-L-14-336 weights (eva_clip). incompatible_keys.missing_keys",
    "created_at": "2024-10-08T23:45:39Z",
    "closed_at": "2024-10-12T19:44:25Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/23",
    "body": "Hi ! \r\nFirst I would like to thank you that much as this opportunity of a Comfy implementation you offered us is sooooo awesome !!!!!!\r\nThanks Thanks Thanks Thanks Thanks \r\n\r\nBut I have a question : \r\nOn quite every EVA02 load I get this message. Even with this \"error\" results are really quite good.\r\nIs that the result of the \"hacks of encoder section\" you called in description, or just me who encounter an error ?\r\n\r\nRequested to load FluxClipModel_\r\nLoading 1 new model\r\nloaded completely 0.0 4659.615478515625 True\r\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\r\nfind model: C:\\Evolut\\ComfyUI_windows_portable_nvidia_cu121_or_cpu\\ComfyUI_windows_portable\\ComfyUI\\models\\insightface\\models\\antelopev2\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\r\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\r\nfind model: C:\\Evolut\\ComfyUI_windows_portable_nvidia_cu121_or_cpu\\ComfyUI_windows_portable\\ComfyUI\\models\\insightface\\models\\antelopev2\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\r\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\r\nfind model: C:\\Evolut\\ComfyUI_windows_portable_nvidia_cu121_or_cpu\\ComfyUI_windows_portable\\ComfyUI\\models\\insightface\\models\\antelopev2\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\r\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\r\nfind model: C:\\Evolut\\ComfyUI_windows_portable_nvidia_cu121_or_cpu\\ComfyUI_windows_portable\\ComfyUI\\models\\insightface\\models\\antelopev2\\glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\r\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\r\nfind model: C:\\Evolut\\ComfyUI_windows_portable_nvidia_cu121_or_cpu\\ComfyUI_windows_portable\\ComfyUI\\models\\insightface\\models\\antelopev2\\scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\r\nset det-size: (640, 640)\r\nLoaded EVA02-CLIP-L-14-336 model config.\r\nShape of rope freq: torch.Size([576, 64])\r\nLoading pretrained EVA02-CLIP-L-14-336 weights (eva_clip).\r\nincompatible_keys.missing_keys: ['visual.rope.freqs_cos', 'visual.rope.freqs_sin', 'visual.blocks.0.attn.rope.freqs_cos', 'visual.blocks.0.attn.rope.freqs_sin', 'visual.blocks.1.attn.rope.freqs_cos', 'visual.blocks.1.attn.rope.freqs_sin', 'visual.blocks.2.attn.rope.freqs_cos', 'visual.blocks.2.attn.rope.freqs_sin', 'visual.blocks.3.attn.rope.freqs_cos', 'visual.blocks.3.attn.rope.freqs_sin', 'visual.blocks.4.attn.rope.freqs_cos', 'visual.blocks.4.attn.rope.freqs_sin', 'visual.blocks.5.attn.rope.freqs_cos', 'visual.blocks.5.attn.rope.freqs_sin', 'visual.blocks.6.attn.rope.freqs_cos', 'visual.blocks.6.attn.rope.freqs_sin', 'visual.blocks.7.attn.rope.freqs_cos', 'visual.blocks.7.attn.rope.freqs_sin', 'visual.blocks.8.attn.rope.freqs_cos', 'visual.blocks.8.attn.rope.freqs_sin', 'visual.blocks.9.attn.rope.freqs_cos', 'visual.blocks.9.attn.rope.freqs_sin', 'visual.blocks.10.attn.rope.freqs_cos', 'visual.blocks.10.attn.rope.freqs_sin', 'visual.blocks.11.attn.rope.freqs_cos', 'visual.blocks.11.attn.rope.freqs_sin', 'visual.blocks.12.attn.rope.freqs_cos', 'visual.blocks.12.attn.rope.freqs_sin', 'visual.blocks.13.attn.rope.freqs_cos', 'visual.blocks.13.attn.rope.freqs_sin', 'visual.blocks.14.attn.rope.freqs_cos', 'visual.blocks.14.attn.rope.freqs_sin', 'visual.blocks.15.attn.rope.freqs_cos', 'visual.blocks.15.attn.rope.freqs_sin', 'visual.blocks.16.attn.rope.freqs_cos', 'visual.blocks.16.attn.rope.freqs_sin', 'visual.blocks.17.attn.rope.freqs_cos', 'visual.blocks.17.attn.rope.freqs_sin', 'visual.blocks.18.attn.rope.freqs_cos', 'visual.blocks.18.attn.rope.freqs_sin', 'visual.blocks.19.attn.rope.freqs_cos', 'visual.blocks.19.attn.rope.freqs_sin', 'visual.blocks.20.attn.rope.freqs_cos', 'visual.blocks.20.attn.rope.freqs_sin', 'visual.blocks.21.attn.rope.freqs_cos', 'visual.blocks.21.attn.rope.freqs_sin', 'visual.blocks.22.attn.rope.freqs_cos', 'visual.blocks.22.attn.rope.freqs_sin', 'visual.blocks.23.attn.rope.freqs_cos', 'visual.blocks.23.attn.rope.freqs_sin']\r\nLoading PuLID-Flux model.",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/23/comments",
    "author": "InspectaGadget",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-12T19:44:26Z",
        "body": "Hi, you are welcome!\r\n\r\nNo, this warning message has nothing to do with my “hacks.” The incompatible_keys.missing_keys are just a warning that multiple keys from the EVA-CLIP model state dictionary were not used/loaded. This info should not normally be displayed to the user by the loader (it should be suppressed), but this is how the EVA-CLIP loader was already set up. I reused the source code. Maybe when I have time, I will look at it, but for now, it’s just a “cosmetic” issue. :)"
      },
      {
        "user": "ClothingAI",
        "created_at": "2024-10-18T17:21:30Z",
        "body": "Who ar eyou thankin @balazik ? What is the solution anyway?"
      },
      {
        "user": "balazik",
        "created_at": "2024-10-21T23:55:03Z",
        "body": "@ClothingAI Did you read the issue ? It begins with:\r\n\r\n> First I would like to thank you that much as this opportunity of a Comfy implementation you offered us is sooooo awesome !!!!!!\r\nThanks Thanks Thanks Thanks Thanks\r\n\r\nSo I replied.\r\n\r\nSolution to what ? Cometic warning messages ? Please read the whole answer. In the issue there are no exception, nor error messages, all are just warnings that are out of my control. The code that generates them is from OpenAI (eva clip loader). I could re-write it but, it it worth my time, no its not, its just cosmetic."
      }
    ]
  },
  {
    "number": 16,
    "title": "Using this custom node via ComfyScript - i am getting very different results than via ComfyUI editor",
    "created_at": "2024-10-03T15:49:30Z",
    "closed_at": "2024-10-12T19:32:08Z",
    "labels": [],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/16",
    "body": "Here is a minimal example:\r\n\r\n```\r\nfrom comfy_script.runtime.real import *\r\nload()\r\nfrom comfy_script.runtime.real.nodes import *\r\nimport comfy.model_management\r\nfrom comfy.model_management import xformers_enabled, vae_dtype, get_free_memory\r\nimport uuid\r\nimport json\r\nimport io\r\nimport numpy as np\r\nimport pdb\r\nimport os\r\nimport gc\r\nimport sys\r\nfrom torch._C import BoolType\r\nimport torchvision.transforms as T\r\nfrom diffusers.utils import load_image\r\nimport cv2\r\nimport traceback\r\nfrom math import atan2, pi, ceil\r\nimport pdb\r\nfrom PIL import Image, ImageFilter\r\nfrom typing import Optional\r\nimport os\r\nimport torch\r\n\r\ntransform = T.ToPILImage()\r\n\r\ndef run_pulid():\r\n    with torch.inference_mode():\r\n        noise = RandomNoise(52535031757376)\r\n        model = UNETLoader('flux1-dev-fp8.safetensors', 'default')\r\n        pulidflux = PulidFluxModelLoader('pulid_flux_v0.9.0.safetensors')\r\n        eva_clip = PulidFluxEvaClipLoader()\r\n        faceanalysis = PulidFluxInsightFaceLoader('CUDA')\r\n        image, _ = LoadImage('test.jpg')\r\n        model2 = ApplyPulidFlux(model, pulidflux, eva_clip, faceanalysis, image, 1, 0, 1, None)\r\n        clip = DualCLIPLoader('t5xxl_fp16.safetensors', 'clip_l.safetensors', 'flux')\r\n        clip_text_encode_positive_prompt_conditioning = CLIPTextEncode(\"a woman as a pirate sitting on a throne. Behind her is a ship.\", clip)\r\n        clip_text_encode_positive_prompt_conditioning = FluxGuidance(clip_text_encode_positive_prompt_conditioning, 3.5)\r\n        guider = BasicGuider(model2, clip_text_encode_positive_prompt_conditioning)\r\n        sampler = KSamplerSelect('euler')\r\n        denoise = 0.95\r\n        sigmas = BasicScheduler(model, 'beta', 20, denoise)\r\n        image3, _ = LoadImage('scene03.png')\r\n        vae = VAELoader('ae.safetensors')\r\n        latent = VAEEncode(image3, vae)\r\n        latent, _ = SamplerCustomAdvanced(noise, guider, sampler, sigmas, latent)\r\n        image4 = VAEDecode(latent, vae)\r\n        for idx, img in enumerate(image4):\r\n            img = transform(img.permute(2, 0, 1))\r\n            path = \"./pulid_{idx}.jpg\".format(idx=idx)\r\n            img.save(path)\r\n            print(\"Saved image as:\",path)\r\n\r\nrun_pulid()\r\n```\r\n\r\nWhen I run the nodes in ComfyUI (visually), the pipeline works and I am receiving the correct image.\r\nHowever running it via ComfyScript, I am getting a complete different picture which looks nothing alike the person. Something is very off.\r\n",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/16/comments",
    "author": "lschaupp",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-03T16:55:18Z",
        "body": "Sorry, but I will be glad when I find time to implement masking features. I can’t deal with another dependency that might mess up my local installation.\r\n\r\nFrom looking at the code, I see you are reusing the same variable/object name (clip_text_encode_positive_prompt_conditioning):\r\n```\r\nclip_text_encode_positive_prompt_conditioning = CLIPTextEncode(\"a woman as a pirate sitting on a throne. Behind her is a ship.\", clip)\r\nclip_text_encode_positive_prompt_conditioning = FluxGuidance(clip_text_encode_positive_prompt_conditioning, 3.5)\r\n```\r\nIsn’t this the last reference to the object? The CLIPTextEncode object could be destroyed in the meantime. Just guessing.\r\nMay be when I have some spare time I will look at this."
      },
      {
        "user": "lschaupp",
        "created_at": "2024-10-03T18:58:18Z",
        "body": "Thanks for the info.\r\nI have tried your recommendation and it did not have any effect.\r\nJust wanted to make you aware of the issue."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-12T19:32:08Z",
        "body": "Ok, will try this with the new version (still in works), when I have time. Closing."
      }
    ]
  },
  {
    "number": 14,
    "title": "Great work! Thank you!",
    "created_at": "2024-10-03T07:09:56Z",
    "closed_at": "2024-10-03T12:55:09Z",
    "labels": [],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/14",
    "body": null,
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/14/comments",
    "author": "denred0",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-03T12:55:09Z",
        "body": "I'm glad that you like it, its just a prototype, I still have a long way to go."
      }
    ]
  },
  {
    "number": 13,
    "title": "Question about VRAM and RAM requirements",
    "created_at": "2024-10-03T01:24:04Z",
    "closed_at": "2024-10-04T12:54:53Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/13",
    "body": " Hi, I would like to know if this implementation has high requirements for system RAM in addition to VRAM. For example, is there a specific amount of RAM needed for it to work properly? Thank you very much for your response!",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/13/comments",
    "author": "jojotaro1994",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-03T01:50:52Z",
        "body": "Hi, I never measured the RAM requirements. Its relatively hard to do, because depends on what Flux model and what workflow you use.\r\nFor example simple workflow with KSampler and flux1-dev-fp8-e4m3fn, t5xxl_fp8_e4m3fn, flux1_vae and pulid flux loaded as 16bit bfloat takes up ~11 to ~16GB RAM (the range because ComfyUI can load and unload models when generating).\r\n\r\nBut I recomend at least 32GB RAM when you want to play with ComfyUI else it will swap a lot.\r\nThere are also many argument flags that you can use to save memory."
      },
      {
        "user": "Orenji-Tangerine",
        "created_at": "2024-10-03T17:40:59Z",
        "body": "My system RAM peaks at 20GB while VRAM peaks at 15+GB during first run and it takes 80 seconds for a 960x1280 image @ 10 steps, Euler + Simple scheduler. My system build: 4060Ti 16GB + 64GB DDR4, Pytorch 2.1.2 + CU 11.8."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-04T12:54:54Z",
        "body": "No other questions, closing"
      }
    ]
  },
  {
    "number": 12,
    "title": "please support flux1-dev-bnb-nf4",
    "created_at": "2024-10-03T00:19:48Z",
    "closed_at": "2024-10-03T12:53:15Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/12",
    "body": null,
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/12/comments",
    "author": "mikebilly",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-03T01:37:05Z",
        "body": "Hi, have you tried this type of model? I only listed models that I have personally tested, but others might work as well."
      },
      {
        "user": "Minamiyama",
        "created_at": "2024-10-03T10:09:19Z",
        "body": "I tried some nf4 models, it works fine"
      },
      {
        "user": "balazik",
        "created_at": "2024-10-03T12:53:15Z",
        "body": "@Minamiyama Thanks, for testing I will add them to the supported models in README. "
      }
    ]
  },
  {
    "number": 10,
    "title": "RuntimeError: expected scalar type Half but found BFloat16",
    "created_at": "2024-10-02T08:35:17Z",
    "closed_at": "2024-10-02T12:55:30Z",
    "labels": [
      "duplicate"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/10",
    "body": "Is this about me using zluda , used your workflow from reddit and only changed the clip from cuda to cpu",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/10/comments",
    "author": "patientx",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-02T12:55:30Z",
        "body": "Duplicate of #6 "
      },
      {
        "user": "ClothingAI",
        "created_at": "2024-10-18T17:09:25Z",
        "body": "solution?"
      }
    ]
  },
  {
    "number": 7,
    "title": " only one picture is generated each time",
    "created_at": "2024-10-02T05:44:57Z",
    "closed_at": "2024-10-02T20:02:34Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/7",
    "body": "Thank you for your great work!\r\nBut would you like to ask if only one picture is generated each time? If I generate two pictures, the following message will appear?\r\n\r\n# ComfyUI Error Report\r\n## Error Details\r\n- **Node Type:** SamplerCustomAdvanced\r\n- **Exception Type:** RuntimeError\r\n- **Exception Message:** Boolean value of Tensor with more than one value is ambiguous\r\n## Stack Trace\r\n```\r\n File \"C:\\Users\\ntzon\\Projects\\ComfyUI\\execution.py\", line 323, in execute\r\n output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^\r\n..........\r\n.............\r\n## Additional Context\r\n(Please add any additional context or steps to reproduce the error here)",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/7/comments",
    "author": "yangnt",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-02T12:26:25Z",
        "body": "Ah, I see by using `Repeat Latent Batch` (batch processing) I can replicate the error.\r\nI have never tested batch processing before. I'll try to edit it to make it work, but I don't promise anything. First, I have to find out what the sampler requires for batch processing."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-02T20:02:34Z",
        "body": "Funny thing is that I made some patches (not associated with this error) and now it automagically works. \r\nBut also I pulled the newest ComfyUI version. So it is possible that they fixed something.\r\n\r\nPull the new version and the new ComfyUI version and it should work.\r\nI tried:\r\n- Extra options: Batch count (with randomized seed)\r\n- EmptySD3LatentImage: batch_size\r\n- Repeat Latent Batch: amount \r\n\r\nAll variantion and batch generations worked for me.  If not I will reopen the issue.\r\n\r\n"
      },
      {
        "user": "yangnt",
        "created_at": "2024-10-03T13:22:16Z",
        "body": "Thanks.👍"
      }
    ]
  },
  {
    "number": 6,
    "title": "RuntimeError: expected scalar type Half but found BFloat16 when executing SamplerCustomAdvanced node",
    "created_at": "2024-10-02T04:49:31Z",
    "closed_at": "2024-10-02T18:34:44Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/issues/6",
    "body": "ComfyUI_windows\\python_embeded\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2573, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: expected scalar type Half but found BFloat16",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/6/comments",
    "author": "wip163",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-02T12:11:45Z",
        "body": "Hi.  \r\n1. which FLUX.1-dev model did you use ? I mean the dtype (like F8_e4m3fn or F8_e5m2, checkpoint with backed F8 model etc.) If you don't know just paste here the link where you downloaded it. \r\n2. I need to know the HW (exect graphic card name) that you using (if cloud, then what type of pod you use)."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-02T18:37:19Z",
        "body": "Try to pull the new version (of ComfyUI-PuLID-Flux). It should check for bfloat16 support and set the dtype to float16 if not supported. And let me know if it helped."
      },
      {
        "user": "patientx",
        "created_at": "2024-10-02T20:08:12Z",
        "body": "RuntimeError: expected scalar type Half but found BFloat16\r\n\r\nthis is after the update, I am using weight type fp8_e4m3fn, with fp8 t5 and normal clip. I am also using a rx 6600 with zluda on windows."
      },
      {
        "user": "balazik",
        "created_at": "2024-10-02T21:03:50Z",
        "body": "@patientx sorry, not waiting for you to confirm. Ok, changed the code so it detects what dtype will the original unet model use (manual cast) so If the model can run on the system also the pulid should. It should detect your special case running on ZLUDA. \r\n\r\nLet me know if it helped. (here: so I can better understand the needs for all platforms)."
      },
      {
        "user": "wip163",
        "created_at": "2024-10-03T01:53:29Z",
        "body": "I updated to your new version's nodes and it works now, thank you!"
      },
      {
        "user": "patientx",
        "created_at": "2024-10-03T19:48:21Z",
        "body": "> @patientx sorry, not waiting for you to confirm. Ok, changed the code so it detects what dtype will the original unet model use (manual cast) so If the model can run on the system also the pulid should. It should detect your special case running on ZLUDA.\r\n> \r\n> Let me know if it helped. (here: so I can better understand the needs for all platforms).\r\n\r\nNo problem, I can also confirm it now works with the latest version with amd + zluda combo. (mentioned it again because sometimes this causes specific problems). Thanks for the hard work !"
      },
      {
        "user": "hawkonetang",
        "created_at": "2024-10-18T14:39:05Z",
        "body": "Hello! I wonder if the graphics cards of series 20 cannot be used. The prompt is:  Custom sampler (advanced).\r\nexpected scalar type Half but found BFloat16 @balazik "
      },
      {
        "user": "ThisCodeIsMine",
        "created_at": "2024-10-26T16:19:23Z",
        "body": "I have the same error message. Is there anything you can do @balazik . I'm so close on making it work... :)\r\n```\r\n## System Information\r\n- **ComfyUI Version:** v0.2.4-6-g5281090\r\n- **Arguments:** ComfyUI\\main.py --windows-standalone-build\r\n- **OS:** nt\r\n- **Python Version:** 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\r\n- **Embedded Python:** true\r\n- **PyTorch Version:** 2.3.1+cu121\r\n## Devices\r\n\r\n- **Name:** cuda:0 NVIDIA GeForce RTX 2080 Ti : cudaMallocAsync\r\n  - **Type:** cuda\r\n  - **VRAM Total:** 11810832384\r\n  - **VRAM Free:** 1715076544\r\n  - **Torch VRAM Total:** 8589934592\r\n  - **Torch VRAM Free:** 14015936\r\n  \r\n  2024-10-26 17:58:12,152 - root - INFO - got prompt\r\n2024-10-26 17:58:12,194 - root - INFO - Using pytorch attention in VAE\r\n2024-10-26 17:58:12,195 - root - INFO - Using pytorch attention in VAE\r\n2024-10-26 17:58:12,742 - root - INFO - model weight dtype torch.bfloat16, manual cast: torch.float16\r\n2024-10-26 17:58:12,742 - root - INFO - model_type FLUX\r\n2024-10-26 17:58:17,249 - root - INFO - Requested to load FluxClipModel_\r\n2024-10-26 17:58:17,249 - root - INFO - Loading 1 new model\r\n2024-10-26 17:58:21,688 - root - INFO - loaded completely 0.0 3962.80810546875 True\r\n2024-10-26 17:58:24,062 - root - INFO - Loaded EVA02-CLIP-L-14-336 model config.\r\n2024-10-26 17:58:24,080 - root - INFO - Shape of rope freq: torch.Size([576, 64])\r\n2024-10-26 17:58:28,668 - root - INFO - Loading pretrained EVA02-CLIP-L-14-336 weights (eva_clip).\r\n2024-10-26 17:58:29,444 - root - INFO - incompatible_keys.missing_keys: ['visual.rope.freqs_cos', 'visual.rope.freqs_sin', 'visual.blocks.0.attn.rope.freqs_cos', 'visual.blocks.0.attn.rope.freqs_sin', 'visual.blocks.1.attn.rope.freqs_cos', 'visual.blocks.1.attn.rope.freqs_sin', 'visual.blocks.2.attn.rope.freqs_cos', 'visual.blocks.2.attn.rope.freqs_sin', 'visual.blocks.3.attn.rope.freqs_cos', 'visual.blocks.3.attn.rope.freqs_sin', 'visual.blocks.4.attn.rope.freqs_cos', 'visual.blocks.4.attn.rope.freqs_sin', 'visual.blocks.5.attn.rope.freqs_cos', 'visual.blocks.5.attn.rope.freqs_sin', 'visual.blocks.6.attn.rope.freqs_cos', 'visual.blocks.6.attn.rope.freqs_sin', 'visual.blocks.7.attn.rope.freqs_cos', 'visual.blocks.7.attn.rope.freqs_sin', 'visual.blocks.8.attn.rope.freqs_cos', 'visual.blocks.8.attn.rope.freqs_sin', 'visual.blocks.9.attn.rope.freqs_cos', 'visual.blocks.9.attn.rope.freqs_sin', 'visual.blocks.10.attn.rope.freqs_cos', 'visual.blocks.10.attn.rope.freqs_sin', 'visual.blocks.11.attn.rope.freqs_cos', 'visual.blocks.11.attn.rope.freqs_sin', 'visual.blocks.12.attn.rope.freqs_cos', 'visual.blocks.12.attn.rope.freqs_sin', 'visual.blocks.13.attn.rope.freqs_cos', 'visual.blocks.13.attn.rope.freqs_sin', 'visual.blocks.14.attn.rope.freqs_cos', 'visual.blocks.14.attn.rope.freqs_sin', 'visual.blocks.15.attn.rope.freqs_cos', 'visual.blocks.15.attn.rope.freqs_sin', 'visual.blocks.16.attn.rope.freqs_cos', 'visual.blocks.16.attn.rope.freqs_sin', 'visual.blocks.17.attn.rope.freqs_cos', 'visual.blocks.17.attn.rope.freqs_sin', 'visual.blocks.18.attn.rope.freqs_cos', 'visual.blocks.18.attn.rope.freqs_sin', 'visual.blocks.19.attn.rope.freqs_cos', 'visual.blocks.19.attn.rope.freqs_sin', 'visual.blocks.20.attn.rope.freqs_cos', 'visual.blocks.20.attn.rope.freqs_sin', 'visual.blocks.21.attn.rope.freqs_cos', 'visual.blocks.21.attn.rope.freqs_sin', 'visual.blocks.22.attn.rope.freqs_cos', 'visual.blocks.22.attn.rope.freqs_sin', 'visual.blocks.23.attn.rope.freqs_cos', 'visual.blocks.23.attn.rope.freqs_sin']\r\n2024-10-26 17:58:31,844 - root - INFO - Loading PuLID-Flux model.\r\n2024-10-26 17:58:39,170 - root - INFO - Requested to load Flux\r\n2024-10-26 17:58:39,170 - root - INFO - Loading 1 new model\r\n2024-10-26 17:58:47,553 - root - INFO - loaded completely 0.0 7276.8634033203125 True\r\n2024-10-26 17:58:48,340 - root - ERROR - !!! Exception during processing !!! expected scalar type Half but found BFloat16\r\n2024-10-26 17:58:48,345 - root - ERROR - Traceback (most recent call last):\r\n  File \"C:\\Program Files\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 323, in execute"
      },
      {
        "user": "ThisCodeIsMine",
        "created_at": "2024-10-28T07:56:49Z",
        "body": "> Hello! I wonder if the graphics cards of series 20 cannot be used. The prompt is: Custom sampler (advanced). expected scalar type Half but found BFloat16 @balazik\r\nDid you already found a solution? I get the same error with all updated files. Using a NVIDIA GeForce RTX 2080 Ti.\r\nI don't know what the problem could be but I'm sure there must be only a configuration that can be adapted. "
      }
    ]
  },
  {
    "number": 2,
    "title": "Make it work for CFG > 1",
    "created_at": "2024-10-01T23:15:32Z",
    "closed_at": "2024-10-02T18:58:45Z",
    "labels": [],
    "url": "https://github.com/balazik/ComfyUI-PuLID-Flux/pull/2",
    "body": "Hello,\r\n\r\nI noticed that your node makes an error when going for CFG > 1, so I modified those 2 lines to make it work, now you can go for CFGGuider instead of BasicGuider for example.",
    "comments_url": "https://api.github.com/repos/balazik/ComfyUI-PuLID-Flux/issues/2/comments",
    "author": "BadisG",
    "comments": [
      {
        "user": "balazik",
        "created_at": "2024-10-02T18:57:00Z",
        "body": "Oops you are absolutely right. timestep is not a scalar. Thanks for the patch, merging."
      }
    ]
  }
]