[
  {
    "number": 12,
    "title": "[FEATURE]: Can I use Deepseek instead of Openai ?",
    "created_at": "2025-01-09T12:40:48Z",
    "closed_at": "2025-01-11T05:08:47Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/iamarunbrahma/vision-parse/issues/12",
    "body": "### Problem Statement\n\nI have the deepseek api , I wan to use deepseek instead of openai \n\n### Proposed Solution\n\nHow should I do ? Can I just modify the code to do it ?\n\n### Alternatives Considered\n\n_No response_\n\n### Final Checklist\n\n- [X] This feature would be useful to other users\n- [X] I'm willing to help implement this feature",
    "comments_url": "https://api.github.com/repos/iamarunbrahma/vision-parse/issues/12/comments",
    "author": "FLYmoments",
    "comments": [
      {
        "user": "iamarunbrahma",
        "created_at": "2025-01-09T13:47:31Z",
        "body": "Right now, Vision Parse supports local vision models only through Ollama. If you want to include DeepSeek API, maybe you can raise a PR. "
      },
      {
        "user": "iamarunbrahma",
        "created_at": "2025-01-24T10:26:37Z",
        "body": "Hello @FLYmoments, I have added support for deepseek API (deepseek-chat). Can you please check and try it now?"
      }
    ]
  },
  {
    "number": 3,
    "title": "Docker installation?",
    "created_at": "2024-12-23T07:17:23Z",
    "closed_at": "2024-12-24T21:07:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/iamarunbrahma/vision-parse/issues/3",
    "body": "It would be very helpful if we get to install this tool in docker using docker compose .\n\nThanks",
    "comments_url": "https://api.github.com/repos/iamarunbrahma/vision-parse/issues/3/comments",
    "author": "drmetro09",
    "comments": [
      {
        "user": "iamarunbrahma",
        "created_at": "2024-12-24T21:07:03Z",
        "body": "Added support for docker installation. (If you use local models, increase the memory limit as per model size).\r\n\r\nSet a MODEL_NAME environment variable (it's required). If you are using the OpenAI model, set OPENAI_API_KEY. If you are using the Gemini model, set GEMINI_API_KEY."
      },
      {
        "user": "drmetro09",
        "created_at": "2024-12-24T22:19:26Z",
        "body": "> Added support for docker installation. (If you use local models, increase the memory limit as per model size).\n> \n> Set a MODEL_NAME environment variable (it's required). If you are using the OpenAI model, set OPENAI_API_KEY. If you are using the Gemini model, set GEMINI_API_KEY.\n\nCan we use nvidia gpu ? "
      },
      {
        "user": "iamarunbrahma",
        "created_at": "2024-12-25T08:57:06Z",
        "body": "yes, you can. If you have GPU's, uncomment these lines in docker-compose.yml:\r\n\r\ndevices:\r\n            - driver: nvidia\r\n              count: all\r\n              capabilities: [gpu]"
      },
      {
        "user": "drmetro09",
        "created_at": "2024-12-25T10:34:20Z",
        "body": "# if i want to use llama 3.2 vision:11b where should i put the name ? Can you edit the below docker compose yml and put the name ? So i can know where to put the name\r\n/name: vision-parse\r\n\r\nservices:\r\n  vision-parse:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n      args:\r\n        - MODEL_NAME=${MODEL_NAME:?MODEL_NAME is required}\r\n    environment:\r\n      - MODEL_NAME=${MODEL_NAME:?MODEL_NAME is required}\r\n      - OPENAI_API_KEY=${OPENAI_API_KEY:-}  # Optional: For OpenAI models\r\n      - GEMINI_API_KEY=${GEMINI_API_KEY:-}  # Optional: For Gemini models\r\n    volumes:\r\n      - .:/app\r\n    working_dir: /app\r\n    tty: true\r\n    stdin_open: true\r\n    ports:\r\n      - \"11434:11434\"  # Expose Ollama port\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          memory: 16G  # Set memory limit to 16GB\r\n        reservations:\r\n          memory: 8G   # Guarantee at least 8GB\r\n          # Uncomment below lines if you have NVIDIA GPU available\r\n          # devices:\r\n          #   - driver: nvidia\r\n          #     count: all\r\n          #     capabilities: [gpu]\r\n    command: tail -f /dev/null  # Keep container running "
      },
      {
        "user": "drmetro09",
        "created_at": "2024-12-25T10:54:55Z",
        "body": "And i already have a locally hosted ollama docker container running how do i use that in this container of vision parse?"
      },
      {
        "user": "iamarunbrahma",
        "created_at": "2024-12-25T11:29:07Z",
        "body": "1. To use llama3.2-vision:11b, you don't put it directly in the docker-compose.yml. Instead, you set it as an environment variable when running docker-compose. Here's how:\r\n\r\n```bash\r\nexport MODEL_NAME=llama3.2-vision:11b\r\n\r\ndocker-compose down && docker-compose build && docker-compose up -d\r\n```\r\n\r\n2. This DockerFIle will automatically download and setup Ollama within vision-parse docker container, if you use any of the locally hosted models such as llama3.2-vision:11b. You can actually check it by first executing an interactive shell and then run `ollama list` or `ollama ps`:\r\n\r\n```bash\r\ndocker-compose exec vision-parse bash\r\n\r\nollama list\r\n```\r\n"
      },
      {
        "user": "drmetro09",
        "created_at": "2024-12-25T11:37:12Z",
        "body": "I use unraid , so its becoming difficult for me to setup this , this doesnâ€™t have gui ? ,  i tired to install but doesnt work ."
      }
    ]
  },
  {
    "number": 1,
    "title": "[FEATURE]: Speed up parsing",
    "created_at": "2024-12-17T20:21:05Z",
    "closed_at": "2024-12-21T04:30:32Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/iamarunbrahma/vision-parse/issues/1",
    "body": "### Problem Statement\n\nThe processing of each page should happen in parallel\n\n### Proposed Solution\n\nMaybe use vllm\n\n### Alternatives Considered\n\n_No response_\n\n### Final Checklist\n\n- [X] This feature would be useful to other users\n- [X] I'm willing to help implement this feature",
    "comments_url": "https://api.github.com/repos/iamarunbrahma/vision-parse/issues/1/comments",
    "author": "MohamedAliRashad",
    "comments": [
      {
        "user": "iamarunbrahma",
        "created_at": "2024-12-19T12:07:52Z",
        "body": "Hello @MohamedAliRashad ,\r\nThank you for this feature request! Parallelizing page processing is definitely a valuable suggestion that could improve performance, especially for larger PDF documents.\r\n\r\nCurrently, I am processing the pages sequentially in a loop, because most of the end-users might not have high-end GPUs when they will be using local models from Ollama. \r\n\r\nAs you have indicated willingness, would you be interested in starting with a small proof of concept? We could begin by implementing one of these approaches in a separate branch and measure the performance gains."
      }
    ]
  }
]