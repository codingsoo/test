[
  {
    "number": 66,
    "title": "Can input the video stream for detection and save the results. 能否直接输入视频流呢",
    "created_at": "2025-02-13T07:57:50Z",
    "closed_at": "2025-02-17T16:33:47Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/66",
    "body": "and save for yolo format。。",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/66/comments",
    "author": "kuaiqushangzixiba",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2025-02-13T08:26:06Z",
        "body": "这个repo没有支持这个功能，可以考虑将视频抽帧然后再输入进行检测，检测完之后再合并成一个视频。"
      },
      {
        "user": "wanghao9610",
        "created_at": "2025-02-13T08:26:44Z",
        "body": "> and save for yolo format。。\n\n这个功能应该比较简单，可以考虑自己添加一下"
      }
    ]
  },
  {
    "number": 65,
    "title": "After fine-tuning, zero-shot ability disappear",
    "created_at": "2025-02-07T03:02:04Z",
    "closed_at": "2025-02-17T16:33:36Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/65",
    "body": "Thank you for the author's excellent work. When I was fine-tuning my self built dataset, we found the AP of trained classes increased, but AP of the novel classes decreased.\nThe dataset contains 14 classes for fine-tuning and 7 for evaluating zero-shot performance. Is the number of classes too low?\n\nWe look forward to your answer, please help!",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/65/comments",
    "author": "Dreamdusty",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2025-02-07T03:10:30Z",
        "body": "Hi @Dreamdusty, please review issue #58 and let me know if you encounter any other problems."
      }
    ]
  },
  {
    "number": 63,
    "title": "When fine-tuning one's own dataset, the distribution of data printed on the console is incorrect",
    "created_at": "2025-01-21T08:29:22Z",
    "closed_at": "2025-02-17T16:33:59Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/63",
    "body": "Thank you for the author's excellent work. When I was fine-tuning my self built dataset, the data distribution content printed on the console did not match the data distribution in my dataset, but the total number of instances was correct. I have carefully checked and my dataset annotation content is completely fine. Do I need to pay attention to it?\n\n\n`[01/21 16:19:47 d2.data.build]: Distribution of instances among all 24 categories:\n|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n|   Bollworm    | 37535        | Meadow borer  | 28962        | Gryllotalpa.. | 16780        |\n| Agriotes fu.. | 10334        | Nematode tr.. | 5773         | Athetis lep.. | 2078         |\n| Little Gecko  | 765          | Scotogramma.. | 203          |   Armyworm    | 37           |\n| Spodoptera .. | 5            | Anomala cor.. | 0            | Spodoptera .. | 0            |\n| Plutella xy.. | 0            | holotrichia.. | 0            | Rice planth.. | 0            |\n|  Land tiger   | 0            | Yellow tiger  | 0            | eight-chara.. | 0            |\n`",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/63/comments",
    "author": "chycxyzd",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2025-02-03T12:27:34Z",
        "body": "Hi, @chycxyzd You may need to: 1) First, check your custom dataset to ensure it matches the COCO annotation format. 2) Then, verify that the category_id in the annotations is correct."
      }
    ]
  },
  {
    "number": 58,
    "title": "模型微调后成了闭集检测吗",
    "created_at": "2025-01-08T08:17:30Z",
    "closed_at": "2025-01-20T07:10:57Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/58",
    "body": "我用了5个类别的烟火数据微调了模型，模型只能检测这5个类别，我想测下模型开放性，发现像人车这样的类别都检测不出来了，是不是微调后变成了闭集。",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/58/comments",
    "author": "wl654655902",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2025-01-08T14:46:53Z",
        "body": "hello @wl654655902  ，你出现这个问题，可能是因为：全参数微调导致模型知识丢失太多。你可以考虑：1）部分参数微调，比如只微调detector部分，固定住text encoder的参数；2）用更小的学习率进行微调，调整一些超参数。"
      },
      {
        "user": "wl654655902",
        "created_at": "2025-01-09T02:29:28Z",
        "body": "我看ft和pretrain都需要设置model.num_class,训练时才不会报错，这个num_class是微调的数据的类别吗，比如我烟火5类数据，这个num_class=5,如果只是部分参数微调或使用小的学习率等方法，后面测试除烟火其他类，如人车，是可以检测的出来的吗"
      },
      {
        "user": "wanghao9610",
        "created_at": "2025-01-09T02:46:36Z",
        "body": "微调是会损失一些通用性的，你可以先试一下。"
      },
      {
        "user": "wl654655902",
        "created_at": "2025-01-09T03:02:35Z",
        "body": "模型对烟火的检测效果不好，我需要模型能够保持开放性的同时，也能检测烟火，那么我是只用烟火的数据微调模型（model.num_class=5），还是用o365，glodg数据加入烟火数据(model.num_class=365+5)，在全量的数据上微调模型呢"
      },
      {
        "user": "wanghao9610",
        "created_at": "2025-01-09T03:04:06Z",
        "body": "在大数据中，加入一些你的特定数据，这样效果会更好"
      },
      {
        "user": "wl654655902",
        "created_at": "2025-01-09T03:21:28Z",
        "body": "你们的模型是用的什么卡训练的呢，多少张卡，在o365，goldg，cc1m上训了多久，不同的数据用了unified prompt,是不是就不用考虑伪标签了"
      },
      {
        "user": "wanghao9610",
        "created_at": "2025-01-09T03:33:30Z",
        "body": "ovdino-t 用了32张a100-80g训了一周，unified prompt是为了联合图文数据提出的，您如果有图文数据需要加的话，可以考虑采用这种方式不用打伪标签。"
      }
    ]
  },
  {
    "number": 56,
    "title": "can't use the sam2.1",
    "created_at": "2024-12-27T09:49:13Z",
    "closed_at": "2025-01-20T07:12:09Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/56",
    "body": "hello, i install sam2,config the yaml and pt file, the cmd show the \"Building SAM2 model: /ovseg/OV-DINO-main/inits/sam2-main/sam2/configs/sam2.1/sam2.1_hiera_l.yaml\", then begin run,but the sam2 did not work",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/56/comments",
    "author": "1523900304",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-12-30T03:25:48Z",
        "body": "@1523900304 You did't give any error info, I could't help you, please provide more information."
      }
    ]
  },
  {
    "number": 52,
    "title": "SystemError",
    "created_at": "2024-12-03T06:08:24Z",
    "closed_at": "2024-12-30T03:26:19Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/52",
    "body": "(ovd) [zp@localhost ovdino]$ bash scripts/demo.sh /gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/projects/ovdino/configs/ovdino_swin_tiny224_bert_base_infer_demo.py /gis/zhoup/project/open-vocabulary/OV-DINO/weights/ovdino_swint_ogc-coco50.2_lvismv40.1_lvis32.9.pth \"a_tile a_cup\" /gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/demo/imgs/000000017714.jpg /gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/output/img0_vis.jpg\r\n\r\n/gis/conda/ovd/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\r\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\r\nSAM2 is not installed.\r\n[12/03 14:01:19 detectron2]: Arguments: Namespace(config_file='/gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/projects/ovdino/configs/ovdino_swin_tiny224_bert_base_infer_demo.py', sam_config_file='sam2_hiera_l.yaml', sam_init_checkpoint='/gis/zhoup/project/open-vocabulary/OV-DINO/inits/sam2/sam2_hiera_large.pt', webcam=False, video_input=None, input=['/gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/demo/imgs/000000017714.jpg'], category_names=['a_tile', 'a_cup'], output='/gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/output/img0_vis.jpg', min_size_test=800, max_size_test=1333, img_format='RGB', metadata_dataset='coco_2017_val', confidence_threshold=0.5, opts=['train.init_checkpoint=/gis/zhoup/project/open-vocabulary/OV-DINO/weights/ovdino_swint_ogc-coco50.2_lvismv40.1_lvis32.9.pth', 'model.num_classes=2'])\r\n\r\n/gis/conda/ovd/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\r\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\r\n[12/03 14:01:23 fvcore.common.checkpoint]: [Checkpointer] Loading from /gis/zhoup/project/open-vocabulary/OV-DINO/weights/ovdino_swint_ogc-coco50.2_lvismv40.1_lvis32.9.pth ...\r\nWARNING [12/03 14:01:23 d2.checkpoint.c2_model_loading]: Shape of label_enc.weight in checkpoint is torch.Size([150, 256]), while shape of label_enc.weight in model is torch.Size([2, 256]).\r\nWARNING [12/03 14:01:23 d2.checkpoint.c2_model_loading]: label_enc.weight will not be loaded. Please double check and see if this is desired.\r\n[12/03 14:01:23 d2.checkpoint.c2_model_loading]: Following weights matched with submodule model - Total num: 226\r\nWARNING [12/03 14:01:24 fvcore.common.checkpoint]: Skip loading parameter 'label_enc.weight' to the model due to incompatible shapes: (150, 256) in the checkpoint but (2, 256) in the model! You might want to double check if this is expected.\r\nWARNING [12/03 14:01:24 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:\r\nlabel_enc.weight\r\ncategory_names------>:  ['a tile', 'a cup']\r\n[12/03 14:01:24 detectron2]: /gis/zhoup/project/open-vocabulary/OV-DINO/ovdino/demo/imgs/000000017714.jpg: detected 10 instances in 0.82s\r\n\r\nException ignored in tp_clear of: <class 'sentry_sdk.profiler.continuous_profiler.ThreadContinuousScheduler'>\r\nTypeError: object.__init__() takes exactly one argument (the instance to initialize)\r\nException ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7f0d282a5e50>\r\nTraceback (most recent call last):\r\n  File \"/gis/conda/ovd/lib/python3.9/weakref.py\", line 371, in remove\r\nSystemError: <weakref at 0x7f0d282ac860; to 'WeakKeyDictionary' at 0x7f0d282ae580> returned a result with an error set\r\nscripts/demo.sh: line 38: 77555 Segmentation fault      (core dumped) PYTHONPATH=\"$(dirname $0)\":$PYTHONPATH python ./demo/demo.py --config-file $config_file --sam-config-file $sam_config_file --sam-init-checkpoint $sam_init_checkpoint --input $input --output $output --category_names $category_names --opts train.init_checkpoint=$init_ckpt model.num_classes=$num_classes",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/52/comments",
    "author": "the-cat-crying",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-12-03T12:01:27Z",
        "body": "@the-cat-crying This error may be you machine error, you should check your machine envrioment. Output \"detected 10 instances in 0.82s\" denoted the inference have beed done succesfully."
      },
      {
        "user": "the-cat-crying",
        "created_at": "2024-12-04T02:07:35Z",
        "body": "@wanghao9610 Thanks for the answer, I'm checking my environment.so,when will ONNX be supported?"
      }
    ]
  },
  {
    "number": 50,
    "title": "Online demo can‘t be reached",
    "created_at": "2024-10-30T13:26:45Z",
    "closed_at": "2024-11-09T10:36:57Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/50",
    "body": "Thanks for your superior works, i noticed that the online demo can't be reached. Have you changed the websites?",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/50/comments",
    "author": "FantasticZihao",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-10-31T01:45:57Z",
        "body": "@FantasticZihao It is available now. This online web demo will not be supported in the future. If you want to test it, you may need to deploy it yourself. I have provided the guidelines for the online web demo. Thanks for your attention."
      }
    ]
  },
  {
    "number": 47,
    "title": "CUDA out of memory：The Minimum requirements for fine-tuning",
    "created_at": "2024-09-26T05:48:28Z",
    "closed_at": "2024-09-26T11:44:55Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/47",
    "body": "I encountered a CUDA out of memory issue while fine-tuning according to the tutorial. I have two 4090 GPUs. After this issue occurred, I modified the total_batch_size in ovdino/configs/common/data/custom_ovd.py from 16 to 2, but I still faced the CUDA out of memory problem. My dataset consists of RGB images with a size of 512x512x3.\r\n\r\nCould you please let me know if the requirements for the GPU are too high or if there's a problem with my configuration? If you need more information, please let me know.\r\n\r\nThank you very much for your work!",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/47/comments",
    "author": "hyhcompetition",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-09-26T06:32:11Z",
        "body": "Please refer to this issue #21 , try to freeze the text_encoder.\r\n```python\r\nmodel.language_backbone.is_freeze=True\r\n```"
      },
      {
        "user": "hyhcompetition",
        "created_at": "2024-09-26T11:39:17Z",
        "body": "**Thank you very much for your prompt reply, but I'm still encountering this issue after making the changes.**\r\n\r\n**This is the output log:**\r\nDistributed Fine-Tuning with ovdino_swin_tiny224_bert_base_ft_custom_24ep\r\ndist_args: --nproc_per_node=2\r\n[09/26 19:32:19 detectron2]: Rank of current process: 0. World size: 2\r\n/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorShape.cpp:3190.)\r\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\r\n[09/26 19:32:19 detectron2]: Environment info:\r\n----------------------  ---------------------------------------------------------------------------\r\nsys.platform            linux\r\nPython                  3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nnumpy                   1.26.4\r\ndetectron2              0.6 @/home/ps/Documents/Project/OV-DINO/ovdino/detectron2-717ab9/detectron2\r\nCompiler                GCC 9.4\r\nCUDA compiler           CUDA 11.8\r\ndetectron2 arch flags   8.6\r\nDETECTRON2_ENV_MODULE   <not set>\r\nPyTorch                 1.13.1 @/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch\r\nPyTorch debug build     False\r\nGPU available           Yes\r\nGPU 0,1                 NVIDIA GeForce RTX 4090 (arch=8.9)\r\nDriver version          550.78\r\nCUDA_HOME               /usr/local/cuda\r\nPillow                  9.2.0\r\ntorchvision             0.14.1 @/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torchvision\r\ntorchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6\r\nfvcore                  0.1.5.post20221221\r\niopath                  0.1.9\r\ncv2                     4.10.0\r\n----------------------  ---------------------------------------------------------------------------\r\nPyTorch built with:\r\n  - GCC 9.3\r\n  - C++ Version: 201402\r\n  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\r\n  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\r\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\r\n  - LAPACK is enabled (usually provided by MKL)\r\n  - NNPACK is enabled\r\n  - CPU capability usage: AVX2\r\n  - CUDA Runtime 11.7\r\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\r\n  - CuDNN 8.5\r\n  - Magma 2.6.1\r\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \r\n\r\n[09/26 19:32:19 detectron2]: Command line arguments: Namespace(config_file='projects/ovdino/configs/ovdino_swin_tiny224_bert_base_ft_custom_24ep.py', resume=True, eval_only=False, num_gpus=2, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['train.init_checkpoint=/data/Weights/ovdino/ovdino_swint_ogc-coco50.2_lvismv40.1_lvis32.9.pth', 'train.output_dir=/home/ps/Documents/Project/OV-DINO/wkdrs/ovdino_swin_tiny224_bert_base_ft_custom_24ep', 'dataloader.evaluator.output_dir=/home/ps/Documents/Project/OV-DINO/wkdrs/ovdino_swin_tiny224_bert_base_ft_custom_24ep/eval_coco_20240926-193212'])\r\n[09/26 19:32:20 detectron2]: Contents of args.config_file=projects/ovdino/configs/ovdino_swin_tiny224_bert_base_ft_custom_24ep.py:\r\nimport os\r\nimport os.path as osp\r\n\r\nfrom detrex.config import get_config\r\n\r\nfrom .models.ovdino_swin_tiny224_bert_base import model\r\n\r\nmodel_root = os.getenv(\"MODEL_ROOT\", \"./inits\")\r\ninit_checkpoint = osp.join(model_root, \"./swin\", \"swin_tiny_patch4_window7_224.pth\")\r\n\r\n# get default config\r\ndataloader = get_config(\"common/data/custom_ovd.py\").dataloader\r\noptimizer = get_config(\"common/optim.py\").AdamW\r\nlr_multiplier = get_config(\r\n    \"common/coco_schedule.py\"\r\n).lr_multiplier_120k_bs32_24ep_two_steps_warmup\r\ntrain = get_config(\"common/train.py\").train\r\n\r\n# modify training config\r\ntrain.init_checkpoint = init_checkpoint\r\ntrain.output_dir = \"./wkdrs/ovdino_swin_tiny224_bert_base_24ep_ft_coco\"\r\n\r\n# max training iterations, 120000 / 32 * 24 = 90000 -> 90000\r\ntrain.max_iter = 90000\r\ntrain.eval_period = 5000\r\ntrain.log_period = 50\r\ntrain.checkpointer.period = 5000\r\n\r\n# gradient clipping for training\r\ntrain.clip_grad.enabled = True\r\ntrain.clip_grad.params.max_norm = 0.1\r\ntrain.clip_grad.params.norm_type = 2\r\n\r\n# set training devices\r\ntrain.device = \"cuda\"\r\nmodel.device = train.device\r\nmodel.num_classes = 8\r\nmodel.language_backbone.is_freeze=True\r\n# amp\r\ntrain.amp.enabled = True\r\n\r\n# find_unused_parameters\r\n# train.ddp.find_unused_parameters = True\r\n\r\n# modify optimizer config\r\noptimizer.lr = 1e-5\r\noptimizer.betas = (0.9, 0.999)\r\noptimizer.weight_decay = 1e-4\r\noptimizer.params.lr_factor_func = lambda module_name: (\r\n    0.1 if \"language_backbone\" in module_name else 1.0\r\n)\r\n\r\n# modify dataloader config\r\ndataloader.train.num_workers = 8\r\n\r\n# please notice that this is total batch size.\r\n# surpose you're using 4 gpus for training and the batch size for\r\n# each gpu is 64/4 = 4\r\ndataloader.train.total_batch_size = 32\r\n\r\n# dump the testing results into output_dir for visualization\r\ndataloader.evaluator.output_dir = train.output_dir\r\n\r\nWARNING [09/26 19:32:20 d2.config.lazy]: The config contains objects that cannot serialize to a valid yaml. /home/ps/Documents/Project/OV-DINO/wkdrs/ovdino_swin_tiny224_bert_base_ft_custom_24ep/config.yaml is human-readable but cannot be loaded.\r\nWARNING [09/26 19:32:20 d2.config.lazy]: Config is saved using cloudpickle at /home/ps/Documents/Project/OV-DINO/wkdrs/ovdino_swin_tiny224_bert_base_ft_custom_24ep/config.yaml.pkl.\r\n[09/26 19:32:20 detectron2]: Full config saved to /home/ps/Documents/Project/OV-DINO/wkdrs/ovdino_swin_tiny224_bert_base_ft_custom_24ep/config.yaml\r\n[09/26 19:32:20 d2.utils.env]: Using a generated random seed 20519604\r\n/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorShape.cpp:3190.)\r\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\r\n\r\n**the model info**:\r\n[09/26 19:32:21 detectron2]: Model:\r\nOVDINO(\r\n  (backbone): SwinTransformer(\r\n    (patch_embed): PatchEmbed(\r\n      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\r\n      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\r\n    )\r\n    (pos_drop): Dropout(p=0.0, inplace=False)\r\n    (layers): ModuleList(\r\n      (0): BasicLayer(\r\n        (blocks): ModuleList(\r\n          (0): SwinTransformerBlock(\r\n            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=96, out_features=288, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=96, out_features=96, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): Identity()\r\n            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=96, out_features=384, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=384, out_features=96, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (1): SwinTransformerBlock(\r\n            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=96, out_features=288, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=96, out_features=96, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.009)\r\n            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=96, out_features=384, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=384, out_features=96, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n        )\r\n        (downsample): PatchMerging(\r\n          (reduction): Linear(in_features=384, out_features=192, bias=False)\r\n          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n        )\r\n      )\r\n      (1): BasicLayer(\r\n        (blocks): ModuleList(\r\n          (0): SwinTransformerBlock(\r\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=192, out_features=192, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.018)\r\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (1): SwinTransformerBlock(\r\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=192, out_features=192, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.027)\r\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n        )\r\n        (downsample): PatchMerging(\r\n          (reduction): Linear(in_features=768, out_features=384, bias=False)\r\n          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        )\r\n      )\r\n      (2): BasicLayer(\r\n        (blocks): ModuleList(\r\n          (0): SwinTransformerBlock(\r\n            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=384, out_features=1152, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=384, out_features=384, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.036)\r\n            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (1): SwinTransformerBlock(\r\n            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=384, out_features=1152, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=384, out_features=384, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.045)\r\n            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (2): SwinTransformerBlock(\r\n            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=384, out_features=1152, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=384, out_features=384, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.055)\r\n            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (3): SwinTransformerBlock(\r\n            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=384, out_features=1152, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=384, out_features=384, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.064)\r\n            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (4): SwinTransformerBlock(\r\n            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=384, out_features=1152, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=384, out_features=384, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.073)\r\n            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (5): SwinTransformerBlock(\r\n            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=384, out_features=1152, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=384, out_features=384, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.082)\r\n            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n        )\r\n        (downsample): PatchMerging(\r\n          (reduction): Linear(in_features=1536, out_features=768, bias=False)\r\n          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\r\n        )\r\n      )\r\n      (3): BasicLayer(\r\n        (blocks): ModuleList(\r\n          (0): SwinTransformerBlock(\r\n            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=768, out_features=768, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.091)\r\n            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (1): SwinTransformerBlock(\r\n            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n            (attn): WindowAttention(\r\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\r\n              (attn_drop): Dropout(p=0.0, inplace=False)\r\n              (proj): Linear(in_features=768, out_features=768, bias=True)\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n              (softmax): Softmax(dim=-1)\r\n            )\r\n            (drop_path): DropPath(drop_prob=0.100)\r\n            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n            (mlp): Mlp(\r\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\r\n              (act): GELU(approximate='none')\r\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\r\n              (drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n        )\r\n      )\r\n    )\r\n    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\r\n    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\r\n    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (language_backbone): BERTEncoder(\r\n    (lang_model): BertModel(\r\n      (embeddings): BertEmbeddings(\r\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\r\n        (position_embeddings): Embedding(512, 768)\r\n        (token_type_embeddings): Embedding(2, 768)\r\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (encoder): BertEncoder(\r\n        (layer): ModuleList(\r\n          (0): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (1): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (2): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (3): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (4): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (5): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (6): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (7): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (8): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (9): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (10): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n          (11): BertLayer(\r\n            (attention): BertAttention(\r\n              (self): BertSelfAttention(\r\n                (query): Linear(in_features=768, out_features=768, bias=True)\r\n                (key): Linear(in_features=768, out_features=768, bias=True)\r\n                (value): Linear(in_features=768, out_features=768, bias=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n              (output): BertSelfOutput(\r\n                (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (intermediate): BertIntermediate(\r\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              (intermediate_act_fn): GELUActivation()\r\n            )\r\n            (output): BertOutput(\r\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r\n              (dropout): Dropout(p=0.1, inplace=False)\r\n            )\r\n          )\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (position_embedding): PositionEmbeddingSine()\r\n  (neck): ChannelMapper(\r\n    (convs): ModuleList(\r\n      (0): ConvNormAct(\r\n        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\r\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\r\n      )\r\n      (1): ConvNormAct(\r\n        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\r\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\r\n      )\r\n      (2): ConvNormAct(\r\n        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\r\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\r\n      )\r\n    )\r\n    (extra_convs): ModuleList(\r\n      (0): ConvNormAct(\r\n        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\r\n        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\r\n      )\r\n    )\r\n  )\r\n  (transformer): DINOTransformer(\r\n    (encoder): DINOTransformerEncoder(\r\n      (layers): ModuleList(\r\n        (0): BaseTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (1): BaseTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (2): BaseTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (3): BaseTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (4): BaseTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (5): BaseTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n      )\r\n    )\r\n    (decoder): DINOTransformerDecoder(\r\n      (layers): ModuleList(\r\n        (0): GatedTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n            (1): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n            (2): GatedMultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): GatedFFN(\r\n              (activation): ReLU(inplace=True)\r\n              (ffn): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n            (1): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (1): GatedTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n            (1): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n            (2): GatedMultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): GatedFFN(\r\n              (activation): ReLU(inplace=True)\r\n              (ffn): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n            (1): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (2): GatedTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n            (1): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n            (2): GatedMultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): GatedFFN(\r\n              (activation): ReLU(inplace=True)\r\n              (ffn): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n            (1): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (3): GatedTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n            (1): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n            (2): GatedMultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): GatedFFN(\r\n              (activation): ReLU(inplace=True)\r\n              (ffn): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n            (1): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (4): GatedTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n            (1): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n            (2): GatedMultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): GatedFFN(\r\n              (activation): ReLU(inplace=True)\r\n              (ffn): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n            (1): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n        (5): GatedTransformerLayer(\r\n          (attentions): ModuleList(\r\n            (0): MultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n            (1): MultiScaleDeformableAttention(\r\n              (dropout): Dropout(p=0.0, inplace=False)\r\n              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\r\n              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\r\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\r\n              (output_proj): Linear(in_features=256, out_features=256, bias=True)\r\n            )\r\n            (2): GatedMultiheadAttention(\r\n              (attn): MultiheadAttention(\r\n                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\r\n              )\r\n              (proj_drop): Dropout(p=0.0, inplace=False)\r\n            )\r\n          )\r\n          (ffns): ModuleList(\r\n            (0): GatedFFN(\r\n              (activation): ReLU(inplace=True)\r\n              (ffn): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n            (1): FFN(\r\n              (activation): ReLU(inplace=True)\r\n              (layers): Sequential(\r\n                (0): Sequential(\r\n                  (0): Linear(in_features=256, out_features=2048, bias=True)\r\n                  (1): ReLU(inplace=True)\r\n                  (2): Dropout(p=0.0, inplace=False)\r\n                )\r\n                (1): Linear(in_features=2048, out_features=256, bias=True)\r\n                (2): Dropout(p=0.0, inplace=False)\r\n              )\r\n            )\r\n          )\r\n          (norms): ModuleList(\r\n            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n            (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n          )\r\n        )\r\n      )\r\n      (ref_point_head): MLP(\r\n        (layers): ModuleList(\r\n          (0): Linear(in_features=512, out_features=256, bias=True)\r\n          (1): Linear(in_features=256, out_features=256, bias=True)\r\n        )\r\n      )\r\n      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n      (class_embed): ModuleList(\r\n        (0): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n        (1): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n        (2): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n        (3): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n        (4): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n        (5): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n        (6): ClassEmbed(\r\n          (image_embed_proj): Identity()\r\n          (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n          (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n          (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n          (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n        )\r\n      )\r\n      (bbox_embed): ModuleList(\r\n        (0): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n        (1): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n        (2): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n        (3): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n        (4): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n        (5): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n        (6): MLP(\r\n          (layers): ModuleList(\r\n            (0): Linear(in_features=256, out_features=256, bias=True)\r\n            (1): Linear(in_features=256, out_features=256, bias=True)\r\n            (2): Linear(in_features=256, out_features=4, bias=True)\r\n          )\r\n        )\r\n      )\r\n    )\r\n    (tgt_embed): Embedding(900, 256)\r\n    (enc_output): Linear(in_features=256, out_features=256, bias=True)\r\n    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (class_embed): ModuleList(\r\n    (0): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n    (1): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n    (2): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n    (3): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n    (4): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n    (5): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n    (6): ClassEmbed(\r\n      (image_embed_proj): Identity()\r\n      (lang_embed_proj): Linear(in_features=768, out_features=256, bias=True)\r\n      (lang_bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 768])\r\n      (lang_bias0): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n      (lang_log_scale): ParameterList(  (0): Parameter containing: [torch.float32 of size 1])\r\n    )\r\n  )\r\n  (bbox_embed): ModuleList(\r\n    (0): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n    (1): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n    (2): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n    (3): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n    (4): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n    (5): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n    (6): MLP(\r\n      (layers): ModuleList(\r\n        (0): Linear(in_features=256, out_features=256, bias=True)\r\n        (1): Linear(in_features=256, out_features=256, bias=True)\r\n        (2): Linear(in_features=256, out_features=4, bias=True)\r\n      )\r\n    )\r\n  )\r\n  (criterion): Criterion DINOCriterion\r\n      matcher: Matcher HungarianMatcher\r\n          cost_class: 2.0\r\n          cost_bbox: 5.0\r\n          cost_giou: 2.0\r\n          cost_class_type: focal_loss_cost\r\n          focal cost alpha: 0.25\r\n          focal cost gamma: 2.0\r\n      losses: ['class', 'boxes']\r\n      loss_class_type: focal_loss\r\n      weight_dict: {'loss_class': 1, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_class_dn': 1, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_class_enc': 1, 'loss_bbox_enc': 5.0, 'loss_giou_enc': 2.0, 'loss_class_dn_enc': 1, 'loss_bbox_dn_enc': 5.0, 'loss_giou_dn_enc': 2.0, 'loss_class_bcls_enc': 1, 'loss_bbox_bcls_enc': 5.0, 'loss_giou_bcls_enc': 2.0, 'loss_class_dn_bcls_enc': 1, 'loss_bbox_dn_bcls_enc': 5.0, 'loss_giou_dn_bcls_enc': 2.0, 'loss_class_0': 1, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_class_dn_0': 1, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_class_1': 1, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_class_dn_1': 1, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_class_2': 1, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_class_dn_2': 1, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_class_3': 1, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_class_dn_3': 1, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_class_4': 1, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_class_dn_4': 1, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0}\r\n      num_classes: 8\r\n      eos_coef: None\r\n      focal loss alpha: 0.25\r\n      focal loss gamma: 2.0\r\n  (label_enc): Embedding(8, 256)\r\n)\r\n\r\nCategory ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\r\n\r\nLoading sarmsi_train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 31875.16it/s]\r\nLoaded 41 data points from sarmsi_train, template: identity\r\nSample: ['bus', 'civilian', 'sedan', 'construction machinery', 'military tank', 'military truck', 'truck', 'minivan']\r\n[09/26 19:32:22 d2.data.build]: Removed 0 images with no usable annotations. 41 images left.\r\n[09/26 19:32:22 d2.data.common]: Serializing 41 elements to byte tensors and concatenating them all ...\r\n[09/26 19:32:22 d2.data.common]: Serialized dataset takes 0.02 MiB\r\n\r\nCategory ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\r\n\r\nLoading sarmsi_train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 23984.17it/s]\r\n[09/26 19:32:22 fvcore.common.checkpoint]: [Checkpointer] Loading from /data/Weights/ovdino/ovdino_swint_ogc-coco50.2_lvismv40.1_lvis32.9.pth ...\r\nWARNING [09/26 19:32:22 d2.checkpoint.c2_model_loading]: Shape of label_enc.weight in checkpoint is torch.Size([150, 256]), while shape of label_enc.weight in model is torch.Size([8, 256]).\r\nWARNING [09/26 19:32:22 d2.checkpoint.c2_model_loading]: label_enc.weight will not be loaded. Please double check and see if this is desired.\r\n[09/26 19:32:22 d2.checkpoint.c2_model_loading]: Following weights matched with submodule model - Total num: 226\r\nWARNING [09/26 19:32:23 fvcore.common.checkpoint]: Skip loading parameter 'label_enc.weight' to the model due to incompatible shapes: (150, 256) in the checkpoint but (8, 256) in the model! You might want to double check if this is expected.\r\nWARNING [09/26 19:32:23 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:\r\nlabel_enc.weight\r\n[09/26 19:32:23 d2.engine.train_loop]: Starting training from iteration 0\r\n\r\n**the Error info**:\r\nERROR [09/26 19:32:28 d2.engine.train_loop]: Exception during training:\r\nTraceback (most recent call last):\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detectron2-717ab9/detectron2/engine/train_loop.py\", line 149, in train\r\n    self.run_step()\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/tools/train_net.py\", line 99, in run_step\r\n    loss_dict = self.model(data)\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\r\n    output = self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\r\n    return module_to_run(*inputs[0], **kwargs[0])\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/projects/ovdino/modeling/ovdino.py\", line 281, in forward\r\n    ) = self.transformer(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/projects/ovdino/modeling/dino_transformer.py\", line 803, in forward\r\n    memory = self.encoder(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/projects/ovdino/modeling/dino_transformer.py\", line 446, in forward\r\n    query = layer(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detrex/layers/transformer.py\", line 161, in forward\r\n    query = self.attentions[attn_index](\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detrex/layers/multi_scale_deform_attn.py\", line 371, in forward\r\n    output = output.to(torch.float16)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.64 GiB total capacity; 22.64 GiB already allocated; 31.31 MiB free; 23.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n[09/26 19:32:28 d2.engine.hooks]: Total training time: 0:00:04 (0:00:00 on hooks)\r\n[09/26 19:32:28 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 23179M\r\nTraceback (most recent call last):\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/./tools/train_net.py\", line 329, in <module>\r\n    launch(\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detectron2-717ab9/detectron2/engine/launch.py\", line 67, in launch\r\n    mp.spawn(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\r\n    while not context.join():\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 160, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detectron2-717ab9/detectron2/engine/launch.py\", line 126, in _distributed_worker\r\n    main_func(*args)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/tools/train_net.py\", line 320, in main\r\n    do_train(args, cfg)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/tools/train_net.py\", line 288, in do_train\r\n    trainer.train(start_iter, cfg.train.max_iter)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detectron2-717ab9/detectron2/engine/train_loop.py\", line 149, in train\r\n    self.run_step()\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/tools/train_net.py\", line 99, in run_step\r\n    loss_dict = self.model(data)\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\r\n    output = self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\r\n    return module_to_run(*inputs[0], **kwargs[0])\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/projects/ovdino/modeling/ovdino.py\", line 281, in forward\r\n    ) = self.transformer(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/projects/ovdino/modeling/dino_transformer.py\", line 803, in forward\r\n    memory = self.encoder(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/projects/ovdino/modeling/dino_transformer.py\", line 446, in forward\r\n    query = layer(\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detrex/layers/transformer.py\", line 161, in forward\r\n    query = self.attentions[attn_index](\r\n  File \"/data/anaconda3/envs/ovdino/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/ps/Documents/Project/OV-DINO/ovdino/detrex/layers/multi_scale_deform_attn.py\", line 305, in forward\r\n    value = value.masked_fill(key_padding_mask[..., None], float(0))\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 1; 23.64 GiB total capacity; 22.48 GiB already allocated; 47.06 MiB free; 22.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n"
      },
      {
        "user": "hyhcompetition",
        "created_at": "2024-09-26T11:44:55Z",
        "body": "Sorry, my mistake. It seems that in addition to modifying the batch size parameter in custom_ovd.py, I also need to change the batch size parameter in ovdino/projects/ovdino/configs/ovdino_swin_tiny224_bert_base_ft_custom_24ep.py. What’s the difference between these two parameters?\r\n\r\nAfter modifying the parameters in ovdino/projects/ovdino/configs/ovdino_swin_tiny224_bert_base_ft_custom_24ep.py , I successfully started fine-tuning!"
      }
    ]
  },
  {
    "number": 46,
    "title": "Online demo page can't  be openned",
    "created_at": "2024-09-25T14:17:56Z",
    "closed_at": "2024-09-30T08:05:13Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/46",
    "body": "Online demo page can't  be openned",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/46/comments",
    "author": "YushengZhang",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-09-26T01:12:14Z",
        "body": "@YushengZhang Try it again."
      }
    ]
  },
  {
    "number": 45,
    "title": "How to train on heterogeneous datasets？",
    "created_at": "2024-09-25T06:10:49Z",
    "closed_at": "2024-09-30T08:05:51Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/45",
    "body": "Thank you for your amazing work, I have a question I hope we can discuss together.\r\n\r\nIf there are several target detection datasets labeled with different objects in similar scenes (e.g., one part labeled with cars, one part labeled with pedestrians), can we use such a heterogeneous dataset to train a model to predict an all-class detection result? According to the idea of language aware query selection and language aware decoder it should be possible to accomplish such a function.\r\n\r\nDo you have any idea about this? If this idea is valid then it would be a meaningful thought in application.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/45/comments",
    "author": "wwf1995",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-09-25T07:11:46Z",
        "body": "@wwf1995 Hello, it is possible to train a detection model on heterogeneous datasets, while you need to keep the semantics consistent for the same categories across different datasets (they don't need to be the whole same nouns but share same semantics, e.g. person in dataset1, while human in dataset2)."
      },
      {
        "user": "wwf1995",
        "created_at": "2024-09-25T08:21:50Z",
        "body": "@wanghao9610 One concern I have is whether the negative samples generated during training will affect the detection of the different classes？For example, if an image has vehicles and pedestrians, but only the pedestrians are labeled and not the vehicles, will it affect the results of the model's vehicle detection box when used as training data?"
      },
      {
        "user": "wanghao9610",
        "created_at": "2024-09-25T08:31:16Z",
        "body": "Actually, almost all datasets are not labeled sufficiently. In other words, there exists a lot of images that are not annotated for all instances defined in the categories. And in the training schedule, the negative samples are considered for labeld data if the annotation is right. So, I think the insufficient annotation will not affect the detection performance dramatically."
      }
    ]
  },
  {
    "number": 42,
    "title": "ovdino+sam conda environment ,error,  run:cd $root_dir/ovdino bash scripts/app.sh \\   projects/ovdino/configs/ovdino_swin_tiny224_bert_base_infer_demo.py \\   ../inits/ovdino/ovdino_swint_ogc-coco50.2_lvismv40.1_lvis32.9.pth",
    "created_at": "2024-09-10T06:39:05Z",
    "closed_at": "2024-09-11T03:03:40Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/42",
    "body": null,
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/42/comments",
    "author": "U201311",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-09-10T08:48:30Z",
        "body": "@U201311 You need provide more detail information to debug. "
      },
      {
        "user": "U201311",
        "created_at": "2024-09-10T10:18:34Z",
        "body": "> @U201311 You need provide more detail information to debug.\r\n\r\nok, thanks, I fix it, somewrong about environment"
      }
    ]
  },
  {
    "number": 38,
    "title": "Ask for different size of the checkpoint",
    "created_at": "2024-09-03T02:42:12Z",
    "closed_at": "2024-09-09T02:43:48Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/38",
    "body": "Have you tried to pre-train the model using the image backbone larger than swint？Like swinb or swinl?",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/38/comments",
    "author": "Cloud65000",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-09-03T06:28:44Z",
        "body": "We don't have much more computational resources to train the model. Even OV-DINO converges fast with only 24 epochs, while pre-training OV-DINO with Swin-L will take almost 32 A100 GPUs for two weeks, which is too expensive for us to afford."
      }
    ]
  },
  {
    "number": 37,
    "title": "Does the demo inference code use NMS for predicted bounding boxes?",
    "created_at": "2024-08-31T07:47:24Z",
    "closed_at": "2024-09-04T02:42:08Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/37",
    "body": "Hi authors, thanks you for your amazing work! I wonder does the demo inference code use NMS for predicted bounding boxes? If so, could you please share where did you use NMS?",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/37/comments",
    "author": "xingchenzhao",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-31T13:57:39Z",
        "body": "OV-DINO is a DETR-like framework, which eliminates the NMS process."
      }
    ]
  },
  {
    "number": 36,
    "title": "Curiosity about how to solve different labels towards same objects among different dataset.",
    "created_at": "2024-08-30T10:16:47Z",
    "closed_at": "2024-09-04T02:42:20Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/36",
    "body": "I'm curious about one factor of pre-trained datasets. Are there any of the following in your pre-training dataset： The label descriptions of the same target object are different in different datasets.  For instance, COCO describes birds with the label \"bird\", but Obj365 describes birds with the label \"wild bird\". How do you solve this kind of problem in the pre-trained process?  During the inference process, if the model \"eats\" a picture with birds for example, how will it predict the object birds? Will the model predict it as \"bird\" or \"wild bird\"?",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/36/comments",
    "author": "Cloud65000",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-31T14:00:31Z",
        "body": "In pre-training, we keep the name as the original. Actually, the ambiguity is not important for pre-training, different people may call the same thing with different names, while the two names are the same thing."
      },
      {
        "user": "Cloud65000",
        "created_at": "2024-09-03T02:25:14Z",
        "body": "I see. I guess we can keep the name as the original because BERT will help us generate similar embeddings for those different descriptions of the same object."
      }
    ]
  },
  {
    "number": 34,
    "title": "About the scalability of OV-DINO",
    "created_at": "2024-08-30T02:07:24Z",
    "closed_at": "2024-09-09T02:43:58Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/34",
    "body": "Dear authors,\r\n\r\nHave you conducted some experiments to evaluate the scalability of OV-DINO, like using objects365-v2 to train the model？The Grounding DINO-L model was trained using the v2 version. \r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/34/comments",
    "author": "kaijinz0228",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-31T13:38:53Z",
        "body": "Hello, @kaijinz0228 , thank you for your interest in our work. We don't have enough computational resources to conduct more experiments, while it is almost certain that OV-DINO will obtain better results with more training data or a larger model size. We have established O365 pre-training code, you could pre-train it on the O365 v2 dataset."
      }
    ]
  },
  {
    "number": 21,
    "title": "训练很慢",
    "created_at": "2024-08-15T09:12:08Z",
    "closed_at": "2024-08-16T02:58:33Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/21",
    "body": "finetune的时候，很慢，有办法提高速度吗？我是固定几个目标的检测，是不是text的特征可以预先生成好？",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/21/comments",
    "author": "zuoshaobo",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-15T09:24:11Z",
        "body": "训练慢主要是所有的模型权重都需要更新，你可以考虑在ovdino/projects/ovdino/configs/ovdino_swin_tiny224_bert_base_ft_custom_24ep.py L37下面添加下面的代码：\r\n```python\r\nmodel.language_backbone.is_freeze=True\r\n```\r\n固定住text encoder的参数。"
      },
      {
        "user": "zuoshaobo",
        "created_at": "2024-08-16T00:29:47Z",
        "body": "好的，谢谢"
      }
    ]
  },
  {
    "number": 17,
    "title": "Are there any differences between pre-training and fine-tuning?",
    "created_at": "2024-08-12T02:23:15Z",
    "closed_at": "2024-08-21T11:09:09Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/17",
    "body": "Dear authors, \r\n\r\nI tried to reproduce the pre-training of OV-DINO on Objects365 using the fine-tuning code, but the training process seemed to be somehow abnormal. After 6 epochs, i got 0.371 mAP zero-shot on coco, which is quite far away from the result reported in the paper (0.495). I used the exact same pre-training config mentioned in the paper, except the batch size (i use 48 vs 128 used in paper). \r\nI wonder whether there are differences between the pre-training and fine-tuning process? If possible, could you make your pre-training logs available? \r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/17/comments",
    "author": "kaijinz0228",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-12T02:30:56Z",
        "body": "Thanks for your attention to our work. The O365 pre-tranining pipeline is almost same with the COCO finetuning pipeline, you can reproduce the results follow the COCO pipeline in theory. There may existe some bugs in your pre-training, I will update the O365 pre-training code and log in this week."
      },
      {
        "user": "kaijinz0228",
        "created_at": "2024-08-12T02:37:59Z",
        "body": "Thank you for your response and code sharing."
      },
      {
        "user": "wanghao9610",
        "created_at": "2024-08-15T06:45:28Z",
        "body": "@kaijinz0228 Hi, I have released the O365 pre-training code and log, you could re-pull the repo and try to reproduce our resutls. If you have any problem, feel free to raise an isseue again.\r\n**PLEASE NOTE**: The batch size is difference between the code and paper, the batch size for O365 pre-training is 64 on 16 GPUs as for the small dataset size. The batch size for [O365, GoldG] and [O365, GoldG, CC1M‡] pre-training are 128 on 32 GPUs, it keeps consistent with the paper reported. I will release the code and their log after our paper is accepted."
      }
    ]
  },
  {
    "number": 13,
    "title": "微调时的训练过程图",
    "created_at": "2024-08-08T13:28:47Z",
    "closed_at": "2024-08-09T07:33:16Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/13",
    "body": "您好，想请问一下在coco数据集上微调时，有没有支持查看训练过程的参数（tensorboard类似的）？",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/13/comments",
    "author": "Zwh195",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-09T02:29:29Z",
        "body": "默认是支持tensorboard的。你可以查看一下你的训练输出路径中是否有 events.out.tfevents.* 的文件，如果有则说明已经有tensorboard的日志了。进入到训练输出路径中，执行以下命令：\r\n```bash\r\ntensorboard --logdir=./\r\n```\r\n在这之前，你可能需要先安装一下tensorboard。参考：pip install tensorboard 。"
      },
      {
        "user": "Zwh195",
        "created_at": "2024-08-09T07:33:14Z",
        "body": "好的好的，谢谢您！"
      }
    ]
  },
  {
    "number": 12,
    "title": "CC1M data",
    "created_at": "2024-08-08T10:17:12Z",
    "closed_at": "2024-08-09T03:19:38Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/12",
    "body": "nice work!\r\n\r\ncan you share the  CC1M  data , many thanks ",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/12/comments",
    "author": "fujianhai",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-08T10:42:08Z",
        "body": "The whole pre-training code and datasets detail will be released after our paper accepted, and I will update the pre-training stage on O365 in the next few weeks, please stay tuned."
      }
    ]
  },
  {
    "number": 10,
    "title": "questions about implement and deploy details  ",
    "created_at": "2024-08-08T03:13:58Z",
    "closed_at": "2024-08-09T03:19:50Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/10",
    "body": "Nice work !\r\nand I have two questions:\r\n1. How many GPUs needed for full-training ?\r\n2. Is there any guide for ONNX export ?\r\n\r\nmany thanks !",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/10/comments",
    "author": "math-yyj",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-08T03:20:39Z",
        "body": "Thanks for your attention to our work. \r\n(1) OV-DINO is pre-trained on 32 A100 GPUs with batch size 128, then finetuned on 8 A100 GPUs with batch size 32.\r\n(2) Good suggestion, I will add it to the road map. I'm the only one coding, please stay tuned."
      },
      {
        "user": "math-yyj",
        "created_at": "2024-08-08T03:25:06Z",
        "body": "> Thanks for your attention to our work. (1) OV-DINO is pre-trained on 32 A100 GPUs with batch size 128, then finetuned on 8 A100 GPUs with batch size 32. (2) Good suggestion, I will add it to the road map. I'm the only one coding, please stay tuned.\r\n\r\nThank you for your kindly reply. \r\n\r\nAnd, how many hours did pretrain and finetune cost ?     "
      },
      {
        "user": "wanghao9610",
        "created_at": "2024-08-08T03:39:47Z",
        "body": "OV-DINO<sup>1</sup> pre-trained on O365 takes about 1 day, OV-DINO<sup>2</sup> pre-trained on (O365, GoldG) takes about 3 days, and OV-DINO<sup>3</sup> pre-trained on (O365, GoldG, CC1M‡) takes about 1 week, the fine-tuninging on COCO takes about 0.5 day. \r\nNote: The above training costs are not accurate exactly, just provided for clarification."
      }
    ]
  },
  {
    "number": 9,
    "title": "请问有更大的Image Encoder预训练效果吗？",
    "created_at": "2024-08-07T09:28:33Z",
    "closed_at": "2024-08-07T11:17:52Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/9",
    "body": "如题：请问作者有在更大的Image Encoder例如swin-L等进行过预训练吗，zero-short和fine-tuning指标如何？",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/9/comments",
    "author": "menghan1124",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-07T09:33:41Z",
        "body": "暂时没有。Swin-L预训练需要的计算资源太大，没有足够的计算资源支持我完成训练😅"
      }
    ]
  },
  {
    "number": 3,
    "title": "how to take image feature as prompt ?",
    "created_at": "2024-08-02T03:23:45Z",
    "closed_at": "2024-08-02T04:13:51Z",
    "labels": [],
    "url": "https://github.com/wanghao9610/OV-DINO/issues/3",
    "body": "use CLIP image encode is ok ?",
    "comments_url": "https://api.github.com/repos/wanghao9610/OV-DINO/issues/3/comments",
    "author": "Apm5",
    "comments": [
      {
        "user": "wanghao9610",
        "created_at": "2024-08-02T03:47:10Z",
        "body": "OV-DINO doesn't support taking image feature as prompt, only category name text prompt is available as open-vocabulary setting."
      },
      {
        "user": "Apm5",
        "created_at": "2024-08-02T04:13:32Z",
        "body": "ok. thank you"
      }
    ]
  }
]