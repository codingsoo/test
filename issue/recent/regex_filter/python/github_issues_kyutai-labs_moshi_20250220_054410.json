[
  {
    "number": 202,
    "title": "Adding Q8 quantization with bitsandbytes",
    "created_at": "2025-02-04T12:11:06Z",
    "closed_at": "2025-02-04T13:32:06Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/202",
    "body": "## Checklist\r\n\r\n- [x ] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [ x] Run pre-commit hook.\r\n- [x ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\nAdding Moshi q8 with bitsandbytes, not clear if there is no loss of IQ in the model, but it runs.",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/202/comments",
    "author": "adefossez",
    "comments": [
      {
        "user": "thiswillbeyourgithub",
        "created_at": "2025-02-04T13:56:19Z",
        "body": "Nice!\r\nIf you don't mind me asking : what are the VRAM requirements to get a 30s chat with moshi with and without int8 quantization?"
      },
      {
        "user": "adefossez",
        "created_at": "2025-02-04T14:21:57Z",
        "body": "Thanks @thiswillbeyourgithub \r\nQ8, full context: 12GB\r\nBF16, full context: 18GB\r\n\r\nwith 30 seconds context (will still talk after but get completely out of distribution)\r\nyou need to edit the line `'context': 3000`, down to 375 in `loaders.py`.\r\nQ8, 30 secs context: 10GB\r\nBF16, 30 secs context: 17GB"
      }
    ]
  },
  {
    "number": 189,
    "title": "Mimi: Reported Bitrate vs Actual Bitrate",
    "created_at": "2025-01-23T08:33:58Z",
    "closed_at": "2025-01-23T09:47:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/189",
    "body": "### Due diligence\n\n- [x] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nThanks for the great work, The bitrate of the released Mimi model seems to be 4.4kbps and not 1.1kbps. I see 32 layers of quantization, a codebook size of 2048 and 12.5 Hz token rate. Have you trained the model to work with different number quantization layers? ",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/189/comments",
    "author": "Hazel1994",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2025-01-23T09:29:15Z",
        "body": "Indeed we released the full 32 RVQ version of mimi which bitrate is 4.4kbps, if you want to have the same setup as in the paper you should only use the first 8 layers - as it's a residual quantization, this will retain most of the signal quality and there is no need for a separately trained model for this. This is actually what is done with moshi which only receives the first 8 layers for the user and produces the first 8 layers for the assistant."
      }
    ]
  },
  {
    "number": 179,
    "title": "where is the EPAD token?",
    "created_at": "2025-01-02T09:30:17Z",
    "closed_at": "2025-01-02T09:33:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/179",
    "body": "### Due diligence\r\n\r\n- [X] I have done my due diligence in trying to find the answer myself.\r\n\r\n### Topic\r\n\r\nThe paper\r\n\r\n### Question\r\n\r\nIn the paper's equation (5) the author defined two special tokens: PAD and EPAD. However, in the text_tokenizer's vocab, I cannot find any EPAD token. Following is the first few of tokens:\r\n\r\n```\r\n<unk>\r\n<s>\r\n</s>\r\n<pad>\r\n<0x00>\r\n<0x01>\r\n<0x02>\r\n<0x03>\r\n<0x04>\r\n<0x05>\r\n<0x06>\r\n<0x07>\r\n<0x08>\r\n<0x09>\r\n<0x0A>\r\n``` \r\n\r\nMaybe \\<pad\\> corresponds to PAD token, where is the EPAD token in the vocab? \r\n\r\nThank you in advance.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/179/comments",
    "author": "jtkim-kaist",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2025-01-02T09:32:09Z",
        "body": "See #123 "
      },
      {
        "user": "jtkim-kaist",
        "created_at": "2025-01-02T09:33:15Z",
        "body": "Thank you! "
      }
    ]
  },
  {
    "number": 175,
    "title": "Semantic token inference for user stream",
    "created_at": "2024-12-26T01:50:39Z",
    "closed_at": "2024-12-26T08:43:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/175",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe PyTorch implementation\n\n### Question\n\n(My question is about the behavior of depformer in the training phase) I found that the semantic token inference for **moshi** stream is based on depformer's text emb and transformer_out \r\n\r\n```python\r\n        depformer_input = transformer_out\r\n        if self.depformer_multi_linear:\r\n            depformer_input = self.depformer_in[depformer_cb_index](depformer_input)  # depformer_in: cb_index 별로 trasnforme_out encoding\r\n        else:\r\n            depformer_input = self.depformer_in[0](depformer_input)\r\n        if depformer_cb_index == 0:\r\n            last_token_input = self.depformer_text_emb(sequence[:, 0])\r\n        else:\r\n            last_token_input = self.depformer_emb[depformer_cb_index - 1](\r\n                sequence[:, 0]\r\n            )\r\n        depformer_input = depformer_input + last_token_input\r\n        assert depformer_input.shape[1] == 1\r\n        # depformer_input is [B, 1, depformer_dim].\r\n        # The streaming state of the depformer ensures that the proper layer is run.\r\n        dep_output = self.depformer(depformer_input)\r\n``` \r\n\r\nHowever, for the user stream, there is **no text_emb** so how can do the inference for semantic token? At the first time I see the paper, the author wrote that they simply concat the text, moshi and user stream along the codebook dimension so I guess the semantice token inference for user stream can be done with last acoustic token for moshi stream; however, in the code, there is no depformer_in for the last acoustic token with the comment \"# Only using up to dep_q - 1 because the last codebook is never an input to Depformer.\" ; so my guess is incorrect.\r\n\r\nCould you let me know how the user stream's semantic token inference can be done? did you insert some zero values between user and moshi stream? or use user's text stream only for the training phase?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/175/comments",
    "author": "jtkim-kaist",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-12-26T07:41:11Z",
        "body": "I'm not sure to understand your guess correctly but I think it's correct.\r\n- The main transformer produces `transformer_out` and the text token.\r\n- The depformer produces 16 audio tokens, 8 for moshi, 8 for the user (in that order).\r\n- The first depformer layer uses the text token.\r\n- Each subsequent depformer layer uses the token from the previous layer.\r\nWith this in mind, you can see that the semantic token for moshi is based on `transformer_out` and the text token, and the semantic token for the user is based on `transformer_out` and the last acoustic token for moshi. However note that the depformer layers use a common kv cache so the layer predicting the semantic token for the user can attend to information about the text token.\r\nFinally the comment saying that \"Only using up to dep_q - 1 because the last codebook is never an input to Depformer.\", this means that the token generated by the last depformer layer (which is the last acoustic token for the user) is not used by any depformer layer as there is no subsequent layer."
      },
      {
        "user": "jtkim-kaist",
        "created_at": "2024-12-26T08:22:52Z",
        "body": "Thank you for the fast response! I attached the further detail of my question depending on your comments.\r\n----------------------------------------------------------------------------------------------------\r\nThere is a contradiction between your comments (also this is what I pointed out):\r\n\r\n_comment A_\r\n\"the semantic token for the user is based on transformer_out and the **last acoustic token for moshi.**\"\r\n\r\nand\r\n\r\n_comment B_\r\n\"this means that the token generated by **the last depformer layer** (which is the last acoustic token for the user) is not used by any depformer layer as there is no subsequent layer.\"\r\n\r\nBased on _comment A_, the last acoustic token for moshi is necessary for the user's semantic token inference, However, due to the absence of the last depformer layer, we cannot get the **last acoustic token embedding for moshi**, \r\n```python\r\nself.depformer_text_emb\r\nScaledEmbedding(32001, 1024)\r\n\r\nself.depformer_emb\r\nModuleList(\r\n  (0-6): 7 x ScaledEmbedding(2049, 1024)\r\n)\r\n``` \r\nIn the above code block, we can check that the total number of embedding layer is 8 (1 text, 7 acoustic); thus we only can get the 7th acoustic embedding, while the 8th (last) acoustic embedding for moshi is necessary for the user's semantic token inference.\r\n\r\nTherefore, the question is: how can I get the 8th (last) acoustic embedding for moshi for the user's semantic token inference without 8th (last) depformer_emb layer.\r\n \r\nAlso, cache related stuff may be out of scope in our discussion because caching technique may be used only for the inference phase, not for the training phase.  "
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-12-26T08:33:21Z",
        "body": "When training, the depformer uses 16 layers. However when running inference we only need to produce the tokens for Moshi so only run the first 8 layers, hence the released weights only use 8 layers and never generate the user audio tokens.\r\nWhat I meant by \"KV cache shared by the layers\" is the causal attention mechanism that is used by the depformer and applies both to training and inference (but indeed the term KV cache is not very appropriate here)."
      },
      {
        "user": "jtkim-kaist",
        "created_at": "2024-12-26T08:43:27Z",
        "body": "I perfectly understand the points from \"the depformer uses 16 layers\" Thank you for your great contributions and fast & kind answer!"
      }
    ]
  },
  {
    "number": 173,
    "title": "Client UI update",
    "created_at": "2024-12-19T13:45:55Z",
    "closed_at": "2024-12-20T09:34:25Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/173",
    "body": "## Checklist\r\n\r\n- [ ] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [ ] Run pre-commit hook.\r\n- [ ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\n\r\nClient UI Update with persistent settings",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/173/comments",
    "author": "ameroyer",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-12-20T09:34:30Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 164,
    "title": "Add Gradio Demo",
    "created_at": "2024-12-04T21:30:32Z",
    "closed_at": "2024-12-04T21:59:07Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/164",
    "body": "## Checklist\r\n\r\n- [x] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [x] Run pre-commit hook.\r\n- [ ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n> I, @freddyaboulton, confirm that I have read and understood the terms of the CLA of Kyutai-labs, as outlined in the repository's CONTRIBUTING.md, and I agree to be bound by these terms.\r\n\r\n## PR Description\r\n\r\nAdds a Gradio demo to let developers built web apps with moshi entirely with python\r\n\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/164/comments",
    "author": "freddyaboulton",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-12-04T21:59:19Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 144,
    "title": "Mimi Model Quantizer Configuration",
    "created_at": "2024-10-21T05:54:59Z",
    "closed_at": "2024-10-21T06:04:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/144",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nHello, I would like to ask a question. In the paper, we see that the Mimi model has Q=8 quantizers, but in the open-source Mimi model on Hugging Face, the default setting is num_quantizers=32. Are the configurations of the open-source Mimi and the one in the paper different?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/144/comments",
    "author": "WWWWxp",
    "comments": [
      {
        "user": "hdmjdp",
        "created_at": "2024-11-01T09:15:13Z",
        "body": "same question"
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-11-05T17:49:17Z",
        "body": "We have different versions of mimi, typically with 8, 16, and 32 quantizers. The open-source release has the full 32 quantizers though when using moshi we only generate the first 8 levels so have a slightly degraded audio quality."
      }
    ]
  },
  {
    "number": 143,
    "title": "Does the Mimi Codec's Weight Change During Moshi Training?",
    "created_at": "2024-10-21T03:18:03Z",
    "closed_at": "2024-10-24T06:34:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/143",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nWhat is the training process like?\r\nMy current understanding of the training process is: first, the Mimi codec and the Helium text language model are trained separately, and then Moshi is trained.\r\nI would like to know whether Mimi's weights are updated during the training of Moshi?\r\n\r\nThank you for the great work",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/143/comments",
    "author": "qa6300525",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-21T10:31:12Z",
        "body": "Mimi weights are frozen during the training of the moshi part."
      }
    ]
  },
  {
    "number": 140,
    "title": "Is an Intel build on the roadmap?",
    "created_at": "2024-10-14T17:12:10Z",
    "closed_at": "2024-10-16T04:39:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/140",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe MLX implementation\n\n### Question\n\nWill there ever be a release for this for an IntelChipset?\r\nIf so; where will this exist on a roadmap?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/140/comments",
    "author": "rubbercable",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-15T02:58:57Z",
        "body": "This sounds more like a MLX question: our MLX implementation doesn't have anything specific to apple silicon so if MLX start supporting computer with intel chipsets (and the associated gpus) at some point, moshi should benefit from it."
      }
    ]
  },
  {
    "number": 139,
    "title": "questions for M1 Mac",
    "created_at": "2024-10-13T22:41:59Z",
    "closed_at": "2024-10-17T06:47:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/139",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe MLX implementation\n\n### Question\n\nI have an M1 Macbook pro with 64Gb and the mlx version seems to work properly.  Thank you very much for this exciting project.  I just have a couple of lingering questions:\r\n* can I use the pytorch version and its larger model, or is that cuda only?\r\n* can I use moshiko/moshika with the web interface?\r\n* I'm not sure what Rust/Candle are supposed to do.  Is this relevant for the M1 Mac?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/139/comments",
    "author": "scalar27",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-16T04:43:43Z",
        "body": "- The models for the pytorch and mlx versions are exactly the same, so there isn't much interest in running the pytorch version on a mac.\r\n- You can use the web UI via `python -m moshi_mlx.local_web` (see the readme).\r\n- Rust/Candle provides an alternative implementation which is useful for larger deployments of the model, probably not much relevant for running locally on a m1 mac."
      },
      {
        "user": "scalar27",
        "created_at": "2024-10-16T16:56:46Z",
        "body": "Thanks.  For the second one, I'd like to use the web UI with the moshika voice.  Is that possible?"
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-16T16:58:17Z",
        "body": "Yes there are examples of how to do this in the readme."
      }
    ]
  },
  {
    "number": 136,
    "title": "Update Queue.tsx",
    "created_at": "2024-10-11T12:03:27Z",
    "closed_at": "2024-10-11T13:10:31Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/136",
    "body": "## Checklist\r\n\r\n- [ ] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [ ] Run pre-commit hook.\r\n- [ ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\n\r\n<!-- Description for the PR -->\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/136/comments",
    "author": "maulikml",
    "comments": [
      {
        "user": "adefossez",
        "created_at": "2024-10-11T13:10:31Z",
        "body": "No comments, no checklist, closing the PR."
      }
    ]
  },
  {
    "number": 131,
    "title": "Modeling \"other\" audio stream",
    "created_at": "2024-10-04T23:26:42Z",
    "closed_at": "2024-10-07T06:49:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/131",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe PyTorch implementation\n\n### Question\n\nHi!\r\nI'm interested in exploring/playing with Moshi's ability to model duplex audio.\r\nIIUC, the model released only includes the weights to decode the Moshi audio stream. (dep_q == 8)\r\nIs there a place to get the right weights to generate the other audio stream tokens (specifically the projection and decoder layers for the depformer, dep_q == 16)?\r\nThanks!",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/131/comments",
    "author": "ArEsKay3",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-05T15:38:55Z",
        "body": "We haven't released a model with the 16 depformer layers at this point. This may be the case when we release the fine-tuning code though we don't have an exact timeline for this yet."
      }
    ]
  },
  {
    "number": 129,
    "title": "Moshi Rust server doesn't start - model.safetensors has been renamed",
    "created_at": "2024-10-04T00:23:39Z",
    "closed_at": "2024-10-04T06:24:44Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/129",
    "body": "### Backend impacted\n\nThe Rust implementation\n\n### Operating system\n\nLinux\n\n### Hardware\n\nGPU with CUDA\n\n### Description\n\nDuring starting Rust server:\r\n   `cargo run --features cuda --bin moshi-backend -r -- --config moshi-backend/config.json standalone`\r\nThere is an error:\r\n  `Error: Failed to download '/home/ja/tmp/moshiko_rs_301e30bf@120.safetensors' file`\r\n\r\nIt looks like someone renamed `moshiko_rs_301e30bf@120.safetensors` to `model.safetensors`, and forgot update related scripts.\r\nCan you fix it? Or provide instruction what to change?\r\n\n\n### Extra information\n\nI hope description is enough.\r\nI encourage You to reproduce - following the official instruction in README - just pretend you are doing it first time.\r\n\n\n### Environment\n\nIt doesn't really matter, but I have:\r\n- OS: Ubuntu 22.04.1\r\n- Python version: 3.12.2\r\n- Torch version: 2.4.1\r\n- CUDA version: 12.4\r\n- GPU: RTX 3090",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/129/comments",
    "author": "pcbua",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-04T06:24:36Z",
        "body": "Thanks for reporting this, indeed the files were renamed but the configs weren't updated, should be fixed in #130 "
      }
    ]
  },
  {
    "number": 123,
    "title": "Which token ID is used to represent the empty token in the delay pattern?",
    "created_at": "2024-09-29T08:44:19Z",
    "closed_at": "2024-10-02T08:42:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/123",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nHi, I want to ask which token ID is used to represent the empty token in the delay pattern? In your Figure 4, I see you use token 0 denotes the empty token. I want to confirm that whether 0 is used as the empty token during training? This token will be considered when calculating the loss?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/123/comments",
    "author": "yangdongchao",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-29T08:49:53Z",
        "body": "We use token 0 (EPAD in the paper) and 3 (PAD) as in figure 4 page 15. The weight of padding tokens are reduced in the training loss, see \"Moshi pre-training\" in section 4.4 page 20."
      },
      {
        "user": "yangdongchao",
        "created_at": "2024-09-29T11:29:26Z",
        "body": "> We use token 0 (EPAD in the paper) and 3 (PAD) as in figure 4 page 15. The weight of padding tokens are reduced in the training loss, see \"Moshi pre-training\" in section 4.4 page 20.\r\n\r\nThank you your reply. \r\nI can understand that you use EPAD and PAD to padding the text sequence. My concern about the empty token in the acoustic token.  For example, taking timesteps=4 and n_q=3, delays=1, the multi-codebook sequence:\r\n        [[text_1,          text_2,          text_3,          text_4],\r\n        [Semantic_1,  Semantic_2, Semantic_3, Semantic_4],\r\n        [Acoustic_1,   Acoustic_2,  Acoustic_3,   Acoustic_4]]\r\n        The resulting sequence obtained from the returned pattern is:\r\n        [[ text_1,  text_2,         text_3,          text_4,           S],\r\n        [ S,          Semantic_1,  Semantic_2, Semantic_3,  Semantic_4],\r\n        [ S,          Acoustic_1,    Acoustic_2,  Acoustic_3,   Acoustic_4]]\r\n\r\nSo, my question is, which token will be used as S. Because I want to finetuning your checkpoint, so I want to keep the same token with you."
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-29T11:31:59Z",
        "body": "Token 2048 is used for the initial timesteps of the audio codebooks."
      },
      {
        "user": "yangdongchao",
        "created_at": "2024-09-29T11:53:24Z",
        "body": "> Token 2048 is used for the initial timesteps of the audio codebooks.\r\n\r\nThank you for your reply.  "
      }
    ]
  },
  {
    "number": 120,
    "title": "Tweak the CI for libpython.",
    "created_at": "2024-09-27T12:37:37Z",
    "closed_at": "2024-09-27T12:51:39Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/120",
    "body": "Install the python dev package for the rust CI.",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/120/comments",
    "author": "LaurentMazare",
    "comments": [
      {
        "user": "Alaodeha1453",
        "created_at": "2024-09-27T13:15:48Z",
        "body": "### \r\n\r\n> [**__**](url)"
      }
    ]
  },
  {
    "number": 116,
    "title": "About llm",
    "created_at": "2024-09-26T07:45:18Z",
    "closed_at": "2024-10-04T07:22:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/116",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nWhy not use the existing llm and train it yourself from scratch?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/116/comments",
    "author": "wntg",
    "comments": [
      {
        "user": "Strive-for-excellence",
        "created_at": "2024-09-27T06:07:09Z",
        "body": "I have the same problem,why not use the llama 7B."
      },
      {
        "user": "adefossez",
        "created_at": "2024-09-27T10:45:55Z",
        "body": "Llama has a restrictive license, similarly to a number of models released by GAFAM (e.g. Moshi would have to be called LlamaMoshi). Now there are some more alternatives, but they didn't exist when we started the project. Finally, we also have a co-training phase where we keep feeding text batches to the model, and for that we need a text training pipeline."
      }
    ]
  },
  {
    "number": 111,
    "title": "Question for training details ",
    "created_at": "2024-09-24T12:28:48Z",
    "closed_at": "2024-09-26T09:33:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/111",
    "body": "### Due diligence\r\n\r\n- [X] I have done my due diligence in trying to find the answer myself.\r\n\r\n### Topic\r\n\r\nThe paper\r\n\r\n### Question\r\n\r\nHi, dear authors\r\n\r\n1. Moshi pretraining and post-training\r\nThe two stages both involve a portion of text-only for language modeling training to preserve lanauge capability. I'm wondering which component produces the logit for next text token, is it the orginal LM head of Helium or the first prediciton of depth transformer?\r\n\r\n2. Moshi pretraining\r\nSince we are doing generative modeling instead of BERT-style masked prediction, I don't quite understand what this means and why this is necessary:\r\n> We mask the corresponding text tokens with a probability of 30%.\r\n\r\n3. Multi-stream training\r\nStarting from Moshi Post-training stage, the model continues to train on multi-stream data. In Figure 4, It's obvious that at inference time, the model always predicts \"text token -> moshi stream\" and ignores the user stream. But at training time, do you optimize on \"text token -> moshi stream -> user stream\"? i.e., do you calculate the loss of predicting user stream tokens?\r\n\r\n4. Initialization for Moshi pretraining\r\nMoshi is initialized from Helium, an un-aligned language model incapable of basic conversation and instruction following. Have you tried a Helium-instruct model to initialize single audio stream training? Maybe this can reduce the loss of language capability with following audio finetunings.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/111/comments",
    "author": "pluiez",
    "comments": [
      {
        "user": "TianwenWei",
        "created_at": "2024-09-25T02:38:15Z",
        "body": "For the first question, it can be seen from the code that it is the temporal transformer that produces the logit for the next text token. "
      }
    ]
  },
  {
    "number": 110,
    "title": "how to implement interruption? how two streams work together from the code.",
    "created_at": "2024-09-24T09:02:27Z",
    "closed_at": "2024-10-04T06:54:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/110",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nCould you please provide more details on how to implement interruption and signal to the model when to respond? For example, how does the model recognize when the user has finished speaking, especially in noisy environments? and how two streams work together from the code.",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/110/comments",
    "author": "stepbystep88",
    "comments": [
      {
        "user": "adefossez",
        "created_at": "2024-09-27T10:49:19Z",
        "body": "There is no explicit signaling. The model consistently output an audio stream, sometimes decoding to silence, sometimes not. One can derive that the model is silent if the text tokens it outputs are PAD tokens (e.g. 3).\r\nThe system is not perfect, in particular due to some biases in the synthetic data used which can make Moshi think the user is not done talking while they are, but we will be looking into improving that in the future."
      }
    ]
  },
  {
    "number": 105,
    "title": "Has there been an assessment of ASR function?",
    "created_at": "2024-09-23T12:27:17Z",
    "closed_at": "2024-09-25T09:05:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/105",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe PyTorch implementation\n\n### Question\n\nAbout asr capability, can it be used directly as an asr model? Have you performed any evaluations on any datasets?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/105/comments",
    "author": "zly-idleness",
    "comments": [
      {
        "user": "zly-idleness",
        "created_at": "2024-09-25T08:56:21Z",
        "body": "So , the trained ASR model is not released right ? the released moshi model can not directly use to perform ASR tasks by just changing the delays?\r\n\r\nI have change delays like this , but the output seems not doing transcription:\r\n ```py\r\n # \"delays\": [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\r\n    \"delays\": [4, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2],\r\n   ```\r\n  "
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-25T09:00:27Z",
        "body": "The released model would not do ASR indeed, you need to retrain it after changing the delays. We aim at releasing the training codebase at some point but don't have an estimated timeline for this yet."
      },
      {
        "user": "zly-idleness",
        "created_at": "2024-09-25T09:05:53Z",
        "body": "> The released model would not do ASR indeed, you need to retrain it after changing the delays. We aim at releasing the training codebase at some point but don't have an estimated timeline for this yet.\r\n\r\nThank You ! Looking forward to the training codebase release."
      }
    ]
  },
  {
    "number": 103,
    "title": "any smarter?",
    "created_at": "2024-09-23T00:35:27Z",
    "closed_at": "2024-10-04T07:28:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/103",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nOther / All\n\n### Question\n\nis it posible to make this model smarter using LLAMA or just by scaling?\r\n\r\nIf yes can someone help with this?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/103/comments",
    "author": "OpenMachinesAI",
    "comments": [
      {
        "user": "FerLuisxd",
        "created_at": "2024-09-27T05:16:03Z",
        "body": "Same question, I might be wrong but I think they are using whisper for the STT and they trained their own TTS"
      },
      {
        "user": "OpenMachinesAI",
        "created_at": "2024-10-01T05:13:31Z",
        "body": "idk i think its all one multimodal model?"
      },
      {
        "user": "St3p99",
        "created_at": "2024-10-01T15:42:11Z",
        "body": "Yes, if you manage to adapt the Moshi architecture to use a robust pretrained LLM (Llama 3.1) instead of Helium (Temporal Transformer)."
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-04T07:28:38Z",
        "body": "Indeed it's a all in one model so switching the text backbone requires some significant retraining."
      }
    ]
  },
  {
    "number": 88,
    "title": "fix: make profile actually working",
    "created_at": "2024-09-20T10:51:18Z",
    "closed_at": "2024-09-20T22:44:32Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/88",
    "body": "## Checklist\r\n\r\n- [x] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [x] Run pre-commit hook.\r\n- [x] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\n\r\njust trying on windows, but notice your `profile` config is not working because they are not in workspace root for rust, and so release version wouldn't include symbol and bad for debugging, hence a quick fix\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/88/comments",
    "author": "discord9",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-20T22:44:36Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 87,
    "title": "Inference settings for Spoken QA benchmark",
    "created_at": "2024-09-20T09:55:04Z",
    "closed_at": "2024-10-12T05:51:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/87",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nI want to perform Spoken Question Answering as in paper section 5.5\r\n\r\nBut I'm not sure which setting is proper. Moshi always generate their greetings first like \"Hello, How are you today?\". \r\n\r\nIs it okay to input spoken question at first, which will overlap greeting? also how to detect moshi's answer finished? should i check text pad token?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/87/comments",
    "author": "sphmel",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-04T12:35:29Z",
        "body": "If I remember correctly the spoken QA setup was not using the conversational moshi model (i.e. the ones with the greetings) but rather the base model that was later fine tuned to be conversational. So at the moment, there is no easy way to run this benchmark in the same setting, you could try just waiting for the greetings to end (either wait for ~5s or use some vad) and ask the question. Would be interesting to know the result if you end up doing so."
      }
    ]
  },
  {
    "number": 83,
    "title": "Will release base model?",
    "created_at": "2024-09-20T03:54:55Z",
    "closed_at": "2024-10-04T06:54:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/83",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe paper\n\n### Question\n\nGreat work! \r\nWill Will release base model? ",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/83/comments",
    "author": "Mddct",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-04T06:54:00Z",
        "body": "Answered in #89 :)"
      }
    ]
  },
  {
    "number": 78,
    "title": "Can i train with a new language?",
    "created_at": "2024-09-19T17:54:54Z",
    "closed_at": "2024-10-04T06:44:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/78",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nOther / All\n\n### Question\n\nI want to train to speak in Portuguese, how could I continue with the training?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/78/comments",
    "author": "eletroswing",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-10-02T07:26:42Z",
        "body": "As mentioned in the FAQ, we plan on releasing some training/fine-tuning code for this but don't have yet an estimate on the timeline."
      }
    ]
  },
  {
    "number": 76,
    "title": "Custom prompt",
    "created_at": "2024-09-19T16:35:45Z",
    "closed_at": "2024-09-27T10:40:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/76",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe MLX implementation\n\n### Question\n\nIs there a way to set a custom prompt, to make the LLM focus on a particular topic?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/76/comments",
    "author": "haqatak",
    "comments": [
      {
        "user": "tomasmcm",
        "created_at": "2024-09-19T17:21:44Z",
        "body": "similar to this, how would one build a RAG system with this? \r\nWe would need access to what the user is talking about and a way to append data related to that for the model to use in the response."
      },
      {
        "user": "amine7777",
        "created_at": "2024-09-19T22:03:21Z",
        "body": "I think this will be a game changer !"
      },
      {
        "user": "hp2413",
        "created_at": "2024-09-20T05:43:29Z",
        "body": "As per the FAQs, I think at the moment it is not supported\r\n\r\nCan I change Moshi's voice / personality?\r\nThis would require fine tuning, which is not currently supported."
      },
      {
        "user": "haqatak",
        "created_at": "2024-09-20T07:47:14Z",
        "body": "prompt engineering is not the same as fine tuning :)"
      },
      {
        "user": "ajayjakkampudi",
        "created_at": "2024-09-23T10:37:52Z",
        "body": "How can I use custom/system prompts in this code to ensure that the chatbot follows specific instructions?\r\nThe chatbot should be able to respond to the user according to the given instruction mentioned in the prompt."
      },
      {
        "user": "adefossez",
        "created_at": "2024-09-27T10:40:23Z",
        "body": "There is no support at the moment to do that, as mentioned in the FAQ! "
      }
    ]
  },
  {
    "number": 73,
    "title": "added `moshi-*` scripts/entrypoints to pyproject.toml",
    "created_at": "2024-09-19T14:14:47Z",
    "closed_at": "2024-09-20T12:13:56Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/73",
    "body": "## Checklist\r\n\r\n- [x] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [x] Run pre-commit hook.\r\n- [x] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## CLA\r\n\r\nI, Erik Bjäreholt (@ErikBjare), confirm that I have read and understood the terms of the CLA of Kyutai-labs, as outlined in the repository's CONTRIBUTING.md, and I agree to be bound by these terms.\r\n\r\n## PR Description\r\n\r\nWith this change you can now install moshi with:\r\n - `pipx install moshi` (once it's on PyPI)\r\n - `pipx install -e ./moshi/moshi` (which I tested)\r\n\r\nThis wasn't possible before since pipx requires a valid entrypoint to allow installation.",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/73/comments",
    "author": "ErikBjare",
    "comments": [
      {
        "user": "adefossez",
        "created_at": "2024-09-19T14:22:08Z",
        "body": "Also you need to agree to the CLA by following the instructions in contributing.md"
      },
      {
        "user": "ErikBjare",
        "created_at": "2024-09-19T14:31:35Z",
        "body": "Alright, fixed it up and got it running now. Very cool!\r\n\r\nLet me know if there's anything else I should do."
      },
      {
        "user": "adefossez",
        "created_at": "2024-09-19T14:58:20Z",
        "body": "final nit before I can accept"
      },
      {
        "user": "ErikBjare",
        "created_at": "2024-09-19T17:13:34Z",
        "body": "Fixed!"
      }
    ]
  },
  {
    "number": 67,
    "title": "Plans for Integrating Moshi with Native Huggingface",
    "created_at": "2024-09-19T09:24:22Z",
    "closed_at": "2024-09-20T05:52:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/67",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe PyTorch implementation\n\n### Question\n\nThank you for your impressive work. Do you have any plans to integrate Moshi with the native Huggingface?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/67/comments",
    "author": "patrick-tssn",
    "comments": [
      {
        "user": "julien-c",
        "created_at": "2024-09-19T12:42:38Z",
        "body": "What do you mean exactly @patrick-tssn?"
      },
      {
        "user": "patrick-tssn",
        "created_at": "2024-09-19T12:50:19Z",
        "body": "I mean incorporate Moshi into Huggingface models like Mimi:\r\n`from transformers import MimiModel, AutoFeatureExtractor`"
      },
      {
        "user": "adefossez",
        "created_at": "2024-09-19T15:30:39Z",
        "body": "The amazing team at Hugging Face is working on the integration right now. Might take a few days to land :)"
      }
    ]
  },
  {
    "number": 65,
    "title": "chore: update phase-vocoder.min.js",
    "created_at": "2024-09-19T07:55:55Z",
    "closed_at": "2024-09-19T13:40:38Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/65",
    "body": "\r\n\r\n## Checklist\r\n\r\n- [ ] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [ ] Run pre-commit hook.\r\n- [ ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\noverriden -> overridden\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/65/comments",
    "author": "eltociear",
    "comments": [
      {
        "user": "adefossez",
        "created_at": "2024-09-19T13:40:38Z",
        "body": "Thanks for your PR. however this file was a left over from a previous version and we don't actually need it. I've removed it!"
      }
    ]
  },
  {
    "number": 60,
    "title": "fix typo in requirements.txt",
    "created_at": "2024-09-19T01:05:52Z",
    "closed_at": "2024-09-19T01:08:58Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/60",
    "body": "## Checklist\r\n\r\n- [x] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [ ] Run pre-commit hook.\r\n- [ ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\n\r\n<!-- Description for the PR -->\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/60/comments",
    "author": "dsa",
    "comments": [
      {
        "user": "dsa",
        "created_at": "2024-09-19T01:07:39Z",
        "body": "I, dsa, confirm that I have read and understood the terms of the CLA of Kyutai-labs, as outlined in the repository's CONTRIBUTING.md, and I agree to be bound by these terms."
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-19T01:09:04Z",
        "body": "Thanks Russ!"
      }
    ]
  },
  {
    "number": 49,
    "title": "chore: update hub id to kyutai org.",
    "created_at": "2024-09-18T13:10:15Z",
    "closed_at": "2024-09-18T13:16:12Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/49",
    "body": "## Checklist\r\n\r\n- [ ] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.\r\n- [ ] Run pre-commit hook.\r\n- [ ] If you changed Rust code, run `cargo check`, `cargo clippy`, `cargo test`.\r\n\r\n## PR Description\r\n\r\n<!-- Description for the PR -->\r\n",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/49/comments",
    "author": "Vaibhavs10",
    "comments": [
      {
        "user": "adefossez",
        "created_at": "2024-09-18T13:16:12Z",
        "body": "seems like we both did it at the same time, closing this pr!"
      }
    ]
  },
  {
    "number": 17,
    "title": "feat: add modular download from hub for mlx server.",
    "created_at": "2024-09-10T14:40:54Z",
    "closed_at": "2024-09-10T15:41:02Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/17",
    "body": null,
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/17/comments",
    "author": "Vaibhavs10",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-10T15:41:06Z",
        "body": "Perfect, thanks!"
      }
    ]
  },
  {
    "number": 16,
    "title": "feat: add modular download from hub for pytorch server.",
    "created_at": "2024-09-10T14:13:22Z",
    "closed_at": "2024-09-10T14:47:36Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/16",
    "body": null,
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/16/comments",
    "author": "Vaibhavs10",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-10T14:47:41Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 15,
    "title": "feature: Pull from Hub & generate keys",
    "created_at": "2024-09-09T16:49:50Z",
    "closed_at": "2024-09-11T09:22:15Z",
    "labels": [],
    "url": "https://github.com/kyutai-labs/moshi/pull/15",
    "body": "This PR does 2 things:\r\n1. Pulls from the hub if any of the 3 modelling files cannot be found\r\n2. Generates a localhost self-signed `cert.pem` and `key.pem` if they're not found.",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/15/comments",
    "author": "FL33TW00D",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-09-11T09:22:11Z",
        "body": "Great, thanks for adding this!"
      }
    ]
  }
]