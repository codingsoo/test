[
  {
    "number": 873,
    "title": "Removed all useless try except around imports as pipmaster already handles that",
    "created_at": "2025-02-19T18:54:37Z",
    "closed_at": "2025-02-19T20:20:57Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/873",
    "body": "The try catch hides the real errors when importing the libraries. pipmaster ensures that the library is installed so if the import fails, it is a problem related to something else and we need to make it visible to allow detection of potential problems.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/873/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T19:20:51Z",
        "body": "Merci üôèüèª"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-02-19T19:51:10Z",
        "body": "I just linted everything correctly"
      }
    ]
  },
  {
    "number": 872,
    "title": "Fix ImportError by Removing Redundant Try-Except Block",
    "created_at": "2025-02-19T17:53:35Z",
    "closed_at": "2025-02-19T18:34:17Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/872",
    "body": "This PR addresses an issue where an ImportError for the networkx library was incorrectly raised due to a try-except block that masked the real problem. The actual issue was a ModuleNotFoundError for the past module, which is part of the future package.\r\n\r\nSince pipmaster is used to verify that the module is already installed, no need to use that extra try except especially that it hides the real error.\r\n\r\nThis fixes issue: #871\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/872/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T18:10:48Z",
        "body": "Thanks for sharing could please remove all try catch in the same time ?\r\n"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-02-19T18:31:20Z",
        "body": "Ok, I'll do that"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T18:34:07Z",
        "body": "Thanks a lot "
      }
    ]
  },
  {
    "number": 871,
    "title": "After updating the branch, I have networkx error",
    "created_at": "2025-02-19T17:49:17Z",
    "closed_at": "2025-02-19T18:41:52Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/871",
    "body": "Hi, I just did a git pull and now I can't run the application with the default confioguration:\nI get this\n```\n  File \"/opt/lollms/lightrag/LightRAG/lightrag/kg/networkx_impl.py\", line 27, in <module>\n    raise ImportError(\nImportError: `networkx` library is not installed. Please install it via pip: `pip install networkx`.\n```\n\nThis is not correct. We have a pipmaster check before networkx import.\n\nSo it is actually installed. When I do pip install networkx it says that it is already installed. So I removed the try except to actually see what is the real error here and it looks like it comes from this :\n\nhyppo/kgof/fssd.py\", line 4, in <module>\n    from past.utils import old_div\nModuleNotFoundError: No module named 'past'\n\nAfter some investigation it seems that I have to install future to get past working. LOOOOOL\n\n```bash\npip install future\n```\n\nSo I have removed the try except thing, it was hiding real errors. If it fails let it fail and we can see what's happening. That's better than hiding the errror begding another except.\n\nIn all case, if it wasn't installed pipmaster would have detected that.\n\nI'll do a PR",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/871/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T18:41:56Z",
        "body": "Thanks a lot"
      }
    ]
  },
  {
    "number": 866,
    "title": "Issue with demo",
    "created_at": "2025-02-19T11:44:20Z",
    "closed_at": "2025-02-19T18:42:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/866",
    "body": "Hi,\nTried to run the sample book example code .\nDuring indexing i get:\nERROR:lightrag:Failed to process document doc-addb4618e1697da0445ec72a648e1f92: 'e'‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.19s/batch]\nTraceback (most recent call last):\n  File \"/U..../lightrag/lightrag.py\", line 491, in ainsert\n    raise e\n  File \"/U..../lightrag/lightrag.py\", line 450, in ainsert\n    maybe_new_kg = await extract_entities(\n                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"..../lightrag/operate.py\", line 332, in extract_entities\n    examples = examples.format(**example_context_base)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyError: 'e'\n\nWhen i switch back to v1.1.5 - all works fine ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/866/comments",
    "author": "liornabat",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T11:47:56Z",
        "body": "Hello, could you please provide more examples, books, and more context to reproduce it.\n\nThanks."
      },
      {
        "user": "liornabat",
        "created_at": "2025-02-19T12:14:23Z",
        "body": "Hi,\nHere is the code i'm running:\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import  openai_embed,gpt_4o_complete\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\n\nEMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\nEMBEDDING_MAX_TOKEN_SIZE = int(os.environ.get(\"EMBEDDING_MAX_TOKEN_SIZE\", 8192))\nWORKING_DIR = \"./book\"\nSOURCE_DIR=\"./book/source\"\n\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=EMBEDDING_MODEL,\n    )\n\nasync def get_embedding_dimension():\n    test_text = [\"This is a test sentence.\"]\n    embedding = await embedding_func(test_text)\n    return embedding.shape[1]\n\nasync def create_embedding_function_instance():\n    embedding_dimension = await get_embedding_dimension()\n    return EmbeddingFunc(\n        embedding_dim=embedding_dimension,\n        max_token_size=EMBEDDING_MAX_TOKEN_SIZE,\n        func=embedding_func,\n    )\n\nasync def index():\n    embedding_func_instance = await create_embedding_function_instance()\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=gpt_4o_complete,\n        embedding_func=embedding_func_instance,\n    )\n\n    try:\n        with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            await rag.ainsert(content)\n    except Exception as e:\n        print(f\"Error in index: {str(e)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(index())\n\non v1.1.7 i get these errors:\n\nTraceback (most recent call last):\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/lightrag/kg/networkx_impl.py\", line 25, in <module>\n    from graspologic import embed\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/graspologic/__init__.py\", line 8, in <module>\n    import graspologic.inference\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/graspologic/inference/__init__.py\", line 6, in <module>\n    from .latent_distribution_test import latent_distribution_test\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/graspologic/inference/latent_distribution_test.py\", line 7, in <module>\n    from hyppo.ksample import KSample\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/hyppo/__init__.py\", line 5, in <module>\n    import hyppo.kgof\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/hyppo/kgof/__init__.py\", line 2, in <module>\n    from .fssd import FSSD, FSSDH0SimCovObs\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/hyppo/kgof/fssd.py\", line 4, in <module>\n    from past.utils import old_div\nModuleNotFoundError: No module named 'past'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/book.py\", line 55, in <module>\n    asyncio.run(index())\n  File \"/Users/liornabat/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/liornabat/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/liornabat/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 686, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/book.py\", line 41, in index\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 34, in __init__\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/lightrag/lightrag.py\", line 503, in __post_init__\n    self.chunk_entity_relation_graph: BaseGraphStorage = self.graph_storage_cls(  # type: ignore\n                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/lightrag/lightrag.py\", line 197, in import_class\n    module = importlib.import_module(module_name, package=package)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/liornabat/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/Users/liornabat/development/projects/liornabat/lightrag-codebase/lightrag/kg/networkx_impl.py\", line 28, in <module>\n    raise ImportError(\nImportError: `networkx` library is not installed. Please install it via pip: `pip install networkx`.\n\n\nwhen i run the code with version v1.1.5 all works fine.\n\nBTW, all the packages are installed correctly"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T12:44:33Z",
        "body": "Could you please try first pip install networkx."
      },
      {
        "user": "hakankozakli",
        "created_at": "2025-02-19T13:58:20Z",
        "body": "I am also seeing the same error.\n    \nraise ImportError(\nImportError: `networkx` library is not installed. Please install it via pip: `pip install networkx`.\n\n(myenv) nirvana@Nirvana:~/LightRAG$ pip install networkx\nRequirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (3.4.2)"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T14:21:52Z",
        "body": "Could please try with the version on master, I think I solved it."
      },
      {
        "user": "liornabat",
        "created_at": "2025-02-19T15:16:52Z",
        "body": "@YanSte , Sorry, but the same..\nBy the Way,\nWhen i change the import part to:\ntry:\n    import networkx as nx\n    from graspologic import embed\nexcept ImportError:\n    # Install required packages if they're missing\n    if not pm.is_installed(\"networkx\"):\n        pm.install(\"networkx\")\n    if not pm.is_installed(\"graspologic\"):\n        pm.install(\"graspologic\")\n\n    try:\n        import networkx as nx\n        from graspologic import embed\n    except ImportError as e:\n        logger.error(\"Failed to import required packages even after installation\")\n        raise ImportError(\n            \"Failed to import networkx or graspologic. Please ensure they are properly installed.\"\n        ) from e\n\nwe don\\t get the error:\n`networkx` library is not installed. Please install it via pip: `pip install networkx`.\n\nbut we get new one that indicates the source of the problem.\nUsers/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/graspologic/match/wrappers.py:74: SyntaxWarning: invalid escape sequence '\\s'\n  \"\"\"\n/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/graspologic/pipeline/embed/adjacency_spectral_embedding.py:37: SyntaxWarning: invalid escape sequence '\\S'\n  \"\"\"\n/Users/liornabat/development/projects/liornabat/lightrag-codebase/.venv/lib/python3.12/site-packages/graspologic/pipeline/embed/laplacian_spectral_embedding.py:38: SyntaxWarning: invalid escape sequence '\\S'\n  \"\"\"\n"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T18:42:38Z",
        "body": "With the new version 1.1.9 it's fixe."
      }
    ]
  },
  {
    "number": 861,
    "title": "ÈÅáÂà∞‰∫Üjson.decoder.JSONDecodeError: Expecting ',' delimiter: line 23697 column 16195 (char 59959183)",
    "created_at": "2025-02-19T10:43:39Z",
    "closed_at": "2025-02-20T06:40:03Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/861",
    "body": "ÂâçÂá†Ê¨°ÈÉΩÂèØ‰ª•Ê≠£Â∏∏ËøêË°åÔºå‰πüÂü∫Êú¨‰∏ä‰ª£Á†ÅÂèòÂä®ÔºåÂ∞±Âá∫Áé∞‰∫Ü‰ª•‰∏ãÈóÆÈ¢òÔºåÊÉ≥ÈóÆ‰∏Ä‰∏ãÊòØ‰ªÄ‰πàÈóÆÈ¢òÔºåÊòØ./dickens_newÂá∫‰∫ÜÈóÆÈ¢òÂêóÔºåÂ∫îËØ•Â¶Ç‰ΩïËß£ÂÜ≥ÔºåË∞¢Ë∞¢\n\nINFO:lightrag:Logger initialized for working directory: ./dickens_new\nTraceback (most recent call last):\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag_ollama_answer.py\", line 19, in <module>\n    rag = LightRAG(\n  File \"<string>\", line 32, in __init__\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag/lightrag.py\", line 220, in __post_init__\n    self.llm_response_cache = self.key_string_value_json_storage_cls(\n  File \"<string>\", line 6, in __init__\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag/storage.py\", line 34, in __post_init__\n    self._data = load_json(self._file_name) or {}\n  File \"/media/z8-2/wxy/LightRAG/LightRAG-main/lightrag/utils.py\", line 150, in load_json\n    return json.load(f)\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/__init__.py\", line 293, in load\n    return loads(fp.read(),\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/home/z8-2/anaconda3/envs/wxy_lightrag/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n    obj, end = self.scan_once(s, idx)\njson.decoder.JSONDecodeError: Expecting ',' delimiter: line 23697 column 16195 (char 59959183)",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/861/comments",
    "author": "xiayi0409",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T18:46:40Z",
        "body": "Thanks for sharing, could you please share context and code.\n\nThanks."
      },
      {
        "user": "xiayi0409",
        "created_at": "2025-02-20T06:40:03Z",
        "body": "\n> Thanks for sharing, could you please share context and code.\n> \n> Thanks.\n\nThank you for your reply! I just found the problem and have solved it. There is a problem in kv_store_llm_response_cache.json in my Lightrag's dickens. I think it may be that I forced the program to be paused during the operation, which caused the LLM response to have character problems. Delete it and run it again and it will be normal! \nThanks."
      }
    ]
  },
  {
    "number": 855,
    "title": "Run examples/lightrag_zhipu_postgres_demo.py failed:  RuntimeWarning: coroutine 'LightRAG.initialize_storages' was never awaited",
    "created_at": "2025-02-19T08:49:00Z",
    "closed_at": "2025-02-19T09:38:18Z",
    "labels": [
      "question",
      "lightrag-server"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/855",
    "body": "```\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 720, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/private/var/www/LightRAG/hello2.py\", line 32, in main\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n    ...<15 lines>...\n        vector_storage=\"PGVectorStorage\",\n    )\n  File \"<string>\", line 34, in __init__\n  File \"/private/var/www/LightRAG/lightrag/lightrag.py\", line 563, in __post_init__\n    loop.run_until_complete(self.initialize_storages())\n    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 696, in run_until_complete\n    self._check_running()\n    ~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 632, in _check_running\n    raise RuntimeError('This event loop is already running')\nRuntimeError: This event loop is already running\nINFO:Creating a new event loop in main thread.\n<sys>:0: RuntimeWarning: coroutine 'LightRAG.initialize_storages' was never awaited\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/855/comments",
    "author": "tevooli",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T09:11:33Z",
        "body": "Are you using a notebook ?\n\n``\nimport nest_asyncio\nnest_asyncio.apply()\n``\n"
      },
      {
        "user": "tevooli",
        "created_at": "2025-02-19T09:32:33Z",
        "body": "@YanSte Thank you. This line of code solved this error."
      }
    ]
  },
  {
    "number": 844,
    "title": "Fix office file indexing problem",
    "created_at": "2025-02-18T17:44:10Z",
    "closed_at": "2025-02-19T07:34:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/844",
    "body": "- Fix docx pptx indexing error\r\n- Add xlsx support",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/844/comments",
    "author": "danielaskdd",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T07:34:36Z",
        "body": "Thanks for sharing üôèüèª"
      }
    ]
  },
  {
    "number": 839,
    "title": "TypeError: Can't instantiate abstract class JsonDocStatusStorage without an implementation for abstract method 'drop'",
    "created_at": "2025-02-18T10:10:31Z",
    "closed_at": "2025-02-18T12:14:52Z",
    "labels": [
      "old-issue"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/839",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/839/comments",
    "author": "ultrageopro",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-18T10:25:06Z",
        "body": "Hi,\nCould you please update we fixed this issue."
      }
    ]
  },
  {
    "number": 837,
    "title": "Fixes and Enhancements for PostgreSQL and JSON Document Storage",
    "created_at": "2025-02-18T09:33:48Z",
    "closed_at": "2025-02-18T15:46:14Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/837",
    "body": "#### Description:\r\nThis pull request addresses several issues related to document storage in PostgreSQL and JSON handling. The following changes have been implemented:\r\n\r\n1. **Added Missing Drop Methods**\r\n   - Implemented drop methods for postgresql as well as json doc status.\r\n\r\n2. **Fixed Key Filtering for PostgreSQL**\r\n   - Corrected the key filtering logic to enhance data retrieval accuracy.\r\n\r\n3. **Fixed PostgreSQL Implementation for Status Gathering**\r\n   - Updated the status gathering implementation to ensure it functions correctly with PostgreSQL.\r\n\r\n#### Linting:\r\n- Ensured that the code adheres to the linting standards for better readability and maintainability.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/837/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "ParisNeo",
        "created_at": "2025-02-18T10:47:29Z",
        "body": "How are we going to handle wiping out the database?"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-18T14:05:30Z",
        "body": "Small reviews to apply after that I will merge."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-18T14:19:04Z",
        "body": "By the way thanks for the documentation for Postgres."
      }
    ]
  },
  {
    "number": 835,
    "title": "Can't instantiate abstract class JsonDocStatusStorage without an implementation for abstract method 'drop'",
    "created_at": "2025-02-18T09:17:22Z",
    "closed_at": "2025-02-18T09:22:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/835",
    "body": "Hi I am trying to run lightrag with ollama llama3.2:3b but facing this error\n\nINFO:Logger initialized for working directory: ./dickens\nINFO:Load KV llm_response_cache with 0 data\nINFO:Load KV full_docs with 0 data\nINFO:Load KV text_chunks with 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 0 data\nTraceback (most recent call last):\n  File \"....../LightRAG/examples/lightrag_ollama_demo.py\", line 16, in <module>\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 33, in __init__\n  File \"......./LightRAG/lightrag/lightrag.py\", line 523, in __post_init__\n    self.doc_status: DocStatusStorage = self.doc_status_storage_cls(\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"....../LightRAG/lightrag/lightrag.py\", line 198, in import_class\n    return cls(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\nTypeError: Can't instantiate abstract class JsonDocStatusStorage without an implementation for abstract method 'drop'\n\nDid anyone face this issue",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/835/comments",
    "author": "Sachinpr16",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-18T09:20:16Z",
        "body": "Hi, on it"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-18T09:22:48Z",
        "body": "Done could you update."
      },
      {
        "user": "Sachinpr16",
        "created_at": "2025-02-18T09:26:01Z",
        "body": "Thanks a lot YanSte its working now"
      }
    ]
  },
  {
    "number": 833,
    "title": "Implement dynamic database module imports",
    "created_at": "2025-02-18T08:27:23Z",
    "closed_at": "2025-02-18T14:13:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/833",
    "body": "- Dynamic import database module\r\n- Consolidate database instance management\r\n- Enhance error handling and logging\r\n- Casting datetime to str for DocStatusResponse",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/833/comments",
    "author": "danielaskdd",
    "comments": [
      {
        "user": "ArnoChenFx",
        "created_at": "2025-02-18T10:54:01Z",
        "body": "Please do not explicitly call the specific code of the backend in the server. All the functionality of the backend should only be written in the backend's implementation files."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-18T13:13:02Z",
        "body": "Thanks for sharing and your help. üôèüèª\r\n\r\nLet me know when you are done."
      },
      {
        "user": "danielaskdd",
        "created_at": "2025-02-18T14:07:54Z",
        "body": "> Thanks for sharing and your help. üôèüèª\r\n> \r\n> Let me know when you are done.\r\n\r\nReady for merge"
      }
    ]
  },
  {
    "number": 829,
    "title": "after re-installing (re-cloning) lightrag, when I do pip install -e . this error occurs",
    "created_at": "2025-02-18T07:24:21Z",
    "closed_at": "2025-02-18T09:07:18Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/829",
    "body": "          ~~~~~~~~~~~~~~~~~~~~~^^\n        File \"<string>\", line 111, in finalize_options\n      AttributeError: 'dict' object has no attribute '__NUMPY_SETUP__' and no __dict__ for setting new attributes\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n√ó Encountered error while generating package metadata.\n‚ï∞‚îÄ> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n..",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/829/comments",
    "author": "psygenlab",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-18T07:50:49Z",
        "body": "Hi thanks for sharing,\n\nCould you please check your python version 3.10?"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-18T09:07:17Z",
        "body": "After testing all good."
      },
      {
        "user": "psygenlab",
        "created_at": "2025-02-18T16:30:43Z",
        "body": "oh!, I am using python 3.12, I will try python 3.10 then"
      }
    ]
  },
  {
    "number": 828,
    "title": "Retrieval of information is failing using Mistral API endpoint",
    "created_at": "2025-02-18T06:31:40Z",
    "closed_at": "2025-02-19T20:39:30Z",
    "labels": [
      "community-libs",
      "mistral"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/828",
    "body": "Hi Team,\nI am trying to implement LightRAG with Mistral API endpoint for Model: mistralai/Mixtral-8x7B-Instruct-v0.1. When I am inserting new document which is not available online and asking document specific query, the model is not able to retrieve the content.  I have also tried by deleting earlier documents and inserting the new document alone but still not able to get the success.  While LightRAG implementation with local ollama model :qwen2 is able to retrieve the information properly. \n\nlightrag) PS C:\\Lightrag\\LG_mistral\\lightrag\\examples> python lightrag_mistral_chat_pdf_JBS.py\nINFO:Logger initialized for working directory: C:/Lightrag/LG_mistral/LightRAG/examples\nINFO:Load KV json_doc_status_storage with 0 data\nINFO:Load KV llm_response_cache with 6 data\nINFO:Load KV full_docs with 4 data\nINFO:Load KV text_chunks with 444 data\nINFO:Loaded graph from C:/Lightrag/LG_mistral/LightRAG/examples\\graph_chunk_entity_relation.graphml with 2089 nodes, 1194 edges\nINFO:Load (1978, 4096) data\nINFO:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': 'C:/Lightrag/LG_mistral/LightRAG/examples\\\\vdb_entities.json'} 1978 data\nINFO:Load (1194, 4096) data\nINFO:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': 'C:/Lightrag/LG_mistral/LightRAG/examples\\\\vdb_relationships.json'} 1194 data\nINFO:Load (482, 4096) data\nINFO:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': 'C:/Lightrag/LG_mistral/LightRAG/examples\\\\vdb_chunks.json'} 482 data\nINFO:Loaded document status storage with 4 records\nINFO:Processing 1 new unique documents\nProcessing batch 1:   0%|                                                                                                                      | 0/1 [00:00<?, ?it/s]INFO:Inserting 1 vectors to chunks\nGenerating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.54s/batch]\nGenerating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàWARNING:Didn't extract any relationships‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.54s/batch] \nINFO:Inserting 8 vectors to entities\nGenerating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.18s/batch]\nINFO:Inserting 0 vectors to relationships‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.16s/batch] \nWARNING:You insert an empty data to vector DB\nINFO:Writing graph with 2097 nodes, 1194 edges\nProcessing batch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.58s/it]",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/828/comments",
    "author": "j-shah7",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-18T09:07:40Z",
        "body": "We are not supporting Mistral for the moment."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T20:38:49Z",
        "body": "Mistral doesn't support OpenAI calls."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T20:39:28Z",
        "body": "If you would like, you can help us with this LLM."
      }
    ]
  },
  {
    "number": 824,
    "title": "Update README.md - Imports",
    "created_at": "2025-02-17T23:13:36Z",
    "closed_at": "2025-02-18T07:37:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/824",
    "body": "Edited examples to use ```from lightrag.lightrag import LightRAG, QueryParam``` instead of ```from lightrag import LightRAG, QueryParam``` (latter causes Unresolved reference error upon pip install of LightRAG v1.1.6)",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/824/comments",
    "author": "VeiledTee",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-18T07:37:06Z",
        "body": "Thanks !"
      }
    ]
  },
  {
    "number": 820,
    "title": "fix: float usage in CONNECTION_TIMEOUT and CONNECTION_ACQUISITION_TIMEOUT",
    "created_at": "2025-02-17T18:02:25Z",
    "closed_at": "2025-02-17T18:05:03Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/820",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/820/comments",
    "author": "ultrageopro",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T18:05:00Z",
        "body": "Thanks a lot. üôèüèª"
      }
    ]
  },
  {
    "number": 819,
    "title": "Added full documentation on how to install postgresql and use it with lightrag on ubuntu system",
    "created_at": "2025-02-17T17:47:20Z",
    "closed_at": "2025-02-17T20:05:58Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/819",
    "body": "## Description\r\n\r\nThis pull request updates the documentation to include detailed steps for setting up PostgreSQL with LightRAG. The instructions ensure that the `pgvector` extension is activated specifically within the database intended for use with LightRAG.\r\n\r\n## Changes Made\r\n\r\n- Added installation steps for PostgreSQL and pgvector.\r\n- Updated instructions for creating a new database and activating the extension within it.\r\n- Included configuration settings for LightRAG to work with PostgreSQL.\r\n\r\n## Why This AdditionWas Necessary\r\n\r\nLightrag uses a non standard extention of postgresql (pgvector). It took me time to search and fo through this so I wanted to write a full documentation on how to do this so that other users won't waste much time.\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/819/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T17:49:32Z",
        "body": "Thanks a lot, Please make sure to apply the lint."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-02-17T19:09:13Z",
        "body": "Indeed!"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-02-17T19:40:26Z",
        "body": "Linting done.\r\nAnd sorry for keeping the AI output. LOL\r\nThat was the last update and I was in a hurry so I did copy paste without paying attention.\r\nNow that part is fixed and the linting is also done.\r\n\r\nI also added the Icon that @ArnoChenFx  asked for with alpha channel at the assets folder"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T20:05:23Z",
        "body": "Great, thanks a lot üôèüèª\r\n\r\nPS : Je vais l'utiliser √† 100%"
      }
    ]
  },
  {
    "number": 813,
    "title": "Python 3.11 forced dependency (StrEnum)",
    "created_at": "2025-02-17T14:28:51Z",
    "closed_at": "2025-02-17T17:28:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/813",
    "body": "In lightrag/base.py there is en enum import\n\nfrom enum import StrEnum\n\nStrEnum is only available on python 3.11 and broke the python 3.10 implementation.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/813/comments",
    "author": "puppetm4st3r",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T15:22:43Z",
        "body": "Good point, could ma pull request ?"
      },
      {
        "user": "puppetm4st3r",
        "created_at": "2025-02-17T15:29:20Z",
        "body": "im seeying a few more bugs in pg impl, let me try to correct, then I can pull!"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T17:00:00Z",
        "body": "Thanks a lot @puppetm4st3r "
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T17:28:46Z",
        "body": "I fixed the enumeration @puppetm4st3r\nIf you have any suggestions for improvement, don't hesitate :)"
      }
    ]
  },
  {
    "number": 811,
    "title": "Fixed broken ainsert_custom_kg()",
    "created_at": "2025-02-17T14:15:53Z",
    "closed_at": "2025-02-19T14:20:37Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/811",
    "body": "Fix for #745 \r\n\r\nCustom knowledge graph insertion fails due to required properties not provided (tokens, chunk_order_id, full_doc_id, status).",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/811/comments",
    "author": "da-luggas",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T09:21:44Z",
        "body": "@da-luggas Could please apply the lint ? \r\n\r\nThanks "
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T11:28:59Z",
        "body": "@da-luggas Could you please update the documentation base and your changes ? And update the code sample.\r\n\r\nThanks !"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T14:19:57Z",
        "body": "Super thanks for sharing !"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-19T14:20:23Z",
        "body": "Super Thanks for sharing !"
      }
    ]
  },
  {
    "number": 808,
    "title": "Oracle Native calls",
    "created_at": "2025-02-17T11:48:47Z",
    "closed_at": "2025-02-18T17:22:29Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/808",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/808/comments",
    "author": "theAI-samurai",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T20:09:40Z",
        "body": "By the way, thanks for sharing."
      }
    ]
  },
  {
    "number": 806,
    "title": "Added system prompt support in all modes",
    "created_at": "2025-02-17T11:17:25Z",
    "closed_at": "2025-02-17T12:17:21Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/806",
    "body": "- Bring your own custom system prompt and do query\r\n- Supported in all forms of search type\r\n- If you don't pass your own system prompt, answer will be generated using lightRAG by default system prompt defined in each mode",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/806/comments",
    "author": "MdNazishArmanShorthillsAI",
    "comments": [
      {
        "user": "MdNazishArmanShorthillsAI",
        "created_at": "2025-02-17T11:19:10Z",
        "body": "@LarFii Please review let me know if there is any changes"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T12:16:30Z",
        "body": "Looks good to me. Thanks for sharing."
      },
      {
        "user": "MdNazishArmanShorthillsAI",
        "created_at": "2025-02-17T12:47:16Z",
        "body": "> Looks good to me. Thanks for sharing.\r\n\r\nWelcome"
      }
    ]
  },
  {
    "number": 804,
    "title": "how to restore lightRAG instance from workdirÔºàalready extract some knowledgeÔºâÔºü",
    "created_at": "2025-02-17T08:47:23Z",
    "closed_at": "2025-02-17T20:19:35Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/804",
    "body": "how to restore lightRAG instance from workdirÔºàalready extract some knowledgeÔºâÔºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/804/comments",
    "author": "hxtkyne",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T20:15:10Z",
        "body": "Hi,\n\nUsing the path, and via docker a volume."
      },
      {
        "user": "hxtkyne",
        "created_at": "2025-02-17T20:15:41Z",
        "body": "ÈÇÆ‰ª∂Â∑≤Êî∂Âà∞‰∫ÜÂì¶‰∫≤ÔºåÁªèÂ∏∏ËÅîÁ≥ªÂïä„ÄÇÂ•ΩÊúãÂèãËµ∞‰∏ÄÁîü"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T20:19:06Z",
        "body": "ÂêåÊ†∑Âú∞„ÄÇ"
      }
    ]
  },
  {
    "number": 799,
    "title": "[Bug][Neo4j] get_edge() returns incorrect edge properties due to schema mismatch",
    "created_at": "2025-02-16T19:21:04Z",
    "closed_at": "2025-02-17T17:36:17Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/799",
    "body": "## Description\nWhen using Neo4j as `LIGHTRAG_GRAPH_STORAGE` ~with a non-empty `NAMESPACE_PREFIX`~, the web UI query fails because the edge data is missing the required 'description' field. This issue doesn't occur when:\n- ~`NAMESPACE_PREFIX` is empty, or~ (I can't reproduce this today; maybe I made a mistake yesterday.) \n- Using MongoDB as `LIGHTRAG_GRAPH_STORAGE`\n\n## Error Message\n```\n[ERROR][2025-02-16 19:07:40] Traceback (most recent call last):\n  File \"/app/lightrag/api/lightrag_server.py\", line 1299, in query_text\n    response = await rag.aquery(\n  File \"/app/lightrag/lightrag.py\", line 1022, in aquery\n    response = await kg_query(\n  File \"/app/lightrag/operate.py\", line 659, in kg_query\n    context = await _build_query_context(\n  File \"/app/lightrag/operate.py\", line 1038, in _build_query_context\n    ll_data, hl_data = await asyncio.gather(\n  File \"/app/lightrag/operate.py\", line 1355, in _get_edge_data\n    edge_datas = truncate_list_by_token_size(\n  File \"/app/lightrag/utils.py\", line 228, in truncate_list_by_token_size\n    tokens += len(encode_string_by_tiktoken(key(data)))\n  File \"/app/lightrag/operate.py\", line 1357, in <lambda>\n    key=lambda x: x[\"description\"],\nKeyError: 'description'\n```\n\n## Debug Information\nThe edge data object `x` contains the following in this case:\n```python\n{\n    'src_id': '\"XXXXXX\"', 'tgt_id': '\"XXXXXXX\"', 'rank': 11, 'created_at': None, 'weight': 0.0, 'source_id': None, 'target_id': None\n}\n```\n\n## Environment\n- Storage: Neo4j\n\n## Expected Behavior\nData should include a 'description' field when retrieved from Neo4j.  I also believe that these keys\n```\n'created_at': None, 'weight': 0.0, 'source_id': None, 'target_id': None\n```\nshould contain non-trivial data.\n\n## Current Behavior\nData is missing the 'description' field, causing the query to fail.\n\nPS: Additionally, I'd like to understand if there are any differences in retrieval results when using MongoDB versus Neo4j as the graph storage backend, thanks.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/799/comments",
    "author": "atYuguo",
    "comments": [
      {
        "user": "atYuguo",
        "created_at": "2025-02-17T14:05:45Z",
        "body": "After investigating the code, I found the issue is related to a mismatch between the `edge_property` formats in different parts of the codebase.\n\n1. In the following part of `neo4j_impl.py`\n```python\n@@ -278,14 +278,16 @@ class Neo4JStorage(BaseGraphStorage):\n\n                result = await session.run(query)\n                record = await result.single()\n                if record and \"edge_properties\" in record:\n                    try:\n                        result = dict(record[\"edge_properties\"])\n\n                        # Ensure required keys exist with defaults\n                        required_keys = {\n                            \"weight\": 0.0,\n                            \"source_id\": None,\n                            \"target_id\": None,\n\n                        }\n                        for key, default_value in required_keys.items():\n                            if key not in result:\n@@ -305,20 +307,20 @@ class Neo4JStorage(BaseGraphStorage):\n                            f\"and {entity_name_label_target}: {str(e)}\"\n                        )\n                        # Return default edge properties on error\n                        return {\"weight\": 0.0, \"source_id\": None, \"target_id\": None}\n\n                logger.debug(\n                    f\"{inspect.currentframe().f_code.co_name}: No edge found between {entity_name_label_source} and {entity_name_label_target}\"\n                )\n                # Return default edge properties when no edge found\n                return {\"weight\": 0.0, \"source_id\": None, \"target_id\": None}\n\n        except Exception as e:\n            logger.error(\n                f\"Error in get_edge between {source_node_id} and {target_node_id}: {str(e)}\"\n            )\n            # Return default edge properties on error\n            return {\"weight\": 0.0, \"source_id\": None, \"target_id\": None}\n\n    async def get_node_edges(self, source_node_id: str) -> list[tuple[str, str]] | None:\n        node_label = source_node_id.strip('\"')\n```\n, there are two issues:\n   - The condition `if record and \"edge_properties\" in record` never evaluates to true\n   - The default edge properties do not match the expected format\n\n2. Current Edge Property Format:\n   ```python\n   # In neo4j_impl.py\n   # Default edge properties\n   {\n       \"weight\": 0.0,\n       \"source_id\": None,\n       \"target_id\": None  # Note this field\n   }\n   ```\nalso\n   ```python\n   return {\"weight\": 0.0, \"source_id\": None, \"target_id\": None}\n   ```\n\n3. Expected Edge Property Format (from `operate.py`):\n   ```python\n   # In operate.py\n   edge_data = {\n       \"weight\": weight,\n       \"description\": description,  # Required field that's missing\n       \"keywords\": keywords,\n       \"source_id\": source_id\n       # No 'target_id' field\n   }\n   ```\n\n## Suggested Fix\n```python\n@@ -278,14 +278,16 @@ class Neo4JStorage(BaseGraphStorage):\n\n                result = await session.run(query)\n                record = await result.single()\n                if record:\n                    try:\n                        result = dict(record[\"edge_properties\"])\n                        logger.info(f\"Result: {result}\")\n                        # Ensure required keys exist with defaults\n                        required_keys = {\n                            \"weight\": 0.0,\n                            \"source_id\": None,\n                            \"description\": None,\n                            \"keywords\": None,\n                        }\n                        for key, default_value in required_keys.items():\n                            if key not in result:\n@@ -305,20 +307,20 @@ class Neo4JStorage(BaseGraphStorage):\n                            f\"and {entity_name_label_target}: {str(e)}\"\n                        )\n                        # Return default edge properties on error\n                        return {\"weight\": 0.0, \"description\": None, \"keywords\": None, \"source_id\": None}\n\n                logger.debug(\n                    f\"{inspect.currentframe().f_code.co_name}: No edge found between {entity_name_label_source} and {entity_name_label_target}\"\n                )\n                # Return default edge properties when no edge found\n                return {\"weight\": 0.0, \"description\": None, \"keywords\": None, \"source_id\": None}\n\n        except Exception as e:\n            logger.error(\n                f\"Error in get_edge between {source_node_id} and {target_node_id}: {str(e)}\"\n            )\n            # Return default edge properties on error\n            return {\"weight\": 0.0, \"description\": None, \"keywords\": None, \"source_id\": None}\n\n    async def get_node_edges(self, source_node_id: str) -> list[tuple[str, str]] | None:\n        node_label = source_node_id.strip('\"')\n```"
      },
      {
        "user": "spo0nman",
        "created_at": "2025-02-17T17:33:31Z",
        "body": "With your changes i get a TypeError because `source_id` is None when trying to split it. We have to modify the `get_edge` method to ensure `source_id` is always a string, even when empty.\n\n\n\n`    async def get_edge(\n        self, source_node_id: str, target_node_id: str\n    ) -> dict[str, str] | None:\n        try:\n            entity_name_label_source = source_node_id.strip('\"')\n            entity_name_label_target = target_node_id.strip('\"')\n\n            async with self._driver.session(database=self._DATABASE) as session:\n                query = f\"\"\"\n                MATCH (start:`{entity_name_label_source}`)-[r]->(end:`{entity_name_label_target}`)\n                RETURN properties(r) as edge_properties\n                LIMIT 1\n                \"\"\".format(\n                    entity_name_label_source=entity_name_label_source,\n                    entity_name_label_target=entity_name_label_target,\n                )\n\n                result = await session.run(query)\n                record = await result.single()\n                if record and \"edge_properties\" in record:\n                    try:\n                        result = dict(record[\"edge_properties\"])\n                        logger.info(f\"Result: {result}\")\n                        # Ensure required keys exist with defaults\n                        required_keys = {\n                            \"weight\": 0.0,\n                            \"source_id\": \"\",  # Changed from None to empty string\n                            \"target_id\": \"\",  # Changed from None to empty string\n                            \"description\": \"\",\n                            \"keywords\": \"\",\n                        }\n                        for key, default_value in required_keys.items():\n                            if key not in result or result[key] is None:  # Also check for None values\n                                result[key] = default_value\n                                logger.warning(\n                                    f\"Edge between {entity_name_label_source} and {entity_name_label_target} \"\n                                    f\"missing {key}, using default: {default_value}\"\n                                )\n\n                        logger.debug(\n                            f\"{inspect.currentframe().f_code.co_name}:query:{query}:result:{result}\"\n                        )\n                        return result\n                    except (KeyError, TypeError, ValueError) as e:\n                        logger.error(\n                            f\"Error processing edge properties between {entity_name_label_source} \"\n                            f\"and {entity_name_label_target}: {str(e)}\"\n                        )\n                        # Return default edge properties on error\n                        return {\n                            \"weight\": 0.0,\n                            \"source_id\": \"\",\n                            \"target_id\": \"\",\n                            \"description\": \"\",\n                            \"keywords\": \"\"\n                        }\n\n                logger.debug(\n                    f\"{inspect.currentframe().f_code.co_name}: No edge found between {entity_name_label_source} and {entity_name_label_target}\"\n                )\n                # Return default edge properties when no edge found\n                return {\n                    \"weight\": 0.0,\n                    \"source_id\": \"\",\n                    \"target_id\": \"\",\n                    \"description\": \"\",\n                    \"keywords\": \"\"\n                }\n\n        except Exception as e:\n            logger.error(\n                f\"Error in get_edge between {source_node_id} and {target_node_id}: {str(e)}\"\n            )\n            # Return default edge properties on error\n            return {\n                \"weight\": 0.0,\n                \"source_id\": \"\",\n                \"target_id\": \"\",\n                \"description\": \"\",\n                \"keywords\": \"\"\n            }`"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T17:36:15Z",
        "body": "Thanks a lot."
      }
    ]
  },
  {
    "number": 791,
    "title": "Refactor File Indexing for Background Asynchronous Processing",
    "created_at": "2025-02-15T14:55:14Z",
    "closed_at": "2025-02-16T14:07:31Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/791",
    "body": "This PR refactors the file indexing process in LightRAG server to leverage background asynchronous processing, improving the responsiveness of the API and overall user experience.\r\n\r\n**Key Changes:**\r\n\r\n*   **Asynchronous File Indexing:** Introduces asynchronous tasks for file processing, allowing the API to return immediately while indexing occurs in the background.\r\n*   **Temporary File Handling:** Implements a temporary file mechanism for uploaded files, ensuring proper handling and cleanup during asynchronous indexing.\r\n*   **Improved Error Handling:** Enhances error handling and logging throughout the file indexing process, providing better insights into potential issues.\r\n*   **Response Handling:** Updates API responses to provide immediate feedback to the user, indicating that file processing is in progress.\r\n*   **Consolidated File Parsing Logic:** Identifies and merges duplicated file parsing implementations across different API endpoints into a single, reusable function.\r\n\r\n**Benefits:**\r\n\r\n*   **Improved Responsiveness:** The API remains responsive during file indexing, preventing delays for other requests.\r\n*   **Enhanced User Experience:** Users receive immediate feedback upon uploading files, providing a smoother and more intuitive experience.\r\n*   **Robust Error Handling:** Comprehensive error handling and logging provide better insights into potential issues, facilitating troubleshooting and maintenance.\r\n*   **Reduced Code Duplication:**  Consolidation of file parsing logic reduces code duplication, improving code maintainability and reducing the risk of inconsistencies.\r\n\r\n**Detailed Changes:**\r\n\r\n*   Replaced synchronous file indexing with asynchronous tasks using `BackgroundTasks` in FastAPI.\r\n*   Introduced temporary file handling for uploaded files, saving them to a temporary location before indexing.\r\n*   Implemented error handling and logging for file processing, capturing exceptions and providing detailed error messages.\r\n*   Updated API responses to provide immediate feedback to the user, indicating that file processing is in progress.\r\n*   Implemented cleanup of temporary files after successful or failed indexing.\r\n*   **Extracted common file parsing code into a dedicated function (e.g., `parse_file_content`) and reused it across the `/documents/upload`, `/documents/file`, and `/documents/batch` endpoints.**\r\n*   **Removed duplicated parsing logic from individual endpoint handlers.**\r\n\r\n**Additional Notes:**\r\n\r\n*   Consider testing the changes thoroughly to ensure they function as expected and do not introduce any regressions.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/791/comments",
    "author": "ArnoChenFx",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-15T22:45:28Z",
        "body": "@ArnoChenFx by the way, thank you very much for this PR.\r\n\r\nIf you can apply, my feedback could be really awesome."
      },
      {
        "user": "ArnoChenFx",
        "created_at": "2025-02-16T13:33:01Z",
        "body": "> @ArnoChenFx by the way, thank you very much for this PR.\r\n> \r\n> If you can apply, my feedback could be really awesome.\r\n\r\nApplied already, thanks for your suggestion.\r\n"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-16T13:55:23Z",
        "body": "Super thanks a lot !"
      }
    ]
  },
  {
    "number": 789,
    "title": "ËØ∑ÈóÆÊúâÂÖ®Â•óÁöÑ‰∏≠ÊñáÁöÑÈ°πÁõÆËØ¥ÊòéÂêóÔºü",
    "created_at": "2025-02-15T08:51:24Z",
    "closed_at": "2025-02-19T09:27:04Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/789",
    "body": "Â¶ÇÈ¢òÔºåËôΩÁÑ∂Ëã±Êñá‰πüÂèØ‰ª•Ôºå‰ΩÜÊòØ‰∏≠ÊñáÁöÑËµÑÊñôÔºåÁõ∏‰ø°‰ΩúËÄÖÂíåËØªËÄÖÈÉΩÂèØ‰ª•Êõ¥Â•ΩÔºåÊõ¥ÂáÜÁ°ÆÁöÑÁêÜËß£",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/789/comments",
    "author": "sparkzzt",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T20:16:35Z",
        "body": "ÊàëËÆ§‰∏∫ @LarFii ÂèØ‰ª•ÂÅöÂà∞„ÄÇ"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-18T18:46:47Z",
        "body": "@LarFii Could you have a look ? Thanks"
      },
      {
        "user": "LarFii",
        "created_at": "2025-02-19T06:21:21Z",
        "body": "Ê≤°ÈóÆÈ¢òÔºåËøá‰∏ÄÊÆµÊó∂Èó¥Êàë‰ºöÊï¥ÁêÜ‰∏Ä‰∏™‰∏≠ÊñáÁöÑreadme"
      }
    ]
  },
  {
    "number": 762,
    "title": "Rename of the STORAGE dict key",
    "created_at": "2025-02-13T06:52:54Z",
    "closed_at": "2025-02-17T18:01:46Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/762",
    "body": "In lightrag module\n\nSTORAGES dict key defined for  Milvus is \"MilvusVectorDBStorge\": \".kg.milvus_impl\",\n\nit should \"_MilvusVectorDBStor**a**ge_\"",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/762/comments",
    "author": "satyapriyas4u",
    "comments": [
      {
        "user": "ArnoChenFx",
        "created_at": "2025-02-13T14:33:00Z",
        "body": "fixed"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T18:01:44Z",
        "body": "Thanks @ArnoChenFx \n"
      }
    ]
  },
  {
    "number": 757,
    "title": "ERROR:lightrag:Error while deleting document : 'JsonKVStorage' object has no attribute 'get'",
    "created_at": "2025-02-12T17:36:13Z",
    "closed_at": "2025-02-14T01:42:44Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/757",
    "body": "Currently, the following code does not work: `rag.delete_by_doc_id(doc_id=\"doc-e8d7db6e5bcf7ec440ee8831e5842882\")` when using JsonKVStorage class.\n\nTerminal output: \n```\nLightRAG$ python lightrag_test.py \nINFO:nano-vectordb:Load (26, 3072) data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './sample_dir_2/vdb_entities.json'} 26 data\nINFO:nano-vectordb:Load (25, 3072) data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './sample_dir_2/vdb_relationships.json'} 25 data\nINFO:nano-vectordb:Load (3, 3072) data\nINFO:nano-vectordb:Init {'embedding_dim': 3072, 'metric': 'cosine', 'storage_file': './sample_dir_2/vdb_chunks.json'} 3 data\nINFO:lightrag:Loaded document status storage with 2 records\nERROR:lightrag:Error while deleting document doc-e8d7db6e5bcf7ec440ee8831e5842882: 'JsonDocStatusStorage' object has no attribute 'get'\n```\n\nBisected issue to this commit: `fe3050adcea36c67bac854f20faa762d6b187e8b` which gives error of `ERROR:lightrag:Error while deleting document doc-e8d7db6e5bcf7ec440ee8831e5842882: 'JsonDocStatusStorage' object has no attribute 'filter'`",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/757/comments",
    "author": "fatehss",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-13T19:42:03Z",
        "body": "Pull request is coming"
      }
    ]
  },
  {
    "number": 753,
    "title": "ÊèíÂÖ•Êï∞ÊçÆÊó†Ê≥ïÊäΩÂèñÂÆû‰ΩìÂíåÂÖ≥Á≥ª",
    "created_at": "2025-02-12T06:02:29Z",
    "closed_at": "2025-02-17T22:32:30Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/753",
    "body": "INFO:Processing 1 new unique documents\nProcessing batch 1:   0%|                                                                        | 0/1 [00:00<?, ?it/s]INFO:Inserting 0 vectors to chunks\nWARNING:You insert an empty data to vector DB\n                                                                         WARNING:Didn't extract any entities and relationships, maybe your LLM is not working\nERROR:Failed to process document doc-d41d8cd98f00b204e9800998ecf8427e: Failed to extract entities and relationships\nTraceback (most recent call last):\n  File \"E:\\LightRAG\\LightRAG\\lightrag\\lightrag.py\", line 491, in ainsert\n    raise e\n  File \"E:\\LightRAG\\LightRAG\\lightrag\\lightrag.py\", line 460, in ainsert\n    raise Exception(\nException: Failed to extract entities and relationships",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/753/comments",
    "author": "effectzhang",
    "comments": [
      {
        "user": "effectzhang",
        "created_at": "2025-02-12T06:08:15Z",
        "body": "ÊµãËØïÂ§ßÊ®°ÂûãÂíåEmbeddingÊ®°ÂûãÈÉΩÂèØ‰ª•ËÅîÈÄö\n\nllm_model_func: I'm a large language model, so I don't have feelings like humans do. I exist to provide information and help answer questions. Is there something specific you'd like to know or discuss?\nembedding_func: [[-0.00341724 0.02748858 -0.04093162 ... -0.01901607 -0.01260521\n-0.00622259]]\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens_sd\\vdb_entities.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens_sd\\vdb_relationships.json'} 0 data\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './dickens_sd\\vdb_chunks.json'} 0 data\nINFO:lightrag:Loaded document status storage with 1 records"
      },
      {
        "user": "KaymeKaydex",
        "created_at": "2025-02-12T12:13:22Z",
        "body": "nano vectordb is very bad as production ready solution, we had the same bugs, try to use croma instead"
      }
    ]
  },
  {
    "number": 750,
    "title": "Integrate gemini client into Lightrag",
    "created_at": "2025-02-11T18:17:44Z",
    "closed_at": "2025-02-12T05:39:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/750",
    "body": "I integrated Gemini client into LightRAG. So users can now use Gemini too to query using LightRAG.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/750/comments",
    "author": "gurjot-05",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-12T05:39:04Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 743,
    "title": "new error in document processing",
    "created_at": "2025-02-10T21:01:04Z",
    "closed_at": "2025-02-17T18:02:23Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/743",
    "body": "\nHi everypdy and great thanks for this excellent work! \n\nafter updating my Lightrag environment today the following error occurs in a seemingly random and not reproducible way. \n\n```\nFile \"[...]/[project]/myingestscript.py\", line 207, in fetch_item\n  rag.insert(myingeststuff)\nFile \"[...]/LightRAG/lightrag/lightrag.py\", line 376, in insert\n  return loop.run_until_complete(\n         ^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/[...]/anaconda3/envs/[project]/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n  return future.result()\n         ^^^^^^^^^^^^^^^\nFile \"[...]/LightRAG/lightrag/lightrag.py\", line 396, in ainsert\n  await self.apipeline_process_enqueue_documents(\nFile \"[...]/LightRAG/lightrag/lightrag.py\", line 515, in apipeline_process_enqueue_documents\n  failed_docs = await self.doc_status.get_failed_docs()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"[...]/LightRAG/lightrag/kg/jsondocstatus_impl.py\", line 98, in get_failed_docs\n  return {\n         ^\nFile \"[...]/LightRAG/lightrag/kg/jsondocstatus_impl.py\", line 99, in <dictcomp>\n  k: DocProcessingStatus(**v)\n\nTypeError: DocProcessingStatus.__init__() missing 4 required positional arguments: 'content', 'content_summary', 'content_length', and 'created_at'\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/743/comments",
    "author": "TimmLehmberg",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-11T13:56:36Z",
        "body": "This bug has been fixed in the latest code."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T18:02:21Z",
        "body": "@LarFii Thanks\n"
      }
    ]
  },
  {
    "number": 742,
    "title": "Feature Request: Allow Prompt Input in Mode Mix",
    "created_at": "2025-02-10T12:54:39Z",
    "closed_at": "2025-02-19T20:30:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/742",
    "body": "Hello,\n\nCurrently, in Mode Mix, it is not possible to directly pass a prompt as input. This limitation restricts flexibility in generating mixed outputs dynamically.\n\nCould you consider adding support for prompt input in Mode Mix? This would enable more customized and controlled output generation, improving usability for various applications.\n\nLooking forward to your feedback.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/742/comments",
    "author": "FeHuynhVI",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T20:30:19Z",
        "body": "Hi, you can use the last version you can."
      }
    ]
  },
  {
    "number": 741,
    "title": "fix(postgres): update document status with partial update instead of ‚Ä¶",
    "created_at": "2025-02-10T12:07:25Z",
    "closed_at": "2025-02-11T14:14:52Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/741",
    "body": "### Fix: Document Status Update & Content Insertion for PostgreSQL\r\n\r\n### Description\r\nThis PR fixes two key issues in PostgreSQL storage:\r\n\r\n- Document content was not being properly stored due to incorrect data handling.\r\n- Status updates were overwriting all fields instead of modifying only status, chunks_count, and updated_at.\r\n\r\n### Changes\r\n\r\n- Fixed content insertion issue in PGDocStatusStorage.\r\n- Updated status handling to prevent unnecessary field overwrites.\r\n- Ensured proper datetime handling in PostgreSQL queries.\r\n\r\n### Testing\r\n\r\n- Verified that document content is now correctly stored.\r\n- Checked that status updates only modify relevant fields.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/741/comments",
    "author": "Brenon28",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-11T14:15:08Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 734,
    "title": "Fixed Pipe Insert & Code Cleanup",
    "created_at": "2025-02-08T21:58:09Z",
    "closed_at": "2025-02-09T15:29:17Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/734",
    "body": "This pull request addresses the issue with the Pipe insert and ensures proper functioning of the following pipeline processes:\r\n\r\napipeline_process_documents\r\n\r\napipeline_process_chunks\r\n\r\napipeline_process_extract_graph\r\n\r\nAdditionally, the code has been cleaned up to improve clarity and maintainability, with type annotations added for better readability and robustness.\r\n\r\nAnd process more document in Async way.\r\n----\r\n\r\n**Remarks**\r\n\r\n@LarFii \r\n\r\nI have a concern regarding the project's focus.  \r\n\r\nInitially, the primary goal was RAG Graph, but I notice an increasing number of unnecessary tool integrations, such as openwebui_tool and others etc. These additions may lead to a loss of focus and introduce extra maintenance overhead.\r\n\r\nWould it be beneficial to reassess and streamline the scope to ensure the project remains aligned with its core objective?\r\n\r\nAnd also if writing test is in the scope to not have regression.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/734/comments",
    "author": "YanSte",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-09T15:17:03Z",
        "body": "Thank you so much for your contribution. After reviewing, I will merge it as soon as possible. As for the tool integrations, they are intended to make LightRAG more convenient and versatile to use. Regarding the RAG capabilities, I‚Äôm currently working on developing a new RAG system. Thanks again!"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-09T15:53:51Z",
        "body": "@LarFii did you test my pull request ?\n\nBecause I just overview the test.\n\nJust to be sure."
      }
    ]
  },
  {
    "number": 724,
    "title": "feat: trimming the model‚Äôs reasoning",
    "created_at": "2025-02-06T19:58:28Z",
    "closed_at": "2025-02-07T05:07:28Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/724",
    "body": "#### Using \"Thinking\" Models (e.g., DeepSeek)\r\n\r\nTo return only the model's response, you can pass `reasoning_tag` in `llm_model_kwargs`.\r\n\r\nFor example, for DeepSeek models, `reasoning_tag` should be set to `think`.\r\n\r\nThis may be useful to handle responses such as \r\n\r\n```\r\n<think>\r\n\r\n</think>\r\n\r\nHello! How can I assist you today? üòä\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/724/comments",
    "author": "ultrageopro",
    "comments": [
      {
        "user": "albertyou2",
        "created_at": "2025-02-07T06:48:32Z",
        "body": "ËÉΩÂàÜ‰∫´‰∏Ä‰∏ãdeepseek‰ΩøÁî®ÁöÑ‰æãÂ≠ê‰ª£Á†Å‰πàÔºåË∞¢Ë∞¢"
      }
    ]
  },
  {
    "number": 721,
    "title": "Postgres Query Error - Ollama and Postgres",
    "created_at": "2025-02-06T09:38:48Z",
    "closed_at": "2025-02-19T20:29:45Z",
    "labels": [
      "bug",
      "postgres"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/721",
    "body": "Traceback (most recent call last):\n  File \"/Users/apple/Documents/Projects/DeepStreet/llm-call/agents/lightRAG.py\", line 114, in <module>\n    asyncio.run(main())\n  File \"/Users/apple/.pyenv/versions/3.10.12/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Users/apple/.pyenv/versions/3.10.12/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/apple/Documents/Projects/DeepStreet/llm-call/agents/lightRAG.py\", line 80, in main\n    await rag.aquery(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/lightrag.py\", line 919, in aquery\n    response = await kg_query(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/operate.py\", line 618, in kg_query\n    context = await _build_query_context(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/operate.py\", line 997, in _build_query_context\n    ll_data, hl_data = await asyncio.gather(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/operate.py\", line 1059, in _get_node_data\n    results = await entities_vdb.query(query, top_k=query_param.top_k)\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 419, in query\n    results = await self.db.query(sql, params=params, multirows=True)\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/lightrag/kg/postgres_impl.py\", line 111, in query\n    rows = await connection.fetch(sql, *params.values())\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 690, in fetch\n    return await self._execute(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 1864, in _execute\n    result, _ = await self.__execute(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 1961, in __execute\n    result, stmt = await self._do_execute(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 2004, in _do_execute\n    stmt = await self._get_statement(\n  File \"/Users/apple/.pyenv/versions/python-3.10.12/lib/python3.10/site-packages/asyncpg/connection.py\", line 432, in _get_statement\n    statement = await self._protocol.prepare(\n  File \"asyncpg/protocol/protocol.pyx\", line 165, in prepare\nasyncpg.exceptions.PostgresSyntaxError: subquery in FROM must have an alias\nHINT:  For example, FROM (SELECT ...) [AS] foo.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/721/comments",
    "author": "Mcode2020",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T20:29:43Z",
        "body": "Hi, Please update with the last version.\n\n"
      }
    ]
  },
  {
    "number": 715,
    "title": "Fix: AttributeError in NanoVectorDB Initialization",
    "created_at": "2025-02-05T08:12:34Z",
    "closed_at": "2025-02-07T05:12:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/715",
    "body": "Issue\r\nThe following error occurred during the initialization of NanoVectorDB in nano_vector_db_impl.py:\r\n\r\nFile \".../LightRAG/LightRAG/lightrag/kg/nano_vector_db_impl.py\", line 92, in __post_init__\r\n    self.embedding_func.embedding_dim, storage_file=self._client_file_name\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'function' object has no attribute 'embedding_dim'\r\n\r\nRoot Cause\r\nThe embedding_func variable was declared locally instead of being assigned to self.embedding_func.\r\nAs a result, self.embedding_func was referring to a function instead of an instance of EmbeddingFunc, causing the AttributeError.\r\n\r\nChanges Made\r\n1. Explicitly Initialized self.embedding_func in __post_init__ and query using:\r\n\r\nself.embedding_func = EmbeddingFunc(\r\n    embedding_dim=4096, max_token_size=8192, func=gpt_4o_mini_complete\r\n)\r\n\r\n2. Imported EmbeddingFunc and gpt_4o_mini_complete to avoid reference errors.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/715/comments",
    "author": "Vasundhhara",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-05T12:42:38Z",
        "body": "Thanks for your contribution!  But there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass."
      },
      {
        "user": "danielaskdd",
        "created_at": "2025-02-05T15:30:12Z",
        "body": "@LarFii the embedding function is setup by post_init of LightRAG according to user preference. It can not be set to a fix method in the DB implementation."
      },
      {
        "user": "ArnoChenFx",
        "created_at": "2025-02-05T15:35:53Z",
        "body": "It's not a good idea to create the embedding_func in NanoVectorDBStorage. The embedding_func of VectorDB is automatically passed in by LightRAG. You only need to pass the embedding_func as a parameter when creating LightRAG.\r\n\r\n```python\r\nfrom lightrag.llm.openai import gpt_4o_complete, openai_embed\r\n\r\nrag = LightRAG(\r\n    llm_model_func=gpt_4o_complete,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1024, \r\n        max_token_size=8192, \r\n        func=lambda texts: openai_embed(\r\n            texts,\r\n            model=\"text-embedding-3-small\"\r\n        )\r\n    )\r\n)\r\n```"
      },
      {
        "user": "LarFii",
        "created_at": "2025-02-05T18:04:39Z",
        "body": "> @LarFii the embedding function is setup by post_init of LightRAG according to user preference. It can not be set to a fix method in the DB implementation.\r\n\r\nOh, I got it. Thanks!"
      },
      {
        "user": "Vasundhhara",
        "created_at": "2025-02-06T08:12:16Z",
        "body": "@ArnoChenFx This solved the error without having to initialize the embedding_func in NanoVectorDBStorage. Thanks."
      }
    ]
  },
  {
    "number": 713,
    "title": "lightrag-viewer not working for MacOS",
    "created_at": "2025-02-05T05:20:25Z",
    "closed_at": "2025-02-06T17:06:04Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/713",
    "body": "### Error Description\n\nAfter launching  lightrag-viewer, click on the \"Laod GraphML\" button, the program aborted immediately.\n\n### Error Log\n\n```\n‚ùØ lightrag-viewer\n2025-02-05 13:00:57.421 python3[11401:1260984] +[IMKClient subclass]: chose IMKClient_Modern\n2025-02-05 13:00:57.421 python3[11401:1260984] +[IMKInputSession subclass]: chose IMKInputSession_Modern\nTraceback (most recent call last):\n  File \"/Users/ydh/mycode/ai/LightRAG/venv/bin/lightrag-viewer\", line 8, in <module>\n    sys.exit(main())\n  File \"/Users/ydh/mycode/ai/LightRAG/lightrag/tools/lightrag_visualizer/graph_visualizer.py\", line 1222, in main\n    immapp.run(runner_params)\n  File \"/Users/ydh/mycode/ai/LightRAG/venv/lib/python3.10/site-packages/imgui_bundle/_patch_runners_add_save_screenshot_param.py\", line 38, in patched_run\n    run_backup(*args, **kwargs)\nSystemError: nanobind::detail::nb_func_error_except(): exception could not be translated!\n```\n\n### Environment\n\n- OS: macOS 15.3\n- Python: 3.10.16\n\n### pip information\n\n```\n‚ùØ pip list\nPackage                   Version        Editable project location\n------------------------- -------------- -----------------------------\naccelerate                1.2.1\naenum                     3.1.15\naioboto3                  13.3.0\naiobotocore               2.16.0\naiofiles                  24.1.0\naiohappyeyeballs          2.4.4\naiohttp                   3.11.11\naioitertools              0.12.0\naiosignal                 1.3.2\nannotated-types           0.7.0\nanyio                     4.8.0\nanytree                   2.12.1\nascii_colors              0.5.0\nasttokens                 3.0.0\nasync-timeout             4.0.3\nasyncpg                   0.30.0\nattrs                     24.3.0\nautograd                  1.7.0\nbeartype                  0.18.5\nbeautifulsoup4            4.12.3\nboto3                     1.35.81\nbotocore                  1.35.81\nbrotlipy                  0.7.0\ncachetools                5.5.0\ncertifi                   2024.12.14\ncffi                      1.17.1\ncfgv                      3.4.0\ncharset-normalizer        3.4.1\nclick                     8.1.8\nconfigparser              7.1.0\ncontourpy                 1.3.1\ncryptography              44.0.0\ncycler                    0.12.1\ndecorator                 5.1.1\ndeepsearch-glm            1.0.0\ndill                      0.3.9\ndistlib                   0.3.9\ndistro                    1.9.0\ndnspython                 2.7.0\ndocling                   2.17.0\ndocling-core              2.16.1\ndocling-ibm-models        3.3.0\ndocling-parse             3.1.2\neasyocr                   1.7.2\net_xmlfile                2.0.0\nexceptiongroup            1.2.2\nexecuting                 2.1.0\nfastapi                   0.115.6\nfilelock                  3.16.1\nfiletype                  1.2.0\nfonttools                 4.55.3\nfrozenlist                1.5.0\nfsspec                    2024.12.0\ngensim                    4.3.3\nglcontext                 3.0.0\nglfw                      2.8.0\ngraspologic               3.4.1\ngraspologic-native        1.2.1\ngreenlet                  3.1.1\ngremlinpython             3.7.3\ngrpcio                    1.67.1\nh11                       0.14.0\nhnswlib                   0.8.0\nhttpcore                  1.0.7\nhttpx                     0.27.2\nhuggingface-hub           0.27.1\nhyppo                     0.4.0\nidentify                  2.6.5\nidna                      3.10\nimageio                   2.37.0\nimgui-bundle              1.6.2\nipython                   8.31.0\nisodate                   0.7.2\njedi                      0.19.2\nJinja2                    3.1.5\njiter                     0.8.2\njmespath                  1.0.1\njoblib                    1.4.2\njsonlines                 3.1.0\njsonpickle                4.0.1\njsonref                   1.1.0\njsonschema                4.23.0\njsonschema-specifications 2024.10.1\nkiwisolver                1.4.8\nlazy_loader               0.4\nlightrag-hku              1.1.5          /Users/ydh/mycode/ai/LightRAG\nllvmlite                  0.43.0\nlxml                      5.3.0\nmarkdown-it-py            3.0.0\nmarko                     2.1.2\nMarkupSafe                3.0.2\nmatplotlib                3.10.0\nmatplotlib-inline         0.1.7\nmdurl                     0.1.2\nmilvus-lite               2.4.11\nmoderngl                  5.12.0\nmpire                     2.10.2\nmpmath                    1.3.0\nmultidict                 6.1.0\nmultiprocess              0.70.17\nmunch                     4.0.0\nnano-vectordb             0.0.4.3\nneo4j                     5.27.0\nnest-asyncio              1.6.0\nnetworkx                  3.4.2\nninja                     1.11.1.3\nnodeenv                   1.9.1\nnumba                     0.60.0\nnumpy                     1.26.4\nollama                    0.4.6\nopenai                    1.59.7\nopencv-python-headless    4.11.0.86\nopenpyxl                  3.1.5\noracledb                  2.5.1\npackaging                 24.2\npandas                    2.2.3\nparso                     0.8.4\npatsy                     1.0.1\npexpect                   4.9.0\npillow                    10.4.0\npip                       25.0\npipmaster                 0.4.0\nplatformdirs              4.3.6\nPOT                       0.9.5\npre_commit                4.0.1\nprompt_toolkit            3.0.48\npropcache                 0.2.1\nprotobuf                  5.29.3\npsutil                    6.1.1\npsycopg                   3.2.3\npsycopg-binary            3.2.3\npsycopg-pool              3.2.4\nptyprocess                0.7.0\npure_eval                 0.2.3\npyclipper                 1.3.0.post6\npycparser                 2.22\npydantic                  2.10.5\npydantic_core             2.27.2\npydantic-settings         2.7.1\nPyGLM                     2.7.3\nPygments                  2.19.1\nPyJWT                     2.8.0\npymilvus                  2.5.3\npymongo                   4.10.1\nPyMySQL                   1.1.1\npynndescent               0.5.13\nPyOpenGL                  3.1.9\npyparsing                 3.2.1\nPyPDF2                    3.0.1\npypdfium2                 4.30.1\npython-bidi               0.6.3\npython-dateutil           2.9.0.post0\npython-docx               1.1.2\npython-dotenv             1.0.1\npython-louvain            0.16\npython-multipart          0.0.20\npython-pptx               1.0.2\npytz                      2024.2\npyvis                     0.3.2\nPyYAML                    6.0.2\nredis                     5.2.1\nreferencing               0.36.2\nregex                     2024.11.6\nrequests                  2.32.3\nrich                      13.9.4\nrpds-py                   0.22.3\nRtree                     1.3.0\ns3transfer                0.10.4\nsafetensors               0.5.2\nscikit-image              0.25.1\nscikit-learn              1.6.1\nscipy                     1.12.0\nseaborn                   0.13.2\nsemchunk                  2.2.2\nsetuptools                75.8.0\nshapely                   2.0.7\nshellingham               1.5.4\nsix                       1.17.0\nsmart-open                7.1.0\nsniffio                   1.3.1\nsoupsieve                 2.6\nSQLAlchemy                2.0.37\nsseclient-py              1.8.0\nstack-data                0.6.3\nstarlette                 0.41.3\nstatsmodels               0.14.4\nsympy                     1.13.3\ntabulate                  0.9.0\ntenacity                  9.0.0\nthreadpoolctl             3.5.0\ntifffile                  2025.1.10\ntiktoken                  0.8.0\ntk                        0.1.0\ntokenizers                0.19.1\ntorch                     2.2.2\ntorchvision               0.17.2\ntqdm                      4.67.1\ntraitlets                 5.14.3\ntransformers              4.42.4\ntyper                     0.12.5\ntyping_extensions         4.12.2\ntzdata                    2024.2\nujson                     5.10.0\numap-learn                0.5.7\nurllib3                   2.3.0\nuvicorn                   0.34.0\nvirtualenv                20.29.0\nwcwidth                   0.2.13\nwrapt                     1.17.1\nXlsxWriter                3.2.0\nxxhash                    3.5.0\nyarl                      1.18.3\nzhipuai                   2.1.5.20250106\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/713/comments",
    "author": "danielaskdd",
    "comments": [
      {
        "user": "ArnoChenFx",
        "created_at": "2025-02-05T16:22:11Z",
        "body": "fixed #718"
      },
      {
        "user": "danielaskdd",
        "created_at": "2025-02-05T16:38:03Z",
        "body": "Thanks :)"
      }
    ]
  },
  {
    "number": 709,
    "title": "Reapply \"Integrated the GraphML Visualizer as an optional component of LightRAG\"",
    "created_at": "2025-02-04T18:34:14Z",
    "closed_at": "2025-02-04T18:38:16Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/709",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/709/comments",
    "author": "ArnoChenFx",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-04T18:39:28Z",
        "body": "Thank you so much!"
      },
      {
        "user": "LarFii",
        "created_at": "2025-02-04T18:42:38Z",
        "body": "What I was unsure about is whether `extra/VisualizationTool/graph_visualizer.py` was handled correctly, because I noticed that there isn‚Äôt this file under `lightrag/tools/lightrag_visualizer`."
      },
      {
        "user": "LarFii",
        "created_at": "2025-02-04T18:45:54Z",
        "body": "Oh, I found it. It's in `init.py`. Do we need to change the file name?"
      },
      {
        "user": "LarFii",
        "created_at": "2025-02-04T18:50:53Z",
        "body": "I looked through the doc, and I think I understand it now. Thank you so much for your help!"
      }
    ]
  },
  {
    "number": 696,
    "title": "Add the ability to specify a path for saving lightrag.log",
    "created_at": "2025-02-02T11:07:43Z",
    "closed_at": "2025-02-03T16:12:49Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/696",
    "body": "When initializing LightRAG, you can specify `log_dir`, where `lightrag.log` will be saved.\r\nThe default value is the project‚Äôs root directory.\r\n\r\nThis can be useful if the project contains other log files that should be stored in a dedicated directory.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/696/comments",
    "author": "ultrageopro",
    "comments": [
      {
        "user": "varyakchau",
        "created_at": "2025-02-02T18:32:54Z",
        "body": "Please, review this pull request, I badly need this feature!"
      }
    ]
  },
  {
    "number": 692,
    "title": "Â¶ÇÊûú‰∏çÂ∏åÊúõ‰ΩøÁî®Incremental InsertÔºåÊØèÊ¨°ËØªÂèñÁöÑÊñáÊ°£ÊÉ≥ÂàÜÂºÄÔºåÈúÄË¶ÅÊÄé‰πàÂÅö",
    "created_at": "2025-02-01T11:39:35Z",
    "closed_at": "2025-02-17T22:32:54Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/692",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/692/comments",
    "author": "zldeng1984",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-04T19:24:05Z",
        "body": "ÂèØ‰ª•ÈÄöËøáÊõ¥Êç¢Â∑•‰ΩúÁõÆÂΩïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò"
      }
    ]
  },
  {
    "number": 686,
    "title": "Implement Faiss Vector Storage Support for LightRAG",
    "created_at": "2025-01-31T13:33:33Z",
    "closed_at": "2025-01-31T15:34:53Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/686",
    "body": "## Overview\r\nThis pull request introduces the integration of Faiss as a backend for vector storage in LightRAG, enhancing its capabilities for efficient similarity search and retrieval in large-scale environments. By utilizing Faiss's robust indexing features, LightRAG can now handle denser vector spaces and offer faster query response times.\r\n\r\n## Changes\r\n- **FaissVectorDBStorage Class**: A new storage class that utilizes Faiss for storing and retrieving vectors. Supports cosine similarity through inner product searches in normalized vector spaces.\r\n- **Embedding Functionality**: Integration of `sentence-transformers` for generating embeddings, with an option to switch to OpenAI embeddings for larger dimensions.\r\n- **Configuration Enhancements**: Added `vector_db_storage_cls_kwargs` to allow dynamic threshold settings for cosine similarity during initialization.\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/686/comments",
    "author": "gurjot-05",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-31T15:34:49Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 681,
    "title": "Improve prompts to avoid LLM make up answer",
    "created_at": "2025-01-30T17:02:06Z",
    "closed_at": "2025-01-31T15:32:54Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/681",
    "body": "Improve prompts to avoid make-up respond from LLM like qwen-plus when very long context is provided. \r\n\r\nThe original prompts, when the context length is sufficient, may cause certain models (such as qwen-plus) to deviate from the prompt's guidance and fabricate answers using their own knowledge. Sometimes, large language models switch to English instead of responding in the questioner's language. \r\n\r\nThe new prompts aim to strengthen the constraints and guidance capabilities over LLMs, and have achieved good results in actual testing.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/681/comments",
    "author": "danielaskdd",
    "comments": [
      {
        "user": "ParisNeo",
        "created_at": "2025-01-31T07:35:02Z",
        "body": "Great update."
      },
      {
        "user": "danielaskdd",
        "created_at": "2025-01-31T07:43:16Z",
        "body": "> Great update.\r\n\r\nThx :)"
      }
    ]
  },
  {
    "number": 673,
    "title": "Does lightrag have a corresponding web front-end? Can an API interface be provided for third-party calling?",
    "created_at": "2025-01-29T19:01:00Z",
    "closed_at": "2025-02-17T22:41:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/673",
    "body": "Does lightrag have a corresponding web front-end? Can an API interface be provided for third-party calling?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/673/comments",
    "author": "Twinhead880",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T22:41:32Z",
        "body": "Hi,\n\nYes."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T22:41:41Z",
        "body": "Please follow the doc folder."
      }
    ]
  },
  {
    "number": 664,
    "title": "the naive method need some kind of history now   when running lightrag_openai_demo.py",
    "created_at": "2025-01-28T03:07:14Z",
    "closed_at": "2025-02-17T22:45:14Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/664",
    "body": "`Traceback (most recent call last):\n  File \"/home/ginanjar/lightrag/lightrag_openai_demo.py\", line 50, in <module>\n    rag.query(query=\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n  File \"/home/ginanjar/.local/lib/python3.11/site-packages/lightrag/lightrag.py\", line 888, in query\n    return loop.run_until_complete(self.aquery(query, param))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/ginanjar/.local/lib/python3.11/site-packages/lightrag/lightrag.py\", line 910, in aquery\n    response = await naive_query(\n               ^^^^^^^^^^^^^^^^^^\n  File \"/home/ginanjar/.local/lib/python3.11/site-packages/lightrag/operate.py\", line 1538, in naive_query\n    sys_prompt = sys_prompt_temp.format(\n                 ^^^^^^^^^^^^^^^^^^^^^^^\nKeyError: 'history'`\n\n\nother methods (local, global, hybrid) works just fine\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/664/comments",
    "author": "gutama",
    "comments": [
      {
        "user": "Sunmark25",
        "created_at": "2025-02-11T05:31:22Z",
        "body": "already solved in version 1.1.5, try updating to the latest version of Lightrag"
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T22:45:12Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 662,
    "title": "AttributeError: 'function' object has no attribute 'embedding_dim' in \"lightrag_openai_demo.py\" (PLEASE EXPLAIN THE FIX)",
    "created_at": "2025-01-27T22:04:01Z",
    "closed_at": "2025-01-28T14:37:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/662",
    "body": "Hi\n\nI am getting the following error in the lightrag_openai_demo.py. Can someone please help me fix it?\n\nTraceback (most recent call last):\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\examples\\lightrag_openai_demo.py\", line 11, in <module>\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 32, in __init__\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\lightrag\\lightrag.py\", line 254, in __post_init__\n    self.entities_vdb = self.vector_db_storage_cls(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\lightrag\\lightrag.py\", line 80, in import_class\n    return cls(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 8, in __init__\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\lightrag\\kg\\nano_vector_db_impl.py\", line 84, in __post_init__\n    self.embedding_func.embedding_dim, storage_file=self._client_file_name\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'function' object has no attribute 'embedding_dim'",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/662/comments",
    "author": "ArpitNarain",
    "comments": [
      {
        "user": "napoleon9000",
        "created_at": "2025-01-28T01:31:32Z",
        "body": "Same here"
      },
      {
        "user": "gutama",
        "created_at": "2025-01-28T03:03:51Z",
        "body": "```\nimport os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm.openai import openai_embed  # or your custom embedding\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,\n    # llm_model_func=gpt_4o_complete\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1536,\n        max_token_size=8192,\n        func=lambda texts: openai_embed(texts, model=\"text-embedding-3-small\")\n    )\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n\n# Perform naive search\n# print(\n#     rag.query(query=\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\", ))\n# )\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\nprint(\n    rag.query(query=\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n```\n\n\nshould be work\n"
      },
      {
        "user": "ArpitNarain",
        "created_at": "2025-01-28T14:37:05Z",
        "body": "Thank you so much, it worked."
      }
    ]
  },
  {
    "number": 655,
    "title": "Fix: history_turns variable name in kg_query function",
    "created_at": "2025-01-26T16:08:53Z",
    "closed_at": "2025-01-27T07:13:07Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/655",
    "body": "## Description\r\nFixed an AttributeError in `kg_query` function where it was trying to access `history_window_size` when the `QueryParam` class uses `history_turns` as the variable name.\r\n\r\n## Issue\r\nWhen using the RAG query functionality with chat history, the following error occurs:\r\n\r\n```python\r\nAttributeError: 'QueryParam' object has no attribute 'history_window_size'\r\n```\r\n\r\n## Fix\r\nUpdated the variable reference in `lightrag/operate.py` to use `history_turn` instead of `history_window_size` to match the `QueryParam` class implementation.\r\n\r\n## Testing\r\n- Tested the chat functionality with conversation history\r\n- Verified that the query parameter correctly processes the history turns",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/655/comments",
    "author": "usr9",
    "comments": [
      {
        "user": "danielaskdd",
        "created_at": "2025-01-26T18:50:56Z",
        "body": "This issue already fixed in PR #650 "
      }
    ]
  },
  {
    "number": 651,
    "title": "Retain only metadata of document",
    "created_at": "2025-01-25T15:35:10Z",
    "closed_at": "2025-02-19T20:26:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/651",
    "body": "Is there a way to keep only metadata without keeping a copy of the document?\n\nFrom the implementation, it seems that only the metadata of the document is always needed, so it would be nice to reduce the amount of space consumed by lightrag if it is not needed.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/651/comments",
    "author": "soichisumi",
    "comments": [
      {
        "user": "ParisNeo",
        "created_at": "2025-01-27T07:35:00Z",
        "body": "I don't see why it wouldn't be opssible. In fact If you remove the document from the input folder I think it stays in the graph."
      },
      {
        "user": "soichisumi",
        "created_at": "2025-01-27T09:43:03Z",
        "body": "lightrag saves the loaded documents as kv_store_full_docs.json.\nMy understanding is that lightrag also stores document metadata in `kv_store_full_docs.json`, so deleting it would cause problems.\n\nAlso, since deleting this file would be manual, I still think it would be better to have an option to delete it after loading, even the file is deletable."
      }
    ]
  },
  {
    "number": 648,
    "title": "Problem with rag.ainsert in PostgreSQL in new 1.1.4 build",
    "created_at": "2025-01-25T12:39:19Z",
    "closed_at": "2025-02-05T04:12:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/648",
    "body": "Hi,\nLightRAG 1.1.3 was working fine with PostgreSQL example, i just updated repo 1.1.4 and i started getting errors:\n\nNotImplementedError                       Traceback (most recent call last)\nCell In[4], line 36\n     32 rag.chunk_entity_relation_graph.embedding_func = rag.embedding_func\n     35 with open(\"/Users/test/k2graphrag/LightRAG/inputdata/1-19.txt\") as f:\n---> 36     rag.ainsert(f.read())\n\nFile ~/k2graphrag/LightRAG/lightrag/lightrag.py:324, in LightRAG.insert(self, string_or_strings, split_by_character, split_by_character_only)\n    320 def insert(\n    321     self, string_or_strings, split_by_character=None, split_by_character_only=False\n    322 ):\n    323     loop = always_get_an_event_loop()\n--> 324     return loop.run_until_complete(\n    325         self.ainsert(string_or_strings, split_by_character, split_by_character_only)\n    326     )\n\nFile /opt/anaconda3/envs/lightrag/lib/python3.11/site-packages/nest_asyncio.py:98, in _patch_loop.<locals>.run_until_complete(self, future)\n     95 if not f.done():\n     96     raise RuntimeError(\n     97         'Event loop stopped before Future completed.')\n---> 98 return f.result()\n\nFile /opt/anaconda3/envs/lightrag/lib/python3.11/asyncio/futures.py:203, in Future.result(self)\n    201 self.__log_traceback = False\n    202 if self._exception is not None:\n--> 203     raise self._exception.with_traceback(self._exception_tb)\n    204 return self._result\n\nFile /opt/anaconda3/envs/lightrag/lib/python3.11/asyncio/tasks.py:277, in Task.__step(***failed resolving arguments***)\n    273 try:\n    274     if exc is None:\n    275         # We use the `send` method directly, because coroutines\n    276         # don't have `__iter__` and `__next__` methods.\n--> 277         result = coro.send(None)\n    278     else:\n    279         result = coro.throw(exc)\n\nFile ~/k2graphrag/LightRAG/lightrag/lightrag.py:361, in LightRAG.ainsert(self, string_or_strings, split_by_character, split_by_character_only)\n    347 new_docs = {\n    348     compute_mdhash_id(content, prefix=\"doc-\"): {\n    349         \"content\": content,\n   (...)\n    356     for content in unique_contents\n    357 }\n    359 # 3. Filter out already processed documents\n    360 # _add_doc_keys = await self.doc_status.filter_keys(list(new_docs.keys()))\n--> 361 _add_doc_keys = {\n    362     doc_id\n    363     for doc_id in new_docs.keys()\n    364     if (current_doc := await self.doc_status.get_by_id(doc_id)) is None\n    365     or current_doc[\"status\"] == DocStatus.FAILED\n    366 }\n    367 new_docs = {k: v for k, v in new_docs.items() if k in _add_doc_keys}\n    369 if not new_docs:\n\nFile ~/k2graphrag/LightRAG/lightrag/lightrag.py:364, in <setcomp>(.0)\n    347 new_docs = {\n    348     compute_mdhash_id(content, prefix=\"doc-\"): {\n    349         \"content\": content,\n   (...)\n    356     for content in unique_contents\n    357 }\n    359 # 3. Filter out already processed documents\n    360 # _add_doc_keys = await self.doc_status.filter_keys(list(new_docs.keys()))\n    361 _add_doc_keys = {\n    362     doc_id\n    363     for doc_id in new_docs.keys()\n--> 364     if (current_doc := await self.doc_status.get_by_id(doc_id)) is None\n    365     or current_doc[\"status\"] == DocStatus.FAILED\n    366 }\n    367 new_docs = {k: v for k, v in new_docs.items() if k in _add_doc_keys}\n    369 if not new_docs:\n\nFile ~/k2graphrag/LightRAG/lightrag/base.py:82, in BaseKVStorage.get_by_id(self, id)\n     81 async def get_by_id(self, id: str) -> Union[T, None]:\n---> 82     raise NotImplementedError\n\nNotImplementedError: ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/648/comments",
    "author": "k2ai",
    "comments": [
      {
        "user": "ShanGor",
        "created_at": "2025-02-03T08:50:14Z",
        "body": "should be fixed, please retry."
      },
      {
        "user": "k2ai",
        "created_at": "2025-02-05T04:12:05Z",
        "body": "Thanks Shan, it's resolved now."
      }
    ]
  },
  {
    "number": 647,
    "title": "feat:  Added webui management, including file upload, text upload, Q&‚Ä¶",
    "created_at": "2025-01-25T10:41:15Z",
    "closed_at": "2025-01-26T22:32:15Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/647",
    "body": "Added webui management, including file upload, text upload, Q&A query, graph database management (can view tags, view knowledge graph based on tags), system status (whether it is good, data storage status, model status, path),request /webui/index.html \r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/647/comments",
    "author": "18277486571HYB",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-25T14:40:04Z",
        "body": "Thanks for your contributions! It would be helpful to change all the comments to English so that other developers can easily work on the project in the future. Thank you again!"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-26T11:00:09Z",
        "body": "Hi, I can translate them if you want.\r\nAnd I think we can merge my webui with this one if you don't mind.\r\nI can do that after the pull request is merged."
      },
      {
        "user": "18277486571HYB",
        "created_at": "2025-01-26T11:24:56Z",
        "body": "WhateverÔºåyou can do"
      },
      {
        "user": "18277486571HYB",
        "created_at": "2025-01-26T11:26:32Z",
        "body": "Do you need any assistance from me?"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-26T16:12:09Z",
        "body": "I just need to wait for it to be accepted before I can fuse it so we can start from a clean slate.\r\n\r\nI took a look at your code, I think I can handle it without help :) .\r\nIf I really don't understand something I'll just ask you.\r\n\r\nThanks alot."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-26T23:21:07Z",
        "body": "Well I just checked it. In fact, it is not much more advanced than my webui. The things you add (the graph management) are not implemented so they don't work. Do you plan on adding implementation of that?\r\n\r\nAlso, you shouldn't use BASE_URL in the lightrag.js script as it is already served from a specific url and you don't need to use any BASI_URLK, just use /the end point and it will work even if you change the server address.\r\n\r\nI'm keeping my webui, but I'll add some ideas from yours and i'll also add a placeholder so that you can implement graph visualization if you like that."
      }
    ]
  },
  {
    "number": 642,
    "title": "asyncio optimizations",
    "created_at": "2025-01-24T15:10:56Z",
    "closed_at": "2025-01-24T17:52:25Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/642",
    "body": "- Makes the hybrid mode 2x faster by executing local and global pipelines in parallel\r\n- Optimizes other pieces of code in the same way",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/642/comments",
    "author": "dimatill",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-24T17:52:21Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 638,
    "title": "csv error (utils/list_of_list_to_csv()) when rag is querying",
    "created_at": "2025-01-24T09:22:37Z",
    "closed_at": "2025-02-17T22:36:09Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/638",
    "body": "When performing rag.aquery(), an error occurs:\n\n```\nFile \"C:\\Users\\admin\\Documents\\RAG\\LightRAG\\lightrag\\lightrag.py\", line 888, in aquery\n    response = await kg_query(\n  File \"C:\\Users\\admin\\Documents\\RAG\\LightRAG\\lightrag\\operate.py\", line 660, in kg_query\n    context = await _build_query_context(\n  File \"C:\\Users\\admin\\Documents\\RAG\\LightRAG\\lightrag\\operate.py\", line 964, in _build_query_context\n    ) = await _get_node_data(\n  File \"C:\\Users\\admin\\Documents\\RAG\\LightRAG\\lightrag\\operate.py\", line 1112, in _get_node_data\n    text_units_context = list_of_list_to_csv(text_units_section_list)\n  File \"C:\\Users\\admin\\Documents\\RAG\\LightRAG\\lightrag\\utils.py\", line 227, in list_of_list_to_csv\n    writer.writerows(data)\n_csv.Error: need to escape, but no escapechar set\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/638/comments",
    "author": "madlogos",
    "comments": [
      {
        "user": "AurYou",
        "created_at": "2025-02-10T11:35:42Z",
        "body": "Got the same error. Any idea to correct? "
      },
      {
        "user": "madlogos",
        "created_at": "2025-02-10T14:26:46Z",
        "body": "I revised `list_of_list_to_csv()` in operate.py and it seems to work.\n\n```python\ndef list_of_list_to_csv(data: List[List[str]]) -> str:\n¬† ¬† output = io.StringIO()\n¬† ¬† writer = csv.writer(output, escapechar=\"|\")\n¬† ¬† writer.writerows(data)\n¬† ¬† return output.getvalue()\n```"
      },
      {
        "user": "AurYou",
        "created_at": "2025-02-10T16:41:58Z",
        "body": "works for me !!!"
      }
    ]
  },
  {
    "number": 631,
    "title": "feat: Êñ∞Â¢ûiniÊñá‰ª∂ËØªÂèñÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÊñπÂºèÔºåÊñπ‰æøÁîü‰∫ßÁéØÂ¢ÉÔºå‰øÆÊîπLightrag ainsertÊñπÊ≥ï_add_doc_keysËé∑ÂèñÊñπÂºèÔºåÂéü‚Ä¶",
    "created_at": "2025-01-23T15:00:28Z",
    "closed_at": "2025-01-24T17:44:46Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/631",
    "body": "feat: Êñ∞Â¢ûiniÊñá‰ª∂ËØªÂèñÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÊñπÂºèÔºåÊñπ‰æøÁîü‰∫ßÁéØÂ¢ÉÔºå‰øÆÊîπLightrag ainsertÊñπÊ≥ï_add_doc_keysËé∑ÂèñÊñπÂºèÔºåÂéüÊù•Âè™ËøáÊª§Â≠òÂú®ÁöÑÔºå‰ΩÜËøô‰ºöËÆ©Â§±Ë¥•ÁöÑÊñáÊ°£Êó†Ê≥ïÂÜçÊ¨°Â≠òÂÇ®ÔºåÊñ∞Â¢û--chunk_sizeÂíå--chunk_overlap_sizeÊñπ‰æøÁîü‰∫ßÁéØÂ¢ÉÔºåÊñ∞Â¢ûllm_bindingÔºöopenai-ollama Êñπ‰æøÁî®openaiÁöÑÂêåÊó∂‰ΩøÁî®ollama embedding \r\n\r\nÊúüÂæÖÊÇ®ÁöÑÂêàÂπ∂",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/631/comments",
    "author": "18277486571HYB",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-24T08:32:12Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÊÇ®ÁöÑË¥°ÁåÆÔºåÊúâ‰∏Ä‰∫õlinting errorsÈúÄË¶ÅÂú®Êèê‰∫§ÂâçËøêË°åpre-commit run --all-files‰øÆÂ§ç‰∏Ä‰∏ã„ÄÇÂè¶Â§ñÔºåÂèØ‰ª•ÊääÊ≥®Èáä‰øÆÊîπÊàêËã±ÊñáÂêóÔºåÊñπ‰æøÂÖ∂‰ªñÂºÄÂèëËÄÖËøõË°åÂêéÁª≠ÂºÄÂèë„ÄÇÂÜçÊ¨°ÊÑüË∞¢ÔºÅÔºÅ"
      }
    ]
  },
  {
    "number": 629,
    "title": "The expanded size of the tensor (1750) must match the existing size (512)",
    "created_at": "2025-01-23T06:50:01Z",
    "closed_at": "2025-02-18T20:30:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/629",
    "body": "\nhf_embedding Ê®°ÂûãÊòØ bge-large-zh-v1.5\n embedding_dim=1024\nllm_model_name ÊòØ chatglm-6b\nÊÄªÊòØÊä•Èîô The expanded size of the tensor (1750) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [32, 1750].  Tensor sizes: [1, 512]\nÊ±ÇÊïôÂ¶Ç‰ΩïËß£ÂÜ≥ÔºÅ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/629/comments",
    "author": "jiugeanan",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-18T20:30:55Z",
        "body": "You "
      }
    ]
  },
  {
    "number": 626,
    "title": "Delete in Postgresql",
    "created_at": "2025-01-22T11:25:57Z",
    "closed_at": "2025-01-24T03:59:57Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/626",
    "body": "Hi,\ni processed 800 plus files but in 2 files i got erros and I want to delete those documents using given delete code as below. But I am getting error:\n\n\n**ERROR:**\nINFO:Using the label default for PostgreSQL as identifier\nINFO:Connected to PostgreSQL database at localhost:5432/testdb\nINFO:Logger initialized for working directory: /Users/abc/LightRAG/inputdata\nINFO:Load KV json_doc_status_storage with 0 data\nINFO:Load KV llm_response_cache with 0 data\nINFO:Load KV full_docs with 0 data\nINFO:Load KV text_chunks with 0 data\nINFO:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '/Users/abc/LightRAG/inputdata/vdb_entities.json'} 0 data\nINFO:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '/Users/abc/LightRAG/inputdata/vdb_relationships.json'} 0 data\nINFO:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '/Users/abc/LightRAG/inputdata/vdb_chunks.json'} 0 data\nINFO:Loaded document status storage with 0 records\nWARNING:Document doc-9bba763eea0d295bc7ad357b2a70e974 not found\n\n**MY CODE:**\nROOT_DIR = os.environ.get(\"ROOT_DIR\")\nWORKING_DIR = f\"{ROOT_DIR}/inputdata\"\n\nlogging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\n# POSTGRESQL AGE EXTENTION\nos.environ[\"AGE_GRAPH_NAME\"] = \"testdbage\"\n\npostgres_db = PostgreSQLDB(\n    config={\n        \"host\": \"localhost\",\n        \"port\": 5432,\n        \"user\": \"postgres\",\n        \"password\": \"password\",\n        \"database\": \"testdb\",\n    }\n)\n\n\nawait postgres_db.initdb()\n# Check if PostgreSQL DB tables exist, if not, tables will be created\n\n\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1536,\n        max_token_size=8192,\n        func=lambda texts: openai_embedding(texts),\n    ),\n)\n\n\n\n#  Delete Document: Deleting entities and relationships associated with the document by doc id\nrag.delete_by_doc_id(\"doc-9bba763eea0d295bc7ad357b2a70e974\")",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/626/comments",
    "author": "k2ai",
    "comments": [
      {
        "user": "nickjfrench",
        "created_at": "2025-01-22T14:21:40Z",
        "body": "I think the delete_by_doc_id may be broken in multiple areas. I posted a similar issue when using Neo4j as the Graph DB. \n#619 "
      },
      {
        "user": "LarFii",
        "created_at": "2025-01-23T06:57:34Z",
        "body": "From the logs, it seems that no files were loaded, which suggests there might be an issue with the working directory path. Also, I am not familiar with other storage methods, and I've only implemented the delete functionality for nano db. Unfortunately, the other storage options are not supported at the moment. My apologies for the inconvenience."
      },
      {
        "user": "k2ai",
        "created_at": "2025-01-24T03:59:57Z",
        "body": "Thank you for the prompt response....\nPath and the working directory are fine because I am indexing with the same. As you said it might be an issue with PostgreSQL because you developed considering the nano database. I am hoping it will be developed for PostgreSQL as well.\n\nBy the way, I tested LightRAG with my data, and it is working very well. Thank you very much team behind LightRAG."
      }
    ]
  },
  {
    "number": 623,
    "title": "add qdrant as vectorDB",
    "created_at": "2025-01-22T03:00:56Z",
    "closed_at": "2025-02-13T10:16:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/623",
    "body": "Add qdrant adaptation under the kg directory, and add the corresponding ID generation method in utils.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/623/comments",
    "author": "wangm23456",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-22T07:57:19Z",
        "body": "Thank you very much for your valuable contribution! Could you please update the comments to English? This will help facilitate further development by others in the future. Also, there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass."
      }
    ]
  },
  {
    "number": 621,
    "title": "Build a graph viewer",
    "created_at": "2025-01-21T22:20:24Z",
    "closed_at": "2025-02-17T22:36:29Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/621",
    "body": "I'll build a 3D graph visualization tool for lightrag to allow viewing the graph and exploring it.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/621/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T22:36:29Z",
        "body": "Thanks !"
      }
    ]
  },
  {
    "number": 616,
    "title": "feat: Â¢ûÂä†redis KVÂ≠òÂÇ®ÔºåÂ¢ûÂä†openai+neo4j+milvus+redisÁöÑdemoÊµãËØïÔºåÊñ∞Â¢ûlightrag.py: R‚Ä¶",
    "created_at": "2025-01-21T14:08:05Z",
    "closed_at": "2025-01-23T03:49:57Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/616",
    "body": "feat: Â¢ûÂä†redis KVÂ≠òÂÇ®ÔºåÂ¢ûÂä†openai+neo4j+milvus+redisÁöÑdemoÊµãËØïÔºåÊñ∞Â¢ûlightrag.py: RedisKVStorage,Êñ∞Â¢ûrequirementText: aioredis ÊúüÂæÖËÉΩÂêåÊÑèÂêàÂπ∂",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/616/comments",
    "author": "18277486571HYB",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-22T07:55:53Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÊÇ®ÁöÑË¥°ÁåÆÔºåÊúâ‰∏Ä‰∫õlinting errorsÈúÄË¶ÅÂú®Êèê‰∫§ÂâçËøêË°å`pre-commit run --all-files`‰øÆÂ§ç‰∏Ä‰∏ã"
      }
    ]
  },
  {
    "number": 614,
    "title": "Hi big hands, do you have a plan to support Elastic Search as storage (both vdb and kv)?",
    "created_at": "2025-01-21T10:48:53Z",
    "closed_at": "2025-02-19T20:25:58Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/614",
    "body": "Or anyone in the community has such a plan?\nMany thanks!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/614/comments",
    "author": "madlogos",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T20:25:51Z",
        "body": "Hi, Not for the moment sorry. We secure first the project."
      }
    ]
  },
  {
    "number": 611,
    "title": "Running locally  - Information from Neo4j is not being retrieved when querying LightRAG",
    "created_at": "2025-01-20T18:40:43Z",
    "closed_at": "2025-02-17T22:44:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/611",
    "body": "Hi! I'm having an issue where when I query my LightRAG instance (with Neo4j as the database) no information is being retrieved unless I am uploading information in the same script. How can I remedy this? My local Neo4j instance is running and clearly LightRAG can connect to the instance - but only when I'm inserting data which baffles me a little bit haha. Any help would be appreciated!!\n\nMy implementation using Azure:\n\n```\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nimport numpy as np\nfrom dotenv import load_dotenv\nimport logging\nfrom openai import AzureOpenAI\n\nlogging.basicConfig(level=logging.INFO)\n\nload_dotenv()\n\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\n\nWORKING_DIR = \"./local_neo4jWorkDir\"\n\nif os.path.exists(WORKING_DIR):\n    import shutil\n\n    shutil.rmtree(WORKING_DIR)\n\nos.mkdir(WORKING_DIR)\n\n\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], **kwargs\n) -> str:\n    client = AzureOpenAI(\n        api_key=AZURE_OPENAI_API_KEY,\n        api_version=AZURE_OPENAI_API_VERSION,\n        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    )\n\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    if history_messages:\n        messages.extend(history_messages)\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    chat_completion = client.chat.completions.create(\n        model=AZURE_OPENAI_DEPLOYMENT,  # model = \"deployment_name\".\n        messages=messages,\n        temperature=kwargs.get(\"temperature\", 0),\n        top_p=kwargs.get(\"top_p\", 1),\n        n=kwargs.get(\"n\", 1),\n    )\n    return chat_completion.choices[0].message.content\n\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    client = AzureOpenAI(\n        api_key=AZURE_OPENAI_API_KEY,\n        api_version=AZURE_EMBEDDING_API_VERSION,\n        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    )\n    embedding = client.embeddings.create(model=AZURE_EMBEDDING_DEPLOYMENT, input=texts)\n\n    embeddings = [item.embedding for item in embedding.data]\n    return np.array(embeddings)\n\n\nasync def test_funcs():\n    result = await llm_model_func(\"How are you?\")\n    print(\"llm_model_func response: \", result)\n\n    result = await embedding_func([\"How are you?\"])\n    print(\"embedding_func result: \", result.shape)\n    print(\"embedding dimension: \", result.shape[1])\n\n\nasyncio.run(test_funcs())\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    graph_storage=\"Neo4JStorage\",\n    embedding_func=EmbeddingFunc(\n        embedding_dim=int(os.getenv(\"EMBEDDING_DIM\")),\n        max_token_size=8192,\n        func=embedding_func,\n    ),\n)\n\ndocument_dir = 'Documents'\nmetadata_dir = 'Metadata'\nto_insert = []\nfor filename in os.listdir(document_dir):\n    filepath = os.path.join(document_dir, filename)\n    doc = open(filepath)\n    to_insert.append(doc.read())\n\nfor filename in os.listdir(metadata_dir):\n    filepath = os.path.join(metadata_dir, filename)\n    meta = open(filepath)\n    to_insert.append(meta.read())\n\nrag.insert(to_insert)\n\nquery_text = \"Summarize the contents of all documents edited by John Doe between Jan 10 and Jan 17 2000.\"\n\nprint(\"\\033[1;32mResult (Naive):\\033[0m\")\nprint(rag.query(query_text, param=QueryParam(mode=\"naive\")))\n\nprint(\"\\033[1;32mResult (Local):\\033[0m\")\nprint(rag.query(query_text, param=QueryParam(mode=\"local\")))\n\nprint(\"\\033[1;32mResult (Global):\\033[0m\")\nprint(rag.query(query_text, param=QueryParam(mode=\"global\")))\n\nprint(\"\\033[1;32mResult (Hybrid):\\033[0m\")\nprint(rag.query(query_text, param=QueryParam(mode=\"hybrid\")))\n```\n\nAnswers when inserting information into LightRAG:\n```\nResult (Naive):\nThe varied nature of these topics showcases Heavey's broad editorial interests and his ability to handle a wide spectrum of content.\nResult (Local):\n### Overview of Documents Edited by John Doe\n\nJohn Doe has been involved in the editing and creation of several documents within the specified timeframe of January 10 to January 17, 2000. His contributions span across various topics, showcasing his versatility and expertise in handling different subject matters. Below is a summary of the documents he edited during this period:        \n\n#### 1. Green: The Life of the Color Party\n- **Last Accessed:** January 16, 2000\n- **Role:** Last Editor\n- **Summary:** This document appears to delve into the significance and various aspects of the color green within the context of a political or social movement referred to as the \"Color Party.\" John Doe's role as the last editor suggests he significantly shaped the final content and presentation of the document. The document was last accessed on January 16, 2000, indicating its relevance and timeliness around this date.\n\n#### 2. The Saucy Life of Soy: A Culinary Superhero's Tale\n- **Last Accessed:** January 16, 2000\n- **Role:** Last Editor\n- **Summary:** This document discusses the culinary uses and significance of soy, portraying it as a \"culinary superhero.\" John Doe's editing contributions likely focused on refining the content to better highlight soy's versatile role in culinary arts. The document's last access on the same date as \"Green: The Life of the Color Party\" underscores its contemporary relevance.\n\n#### 3. The Invention of the Keyboard\n- **Last Accessed:** January 15, 2000\n- **Role:** Creator\n- **Summary:** John Doe is credited with creating this document, which discusses the historical development and impact of the keyboard. Although the last edit was made by another individual, Jane Doe, John's foundational work laid the groundwork for the document's content. The document was last accessed on January 15, 2000, suggesting ongoing interest in the topic.\n\n### Commentary\n\nJohn Doe's involvement in these documents highlights his broad range of interests and expertise, from technological history to culinary arts and political movements. His role as both a creator and editor allows him to influence the content significantly, ensuring that each document not only informs but also engages its audience effectively. The access dates close to the time of his last edits also suggest that these documents are of current interest and relevance, likely reflecting recent updates or public engagements with the topics.\n\nResult (Global):\n### Summary of Documents Edited by John Doe\n\nJohn Doe edited two significant documents between January 10 and January 17, 2000. Below are the summaries of each document based on the content and context provided:\n\n#### 1. **Green: The Life of the Color Party**\n   - **Date Accessed:** January 16, 2000\n   - **Created by:** ChatGPT\n   - **Last Edited by:** John Doe\n\n   This document portrays green as a vibrant and essential color in both nature and human life. It is described as the extrovert of the color spectrum, actively involved in various aspects of the environment and human perception. Green symbolizes life, renewal, vitality, and has a playful side associated with springtime and fun. The document emphasizes green's role in transforming spaces like living rooms into calming, zen havens, enhancing both the aesthetic and mood of the environment. It also highlights green's crucial role in plant photosynthesis and its cultural significance, represented in elements like shamrocks and neon signs. \n   \n#### 2. **The Invention of the Keyboard**\n   - **Date Accessed:** January 15, 2000\n   - **Created by:** John Doe\n   - **Last Edited by:** Jane Doe\n\n   This document details the invention of the keyboard by John Key in 2005, motivated by his interest in making switchboards more accessible. The keyboard is portrayed as a significant technological advancement initially aimed at enhancing accessibility. As of the document's context, keyboards are predominantly used in Europe, indicating a regional preference or limitation in their usage.\n\nBoth documents reflect John Doe's involvement in diverse topics, from the vibrant impact of colors to technological innovations, showcasing his versatility and expertise in handling different subject matters.\n\nResult (Hybrid):\n### Summary of Documents Edited by John Doe\n\nJohn Doe has been involved in the editing of several documents between January 10 and January 17, 2000. Below is a summary of the contents of these documents based on the available data:\n\n#### 1. **The Invention of the Keyboard**\n   - **Creation and Editing**: Created by John Doe and last edited by Jane Doe.\n   - **Content Summary**: This document discusses the historical event in 2005 when keyboards were invented by John Key. John Key was motivated by his interest in switchboards and aimed to make them more accessible. The document highlights that keyboards are now used exclusively in Europe.\n   - **Date Accessed**: Last accessed on January 15, 2000.\n\n#### 2. **Green: The Life of the Color Party**\n   - **Creation and Editing**: Created by ChatGPT and last edited by John Doe.\n   - **Content Summary**: This document portrays green as a vibrant and essential  color, symbolizing life, renewal, and vitality. It describes green's significant influence in nature and human environments, emphasizing its role in transforming living spaces like living rooms into calming zen havens. The document also touches on the playful aspects of green, associating it with elements like shamrocks and springtime pranks.\n   - **Date Accessed**: Last accessed on January 16, 2000.\n\n#### 3. **The Saucy Life of Soy: A Culinary Superhero's Tale**\n   - **Creation and Editing**: Created by ChatGPT and last edited by John Doe.\n   - **Content Summary**: Although the detailed content of this document is not provided in the data, it is indicated that the document discusses the culinary uses and significance of soy, portraying it as a \"culinary superhero.\" \n   - **Date Accessed**: Last accessed on January 16, 2000.\n\nThese documents cover a range of topics from technological history to the influence of colors and culinary discussions, showcasing John Doe's versatility and expertise in handling diverse subject matters.\n\n```\n\nAnswers when information is not inserted into LightRAG **BUT** does exist in Neo4j KG:\n```\nResult (Naive):\nSorry, I'm not able to provide an answer to that question.\n\nResult (Local):\nIt appears that there are no data tables provided in your query, which means I don't have access to specific information about documents edited by John Doe or any other details relevant to your request. Without this data, I'm unable to provide a summary of the contents of documents edited by John Doe between January 10 and January 17, 2000.\n\nIf you have access to specific documents or data that you can share, please provide them, and I would be happy to help analyze or summarize the information accordingly.\n\nResult (Global):\nIt appears that there are no data tables provided in the prompt that contain information about documents edited by John Doe or any activities within the specified date range of January 10 to January 17, 2000. Therefore, I'm unable to provide a summary of the contents of documents edited by John Doe during that period.\n\nIf you have specific documents or data that you can provide, I would be happy to help analyze and summarize that information for you.\n\nResult (Hybrid):\nIt appears that there are no data tables provided in the prompt, which means I don't have access to specific information about documents edited  by John Doe or any other details relevant to your query. Without this data, I'm unable to provide a summary of the contents of documents edited by John Doe between January 10 and January 17, 2000.\n\nIf you have access to specific documents or data that you can share, I would be more than happy to help analyze and summarize that information for you.\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/611/comments",
    "author": "VeiledTee",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-23T06:36:57Z",
        "body": "If you maintain a consistent working directory, the data you have inserted should be retrievable, as the insertion and query processes are separate. So, 'Information is not inserted into LightRAG BUT does exist in Neo4j KG' might be because these pieces of information have never been inserted properly into LightRAG in the first place."
      }
    ]
  },
  {
    "number": 608,
    "title": "Temporary fix to introduced problem in ollama and lollm",
    "created_at": "2025-01-20T08:07:28Z",
    "closed_at": "2025-01-20T09:23:28Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/608",
    "body": "Hi,\r\n\r\nSorry. I just found out that passing the headers to the ollama and lollms directly from llm has a conflict with another header generated inside this so we need to rewrite it.\r\n\r\nTo fix the problem I just removed the header asignment temporarily and I'll rewrite it this evening.\r\n\r\nI tested it last night but it seems that I wasn't testing the right version (my bad)\r\n\r\nSorry for the error. This should fix it.\r\nHave a nice day",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/608/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-20T08:24:10Z",
        "body": "No worries. But there are some linting errors that need to be fixed.\r\nThanks, and have a great day!"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-20T08:57:48Z",
        "body": "Ok, I suspect that the real problem comes fom another update beceause yesterday it worked on my PC.\r\nI'll analyze the problem more then make a pull request when nready. I'll also do linting "
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-20T09:08:31Z",
        "body": "OK, found the problem and fixed it. Now everything is working fine. We just need to update the linting and we're back on track."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-20T09:11:05Z",
        "body": "Would you please accept and merge the update then do a linting with pre-commit. I can't do that immediately. I can do it this evening when I'm back home, but It would be wize not to keep the repo with a serious error like this for long.\r\n\r\nThanks"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-20T09:13:33Z",
        "body": "TO sum up the problem:\r\nI have added the possibility to access lollms or ollama behind a secure access with BEARING Key.\r\n\r\nIn the code I specified that the header will be None if no key is specified.\r\nBut it shouldn't be none, it should at least be an empty dict or a dict with content type.\r\n\r\nSo I have updated this so that if no key is pecified, we use a default header."
      },
      {
        "user": "LarFii",
        "created_at": "2025-01-20T09:23:25Z",
        "body": "Sure, no problem."
      }
    ]
  },
  {
    "number": 595,
    "title": "Query with your own prompt type",
    "created_at": "2025-01-17T11:08:20Z",
    "closed_at": "2025-01-27T03:55:47Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/595",
    "body": "Feature:\r\n Query by adding your own prompt default prompt type will be of light rag\r\nBring your own prompt and do query\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/595/comments",
    "author": "MdNazishArmanShorthillsAI",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-19T05:00:47Z",
        "body": "Thanks for your contribution! But there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass. Also, you can add a brief usage section in the `README.md`."
      }
    ]
  },
  {
    "number": 594,
    "title": "Simulate an OpenAI/LiteLLM/Ollama server in API",
    "created_at": "2025-01-17T07:59:29Z",
    "closed_at": "2025-02-17T22:35:01Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/594",
    "body": "Hi there,\n\nI have an idea. Do you mind if I add a simulation of all OpenAI/LiteLLM or Ollama endpoints to the lightRag API so that this can be used as an augmented assistant. So the user puts his data somewhere, activates the server, then he can use it as an LLM and all RAG is hidden beneath. \n\nI can make that an option, integrate it in Docker etc...\n\nBest regards",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/594/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-23T06:45:17Z",
        "body": "Sorry for the late reply. This is a great idea, and I fully support you moving forward with it."
      }
    ]
  },
  {
    "number": 592,
    "title": "Add Ollama compatible API server",
    "created_at": "2025-01-16T13:31:47Z",
    "closed_at": "2025-01-17T06:29:31Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/592",
    "body": "This PR implements an Ollama-compatible API interface for LightRAG, aiming to emulate LightRAG as an Ollama model. This allows AI chat frontends that support Ollama models, such as Open WebUI, to access LightRAG. After starting the lightrag-ollama service, you can add an Ollama-type connection in the Open WebUI admin console, and a model named lightrag:latest will appear in Open WebUI's model management interface. Users can then send queries to LightRAG through the chat interface.\r\n\r\nThe `/api/chat` endpoint determines which LightRAG query mode to use by recognizing query string prefixes. The supported prefixes include:\r\n* /local \r\n* /global \r\n* /hybrid \r\n* /naive \r\n* /mix\r\n\r\nWhen no prefix is present in query string, the hybrid mode is used by default.\r\n\r\nps:\r\nFixing /query and /query/stream endpoint problem when query is hitting LLM respond cache, whereas `aquery` return normal string instead of streaming object. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/592/comments",
    "author": "danielaskdd",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-17T04:13:06Z",
        "body": "Thank you very much for your valuable contribution! Could you please update the comments to English? This will help facilitate further development by others in the future. Also, there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass."
      },
      {
        "user": "danielaskdd",
        "created_at": "2025-01-17T06:15:14Z",
        "body": "> Thank you very much for your valuable contribution! Could you please update the comments to English? This will help facilitate further development by others in the future. Also, there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass.\r\n\r\nWorks doneÔºÅ Thx."
      },
      {
        "user": "LarFii",
        "created_at": "2025-01-17T06:28:43Z",
        "body": "Thanks again!"
      }
    ]
  },
  {
    "number": 591,
    "title": "add readme_zh",
    "created_at": "2025-01-16T13:14:10Z",
    "closed_at": "2025-01-17T04:07:28Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/591",
    "body": "add openai_README_zh",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/591/comments",
    "author": "luohuanhuan2019",
    "comments": [
      {
        "user": "luohuanhuan2019",
        "created_at": "2025-01-16T13:14:25Z",
        "body": "openai_README_zh"
      },
      {
        "user": "luohuanhuan2019",
        "created_at": "2025-01-16T13:54:18Z",
        "body": "Prompt words to keep the pronunciation consistent"
      },
      {
        "user": "LarFii",
        "created_at": "2025-01-17T04:07:19Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 590,
    "title": "Enhance Robustness of insert Method with Pipeline Processing and Caching Mechanisms",
    "created_at": "2025-01-16T05:43:58Z",
    "closed_at": "2025-01-16T06:20:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/590",
    "body": "The current `insert` method in LightRAG writes results to persistent storage only upon successful completion of all operations. This design means that any error‚Äîbe it network issues, database failures, or bugs‚Äîforces users to restart the entire process, even if 99% of the work was completed. This all-or-nothing approach can lead to significant inefficiencies and user frustration.\r\n\r\n**Enhancements Introduced:**\r\n\r\n1. **Data Storage Optimization with LLM Cache:**\r\n   - Implemented `enable_llm_cache_for_entity_extract = True` to ensure that previously sent prompts are retrieved directly from the cache, reducing redundant API calls and enhancing efficiency.\r\n\r\n2. **Pipeline-Based Knowledge Graph Construction:**\r\n   - Refactored the `insert` process into three distinct asynchronous pipeline stages, each capable of resuming from interruptions without reprocessing completed tasks:\r\n\r\n     1. **`apipeline_process_documents(self, string_or_strings)`**:\r\n        - Removes duplicate input texts.\r\n        - Generates document IDs.\r\n        - Filters out documents already stored.\r\n        - Stores new documents.\r\n        - Sets document status to pending.\r\n\r\n     2. **`apipeline_process_chunks(self)`**:\r\n        - Retrieves pending and failed documents.\r\n        - Splits documents into chunks.\r\n        - Inserts chunks, invoking the embedding model.\r\n        - Sets chunk status to pending.\r\n        - Updates document status to processed.\r\n\r\n     3. **`apipeline_process_extract_graph(self)`**:\r\n        - Retrieves pending or failed chunks.\r\n        - Extracts entities and relationships from each chunk using the LLM model.\r\n        - Inserts entities and relationships.\r\n        - Updates chunk status to processed.\r\n\r\n   - This structured approach ensures that each LLM call is utilized effectively, with results either stored in the cache or directly in entities and relationships, eliminating wasted computations.\r\n\r\n**Note:**\r\n\r\nThe pipeline methods have been adapted for **Oracle database**, necessitating the addition of a status field to track the state of documents and chunks. The recently committed `JsonDocStatusStorage` was not utilized due to its complexity.\r\n\r\nThe original `insert` is still available, so non-Oracle storage can still use the old code\r\n\r\nThese enhancements collectively improve the robustness and efficiency of the `insert` method, ensuring that users can resume interrupted processes without redundant operations, thereby enhancing the overall user experience with LightRAG.\r\n\r\n**Additional Optimizations:**\r\n\r\n- **Test File Organization:**\r\n  - Moved certain test files to the `example` folder for better project structure.\r\n\r\n- **Selective Storage Loading:**\r\n  - Modified `lightrag.py` to load only the necessary storage classes instead of all 20, improving resource utilization.\r\n\r\n- **Global Configuration Handling:**\r\n  - Embedded `global_config` as a fixed parameter in `lightrag.py`, removing the need to pass it during each class instantiation.\r\n\r\n- **Non-ASCII Character Support:**\r\n  - In `operate.py` at line 356, added `ensure_ascii=False` to `json.dumps(history_messages)` to prevent non-English characters from being converted to ASCII.\r\n\r\n- **LLM Call Statistics:**\r\n  - Introduced `statistic_data` in `utils.py` to track the number of LLM calls during knowledge graph construction.\r\n\r\n- **Logging Level Adjustment:**\r\n  - Set `logging.getLogger(\"httpx\").setLevel(logging.WARNING)` in `utils.py` to suppress excessive log outputs.\r\n\r\n- **Bug Fixes:**\r\n  - In `utils.py` at line 462, changed `if mode == \"naive\"` to `if mode == \"default\"`, resolving a bug that previously hindered execution.\r\n  - At line 484 in `utils.py`, updated `embedding_model_func = hashing_kv.global_config[\"embedding_func\"][\"func\"]` to append `.func`, correcting a bug that prevented proper function execution.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/590/comments",
    "author": "jin38324",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-16T06:20:05Z",
        "body": "Thanks for your contributions!"
      }
    ]
  },
  {
    "number": 585,
    "title": "Add custom function with separate keyword extraction for user's query and a separate prompt",
    "created_at": "2025-01-14T16:59:02Z",
    "closed_at": "2025-01-16T06:13:20Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/585",
    "body": "This PR introduces a significant enhancement to our RAG module's keyword extraction capabilities by implementing the `query_with_separate_keyword_extraction` function. Previously, our approach involved extracting keywords from a combined string of both the user's prompt and the query. This often led to the extraction of irrelevant keywords, as the prompt might contain additional language that does not necessarily reflect the core intent of the query.\r\n\r\n## Changes Made\r\nThe new function, query_with_separate_keyword_extraction, addresses this issue by explicitly separating the user's query from the prompt. This allows the system to perform keyword extraction solely on the query, ensuring that only relevant keywords are considered when forming the response. The prompt is then utilized to format the response appropriately, without influencing the keyword extraction process.\r\n\r\n## Example usage\r\n```\r\nrag.query_with_separate_keyword_extraction(\r\n    \"What are the top themes in this story?\",\r\n    \"You need to answer me this question in as simple language as you can. Just know that a 5 year old should understand it.\",\r\n    param=QueryParam(mode=\"hybrid\")\r\n)\r\n```\r\nThis improvement is expected to enhance user interaction by making the responses not only more relevant but also tailored to the context provided by the prompt. It prevents the dilution of keyword significance when the prompt includes supplementary instructions or context not directly related to the content of the query.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/585/comments",
    "author": "gurjot-05",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-16T04:56:00Z",
        "body": "Thank you so much for your contribution! Could you please add a detailed explanation in the `README.md` and include specific usage examples in the `examples/`?"
      },
      {
        "user": "gurjot-05",
        "created_at": "2025-01-16T05:25:11Z",
        "body": "> Thank you so much for your contribution! Could you please add a detailed explanation in the `README.md` and include specific usage examples in the `examples/`?\r\n\r\nOn it!"
      },
      {
        "user": "gurjot-05",
        "created_at": "2025-01-16T05:58:05Z",
        "body": "> Thank you so much for your contribution! Could you please add a detailed explanation in the `README.md` and include specific usage examples in the `examples/`?\r\n\r\nI have added the detailed explanation in `README.md` and also included specific usage example in `examples/`"
      },
      {
        "user": "LarFii",
        "created_at": "2025-01-16T06:13:16Z",
        "body": "Thank you again!"
      }
    ]
  },
  {
    "number": 583,
    "title": "PostgreSQL SyntaxError of f-string",
    "created_at": "2025-01-14T09:37:06Z",
    "closed_at": "2025-02-17T22:35:33Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/583",
    "body": "```\r\n from lightrag.kg.postgres_impl import PostgreSQLDB   # type:ignore\r\nE     File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 415\r\nE       sql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({\",\".join([f\"'{_id}'\" for _id in data])})\"\r\nE                                                                                  ^\r\nE   SyntaxError: f-string: expecting '}'\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/583/comments",
    "author": "chensylz",
    "comments": [
      {
        "user": "qiaobaxiaolu",
        "created_at": "2025-01-14T09:50:27Z",
        "body": "mark"
      },
      {
        "user": "qiaobaxiaolu",
        "created_at": "2025-01-20T07:26:06Z",
        "body": " id_list = \",\".join([f\"'{_id}'\" for _id in data])\n        sql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({id_list})\""
      }
    ]
  },
  {
    "number": 574,
    "title": "Failed to call `/documents/scan` endpoint using server.",
    "created_at": "2025-01-12T09:20:15Z",
    "closed_at": "2025-01-13T02:47:26Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/574",
    "body": "I am reaching out to report an issue encountered during the testing phase of your system. The details are as follows:\r\n\r\n## Issue Description:\r\n1. I uploaded a mock file (.txt) to initiate a test scan.\r\n2. While the response indicates a successful operation, no documents were loaded into the system. \r\n3. The uploaded file should be successfully scanned, and the document count should reflect the number of documents processed.\r\n\r\nBelow is the response for reference:\r\n```json\r\n{\r\n  \"status\": \"success\",\r\n  \"message\": \"All documents cleared successfully\",\r\n  \"document_count\": 0\r\n}\r\n```\r\nThis suggests that the process completed without errors, but the result is not as expected since no documents are reflected in the system.\r\n## Log Information:\r\n```\r\n- INFO - Logger initialized for working directory\r\n- INFO - Load KV llm_response_cache with 1 data\r\n- INFO - Load KV full_docs with 2 data\r\n- INFO - Load KV text_chunks with 2 data\r\n- INFO - Loaded graph with 5 nodes, 4 edges\r\n- INFO - Loaded document status storage with 3 records\r\n- INFO - Logger initialized for working directory\r\n- INFO - Load KV llm_response_cache with 1 data\r\n- INFO - Load KV full_docs with 2 data\r\n- INFO - Load KV text_chunks with 2 data\r\n- INFO - Loaded graph with 5 nodes, 4 edges\r\n- INFO - Loaded document status storage with 3 records\r\n- INFO - Processing 1 new unique document\r\n- INFO - Inserting 42 vectors to chunks\r\n- INFO - Inserting entities into storage\r\n- INFO - Inserting relationships into storage\r\n- ERROR - Failed to process document doc-[redacted]: 'list' object has no attribute 'upsert'\r\nTraceback (most recent call last):\r\n  File \"lightrag.py\", in ainsert\r\n    raise e\r\n  File \"lightrag.py\", in ainsert\r\n    await self.text_chunks.upsert(chunks)\r\nAttributeError: 'list' object has no attribute 'upsert'. Did you mean: 'insert'?\r\n```\r\nI appreciate your support in resolving this issue at the earliest convenience. Please let me know the next steps or any actions I can take to assist with this.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/574/comments",
    "author": "iridium-soda",
    "comments": [
      {
        "user": "ParisNeo",
        "created_at": "2025-01-13T00:27:34Z",
        "body": "Hi there,\r\n\r\nWhich version of the api are you using?\r\nWhen I scan I get this:\r\n{\r\n  \"status\": \"success\",\r\n  \"indexed_count\": 0,\r\n  \"total_documents\": 7\r\n}\r\n\r\nhere it found 7 documents and it did not index any of them beceause they were already indexed, so it is working as expected.\r\n\r\nThe output you are showing resembles the output you get if you call the clear_documents endpoint:\r\nAll documents cleared successfully\r\n\r\n\r\nHere is the code:\r\n```python\r\n    @app.delete(\r\n        \"/documents\",\r\n        response_model=InsertResponse,\r\n        dependencies=[Depends(optional_api_key)],\r\n    )\r\n    async def clear_documents():\r\n        try:\r\n            rag.text_chunks = []\r\n            rag.entities_vdb = None\r\n            rag.relationships_vdb = None\r\n            return InsertResponse(\r\n                status=\"success\",\r\n                message=\"All documents cleared successfully\",\r\n                document_count=0,\r\n            )\r\n        except Exception as e:\r\n            raise HTTPException(status_code=500, detail=str(e))\r\n```\r\n\r\nMake sure you are using the last updated version on the main github.\r\n\r\n\r\nI have submitted some updates to fix the API and hopefully they'll merge them soon."
      },
      {
        "user": "iridium-soda",
        "created_at": "2025-01-13T02:47:26Z",
        "body": "I noticed a major update on the API server. I‚Äôll wait until it stabilizes before trying again. In the meantime, I‚Äôll use the native package as a temporary solution. This might be an isolated incident."
      }
    ]
  },
  {
    "number": 566,
    "title": "Seeking Clarity: Purpose of content_keywords",
    "created_at": "2025-01-10T09:29:00Z",
    "closed_at": "2025-02-17T11:02:11Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/566",
    "body": " I haven't observed its application during the retrieval phase.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/566/comments",
    "author": "Authorlove",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-12T05:43:20Z",
        "body": "It's a reserved interface that hasn't been used yet."
      }
    ]
  },
  {
    "number": 564,
    "title": "ÊµãËØï‰ΩøÁî®PostgreSQLÊï∞ÊçÆÂ∫ìÊó∂Âá∫Áé∞ÈîôËØØ",
    "created_at": "2025-01-10T07:03:43Z",
    "closed_at": "2025-01-13T01:24:15Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/564",
    "body": "\r\n\r\n\r\nËøêË°åÊó∂Âá∫Áé∞ÈîôËØØÔºö\r\n```\r\nINFO:lightrag:Using the label default for PostgreSQL as identifier\r\nINFO:lightrag:Connected to PostgreSQL database at localhost:5432/rag\r\nERROR:lightrag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\r\nSELECT 1 FROM LIGHTRAG_DOC_CHUNKS LIMIT 1\r\nNone\r\nERROR:lightrag:Failed to check table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\r\nERROR:lightrag:PostgreSQL database error: relation \"lightrag_doc_chunks\" does not exist\r\nINFO:lightrag:Created table LIGHTRAG_DOC_CHUNKS in PostgreSQL database\r\nERROR:lightrag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\r\nSELECT 1 FROM LIGHTRAG_VDB_ENTITY LIMIT 1\r\nNone\r\nERROR:lightrag:Failed to check table LIGHTRAG_VDB_ENTITY in PostgreSQL database\r\nERROR:lightrag:PostgreSQL database error: relation \"lightrag_vdb_entity\" does not exist\r\nINFO:lightrag:Created table LIGHTRAG_VDB_ENTITY in PostgreSQL database\r\nERROR:lightrag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\r\nSELECT 1 FROM LIGHTRAG_VDB_RELATION LIMIT 1\r\nNone\r\nERROR:lightrag:Failed to check table LIGHTRAG_VDB_RELATION in PostgreSQL database\r\nERROR:lightrag:PostgreSQL database error: relation \"lightrag_vdb_relation\" does not exist\r\nINFO:lightrag:Created table LIGHTRAG_VDB_RELATION in PostgreSQL database\r\nINFO:lightrag:Finished checking all tables in PostgreSQL database\r\nembedding_dim=1024\r\nINFO:lightrag:Logger initialized for working directory: /home/lakala/tlw/LightRAG/Q&A-pg\r\nINFO:lightrag:All documents have been processed or are duplicates\r\n==== Trying to test the rag queries ====\r\n**** Start Naive Query ****\r\nERROR:lightrag:PostgreSQL database error: subquery in FROM must have an alias\r\nHINT:  For example, FROM (SELECT ...) [AS] foo.\r\nSELECT id FROM\r\n        (SELECT id, 1 - (content_vector <=> `'[-0.011011348105967045,..... \r\n\r\n... \r\n...\r\n6948,-0.005379492416977882,-0.029000142589211464,0.010410051792860031]'::vector) as distance\r\n        FROM LIGHTRAG_DOC_CHUNKS where workspace=$1)\r\n        WHERE distance>$2 ORDER BY distance DESC  LIMIT $3\r\n\r\n{'workspace': 'default', 'better_than_threshold': 0.2, 'top_k': 60}\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/xxx/LightRAG/postgres_test.py\", line 191, in <module>\r\n    asyncio.run(main())\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/home/xxx/xxx/LightRAG/postgres_test.py\", line 157, in main\r\n    await rag.aquery(\r\n  File \"/home/xxx/xxx/LightRAG/lightrag/lightrag.py\", line 632, in aquery\r\n    response = await naive_query(\r\n  File \"/home/xxx/xxx/LightRAG/lightrag/operate.py\", line 1169, in naive_query\r\n    results = await chunks_vdb.query(query, top_k=query_param.top_k)\r\n  File \"/home/xxx/xxx/LightRAG/lightrag/kg/postgres_impl.py\", line 391, in query\r\n    results = await self.db.query(sql, params=params, multirows=True)\r\n  File \"/home/xxx/xxx/LightRAG/lightrag/kg/postgres_impl.py\", line 104, in query\r\n    rows = await connection.fetch(sql, *params.values())\r\n  File \"/usr/local/lib/python3.10/dist-packages/asyncpg/connection.py\", line 690, in fetch\r\n    return await self._execute(\r\n  File \"/usr/local/lib/python3.10/dist-packages/asyncpg/connection.py\", line 1864, in _execute\r\n    result, _ = await self.__execute(\r\n  File \"/usr/local/lib/python3.10/dist-packages/asyncpg/connection.py\", line 1961, in __execute\r\n    result, stmt = await self._do_execute(\r\n  File \"/usr/local/lib/python3.10/dist-packages/asyncpg/connection.py\", line 2004, in _do_execute\r\n    stmt = await self._get_statement(\r\n  File \"/usr/local/lib/python3.10/dist-packages/asyncpg/connection.py\", line 432, in _get_statement\r\n    statement = await self._protocol.prepare(\r\n  File \"asyncpg/protocol/protocol.pyx\", line 165, in prepare\r\nasyncpg.exceptions.PostgresSyntaxError: subquery in FROM must have an alias\r\nHINT:  For example, FROM (SELECT ...) [AS] foo.\r\n\r\n```\r\nËØ∑ÈóÆÂ¶Ç‰ΩïËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÂë¢Ôºü Ë∞¢Ë∞¢~",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/564/comments",
    "author": "WellTung666",
    "comments": [
      {
        "user": "SherlockBULABULA",
        "created_at": "2025-01-14T07:09:02Z",
        "body": "ÊÄé‰πàËß£ÂÜ≥ÁöÑÔºåËá™Â∑±Âª∫Ë°®Âêó"
      },
      {
        "user": "vighneshpp1986",
        "created_at": "2025-01-16T02:45:13Z",
        "body": "> How to solve it, do you build your own table?\r\n\r\nThe error you're seeing indicates that the pgvector extension is not installed in your PostgreSQL instance. \r\n\r\nFor a Local PostgreSQL Installation (Using apt for Ubuntu/Debian):\r\nIf you are running PostgreSQL locally on Ubuntu or a Debian-based system, you can install pgvector via apt (if the repository supports it) or by compiling from source.\r\n\r\nInstall from apt repository\r\n\r\n``` bash\r\nsudo apt update\r\nsudo apt install postgresql-<version>-pgvector\r\n```\r\n\r\nOnce I installed this extension, it worked for me.\r\n"
      },
      {
        "user": "WellTung666",
        "created_at": "2025-01-16T06:45:50Z",
        "body": "> ÊÄé‰πàËß£ÂÜ≥ÁöÑÔºåËá™Â∑±Âª∫Ë°®Âêó\r\n\r\nËß£ÂÜ≥ÊñπÊ≥ïÔºö‰∏ãËΩΩÂπ∂ÂêØÁî® PGVectorÔºå‰øÆÊîπLightRAG\\lightrag\\kg\\postgres_impl.py‰∏≠1182~1197Ë°å„ÄÇ\r\n```\r\n    # SQL for VectorStorage\r\n    \"entities\": \"\"\"SELECT entity_name FROM\r\n            (SELECT id, entity_name, 1 - (content_vector <=> '[{embedding_string}]'::vector) as distance\r\n            FROM LIGHTRAG_VDB_ENTITY where workspace=$1) AS subquery\r\n            WHERE distance>$2 ORDER BY distance DESC  LIMIT $3\r\n           \"\"\",\r\n    \"relationships\": \"\"\"SELECT source_id as src_id, target_id as tgt_id FROM\r\n            (SELECT id, source_id,target_id, 1 - (content_vector <=> '[{embedding_string}]'::vector) as distance\r\n            FROM LIGHTRAG_VDB_RELATION where workspace=$1) AS subquery\r\n            WHERE distance>$2 ORDER BY distance DESC  LIMIT $3\r\n           \"\"\",\r\n    \"chunks\": \"\"\"SELECT id FROM\r\n            (SELECT id, 1 - (content_vector <=> '[{embedding_string}]'::vector) as distance\r\n            FROM LIGHTRAG_DOC_CHUNKS where workspace=$1) AS subquery\r\n            WHERE distance>$2 ORDER BY distance DESC  LIMIT $3\r\n           \"\"\",\r\n```\r\n"
      }
    ]
  },
  {
    "number": 562,
    "title": "Error running Postgres Demo - SyntaxError",
    "created_at": "2025-01-09T17:05:10Z",
    "closed_at": "2025-02-17T11:02:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/562",
    "body": "Error running DEMO . I'm using Python 3.11. \r\n\r\n/home/asksuite/asksuite-dev/test-lightrag/venv311/bin/python /home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py \r\nTraceback (most recent call last):\r\n  File \"/home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py\", line 5, in <module>\r\n    from lightrag.kg.postgres_impl import PostgreSQLDB\r\n  File \"/home/asksuite/asksuite-dev/test-lightrag/venv311/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 406\r\n    sql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({\",\".join([f\"'{_id}'\" for _id in data])})\"\r\n                                                                               ^\r\nSyntaxError: f-string: expecting '}'",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/562/comments",
    "author": "pavei",
    "comments": [
      {
        "user": "WellTung666",
        "created_at": "2025-01-10T02:57:07Z",
        "body": "same errorÔºÅ "
      },
      {
        "user": "WellTung666",
        "created_at": "2025-01-10T03:45:48Z",
        "body": "> Error running DEMO . I'm using Python 3.11.\r\n> \r\n> /home/asksuite/asksuite-dev/test-lightrag/venv311/bin/python /home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py Traceback (most recent call last): File \"/home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py\", line 5, in from lightrag.kg.postgres_impl import PostgreSQLDB File \"/home/asksuite/asksuite-dev/test-lightrag/venv311/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 406 sql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({\",\".join([f\"'{_id}'\" for _id in data])})\" ^ SyntaxError: f-string: expecting '}'\r\n\r\nThis seems to solve the problem.\r\n\r\n```\r\nid_list = \",\".join([f\"'{_id}'\" for _id in data])\r\nsql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({id_list})\"\r\n```"
      },
      {
        "user": "pavei",
        "created_at": "2025-01-10T18:32:57Z",
        "body": "Yes, it works! "
      }
    ]
  },
  {
    "number": 559,
    "title": "insert ÊñπÊ≥ïÂÆåÂÖ®Êä•ÈîôÔºöhgfaceÁöÑembedingÊñπÂºè",
    "created_at": "2025-01-09T06:05:20Z",
    "closed_at": "2025-02-17T11:02:09Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/559",
    "body": "  embedding_func=EmbeddingFunc(\r\n         embedding_dim=1792,\r\n        max_token_size=512,\r\nfunc=lambda texts: hf_embedding(\r\n            textÔºå\r\n tokenizer=AutoTokenizer.from_pretrained(\r\n        embed_model=AutoModel.from_pretrained(\r\n         \r\nRuntimeError: The expanded size of the tensor (1036Ôºâmust match the existing size (512) at non-singleto dimension 1.  Target sizes: [32, 1036].   Tensor sizes: [1, 512]",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/559/comments",
    "author": "Kyrie666",
    "comments": [
      {
        "user": "chericher",
        "created_at": "2025-01-11T07:09:56Z",
        "body": "Hello, i also have same problem, how do you solve it ?"
      },
      {
        "user": "Kyrie666",
        "created_at": "2025-01-11T09:55:29Z",
        "body": "> Hello, i also have same problem, how do you solve it ?\r\n  I was able to run it successfully by changing the chunk size and resetting the embedding dimensions. However, the specific reasons are still unclear to me; I can only speculate that it might be due to the embedding model not being large enough, the input document being too long, or unreasonable chunk sizes, among other factors. Further exploration is needed to clarify this.\r\n"
      }
    ]
  },
  {
    "number": 557,
    "title": "‰ºòÂåñ‰ªÖÂ≠óÁ¨¶ÂàÜÂâ≤ÂèÇÊï∞",
    "created_at": "2025-01-09T03:59:45Z",
    "closed_at": "2025-01-09T07:29:35Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/557",
    "body": "Â¢ûÂä†‰ªÖÂ≠óÁ¨¶ÂàÜÂâ≤ÂèÇÊï∞ÔºåÂ¶ÇÊûúÂºÄÂêØÔºå‰ªÖÈááÁî®Â≠óÁ¨¶ÂàÜÂâ≤Ôºå‰∏çÂºÄÂêØÔºåÂú®ÂàÜÂâ≤ÂÆå‰ª•ÂêéÂ¶ÇÊûúchunkËøáÂ§ßÔºå‰ºöÁªßÁª≠Ê†πÊçÆtoken sizeÂàÜÂâ≤ÔºåÊõ¥Êñ∞ÊµãËØïÊñá‰ª∂",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/557/comments",
    "author": "tongshiyuan",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-09T07:29:32Z",
        "body": "ÊÑüË∞¢ÔºÅ"
      }
    ]
  },
  {
    "number": 555,
    "title": "Restore backwards compatibility for LightRAG's ainsert method",
    "created_at": "2025-01-08T19:53:40Z",
    "closed_at": "2025-01-09T07:27:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/555",
    "body": "Makes the split_by_character parameter optional in ainsert method to maintain backwards compatibility with existing code.\r\n\r\nDetails:\r\n\r\nThe ainsert method in LightRAG class was recently modified to include a new split_by_character parameter\r\nThis change broke backwards compatibility for code using the previous method signature\r\nFixed by making split_by_character parameter optional with a default value of None\r\nImpact:\r\n\r\nRestores compatibility with existing code using the old method signature\r\nMaintains new functionality for users who want to use character splitting\r\nNo breaking changes",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/555/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-09T07:27:10Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 554,
    "title": "Failed to Install API Server via Pip",
    "created_at": "2025-01-08T07:08:38Z",
    "closed_at": "2025-01-11T12:20:24Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/554",
    "body": "I started evaluating this repository using a clean Python 3.13 container based on Debian. Unfortunately, I encountered an issue right from the start when attempting to install LightRAG using the safest approach: cloning the repository and manually installing the API server via pip. The installation process failed with the following output:\r\n\r\n```\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  √ó Preparing metadata (pyproject.toml) did not run successfully.\r\n  ‚îÇ exit code: 1\r\n  ‚ï∞‚îÄ> [78 lines of output]\r\n      /tmp/pip-build-env-hnu92sc_/overlay/lib/python3.13/site-packages/setuptools/_distutils/dist.py:270: UserWarning: Unknown distribution option: 'test_suite'\r\n        warnings.warn(msg)\r\n      /tmp/pip-build-env-hnu92sc_/overlay/lib/python3.13/site-packages/setuptools/_distutils/dist.py:270: UserWarning: Unknown distribution option: 'tests_require'\r\n        warnings.warn(msg)\r\n      ...\r\n      AttributeError: 'dict' object has no attribute '__NUMPY_SETUP__' and no __dict__ for setting new attributes\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n√ó Encountered error while generating package metadata.\r\n‚ï∞‚îÄ> See above for output.\r\n```\r\n\r\nThis issue prevents me from proceeding with my evaluation of LightRAG, which is intended to serve as a critical component of my personal project.\r\n\r\nAdditionally, I found that directly using the package from PyPI is not a viable option. Neither the standard package nor the API server includes the necessary dependencies, which also blocks their usability.\r\n\r\nCould you provide insights into the cause of this issue and suggest a solution to resolve it? Any guidance on proper installation or alternative steps to use LightRAG would be greatly appreciated.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/554/comments",
    "author": "iridium-soda",
    "comments": [
      {
        "user": "iridium-soda",
        "created_at": "2025-01-08T08:11:30Z",
        "body": "The issue appears to be resolved by downgrading Python to version 3.10. However, having a comprehensive installation guide and detailed documentation would be highly appreciated."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-09T15:02:12Z",
        "body": "I guess the project wasn't built correctly for 3.13 and you do not have the right build tools.\r\nI did not test it on anything higher than 3.11. If I have some time I'll try to test that with 3.13\r\n\r\nMaybe another alternative is to make it a uv project. This will simplify everything nd streamline the installation as uv makes things easy."
      },
      {
        "user": "iridium-soda",
        "created_at": "2025-01-11T06:02:11Z",
        "body": "> I guess the project wasn't built correctly for 3.13 and you do not have the right build tools. I did not test it on anything higher than 3.11. If I have some time I'll try to test that with 3.13\r\n> \r\n> Maybe another alternative is to make it a uv project. This will simplify everything nd streamline the installation as uv makes things easy.\r\n\r\nTo enhance usability, it would be great if a production-ready Docker image could be provided. This would offer a more convenient and streamlined way to deploy the service, especially considering that an API service has already been designed for interaction."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-11T11:53:29Z",
        "body": "That was my next idea. Once they accept the new API upgrade, i'll start building a docker image.\nIt's a wip."
      },
      {
        "user": "iridium-soda",
        "created_at": "2025-01-11T12:20:24Z",
        "body": "> That was my next idea. Once they accept the new API upgrade, i'll start building a docker image. It's a wip.\r\n\r\nI look forward to utilizing the Docker image in my project and exploring its functionality."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-13T08:48:52Z",
        "body": "I am already using it with lollms and it really rocks. You set it up, then add it as a new datalake\r\n\r\nI'm gonna add an automatic installation and adding of datalakes in the lollms-webui interface. Lollms has already all the needed interfaces to do that but up till now it uses only classic RAG. With lightrag integration, you'll be able to define and share multiple datalakes and use them (even combine them) when discussion with your AI or when the AI is doing some agentic development."
      },
      {
        "user": "Strophe27",
        "created_at": "2025-01-16T18:58:04Z",
        "body": "> That was my next idea. Once they accept the new API upgrade, i'll start building a docker image. It's a wip.\n\nThat sounds great ! looking forward to seeing the progress whenever it‚Äôs ready. In the meantime, I‚Äôm a bit stuck with deploying LightRag in Docker, so I‚Äôll be eagerly awaiting any updates on this. Keep up the amazing work!"
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-16T20:33:47Z",
        "body": "Hi there, my pull request has been accepted, so I'll start the docker development right now!\nI hope I get a fully working docker image tonight."
      },
      {
        "user": "ParisNeo",
        "created_at": "2025-01-17T01:37:29Z",
        "body": "OK, it is done. I hope they accept the pull request made some documentation. if you have questions don't hesitate to ask."
      },
      {
        "user": "Strophe27",
        "created_at": "2025-01-17T01:42:56Z",
        "body": "Merci beaucoup, incroyable tout ton taf et talents (lollms‚Ä¶)"
      },
      {
        "user": "edward-yuen",
        "created_at": "2025-02-19T20:31:31Z",
        "body": "Hi, I tried to run the docker container but was getting an error that the module 'past' was not found. That module is part of 'future' and I added it to the requirements.txt files which fixed the docker build. "
      }
    ]
  },
  {
    "number": 552,
    "title": "just could not get started from pip install -e .",
    "created_at": "2025-01-07T06:12:49Z",
    "closed_at": "2025-02-17T11:02:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/552",
    "body": "I tried several times but all failed. Just as the introduction video said, creating a env with python==3.10, git clone the source code, and then pip install -e . \r\ncould some one help me with this?\r\n-----------------------------------------error info ----------------------------------------------------------------------\r\n      error: Command \"gcc -pthread -B /home/daishengran/anaconda3/envs/rag2/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/daishengran/anaconda3/envs/rag2/include -fPIC -O2 -isystem /home/daishengran/anaconda3/envs/rag2/include -fPIC -I/home/daishengran/anaconda3/envs/rag2/lib/python3.10/site-packages/numpy/_core/include -I/home/daishengran/anaconda3/envs/rag2/include/python3.10 -c numba/_dispatcher.c -o build/temp.linux-x86_64-cpython-310/numba/_dispatcher.o\" failed with exit status 1\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for numba\r\n  Running setup.py clean for numba\r\nFailed to build numba\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numba)\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/552/comments",
    "author": "userdsr",
    "comments": [
      {
        "user": "yulink1981",
        "created_at": "2025-01-07T06:48:20Z",
        "body": "you need reinstall vs2015 C++ env"
      },
      {
        "user": "wyk777777",
        "created_at": "2025-01-08T02:18:21Z",
        "body": "I have the same problem. Have you solved itÔºü"
      },
      {
        "user": "userdsr",
        "created_at": "2025-01-08T02:38:25Z",
        "body": "> I have the same problem. Have you solved itÔºü\r\n\r\nyes, I solved that problem by installing those packages one by one"
      },
      {
        "user": "arnenismail67",
        "created_at": "2025-01-10T00:15:00Z",
        "body": "i put # next hnswlib in requeirement.txt file,  repeat pip install -e .  "
      }
    ]
  },
  {
    "number": 551,
    "title": "Â¢ûÂä†chunkÁöÑÂ≠óÁ¨¶ÂàÜÂâ≤ÂäüËÉΩ",
    "created_at": "2025-01-06T16:58:09Z",
    "closed_at": "2025-01-07T08:38:04Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/551",
    "body": "Ê∑ªÂä†Â≠óÁ¨¶ÂàÜÂâ≤ÂäüËÉΩÔºåÂú®‚Äúinsert‚ÄùÂáΩÊï∞‰∏≠Â¶ÇÊûúÂ¢ûÂä†ÂèÇÊï∞split_by_characterÔºåÂàô‰ºöÊåâÁÖßsplit_by_characterËøõË°åÂ≠óÁ¨¶ÂàÜÂâ≤ÔºåÊ≠§Êó∂Â¶ÇÊûúÊØè‰∏™ÂàÜÂâ≤ÂêéÁöÑchunkÁöÑtokensÂ§ß‰∫émax_token_sizeÔºåÂàô‰ºöÁªßÁª≠Êåâtoken_sizeÂàÜÂâ≤ÔºàtodoÔºöËÄÉËôëÂ≠óÁ¨¶ÂàÜÂâ≤ÂêéËøáÁü≠ÁöÑchunkÂ§ÑÁêÜÔºâ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/551/comments",
    "author": "tongshiyuan",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-07T08:16:19Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ‰ΩÜÊòØÊúâ‰∏Ä‰∫õlinting errorsÔºåÈúÄË¶ÅÂú®Êèê‰∫§ÂâçËøêË°å pre-commit run --all-files Êù• check ‰∏Ä‰∏ã"
      }
    ]
  },
  {
    "number": 548,
    "title": "Documents Page numbers in metadata for Retrieval process",
    "created_at": "2025-01-06T10:17:49Z",
    "closed_at": "2025-02-17T11:02:07Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/548",
    "body": "Is it possible to retrieve the document page numbers for text-unit as metadata",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/548/comments",
    "author": "Sumith24",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-07T04:21:34Z",
        "body": "We will consider adding more document metadata in the future."
      },
      {
        "user": "Aillian",
        "created_at": "2025-01-14T06:28:52Z",
        "body": "Any updates about this? Very important feature!"
      },
      {
        "user": "lcoEntreprise",
        "created_at": "2025-01-16T14:06:02Z",
        "body": "Hello, any news? very important to be able to go back to the source of the documents to ‚Äúunderstand‚Äù and check that the model isn't hallucinating, all the more so when you have a large corpus of documents)."
      }
    ]
  },
  {
    "number": 547,
    "title": "Fix:Optimized logic for automatic switching modes when keywords do not exist",
    "created_at": "2025-01-06T09:01:00Z",
    "closed_at": "2025-01-07T13:51:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/547",
    "body": "Keep the logic in  `kg_query` and `_build_query_context `consistent, and avoid duplicate and redundant processing.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/547/comments",
    "author": "n3A87",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-07T08:17:47Z",
        "body": "Thanks for your contribution! But there are some linting errors. Please make sure to run pre-commit run --all-files before submitting to ensure all linting checks pass."
      }
    ]
  },
  {
    "number": 545,
    "title": "Enhance the llm_cache_kv_store, enable the llm_cache for entity extraction and revise readme",
    "created_at": "2025-01-06T04:59:22Z",
    "closed_at": "2025-01-06T07:24:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/545",
    "body": "To solve the paint point that when the beginners trying to run the first test, it might failed somewhere, and the LLM response for entity extractions are all wasted, gone with money and time.\r\n- Provide an option to enable the llm cache for entity extraction (optional and by default not enabled). Which can significantly save the beginner's money and time.\r\n- Revised the readme about postgres implementation.\r\n- Added tutorial for contributors to do pre-commit.\r\n\r\nTested the codes with below scenarios:\r\n- Test with enabling the llm cache, for default storage: passed\r\n- Test with disabling the llm cache, for default storage: passed\r\n- Test with enabling the llm cache, for postgres storage: passed\r\n- Test with disabling the llm cache, for postgres storage: passed",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/545/comments",
    "author": "ShanGor",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-06T07:24:56Z",
        "body": "Thanks for your contributions!"
      }
    ]
  },
  {
    "number": 542,
    "title": "Ê∑ªÂä†ÈÄâÂèñNeo4jÊåáÂÆöÊï∞ÊçÆÂ∫ìÂäüËÉΩÁöÑÊîØÊåÅ",
    "created_at": "2025-01-04T13:52:35Z",
    "closed_at": "2025-01-05T04:46:45Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/542",
    "body": "issue:#535 ‰∏≠ÊèêÂà∞neo4j_impl‰∏≠ÊòØÂê¶ËÉΩÂ§üÂÆûÁé∞ÈÄâÂèñÊåáÂÆöÊï∞ÊçÆÂ∫ìÁöÑÂäüËÉΩ„ÄÇ‰∫ãÂÆû‰∏äÔºåNeo4j Desktop/EnterpriseÁâà‰∏≠ÊòØÊîØÊåÅÂàõÂª∫ÂíåÂàáÊç¢‰∏çÂêåÁöÑÊï∞ÊçÆÂ∫ìÁöÑÂäüËÉΩÁöÑ„ÄÇÂú®ËøôÈáåÊàëÂÅö‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂÆûÁé∞„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/542/comments",
    "author": "xiyihan0",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-01-04T14:15:37Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ‰ΩÜÊòØÊúâ‰∏Ä‰∫õlinting errorsÔºåÈúÄË¶ÅÂú®Êèê‰∫§ÂâçËøêË°å pre-commit run --all-files Êù• check ‰∏Ä‰∏ã"
      },
      {
        "user": "xiyihan0",
        "created_at": "2025-01-04T14:35:58Z",
        "body": "ÊÑüË∞¢ÊåáÊ≠£ÔºÅËøôÈáåÂ∑≤Áªè‰øÆÂ§ç‰∫ÜÊ†ºÂºèÈóÆÈ¢ò‰∫Ü"
      }
    ]
  },
  {
    "number": 535,
    "title": "How to Distinguish Different Databases When Using Neo4j for Storage",
    "created_at": "2025-01-02T09:26:28Z",
    "closed_at": "2025-02-17T11:02:03Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/535",
    "body": "Hello, we are encountering some issues regarding multi-database management and data isolation while using Neo4j for data storage. The details are as follows:\r\n\r\nHow to specify the database in the code?\r\n\r\nIs it possible to explicitly specify the database in the code?\r\nIs database distinction achieved by using different users?\r\n\r\nShould different databases be distinguished by assigning different users?\r\nLooking forward to your reply. Thank you!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/535/comments",
    "author": "SherlockBULABULA",
    "comments": [
      {
        "user": "CJ-xchina",
        "created_at": "2025-01-02T09:34:18Z",
        "body": "same question ! "
      },
      {
        "user": "Linkliqi",
        "created_at": "2025-01-03T03:14:31Z",
        "body": "same question"
      },
      {
        "user": "LarFii",
        "created_at": "2025-01-03T04:52:54Z",
        "body": "Currently, distinction is achieved by specifying different instances. Of course, using different users is also a viable option."
      }
    ]
  },
  {
    "number": 530,
    "title": "BUG : run the code of \"rag.insert...\", Matrix content is lost.",
    "created_at": "2024-12-30T13:44:33Z",
    "closed_at": "2025-02-17T11:02:02Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/530",
    "body": "when I ran the program of \"rag.insert()...\" , I found my llm was offline.\r\nso I stoped my program.\r\nAfter I started my llm, went on to run the program, my program had error as:   json.decoder.JSONDecodeError: Expecting value: line 1 column\r\n\r\nI checked vdb_relationships.json.  \r\n\"matrix\":   the content had lost.\r\n\r\n ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/530/comments",
    "author": "KevinZhuoGpt",
    "comments": [
      {
        "user": "Idol-Dou2021",
        "created_at": "2025-01-13T06:02:20Z",
        "body": "Ëß£ÂÜ≥‰∫ÜÂêóÔºåÊàë‰πüÈÅáÂà∞Ëøô‰∏™ÈóÆÈ¢ò‰∫ÜÔºåÊúÄÂêé‰Ω†ÊòØ‰ΩøÁî®Âì™‰∏™Â§ßÊ®°ÂûãÁöÑÂë¢"
      }
    ]
  },
  {
    "number": 527,
    "title": "fix: change exception type",
    "created_at": "2024-12-29T17:50:27Z",
    "closed_at": "2024-12-30T18:46:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/527",
    "body": "we should use `APITimeoutError` instead of `Timeout` in `retry_if_exception_type` because `Timeout` is not a a kind of `BaseException`. But `retry_if_exception_type`  needs the input type to be `BaseException`.\r\n\r\n```\r\nclass retry_if_exception_type(retry_if_exception):\r\n    \"\"\"Retries if an exception has been raised of one or more types.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        exception_types: typing.Union[\r\n            typing.Type[BaseException],\r\n            typing.Tuple[typing.Type[BaseException], ...],\r\n        ] = Exception,\r\n    ) -> None:\r\n        self.exception_types = exception_types\r\n        super().__init__(lambda e: isinstance(e, exception_types))\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/527/comments",
    "author": "ChenZiHong-Gavin",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-30T04:04:49Z",
        "body": "Thanks! But there are some linting errors. Please make sure to run pre-commit run --all-files before submitting to ensure all linting checks pass."
      },
      {
        "user": "ChenZiHong-Gavin",
        "created_at": "2024-12-30T17:36:48Z",
        "body": "> Thanks! But there are some linting errors. Please make sure to run pre-commit run --all-files before submitting to ensure all linting checks pass.\r\n\r\nSure. I've fixed the linting errors."
      }
    ]
  },
  {
    "number": 519,
    "title": "ÂêëÂéüÁü•ËØÜÂõæË∞±‰∏≠Âä†ÂÖ•Êñ∞Êï∞ÊçÆÂèØËÉΩÂØºËá¥ LLM ÂØπÊóßÊï∞ÊçÆÁöÑÊïàÊûúÂèòÂ∑ÆÊÄé‰πàËß£ÂÜ≥",
    "created_at": "2024-12-27T13:56:01Z",
    "closed_at": "2024-12-30T01:28:30Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/519",
    "body": "ÂêëÂéüÁü•ËØÜÂõæË∞±‰∏≠Âä†ÂÖ•Êñ∞Êï∞ÊçÆÂèØËÉΩÂØºËá¥ LLM ÂØπÊóßÊï∞ÊçÆÁöÑÊïàÊûúÂèòÂ∑ÆÊÄé‰πàËß£ÂÜ≥",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/519/comments",
    "author": "ShayLyu",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-29T12:14:43Z",
        "body": "Áé∞Âú®Êñ∞Â¢û‰∫ÜÂõæË∞±ÂØπÊó∂Â∫èÈóÆÈ¢òÁöÑÊîØÊåÅÔºåLLM‰ºöÂæóÂà∞Êñ∞ÊóßÊï∞ÊçÆÊèíÂÖ•ÁöÑÊó∂Èó¥Êà≥ÔºåËÉΩÂ§üËß£ÂÜ≥‰∏ÄÈÉ®ÂàÜÈóÆÈ¢ò"
      }
    ]
  },
  {
    "number": 516,
    "title": "Fix the async issue while running on Windows",
    "created_at": "2024-12-27T04:06:51Z",
    "closed_at": "2024-12-27T07:44:02Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/516",
    "body": "When running with Apache AGE impl on Windows (with psycopg), it will got async loop error, the fix is to register asyncio with WindowsSelectorEventLoopPolicy. Only effect on Windows. Tested okay.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/516/comments",
    "author": "ShanGor",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-27T07:43:36Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 507,
    "title": "API Installation Options and Documentation Improvements",
    "created_at": "2024-12-24T09:34:39Z",
    "closed_at": "2024-12-26T01:09:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/507",
    "body": "## Changes\r\nThis PR introduces several improvements to the installation process and documentation:\r\n\r\n1. Added API support as an optional installation feature:\r\n   - Added extras_require for API dependencies\r\n   - Separated core and API requirements\r\n   - Made API installation optional using `pip install \"lightrag-hku[api]\"`\r\n\r\n2. Reorganized API implementation:\r\n   - Created proper entry points for all servers\r\n   - Added consistent command-line interface\r\n   - Improved server configuration options\r\n\r\n3. Unified documentation in README.md:\r\n   - Combined all API documentation\r\n   - Added clear installation instructions\r\n   - Included comprehensive configuration options\r\n   - Added detailed usage examples\r\n   - Documented automatic vectorization features\r\n\r\n## Benefits\r\n- Cleaner installation process\r\n- Reduced dependencies for users who don't need API\r\n- Better organized codebase\r\n- Improved documentation and examples\r\n- Clearer API usage instructions\r\n\r\n## Testing\r\n- Tested installation with and without API support\r\n- Verified all entry points work correctly\r\n- Confirmed documentation accuracy\r\n- Tested automatic vectorization features\r\n\r\n## Notes\r\nThis PR improves the overall user experience by making the API optional and providing clearer documentation while maintaining all existing functionality.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/507/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-24T11:20:55Z",
        "body": "Thank you for your contribution! However, after merging the previous PR, there are now some conflicting files. I've tried to resolve the conflicts, but I wasn't able to succeed. Could you please assist me in resolving them?"
      },
      {
        "user": "ParisNeo",
        "created_at": "2024-12-25T16:50:11Z",
        "body": "Let me recover the latest version and try to fix the merge errors"
      },
      {
        "user": "ParisNeo",
        "created_at": "2024-12-25T17:02:08Z",
        "body": "oh I see the problem :)\r\nSomeone else did add an api but only for azure openai and it doesn't install as mine (the user still needs to do many things manually).\r\nMy apis are callable once installed with pip install -e . [api] or if you build a pipy module they'll be able to easily install it with pip directly.\r\n\r\nthe user will be able after installing to do:\r\nollama-lightrag-server --model mistral-nemo:latest --embedding-model bge-m3 --embedding-dim 1024\r\n\r\nfrom anywhere in the system.   \r\n\r\n\r\nI respect the work of the other dude so I'll just merge the two and provide both options.\r\nIf he acceptsd I can harmonize our api endpoints. I wasd planning on adding litellm, openwebui and others."
      },
      {
        "user": "ParisNeo",
        "created_at": "2024-12-25T17:05:30Z",
        "body": "OK, merged correctly.\r\nYou can accept if you want."
      },
      {
        "user": "ParisNeo",
        "created_at": "2024-12-25T17:29:01Z",
        "body": "As of the harmonization of the endpoints it looks like he just copied my endpoints so they are exactly the same.\r\n:)\r\nHe also copied my documentation.\r\n\r\nI have no problem with that. He added a new functionality so that's fine.\r\n\r\nIf you please I can just remove the readme he added and put everything with my documentation in the main README and add change his code to be also directly callable from anywhere.\r\n\r\nLet's start by accepting this pull request then I'll do that next :)"
      },
      {
        "user": "LarFii",
        "created_at": "2024-12-26T07:10:38Z",
        "body": "> As of the harmonization of the endpoints it looks like he just copied my endpoints so they are exactly the same. :) He also copied my documentation.\r\n> \r\n> I have no problem with that. He added a new functionality so that's fine.\r\n> \r\n> If you please I can just remove the readme he added and put everything with my documentation in the main README and add change his code to be also directly callable from anywhere.\r\n> \r\n> Let's start by accepting this pull request then I'll do that next :)\r\n\r\nLooking forward to your future contributions! : )"
      }
    ]
  },
  {
    "number": 505,
    "title": "GremlinStorage: support more Gremlin compatible DBs",
    "created_at": "2024-12-23T14:11:16Z",
    "closed_at": "2024-12-23T16:03:35Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/505",
    "body": "Tested on ArcadeDB with Gremlin plugin. The main change is using \"entity_name\" vertex property instead of the label as a node_id since different implementations have different restrictions on label names.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/505/comments",
    "author": "alllexx88",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-23T16:03:43Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 504,
    "title": "Added Azure OpenAI api sample with streaming",
    "created_at": "2024-12-23T13:38:27Z",
    "closed_at": "2024-12-24T11:01:46Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/504",
    "body": "This pull request introduces several significant changes to the API server, including new environment variables, an updated README for Azure OpenAI, and enhancements to the `azure_openai_complete_if_cache` function. The most important changes are summarized below:\r\n\r\n### Azure OpenAI API:\r\n\r\n* Added new API sample for Azure OpenAI including query with streaming response.\r\n\r\n### Environment Configuration:\r\n\r\n* Added new environment variables for Azure OpenAI API configuration in `api/.env.aoi.example` and `examples/.env.oai.example`.\r\n\r\n### Documentation:\r\n\r\n* Updated `api/README_AZURE_OPENAI.md` with comprehensive setup instructions, feature descriptions, and API endpoint details for the LightRAG API server.\r\n\r\n### Dependencies:\r\n\r\n* Updated `api/requirements.txt` to include new dependencies such as `nest_asyncio`, `lightrag-hku`, `tqdm`, `aioboto3`, `numpy`, `ollama`, `torch`, `openai`, `tenacity`, `transformers`, `tiktoken`, `nano_vectordb`, and `python-dotenv`.\r\n\r\n### Code Enhancements:\r\n\r\n* Enhanced the `azure_openai_complete_if_cache` function in `lightrag/llm.py` to handle different response formats and support asynchronous streaming of responses.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/504/comments",
    "author": "congiuluc",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-23T16:18:14Z",
        "body": "Thank you for your contributions! I really appreciate the work you've done.\r\n\r\nI have a couple of small suggestions regarding `requirements.txt`:\r\n\r\n1.  `torch`  and `ollama` may not be strictly necessary.\r\n2. The `lightrag-hku` package might not always be the latest version, so I would recommend installing it from the source instead.\r\n\r\nAlso, I noticed some linting errors. Please make sure to run pre-commit run --all-files before submitting to ensure all linting checks pass.\r\n\r\nThanks again for your contributions!\r\n"
      },
      {
        "user": "congiuluc",
        "created_at": "2024-12-24T09:05:32Z",
        "body": "Hi @LarFii about `requirements.txt` I added all the dependencies needed to deploy the web api on Azure.\r\nI Know that  `torch`  and `ollama` are not necessary but are needed by `lightrag-hku` package.\r\nI run `pre-commit run --all-files` and now the PR is ready for review."
      }
    ]
  },
  {
    "number": 502,
    "title": "Add LoLLMs Integration to LightRAG",
    "created_at": "2024-12-21T23:40:48Z",
    "closed_at": "2024-12-22T15:07:39Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/502",
    "body": "## Description\r\nThis PR adds support for LoLLMs (Lord of Large Language Models) as a new LLM backend for LightRAG, expanding the available options beyond Ollama. LoLLMs is a versatile AI framework that supports multiple LLM backends and provides comprehensive text generation and embedding capabilities.\r\n\r\n## Key Features\r\n- Added `lollms_model_if_cache` for text generation support\r\n- Added `lollms_model_complete` as a high-level interface\r\n- Added `lollms_embed` for document embedding support\r\n- Full streaming support for text generation\r\n- Compatible with existing LightRAG interfaces\r\n- A fastapi service using lollms as backend\r\n\r\n## Implementation Details\r\n- New async client implementation for LoLLMs server communication\r\n- Support for both streaming and non-streaming text generation\r\n- Embedding generation using LoLLMs' configurable vectorizers (semantic, tfidf, openai, ollama)\r\n- Maintains compatibility with existing workflow and interfaces\r\n\r\n## Benefits\r\n- Expanded LLM backend options for users\r\n- Access to LoLLMs' diverse model ecosystem\r\n- Multiple embedding vectorizer options\r\n- Seamless integration with existing LightRAG features\r\n\r\n## Usage Example\r\n```python\r\nfrom lightrag import lollms_model_complete\r\n\r\n# Text generation\r\nresponse = await lollms_model_complete(\r\n    prompt=\"Your prompt\",\r\n    system_prompt=\"Optional system prompt\"\r\n)\r\n\r\n# Embeddings\r\nembeddings = await lollms_embed([\"Text 1\", \"Text 2\"])\r\n```\r\n\r\n## Testing\r\n- Tested text generation with both streaming and non-streaming modes\r\n- Verified embedding generation with different vectorizer configurations\r\n- Ensured compatibility with existing LightRAG features\r\n\r\n## Dependencies\r\n- Requires LoLLMs server running and accessible\r\n- Added aiohttp for async HTTP communication",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/502/comments",
    "author": "ParisNeo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-22T15:07:35Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 495,
    "title": "Poor node deduplication - how to have more control?",
    "created_at": "2024-12-19T16:55:32Z",
    "closed_at": "2025-02-17T11:01:33Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/495",
    "body": "I see issues with node deduplication.  Ingesting transcripts of informal conversations, I am getting, for example, duplicate nodes for what is clearly the same person, e.g., \"John Doe\" and \"John T. Doe\".  Is there a way to have more control over this, of even post-training, a capability to collapse these nodes into a single one?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/495/comments",
    "author": "jbartot",
    "comments": [
      {
        "user": "rabner",
        "created_at": "2024-12-19T19:24:36Z",
        "body": "I've posted the same question on discord yesterday. I'm testing LightRAG with scientific papers which use abbreviated names very often. The same is for entities that are rephrased. "
      },
      {
        "user": "jbartot",
        "created_at": "2024-12-19T23:21:43Z",
        "body": "@rabner - I guess I could try to preprocess the raw text and normalize the names before training, but being able to have some control over the deduping seems like a basic capability that is still missing.  "
      },
      {
        "user": "flight505",
        "created_at": "2025-01-05T17:08:22Z",
        "body": "@jbartot Hi I am having the same issues, also dealing with scientific papers as @rabner. Did any of you find a solution? Also I am trying to insure references are consistant which seems like a hot topic in the RAG world. One obvious issue is papers are referenced with different styles even based on publisher. I have tried using Marker with llm and made some changes to the joint metadata.json, each paper then has several fields including the equations and references. I am not sure if is the right way of doing it, since I am also creating opportunities for introducing errors.       \r\nWhat are you using for converting pdf to txt? "
      }
    ]
  },
  {
    "number": 491,
    "title": "Please add streaming output, as it is very useful for us.",
    "created_at": "2024-12-19T05:42:23Z",
    "closed_at": "2025-02-17T11:01:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/491",
    "body": "streaming output",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/491/comments",
    "author": "valueLzy",
    "comments": [
      {
        "user": "adamwuyu",
        "created_at": "2025-01-19T13:11:52Z",
        "body": "Is it done?"
      }
    ]
  },
  {
    "number": 490,
    "title": "Error calling ChatGLM Embedding API",
    "created_at": "2024-12-19T03:22:47Z",
    "closed_at": "2025-02-17T11:01:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/490",
    "body": "zhipuË∞ÉÁî®APIÂá∫ÈîôÔºåËÄåÂèØ‰ª•Áªô‰∏™Á§∫‰æãÂêóÔºå‰ª•‰∏ãÊòØÊñáÊ°£‰ª£Á†ÅÂíåÊä•Èîô‰ø°ÊÅØÔºåË∞ÉÁî®ÁöÑÂÖçË¥πÊ®°ÂûãÔºåÂπ∂Ê≤°ÊúâÊ¨†Ë¥π\r\nasync def llm_model_func(\r\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\r\n) -> str:\r\n    return await zhipu_complete_if_cache(\r\n        prompt,\r\n        \"glm-4-Flash\",\r\n        system_prompt=system_prompt,\r\n        history_messages=history_messages,\r\n        api_key=os.getenv(\"ZHIPUAI_API_KEY\"),\r\n        **kwargs\r\n    )\r\n\r\nasync def embedding_func(texts: list[str]) -> np.ndarray:\r\n    return await zhipu_embedding(\r\n        texts,\r\n        model=\"embedding-3\",\r\n        api_key=os.getenv(\"ZHIPUAI_API_KEY\"),\r\n    )\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=384,\r\n        max_token_size=8192,\r\n        func=embedding_func\r\n    )\r\n)\r\nÊä•Èîô‰ø°ÊÅØÔºö\r\nException: Error calling ChatGLM Embedding API: Error code: 429, with error text {\"error\":{\"code\":\"1113\",\"message\":\"ÊÇ®ÁöÑË¥¶Êà∑Â∑≤Ê¨†Ë¥πÔºåËØ∑ÂÖÖÂÄºÂêéÈáçËØï„ÄÇ\"}}\r\nGenerating embeddings:   0%|                                                    | 0/2 [00:25<?, ?batch/s]",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/490/comments",
    "author": "Dullne",
    "comments": [
      {
        "user": "thundax-lyp",
        "created_at": "2024-12-20T02:38:29Z",
        "body": "ÊèêÁ§∫EmbeddingÊé•Âè£Ê¨†Ë¥πÔºåembedding-3 ‰∏çÊòØÂÖçË¥πÁöÑÂêß"
      },
      {
        "user": "Dullne",
        "created_at": "2024-12-20T14:42:01Z",
        "body": "> ÊèêÁ§∫EmbeddingÊé•Âè£Ê¨†Ë¥πÔºåembedding-3 ‰∏çÊòØÂÖçË¥πÁöÑÂêß\r\n\r\nembedding-2‰πüÊòØ‰∏ÄÊ†∑ÁöÑÈîôËØØ\r\n"
      }
    ]
  },
  {
    "number": 489,
    "title": "ENCODER = tiktoken.encoding_for_model(model_name) tiktoken openaiÔºü",
    "created_at": "2024-12-19T03:12:08Z",
    "closed_at": "2025-02-17T11:01:31Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/489",
    "body": "Actually, I want to try the local HF model, but according to the sample code, an error will be reported\r\n('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')) I looked carefully,\r\ndef encode_string_by_tiktoken(content: str, model_name: str = \"gpt-4o\"):\r\nglobal ENCODER\r\nif ENCODER is None:\r\nENCODER = tiktoken.encoding_for_model(model_name)\r\nWill chunking call openai? this make me confused,can you tell me why? and how to use local-noly models? thank you",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/489/comments",
    "author": "kyre-99",
    "comments": [
      {
        "user": "yedunnn",
        "created_at": "2025-02-17T10:18:25Z",
        "body": "are you solve it?\n"
      }
    ]
  },
  {
    "number": 487,
    "title": "Why Using Labels for Node Names and Label-Based Search Optimization in Neo4j",
    "created_at": "2024-12-18T14:51:11Z",
    "closed_at": "2025-02-17T11:01:30Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/487",
    "body": "I'm currently using LightRAG with Neo4J implementations, I have some question about the get_node function:\r\nI would like to know why are node names typically implemented as labels rather than as properties within the nodes. And why the node search is not optimized by using an Index on that name property",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/487/comments",
    "author": "domenicopolicastro",
    "comments": [
      {
        "user": "hixulei",
        "created_at": "2024-12-21T18:55:33Z",
        "body": "Âõ†‰∏∫Ëøô‰∏™ÈóÆÈ¢òÔºåÂØºËá¥‰ΩøÁî®neo4jÊï∞ÊçÆÂ∫ìÊü•ËØ¢ÂÆåÂÖ®Êü•ËØ¢‰∏çÂà∞ÁªìÊûú„ÄÇ  ËøôÊòØ‰∏™bug"
      },
      {
        "user": "ChangTianshu",
        "created_at": "2024-12-26T13:51:23Z",
        "body": "Same issue for me,  the node label is incorrect and the node has no \"name\" property.  "
      }
    ]
  },
  {
    "number": 483,
    "title": "support TiDBGraphStorage",
    "created_at": "2024-12-18T03:01:39Z",
    "closed_at": "2024-12-19T08:42:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/483",
    "body": "implement graph storage by tidb\r\n- add `TiDBGraphStorage`",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/483/comments",
    "author": "Weaxs",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-19T08:42:47Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 468,
    "title": "Possibility to add metadata to chunks and return that with the result in json structure",
    "created_at": "2024-12-14T18:44:53Z",
    "closed_at": "2025-02-17T11:00:31Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/468",
    "body": "It would be great to add metadata (page#, document-title, etc.) to the chunks and retrieve them along with the answer in json format for reference purposes. Would be a great check for grounding + actually a classic RAG functionality to provide references (especially for larger set of documents) along with the answers for follow-up actions and documentation. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/468/comments",
    "author": "ewagner70",
    "comments": [
      {
        "user": "Arslan-Mehmood1",
        "created_at": "2024-12-15T16:11:04Z",
        "body": "+1"
      },
      {
        "user": "jbartot",
        "created_at": "2024-12-16T18:41:43Z",
        "body": "+1"
      },
      {
        "user": "TZhanabekov",
        "created_at": "2024-12-22T17:19:36Z",
        "body": "+1"
      },
      {
        "user": "Aillian",
        "created_at": "2025-01-14T05:33:37Z",
        "body": "any updates regarding this? Very important feature!"
      },
      {
        "user": "Arslan-Mehmood1",
        "created_at": "2025-01-14T05:35:46Z",
        "body": "> any updates regarding this? Very important feature!\r\n\r\n+1"
      },
      {
        "user": "ArindamRoy23",
        "created_at": "2025-02-17T06:47:03Z",
        "body": "+1 "
      }
    ]
  },
  {
    "number": 463,
    "title": "ËØ∑ÈóÆÂ¶Ç‰ΩïÊéßÂà∂ÊäΩÂèñÂÆû‰ΩìÂíåÂÖ≥Á≥ªËøáÁ®ã‰∏≠Â§ßÊ®°ÂûãÁöÑÂπ∂ÂèëÈáè",
    "created_at": "2024-12-13T08:38:09Z",
    "closed_at": "2024-12-20T08:54:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/463",
    "body": "ÊàëÁúã‰ª£Á†Å‰∏≠Áî®ÁöÑAsyncOpenAIËøõË°åÂ§ßÊ®°ÂûãÁöÑË∞ÉÁî®Ôºå‰ΩÜÊòØÂõ†‰∏∫ÁõÆÂâçÈ°πÁõÆ‰∏≠Ê≤°ÊúâÊéßÂà∂Âπ∂ÂèëÊï∞ÈáèÁöÑÂèÇÊï∞ÔºåÂØºËá¥chunkÂàáÁöÑÂ§öÁöÑÊó∂ÂÄôÂπ∂ÂèëÈáèÂ§ßÔºåËÆ°ÁÆóÂèòÊÖ¢ÔºåÂ§ßÊ®°ÂûãÈïøÊó∂Èó¥Êó†Ê≥ïËøîÂõûÁªìÊûúÂØºËá¥Â§±Ë¥•„ÄÇ\r\nÊàëÂú®È°πÁõÆ‰∏≠Áî®semaphore = asyncio.Semaphore(max_concurrent_tasks)Â¢ûÂä†Âπ∂ÂèëÈôêÂà∂Ôºå‰ΩÜÊòØÂπ∂Êú™ÁîüÊïà„ÄÇ\r\nËØ∑ÈóÆÂêÑ‰ΩçÂ§ß‰Ω¨ÊòØÂ¶Ç‰ΩïËß£ÂÜ≥Ëøô‰∏™Âπ∂ÂèëÈáèÈóÆÈ¢òÁöÑÔºåÂ¶Ç‰ΩïËøõË°åË∞ÉÊï¥„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/463/comments",
    "author": "wi162yyxq",
    "comments": [
      {
        "user": "hixulei",
        "created_at": "2024-12-16T10:03:36Z",
        "body": "ÊàëËøôÈáå‰πüÈÅáÂà∞‰∫ÜÁ∫ø‰∏äapiÂ§ßÊ®°ÂûãÊä•ÈîôÁöÑÊÉÖÂÜµÔºåÂ∫îËØ•ÊòØËØ∑Ê±ÇËøá‰∫éÈ¢ëÁπÅ‰∫Ü„ÄÇÊ±ÇËß£„ÄÇ"
      },
      {
        "user": "kiss0840",
        "created_at": "2024-12-16T11:00:23Z",
        "body": "ÈªòËÆ§ÊâÄÊúâchunkÊï∞ÊçÆ‰∏ÄÊ¨°ÊÄßÂ§ÑÁêÜÔºåÊâπÈáèÂ§ÑÁêÜÂèØÈÅøÂÖçLLMÊãíËΩΩ„ÄÇ\r\nÊñπÊ≥ïÔºö‰øÆÊîπ operate.py ÁöÑ async def extract_entities ÊñπÊ≥ïÔºåÂ∞Ü await asyncio.gather(*task) ËøõË°åÂàÜÊâπÂ§ÑÁêÜ\r\n"
      }
    ]
  },
  {
    "number": 461,
    "title": "fix: update operate.py",
    "created_at": "2024-12-12T20:49:07Z",
    "closed_at": "2024-12-13T07:10:54Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/461",
    "body": "1. ÈÅøÂÖçÂèòÈáèÂú®ËµãÂÄº‰πãÂâçÂ∞±Ë¢´ÂºïÁî®\r\n2. Ëß£ÂÜ≥Êú™ÊâæÂà∞entityËøîÂõûNoneÂØºËá¥ÁöÑunpackÈóÆÈ¢ò",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/461/comments",
    "author": "tjyiiuan",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-13T07:10:51Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ"
      }
    ]
  },
  {
    "number": 460,
    "title": "[Enh]: add support for elasticsearch",
    "created_at": "2024-12-12T17:27:42Z",
    "closed_at": "2025-02-17T11:00:30Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/460",
    "body": "Dear LightRAG team,\r\nas elasticsearch is mainly used in scientific community, it could be good to support it both at KV, vector, and graph.\r\nwill you planned it in your backlog?\r\nThanks",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/460/comments",
    "author": "flefevre",
    "comments": [
      {
        "user": "mieco",
        "created_at": "2024-12-13T05:44:31Z",
        "body": "+1"
      }
    ]
  },
  {
    "number": 459,
    "title": "Adding Azure AI Search / Customized database",
    "created_at": "2024-12-12T16:04:10Z",
    "closed_at": "2025-02-17T11:00:30Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/459",
    "body": "Hello,\r\n\r\nThanks for the example repo examples/lightrag_azure_openai_demo.py.\r\n\r\nIs it possible to extend the repo with a customized database, e.g., Azure AI Search? I would highly appreciate any guidance or code on how to implement Azure AI Search.\r\n\r\nThanks a lot,\r\n\r\nAnna",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/459/comments",
    "author": "annawiewer",
    "comments": [
      {
        "user": "wangzhen38",
        "created_at": "2025-01-08T07:42:59Z",
        "body": "+1"
      }
    ]
  },
  {
    "number": 458,
    "title": "fix: fix variable name(entitiy->entity)",
    "created_at": "2024-12-12T16:00:38Z",
    "closed_at": "2024-12-13T07:08:44Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/458",
    "body": "fix typo",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/458/comments",
    "author": "ChenZiHong-Gavin",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-13T07:08:41Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 453,
    "title": "AsyncCompletions.create() got an unexpected keyword argument 'keyword_extraction",
    "created_at": "2024-12-11T09:09:25Z",
    "closed_at": "2024-12-11T09:11:26Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/453",
    "body": "```INFO:lightrag:Logger initialized for working directory: D:\\LightRAG-main\\test\\dickens_harry_deepseek\r\nINFO:lightrag:Load KV llm_response_cache with 308 data\r\nINFO:lightrag:Load KV full_docs with 1 data\r\nINFO:lightrag:Load KV text_chunks with 13 data\r\nINFO:lightrag:Loaded graph from D:\\LightRAG-main\\test\\dickens_harry_deepseek\\graph_chunk_entity_relation.graphml with 148 nodes, 167 edges\r\nINFO:nano-vectordb:Load (148, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': 'D:\\\\LightRAG-main\\\\test\\\\dickens_harry_deepseek\\\\vdb_entities.json'} 148 data\r\nINFO:nano-vectordb:Load (167, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': 'D:\\\\LightRAG-main\\\\test\\\\dickens_harry_deepseek\\\\vdb_relationships.json'} 167 data\r\nINFO:nano-vectordb:Load (13, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': 'D:\\\\LightRAG-main\\\\test\\\\dickens_harry_deepseek\\\\vdb_chunks.json'} 13 data\r\nÂºÄÂßãÊèêÈóÆÂêßÔºÅ\r\n‰Ω†Â•Ω\r\nAn error occurred: AsyncCompletions.create() got an unexpected keyword argument 'keyword_extraction'```\r\nÂú®Êü•ËØ¢ÁöÑÊó∂ÂÄôÊä•ÈîôÔºåËØ∑ÈóÆËØ•ÊÄé‰πàÂäû",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/453/comments",
    "author": "Z-oo883",
    "comments": [
      {
        "user": "Eric19861031",
        "created_at": "2024-12-12T06:37:44Z",
        "body": "ÂêåÈóÆ"
      },
      {
        "user": "Z-oo883",
        "created_at": "2024-12-12T06:42:25Z",
        "body": "> ÂêåÈóÆ\r\n\r\nÊõ¥Êñ∞‰πãÂêéÂ§ö‰∫Ü‰∏™ÂèÇÊï∞ÔºåÂä†‰∏äÂ∞±Â•Ω‰∫Ü„ÄÇ\r\nasync def llm_model_func(\r\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\r\n) -> str:\r\n‰∏äÈù¢Ëøô‰∏™ÂáΩÊï∞ÁöÑkeyword_extraction=FalseËøô‰∏™ÂèÇÊï∞"
      },
      {
        "user": "Eric19861031",
        "created_at": "2024-12-12T07:04:59Z",
        "body": "Ë∞¢Ë∞¢ÂïäÔºåÂéüÊù•ÊòØÂ§ö‰∫Ü‰∏™Â±ïÁ§∫"
      }
    ]
  },
  {
    "number": 452,
    "title": "support TiDB: add TiDBKVStorage, TiDBVectorDBStorage",
    "created_at": "2024-12-11T08:35:08Z",
    "closed_at": "2024-12-12T03:35:26Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/452",
    "body": "\r\nsupport tidb cloud\r\n\r\n- add TiDBKVStorage\r\n- add TiDBVectorDBStorage",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/452/comments",
    "author": "Weaxs",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-12T02:11:53Z",
        "body": "Thanks! But there are some linting errors. Please make sure to run pre-commit run --all-files before submitting to ensure all linting checks pass."
      }
    ]
  },
  {
    "number": 451,
    "title": "content_keywords & high_level_keywords",
    "created_at": "2024-12-11T04:25:56Z",
    "closed_at": "2025-02-17T11:00:28Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/451",
    "body": "The entity and relationship extraction prompt in the prompt.py file has the following line to identify content_keywords & high_level_keywords\r\n\r\n```\r\n 3. Identify high-level key words that summarize the main concepts, themes, or topics of the entire text. These should capture the \r\n overarching ideas present in the document.\r\n Format the content-level key words as (\"content_keywords\"{tuple_delimiter}<high_level_keywords>)\r\n\r\n```\r\nHowever, the sample output only include list of Entities and Relationships that were identified by LLM. So what is the purpose of that prompt to extract the content_keywords & high_level_keywords? I am very interested in the idea and just curious how I can use that information if they are extracted.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/451/comments",
    "author": "eddprogrammer",
    "comments": [
      {
        "user": "ChristianWunderlich",
        "created_at": "2024-12-11T07:30:35Z",
        "body": "@eddprogrammer in fact, the keywords are just retrieved via a LLM call. Those keywords are then embedded and used to get similar items from either the entity or relationship vector store. Depending which query type you use.\r\n\r\nSo when working with naive, local or global query types, the retrieval, embedding and usage of those keywords a key!"
      },
      {
        "user": "eddprogrammer",
        "created_at": "2024-12-11T18:43:44Z",
        "body": "Hi Christian,\r\nUnderstand keywords will be used when querying, but I am curious what is the use when building the entities and relationships stage, especially considering the fact that they are not being returned by the LLM call, or stored. "
      }
    ]
  },
  {
    "number": 449,
    "title": "Question about trained/chunked rag",
    "created_at": "2024-12-10T17:04:47Z",
    "closed_at": "2025-02-17T11:00:27Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/449",
    "body": "I have a two part question about after we train a rag model. \r\n\r\nIs there a way to retrieve the embeddings, chunks, and relationships jsons so that we don't have to reform the graph every run? If there's an example somewhere I'd love to see it. \r\n\r\nsecond part is, lets say I broke down my data into two sections and ran LightRag on both separately. If I wanted to skip out on running LightRag on the entire data, is it possible to somehow join the two files together for LightRag to pick up? Such as somehow joining the embeddings, chunks and relationship jsons together? \r\n\r\nThank you. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/449/comments",
    "author": "anguy044",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-13T10:31:39Z",
        "body": "**For the first question**: Once the graph has been inserted, you can simply reload the working directory to retrieve the embeddings, chunks, and relationships. There‚Äôs no need to reinsert the data every time.\r\n\r\n**For the second question**: You can consider merging the cache files first. By doing so, the same documents won‚Äôt trigger additional LLM calls during insertion. Once the cache files are merged, you can simply insert the combined data into one of the LightRAG instances."
      },
      {
        "user": "anguy044",
        "created_at": "2024-12-18T18:41:03Z",
        "body": "Thank you, I got the first part working fine. Will try the second part of merging caches today and see how it goes, thank you for the response. "
      }
    ]
  },
  {
    "number": 445,
    "title": "ÂÖ≥‰∫éÂçïÁ∫Ø‰ΩøÁî®LightRAG",
    "created_at": "2024-12-10T11:23:06Z",
    "closed_at": "2024-12-11T10:13:49Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/445",
    "body": "ËØ∑ÈóÆÂ¶ÇÊûú‰∏çÁî®Â§ßÊ®°ÂûãÔºåÂçïÁ∫Ø‰ΩøÁî®LightRAGÂàÜÊûê‰∏Ä‰∏™ÊñáÊ°£ÂÆû‰Ωì‰∏éÂÆû‰ΩìÈó¥„ÄÅÂÆû‰Ωì‰∏é‰∫ã‰ª∂„ÄÅ‰∫ã‰ª∂‰∏é‰∫ã‰ª∂ÁöÑÂÖ≥Á≥ªÔºåËøô‰∏™ÂèØ‰ª•ËæìÂá∫Âá∫Êù•ÂêóÔºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/445/comments",
    "author": "joseph16388",
    "comments": [
      {
        "user": "magicyuan876",
        "created_at": "2024-12-11T01:59:26Z",
        "body": "ÊèêÂèñÂÆû‰ΩìÂíåÂÖ≥Á≥ªÊú¨Ë∫´Â∞±ÊòØÁî®LLMÊù•ÊäΩÂèñÁöÑÂëÄÔºå‰∏çÁî®LLMËÇØÂÆö‰∏çË°åÂëÄ"
      },
      {
        "user": "joseph16388",
        "created_at": "2024-12-11T10:13:47Z",
        "body": "> ÊèêÂèñÂÆû‰ΩìÂíåÂÖ≥Á≥ªÊú¨Ë∫´Â∞±ÊòØÁî®LLMÊù•ÊäΩÂèñÁöÑÂëÄÔºå‰∏çÁî®LLMËÇØÂÆö‰∏çË°åÂëÄ\r\n\r\nÂ•ΩÁöÑÔºå3Q"
      }
    ]
  },
  {
    "number": 444,
    "title": "Fix/lazy import",
    "created_at": "2024-12-10T09:15:21Z",
    "closed_at": "2024-12-11T06:19:48Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/444",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/444/comments",
    "author": "davidleon",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-11T06:19:46Z",
        "body": "Thank you for your contribution! Next time, you can consider combining multiple PRs into one for better organization."
      }
    ]
  },
  {
    "number": 439,
    "title": "Looking for 'openaipublic.blob.core.windows.net' even using only local ollama models",
    "created_at": "2024-12-10T00:55:01Z",
    "closed_at": "2025-02-17T11:00:26Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/439",
    "body": "Using exclusively local models (LLM and embeddings, in this case using Ollama), LighRAG fails to run because it seems to try to connect to openaipublic.blob.core.windows.net. How to prevent this from happening? In a secure environment and a disconnected one, as it stands LightRAG won't work because of this.\r\n\r\nThe error message: \r\n.venv/lib/python3.11/site-packages/requests/adapters.py\", line 700, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/o200k_base.tiktoken (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f8169435350>: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno -3] Temporary failure in name resolution)\"))\r\n\r\nCurrently this is the only issue for running LightRAG in a securely disconnected environment.\r\n\r\nAny idea how to solve this?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/439/comments",
    "author": "miary",
    "comments": [
      {
        "user": "junzhin",
        "created_at": "2024-12-12T07:08:46Z",
        "body": "+1"
      }
    ]
  },
  {
    "number": 436,
    "title": "chore: update llm.py",
    "created_at": "2024-12-09T13:08:14Z",
    "closed_at": "2024-12-09T14:24:14Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/436",
    "body": "intialize -> initialize",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/436/comments",
    "author": "eltociear",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T14:24:11Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 435,
    "title": "fix unicode escape. for the case of \"/utils\" in the response.",
    "created_at": "2024-12-09T10:56:14Z",
    "closed_at": "2024-12-09T12:54:58Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/435",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/435/comments",
    "author": "davidleon",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T12:54:56Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 433,
    "title": "‰ΩøÁî® pip install lightrag-hku ÂÆâË£ÖÈúÄË¶ÅÈ¢ùÂ§ñÂÆâË£ÖÂÖ∂‰ªñÂ∫ìÊâçËÉΩËøêË°å",
    "created_at": "2024-12-09T09:45:08Z",
    "closed_at": "2025-02-17T11:00:25Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/433",
    "body": "ÊòØÁî® pip ÂÆâË£ÖÈúÄË¶ÅÂÆâË£ÖÈ¢ùÂ§ñÁöÑÂ∫ìÔºåÊàëÂ∞±ÊÉ≥ÁÆÄÂçïËøêË°å‰∏Ä‰∫õ‰æãÂ≠êÔºåÊ≤°ÊúâÁî®Âà∞ neo4j„ÄÅollama„ÄÅnano_vectordb„ÄÅoracledb..ÔºåÈúÄË¶Å‰∏Ä‰∏™‰∏Ä‰∏™ÂÆâË£ÖÔºåÊÑüËßâÊòØ‰∏çÊòØÂèØ‰ª•‰ºòÂåñ‰∏ãÔºåÂèòÊàêÂèØÈÄâÈ°πÁî®Âà∞Êó∂Âú®‰ΩøÁî®„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/433/comments",
    "author": "HuiDBK",
    "comments": [
      {
        "user": "t1waz",
        "created_at": "2024-12-10T14:48:44Z",
        "body": "learn english pls"
      }
    ]
  },
  {
    "number": 432,
    "title": "fix(lightrag): use is_closed() instead of _closed",
    "created_at": "2024-12-09T09:12:50Z",
    "closed_at": "2024-12-09T10:08:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/432",
    "body": "`is_closed()` is better practice",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/432/comments",
    "author": "ChenZiHong-Gavin",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T10:08:37Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 429,
    "title": "fix extra kwargs error: keyword_extraction.",
    "created_at": "2024-12-09T07:32:20Z",
    "closed_at": "2024-12-09T10:07:31Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/429",
    "body": "add lazy_external_load to reduce external lib deps whenever it's not necessary for user.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/429/comments",
    "author": "davidleon",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T10:07:27Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 426,
    "title": "FIX: Correct quote formatting for JINA_API_KEY usage",
    "created_at": "2024-12-09T06:16:53Z",
    "closed_at": "2024-12-09T10:01:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/426",
    "body": "### Description:\r\nThis pull request resolves a syntax issue in the `Authorization` header definition within the `jina_embedding` function. The double quotes in the f-string caused parsing errors due to improper nesting. The fix replaces the nested double quotes with single quotes for the `os.environ` key.\r\n\r\n### Changes:\r\n- **File Modified**: `llm.py`\r\n- Updated the `Authorization` header line:\r\n  ```python\r\n  \"Authorization\": f\"Bearer {os.environ['JINA_API_KEY']}\",\r\n  ```\r\n\r\n### Impact:\r\n- Prevents syntax errors during runtime.\r\n- Enhances readability and follows best practices for quote usage in Python.\r\n\r\n### Testing:\r\n- Code runs without syntax issues.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/426/comments",
    "author": "donbr",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T10:01:55Z",
        "body": "Thanks! We have fixed this issue."
      }
    ]
  },
  {
    "number": 425,
    "title": "Fix: syntaxError",
    "created_at": "2024-12-09T06:12:27Z",
    "closed_at": "2024-12-09T09:59:14Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/425",
    "body": "simple fix double quote error.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/425/comments",
    "author": "guyuecode",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T09:59:21Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 423,
    "title": "add jina embedding",
    "created_at": "2024-12-08T14:21:38Z",
    "closed_at": "2024-12-09T02:18:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/423",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/423/comments",
    "author": "davidleon",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T02:18:47Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 417,
    "title": "Add support for OpenAI Compatible Streaming output and delete unreachable code",
    "created_at": "2024-12-07T02:38:00Z",
    "closed_at": "2024-12-09T09:56:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/417",
    "body": "#402",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/417/comments",
    "author": "partoneplay",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T09:56:27Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 415,
    "title": "Fix: Ensure entity_or_relation_name is a string in _handle_entity_relation_summary",
    "created_at": "2024-12-06T15:32:17Z",
    "closed_at": "2024-12-09T09:36:07Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/415",
    "body": "# Description\r\n\r\nI encountered an issue where the function `_handle_entity_relation_summary` raised the following error:\r\n\r\n```python\r\nArgument 'entity_or_relation_name' has incorrect type (expected str, got tuple)\r\n```\r\n\r\nThis error occurred because the function was being called with a tuple `(src_id, tgt_id)` instead of a string. To resolve this issue, I made the following changes:\r\n\r\n### Converted the Tuple to a Formatted String\r\n\r\nReplaced the call to `_handle_entity_relation_summary` with:\r\n\r\n```python\r\ndescription = await _handle_entity_relation_summary(\r\n    f\"({src_id}, {tgt_id})\", description, global_config\r\n)\r\n```\r\n\r\nThis ensures that the `entity_or_relation_name` argument is always passed as a string.\r\n\r\n### Maintained Consistency\r\n\r\nUpdated the code to use formatted strings for better readability and alignment with the function's expectations.\r\n\r\n---\r\n\r\n## Steps to Reproduce the Issue\r\n\r\n1. Use the `_merge_edges_then_upsert` function with a graph edge where the tuple `(src_id, tgt_id)` is passed to `_handle_entity_relation_summary`.\r\n2. Observe the type mismatch error when the function is executed.\r\n\r\n---\r\n\r\n## Solution\r\n\r\nEnsure `entity_or_relation_name` is a string by using a formatted string `f\"({src_id}, {tgt_id})\"` instead of the tuple.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/415/comments",
    "author": "SaujanyaV",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T09:36:03Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 409,
    "title": "Update oracle_impl.py",
    "created_at": "2024-12-06T03:06:37Z",
    "closed_at": "2024-12-06T03:08:29Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/409",
    "body": "Fixed typing error in python3.9",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/409/comments",
    "author": "zsuroy",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-06T03:08:26Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 402,
    "title": "ightrag_api_openai_compatible_demo.py Âí±‰ª¨Ëøô‰∏™API‰ªÄ‰πàÊó∂ÂÄôÂèØ‰ª•ÊîØÊåÅÊµÅÂºèËæìÂá∫Âë¢Ôºü",
    "created_at": "2024-12-05T10:40:50Z",
    "closed_at": "2025-02-17T10:59:52Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/402",
    "body": "ightrag_api_openai_compatible_demo.py Âí±‰ª¨Ëøô‰∏™API‰ªÄ‰πàÊó∂ÂÄôÂèØ‰ª•ÊîØÊåÅÊµÅÂºèËæìÂá∫Âë¢Ôºü\r\nÁîöÊòØÊúüÂæÖÔºÅÔºÅÔºÅÔºÅ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/402/comments",
    "author": "lonnys",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T13:28:35Z",
        "body": "ÂêéÁª≠Êàë‰ª¨‰ºöÊ∑ªÂä†Ëøô‰∏™ÂäüËÉΩ"
      }
    ]
  },
  {
    "number": 401,
    "title": "‰øÆÊîπÂÆû‰ΩìÂíåÂÖ≥Á≥ªÊèêÂèñÊñπÂºèÔºåÂàÜ‰∏∫‰∏§Ê≠•ËøõË°åÔºåÈ¶ñÂÖàÊèêÂèñÂÆû‰ΩìÔºåÁÑ∂ÂêéÊ†πÊçÆÂÆû‰ΩìÊèêÂèñÂÖ≥Á≥ª„ÄÇ",
    "created_at": "2024-12-05T08:36:27Z",
    "closed_at": "2025-01-16T05:16:09Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/401",
    "body": "1.Â¢ûÂä†prompt_cn.pyÊñá‰ª∂ÔºåÊîπ‰∏∫‰ΩøÁî®‰∏≠ÊñáÊñπÂºèÊèêÂèñÂÆû‰ΩìÂíåÂÖ≥Á≥ªÔºõ\r\n2.‰øÆÊîπÂÆû‰ΩìÂíåÂÖ≥Á≥ªÊèêÂèñÊñπÂºèÔºåÂàÜ‰∏∫‰∏§Ê≠•ËøõË°åÔºåÈ¶ñÂÖàÊèêÂèñÂÆû‰ΩìÔºåÁÑ∂ÂêéÊ†πÊçÆÂÆû‰ΩìÊèêÂèñÂÖ≥Á≥ª„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/401/comments",
    "author": "bumaple",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T13:15:37Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢Êèê‰æõ‰∏≠ÊñáÊîØÊåÅÔºÅÊèêÂèñÊñπÂºèÈÇ£ÈáåÂàÜÊàê‰∏§Ê≠•‰ºö‰∏ç‰ºöÂΩ±ÂìçÊäΩÂèñÈÄüÂ∫¶Âë¢ÔºåÂõ†‰∏∫Áé∞Âú®ÊäΩÂèñÈò∂ÊÆµÂ∑≤ÁªèÊØîËæÉË¥πÊó∂Èó¥‰∫Ü"
      }
    ]
  },
  {
    "number": 399,
    "title": "Add MongoDB as KV storage",
    "created_at": "2024-12-05T06:01:08Z",
    "closed_at": "2024-12-05T10:21:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/399",
    "body": "`examples/lightrag_ollama_neo4j_milvus_demo.py` -> `examples/lightrag_ollama_neo4j_milvus_mongo_demo.py`",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/399/comments",
    "author": "partoneplay",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T10:21:47Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 398,
    "title": "fix(llm): ÁßªÈô§ openai_complete_if_cache Ë∞ÉÁî®‰∏≠ÁöÑ keyword_extraction ÂèÇÊï∞",
    "created_at": "2024-12-05T03:05:55Z",
    "closed_at": "2024-12-05T05:14:44Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/398",
    "body": "- ‰ªé kwargs ‰∏≠ÁßªÈô§ keyword_extraction ÂèÇÊï∞Ôºå‰ª•Ëß£ÂÜ≥Âú®‰ΩøÁî® openai Êé•Âè£È£éÊ†ºËá™ÂÆö‰πâÊ®°ÂûãÊó∂ÊåáÂÆö keyword_extraction=True ÂØºËá¥ÁöÑ openai Ê®°ÂûãÊó†Ê≥ïÊ≠£Â∏∏Â∑•‰ΩúÁöÑÈóÆÈ¢ò",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/398/comments",
    "author": "magicyuan876",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T03:57:50Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºåÊàëÂ∑≤ÁªèÁªü‰∏ÄËøõË°å‰∫Ü‰øÆÊîπ„ÄÇ"
      }
    ]
  },
  {
    "number": 397,
    "title": "rename example file",
    "created_at": "2024-12-05T03:04:38Z",
    "closed_at": "2024-12-05T03:57:27Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/397",
    "body": "rename file lightrag_api_oracle_demo..py -> lightrag_api_oracle_demo.py",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/397/comments",
    "author": "tmuife",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T03:57:24Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 396,
    "title": "fix: example llm_model_func unexpected keyword argument error",
    "created_at": "2024-12-04T16:17:21Z",
    "closed_at": "2024-12-05T05:56:25Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/396",
    "body": "- If the keyword_extraction argument is not passed explicitly, it is passed as kwargs to the `openai_async_client.chat.completions.create` function. Raised an error `TypeError: AsyncCompletions.create() got an unexpected keyword argument 'keyword_extraction'`.\r\n- how to fix: add the `keyword_extraction` parameter just like implementations such as `gpt_4o_mini_complete`.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/396/comments",
    "author": "linbuxiao",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T04:02:07Z",
        "body": "Thanks! Regarding this issue, I have made the necessary updates."
      }
    ]
  },
  {
    "number": 393,
    "title": "Add Milvus as vector storage",
    "created_at": "2024-12-04T11:55:53Z",
    "closed_at": "2024-12-05T04:05:30Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/393",
    "body": "#151",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/393/comments",
    "author": "partoneplay",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-04T12:10:06Z",
        "body": "Thanks! But there are some linting errors. Please make sure to run `pre-commit run --all-files` to ensure everything is up to standard."
      }
    ]
  },
  {
    "number": 392,
    "title": "edit README.md",
    "created_at": "2024-12-04T07:24:28Z",
    "closed_at": "2024-12-04T12:00:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/392",
    "body": "- adding lightrag_nvidia_demo.py on code structure inside README.md\r\n- pre commit fix ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/392/comments",
    "author": "MRX760",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-04T12:01:45Z",
        "body": "Thanks! I‚Äôve just made all the necessary changes and updated"
      }
    ]
  },
  {
    "number": 388,
    "title": "‰øÆÊîπÊó•ÂøóÊñá‰ª∂Ë∑ØÂæÑ",
    "created_at": "2024-12-04T00:47:08Z",
    "closed_at": "2024-12-04T03:03:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/388",
    "body": "- Âõ†‰∏∫LightRAGÁöÑÂá†‰πéÈÉΩÊòØÂØºÂÖ•ÁöÑutils‰∏≠ÁöÑÂÖ®Â±ÄloggerÂØπË±°ÔºåÂΩìÂ§ö‰∏™ragÂÆû‰æãÁöÑÊó∂ÂÄôÂπ∂Êó†Ê≥ïÂÆåÂÖ®ÊääÊó•ÂøóËÆ∞ÂΩïÂà∞ÂØπÂ∫îÁöÑworking_dir,Âπ∂‰∏îÂ∫îÁî®‰∏≠Âà†Èô§working_dirÊó∂‰ºöÁî±‰∫éloggerÁöÑÂè•ÊüÑÊó†Ê≥ïÂà†Èô§\r\n- Ê≠§‰øÆÊîπÁÆÄÂåñ‰∫ÜÊó•ÂøóÊñá‰ª∂ÁöÑË∑ØÂæÑÔºå‰∏çÂÜç‰æùËµñ‰∫é working_dir Â±ûÊÄßÔºåÊó•ÂøóÊñá‰ª∂Áã¨Á´ã‰∫éworking_dir\r\n- ËøôÊ†∑ÂÆûÈôÖ‰ΩøÁî®‰∏≠ÂèØ‰ª•Â§ö‰∏™‰∏çÂêå‰∏ªÈ¢òÁöÑÊñá‰ª∂ÈõÜÂèØ‰ª•ÊåâÁÖßworking_dirÊù•ÂàÜÂºÄÊûÑÂª∫Áü•ËØÜÂõæË∞±ÈÅøÂÖç‰∫íÁõ∏ÂΩ±Âìç„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/388/comments",
    "author": "magicyuan876",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-04T03:03:09Z",
        "body": "ÊÑüË∞¢ÔºÅ"
      }
    ]
  },
  {
    "number": 384,
    "title": "fix examples' prompt bug",
    "created_at": "2024-12-03T14:34:32Z",
    "closed_at": "2024-12-04T03:17:07Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/384",
    "body": "1. examples in the latest main branch was changed to be cutomizable, but a bug was introduced, that the string literal format is missing for the `PROMPTS[\"entity_extraction_examples\"` in `lightrag/prompt.py` . It is directly used in the line 265 of `lightrag/operate.py` without formating.\r\n2. fixed a typo (missing right bracket) in the system prompt for entity extraction.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/384/comments",
    "author": "Ecocytus",
    "comments": [
      {
        "user": "Ecocytus",
        "created_at": "2024-12-03T14:37:03Z",
        "body": "Just saw this problem has already been issued here #377 "
      },
      {
        "user": "LarFii",
        "created_at": "2024-12-04T03:17:04Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ"
      }
    ]
  },
  {
    "number": 378,
    "title": "fix neo4jstorage bug",
    "created_at": "2024-12-03T08:08:11Z",
    "closed_at": "2024-12-04T03:11:19Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/378",
    "body": "fix missing required positional argument and wrong argument name",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/378/comments",
    "author": "doosenn",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-04T03:11:15Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 376,
    "title": "Get Nodes and Connected Nodes Used for Response Generation",
    "created_at": "2024-12-03T04:37:39Z",
    "closed_at": "2025-02-17T10:59:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/376",
    "body": "I need functionality to retrieve the nodes that were utilized in generating a response, along with the next connected nodes for each of those nodes.\r\n\r\nSpecifically:\r\n- Identify all the nodes actively contributing to the response generation.\r\n- For each identified node, fetch the details of its directly connected successor nodes.\r\n\r\nThis feature would significantly enhance the search capability and exploration of the knowledge graph, allowing for deeper insights into how responses are constructed and improving traceability.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/376/comments",
    "author": "abhigrazitti",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T02:31:31Z",
        "body": "We currently support returning the retrieved content using the `only_need_context` parameter. However, further modifications may be needed to meet specific requirements."
      }
    ]
  },
  {
    "number": 372,
    "title": "Enhancement: Support for Extracting Insights from Electrical Drawings in LightRAG",
    "created_at": "2024-12-02T16:21:08Z",
    "closed_at": "2025-02-17T10:59:49Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/372",
    "body": "LightRAG's ability to process large amounts of text with a knowledge graph is great. Could this capability be extended to include the handling of technical drawings, such as electrical schematics? While tools like ChatGPT-4o can analyze individual drawings, scaling to large volumes of text and drawings for pattern recognition or insights seems like a limitation.\r\n\r\nI want to offer the idea to extend the ability to extract key insights, identify patterns, and connect information across both text and visual data, particularly in fields like engineering.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/372/comments",
    "author": "kevinsosborne",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-09T02:33:16Z",
        "body": "This would likely require making corresponding changes during the index phase to accommodate the processing of technical drawings alongside text."
      }
    ]
  },
  {
    "number": 370,
    "title": "fix embedding_func issue for oracle database",
    "created_at": "2024-12-02T07:32:18Z",
    "closed_at": "2024-12-02T08:04:01Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/370",
    "body": "Add embedding_func for graph database, it's deleted in commit 5661d76860436f7bf5aef2e50d9ee4a59660146c\r\nThis will fix oracle database issue.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/370/comments",
    "author": "jin38324",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-02T08:03:58Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 368,
    "title": "Added embedding_func argument in Neo4JStorage class's __init__ method in neo4j implementation",
    "created_at": "2024-12-01T21:17:24Z",
    "closed_at": "2024-12-02T08:02:53Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/368",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/368/comments",
    "author": "AdityaKalraShorthillsAI",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-02T08:02:50Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 365,
    "title": "examples\\lightrag_ollama_demo.py  error :Could not automatically map gpt-4o-mini to a tokeniser",
    "created_at": "2024-12-01T10:22:20Z",
    "closed_at": "2025-02-17T10:59:47Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/365",
    "body": " python lightrag_ollama_demo.py\r\nINFO:numexpr.utils:Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\r\nINFO:numexpr.utils:NumExpr defaulting to 8 threads.\r\nINFO:lightrag:Logger initialized for working directory: ./dickens\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Loaded graph from ./dickens\\graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\r\nINFO:nano-vectordb:Load (0, 768) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 768) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 768) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './dickens\\\\vdb_chunks.json'} 0 data\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nChunking documents:   0%|                                                                          | 0/1 [00:00<?, ?doc/s]\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"E:\\data1\\project\\software\\LightRAG\\examples\\lightrag_ollama_demo.py\", line 31, in <module>\r\n    rag.insert(f.read())\r\n  File \"e:\\data1\\project\\software\\lightrag\\lightrag\\lightrag.py\", line 225, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"D:\\ProgramData\\miniconda3\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"e:\\data1\\project\\software\\lightrag\\lightrag\\lightrag.py\", line 254, in ainsert\r\n    for dp in chunking_by_token_size(\r\n  File \"e:\\data1\\project\\software\\lightrag\\lightrag\\operate.py\", line 35, in chunking_by_token_size\r\n    tokens = encode_string_by_tiktoken(content, model_name=tiktoken_model)\r\n  File \"e:\\data1\\project\\software\\lightrag\\lightrag\\utils.py\", line 140, in encode_string_by_tiktoken\r\n    ENCODER = tiktoken.encoding_for_model(model_name)\r\n  File \"C:\\Users\\qsfzz\\AppData\\Roaming\\Python\\Python310\\site-packages\\tiktoken\\model.py\", line 97, in encoding_for_model\r\n    return get_encoding(encoding_name_for_model(model_name))\r\n  File \"C:\\Users\\qsfzz\\AppData\\Roaming\\Python\\Python310\\site-packages\\tiktoken\\model.py\", line 84, in encoding_name_for_model\r\n    raise KeyError(\r\nKeyError: 'Could not automatically map gpt-4o-mini to a tokeniser. Please use `tiktoken.get_encoding` to explicitly get the tokeniser you expect.'",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/365/comments",
    "author": "qidasheng",
    "comments": [
      {
        "user": "GreenerZ",
        "created_at": "2024-12-02T07:19:58Z",
        "body": "upgrade tiktoken"
      }
    ]
  },
  {
    "number": 363,
    "title": "What data format do you recommend for PDF input.",
    "created_at": "2024-12-01T06:09:40Z",
    "closed_at": "2025-02-17T10:59:47Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/363",
    "body": "I am working on a project that involves providing LightRAG with hundreds of PDFs for queries. I want to ensure that the data is processed efficiently and accurately.\n\n\t1.\tWhat is the optimal format for extracting and converting the PDF content for LightRAG? Would Markdown provide better structure, or is another format, such as JSON, for batch processing?\n\t2.\tAre there any specific guidelines or preprocessing steps to follow when handling large datasets like this to ensure optimal embedding and contextual understanding?\n\t3.\tDoes LightRAG have limitations or recommendations for handling PDFs with mixed content, such as text, images, and tables?\n\nThank you!\n\nBest regards,\nKevin",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/363/comments",
    "author": "kevinsosborne",
    "comments": [
      {
        "user": "nickakube",
        "created_at": "2024-12-01T09:22:14Z",
        "body": "I think hands down markdown format is the best, please use marker github to first convert the PDFs to MD format before feeding them to LightRAG. This is purely my experience."
      },
      {
        "user": "kevinsosborne",
        "created_at": "2024-12-04T22:41:16Z",
        "body": "Thank you @nickakube. I was leaning towards that method and that seems to be the best way. Thank you for sharing your experience. "
      }
    ]
  },
  {
    "number": 359,
    "title": "Update NanoVectorDBStorage upsert to use hash values for IDs",
    "created_at": "2024-11-30T12:55:00Z",
    "closed_at": "2024-12-02T10:02:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/359",
    "body": "- In the delete_entity method, a hash value is calculated when deleting an entity.\r\n- On the other hand, in the upsert method, the index was used directly as the ID when adding an entity.\r\n- Therefore, I modified the upsert method to also calculate a hash value.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/359/comments",
    "author": "FatRicePaddyyyy",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-02T08:01:21Z",
        "body": "Thanks for your suggestion! But the hash value is already calculated in `operate.py` before calling the `upsert` method."
      },
      {
        "user": "FatRicePaddyyyy",
        "created_at": "2024-12-02T10:01:50Z",
        "body": "I see. Sorry for jumping to conclusions. I'll cancel the pull request."
      }
    ]
  },
  {
    "number": 355,
    "title": "AssertionError: Embedding dim mismatch, expected: 768, but loaded: 1536 when running ollama demo file",
    "created_at": "2024-11-30T00:14:45Z",
    "closed_at": "2025-02-17T10:59:02Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/355",
    "body": "demo example with ollama gives the error\r\nAssertionError: Embedding dim mismatch, expected: 768, but loaded: 1536\r\nchanged the parameter to 1536 from the default 768 and then got a different error\r\n\r\nthe last lines from the output are below\r\nGenerating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:40<00:00, 20.39s/batch]\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"D:\\Github\\LightRAG\\examples\\lightrag_ollama_demo.py\", line 35, in <module>\r\n    rag.insert(f.read())\r\n  File \"d:\\github\\lightrag\\lightrag\\lightrag.py\", line 225, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"c:\\Users\\sjsha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"d:\\github\\lightrag\\lightrag\\lightrag.py\", line 273, in ainsert\r\n    await self.chunks_vdb.upsert(inserting_chunks)\r\n  File \"d:\\github\\lightrag\\lightrag\\storage.py\", line 112, in upsert\r\n    results = self._client.upsert(datas=list_data)\r\n  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nano_vectordb\\dbs.py\", line 100, in upsert\r\n    self.__storage[\"matrix\"][i] = update_d[f_VECTOR].astype(Float)\r\nIndexError: index 0 is out of bounds for axis 0 with size 0",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/355/comments",
    "author": "s-github-2",
    "comments": [
      {
        "user": "s-github-2",
        "created_at": "2024-11-30T23:34:06Z",
        "body": "deleted the dickens folder created earlier by an incomplete execution of the example script and this seems to have fixed at leat the initial error issue. Rerunning the ollama demo script now, its proceeding further instead of erroring out at the load model with the above error. But its a long slow process 'Extracting Entities from chunks' so will see later if it finishes to the end without errors"
      },
      {
        "user": "j-shah7",
        "created_at": "2025-01-17T06:17:42Z",
        "body": "Hi i have used other working directory still showing the same error. Can you please tell me how you have resolved the error? Thanks"
      },
      {
        "user": "j-shah7",
        "created_at": "2025-01-21T06:21:44Z",
        "body": "Hi i am getting the same error message. can any one help me in this regard how to resolve this error? Thanks"
      }
    ]
  },
  {
    "number": 354,
    "title": "fix for #209",
    "created_at": "2024-11-29T20:28:06Z",
    "closed_at": "2024-12-02T07:53:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/354",
    "body": "function was returning a closed event loop.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/354/comments",
    "author": "TashaSkyUp",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-02T07:53:04Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 353,
    "title": "JSON mode support for ollama and openai gpt",
    "created_at": "2024-11-29T16:09:12Z",
    "closed_at": "2024-12-02T07:52:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/353",
    "body": "#287 \r\n- Added `keyword_extraction=False` to all `*_complete` functions. If `True`, each function handles its own JSON mode logic.\r\n- Moved `locate_json_string_body_from_string` to `*_complete` functions as the default JSON logic for `keyword_extraction`.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/353/comments",
    "author": "b10902118",
    "comments": [
      {
        "user": "b10902118",
        "created_at": "2024-12-01T08:21:09Z",
        "body": "Fixed and checked in my fork's workflow. Sorry new to the process."
      },
      {
        "user": "LarFii",
        "created_at": "2024-12-02T07:52:05Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 352,
    "title": "Can we Insert KG from Neo4j database?",
    "created_at": "2024-11-29T16:05:18Z",
    "closed_at": "2025-02-17T10:59:02Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/352",
    "body": "Nice work!\r\nI noticed that your team added the ability to insert custom KG a few days ago, as well as the support for Neo4JStorage that already existed before.\r\nI wonder if there exists any way to get the KG directly from the Neo4j database for insertion?\r\nSuppose I use another tool (such as Docs2KG) to extract the file as KG and store it in Neo4j database, can I directly use Lightrag based on it?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/352/comments",
    "author": "Yi-Eaaa",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T13:24:15Z",
        "body": "Thank you for your suggestion! That's indeed a great idea, and we will consider adding support for it in the future."
      },
      {
        "user": "adamwuyu",
        "created_at": "2025-01-08T02:06:21Z",
        "body": "> Thank you for your suggestion! That's indeed a great idea, and we will consider adding support for it in the future.\r\n\r\nDoes this means we can instert KG into database, but can not extract KG from database and do QA?"
      },
      {
        "user": "javi-horizon",
        "created_at": "2025-01-21T18:34:40Z",
        "body": "I have the same question, I have generated a KG and I have it in Neo4j format, I would like to use this framework to answer questions, is there a way to do this now?"
      }
    ]
  },
  {
    "number": 351,
    "title": "Whether the custom knowledge graph has a size limit",
    "created_at": "2024-11-29T11:14:49Z",
    "closed_at": "2025-02-17T10:59:02Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/351",
    "body": "A few days ago, the author updated Lightrag's ability to embed custom knowledge graph, but for lightrag, the default token limit per chunk is 1200, I wonder if this also applies to custom knowledge graph.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/351/comments",
    "author": "zel2023",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-05T13:25:19Z",
        "body": "In theory, there is no such limitation for custom knowledge graphs. "
      }
    ]
  },
  {
    "number": 350,
    "title": "Step 1 Stuck",
    "created_at": "2024-11-29T09:05:40Z",
    "closed_at": "2024-12-01T08:35:25Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/350",
    "body": "ÂΩìÊàëËøêË°åStep_1.pyÊó∂ÔºåËæìÂá∫‰∏ÄÁõ¥Âç°Âú®Generating embeddingsÔºåÂ¶Ç‰∏ãÊâÄÁ§∫ÔºåËØ∑ÈóÆËøôÊòØ‰ªÄ‰πàÂéüÂõ†ÔºüÊàëËØ•Â¶Ç‰ΩïËß£ÂÜ≥Ôºü\r\n```bash\r\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../agriculture/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 1536) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../agriculture/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 1536) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': '../agriculture/vdb_chunks.json'} 0 data\r\nINFO:lightrag:[New Docs] inserting 12 docs\r\nChunking documents:   8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                    | 1/12 [00:00<00:08,  1.\r\nChunking documents:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                            | 2/12 [00:00<00\r\nChunking documents:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                     | 3/12 [0Chunking documents:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             |Chunking documents:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                              Chunking documents:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      Chunking documents:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                Chunking documents:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñéChunking documents:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:03<00:00,  3.03doc/s]\r\nINFO:lightrag:[New Chunks] inserting 1756 chunks\r\nINFO:lightrag:Inserting 1756 vectors to chunks\r\nGenerating embeddings:   0%|                                                                                               | 0/55 [00:00<?, ?batch/s]\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nInsertion failed, retrying (1/3), error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\r\nINFO:lightrag:[New Docs] inserting 12 docs\r\nChunking documents:   8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                    | 1/12 [00:00<00:01,  6.Chunking documents:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                            | 2/12 [00:00<00Chunking documents:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                     | 3/12 [0Chunking documents:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             |Chunking documents:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              Chunking documents:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                Chunking documents:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                Chunking documents:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñéChunking documents:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàChunking documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:03<00:00,  3.64doc/s]\r\nINFO:lightrag:[New Chunks] inserting 1756 chunks\r\nINFO:lightrag:Inserting 1756 vectors to chunks\r\nGenerating embeddings:   0%|                                                                                               | 0/55 [00:00<?, ?batch/s]\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/350/comments",
    "author": "fantastic-413",
    "comments": [
      {
        "user": "nickakube",
        "created_at": "2024-12-01T04:53:36Z",
        "body": "before you run the LightRAG insertion please export the OPENAI_API_KEY on Linux machine you are running the code.\r\ne.g export OPENAI_API_KEY=\"get openai API key from openai site, you must sign in and have a paid account, as it will cost you, few dollars\"\r\n\r\ngood luck"
      },
      {
        "user": "fantastic-413",
        "created_at": "2024-12-01T08:35:16Z",
        "body": "Thanks for your reply."
      }
    ]
  },
  {
    "number": 347,
    "title": "Êú¨Âú∞Ë∞ÉÁî®Â§ßÊ®°ÂûãÁöÑÂú∫ÊôØ‰∏ãÔºå‰∏éOpenai‰∏ãËΩΩÁõ∏ÂÖ≥ÁöÑÊé•Âè£Ê≤°ÊúâÂÆåÂÖ®Ëß£ËÄ¶",
    "created_at": "2024-11-29T01:57:29Z",
    "closed_at": "2025-02-17T10:59:01Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/347",
    "body": "ÂÆåÂÖ®Êú¨Âú∞ÊâßË°åÁöÑÂú∫ÊôØ‰∏ãÔºåËøêË°åollama demo (example/lightrag_ollama_demo.py)‰ºöÂ≠òÂú®HTTPSConnectionPoolÊä•ÈîôÔºåÊèêÁ§∫ÈúÄË¶Å‰∏ãËΩΩ o200k_base.tiktokenÊñá‰ª∂„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/347/comments",
    "author": "Lyrics-WangKL",
    "comments": [
      {
        "user": "xiaoshuichong",
        "created_at": "2024-12-07T07:02:11Z",
        "body": "Êàë‰πüÈÅáÂà∞ÂêåÊ†∑ÁöÑÈóÆÈ¢ò"
      },
      {
        "user": "Kyrie666",
        "created_at": "2025-01-09T02:21:36Z",
        "body": "ÊòØtiktokenÁöÑÈóÆÈ¢òÔºåtiktokenÈúÄË¶ÅËÅîÁΩë"
      }
    ]
  },
  {
    "number": 345,
    "title": "add jina embedding support",
    "created_at": "2024-11-28T14:58:35Z",
    "closed_at": "2024-12-08T14:14:09Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/345",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/345/comments",
    "author": "davidleon",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-01T06:42:56Z",
        "body": "Thank you so much for your contributions! It would be great if you could also add a demo for Jina embedding. Before submitting, please make sure to run pre-commit run --all-files to ensure everything is up to standard."
      }
    ]
  },
  {
    "number": 343,
    "title": "fix templating of language in prompts",
    "created_at": "2024-11-28T13:31:39Z",
    "closed_at": "2024-11-28T13:36:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/343",
    "body": "resolves #342 \r\n\r\n- Replace hardcoded chinese language in prompts with templated language\r\n- Template missed instance of English language\r\n- Adjust usage of fixed prompts to supply language as context",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/343/comments",
    "author": "sebastianschramm",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-28T13:36:04Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 340,
    "title": "unsupported operand type(s) for |: 'type' and 'type'",
    "created_at": "2024-11-28T09:36:21Z",
    "closed_at": "2025-02-17T10:59:00Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/340",
    "body": "Having this error:\r\n\r\n---> [45](file:///C:/Users/Admin/Desktop/lightrag/lightrag/lightrag.py:45) from .kg.oracle_impl import OracleKVStorage, OracleGraphStorage, OracleVectorDBStorage\r\n     [47](file:///C:/Users/Admin/Desktop/lightrag/lightrag/lightrag.py:47) # future KG integrations\r\n     [48](file:///C:/Users/Admin/Desktop/lightrag/lightrag/lightrag.py:48) \r\n     [49](file:///C:/Users/Admin/Desktop/lightrag/lightrag/lightrag.py:49) # from .kg.ArangoDB_impl import (\r\n...\r\n    [147](file:///C:/Users/Admin/Desktop/lightrag/lightrag/kg/oracle_impl.py:147)     # logger.info(\"go into OracleDB execute method\")\r\n    [148](file:///C:/Users/Admin/Desktop/lightrag/lightrag/kg/oracle_impl.py:148)     try:\r\n    [149](file:///C:/Users/Admin/Desktop/lightrag/lightrag/kg/oracle_impl.py:149)         async with self.pool.acquire() as connection:\r\n\r\nTypeError: unsupported operand type(s) for |: 'type' and 'type'",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/340/comments",
    "author": "ginisksam",
    "comments": [
      {
        "user": "Emilon1928",
        "created_at": "2024-12-02T13:52:02Z",
        "body": "Using newer python fixed the issue, ''|\" is allowed only in python 3.10+"
      }
    ]
  },
  {
    "number": 332,
    "title": "Issue: Adjusting Model Temperature",
    "created_at": "2024-11-26T16:43:43Z",
    "closed_at": "2025-02-17T10:58:58Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/332",
    "body": "Description\r\nI am using the following example code with LightRAG and the gpt_4o_mini_complete model. However, I noticed that the model generates made-up information that doesn't align with the provided knowledge graph. I would like the model to strictly adhere to the provided context and avoid introducing hallucinations.\r\n\r\nHere are my specific questions:\r\n\r\nHow can I adjust the temperature of the model in this example to make the responses more deterministic and context-based?\r\nShould I also adapt the system prompt to enforce stricter adherence to the provided graph? If so, could you provide guidance or an example of how to adjust the prompt effectively?\r\n\r\ntesting after kg creation: --> not accurate response too much made up\r\n\r\n```\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import gpt_4o_mini_complete\r\n\r\n# working directory\r\nWORKING_DIR = \"./custom_kg\"\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete, \r\n)\r\n\r\n# Query the existing knowledge graph\r\nquestion1 = \"What products are developed by CompanyA?\"\r\nresponse1 = rag.query(question1, param=QueryParam(only_need_context=True, mode=\"local\")) \r\nprint(f\"Question: {question1}\\nResponse: {response1}\\n\")\r\n\r\nquestion2 = \"Where is EventY hosted?\"\r\nresponse2 = rag.query(question2, param=QueryParam(mode=\"local\")) \r\nprint(f\"Question: {question2}\\nResponse: {response2}\")\r\n```\r\n\r\nCode from inster_custom_kg.py\r\n\r\n```\r\nimport os\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import gpt_4o_mini_complete\r\n\r\nWORKING_DIR = \"./custom_kg\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model\r\n    # llm_model_func=gpt_4o_complete  # Optionally, use a stronger model\r\n)\r\n\r\ncustom_kg = {\r\n    \"entities\": [\r\n        {\r\n            \"entity_name\": \"CompanyA\",\r\n            \"entity_type\": \"Organization\",\r\n            \"description\": \"A major technology company\",\r\n            \"source_id\": \"Source1\"\r\n        },\r\n        {\r\n            \"entity_name\": \"ProductX\",\r\n            \"entity_type\": \"Product\",\r\n            \"description\": \"A popular product developed by CompanyA\",\r\n            \"source_id\": \"Source1\"\r\n        },\r\n        {\r\n            \"entity_name\": \"PersonA\",\r\n            \"entity_type\": \"Person\",\r\n            \"description\": \"A renowned researcher in AI\",\r\n            \"source_id\": \"Source2\"\r\n        },\r\n        {\r\n            \"entity_name\": \"UniversityB\",\r\n            \"entity_type\": \"Organization\",\r\n            \"description\": \"A leading university specializing in technology and sciences\",\r\n            \"source_id\": \"Source2\"\r\n        },\r\n        {\r\n            \"entity_name\": \"CityC\",\r\n            \"entity_type\": \"Location\",\r\n            \"description\": \"A large metropolitan city known for its culture and economy\",\r\n            \"source_id\": \"Source3\"\r\n        },\r\n        {\r\n            \"entity_name\": \"EventY\",\r\n            \"entity_type\": \"Event\",\r\n            \"description\": \"An annual technology conference held in CityC\",\r\n            \"source_id\": \"Source3\"\r\n        },\r\n        {\r\n            \"entity_name\": \"CompanyD\",\r\n            \"entity_type\": \"Organization\",\r\n            \"description\": \"A financial services company specializing in insurance\",\r\n            \"source_id\": \"Source4\"\r\n        },\r\n        {\r\n            \"entity_name\": \"ServiceZ\",\r\n            \"entity_type\": \"Service\",\r\n            \"description\": \"An insurance product offered by CompanyD\",\r\n            \"source_id\": \"Source4\"\r\n        }\r\n    ],\r\n    \"relationships\": [\r\n        {\r\n            \"src_id\": \"CompanyA\",\r\n            \"tgt_id\": \"ProductX\",\r\n            \"description\": \"CompanyA develops ProductX\",\r\n            \"keywords\": \"develop, produce\",\r\n            \"weight\": 1.0,\r\n            \"source_id\": \"Source1\"\r\n        },\r\n        {\r\n            \"src_id\": \"PersonA\",\r\n            \"tgt_id\": \"UniversityB\",\r\n            \"description\": \"PersonA works at UniversityB\",\r\n            \"keywords\": \"employment, affiliation\",\r\n            \"weight\": 0.9,\r\n            \"source_id\": \"Source2\"\r\n        },\r\n        {\r\n            \"src_id\": \"CityC\",\r\n            \"tgt_id\": \"EventY\",\r\n            \"description\": \"EventY is hosted in CityC\",\r\n            \"keywords\": \"host, location\",\r\n            \"weight\": 0.8,\r\n            \"source_id\": \"Source3\"\r\n        },\r\n        {\r\n            \"src_id\": \"CompanyD\",\r\n            \"tgt_id\": \"ServiceZ\",\r\n            \"description\": \"CompanyD provides ServiceZ\",\r\n            \"keywords\": \"provide, offer\",\r\n            \"weight\": 1.0,\r\n            \"source_id\": \"Source4\"\r\n        }\r\n    ]\r\n}\r\n\r\nrag.insert_custom_kg(custom_kg)\r\n```\r\n\r\nThe model should:\r\n\r\n- Answer strictly based on the context provided in the knowledge graph.\r\n- Avoid generating additional, fabricated information.\r\nQuestions\r\n\r\nTemperature Adjustment: Where and how can I configure the model's temperature in the above example?\r\nPrompt Adaptation: Would modifying the system prompt improve context adherence, and if so, how should I approach this?\r\n\r\nThanks in advance for the help and guidance!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/332/comments",
    "author": "B14ckFox",
    "comments": [
      {
        "user": "cypridox",
        "created_at": "2024-11-27T10:12:43Z",
        "body": "I recently started testing LightRAG as well (thank you for developing this promising repository). I have encountered the same observation as the author of this issue: the model is insufficiently strict in adhering to the provided context and often produces generic answers.\r\n\r\nWhile adjusting the temperature setting is a potential solution, my usual approach would be to resolve this issue through the system prompt. I attempted to add an instruction directly to the question (e.g., \"limit your answer to the provided context\"), but this resulted in no answers being generated, as the model deemed nothing to be in context.\r\n\r\nI would, therefore, like to have control over modifying the system prompt and the overall query prompt. I reviewed the Python code to locate where these prompts are defined or managed. Unfortunately, the (undocumented) code appears somewhat inaccessible in this regard. For instance, prompt.py seems primarily intended for generating the knowledge graph.\r\n\r\nWhere in the codebase can I view and customize the system prompt and the query prompt used for generating answers from the model?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-27T11:12:26Z",
        "body": "The prompt used for generating responses is defined in the `prompt.py` file, and it is called `rag_response`."
      },
      {
        "user": "B14ckFox",
        "created_at": "2024-11-27T12:32:06Z",
        "body": "and this is how I adjust the temperature?\r\n\r\nasync def gpt_4o_mini_complete(\r\n    prompt, system_prompt=None, history_messages=[], **kwargs\r\n) -> str:\r\n    kwargs.setdefault(\"temperature\", 0) \r\n    return await openai_complete_if_cache(\r\n        \"gpt-4o-mini\",\r\n        prompt,\r\n        system_prompt=system_prompt,\r\n        history_messages=history_messages,\r\n        **kwargs,\r\n    )"
      },
      {
        "user": "sebastianschramm",
        "created_at": "2024-11-28T14:44:47Z",
        "body": "@B14ckFox I believe you can also directly pass your llm parameters via llm_model_kwargs to LightRAG without modifying anything else:\r\n\r\n```python\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete,\r\n    llm_model_kwargs={\"temperature\": 0.0}\r\n)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 331,
    "title": "IndexError: index 0 is out of bounds for dimension 0 with size 0  INFO:lightrag:Writing graph with 0 nodes, 0 edges",
    "created_at": "2024-11-26T10:36:20Z",
    "closed_at": "2025-02-17T10:58:57Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/331",
    "body": "INFO:lightrag:Logger initialized for working directory: /e2e-data/users/lzh/code/LightRAG-main/workspace\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/e2e-data/users/lzh/code/LightRAG-main/workspace/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/e2e-data/users/lzh/code/LightRAG-main/workspace/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': '/e2e-data/users/lzh/code/LightRAG-main/workspace/vdb_chunks.json'} 0 data\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 42 chunks\r\nINFO:lightrag:Inserting 42 vectors to chunks\r\nINFO:lightrag:[Entity Extraction]...\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:21<00:00,  5.27s/it]\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/examples/lightrag_hf_demo.py\", line 40, in <module>\r\n    rag.insert(f.read())\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/lightrag.py\", line 225, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/lightrag.py\", line 274, in ainsert\r\n    maybe_new_kg = await extract_entities(\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/operate.py\", line 333, in extract_entities\r\n    results = await asyncio.gather(\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/operate.py\", line 272, in _process_single_content\r\n    final_result = await use_llm_func(hint_prompt)\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/utils.py\", line 89, in wait_func\r\n    result = await func(*args, **kwargs)\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/llm.py\", line 510, in hf_model_complete\r\n    return await hf_model_if_cache(\r\n  File \"/e2e-data/users/lzh/code/LightRAG-main/lightrag/llm.py\", line 286, in hf_model_if_cache\r\n    output = hf_model.generate(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1914, in generate\r\n    result = self._sample(\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2648, in _sample\r\n    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1277, in prepare_inputs_for_generation\r\n    past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\r\nIndexError: index 0 is out of bounds for dimension 0 with size 0",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/331/comments",
    "author": "emt0re0",
    "comments": [
      {
        "user": "emt0re0",
        "created_at": "2024-11-26T10:37:56Z",
        "body": "`rag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,\r\n    llm_model_name=\"/e2e-data/users/lzh/model/qwen2.5-7B-Instruct\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=384,\r\n        max_token_size=5000,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\r\n                \"/e2e-data/users/lzh/model/all-MiniLM-L6-v2\"\r\n            ),\r\n            embed_model=AutoModel.from_pretrained(\r\n                \"/e2e-data/users/lzh/model/all-MiniLM-L6-v2\"\r\n            ),\r\n        ),\r\n    ),\r\n)`"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-27T11:14:41Z",
        "body": "From the log, it appears that no content was extracted. I would suggest checking if the communication with the LLM is working properly, as this could be the cause of the issue."
      },
      {
        "user": "zhemingbuhuichong",
        "created_at": "2024-12-18T11:32:52Z",
        "body": "I also have this question.That is because you are using Global Proxy.Check your internet or VPN."
      },
      {
        "user": "shalini-agarwal",
        "created_at": "2025-01-02T21:39:31Z",
        "body": "Hey @zhemingbuhuichong , I am also running into this error of graph being written with 0 nodes and 0 edges. Were you able to resolve this issue?"
      }
    ]
  },
  {
    "number": 329,
    "title": "What if the extracted entities have the same name and different types? For example,  a person's name is Washington and a location is called Washington.",
    "created_at": "2024-11-26T07:07:37Z",
    "closed_at": "2025-02-17T10:58:56Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/329",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/329/comments",
    "author": "syf0119",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-27T11:06:59Z",
        "body": "In cases where extracted entities have the same name but different types, such as a person and a location both called \"Washington,\" the two nodes will be merged. However, the node description will include information specifying the different types to distinguish between them. This allows for clear identification of the distinct entities despite sharing the same name."
      }
    ]
  },
  {
    "number": 326,
    "title": "mixÊï∞ÊçÆÈõÜ‰∏äÁöÑÊµãËØï",
    "created_at": "2024-11-25T06:05:08Z",
    "closed_at": "2025-02-17T10:58:56Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/326",
    "body": "Êúâ‰∫∫Â§çÁé∞ËøáËÆ∫Êñá‰∏≠ÁöÑÊµãËØïÂêóÔºü\r\nÊàëÂú®mixÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÊµãËØïÔºåÂÆåÂÖ®ÊåâÁÖßËÆ∫Êñá‰∏äÁöÑÊ≠•È™§ÂÅöÁöÑÔºå‰ΩÜÊòØÊµãËØïÁªìÊûúlightragÁöÑË°®Áé∞Â•ΩÂÉèÊØînative ragË¶ÅÂ∑ÆÔºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/326/comments",
    "author": "yiwalk23li",
    "comments": [
      {
        "user": "zhaoyueyan0216",
        "created_at": "2024-11-25T10:43:27Z",
        "body": "I couldn't find where the dataset is. Could you tell me where it is?thank you"
      }
    ]
  },
  {
    "number": 325,
    "title": "Enhance Query Logic and Add Configurable Features",
    "created_at": "2024-11-25T06:01:08Z",
    "closed_at": "2024-11-27T10:44:01Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/325",
    "body": "### 1. Unified Query Functions\r\nThe previously separate query functions (`local_query`, `global_query`, `hybrid_query`) shared significant duplicated code. I merged them into a single function, `kg_query`, to improve maintainability and clarity.\r\n\r\nTo achive this:\r\n- The query mode is now determined by the `query_param.mode` parameter.\r\n- Enhanced keyword handling: when no valid keyword is returned, the query mode adapts dynamically (e.g., switching from hybrid to global or returning an error).\r\n- Unified context generation using `_build_query_context`, streamlining the process into a single return point.\r\n\r\n### 2. Configurable Example Count in Prompts\r\nIn the original prompt logic, examples occupied a large number of tokens, increasing cost and response time. This update introduces a configurable parameter, `addon_params = {\"example_number\": 1}`, to limit the number of examples used in entity extraction and keyword generation.\r\n\r\nTo achive this:\r\n- Updated prompt.py to split prompts into a list of examples.\r\n- If example_number is not specified or exceeds the available examples, all (default: 3) are used.\r\n\r\n### 3. Added Language Support for Entity Extraction\r\nEntity extraction now supports language customization via `addon_params = {\"language\": \"Simplified Chinese\"}`.\r\n\r\nTo achive this:\r\n- Added a `language` variable in the prompt's string template.\r\n- Introduced `default_language` in prompt.py (default: English). If no language is specified in addon_params, the default value is used.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/325/comments",
    "author": "jin38324",
    "comments": [
      {
        "user": "jin38324",
        "created_at": "2024-11-26T02:37:03Z",
        "body": "I forgot one more feature.\r\nThe `only_need_prompt parameter` is added to` QueryParam`. \r\nThe purpose of this is to enable lightrag to be efficiently integrated into other applications. By setting `only_need_prompt=true`, you can get the prompt constructed after the graph query, and send this prompt directly to LLM to get the answer.\r\n\r\nUse case:\r\n1. Send the prompt word to the LLM API to get streaming output;\r\n2. By enabling the lightrag API server, other applications (such as `Dify`) can use lightrag as an external knowledge base, get the prompt from lightrag, and then call LLM by the application."
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-27T10:43:58Z",
        "body": "Thank you for your contributions! I have merged them."
      }
    ]
  },
  {
    "number": 324,
    "title": "ÊòØÂê¶ÊîØÊåÅÂ§ñÈÉ®Áü•ËØÜÂõæË∞±ÁöÑÁõ¥Êé•ÂØºÂÖ•Ôºü",
    "created_at": "2024-11-25T01:35:29Z",
    "closed_at": "2025-02-17T10:58:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/324",
    "body": "ÊòØÂê¶ÊîØÊåÅÂ§ñÈÉ®Áü•ËØÜÂõæË∞±ÁöÑÁõ¥Êé•ÂØºÂÖ•ÔºüÂ¶ÇÊûúÊ≤°ÊúâÔºåÊòØÂê¶ËÆ°ÂàíÊîØÊåÅÂë¢Ôºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/324/comments",
    "author": "WangAo-0",
    "comments": [
      {
        "user": "Jiangqipu",
        "created_at": "2024-11-26T00:17:21Z",
        "body": "‰ΩúËÄÖÊò®Â§©Â∑≤ÁªèË∑üÊñ∞‰∫ÜËá™ÂÆö‰πâÂØºÂÖ•Ê®°Âùó"
      }
    ]
  },
  {
    "number": 323,
    "title": "how to return the source files used to answer a query? Like Asking LighRag to find a legal document relating to a contract for purchase of shares for X company.?",
    "created_at": "2024-11-22T08:54:26Z",
    "closed_at": "2025-02-17T10:58:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/323",
    "body": "how to return the name source files used to answer a query?\r\n\r\nfor example, only information from 1 pdf was retrieved and response was generated.\r\nhow to propogate to the source file ( eg a.pdf ). \r\n\r\nI've checked the source code, there is no metadata getting stored for filename.\r\nOnly document id is there.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/323/comments",
    "author": "Arslan-Mehmood1",
    "comments": [
      {
        "user": "chandasampath",
        "created_at": "2024-11-23T06:17:44Z",
        "body": "+1 on this question."
      },
      {
        "user": "belligerentbeagle",
        "created_at": "2025-01-08T05:20:57Z",
        "body": "+1"
      },
      {
        "user": "Aillian",
        "created_at": "2025-01-14T06:43:01Z",
        "body": "+1"
      }
    ]
  },
  {
    "number": 320,
    "title": "Âà†Èô§Neo4JStorage.has_edge‰∏≠ÂÆö‰πâÁöÑÊ≤°ÊúâÁî®Âà∞ÁöÑÂêåÊ≠•closeÂáΩÊï∞",
    "created_at": "2024-11-22T06:00:22Z",
    "closed_at": "2024-11-22T07:14:25Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/320",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/320/comments",
    "author": "Lizidong01",
    "comments": [
      {
        "user": "Lizidong01",
        "created_at": "2024-11-22T06:07:28Z",
        "body": "ÊÇ®Â•ΩÔºåÁ¨¨‰∏ÄÊ¨°prÔºå‰∏çÂ§™Ê∏ÖÊ•öÊµÅÁ®ãÔºåÂèëÁé∞Á±ª‰∏≠ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™Ê≤°Êúâ‰ΩøÁî®ÁöÑÂáΩÊï∞ÔºåËÄå‰∏îÂ§ñÈÉ®‰πüÂÆö‰πâ‰∫ÜÂêåÂêçÁöÑÂºÇÊ≠•ÂáΩÊï∞Ôºå‰∫éÊòØ‰æøÊèê‰∫ÜËøô‰∏™pr"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-22T07:14:23Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ"
      }
    ]
  },
  {
    "number": 319,
    "title": "batch size is invalid, it should not be larger than 25.",
    "created_at": "2024-11-21T10:01:24Z",
    "closed_at": "2024-11-22T04:22:01Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/319",
    "body": "openai.BadRequestError: Error code: 400 - {'error': {'code': 'InvalidParameter', 'param': None, 'message': 'batch size is invalid, it should not be larger than 25.: payload.input.contents', 'type': 'InvalidParameter'}, 'id': 'ce6c0c89-a338-9df8-afa8-acbd4d544f35', 'request_id': 'ce6c0c89-a338-9df8-afa8-acbd4d544f35'}\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/319/comments",
    "author": "yslion",
    "comments": [
      {
        "user": "houxilong",
        "created_at": "2024-11-22T07:20:23Z",
        "body": "same problems,how did you solve it."
      },
      {
        "user": "chandasampath",
        "created_at": "2024-11-23T06:33:40Z",
        "body": "Is there a direct integration to OpenAI Batch with LightRAG?"
      },
      {
        "user": "yslion",
        "created_at": "2024-11-26T10:30:59Z",
        "body": "> same problems,how did you solve it.\r\n\r\nembedding Áî®‰∫ÜÂçÉÈóÆÁöÑtext-embedding-v2ÔºåËøôÁé©ÊÑè‰∏çË°å„ÄÇopenaiÁöÑÂéüÁâàÊòØÊ≤°ÈóÆÈ¢òÁöÑ„ÄÇÊúÄÂêéÊç¢‰∫ÜË±ÜÂåÖÂÆ∂ÁöÑÂ•Ω‰∫Ü"
      },
      {
        "user": "houxilong",
        "created_at": "2024-11-26T14:23:41Z",
        "body": "Qwen2 text-embedding-V2 support batch size is 25, so add parameter \"embedding_batch_num\" to 25, as follows:\r\n\r\n`        rag = LightRAG(\r\n            working_dir=WORKING_DIR,\r\n            llm_model_func=llm_model_func,\r\n            embedding_func=EmbeddingFunc(\r\n                embedding_dim=1536,\r\n                max_token_size=8192,\r\n                func=embedding_func,\r\n                embedding_batch_num=25\r\n            )`"
      },
      {
        "user": "yaohanze",
        "created_at": "2025-01-13T02:55:45Z",
        "body": "> Qwen2 text-embedding-V2 support batch size is 25, so add parameter \"embedding_batch_num\" to 25, as follows:\r\n> \r\n> ` rag = LightRAG( working_dir=WORKING_DIR, llm_model_func=llm_model_func, embedding_func=EmbeddingFunc( embedding_dim=1536, max_token_size=8192, func=embedding_func, embedding_batch_num=25 )`\r\n\r\nembedding_batch_number should be a parameter of LightRAG instead of EmbeddingFunc."
      }
    ]
  },
  {
    "number": 318,
    "title": "fix: ‰øÆÊîπ‰∫Ülocal search‰∏≠ÁöÑrelation_countsËÆ°Êï∞ËßÑÂàô",
    "created_at": "2024-11-21T06:38:44Z",
    "closed_at": "2024-11-22T07:14:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/318",
    "body": "ÊÇ®Â•ΩÔºåÁúãÂà∞ÊÇ®ÁöÑÈ°πÁõÆÂØπÊàëÂæàÊúâÂêØÂèë„ÄÇ‰ΩÜÊàëÂú®‰ΩøÁî® local search Êó∂ÔºåÂèëÁé∞Ê∫êÁ†Å‰∏≠`/LightRAG/lightrag/operate.py`  Âú®Âè¨ÂõûÂÆû‰ΩìÊúÄÁõ∏ÂÖ≥ÊñáÊú¨ÂçïÂÖÉ`_find_most_related_text_unit_from_entities`ÂáΩÊï∞ÁöÑÊó∂ÂÄôÔºå‰∏∫‰∫ÜÂØπÊñáÊú¨ÂçïÂÖÉÈáçË¶ÅÊÄßËøõË°åÊéíÂ∫èÊâÄ‰ΩøÁî®Âà∞‰∫Ürelation_counts„ÄÇ\r\n‰ΩÜÂú®Áé∞ÊúâÁöÑÈÄªËæë‰∏≠ÔºåÂ¶ÇÊûú‰∏Ä‰∏™ÊñáÊú¨ÂçïÂÖÉc_id‰∏éÂ§ö‰∏™ÂÆû‰ΩìËäÇÁÇπÁõ∏ÂÖ≥ËÅîÔºå‰∏îËøô‰∫õÂÆû‰ΩìËäÇÁÇπÁöÑ‰∏ÄË∑≥Áõ∏ÈÇªËäÇÁÇπÊï∞Èáè‰∏çÂêåÔºåÈÇ£‰πàÂÖàÈÅçÂéÜÂà∞‰∏ÄË∑≥Áõ∏ÈÇªËäÇÁÇπÂ∞ëÁöÑÂÆû‰ΩìËäÇÁÇπÊó∂Ôºårelation_countsÂèØËÉΩ‰ºöË¢´‰Ωé‰º∞„ÄÇ\r\nÊâÄ‰ª•Êàë‰øÆÊîπ‰∏∫ÔºöÂú®ÊØèÊ¨°ÈÅçÂéÜÊó∂Êõ¥Êñ∞relation_countsÔºåËÄå‰∏çÊòØË∑≥ËøáÂ∑≤ÁªèÂ≠òÂú®ÁöÑÊñáÊú¨ÂçïÂÖÉ„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/318/comments",
    "author": "zzzcccxx",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-22T07:14:03Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÔºÅ"
      }
    ]
  },
  {
    "number": 317,
    "title": "Request for query set",
    "created_at": "2024-11-21T04:42:36Z",
    "closed_at": "2025-02-17T10:58:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/317",
    "body": "hi @LarFii , I want to know if the authors can provide the query set (125 questions) of each dataset for reproduction.\r\nThanks:D",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/317/comments",
    "author": "jeremyzhangsq",
    "comments": [
      {
        "user": "hzwy3c",
        "created_at": "2024-11-24T07:44:56Z",
        "body": "ËøôÈáå‰πüÂêåÊ†∑ÈúÄË¶ÅÔºÅË∞¢Ë∞¢"
      }
    ]
  },
  {
    "number": 316,
    "title": "‰ΩøÁî®AzureOpenAIÂÆûÁé∞ÔºåÊîØÊåÅRPM/TPMÈôêÂà∂„ÄÇ‰øÆÂ§çÂéüÂÖà429ÂìçÂ∫îÂç≥ÊäõÂá∫ÂºÇÂ∏∏ÁöÑÈóÆÈ¢ò",
    "created_at": "2024-11-21T02:37:40Z",
    "closed_at": "2024-11-22T07:13:18Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/316",
    "body": "ÂéüÂÖàÁöÑAzureAIÁöÑDemoÂú®Ë∞ÉÁî®ÊûÑÂª∫Áü•ËØÜÂõæË∞±Êó∂‰ºöË∂ÖËøáÊé•Âè£ÈÄüÁéáÈôêÂà∂ÔºåÊó†ÈáçËØïÊú∫Âà∂ÔºåÂØºËá¥ÊûÑÂª∫ÂõæË∞±Â§±Ë¥•„ÄÇÁé∞Âú®‰øÆÊîπ‰∏∫‰ΩøÁî®AzureOpenAIÂÆòÊñπSDKÂÆûÁé∞",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/316/comments",
    "author": "magicyuan876",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-21T02:59:20Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢Ôºå‰ΩÜÊòØÊúâ‰∏Ä‰∫õ linting errorsÔºåÈúÄË¶ÅÂú®Êèê‰∫§ÂâçËøêË°å`pre-commit run --all-files`‰øÆÂ§ç‰∏Ä‰∏ã"
      },
      {
        "user": "magicyuan876",
        "created_at": "2024-11-21T11:13:17Z",
        "body": "ÈáçÊñ∞Êèê‰∫§‰∫ÜÔºåÁõ¥Êé•Âú®Á∫øÁºñËæëÁöÑÔºå‰∏çÂ•ΩÊÑèÊÄùÔºåÁé∞Âú®Â∑≤ÁªèÊú¨Âú∞Ê£ÄÊü•Ëøá‰∫Ü"
      }
    ]
  },
  {
    "number": 314,
    "title": "Discord is expired",
    "created_at": "2024-11-20T17:57:00Z",
    "closed_at": "2024-11-22T07:15:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/314",
    "body": "Discord link on the README is expired",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/314/comments",
    "author": "nihirv",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-22T07:15:35Z",
        "body": "Thanks! I have updated the link."
      }
    ]
  },
  {
    "number": 312,
    "title": "Testing on about 1000 documents and more",
    "created_at": "2024-11-20T13:48:59Z",
    "closed_at": "2025-02-17T10:58:39Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/312",
    "body": "Hello,\r\nI want to test this solution on about 1000 document and more, has anybody test it on that amount of data?. if not does anyone has any recommendations on how to do it like do I need to change the vector store and KG database?, is neo4j or Oracle 23ai better on this big scale?, any other recommendation? .",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/312/comments",
    "author": "kabani7",
    "comments": [
      {
        "user": "GaryDean",
        "created_at": "2024-11-20T23:22:44Z",
        "body": "i have done it on 3500+ txt/md files, of mixed sizes.  very slow, but no problems."
      },
      {
        "user": "kabani7",
        "created_at": "2024-11-21T06:49:15Z",
        "body": "> i have done it on 3500+ txt/md files, of mixed sizes. very slow, but no problems.\r\n\r\ndid you use Neo4j or the default KG?, also did you change the Vector database ?\r\n   "
      },
      {
        "user": "GaryDean",
        "created_at": "2024-11-21T10:48:03Z",
        "body": "just the default. minor additions to vector database."
      },
      {
        "user": "GaryDean",
        "created_at": "2024-11-22T09:09:04Z",
        "body": "See #321 "
      }
    ]
  },
  {
    "number": 310,
    "title": "Multi-User / Multi-Tenant",
    "created_at": "2024-11-19T20:02:59Z",
    "closed_at": "2025-02-17T10:58:38Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/310",
    "body": "Hello!\r\nAfter evaluating a bunch of graph based RAG tools, LightRAG looks like the best!\r\n\r\nQuick question, and perhaps I am missing this somewhere in the documentation: is it possible to have multiple users/tenants?\r\n\r\nFrom all of the examples, it appears as if there is a single, global graph that all queries and inserts use.\r\n\r\nI have a use-case with multiple users, where each user's data and graph must be completely private separate from all other user's graphs.\r\n\r\nHow would one do that with LightRAG?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/310/comments",
    "author": "cromulus",
    "comments": [
      {
        "user": "magicyuan876",
        "created_at": "2024-11-20T01:59:49Z",
        "body": "Different users can use different directories to build knowledge graphs and Q&A systems. `graph_dir_path` is the directory you specify yourself.\r\n`        rag = LightRAG(\r\n            working_dir=graph_dir_path,\r\n            llm_model_func=llm_model_func,\r\n            embedding_func=EmbeddingFunc(\r\n                embedding_dim=embedding_dimension,\r\n                max_token_size=8192,\r\n                func=embedding_func,\r\n            ),\r\n        )`"
      },
      {
        "user": "Arslan-Mehmood1",
        "created_at": "2024-11-23T13:37:19Z",
        "body": "@cromulus  you need to develop a custom solution, and control the database working_dir for each tenant. It's not that difficult. You can ask chatgpt for it."
      }
    ]
  },
  {
    "number": 301,
    "title": "Didn't extract any relationships with gpt_4o_mini_complete, working with gpt_4o_mini_complete",
    "created_at": "2024-11-18T18:03:16Z",
    "closed_at": "2025-02-17T10:58:36Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/301",
    "body": "When using gpt_4o_complete to create the knowledge graph I'm seeing the warning:\r\n`WARNING:lightrag:Didn't extract any relationships, maybe your LLM is not working`\r\n\r\nInstantiation looks like this:\r\n```py\r\nrag = LightRAG(\r\n            working_dir=working_dir,\r\n            llm_model_func=gpt_4o_complete, \r\n            graph_storage=\"Neo4JStorage\",\r\n            log_level=\"INFO\",\r\n        )\r\n```\r\n\r\nI don't see the same warning when using gpt_4o_mini_complete\r\n\r\nThe app is creating a knowledge graph for chunks of some markdown files (originally converted from PDF)\r\nAny thoughts on what could be causing this?\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/301/comments",
    "author": "rcoundon",
    "comments": [
      {
        "user": "rcoundon",
        "created_at": "2024-11-18T18:06:25Z",
        "body": "I'm not sure it's related, but when using the mini there is still a warning but it's seemingly unrelated:\r\n\r\n\r\n`WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: NCRNCSNCT)} {position: line: 1, column: 10, offset: 9} for query: 'MATCH (n:`NCRNCSNCT`) RETURN n'`\r\n\r\nI'm not actually issuing queries at this point, just creating the KGs for my docs.\r\n"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:36:16Z",
        "body": "You should check the output from the LLM during the extraction process. You can directly review the cache files to see if the output is as expected. This can help determine whether the issue lies in the LLM's response or elsewhere in the pipeline."
      },
      {
        "user": "rcoundon",
        "created_at": "2024-11-19T09:28:39Z",
        "body": "> You should check the output from the LLM during the extraction process. You can directly review the cache files to see if the output is as expected. This can help determine whether the issue lies in the LLM's response or elsewhere in the pipeline.\r\n\r\nOk, thanks, I'll take a look and report back"
      },
      {
        "user": "rcoundon",
        "created_at": "2024-11-19T13:25:04Z",
        "body": "I've had a look at  `kv_store_llm_response_cache.json` and `vdb_relationships.json` and there's a fair amount of data there but I'm not sure what I'm looking at. \r\n\r\nHowever, on the response cache I see this:\r\n\r\n> Given the technical and largely descriptive nature of the provided text, it's challenging to identify traditional entities like organizations, persons, geolocations, or events as defined by the constraints. However, I can focus on certain elements like terms related to the overall process described in the content:\r\n> \r\n> 1. Entity Identification:\r\n>    None of the traditional entities (organization, person, geo, event) are clearly specified in the text provided.\r\n> \r\n> 2. Relationships:\r\n>    Lacking traditional entities, there are no clear relationships to be defined among any potential entities.\r\n> \r\n> However, focusing purely on elements present within the text, I can attempt to identify concepts or technical terms that may act as placeholders:\r\n> \r\n> (\"entity\"<|>\"Component Mounting\"<|>\"event\"<|>\"The process of fixing components to the wall across various languages, depicted through diagrams and imagery.\")##  \r\n> (\"entity\"<|>\"Suction\"<|>\"event\"<|>\"Details about the aspiration or suction process via different alignments, like light shaft.\")##  \r\n> (\"entity\"<|>\"Aspiration via Light Shaft\"<|>\"event\"<|>\"Technical specifications regarding how aspiration is carried out using a light shaft in the mounting process.\")##  \r\n> (\"entity\"<|>\"Technical Diagrams\"<|>\"event\"<|>\"Imagery used to illustrate the process of component mounting and air control techniques.\")##  \r\n> \r\n> Since the text does not contain clear, traditional entities, and relationships, the extraction remains limited to the terms and processes identified in the text. If there are further specific details or entities within additional context or a different section of text, please provide more clarity to enable a more refined extraction.\r\n> \r\n\r\nWhich I suspect is the source of the warning I reported.  Would you agree? If so, is there a way to guide the LLM when initiating this process on how to establish entities and relationships?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-21T03:01:09Z",
        "body": "Yes, I think that is the root cause of the issue. Modifying the entity types in `prompt.py` might help resolve it."
      },
      {
        "user": "GaryDean",
        "created_at": "2024-11-22T09:07:29Z",
        "body": "I came across this issue while processing a large number of files. The warning message `WARNING:lightrag:Didn't extract any relationships, maybe your LLM is not working` only ever occurred with _very small files_ (<50 bytes) that, in fact, contained no relevant information.  Solution was to remove such files."
      }
    ]
  },
  {
    "number": 300,
    "title": "fix:error working directory name in Step_1.py",
    "created_at": "2024-11-18T15:09:37Z",
    "closed_at": "2024-11-19T01:45:11Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/300",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/300/comments",
    "author": "WinstonCHEN1",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T01:45:08Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 299,
    "title": "rag.insert() is so slow",
    "created_at": "2024-11-18T11:29:20Z",
    "closed_at": "2025-02-17T10:58:36Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/299",
    "body": "The rag.insert() process is too slow. Can I divide the document into multiple parts, specify different GPUs for insert in the same project, and finally build a knowledge graph about the complete document?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/299/comments",
    "author": "SLKAlgs",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:34:11Z",
        "body": "This approach could lead to conflicts. However, one possible solution is to divide the document into multiple parts, cache all the extraction results from the LLMs for each part, and then merge the caches before performing the insertion. I haven‚Äôt tried this yet, but it should be theoretically feasible."
      },
      {
        "user": "nihirv",
        "created_at": "2024-11-20T22:37:55Z",
        "body": "To mitigate #315, I decided to modify my code to call rag.insert() on each page of a document (i.e. `[rag.insert(page) for page in doc]` vs `rag.insert(doc)`), and am also finding this method very slow"
      },
      {
        "user": "salzubi401",
        "created_at": "2024-11-22T15:15:47Z",
        "body": "+1 the latency is extreme"
      },
      {
        "user": "starimpact",
        "created_at": "2024-12-04T00:40:52Z",
        "body": "+1 very very slow, I am processing one 1M size text file, and have been waited a whole night, but it is only finished 40%,  still on the process...\r\nhow to accelerate it ?\r\n```bash\r\nExtracting entities from chunks:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 123/305 [9:44:15<59:14, 19.53s/chunk]\r\n```"
      }
    ]
  },
  {
    "number": 297,
    "title": "LightRAGËØ¶ÁªÜÁöÑËØÑÊµãÊñπÊ°à",
    "created_at": "2024-11-18T03:54:47Z",
    "closed_at": "2025-02-17T10:58:35Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/297",
    "body": "ËØ∑ÈóÆLightRAGÊúâ‰ΩøÁî®RAGASÁ±ª‰ººÁöÑÊ°ÜÊû∂ËøõË°åËØÑ‰º∞ËøáÂêóÔºüÊúâÂÖ∑‰ΩìÁöÑ‰æãÂ≠ê‰ª£Á†ÅÂêóÔºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/297/comments",
    "author": "WangAo-0",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-11-18T04:44:27Z",
        "body": "ÊàëÁî® Ragas ÊµãËØÑÊØîËæÉËøá Lightrag Âíå GraphragÔºåÂ¶ÇÊûúÈúÄË¶ÅÊàëÂèØ‰ª•Êèê‰∏Ä‰∏™ PR ËØ¶ÁªÜÊèèËø∞"
      },
      {
        "user": "WangAo-0",
        "created_at": "2024-11-18T05:53:30Z",
        "body": "> ÊàëÁî® Ragas ÊµãËØÑÊØîËæÉËøá Lightrag Âíå GraphragÔºåÂ¶ÇÊûúÈúÄË¶ÅÊàëÂèØ‰ª•Êèê‰∏Ä‰∏™ PR ËØ¶ÁªÜÊèèËø∞\r\n\r\nÈúÄË¶ÅÁöÑÔºåÊÑüË∞¢ÊÇ®"
      },
      {
        "user": "chenboju",
        "created_at": "2024-11-19T14:46:21Z",
        "body": "Êàë‰πüÈúÄË¶ÅÔºåÊÑüË¨ù"
      },
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-11-25T03:05:10Z",
        "body": "ÊúÄËøëÊØîËæÉÂøô‰∏çÂ•ΩÊÑèÊÄùÔºåÊàë‰ºöÂ∞ΩÂø´Êèê"
      },
      {
        "user": "huang0752",
        "created_at": "2024-11-29T11:45:53Z",
        "body": "‰Ω†Â•Ω,ÂèØ‰ª•‰πüÂèëÊàë‰∏Ä‰ªΩÂêó,ÊÑüË∞¢"
      }
    ]
  },
  {
    "number": 296,
    "title": "Multilingual Knowledge Graph Construction",
    "created_at": "2024-11-18T03:06:22Z",
    "closed_at": "2025-02-17T10:58:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/296",
    "body": "Please ask bilingual dataset to build knowledge graph, how can I modify the prompt template.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/296/comments",
    "author": "960480350",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:41:12Z",
        "body": "It doesn't seem necessary to modify the prompt. However, you can add specific constraints to the prompt, instructing the LLM not to change the language type during processing."
      }
    ]
  },
  {
    "number": 295,
    "title": "change the type of binding parameters in Oracle23AI",
    "created_at": "2024-11-18T02:05:03Z",
    "closed_at": "2024-11-19T01:44:58Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/295",
    "body": "I have update the parameter binding type in oracle_impl, so that it can handle some key words appearing in column's content\r\n\r\nchange:\r\nlightrag/kg/oracle_impl.py\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/295/comments",
    "author": "tmuife",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T01:44:55Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 294,
    "title": "What is the maximum txt file size?",
    "created_at": "2024-11-18T01:31:41Z",
    "closed_at": "2025-02-17T10:58:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/294",
    "body": "I have a txt file with size at 20M, it halted during the running process by using ollama qwen2.5.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/294/comments",
    "author": "derekcbr",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:42:53Z",
        "body": "The processing speed for large files is highly dependent on the hardware. However, based on our experience, local models tend to be very slow when handling such tasks. If you need to process large files like this, it might be more efficient to use an API instead."
      }
    ]
  },
  {
    "number": 291,
    "title": "ËØ∑ÈóÆËÉΩÂê¶ÊîØÊåÅxlsxÂíåmarkdownÊ†ºÂºèÁöÑÊñá‰ª∂Âë¢ÔºåÂØπ‰∫éragÊù•ËØ¥ÔºåmdÊ†ºÂºèÊàñËÄÖÊ≠£Á°ÆÁéá‰ºöÊõ¥È´òÔºü",
    "created_at": "2024-11-17T05:54:03Z",
    "closed_at": "2025-02-17T10:58:33Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/291",
    "body": "ËØ∑ÈóÆËÉΩÂê¶ÊîØÊåÅxlsxÂíåmarkdownÊ†ºÂºèÁöÑÊñá‰ª∂Âë¢ÔºåÂØπ‰∫éragÊù•ËØ¥ÔºåmdÊ†ºÂºèÊàñËÄÖÊ≠£Á°ÆÁéá‰ºöÊõ¥È´òÔºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/291/comments",
    "author": "wulala-star1",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-11-18T04:45:30Z",
        "body": "Actually, you can directly convert md to txt with reserved md format, LLM can recoganize it"
      }
    ]
  },
  {
    "number": 290,
    "title": "TypeError: 'NoneType' object is not subscriptable",
    "created_at": "2024-11-17T02:59:22Z",
    "closed_at": "2024-11-20T11:44:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/290",
    "body": "{args_hash: {\"return\": response.choices[0].message.content, \"model\": model}}\r\nTypeError: 'NoneType' object is not subscriptable\r\n\r\n‰ΩøÁî®Ë∞ÉÁî®OpenAIÊ®°ÂûãÁöÑ‰ª£Á†ÅÔºåembeddingÊ®°Âûã‰∏∫text-embedding-3-largeÔºåLLM‰∏∫gpt-3.5-turbo„ÄÇÂú®vdbÊï∞ÊçÆÂ∫ìÈáåÈù¢ÊàêÂäü‰øùÂ≠ò‰∫ÜÂÆû‰Ωì‰ª•ÂèäÂÖ≥Á≥ªÔºå‰ΩÜÊòØÊèêÈóÆÊó∂Ôºå‰ªª‰ΩïÈóÆÈ¢òÈÉΩ‰ºöËøîÂõûËøô‰∏™Êä•Èîô„ÄÇ\r\n\r\nÊäΩÂèñÁöÑÊòØÈïøÁØá‰∏≠ÊñáÂ∞èËØ¥\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/290/comments",
    "author": "Tian-ye1214",
    "comments": [
      {
        "user": "Tian-ye1214",
        "created_at": "2024-11-17T08:58:31Z",
        "body": "ÊµãËØï‰∫Ü3Êú¨‰∏≠ÈïøÂ∞èËØ¥ÔºåÂ§ßÊ¶ÇÈÉΩÊòØ100WÂ≠óÂ∑¶Âè≥ÁöÑÈáèÁ∫ßÔºåÂèØËÉΩÊòØpromptÂ§™Èïø‰∫ÜÔºåË∂ÖÂá∫‰∫ÜLLMÁöÑ‰∏ä‰∏ãÊñáÊú¨ÈïøÂ∫¶ÔºåÂú®naiveÊ®°Âºè‰∏ãÊúâÂõûÁ≠îÔºå‰ΩÜÂàáÊç¢Âà∞ÂÖ∂‰ªñ3‰∏™Ê®°ÂºèÂ∞±Ê≤°Êúâ‰∫Ü„ÄÇËØ∑ÈóÆËøô‰∏ÄÁÇπÊòØÂê¶ËÉΩÂ§üÊîπËøõÔºü"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:46:46Z",
        "body": "ÂèØ‰ª•ÂàáÊç¢ÊàêminiÔºåturboÁ°ÆÂÆû‰ºöÊúâËøô‰∏™ÈóÆÈ¢òÔºåÂêåÊó∂ÂèØ‰ª•‰øÆÊîπquery param‰∏≠ÁöÑtopkÔºåÂáèÂ∞èkÂÄº‰πüËÉΩËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇÂêéÁª≠Êàë‰ª¨‰ºöËÄÉËôëÊõ¥Â•ΩÁöÑÂäûÊ≥ïÊù•ÈôêÂà∂‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÇ"
      }
    ]
  },
  {
    "number": 286,
    "title": "fix neo4j bug",
    "created_at": "2024-11-16T07:10:35Z",
    "closed_at": "2024-11-19T07:25:49Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/286",
    "body": "fix the bug which describe in issue #269, and I found neo4j would threw ClientError when thread pool is overload. so I add a retry Exception by the way.\r\n\r\n‰øÆÂ§ç‰∫Ü#269 ÈóÆÈ¢òÔºåÁÑ∂ÂêéÊàëÂèëÁé∞ÂΩìÁ∫øÁ®ãÊ±†‰∏çÂ§üÁî®ÁöÑÊó∂ÂÄôÔºåneo4j‰ºöÊäõÂá∫ClientErrorÔºåÊâÄ‰ª•È°∫ÊâãÂä†‰∫Ü‰∏Ä‰∏™ÈáçËØïÁöÑÂºÇÂ∏∏",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/286/comments",
    "author": "Sucran",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T01:45:47Z",
        "body": "Thanks, but there are some linting errors. Please make sure to run pre-commit run --all-files before submitting to ensure all linting checks pass."
      },
      {
        "user": "Sucran",
        "created_at": "2024-11-19T09:09:54Z",
        "body": "@LarFii Thanks for merge, I got it."
      }
    ]
  },
  {
    "number": 284,
    "title": "Entity and relationship extraction performance ",
    "created_at": "2024-11-15T06:24:50Z",
    "closed_at": "2025-02-17T10:58:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/284",
    "body": "Can provide any information regarding the hardware used and the total entity + relationship extraction processing time it took across each of the 4 evaluation datasets (Agriculture, CS, Legal, Mix) in the LightRAG paper? Testing out LightRAG in test environment on a T4 with using qwen2 on a bout 400,000 tokens split into 3 books around the same topic and the entity and relationship extraction is has been underway for about 12 hours and is still not complete. Curious if there are any rough benchmarks that I can look to get a rough sense of time estimates for entity + relationship extraction, relative to hardware configuration, model type, text subject category, total token amount, etc. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/284/comments",
    "author": "mark-veryawesome",
    "comments": [
      {
        "user": "DirakesSea",
        "created_at": "2024-11-15T09:52:22Z",
        "body": "+1"
      },
      {
        "user": "gcl916",
        "created_at": "2024-11-17T12:59:55Z",
        "body": "For reference:\r\nglm-4-9b + bge m3, llm_model_max_async=3 ( set 4 reach online limit, so 3),online API.\r\n1310 txt, 10MB totally.\r\ntimeÔºönear 20 hours."
      }
    ]
  },
  {
    "number": 283,
    "title": "[node embedding] node_embedding_algorithm ÁöÑÁñëÈóÆ",
    "created_at": "2024-11-15T06:22:01Z",
    "closed_at": "2025-02-17T10:57:51Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/283",
    "body": "ÁªèËøá‰ª£Á†ÅÂàÜÊûêÔºåÊàëÂèëÁé∞ËôΩÁÑ∂Á≥ªÁªü‰∏≠Â£∞Êòé‰∫Ü node_embedding_algorithm=node2vec ‰Ωú‰∏∫ËäÇÁÇπÂµåÂÖ•ÁÆóÊ≥ïÔºå‰ΩÜÂÆûÈôÖËøêË°å‰∏≠Âπ∂Êú™Ë∞ÉÁî® _node2vec_embed ËøõË°åÂµåÂÖ•ËÆ°ÁÆó„ÄÇÂú®ËøõË°å entity„ÄÅrelationship Âíå text units ÁöÑÂè¨ÂõûÊó∂ÔºåÁ≥ªÁªü‰ªÖ‰ªÖ‰ΩøÁî®‰∫ÜÂõæÊï∞ÊçÆÂ∫ìÊúÄÂü∫Á°ÄÁöÑ get_node„ÄÅget_edge Á≠â graph Êìç‰ΩúÊù•Êü•ËØ¢Áõ¥Êé•ÂÖ≥Á≥ªÔºåËøôÁßç‰ΩøÁî®ÊñπÂºèÂÆåÂÖ®ÂèØ‰ª•Áî®ÊôÆÈÄöÁöÑÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÊù•ÂÆûÁé∞„ÄÇÈÇ£‰πàÔºå‰ΩøÁî® graph ÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØÂê¶‰ªÖ‰ªÖÂ±ÄÈôê‰∫éÊï∞ÊçÆÂ≠òÂÇ®ÂíåËÆ°ÁÆó degree Áî®‰∫éÊéíÂ∫èÔºüÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÁõÆÂâçÁöÑÂÆûÁé∞Âπ∂Êú™ÂÖÖÂàÜÂèëÊå• graph Âú®Â§ÑÁêÜÂ§öÁ∫ßÈó¥Êé•ÂÖ≥Á≥ªÊñπÈù¢ÁöÑ‰ºòÂäøÔºåÊØîÂ¶ÇÊó†Ê≥ïÂÆûÁé∞‰ªé A ÁªèËøá B„ÄÅC ÊúÄÁªàÂà∞Ëææ D ËøôÊ†∑ÁöÑÈó¥Êé•ÂÖ≥Á≥ªË∑ØÂæÑÂèëÁé∞„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/283/comments",
    "author": "SpikeYangRc",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:58:03Z",
        "body": "ÈùûÂ∏∏ÊÑüË∞¢ÊÇ®ÁöÑÂÖ≥Ê≥®ÔºåÂÆûÈôÖ‰∏äÂú®ÂÅöÊ£ÄÁ¥¢ÁöÑËøáÁ®ã‰∏≠Êàë‰ª¨ÊòØÂà©Áî®ÂêëÈáèËøõË°åÁöÑÂåπÈÖçÔºåÂêåÊó∂Âú®local‰ø°ÊÅØ‰∏≠‰πü‰ºöÂåÖÂê´ËäÇÁÇπÁöÑ‰∏ÄÈò∂ÈÇªÂ±Ö„ÄÇÂΩìÁÑ∂ÔºåÊàë‰ª¨ÁõÆÂâçÁ°ÆÂÆûÊ≤°ÊúâÂà©Áî®graphÂ§ÑÁêÜÂ§öÁ∫ßÈó¥Êé•ÂÖ≥Á≥ªÔºåËøô‰πüÊòØÊàë‰ª¨Êú™Êù•Êé¢Á¥¢ÁöÑÊñπÂêë‰πã‰∏Ä„ÄÇÂêéÁª≠Êàë‰ª¨‰πü‰ºöÊ∑ªÂä†Êõ¥Â§öÁöÑÂõæÁõ∏ÂÖ≥ÂäüËÉΩÔºåÂåÖÊã¨Ëá™ÂÆö‰πâKGÁ≠âÁ≠â„ÄÇ"
      },
      {
        "user": "imasoul2",
        "created_at": "2024-11-27T23:05:16Z",
        "body": "Hey @LarFii , would you have a roadmap of when node2vec will be utilised in LightRAG, also additionally, would this have huge improvement over the querying stage of RAG?"
      }
    ]
  },
  {
    "number": 281,
    "title": "‰∏çÊñ≠ÂêëÂõæ‰∏≠insertÊñáÊú¨Êó∂‰ºöÊ≠ªÊú∫ÔºåË∑ë‰∏Ä‰ºöÂ∞±Ê≠ªÊú∫Ôºå100%Â§çÁé∞Ôºå‰∏çÁü•ÈÅìÂÆòÊñπÊµãËØïÊúâÊ≤°ÊúâËøô‰∏™ÈóÆÈ¢ò",
    "created_at": "2024-11-15T05:44:49Z",
    "closed_at": "2025-02-17T10:57:47Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/281",
    "body": "ÊúâÂá†‰∏™ÊñáÊú¨Êñá‰ª∂ÔºåÊØèÊ¨°‰∏≤Ë°åËØª100kÁÑ∂Âêé Ë∞ÉÁî®insertÊé•Âè£ÔºållmË∞ÉÁî®ÁöÑÊòØÂ§ñÈÉ®apiÔºåÂú®ÊèíÂÖ•ÁöÑËøáÁ®ã‰∏≠cpuÂà©Áî®ÁéáÂà∞‰∫Ü100%ÔºåË∑ëÁùÄË∑ëÁùÄÂ∞± Ê≠ªÊú∫‰∫Ü„ÄÇ\r\nÂèÇÊï∞ËÆæÁΩÆÔºöchunk_size=1200,  llm_model_max_async = 8,  embedding_func_max_async = 8\r\nÊòØ‰∏çÊòØÂú®ÂêëÂõæ‰∏≠ÊèíÂÖ•ËäÇÁÇπÊàñËÄÖÂéªËäÇÁÇπÂéªÈáçÊó∂Âπ∂ÂèëËøáÂ§ßÔºüÊàñËÄÖÊó∂Â†ÜÊ†àÊ∫¢Âá∫‰∫Ü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/281/comments",
    "author": "wangtianqi1993",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-15T16:24:16Z",
        "body": "ÁõÆÂâçÊàë‰ª¨Ê≤°ÊúâÈÅáÂà∞Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÁõÆÂâçÊµãËØïÁöÑÊúÄÂ§ßÁöÑÊï∞ÊçÆÈõÜÊòØLegalÔºåÂåÖÂê´94‰∏™ÊñáÊú¨Êñá‰ª∂ÔºåtokenÊÄªÊï∞‰∏∫500w„ÄÇ"
      },
      {
        "user": "Sucran",
        "created_at": "2024-11-16T07:13:45Z",
        "body": "Ë∑ëÁùÄË∑ëÁùÄÊ≠ªÊú∫ÔºåÂ∫îËØ•ÊòØÂè™‰ΩøÁî®cpuÁöÑÊó∂ÂÄôÔºåÊúâÂ§™Â§öÊï∞ÊçÆÈúÄË¶ÅÊèíÂÖ•ÂêëÈáèÊï∞ÊçÆÂ∫ìÊó∂Âá∫Áé∞ÁöÑ„ÄÇcpuÊª°Ë¥üËç∑‰∫ÜÔºåÊàëÂú®Ëá™Â∑±mac‰∏ä‰πüÊúâÈÅáÂà∞ËøáÔºåÂú®issue #212 ÊúâÊèêÂà∞ËØ¥Â∞ΩÈáè‰ΩøÁî®gpuÔºåËøôÊ†∑‰ºöÂø´ÁÇπÔºåÊù°‰ª∂ÊúâÈôêÁöÑËØù‰πüÊ≤°ÂäûÊ≥ïÔºåÂèØ‰ª•ÈÄÇÂΩìÂú∞Èôç‰ΩéembeddingÁöÑÂπ∂ÂèëÊï∞‰πüÂèØ‰ª•„ÄÇllmÁöÑÂπ∂ÂèëÊï∞Âü∫Êú¨‰∏çÂ§™ÂΩ±ÂìçcpuÁöÑ‰ΩøÁî®Áéá„ÄÇ @wangtianqi1993 "
      },
      {
        "user": "wangtianqi1993",
        "created_at": "2024-11-18T02:48:03Z",
        "body": "> #212\r\n\r\nÂ§öË∞¢Ëß£Á≠îÔºåÊàëÊòØÂú®4090ÊúçÂä°Âô®‰∏äË∑ëÁöÑÔºåÂºÄÂßãemb‰∏éllmÈÉΩÈÉ®ÁΩ≤Âú®Êú¨Âú∞gpu‰∏äÔºåË∑ë‰∏ÄÊÆµÊó∂Èó¥Â∞±‰ºöÊ≠ªÊú∫ÔºåÊÄÄÁñëÊòØllmÂØπÊòæÂç°Âà©Áî®ËøáÈ´òÔºåÊâÄ‰ª•Êç¢Êàê‰∫ÜÂ§ñÈÉ®llm api+ Êú¨Âú∞gpu ‰∏äÁöÑ embÊ®°ÂûãÔºåÁúãÁúãÈôçÈÄüË∑ëËÉΩ‰∏çËÉΩËß£ÂÜ≥ÈóÆÈ¢ò ÔºåË∞ÉÁî®Â§ñÈÉ®llm api Â§ÑÁêÜËøáÁ®ãÂ∞±ÂæàÊÖ¢‰∫ÜÔºå‰ΩÜÊòØËøáÁ®ã‰∏≠ËøòÊòØ‰ºöÊ≠ªÊú∫Ôºå ÈÇ£Â∞±ÂèØËÉΩÂÉè‰Ω†ËØ¥ÁöÑÈÇ£Ê†∑ ÊèíÂÖ•Êï∞ÊçÆÂ∫ìÊó∂Êª°Ë¥üËç∑‰∫ÜÔºåÊàëÂÜçÁúãÁúãË∞ÉÊï¥‰∏ãÂèÇÊï∞ @Sucran "
      },
      {
        "user": "adamwuyu",
        "created_at": "2025-01-08T02:13:32Z",
        "body": "> Ë∑ëÁùÄË∑ëÁùÄÊ≠ªÊú∫ÔºåÂ∫îËØ•ÊòØÂè™‰ΩøÁî®cpuÁöÑÊó∂ÂÄôÔºåÊúâÂ§™Â§öÊï∞ÊçÆÈúÄË¶ÅÊèíÂÖ•ÂêëÈáèÊï∞ÊçÆÂ∫ìÊó∂Âá∫Áé∞ÁöÑ„ÄÇcpuÊª°Ë¥üËç∑‰∫ÜÔºåÊàëÂú®Ëá™Â∑±mac‰∏ä‰πüÊúâÈÅáÂà∞ËøáÔºåÂú®issue #212 ÊúâÊèêÂà∞ËØ¥Â∞ΩÈáè‰ΩøÁî®gpuÔºåËøôÊ†∑‰ºöÂø´ÁÇπÔºåÊù°‰ª∂ÊúâÈôêÁöÑËØù‰πüÊ≤°ÂäûÊ≥ïÔºåÂèØ‰ª•ÈÄÇÂΩìÂú∞Èôç‰ΩéembeddingÁöÑÂπ∂ÂèëÊï∞‰πüÂèØ‰ª•„ÄÇllmÁöÑÂπ∂ÂèëÊï∞Âü∫Êú¨‰∏çÂ§™ÂΩ±ÂìçcpuÁöÑ‰ΩøÁî®Áéá„ÄÇ @wangtianqi1993\r\n\r\nÊàëÂú®ÊèíÂÖ•ÊúçÂä°Âô®‰∏äÁöÑneo4jÊï∞ÊçÆÂ∫ìÊó∂ÈÅáÂà∞Ëøácpu‰ΩøÁî®ÁéáËøáÈ´òÊä•ÈîôÁöÑÈóÆÈ¢ò„ÄÇÂêéÊù•ÊîπÊàê‰ΩøÁî®Êú¨Âú∞ÁöÑneo4jÊï∞ÊçÆÂ∫ìÂêéËß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢ò„ÄÇ"
      }
    ]
  },
  {
    "number": 280,
    "title": "ÂØπ‰∫éÂ∑≤ÊúâÁöÑ‰∏âÂÖÉÁªÑÊï∞ÊçÆÔºåÂèØ‰ª•ÊÄé‰πàÊûÑÂª∫",
    "created_at": "2024-11-15T03:13:57Z",
    "closed_at": "2024-11-18T01:07:39Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/280",
    "body": "ÁúãÂà∞ÁöÑÁ§∫‰æãÊòØ‰ªéÊñáÊú¨Êï∞ÊçÆÁõ¥Êé•ÊûÑÂª∫ÁöÑÔºå‰ΩÜÊàëÂ∑≤ÁªèÊúâ‰∫Ü‰∏Ä‰∫õ‰∏âÂÖÉÁªÑÊï∞ÊçÆÔºåÂèØ‰ª•ÊÄé‰πàÊûÑÂª∫ÔºüÊúâÊ≤°ÊúâÂØπÂ∫îÁöÑÂáΩÊï∞ÊàñËÄÖÊèêÁ§∫ËÆæÁΩÆÔºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/280/comments",
    "author": "laizef",
    "comments": [
      {
        "user": "laizef",
        "created_at": "2024-11-18T01:07:39Z",
        "body": "Ëá™ÈóÆËá™Á≠î‰∏Ä‰∏ãÔºåÈÄöËøáÊîπÂÜôprompt.pyÔºåÂèØ‰ª•ÊèêÂèñcsvÊñá‰ª∂‰∏≠ÁöÑÂÆû‰ΩìÂíåÂÖ≥Á≥ªÔºåÊúÄÂêéÁöÑÈóÆÁ≠îÊïàÊûú‰πüËøò‰∏çÈîô„ÄÇÂêåÁêÜÔºåÂêÑÁ±ªÂûãÁöÑÊï∞ÊçÆÈÉΩÂèØ‰ª•Áî®Áõ∏‰ººÁöÑÊñπÊ≥ïËß£ÂÜ≥„ÄÇ"
      },
      {
        "user": "mcgillkwok",
        "created_at": "2024-12-03T02:55:56Z",
        "body": "ËÉΩÂàÜ‰∫´‰∏Ä‰∏ãcsvÁöÑÊù•Ê∫êÊï∞ÊçÆÂú®prompt.pyÊòØÂ¶Ç‰ΩïÂÜôÁöÑÂêóÔºü"
      }
    ]
  },
  {
    "number": 277,
    "title": "ÊàëÊµãËØïÂèëÁé∞Âç≥‰ΩøÊòØÂú®‰∏ì‰∏öÁöÑÁü•ËØÜÈ¢ÜÂüüÔºå‰ΩøÁî®lightragÊ®°ÂûãÂõûÁ≠îÁöÑÂáÜÁ°ÆÁéá‰ºº‰πé‰∏çÂ¶ÇËÆ©LLMÁõ¥Êé•ÁªôÂá∫Á≠îÊ°à„ÄÇ",
    "created_at": "2024-11-14T13:56:05Z",
    "closed_at": "2025-02-17T10:57:37Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/277",
    "body": "ÊàëÈ¶ñÂÖàÊûÑÂª∫‰∫ÜÂåªÂ≠¶È¢ÜÂüüÁöÑÁü•ËØÜÂõæË∞±ÔºåÁÑ∂Âêé‰ΩøÁî®Êú¨Âú∞È¢òÂ∫ìËøõË°å‰∫ÜÊµãËØïÔºåÁªìÊûúÂèëÁé∞‰ΩøÁî®ragÁöÑÁªìÊûú‰∏çÂ¶ÇÊ≤°‰ΩøÁî®ragÔºåËøáÈïøÁöÑ‰∏ä‰∏ãÊñá‰ºº‰πé‰ºöÂØºËá¥LLMÂõûÁ≠îÂáÜÁ°ÆÁéáÈôç‰ΩéÔºå‰ΩøÁî®ÁöÑLLMÊòØdeepseek chatÔºå‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÊ≠£Â∏∏ÁöÑÁé∞Ë±°„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/277/comments",
    "author": "sunzhaoyang1",
    "comments": [
      {
        "user": "zzzcccxx",
        "created_at": "2024-11-19T07:47:40Z",
        "body": "ÊàëËßâÂæóÂØπÈÄöÁî®ÈóÆÈ¢òÁöÑËØùËøôÁßçÊÉÖÂÜµÊòØÂêàÁêÜÁöÑÔºåÂõ†‰∏∫Âè¨ÂõûÁöÑÁü•ËØÜ‰∏≠ËÇØÂÆöÂ≠òÂú®ÁùÄ‰∏éËØ•ÈóÆÈ¢ò‰∏çÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÊèèËø∞ÔºåÂØºËá¥Ëøò‰∏çÂ¶ÇÁõ¥Êé•‰ΩøÁî®LLMÂØπËØù„ÄÇ"
      },
      {
        "user": "sunzhaoyang1",
        "created_at": "2024-11-19T08:32:17Z",
        "body": "> ÊàëËßâÂæóÂØπÈÄöÁî®ÈóÆÈ¢òÁöÑËØùËøôÁßçÊÉÖÂÜµÊòØÂêàÁêÜÁöÑÔºåÂõ†‰∏∫Âè¨ÂõûÁöÑÁü•ËØÜ‰∏≠ËÇØÂÆöÂ≠òÂú®ÁùÄ‰∏éËØ•ÈóÆÈ¢ò‰∏çÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÊèèËø∞ÔºåÂØºËá¥Ëøò‰∏çÂ¶ÇÁõ¥Êé•‰ΩøÁî®LLMÂØπËØù„ÄÇ\r\n\r\nÂ•ΩÁöÑË∞¢Ë∞¢ÔºåÊàëÁöÑÊï∞ÊçÆÊòØÂåªÁñóÊñπÈù¢ÁöÑÊï∞ÊçÆÔºåÊÑüËßâÁõ¥Êé•‰ΩøÁî®LLMÁöÑÂáÜÁ°ÆÁéáË¶ÅÈ´ò‰∏ÄÁÇπ„ÄÇ"
      },
      {
        "user": "Ricky-chen1",
        "created_at": "2024-12-04T02:03:16Z",
        "body": "‰Ω†Â•ΩÔºåÊñπ‰æøÂàÜ‰∫´‰∏Ä‰∏ãÁî®‰ªÄ‰πàËæìÂÖ•ÊûÑÂª∫ÁöÑÂåªÂ≠¶È¢ÜÂüüÁü•ËØÜÂõæË∞±ÂêóÔºü\r\n"
      },
      {
        "user": "sunzhaoyang1",
        "created_at": "2024-12-04T02:07:02Z",
        "body": "> ‰Ω†Â•ΩÔºåÊñπ‰æøÂàÜ‰∫´‰∏Ä‰∏ãÁî®‰ªÄ‰πàËæìÂÖ•ÊûÑÂª∫ÁöÑÂåªÂ≠¶È¢ÜÂüüÁü•ËØÜÂõæË∞±ÂêóÔºü\r\n\r\nÊàëÊòØ‰∏≠Âåª‰∏ì‰∏öÁöÑÔºå‰ΩøÁî®‰∫ÜÂõΩÂÆ∂Ê†áÂáÜ+ÊïôÊùê+‰∏≠ÂçéÂåªÂÖ∏Ôºå‰ΩøÁî®ÊâßÂåªÊï∞ÊçÆÈõÜËØÑ‰ª∑„ÄÇ"
      },
      {
        "user": "Ricky-chen1",
        "created_at": "2024-12-04T04:15:26Z",
        "body": "Â•ΩÁöÑË∞¢Ë∞¢"
      }
    ]
  },
  {
    "number": 274,
    "title": "The speed of processing trunks",
    "created_at": "2024-11-14T07:27:20Z",
    "closed_at": "2025-02-17T10:57:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/274",
    "body": "Hi, author:\r\nI wonder if the speed of building graphs slows down with more chunks? Because according to the data set you provided, processing 300 chunks has been running for 6 hours.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/274/comments",
    "author": "icvplayer",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T08:06:04Z",
        "body": "Based on our testing results, using a local model for indexing can be extremely slow. The process is significantly faster when using an API."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:57:10Z",
        "body": "Thanks, close (old issue)."
      }
    ]
  },
  {
    "number": 271,
    "title": "How to read long texts",
    "created_at": "2024-11-14T03:53:38Z",
    "closed_at": "2025-02-17T10:57:00Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/271",
    "body": "Upload a document that has dozens of pages and can read the entire content. Currently, Rag can only read fragments. How to make a large model interpret the entire document, such as a novel, and help me summarize it",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/271/comments",
    "author": "smileyboy2019",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:57:00Z",
        "body": "Thanks, close (old issue)."
      }
    ]
  },
  {
    "number": 270,
    "title": "embedding dimension problem",
    "created_at": "2024-11-13T22:05:00Z",
    "closed_at": "2025-02-17T10:56:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/270",
    "body": "I met the embedding dimension error when I employed the open source embedding model and openai-like one. I changed the dimension from default 4096 to 1024 or 2048, I got this error:\r\n\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nano_vectordb/dbs.py\", line 71, in __post_init__\r\n    storage[\"embedding_dim\"] == self.embedding_dim\r\nAssertionError: Embedding dim mismatch, expected: 1024, but loaded: 4096\r\n\r\nWhen I didn't change the dimension and used:\r\nembedding_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\r\n\r\nI got something wrong with:\r\n\r\nINFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\r\nINFO:lightrag:Logger initialized for working directory: ./dickens\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:lightrag:Loaded graph from ./dickens/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\r\nINFO:nano-vectordb:Load (0, 4096) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 4096) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 4096) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 4096, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 42 data\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 42 chunks\r\nINFO:lightrag:Inserting 42 vectors to chunks\r\nBatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.20it/s]\r\n...\r\n\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"/Users/zhouql1978/dev/LightRAG/test.py\", line 67, in <module>\r\n    rag.insert(f.read())\r\n  File \"/Users/zhouql1978/dev/LightRAG/lightrag/lightrag.py\", line 197, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/Users/zhouql1978/dev/LightRAG/lightrag/lightrag.py\", line 241, in ainsert\r\n    await self.chunks_vdb.upsert(inserting_chunks)\r\n  File \"/Users/zhouql1978/dev/LightRAG/lightrag/storage.py\", line 98, in upsert\r\n    results = self._client.upsert(datas=list_data)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nano_vectordb/dbs.py\", line 92, in upsert\r\n    self.__storage[\"matrix\"][i] = update_d[f_VECTOR].astype(Float)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~^^^\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n\r\nHere's my code:\r\n\r\nembedding_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\r\n\r\nembedding_func = EmbeddingFunc(\r\n        embedding_dim=4096,\r\n        max_token_size=8192,\r\n        func=lambda texts:embedding_model.aget_text_embedding_batch(texts),\r\n    )\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=embedding_func\r\n)\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/270/comments",
    "author": "13331112522",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T08:12:15Z",
        "body": "You can reference this issue #34, maybe can help."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:56:55Z",
        "body": "Thanks, close (old issue)."
      }
    ]
  },
  {
    "number": 268,
    "title": "What's the Default Prompt to use",
    "created_at": "2024-11-13T07:17:37Z",
    "closed_at": "2024-11-29T19:45:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/268",
    "body": "```\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=azure_openai_complete(\r\n        azure_endpoint=\"XXXX\",\r\n        azure_deployment=\"gpt-4o\",\r\n        openai_api_version=\"XXXX\",\r\n        openai_api_key=\"XXXX\",\r\n        temperature=0.1,\r\n        base_url=\"XXXX\",\r\n        api_key=\"XXXX\",\r\n    ),\r\n)\r\n```\r\n\r\n\r\nWhen i Run this code :\r\n**TypeError: azure_openai_complete() missing 1 required positional argument: 'prompt'**\r\n\r\nIs there a default prompt to use \r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/268/comments",
    "author": "vigneshmj1997",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:29:48Z",
        "body": "You can follow this demo `examples/lightrag_azure_openai_demo.py`"
      }
    ]
  },
  {
    "number": 267,
    "title": "Add user_id based insertion",
    "created_at": "2024-11-13T06:45:25Z",
    "closed_at": "2025-02-17T10:56:37Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/267",
    "body": "With the current implementation it's slightly difficult to use LightRag in organizational setting where global documents needs to be restricted within teams and local documents could just be for specific users.\r\n\r\nI haven't checked out the code thoroughly , but how easy/difficult would this be?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/267/comments",
    "author": "Tejaswgupta",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T08:25:46Z",
        "body": "This is indeed an interesting feature. I personally believe it wouldn‚Äôt be very complex to implement. You could simply build separate graphs for different documents and determine the working path before executing the query. However, there might be some additional considerations, such as encryption or access control, to ensure proper security."
      },
      {
        "user": "Tejaswgupta",
        "created_at": "2024-12-25T13:29:24Z",
        "body": "@LarFii would it possible for you to provide some direction to implement this. At as now , we're not concerned about access control, security etc as it's going to be used in our closed environment."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:56:37Z",
        "body": "Thanks, close (old issue)."
      }
    ]
  },
  {
    "number": 266,
    "title": "fix hf embedding to support loading to different device",
    "created_at": "2024-11-13T06:22:17Z",
    "closed_at": "2024-11-13T23:50:52Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/266",
    "body": "as title. currently local hf model embed func doesn't take care of loading model to cuda. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/266/comments",
    "author": "davidleon",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-13T23:51:05Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 265,
    "title": "ËØ∑ÈóÆÂêéÁª≠‰ºöÊúâÊ†πÊçÆÊñá‰ª∂ÂêçÂà†Èô§Êñá‰ª∂‰∏≠ÊâÄÂê´ÊâÄÊúâËäÇÁÇπÁöÑÂäüËÉΩÂêó",
    "created_at": "2024-11-13T02:26:50Z",
    "closed_at": "2024-11-15T16:13:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/265",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/265/comments",
    "author": "small122",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-13T05:09:43Z",
        "body": "ËÉΩÊõ¥ÂÖ∑‰ΩìÂú∞‰∏æ‰∏™‰æãÂ≠êÂêó"
      },
      {
        "user": "small122",
        "created_at": "2024-11-13T06:36:25Z",
        "body": "> ËÉΩÊõ¥ÂÖ∑‰ΩìÂú∞‰∏æ‰∏™‰æãÂ≠êÂêó\r\n\r\nÊÑüË∞¢ÂõûÂ§çÔºåÊúÄËøëÊõ¥Êñ∞ÁöÑÊòØÂà†Èô§Âçï‰∏™ËäÇÁÇπÁöÑÂäüËÉΩÔºåÊàëinsert‰∏Ä‰∏™file‰ºö‰∫ßÁîüÂæàÂ§öËäÇÁÇπÔºåÈÇ£‰πàÊàëËÉΩÈÄöËøáÊØîÂ¶ÇfilenameËøôÊ†∑ÁöÑÂèÇÊï∞Êù•Âà†Èô§ÊâÄÊúâËøô‰∏™file‰∫ßÁîüÁöÑËäÇÁÇπÂêó"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-15T16:13:08Z",
        "body": "ÂêéÁª≠Êàë‰ºöÂä†‰∏äËøô‰∏™ÂäüËÉΩ"
      }
    ]
  },
  {
    "number": 263,
    "title": "Only update storage if there was something to insert",
    "created_at": "2024-11-12T16:31:39Z",
    "closed_at": "2024-11-13T05:11:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/263",
    "body": "Before, the `finally` block would always call `_insert_done()`, which writes out the `vdb_*` and `kv_store_*` files ... even if there was nothing to insert (because all docs had already been inserted).  This was causing the speed of skippable inserts to become very slow as the graph grew.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/263/comments",
    "author": "detaos",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-13T05:11:29Z",
        "body": "Thanks, you're right."
      }
    ]
  },
  {
    "number": 262,
    "title": "LightRAG indexing causing problematic symbols that don't conform to UTF-8 encoding standards",
    "created_at": "2024-11-12T09:45:55Z",
    "closed_at": "2025-02-17T10:56:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/262",
    "body": "Perhaps it's just me, but I think the team needs to investigate how LightRAG handles and generates its indices, especially around character encoding. I can't get around this UTF-8 encoding error. I managed to pin this to a character at index 232 €¨ with a byte value 0x6ec. I couldn't remove or replace this specific character.\r\n\r\nDEBUG:root:Context segment around index 232: [1453, 7879, 27, 91, 29, 7879, 6259\r\n, 7879, 27, 91, 29, 7879, 50, 15918, 311, 769, 575, 1453, 318, 262, 1772, 286, 2\r\n57, 1492, 3706, 705, 32, 15120, 286, 16156, 6, 7256, 284, 607, 2560, 90, 201, 19\r\n8, 220, 366]\r\nDEBUG:root:Segment around problematic index: [1453, 7879, 27, 91, 29, 7879, 6259\r\n, 7879, 27, 91, 29, 7879, 50, 15918, 311, 769, 575, 1453, 318, 262, 1772, 286, 2\r\n57, 1492, 3706, 705, 32, 15120, 286, 16156, 6, 7256, 284, 607, 2560, 90, 201, 19\r\n8, 220, 366]\r\nINFO:root:Character at local index 0: ÷≠ (byte: 0x5ad)\r\nINFO:root:Character at local index 1: ·ªá (byte: 0x1ec7)\r\nINFO:root:Character at local index 2: ‚Üê (byte: 0x1b)\r\nINFO:root:Character at local index 3: [ (byte: 0x5b)\r\nINFO:root:Character at local index 4: ‚Üî (byte: 0x1d)\r\nINFO:root:Character at local index 5: ·ªá (byte: 0x1ec7)\r\nINFO:root:Character at local index 6: ·°≥ (byte: 0x1873)\r\nINFO:root:Character at local index 7: ·ªá (byte: 0x1ec7)\r\nINFO:root:Character at local index 8: ‚Üê (byte: 0x1b)\r\nINFO:root:Character at local index 9: [ (byte: 0x5b)\r\nINFO:root:Character at local index 10: ‚Üî (byte: 0x1d)\r\nINFO:root:Character at local index 11: ·ªá (byte: 0x1ec7)\r\nINFO:root:Character at local index 12: 2 (byte: 0x32)\r\nINFO:root:Character at local index 13: „∏Æ (byte: 0x3e2e)\r\nINFO:root:Character at local index 14: ƒ∑ (byte: 0x137)\r\nINFO:root:Character at local index 15: ÃÅ (byte: 0x301)\r\nINFO:root:Character at local index 16: »ø (byte: 0x23f)\r\nINFO:root:Character at local index 17: ÷≠ (byte: 0x5ad)\r\nINFO:root:Character at local index 18: ƒæ (byte: 0x13e)\r\nINFO:root:Character at local index 19: ƒÜ (byte: 0x106)\r\nINFO:root:Character at local index 20: €¨ (byte: 0x6ec)\r\nINFO:root:Character at local index 21: ƒû (byte: 0x11e)\r\nINFO:root:Character at local index 22: ƒÅ (byte: 0x101)\r\nINFO:root:Character at local index 23: ◊î (byte: 0x5d4)\r\nINFO:root:Character at local index 24: ‡π∫ (byte: 0xe7a)\r\nINFO:root:Character at local index 25: ÀÅ (byte: 0x2c1)\r\nINFO:root:Character at local index 26:   (byte: 0x20)\r\nINFO:root:Character at local index 27: „¨ê (byte: 0x3b10)\r\nINFO:root:Character at local index 28: ƒû (byte: 0x11e)\r\nINFO:root:Character at local index 29: „ºú (byte: 0x3f1c)\r\nINFO:root:Character at local index 30: ‚ô† (byte: 0x6)\r\nINFO:root:Character at local index 31: ·±ò (byte: 0x1c58)\r\nINFO:root:Character at local index 32: ƒú (byte: 0x11c)\r\nINFO:root:Character at local index 33: …ü (byte: 0x25f)\r\nINFO:root:Character at local index 34: ‡®Ä (byte: 0xa00)\r\nINFO:root:Character at local index 35: Z (byte: 0x5a)\r\nINFO:root:Character at local index 36: √â (byte: 0xc9)\r\nINFO:root:Character at local index 37: √Ü (byte: 0xc6)\r\nINFO:root:Character at local index 38: √ú (byte: 0xdc)\r\nINFO:root:Character at local index 39: ≈Æ (byte: 0x16e)\r\nThe log output provides a segment of the context around index 232 and the corresponding characters and their byte values. The problematic symbols aren't part of the original text but are introduced during indexing.\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/262/comments",
    "author": "daamazonbird",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-19T07:24:11Z",
        "body": "Could you provide more details about the context or specific setup where this issue arises?  Is there a way to reliably reproduce the issue? This seems unusual, and providing more detailed information could help identify the root cause and propose a solution."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:56:06Z",
        "body": "Thanks, close (old issue)."
      }
    ]
  },
  {
    "number": 260,
    "title": "Whether the complete 'results.json' of the all dataset & all Methods (NaiveRAG, RQ-RAG, HyDE, GraphRAG, LightRAG) can be provided for reproduction?",
    "created_at": "2024-11-12T07:59:51Z",
    "closed_at": "2024-11-18T07:40:20Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/260",
    "body": "I am in the process of reproducing the experiments from the paper and have successfully completed Step 1, 2, and 3, obtaining the query_result data pairs. \r\n\r\nHowever, I am currently stuck as I do not have the answers generated by other methods, which are required for the batch_eval (it needs two sets of answers).\r\n\r\n```\r\n{\r\n    \"query\": \"What role does community-supported agriculture play in supporting small farms?\",\r\n    \"result\": \"### Community-Supported Agriculture (CSA) and Its Role in Supporting Small Farms\\n\\nCommunity-Supported Agriculture (CSA) is a  that directly connects consumers with local farmers by enabling them to purchase shares of a farm‚Äôs harvest at the beginning of the growing season. This arrangement provides farmers with upfront capital, which can be crucial for purchasing seeds, fertilizers, and other necessary supplies. In turn, subscribers receive regular deliveries of fresh produce throughout the growing season.\\n\\n#### Financial Support\\n\\nOne of the primary benefits of CSA programs for small farms is financial stability. By securing a portion of their income before the growing season begins, farmers can better manage their finances and reduce the risk associated with farming. This is particularly important for small-scale operations, which often operate on tight margins and may struggle with unpredictable weather conditions or market fluctuations.\\n\\n#### Direct Market Access\\n\\nCSA also offers small farms direct access to consumers, bypassing intermediaries such as wholesalers and retailers. This direct connection allows farmers to maintain higher profit margins and ensures that they receive a fair price for their products. Additionally, it enables farmers to build personal relationships with their customers, fostering a sense of community and mutual support.\\n\\n#### Educational Opportunities\\n\\nThrough CSA programs, farmers have the opportunity to educate their members about sustainable agricultural practices, seasonal eating, and the importance of supporting local food systems. These educational efforts can lead to increased consumer awareness and appreciation for locally grown produce, further encouraging community involvement and support.\\n\\n#### Challenges and Limitations\\n\\nWhile CSA can provide significant benefits, it is not without its challenges. For instance, managing a CSA program requires additional time and resources from farmers, including administrative tasks such as marketing, record-keeping, and customer service. Moreover, there can be logistical issues related to distribution and storage of perishable goods.\\n\\n### Conclusion\\n\\nIn summary, Community-Supported Agriculture plays a vital role in supporting small farms by providing financial stability, direct market access, and opportunities for education and community engagement. However, it is essential to acknowledge the challenges involved in managing such programs to ensure their long-term sustainability and success.\\n\\nGiven the input data tables provided do not contain specific details about CSAs or small farms, this overview is based on general knowledge about the topic rather than specific data points.\"\r\n}\r\n```\r\n\r\nThis has led me to wonder if it is possible to use the {cls}_result.json files from Google Drive or Baidu Cloud, which contain results for the methods mentioned in the paper (NaiveRAG, RQ-RAG, HyDE, GraphRAG, LightRAG) across the all datasets (Agriculture, CS, Legal, Mix). \r\n\r\nThis would significantly **enhance the reliability of the experiments and allow anyone to reproduce all the experimental data in Performance Table** using the batch_eval.\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/260/comments",
    "author": "CooFlow",
    "comments": [
      {
        "user": "wizard1203",
        "created_at": "2024-11-14T15:06:51Z",
        "body": "Agree, providing the experiment scripts and files can help reproduce."
      }
    ]
  },
  {
    "number": 259,
    "title": "Hey, I've been wondering, aside from setting top_k, max_token_for_text_unit, and max_token_for_global_context, what other ways can I truncate the context window to make sure my request doesn't exceed the maximum context length? Any tips?",
    "created_at": "2024-11-12T07:01:01Z",
    "closed_at": "2025-02-17T10:55:49Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/259",
    "body": "\r\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 32768 tokens. However, you requested 34273 tokens in the messages, Please reduce the length of the messages.\", 'type': 'BadRequestError', 'param': None, 'code': 400}",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/259/comments",
    "author": "DragonLsy",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:55:49Z",
        "body": "Thanks, close (old issue)."
      },
      {
        "user": "YanSte",
        "created_at": "2025-02-17T10:55:53Z",
        "body": "Thanks, close (old issue)."
      }
    ]
  },
  {
    "number": 257,
    "title": "Add Oracle database as all type of storage (KV/vector/graph)",
    "created_at": "2024-11-12T02:17:51Z",
    "closed_at": "2024-11-12T07:36:11Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/257",
    "body": "This PR introduces **Oracle Database** support to the LightRAG project, covering **key-value (KV), vector, and graph storage.** \r\n\r\nThe integration leverages Oracle's powerful multi-model query capabilities, allowing users to work seamlessly with relational, JSON, and graph data types. In particular, Oracle 23c's vector support and enhanced JSON and graph querying bring significant flexibility and performance improvements to LightRAG, making it easier to integrate in enterprise environments that rely on Oracle infrastructure.\r\n\r\n## Contributor\r\n@jin38324 @tmuife\r\n\r\n## Key Features\r\n- **Unified Knowledge Base Management**: All documents are stored in a single table, with the `WORKSPACE` field distinguishing between different knowledge bases. This eliminates the need to create separate tables for each knowledge base, simplifying data management.\r\n\r\n- **Key-Value Storage Integration**: LightRAG now supports Oracle Database as its key-value storage backend. Oracle has efficient JSON data handling, but here we use relational tables  to store KV data, with documents in the `LIGHTRAG_DOC_FULL` table and chunks in the `LIGHTRAG_DOC_CHUNKS` table, simplifying the structure and facilitating quick access.\r\n\r\n- **Vector Storage Integration**: Leveraging Oracle 23ai‚Äôs vector capabilities, LightRAG enables high-performance vectorized text retrieval, supporting similarity searches for more precise, context-aware answers. Vectors are stored directly in table columns, without requiring separate databases or tables, enhancing ease of use and efficiency.\r\n\r\n- **Graph Storage Integration**: Oracle's advanced graph storage enables LightRAG to manage complex entity relationships, supporting richer, more contextually relevant responses through its two-tier retrieval framework. Entity nodes and edges are stored in `LIGHTRAG_GRAPH_NODES` and `LIGHTRAG_GRAPH_EDGES` tables, and graph data can be queried directly with SQL through a GRAPH view.\r\n\r\n## Importance of This Change\r\nIntegrating Oracle Database aligns with LightRAG's objective of expanding RAG systems to handle complex data structures and interdependencies. Oracle's robust data management features ensure that LightRAG maintains high performance, flexibility, and security, even at scale.\r\n\r\n## Additional Information\r\n**Backward Compatibility**: This enhancement is fully compatible with existing storage setups, allowing Oracle to be added as an optional backend without impacting current configurations.\r\n\r\n**Testing**: Extensive tests have been implemented to validate Oracle‚Äôs performance and functionality as a KV, vector, and graph storage backend across diverse retrieval scenarios.\r\n\r\n**Example**: Sample code demonstrating the use of Oracle Database as a storage solution is provided in the examples.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/257/comments",
    "author": "jin38324",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-12T03:02:23Z",
        "body": "Thanks for your excellent contribution, but there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass."
      },
      {
        "user": "jin38324",
        "created_at": "2024-11-12T05:40:03Z",
        "body": "> Thanks for your excellent contribution, but there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass.\r\n\r\npre commit have fixed."
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-12T07:36:35Z",
        "body": "Thanks. I have merged it."
      }
    ]
  },
  {
    "number": 255,
    "title": "Unstable context on each start",
    "created_at": "2024-11-11T14:52:30Z",
    "closed_at": "2024-11-19T07:24:37Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/255",
    "body": "## Issue Description\r\nI created LightRAG using OpenAI. I am able to query and retrieve context. However I realized that context is unstable (still relevant to query) for same query on each start of LightRAG within my app. It slightly changes (eg. single relationship or single entity) with exact same query params.\r\nI used dunzhang/stella_en_400M_v5 embedding model during creation and querying.\r\nNote: My app also uses async in server level.\r\nNote: LLM params for hl and ll keyword extraction is constant and I checked keywords on different app starts and keywords are constant for same query.\r\n\r\n## Environment\r\npypandoc==1.13\r\nboto3==1.35.32\r\npydantic_core==2.23.4\r\npydantic==2.9.2\r\npasslib==1.7.4\r\nnumpy==1.26.4\r\npandas==2.2.1\r\npython_dateutil==2.8.2\r\npytz==2024.1\r\nRequests==2.31.0\r\ntext_generation==0.6.1\r\nfaiss-cpu==1.7.4\r\nsutime==1.0.1\r\nfuzzywuzzy==0.18.0\r\ntransformers==4.37.2\r\nhaversine\r\npyarrow\r\ncacheout\r\ntermcolor\r\nscikit-learn\r\nregex\r\nnltk\r\nlightrag-hku==0.0.8\r\naioboto3==13.2.0\r\nollama==0.3.3\r\nnano-vectordb==0.0.4.1\r\nopenai\r\nopenai>=0.27.0\r\nneo4j>=5.7.0\r\npybind11>=2.10.0\r\ntorch>=1.13.1\r\ntiktoken>=0.3.0\r\nnetworkx>=3.0\r\nscipy>=1.10.1\r\nspacy>=3.5.2\r\npy2neo>=2021.2.3\r\nnest-asyncio>=1.5.6\r\n\r\n## LightRAG Settings:\r\n```python\r\nparams = QueryParam(\r\n    mode=\"hybrid\", \r\n    only_need_context=True, \r\n    top_k=3, \r\n    max_token_for_text_unit=800,\r\n    max_token_for_global_context=400,\r\n    max_token_for_local_context=400\r\n)\r\n\r\nnode2vec_params = {\r\n    'dimensions': 1024, \r\n    'num_walks': 10, \r\n    'walk_length': 40, \r\n    'window_size': 2, \r\n    'iterations': 3, \r\n    'random_seed': 3\r\n}\r\n\r\nLightRAG(\r\n      working_dir=working_dir,\r\n      llm_model_func=hf_model_complete,\r\n      llm_model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\r\n      llm_model_max_token_size=4500,\r\n      node2vec_params=node2vec_params,\r\n      embedding_func=EmbeddingFunc(\r\n          embedding_dim=1024,\r\n          max_token_size=8192,\r\n          func=lambda texts: embedding_func(texts),\r\n      )\r\n)\r\n```\r\n\r\nAny help would be appreciated",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/255/comments",
    "author": "Burakabdi",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-15T16:08:34Z",
        "body": "I have fixed the bug. You can download the latest code and give it a try (no need to re-insert)."
      }
    ]
  },
  {
    "number": 249,
    "title": "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
    "created_at": "2024-11-11T08:49:13Z",
    "closed_at": "2024-11-19T07:19:54Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/249",
    "body": "I am using a huggingface demo, but with a local model.How to deal with it?please help me , thank you very much!!\r\n**code**:\r\nimport os\r\n\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import hf_model_complete, hf_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,\r\n    llm_model_name=\"/data/Qwen2.5-14B-Instruct\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1024,\r\n        max_token_size=5000,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\r\n                r\"/data/project/raag/bge-large-zh-v1.5/\", model_max_length=512\r\n            ),\r\n            embed_model=AutoModel.from_pretrained(\r\n                r\"/data/project/raag/bge-large-zh-v1.5/\"\r\n            ),\r\n        ),\r\n    ),\r\n)\r\n\r\nwith open(r\"/data/project/raag/caiwu.txt\", \"r\", encoding=\"utf-8\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)\r\nerror:\r\nroot@c15e0721d1a6:~# CUDA_VISIBLE_DEVICES=0  python /data/project/raag/light_rag.py\r\nINFO:lightrag:Logger initialized for working directory: /data/project/raag/dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = /data/project/raag/dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 1024, 'max_token_size': 5000, 'func': <function <lambda> at 0x7f5eaa0bfd90>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function hf_model_complete at 0x7f5dc1cd6b00>,\r\n  llm_model_name = /data/Qwen2.5-14B-Instruct,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x7f5dc1cbfeb0>\r\n\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:lightrag:Loaded graph from /data/project/raag/dickens/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\r\nINFO:nano-vectordb:Load (0, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Load (2, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_chunks.json'} 2 data\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 2 chunks\r\nINFO:lightrag:Inserting 2 vectors to chunks\r\nINFO:lightrag:[Entity Extraction]...\r\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\r\n/usr/local/lib/python3.10/site-packages/accelerate/utils/modeling.py:1390: UserWarning: Current model requires 12544 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\r\n  warnings.warn(\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07<00:00,  1.00it/s]\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"/data/project/raag/light_rag.py\", line 33, in <module>\r\n    rag.insert(f.read())\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/lightrag.py\", line 164, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/lightrag.py\", line 211, in ainsert\r\n    maybe_new_kg = await extract_entities(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/operate.py\", line 331, in extract_entities\r\n    results = await asyncio.gather(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/operate.py\", line 270, in _process_single_content\r\n    final_result = await use_llm_func(hint_prompt)\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/utils.py\", line 87, in wait_func\r\n    result = await func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/llm.py\", line 377, in hf_model_complete\r\n    return await hf_model_if_cache(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/llm.py\", line 286, in hf_model_if_cache\r\n    output = hf_model.generate(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2215, in generate\r\n    result = self._sample(\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3206, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1164, in forward\r\n    outputs = self.model(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 854, in forward\r\n    inputs_embeds = self.embed_tokens(input_ids)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/functional.py\", line 2551, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/249/comments",
    "author": "Z-oo883",
    "comments": [
      {
        "user": "CooFlow",
        "created_at": "2024-11-11T12:49:00Z",
        "body": "I solved it by change the code in llm.py\r\n\r\n`device_map='auto'` to `device_map=None`\r\n\r\n```\r\n@lru_cache(maxsize=1)\r\ndef initialize_hf_model(model_name):\r\n    hf_tokenizer = AutoTokenizer.from_pretrained(\r\n        model_name, device_map=None, trust_remote_code=True\r\n    )\r\n    hf_model = AutoModelForCausalLM.from_pretrained(\r\n        model_name, device_map=None, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\r\n    if hf_tokenizer.pad_token is None:\r\n        hf_tokenizer.pad_token = hf_tokenizer.eos_token\r\n\r\n    return hf_model, hf_tokenizer\r\n```"
      },
      {
        "user": "Z-oo883",
        "created_at": "2024-11-12T02:31:02Z",
        "body": "> torch_dtype=torch.bfloat16).cuda()\r\n\r\nthank you very much!"
      }
    ]
  },
  {
    "number": 245,
    "title": "üòï TypeError: LightRAG.__init__() got an unexpected keyword argument 'llm_model_kwargs'",
    "created_at": "2024-11-10T16:11:04Z",
    "closed_at": "2025-01-23T07:03:11Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/245",
    "body": "`python lightrag_ollama_demo.py \r\nTraceback (most recent call last):\r\n  File \"/LightRAG/examples/lightrag_ollama_demo.py\", line 14, in <module>\r\n    rag = LightRAG(\r\n          ^^^^^^^^^\r\nTypeError: LightRAG.__init__() got an unexpected keyword argument 'llm_model_kwargs'`\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/245/comments",
    "author": "mchzimm",
    "comments": [
      {
        "user": "hfyeomans",
        "created_at": "2024-11-12T00:19:39Z",
        "body": "If you're getting this its because you installed it via normal pip. Install it the recommended way via pip install -e . in the repo directory and this goes away."
      }
    ]
  },
  {
    "number": 244,
    "title": "laleEmor; al te irat arav irersirs exeot for te contention axis must atch exactly, but alono dimansin l. the anavat inex0 has size 1536 ad te anrav at iex l hes sie 768",
    "created_at": "2024-11-10T13:46:40Z",
    "closed_at": "2025-01-23T07:03:00Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/244",
    "body": "‰Ω†Â•ΩÔºåËøòÊòØÊúâ‰ª•‰∏äÊä•Èîô„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/244/comments",
    "author": "hjing100",
    "comments": [
      {
        "user": "hjing100",
        "created_at": "2024-11-10T13:48:00Z",
        "body": "INF0 :light rag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\nFile \"/data/projects/rag/LightRAG-main/test01/example08 lightrag.py\", line 85, in {<}module{>}\r\nrag.insert (f .read( ))\r\nFile \"/data/projects/rag/LightRAG-main/light rag/lightrag .py\", line 167, in insert\r\nretum loop.mn until complete(self.ainsert(st ring or st rings) )\r\nFile \"/data/anaccnda3/envs/test39/lib/pythan3.9/asyncio/base events.py\", line 642, in run ntil complete\r\nretum future. result()\r\nFile \"/data/projects/rag/ightRAG-main/light rag/lightrag .py\", line 211, in ainsert\r\nawait self.chunks vdb.upsert (inserting chunks)\r\nFile \"/data/projects/rag/ightRAG-main/light rag/storage.py\", line 100, in upsert\r\nresults =self. client .upsert(datas-list data)\r\n**File \"/data/anaconda3/envs/test39/lib/pythan3.9/site-packages/nano vectordb/dbs.py\", line 98, in upsert\r\nself. storage[\"matrix\"l= np.vstack([self. storage[\"matrix\"l, new matrix])**\r\nFile \"/data/anacanda3/envs/test39/lib/pythan3.9/site-packages/numpy/core/shape base.py\", line 289, in vstack\r\nretum nx.concatenate(arrs, 0, dtype dtype, casting=casting)\r\nValueError: all the input arrey dimensions except for the concatenation axis must match exactly, but along dimension 1, the arrey at index 0 has size 1536 and the array at index 1 has size 768"
      },
      {
        "user": "hjing100",
        "created_at": "2024-11-10T13:55:36Z",
        "body": "ÈöæÈÅìÊòØÂõ†‰∏∫utils.pyÂÅö‰∫Ü‰ª•‰∏ãÊîπÂä®ÔºöENCODER = AutoTokenizer.from_pretrained(\r\n\"/data/qwen2-72b-instruct\", device_map=\"auto\", trust_remote_code=True\r\n)\r\n\r\ndef encode_string_by_tiktoken(content: str, model_name: str = \"gpt-4o\"):\r\n    global ENCODER\r\n\r\n    model_inputs = ENCODER(content)\r\n    input_ids = model_inputs.input_ids\r\n    tokens = input_ids\r\n\r\n    return tokens\r\n\r\n\r\ndef decode_tokens_by_tiktoken(tokens: list[int], model_name: str = \"gpt-4o\"):\r\n    global ENCODER\r\n\r\n    tokens = torch.IntTensor(tokens)\r\n    content = ENCODER.decode(tokens)\r\n    return content\r\n\r\nËøêË°å‰ª£Á†ÅÊòØÔºö\r\nimport os\r\nimport numpy as np\r\nfrom lightrag import LightRAG, QueryParam\r\n\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom lightrag.llm import hf_model_complete, hf_embedding\r\nfrom transformers import AutoModel, AutoTokenizer\r\nimport time\r\n\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation\r\n    llm_model_name='/data/qwen2-72b-instruct',  # Model name from Hugging Face\r\n\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1536,\r\n        max_token_size=8192,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\"/data/bce-embedding-base_v1\"),\r\n            embed_model=AutoModel.from_pretrained(\"/data/bce-embedding-base_v1\")\r\n        )\r\n    ),\r\n)\r\n\r\nwith open(r\"D:\\code\\LightRAG-main\\book.txt\",\"r\",encoding=\"utf-8\") as f:\r\n    rag.insert(f.read())\r\n\r\n\r\nquery_ = \"ËøôÁØáÊñáÁ´†‰∏ªË¶ÅËÆ≤‰∫Ü‰∫õ‰ªÄ‰πàÔºü\"\r\nprint(rag.query(query_, param=QueryParam(mode=\"global\")))\r\nprint(\"*******************\")\r\ntime.sleep(2)\r\n\r\n"
      },
      {
        "user": "Aniwine",
        "created_at": "2024-11-10T15:07:06Z",
        "body": "use another embedding model,like nomic-embed-text,don't forget mkdir a new workdir ,it works for me"
      },
      {
        "user": "hjing100",
        "created_at": "2024-11-11T02:28:46Z",
        "body": "\r\n\r\n> use another embedding model,like nomic-embed-text,don't forget mkdir a new workdir ,it works for me\r\n\r\nThank you, and it's not work for me.\r\nbce-embedding-base_v1 dimensions is 768, so chang:\r\nlightrag.py\r\nnode2vec_params: dict = field(\r\n        default_factory=lambda: {\r\n            \"dimensions\": 768,  # 1536,\r\n            \"num_walks\": 10,\r\n            \"walk_length\": 40,\r\n            \"window_size\": 2,\r\n            \"iterations\": 3,\r\n            \"random_seed\": 3,\r\n        }\r\n    )\r\n\r\nlightrag_hf_demo.py\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,  \r\n    llm_model_name='/data/qwen2-72b-instruct',  \r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=768,  # 1536,\r\n        max_token_size=8192,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\"/data/bce-embedding-base_v1\"),\r\n            embed_model=AutoModel.from_pretrained(\"/data/bce-embedding-base_v1\")\r\n        )\r\n    ),\r\n)"
      },
      {
        "user": "hjing100",
        "created_at": "2024-11-11T02:48:06Z",
        "body": "have another question:\r\n224-11-11 10:37:23.711056: W extemal/acal xla/xla/tsl/ib/manitoring/collecticn registry.cc:88] Trying to register 2 metrics with the same rame\r\negister a new ane. Please check if you link the metric more than once, or if the name is already used by other metrics.\r\nINF0 :light rag:Writing graph with 0 nodes, 0 edges\r\n/tensorflow/api/tf function. The old value will be erased\r\nraceback(most recent call last):\r\nFile \"/data/projects/rag/LightRAG-main/test02/example08_lightrag.py\", line 85, in {<}module{>}\r\nrag. insert ( f . read( ))\r\nFile \"/data/projects/rag/ightRAG-main/light raglightrag .py\", line 167, in insert\r\nretum loop.run until complete(self.ainsert(string or st rings) )\r\nFile \"/data/anacanda3/envs/test39/lib/pythan3.9/asyncio/base events.py\", line 642, in run until complete\r\nretum future. result()\r\nFile \"/data/projects/rag/LightRAG-main/light rag/lightrag .py\", line 214, in ainsert\r\nmaybe new kg = await extract entities(\r\nFile \"/data/projects/rag/LightRAG-main/light rag/cperate.py\", line 33l, in extract entities\r\nresults =await asycio.cather(\r\nin order to\r\nFile \"/data/projects/rag/LightRAG-main/l ight rag/cperate.py\", line 270, in _process single content\r\nfinal result = await use llm func(hint prompt)\r\nFile \"/data/projects/rag/LightRAG-main/light ragutils.py\", line 92, in wait func\r\nresult = await func(*args, :*.*kwargs)\r\nFile \"/data/projects/rag/LightRAG-main/lightrag/llm.py\", line 510, in hf_model_complete\r\nretum await hf_model_if_cache(\r\n**File \"/data/proiects/rag/LightRAG-main/lightrag/llm.py\", line 286, in hf_model_if_cache\r\noutput = hf_model.generate(**\r\nFile \"/data/anacanda3/envs/test39/lib/pythan3.9/site-packages/torchvutils/ cantextlib.py\", line 115, in decorate cantext\r\nretum func(*args, **kwargs)\r\n\r\n...\r\n\r\nFile \"/data/anacanda3/envs/test39/ib/pythan3.9/typing .py\", line 215,in remve dups flatten\r\nall params = set (params)\r\n**TypeError: urhashable type:'list'**"
      },
      {
        "user": "hjing100",
        "created_at": "2024-11-13T06:59:39Z",
        "body": "ÊéíÊü•Âá∫ÂéüÂõ†ÊòØÔºåÊòØÂõ†‰∏∫ÂÆâË£Ö‰∫ÜtensorflowÔºåÂç∏ËΩΩ‰∫ÜÂ∞±Â•Ω‰∫Ü„ÄÇ\r\nÂàöÂºÄÂßã‰ª•‰∏∫ÊòØÊàëÁöÑÊîπÂä®ÂØºËá¥‰ª£Á†ÅÈáåÂì™ÈáåÊúâÁÇπÈóÆÈ¢òÔºåÂêéÊù•ËøêË°åqwenÊé®ÁêÜÂèëÁé∞‰πüÊúâÂêåÊ†∑ÁöÑÈóÆÈ¢òÔºåÊâçÂèëÁé∞ÊòØimport tensorflowÂ∞±‰ºöÊä•ÊîπÈîô„ÄÇ\r\nËÄå‰∏îLightRAGÂ§ñÁΩëÁéØÂ¢ÉÊ≤°ÊúâtensorflowÂåÖÁöÑÈúÄË¶Å(tensorflowÂÆâË£ÖÊòØ‰πãÂâçÂà´ÁöÑÈ°πÁõÆÈúÄË¶Å):\r\n(LightRAG) C:\\Users\\lenovo>pip list\r\nPackage Version Editable project location\r\n\r\naccelerate 1.1.1\r\naioboto3 13.2.0\r\naiobotocore 2.15.2\r\naiofiles 24.1.0\r\naiohappyeyeballs 2.4.3\r\naiohttp 3.10.10\r\naioitertools 0.12.0\r\naiosignal 1.3.1\r\nannotated-types 0.7.0\r\nanyio 4.6.2.post1\r\nanytree 2.12.1\r\nasttokens 2.4.1\r\nasync-timeout 4.0.3\r\nattrs 24.2.0\r\nautograd 1.7.0\r\nbeartype 0.18.5\r\nboto3 1.35.36\r\nbotocore 1.35.36\r\ncertifi 2024.8.30\r\ncharset-normalizer 3.4.0\r\ncolorama 0.4.6\r\ncontourpy 1.3.0\r\ncycler 0.12.1\r\ndecorator 5.1.1\r\ndistro 1.9.0\r\nexceptiongroup 1.2.2\r\nexecuting 2.1.0\r\nfilelock 3.16.1\r\nfonttools 4.54.1\r\nfrozenlist 1.5.0\r\nfsspec 2024.10.0\r\ngensim 4.3.3\r\ngraspologic 3.4.1\r\ngraspologic-native 1.2.1\r\nh11 0.14.0\r\nhnswlib 0.8.0\r\nhttpcore 1.0.6\r\nhttpx 0.27.2\r\nhuggingface-hub 0.26.2\r\nhyppo 0.4.0\r\nidna 3.10\r\nimportlib_resources 6.4.5\r\nipython 8.18.1\r\njedi 0.19.1\r\nJinja2 3.1.4\r\njiter 0.7.0\r\njmespath 1.0.1\r\njoblib 1.4.2\r\njsonpickle 3.4.2\r\nkiwisolver 1.4.7\r\nlightrag-hku 0.0.8 d:\\code\\lightrag-main\r\nllvmlite 0.43.0\r\nMarkupSafe 3.0.2\r\nmatplotlib 3.9.2\r\nmatplotlib-inline 0.1.7\r\nmpmath 1.3.0\r\nmultidict 6.1.0\r\nnano-vectordb 0.0.4.1\r\nnetworkx 3.2.1\r\nnumba 0.60.0\r\nnumpy 1.26.4\r\nollama 0.3.3\r\nopenai 1.54.3\r\npackaging 24.1\r\npandas 2.2.3\r\nparso 0.8.4\r\npatsy 0.5.6\r\npillow 11.0.0\r\npip 24.2\r\nPOT 0.9.5\r\nprompt_toolkit 3.0.48\r\npropcache 0.2.0\r\npsutil 6.1.0\r\npure_eval 0.2.3\r\npydantic 2.9.2\r\npydantic_core 2.23.4\r\nPygments 2.18.0\r\npynndescent 0.5.13\r\npyparsing 3.2.0\r\npython-dateutil 2.9.0.post0\r\npytz 2024.2\r\npyvis 0.3.2\r\nPyYAML 6.0.2\r\nregex 2024.11.6\r\nrequests 2.32.3\r\ns3transfer 0.10.3\r\nsafetensors 0.4.5\r\nscikit-learn 1.5.2\r\nscipy 1.12.0\r\nseaborn 0.13.2\r\nsetuptools 75.1.0\r\nsix 1.16.0\r\nsmart-open 7.0.5\r\nsniffio 1.3.1\r\nstack-data 0.6.3\r\nstatsmodels 0.14.4\r\nsympy 1.13.1\r\ntenacity 9.0.0\r\nthreadpoolctl 3.5.0\r\ntiktoken 0.8.0\r\ntokenizers 0.20.3\r\ntorch 2.5.1\r\ntqdm 4.67.0\r\ntraitlets 5.14.3\r\ntransformers 4.46.2\r\ntyping_extensions 4.12.2\r\ntzdata 2024.2\r\numap-learn 0.5.7\r\nurllib3 1.26.20\r\nwcwidth 0.2.13\r\nwheel 0.44.0\r\nwrapt 1.16.0\r\nxxhash 3.5.0\r\nyarl 1.17.1\r\nzipp 3.20.2\r\n\r\n(LightRAG) C:\\Users\\lenovo>"
      }
    ]
  },
  {
    "number": 243,
    "title": "Missing or improperly registered torch::class_ error (lightrag_ollama)",
    "created_at": "2024-11-10T09:08:51Z",
    "closed_at": "2024-11-12T09:32:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/243",
    "body": "Hello. I got this message on Bash:\r\nExamining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\r\n\r\nI'm running lightrag_ollama using qwen2 (LLM) and nomic-embed-text (embedding). I've saved indexes of entities and relations created in a directory in the LightRAG main working directory. The plan is to use the LLM to query this generated indexes (without having to recreate the indexes). However, I've run into this roadblocked re path of torch.classes. I've looked everywhere for clues in this repo but couldn't find anything regarding the registration or path of torch.classes. \r\n\r\nHope someone can help!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/243/comments",
    "author": "daamazonbird",
    "comments": [
      {
        "user": "KevinZhuoGpt",
        "created_at": "2024-12-12T02:32:56Z",
        "body": "please share how to solve this issue. Thanks."
      }
    ]
  },
  {
    "number": 241,
    "title": "Added GUI (linked), updated README.md",
    "created_at": "2024-11-10T02:01:25Z",
    "closed_at": "2024-11-11T02:33:18Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/241",
    "body": "I am not sure streamlit is the interface I want to contribute back to the LightRAG project, but willing to share it all under the same MIT license. It seemed you did not like streamlit as well. I am working on taipy or flask next and can contribute that how you suggest.\r\n\r\nI figured I would at least share the link and source code in the README.\r\n\r\nI would like to understand how you benchmark and want to visualize the investigation/verification workflows for future version.\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/241/comments",
    "author": "aiproductguy",
    "comments": [
      {
        "user": "ASAD-BE18",
        "created_at": "2024-11-10T20:14:17Z",
        "body": "If you could, please add Ollama support in the streamlit application using API endpoints"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-11T02:33:10Z",
        "body": "Thanks for your GUI! I'm not very familiar with these visualization methods, so I don't have any specific suggestions. For the evaluation of LightRAG, we use a one-to-one approach similar to GraphRAG. We generate users and tasks for a specified dataset using GPT, then have different RAG systems answer the same query."
      }
    ]
  },
  {
    "number": 237,
    "title": "Regarding modifying LightRAG for multimodal tasks",
    "created_at": "2024-11-09T06:52:16Z",
    "closed_at": "2025-01-23T07:01:57Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/237",
    "body": "I am currently planning to prepend an image to the query section, meaning the query will consist of an image along with a question about it. The system will then search the provided documents to find the answer. My understanding is to first use GPT-4O or other multimodal large models to generate a descriptive caption for the image I want to use, and then concatenate it with the query intended for LightRAG retrieval (e.g., \"What's this?\") to form a new query, which will then produce the retrieval-based answer. Do you think this approach is reasonable?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/237/comments",
    "author": "SLKAlgs",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-12T03:33:41Z",
        "body": "This might require more detailed discussion. For multimodal tasks, the construction of the KG might need to differ. However, I think it‚Äôs also worth trying to convert all the images into text using a multimodal large model."
      }
    ]
  },
  {
    "number": 225,
    "title": "ËØ∑ÈóÆÊîØÊåÅËá™ÂÆö‰πâschemaÂêó",
    "created_at": "2024-11-07T07:46:32Z",
    "closed_at": "2024-11-19T07:25:02Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/225",
    "body": "ÊîØÊåÅËá™ÂÆö‰πâschemaÂêóÔºåÂ§ßÊ®°ÂûãÊäΩÂèñÁöÑÂÆû‰Ωì‰ª•ÂèäÂÖ≥Á≥ªÂæàÊùÇ‰π±ÔºåË¶ÅÊòØÊîØÊåÅËá™ÂÆö‰πâschemaÂ∞±Â•Ω‰∫Ü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/225/comments",
    "author": "he-boy",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-12T03:15:08Z",
        "body": "ÊöÇÊó∂ËøòÊ≤°ÊúâÊèê‰æõÊîØÊåÅÔºåÂêéÁª≠Êàë‰ª¨‰ºöËÄÉËôëÂä†ÂÖ•Ëøô‰∏™ÂäüËÉΩ"
      },
      {
        "user": "adamwuyu",
        "created_at": "2025-01-08T00:17:47Z",
        "body": "ËØ∑ÊîØÊåÅÊ≠§ÂäüËÉΩÔºåÈùûÂ∏∏ÊúüÂæÖÔºåÂèØ‰ª•ÂèÇËÄÉneo4j-graphrag-python"
      }
    ]
  },
  {
    "number": 219,
    "title": "RerankerÊòØÂê¶ÂèØÂä†",
    "created_at": "2024-11-06T08:30:01Z",
    "closed_at": "2025-01-23T06:59:16Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/219",
    "body": "Âú®EmbeddingÊ£ÄÁ¥¢ÂêéÔºåÊúâÊ≤°ÊúâÂøÖË¶ÅÂä†RerankerÊ®°Âûã‰ºòÂåñÊ£ÄÁ¥¢ÁªìÊûúÁöÑÊéíÂ∫èÔºå‰∏çÁü•Â§ß‰Ω¨ÊòØÂê¶ËØïËøáÊïàÊûú",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/219/comments",
    "author": "feihoaze",
    "comments": [
      {
        "user": "ardyli",
        "created_at": "2024-11-06T15:27:28Z",
        "body": "ÊòØÁöÑÊîØÊåÅ"
      },
      {
        "user": "feihoaze",
        "created_at": "2024-11-12T01:13:31Z",
        "body": "Â•ΩÁöÑÔºåË∞¢Ë∞¢ÔºÅ"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-12T03:12:05Z",
        "body": "ÂêéÁª≠Êàë‰ª¨‰ºöÊµãËØï‰∏Ä‰∏ãÂÖ∑‰ΩìÁöÑÊïàÊûú"
      },
      {
        "user": "feihoaze",
        "created_at": "2024-11-12T03:14:18Z",
        "body": "ÂóØÂóØÔºåÂ•ΩÁöÑ"
      }
    ]
  },
  {
    "number": 218,
    "title": "Regarding the reproduction of comparative experimental models.",
    "created_at": "2024-11-06T07:50:31Z",
    "closed_at": "2024-11-11T02:18:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/218",
    "body": "Dear developer, hello. I would like to reproduce the relevant comparison models that appeared in the experiment. How can I ensure that their conditions are the same? I have carefully reviewed the code in the repository and did not find any relevant comparison model code. What should I do and where can I find this code?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/218/comments",
    "author": "Idol-Dou2021",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:32:03Z",
        "body": "Each baseline has its own GitHub repository; we directly cloned the source code from those repositories."
      }
    ]
  },
  {
    "number": 216,
    "title": "Regarding the evaluation indicators section.",
    "created_at": "2024-11-06T05:38:33Z",
    "closed_at": "2024-11-11T02:17:48Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/216",
    "body": "Regarding the evaluation indicators section, how can I evaluate the relevant indicators without calling the batch_ eval. py in OpenAI? Is there any other way? Or other calling methods?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/216/comments",
    "author": "Idol-Dou2021",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:30:50Z",
        "body": "You only need to provide the evaluation questions generated in `batch_eval.py` to the LLM separately."
      }
    ]
  },
  {
    "number": 215,
    "title": "bug fix issue #95",
    "created_at": "2024-11-06T02:45:33Z",
    "closed_at": "2024-11-07T06:54:59Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/215",
    "body": "# LightRAG Bug Fix Report\r\n\r\n## Issue\r\nA TypeError was occurring in the hybrid query mode when trying to access content from text units that contained None values. The error specifically occurred in the `_find_most_related_text_unit_from_entities` function when trying to process text units for token size truncation.\r\n\r\n## Root Cause\r\nThe issue stemmed from insufficient null checks when processing text units in the knowledge graph. Specifically:\r\n\r\n1. Text unit data could be None when retrieved from text_chunks_db\r\n2. The data dictionary could be missing the 'content' field\r\n3. No proper filtering of invalid entries before token size truncation\r\n\r\nKey problematic area was in:\r\n\r\n591:597:LightRAG/lightrag/operate.py\r\n```\r\n    if any([v is None for v in all_text_units_lookup.values()]):\r\n        logger.warning(\"Text chunks are missing, maybe the storage is damaged\")\r\n    all_text_units = [\r\n        {\"id\": k, **v} for k, v in all_text_units_lookup.items() if v is not None\r\n    ]\r\n    all_text_units = sorted(\r\n        all_text_units, key=lambda x: (x[\"order\"], -x[\"relation_counts\"])\r\n\r\n```\r\n\r\n## Solution\r\nAdded comprehensive null checks and data validation throughout the text unit processing pipeline:\r\n\r\n1. Added null check for node data and source_id field:\r\n\r\n\r\n571:575:LightRAG/lightrag/operate.py\r\n```\r\n        for k, v in zip(all_one_hop_nodes, all_one_hop_nodes_data)\r\n        if v is not None\r\n    }\r\n    all_text_units_lookup = {}\r\n    for index, (this_text_units, this_edges) in enumerate(zip(text_units, edges)):\r\n\r\n```\r\n\r\n\r\n2. Added content validation when getting chunk data:\r\n\r\n591:597:LightRAG/lightrag/operate.py\r\n```\r\n    if any([v is None for v in all_text_units_lookup.values()]):\r\n        logger.warning(\"Text chunks are missing, maybe the storage is damaged\")\r\n    all_text_units = [\r\n        {\"id\": k, **v} for k, v in all_text_units_lookup.items() if v is not None\r\n    ]\r\n    all_text_units = sorted(\r\n        all_text_units, key=lambda x: (x[\"order\"], -x[\"relation_counts\"])\r\n\r\n```\r\n\r\n3. Added comprehensive filtering for None values:\r\n\r\n599:604:LightRAG/lightrag/operate.py\r\n```\r\n    all_text_units = truncate_list_by_token_size(\r\n        all_text_units,\r\n        key=lambda x: x[\"data\"][\"content\"],\r\n        max_token_size=query_param.max_token_for_text_unit,\r\n    )\r\n    all_text_units: list[TextChunkSchema] = [t[\"data\"] for t in all_text_units]\r\n\r\n```\r\n\r\n\r\n\r\n\r\nThe changes are backward compatible and require no modifications to the existing API or data structures.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/215/comments",
    "author": "benx13",
    "comments": [
      {
        "user": "benx13",
        "created_at": "2024-11-06T02:46:41Z",
        "body": "Fixes #95 "
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-07T06:55:20Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 213,
    "title": "There ia a typo in the File README.md.",
    "created_at": "2024-11-05T13:03:04Z",
    "closed_at": "2024-11-07T07:26:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/213",
    "body": "There maybe a typo in the fill README.md of the section \"Multi-file Type Support\". Is the \"testract\" right or not? I think that should be \"textract\", since that is \"textract\" in the following code. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/213/comments",
    "author": "yycsn",
    "comments": [
      {
        "user": "yycsn",
        "created_at": "2024-11-05T15:23:05Z",
        "body": "All right."
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:26:10Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 211,
    "title": "Function enhancement",
    "created_at": "2024-11-05T08:51:37Z",
    "closed_at": "2024-11-07T06:47:59Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/211",
    "body": "More powerful Fastapi demo\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/211/comments",
    "author": "monk-after-90s",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T06:47:55Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 208,
    "title": "I want to directly use the locally built knowledge graph for subsequent RAG",
    "created_at": "2024-11-05T05:24:21Z",
    "closed_at": "2025-01-23T06:58:40Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/208",
    "body": "I want to directly use the locally built knowledge graph for subsequent RAG. How can I do this? This involves the needs of vertical fields. It is difficult for general large models to build a satisfactory knowledge graph.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/208/comments",
    "author": "cdcstu",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:20:59Z",
        "body": "That‚Äôs definitely a useful feature, as LLM-generated results are often unsatisfactory. For now, you can try building based on the existing graph structure. We will also provide a method for inserting based on a local KG in the future, though this may take some time."
      }
    ]
  },
  {
    "number": 205,
    "title": "enabling history for a series of questions and answers",
    "created_at": "2024-11-04T15:30:10Z",
    "closed_at": "2024-11-05T15:33:19Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/205",
    "body": "hi there\r\n\r\nIf I want to use LightRAG to build a bot for questioning and answering, how can i enable history that best fit with this repo?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/205/comments",
    "author": "amirsa66",
    "comments": [
      {
        "user": "ziudeso",
        "created_at": "2024-11-10T15:45:06Z",
        "body": "@amirsa66 did you manage to understand how to integrate the history?"
      },
      {
        "user": "amirsa66",
        "created_at": "2024-11-10T16:43:12Z",
        "body": "> @amirsa66 did you manage to understand how to integrate the history?\r\n\r\nYes, it's OK "
      },
      {
        "user": "ziudeso",
        "created_at": "2024-11-11T15:43:39Z",
        "body": "@amirsa66 I'm actually asking how did you do that? =D \r\n"
      },
      {
        "user": "amirsa66",
        "created_at": "2024-11-11T17:19:31Z",
        "body": "> @amirsa66 I'm actually asking how did you do that? =D\r\n\r\n@ziudeso I did it on Streamlit:\r\n\r\n```\r\nimport os\r\nimport streamlit as st\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import gpt_4o_mini_complete\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\n\r\napi_key = os.getenv(\"OPENAI_API_KEY\")\r\n\r\nWORKING_DIR = \"./inputText\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete\r\n)\r\n\r\n\r\n with open(\"./text.txt\", \"r\") as f:\r\n    content = f.read()\r\n    rag.insert(content)\r\n\r\n\r\n# Streamlit UI setup\r\nst.set_page_config(page_title=\"AI bot\", page_icon=\"üñ•Ô∏è\")\r\nst.title(\"AI bot\")\r\n\r\nst.markdown(unsafe_allow_html=True)\r\n\r\n# Initialize chat history in session state\r\nif \"chat_history\" not in st.session_state:\r\n    st.session_state.chat_history = [\r\n        {\"role\": \"AI\", \"content\": \"hi, how can I help?\"},\r\n    ]\r\n\r\n# Function to construct the conversation prompt\r\ndef construct_prompt(user_input, chat_history):\r\n    conversation = \"\"\r\n    for message in chat_history:\r\n        if message[\"role\"] == \"Human\":\r\n            conversation += f\"user: {message['content']}\\n\"\r\n        elif message[\"role\"] == \"AI\":\r\n            conversation += f\"AI bot: {message['content']}\\n\"\r\n    # Add the current user input\r\n    conversation += f\"user: {user_input}\\nAI bot: \"\r\n    return conversation\r\n\r\n# Function to query LightRAG and get the response\r\ndef get_response(prompt):\r\n    response = rag.query(\r\n        prompt,\r\n        param=QueryParam(mode=\"hybrid\")\r\n    )\r\n    return response\r\n\r\n# Chat input widget for user to ask questions\r\nuser_query = st.chat_input(\"ask me ...\")\r\nif user_query:\r\n    # Construct the conversation prompt including history\r\n    prompt = construct_prompt(user_query, st.session_state.chat_history)\r\n\r\n    # Get AI response\r\n    response = get_response(prompt)\r\n\r\n    # Add user question and AI response to chat history\r\n    st.session_state.chat_history.append({\"role\": \"Human\", \"content\": user_query})\r\n    st.session_state.chat_history.append({\"role\": \"AI\", \"content\": response})\r\n\r\n# Display chat history\r\nfor message in st.session_state.chat_history:\r\n    if message[\"role\"] == \"AI\":\r\n        with st.chat_message(\"AI\"):\r\n            st.write(message[\"content\"])\r\n    elif message[\"role\"] == \"Human\":\r\n        with st.chat_message(\"Human\"):\r\n            st.write(message[\"content\"])\r\n```"
      },
      {
        "user": "ziudeso",
        "created_at": "2024-11-26T18:06:05Z",
        "body": "Hu @amirsa66 is that the whole code?\r\nI'm getting `TypeError: MarkdownMixin.markdown() missing 1 required positional argument: 'body'` when running it as streamlit run test_history.py"
      },
      {
        "user": "ziudeso",
        "created_at": "2024-11-26T18:16:47Z",
        "body": "Ok nevermind works like a charm, great job @amirsa66 !!"
      }
    ]
  },
  {
    "number": 202,
    "title": "Some questions about graph output comparison with GraphRAG",
    "created_at": "2024-11-04T03:37:59Z",
    "closed_at": "2024-11-07T15:19:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/202",
    "body": "While reproducing, I noticed that the graph in the output file was slightly different from GraphRAG.\r\n\r\nAre the steps for building a graph in LightRAG the same as those for building a graph in GraphRAG? For example, are the same interfaces or logic used?\r\n\r\nIf so, how should I use the outputs generated by the two projects with each other? Is it possible?\r\n\r\n If not, what is the difference?\r\n \r\n Could you please give me more details? Thanks a lot.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/202/comments",
    "author": "WinstonCHEN1",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:12:43Z",
        "body": "The main difference between LightRAG and GraphRAG in the extraction process is that LightRAG omits communities and instead introduces keywords, making them incompatible with each other. For more details, please refer to our paper."
      },
      {
        "user": "WinstonCHEN1",
        "created_at": "2024-11-07T15:19:32Z",
        "body": "Thanks a lot!!!"
      }
    ]
  },
  {
    "number": 199,
    "title": "content_keywordsÊúâ‰ªÄ‰πàÁî®Ôºü",
    "created_at": "2024-11-03T09:02:32Z",
    "closed_at": "2024-11-11T02:16:45Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/199",
    "body": "ÈÄöËøáÊ∫êÁ†ÅÂèëÁé∞ÔºåÂ§ßÊ®°Âûã‰ºöËøîÂõû content_keywordsËøô‰∏™Â±ûÊÄß„ÄÇ‰ΩÜÊ∫êÁ†Å‰∏≠ÂØπcontent_keywordsÊ≤°ÊúâÂ≠òÂÇ®Ëµ∑Êù•\r\nÂú®Ê£ÄÁ¥¢ÁöÑÊó∂ÂÄôÔºå‰πüÊ≤°ÊúâÁî®Âà∞content_keywords\r\ncontent_keywordsËµ∑Âà∞‰ªÄ‰πà‰ΩúÁî®Ôºü\r\n\r\n===============================================================================\r\n\r\nThrough the source code, it was found that the large model will return the content_keywords attribute. But the content_keywords are not stored in the source code\r\nWhen searching, content_keywords were not used either\r\nWhat is the function of content_keywords?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/199/comments",
    "author": "gujiachun",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:07:21Z",
        "body": "The `content_keywords` is a reserved interface, and we haven't developed any functionality based on it yet."
      }
    ]
  },
  {
    "number": 196,
    "title": "Closed issue",
    "created_at": "2024-11-02T13:46:19Z",
    "closed_at": "2024-11-08T06:19:16Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/196",
    "body": "closed",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/196/comments",
    "author": "unknown918",
    "comments": [
      {
        "user": "lajihaonange",
        "created_at": "2024-11-04T03:41:13Z",
        "body": "Eat :watermelon: in the front."
      },
      {
        "user": "dreamerwhite",
        "created_at": "2024-11-04T04:47:55Z",
        "body": "Á≠âÁ≠îÊ°à"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-04T04:56:14Z",
        "body": "Thank you for your attention.\r\n\r\nRegarding your first question, I believe the comparison is reasonable. If the performance of the two methods is similar, their win rates should be close to 50%, which is also the approach taken by GraphRAG.\r\n\r\nFor the second question, I think it is necessary to consider the global method. Our evaluation framework aligns with GraphRAG's, focusing on global queries across the entire dataset. Furthermore, even for local methods, LightRAG still has an advantage in token consumption compared to GraphRAG's local method. The use of 60 entities is due to the parameter setting of topk=60. For local queries with the same number of entities, LightRAG does not require the addition of community reports, so it maintains a favorable token overhead.\r\n\r\nRegarding the third point, I'm not sure about your specific approach, but the discrepancy you observed may be mainly due to the difference in the number of retrieved entities between GraphRAG local search and LightRAG local search, as LightRAG can be configured to retrieve more than 60 entities.\r\n\r\nFor the fourth question, as mentioned in points 1 and 2, our evaluation framework is consistent with GraphRAG's. The assessment for global questions has been presented in the paper. For local queries, keyword-based retrieval ultimately also utilizes embeddings, but we extract keywords from the query rather than performing semantic retrieval directly with the query.\r\n\r\nThank you again for your questions!"
      }
    ]
  },
  {
    "number": 190,
    "title": "Is there any idea for delete some inserted document from the graph",
    "created_at": "2024-11-01T18:53:07Z",
    "closed_at": "2024-11-11T02:15:56Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/190",
    "body": "Is there any idea for delete some inserted document from the graph?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/190/comments",
    "author": "Maxch3306",
    "comments": [
      {
        "user": "kkk935208447",
        "created_at": "2024-11-04T01:20:01Z",
        "body": "It feels like the increase in the lightrag documentation library is irreversible."
      },
      {
        "user": "Maxch3306",
        "created_at": "2024-11-06T13:25:36Z",
        "body": "What if we add the filename or GUID to the cells for each document?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:02:36Z",
        "body": "We are exploring suitable deletion methods and currently plan to delete from the perspectives of entities and relationships."
      }
    ]
  },
  {
    "number": 189,
    "title": "ÊîØÊåÅÈü≥ËßÜÈ¢ëÊñá‰ª∂Êúâ‰ªÄ‰πàÊÄùË∑ØÂêó",
    "created_at": "2024-11-01T08:48:09Z",
    "closed_at": "2024-11-11T02:15:45Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/189",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/189/comments",
    "author": "wuhongsheng",
    "comments": [
      {
        "user": "masterwang22327",
        "created_at": "2024-11-05T02:58:47Z",
        "body": "Whisper/Clip/VLËΩ¨Êç¢ÊñáÊú¨ÂÜçÊäΩ"
      }
    ]
  },
  {
    "number": 188,
    "title": "When doing batch evaluation, it seems that Answer 1 always wins, even when I switch the order of the answers. Is this reasonable?",
    "created_at": "2024-11-01T08:37:52Z",
    "closed_at": "2024-11-11T02:15:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/188",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/188/comments",
    "author": "chaolili",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:00:34Z",
        "body": "The answers positioned earlier indeed have an advantage. As we mentioned in our paper, we will swap their order and reevaluate."
      }
    ]
  },
  {
    "number": 186,
    "title": "BUG: LightRag Constructor Tries To Create Log File Before Folder Exists",
    "created_at": "2024-10-31T21:14:51Z",
    "closed_at": "2025-01-23T06:58:24Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/186",
    "body": "If you try to create a LightRag object\r\n\r\nrag = LightRag(...)  but leave the working directory blank to use the default \r\n```/lightrag_cache_{datetime.now().strftime('%Y-%m-%d-%H:%M:%S' ```  \r\n\r\nit throws an error since in ```__post_init__``` it calls ```set_logger(log_file)``` before the folder is created.\r\n\r\nFIX: move create of log file and call to set_logger below the creation of the folder.  \r\n\r\nAlso for the default name you have the time in there with colons - on windows at least I don't think thats allowed, should probably be underscores for all seperators or something along those lines.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/186/comments",
    "author": "Justinius",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T06:58:40Z",
        "body": "I'll fix it ASAP."
      }
    ]
  },
  {
    "number": 184,
    "title": "ÂõæË∞±ÊûÑÂª∫ÁöÑËøáÁ®ã‰∏≠Ê≤°ÊúâÂÅöÊåá‰ª£Ê∂àËß£ÂíåÊ∂àÊ≠ß",
    "created_at": "2024-10-31T10:35:39Z",
    "closed_at": "2024-11-03T09:19:15Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/184",
    "body": "RTÔºåÁº∫Â∞ë‰∏äËø∞ËøáÁ®ãÁöÑËØùÔºåÂõæË∞±‰∏≠‰ºöÂåÖÂê´Â§ßÈáèÁöÑÈáçÂ§çÂÆû‰ΩìÔºåËøôÊ†∑Ê£ÄÁ¥¢ÂíåÁîüÊàêÈÉΩ‰ºöÂèóÂà∞ÂΩ±Âìç",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/184/comments",
    "author": "gcy0926",
    "comments": [
      {
        "user": "gcy0926",
        "created_at": "2024-11-01T02:25:49Z",
        "body": "_merge_nodes_then_upsertËøô‰∏™ÂáΩÊï∞ÈáåÈù¢Âè™Ê†πÊçÆÂêçÁß∞Ëé∑ÂèñÂ∑≤ÊúâËäÇÁÇπÔºåÂèØ‰ª•ÈÄöËøáÊü•ËØ≠‰πâÁõ∏‰ººÔºåÁªìÂêàÂ§ßÊ®°ÂûãÂÅöÊ∂àÊ≠ß„ÄÅÂéªÈáçÁöÑËøáÁ®ã„ÄÇ"
      },
      {
        "user": "LarFii",
        "created_at": "2024-11-01T06:08:31Z",
        "body": "ÊÑüË∞¢ÊÇ®ÁöÑÂª∫ËÆÆÔºÅÂêéÁª≠Êàë‰ª¨‰ºöËÄÉËôëÂä†ÂÖ•Ëøô‰∏ÄÊ≠•„ÄÇ"
      }
    ]
  },
  {
    "number": 183,
    "title": "Fix combine_contexts in hybrid_query",
    "created_at": "2024-10-31T07:33:59Z",
    "closed_at": "2024-11-03T09:44:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/183",
    "body": "1. Fix the output of \"combine_contexts\" in \"hybrid_query\"\r\n2. Fix the combine and deduplication in combine_contexts\r\n3. Modify list_of_list_to_csv\r\n4. Add csv_string_to_list",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/183/comments",
    "author": "gogoswift",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-03T09:44:09Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 177,
    "title": "fix:Step_3.py context",
    "created_at": "2024-10-30T15:22:28Z",
    "closed_at": "2024-10-31T00:24:22Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/177",
    "body": "Fix the same bug of Step_3_openai_compatible.py in Step_3.py in reproduce steps",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/177/comments",
    "author": "WinstonCHEN1",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-31T00:24:18Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 173,
    "title": "about the max capacity",
    "created_at": "2024-10-30T09:39:44Z",
    "closed_at": "2024-11-07T06:57:03Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/173",
    "body": "how many data can lightRAG / neo4j store?\r\nlet's say if we have 1 million paper, can I use lightRAG to store it, and query above all papers?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/173/comments",
    "author": "FingerLiu",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-01T06:01:47Z",
        "body": "We haven't tested the upper limit yet. In our experiments, the largest dataset handled was around 5 million tokens, which you can use as a reference."
      }
    ]
  },
  {
    "number": 172,
    "title": "hf_demo.py question?",
    "created_at": "2024-10-30T09:27:48Z",
    "closed_at": "2024-11-07T06:56:21Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/172",
    "body": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.59s/it]\r\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\r\n/data/anaconda3/envs/lfr_CLIP/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\r\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\r\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\r\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/172/comments",
    "author": "lfreee",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-01T06:04:02Z",
        "body": "This should not affect normal operation.\r\n"
      }
    ]
  },
  {
    "number": 169,
    "title": "Step_1 error",
    "created_at": "2024-10-30T01:02:44Z",
    "closed_at": "2024-11-01T04:46:23Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/169",
    "body": "Insertion failed, retrying (1/3), error: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/169/comments",
    "author": "DayanaYuan",
    "comments": [
      {
        "user": "WinstonCHEN1",
        "created_at": "2024-10-30T02:15:48Z",
        "body": "Use the `os` library to set your environment variables"
      }
    ]
  },
  {
    "number": 168,
    "title": "[hotfix-#163] Fix asynchronous problem",
    "created_at": "2024-10-29T15:48:13Z",
    "closed_at": "2024-10-30T01:38:21Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/168",
    "body": "I fixed the asynchronous problem. The issue address is [no.163]\r\n#163 ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/168/comments",
    "author": "Dormiveglia-elf",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-30T01:38:16Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 166,
    "title": "I am trying to use huggingface for llm model and embeddings but ending up with connectivity error for tiktoken.",
    "created_at": "2024-10-29T14:01:54Z",
    "closed_at": "2024-10-29T22:21:00Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/166",
    "body": "Hi Team, I am currently using huggingface llm model and embeddings, it's working fine, but when i am trying to deploy it on interned closed environment, it's haning with below error. \r\n\r\nError:\r\n`\"detail\": \"HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Read timed out. (read timeout=None)\"\r\n`\r\n\r\nI am bit confused here, llm and embeddings are pretty fine wrt to connectivity but can someone please help to understand for what exactly reason we are using the tiktoken model and is it possible to completly avoid it ?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/166/comments",
    "author": "msinha251",
    "comments": [
      {
        "user": "msinha251",
        "created_at": "2024-10-29T22:21:00Z",
        "body": "Fixed it with locally downloading it and using it with tiktoken cache."
      },
      {
        "user": "Lyrics-WangKL",
        "created_at": "2024-11-29T01:50:14Z",
        "body": "Hello, I had the same problem as you. Also locally download the o200k_base.tiktoken file. But I dont know how to use it wit tiktoken cache. Can you introduce the details? @msinha251 Thx~"
      }
    ]
  },
  {
    "number": 163,
    "title": "RuntimeWarning: coroutine 'LightRAG.ainsert' was never awaited   print(f\"An error occurred: {e}\") ",
    "created_at": "2024-10-29T08:02:04Z",
    "closed_at": "2024-10-30T02:42:42Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/163",
    "body": "ËøêË°ålightrag_openai_compatible_demo.pyÊä•Èîô",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/163/comments",
    "author": "small122",
    "comments": [
      {
        "user": "dhdsjy",
        "created_at": "2024-10-29T12:32:21Z",
        "body": "ÊàëÂú®win‰∏ãËøêË°å‰πüÊòØËøôÁßçÈîôËØØÔºåÊèêÁ§∫ÊòØÂçèÁ®ãÁöÑÈóÆÈ¢òÔºå‰ΩÜ‰πüÊ≤°ÂèëÁé∞ÂÖ∑‰ΩìÈîôÂú®Âì™Èáå"
      },
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-29T15:52:52Z",
        "body": "Please refer my PR #168 , I think you can run the program smoothly without error now."
      },
      {
        "user": "small122",
        "created_at": "2024-10-30T02:42:40Z",
        "body": "> Please refer my PR #168 , I think you can run the program smoothly without error now.\r\n\r\nthank youÔºåi get it"
      }
    ]
  },
  {
    "number": 161,
    "title": "Can \"gpt\" be  be replaced with other model?",
    "created_at": "2024-10-29T02:59:30Z",
    "closed_at": "2024-11-01T04:45:49Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/161",
    "body": "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/161/comments",
    "author": "guanzy2012",
    "comments": [
      {
        "user": "TianyuFan0504",
        "created_at": "2024-10-29T06:15:19Z",
        "body": "Yes, you can switch you model to Openai()/Ollama/HF model."
      }
    ]
  },
  {
    "number": 160,
    "title": "LightRAG Â¶Ç‰ΩïÂä†ËΩΩÊú¨Âú∞Â§ßÊ®°Âûã",
    "created_at": "2024-10-29T02:36:28Z",
    "closed_at": "2024-11-01T04:45:41Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/160",
    "body": "Â¶Ç‰ΩïÂä†ËΩΩÊú¨Âú∞Â§ßÊ®°ÂûãÔºåÊØîÂ¶ÇqwenËøôÁ±ªÁöÑ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/160/comments",
    "author": "kennyLSN",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-29T02:49:01Z",
        "body": "ÂèØ‰ª•ÂèÇËÄÉquick start‰∏≠ÁöÑollamaÂíåhugging faceÈÉ®ÂàÜ"
      }
    ]
  },
  {
    "number": 157,
    "title": "local_query: \"IndexError: list index out of range\"",
    "created_at": "2024-10-28T20:18:13Z",
    "closed_at": "2024-11-03T09:16:16Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/157",
    "body": "lib/python3.11/site-packages/lightrag/operate.py\", line 404, in local_query\r\n    result = '{' + result.split('{')[1].split('}')[0] + '}'\r\n                   ~~~~~~~~~~~~~~~~~^^^\r\nIndexError: list index out of range",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/157/comments",
    "author": "mchzimm",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-29T02:18:22Z",
        "body": "You might want to check the content of `result`. It's likely that the LLM didn't output in JSON format."
      },
      {
        "user": "mchzimm",
        "created_at": "2024-10-29T04:48:26Z",
        "body": "`result` - is that a file? because i don't see one in the prj root or in WORKING_DIR folder"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-29T09:01:44Z",
        "body": "I mean you can check the content of the `result` variable to see if the LLM's output format is correct."
      },
      {
        "user": "a896900806",
        "created_at": "2024-10-31T09:32:57Z",
        "body": "> lib/python3.11/site-packages/lightrag/operate.py\", line 404, in local_query result = '{' + result.split('{')[1].split('}')[0] + '}' ~~~~~~~~~~~~~~~~~^^^ IndexError: list index out of range\r\n\r\nYou can go to see the generated result. Maybe your model failed to extract the keywords of the problem."
      }
    ]
  },
  {
    "number": 156,
    "title": "Add ability to pass additional parameters to ollama library ",
    "created_at": "2024-10-28T17:14:55Z",
    "closed_at": "2024-10-29T02:11:22Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/156",
    "body": "This PR contains following improvements:\r\n* More detailed explanation how to work with ollama in README.\r\n* Ready to try ollama example without any additional setup.\r\n* Added ability to pass additional arguments to ollama library like host, timeout etc.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/156/comments",
    "author": "alazarchuk",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-29T02:11:16Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 155,
    "title": "Any plan to support deleting or updating existing nodes?",
    "created_at": "2024-10-28T12:44:10Z",
    "closed_at": "2024-11-01T04:44:37Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/155",
    "body": "It's great to know LightRAG supports incremental updates now, which is not supported by GraphRAG. However, we do really need to support modifying, deleting nodes, do you have any plan to support this in upcoming future? ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/155/comments",
    "author": "hejingkan2005",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-29T02:16:49Z",
        "body": "We do have plans to support this feature, but we haven't yet decided on the approach. It could either be based on the original text or involve directly deleting nodes, which might be easier."
      },
      {
        "user": "yilanhu",
        "created_at": "2024-10-29T05:06:39Z",
        "body": "Thank you @LarFii for the reply! We're also interested in how to solve the issue: LLM can generate different nodes/relationships every time even for the same text."
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-29T09:05:39Z",
        "body": "For LightRAG, the LLM responses for the same query are cached. Therefore, if the query is already in the cache, it will directly return the cached answer."
      },
      {
        "user": "bode135",
        "created_at": "2024-11-11T22:44:33Z",
        "body": "GraphRAG is suported"
      },
      {
        "user": "nikhilmaddirala",
        "created_at": "2024-12-11T20:59:46Z",
        "body": "@LarFii \r\n> We do have plans to support this feature, but we haven't yet decided on the approach. It could either be based on the original text or involve directly deleting nodes, which might be easier.\r\n\r\nI'd love to see support for deleting documents from the input file directly. Deleting nodes and/or relationships would be too complicated for users. Similar to `rag.insert(doc)` it would be great to have something like `rag.delete(doc)` which would automatically update the nodes and relationships.\r\n\r\nThis would also unlock editing documents by simply doing `rag.delete(old_doc)` and `rag.insert(updated_doc)`"
      },
      {
        "user": "LarFii",
        "created_at": "2024-12-12T06:35:19Z",
        "body": "> I'd love to see support for deleting documents from the input file directly. Deleting nodes and/or relationships would be too complicated for users. Similar to `rag.insert(doc)` it would be great to have something like `rag.delete(doc)` which would automatically update the nodes and relationships.\r\n> \r\n> This would also unlock editing documents by simply doing `rag.delete(old_doc)` and `rag.insert(updated_doc)`\r\n\r\nThese features are currently in our plan, and our next updates will focus on document deletion and improved update mechanisms."
      },
      {
        "user": "Pranesh026",
        "created_at": "2024-12-20T16:04:42Z",
        "body": "\r\n> These features are currently in our plan, and our next updates will focus on document deletion and improved update mechanisms.\r\n\r\nHi , the rag.delete_by_entity(\"entity_name\") is not working properly, When I try to run it, the full graph got deleted. Please let me know, if something I am missing out?"
      }
    ]
  },
  {
    "number": 154,
    "title": "'LightRAG.ainsert' was never awaited",
    "created_at": "2024-10-28T08:50:33Z",
    "closed_at": "2024-10-28T09:05:38Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/154",
    "body": "An error occurred: This event loop is already running\r\nLightRAG-main\\examples\\lightrag_openai_compatible_demo.py:107: RuntimeWarning: coroutine 'LightRAG.ainsert' was never awaited\r\n  print(f\"An error occurred: {e}\")\r\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/154/comments",
    "author": "gujiachun",
    "comments": [
      {
        "user": "Sweewangyu",
        "created_at": "2024-10-29T02:19:28Z",
        "body": "heyÔºåHow did you solve itÔºü"
      },
      {
        "user": "gujiachun",
        "created_at": "2024-10-29T02:19:57Z",
        "body": "‰ª•Êî∂Âà∞"
      },
      {
        "user": "small122",
        "created_at": "2024-10-29T08:29:34Z",
        "body": "ÂíãËß£ÂÜ≥ÁöÑÂì•"
      },
      {
        "user": "small122",
        "created_at": "2024-10-29T09:05:49Z",
        "body": "> [Sweewangyu](/Sweewangyu)\r\n\r\nheyÔºådid you solve itÔºü"
      },
      {
        "user": "gujiachun",
        "created_at": "2024-10-29T09:30:19Z",
        "body": "Ëß£ÂÜ≥‰∫ÜÔºåËøõÁ®ãÊùÄÊéâÂ∞±Ë°å‰∫Ü"
      }
    ]
  },
  {
    "number": 152,
    "title": "Áü•ËØÜÂ∫ìÂà†Èô§Â∑≤ÊúâÊñáÊú¨",
    "created_at": "2024-10-28T08:13:37Z",
    "closed_at": "2024-11-01T04:44:29Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/152",
    "body": "ÊÇ®Â•Ω„ÄÇÁé∞Âú®ÁöÑÁü•ËØÜÂ∫ìËÉΩÂ§üÂÆûÁé∞Â¢ûÈáèÊõ¥Êñ∞ÔºåËØ∑ÈóÆÊòØÂê¶ËÉΩÂÆûÁé∞Âà†Èô§Â∑≤ÊúâÁöÑÊñáÊú¨Êï∞ÊçÆ„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/152/comments",
    "author": "yanning169",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-28T08:33:00Z",
        "body": "ÁõÆÂâçËøòÊ≤°ÊúâÂäûÊ≥ïÂÅöÂà∞Âà†Èô§Â∑≤ÊúâÁöÑÊñáÊú¨"
      }
    ]
  },
  {
    "number": 149,
    "title": "Can not modify content.",
    "created_at": "2024-10-27T08:34:38Z",
    "closed_at": "2024-11-01T04:43:46Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/149",
    "body": "I have tested the method for incrementally adding new content. However, I am unable to delete the newly added content or make any modifications to the already embedded content. Is there a plan for intelligent modification of document content in this project?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/149/comments",
    "author": "sj8214834",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-28T07:11:35Z",
        "body": "Currently, there is no method for deleting newly added content (as this would be quite challenging)."
      }
    ]
  },
  {
    "number": 147,
    "title": "Discord link is expired or invalid. ",
    "created_at": "2024-10-27T06:13:21Z",
    "closed_at": "2024-10-28T01:58:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/147",
    "body": "Hi, i am unable to join discord server. Can someone please update the link or share ? ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/147/comments",
    "author": "msinha251",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-28T01:58:34Z",
        "body": "I have updated the link : )"
      }
    ]
  },
  {
    "number": 146,
    "title": "ÂêëÈáèÂ≠òÂÇ®Áª¥Â∫¶ÈóÆÈ¢ò",
    "created_at": "2024-10-26T11:29:45Z",
    "closed_at": "2024-10-27T02:24:47Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/146",
    "body": "### ‰ΩøÁî®Êô∫Ë∞±ÁöÑÂ§ßÊ®°ÂûãÔºå‰∏ÄÁõ¥Âá∫Áé∞ÂêëÈáèÂ≠òÂÇ®ÁöÑÁª¥Â∫¶ÈóÆÈ¢òÔºåÈ∫ªÁÉ¶Â§ß‰Ω¨Â∏ÆÂøôÁúãÁúã\r\n### ËøôÊòØÊàëÁöÑ‰ª£Á†Å\r\n```python\r\nimport os\r\n\r\nimport numpy as np\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.utils import EmbeddingFunc, compute_args_hash\r\nfrom lightrag.base import BaseKVStorage\r\nfrom zhipuai import ZhipuAI\r\n\r\n\r\nWORKING_DIR = \"./dickens\"\r\nZHIPU_APIKEY = \"ÊàëËá™Â∑±ÁöÑkey\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n    \r\n    \r\n\r\nasync def zhipu_model_complete(\r\n     prompt, system_prompt=None, history_messages=[], **kwargs\r\n) -> str:\r\n    model_name = kwargs['hashing_kv'].global_config['llm_model_name']\r\n    return await zhipu_model_if_cache(\r\n        model_name,\r\n        prompt,\r\n        system_prompt=system_prompt,\r\n        history_messages=history_messages,\r\n        **kwargs,\r\n    )\r\n\r\n\r\n\r\nasync def zhipu_model_if_cache(\r\n    model, prompt, system_prompt=None, history_messages=[], **kwargs\r\n) -> str:\r\n    kwargs.pop(\"max_tokens\", None)\r\n    kwargs.pop(\"response_format\", None)\r\n    zhipu_client = ZhipuAI(api_key=ZHIPU_APIKEY)\r\n    messages = []\r\n    if system_prompt:\r\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n\r\n    hashing_kv: BaseKVStorage = kwargs.pop(\"hashing_kv\", None)\r\n    messages.extend(history_messages)\r\n    messages.append({\"role\": \"user\", \"content\": prompt})\r\n    if hashing_kv is not None:\r\n        args_hash = compute_args_hash(model, messages)\r\n        if_cache_return = await hashing_kv.get_by_id(args_hash)\r\n        if if_cache_return is not None:\r\n            return if_cache_return[\"return\"]\r\n\r\n    response = zhipu_client.chat.completions.create(\r\n        model=model, \r\n        messages=messages, \r\n        **kwargs\r\n    )\r\n\r\n    result = response.choices[0].message.content\r\n\r\n    if hashing_kv is not None:\r\n        await hashing_kv.upsert({args_hash: {\"return\": result, \"model\": model}})\r\n\r\n    return result\r\n\r\n    \r\nasync def zhipu_embedding(texts: list[str], embed_model) -> np.ndarray:\r\n    zhipu_client = ZhipuAI(api_key=ZHIPU_APIKEY)\r\n    response = zhipu_client.embeddings.create(model=embed_model, input=texts)\r\n\r\n    return np.array([dp.embedding for dp in response.data])\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=zhipu_model_complete,  \r\n    llm_model_name='GLM-4-Flash',\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=512,\r\n        max_token_size=8192,\r\n        func=lambda texts: zhipu_embedding(\r\n            texts, \r\n            embed_model=\"embedding-3\"\r\n        )\r\n    ),\r\n)\r\n\r\n\r\nwith open(\"./book.txt\", 'r', encoding='utf-8') as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\n# print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")))\r\n\r\n# # Perform local search\r\n# print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\")))\r\n\r\n# # Perform global search\r\n# print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\")))\r\n\r\n# # Perform hybrid search\r\nprint(rag.query(\"please provide the context or content from which I should\", param=QueryParam(mode=\"hybrid\")))\r\n\r\n```\r\n\r\n### Êä•ÈîôÂ¶Ç‰∏ãÔºö\r\n```python\r\nTraceback (most recent call last):\r\n  File \"D:\\code\\py\\LightRAG\\lightrag_zhipu_demo.py\", line 87, in <module>\r\n    rag.insert(f.read())\r\n  File \"D:\\code\\py\\LightRAG\\lightrag\\lightrag.py\", line 166, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\anaconda3\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"D:\\code\\py\\LightRAG\\lightrag\\lightrag.py\", line 210, in ainsert\r\n    await self.chunks_vdb.upsert(inserting_chunks)\r\n  File \"D:\\code\\py\\LightRAG\\lightrag\\storage.py\", line 103, in upsert\r\n    results = self._client.upsert(datas=list_data)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\anaconda3\\Lib\\site-packages\\nano_vectordb\\dbs.py\", line 108, in upsert\r\n    self.__storage[\"matrix\"] = np.vstack([self.__storage[\"matrix\"], new_matrix])\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py\", line 289, in vstack\r\n    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 512 and the array at index 1 has size 2048\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/146/comments",
    "author": "wangsj1018",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-26T12:50:04Z",
        "body": "Please refer to my previous PR #116 "
      },
      {
        "user": "cristianohello",
        "created_at": "2024-10-26T15:03:22Z",
        "body": "ÊÄé‰πà‰øÆÊîπ‰ª£Á†ÅÔºüËøòÊòØÊä•Èîô"
      },
      {
        "user": "wangsj1018",
        "created_at": "2024-10-27T02:09:27Z",
        "body": "> ÊÄé‰πà‰øÆÊîπ‰ª£Á†ÅÔºüËøòÊòØÊä•Èîô\r\n\r\n‰∏çÂ§™‰ºöpython, ÊàëÂ§öÁªÉÁªÉ"
      },
      {
        "user": "wangsj1018",
        "created_at": "2024-10-27T02:24:48Z",
        "body": "\r\n\r\n\r\n> Please refer to my previous PR #116\r\n\r\nthx"
      }
    ]
  },
  {
    "number": 143,
    "title": "[feat] Add API server implementation and endpoints",
    "created_at": "2024-10-26T07:57:19Z",
    "closed_at": "2024-10-28T01:54:02Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/143",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/143/comments",
    "author": "thinkthinking",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-28T01:54:22Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 141,
    "title": "fixing knowledge_graph keyword bug",
    "created_at": "2024-10-26T04:12:10Z",
    "closed_at": "2024-10-26T06:31:02Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/141",
    "body": "#133 #132",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/141/comments",
    "author": "Yazington",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-26T04:57:38Z",
        "body": "Please refer to PR #123 \r\nI think that you didn't modify anything bro."
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-26T05:45:43Z",
        "body": "The typos have been fixed in the latest code. : )"
      },
      {
        "user": "tackhwa",
        "created_at": "2024-10-26T06:26:04Z",
        "body": "@LarFii i think there is still few typo `knwoledge_graph_inst`  in `operate.py`, can you please fix it?, in `_merge_edges_then_upsert` and few other methods also"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-26T06:30:39Z",
        "body": "You're right; that was an oversight on my part. I'll fix it right now."
      },
      {
        "user": "Yazington",
        "created_at": "2024-10-26T18:03:42Z",
        "body": "was using  ruff formatter I think might be reason why builds are failing"
      }
    ]
  },
  {
    "number": 136,
    "title": "Incremental update not working",
    "created_at": "2024-10-25T17:30:21Z",
    "closed_at": "2024-11-01T04:43:24Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/136",
    "body": "When I run on single text file with the azure OpenAI instance it is working, I set the azure variables like endpoint, url etc in the example file itself, but when I run incremental update, it throws Set open ai api key error? ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/136/comments",
    "author": "NIHALPATTANSHETTY",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-26T06:09:39Z",
        "body": "I‚Äôll need to see the specific code and logs to analyze the issue."
      },
      {
        "user": "NIHALPATTANSHETTY",
        "created_at": "2024-10-26T06:32:08Z",
        "body": "File \"C:\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\r\n    raise self._exception\r\n  File \"C:\\Users\\e191866\\AppData\\Roaming\\Python\\Python312\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__    \r\n    result = await fn(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\e191866\\Downloads\\NCG_project\\search_team\\LightRAG\\lightrag\\llm.py\", line 418, in openai_embedding\r\n    AsyncOpenAI() if base_url is None else AsyncOpenAI(base_url=base_url)\r\n    ^^^^^^^^^^^^^\r\n  File \"C:\\Users\\e191866\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_client.py\", line 319, in __init__\r\n    raise OpenAIError(\r\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\r\n\r\nBtw, I am using Azure open AI instance, when I run initially, even though it gives rate limit after some time, it progresses through the entity extraction phase, but for incremental update, this error is given"
      },
      {
        "user": "NIHALPATTANSHETTY",
        "created_at": "2024-10-27T12:50:22Z",
        "body": "Actually, when you try incremental update in the same for loop, it works, but when you try incremental update with the rag instance, rag = LightRAG(working_dir=\"./dickens\")\r\nwithout specifying the LLM model and the embedding function like you do in the previous step, it throws the OPEN AI key not found error, so better to write \r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=embedding_dimension,\r\n        max_token_size=8192,\r\n        func=embedding_func,\r\n    ),\r\n)\r\n, this takes the existing rag instance, with all the previous indexing and runs incremental update properly"
      },
      {
        "user": "NIHALPATTANSHETTY",
        "created_at": "2024-10-27T12:51:58Z",
        "body": "Also, much of the code in llm.py file is not reflected in any of the example files, if we change the retry wrapper in the llm.py, it does nothing when running the example file, instead we have to add our own function like this, \r\nasync with session.post(endpoint, headers=headers, json=payload) as response:\r\n                if response.status == 429:\r\n                    # Retry after waiting for 120 seconds\r\n                    await asyncio.sleep(120)"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-28T07:06:26Z",
        "body": "I'll fix these issues ASAP."
      }
    ]
  },
  {
    "number": 134,
    "title": "[Doc] Added a flowchart illustrating the core algorithm",
    "created_at": "2024-10-25T16:40:33Z",
    "closed_at": "2024-10-26T06:32:00Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/134",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/134/comments",
    "author": "thinkthinking",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-26T06:31:56Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 133,
    "title": "TypeError: extract_entities() got an unexpected keyword argument 'knowledge_graph_inst'",
    "created_at": "2024-10-25T15:11:25Z",
    "closed_at": "2024-10-26T06:01:09Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/133",
    "body": "After installation, run `lightrag_hf_demo.py`, TypeError: extract_entities() got an unexpected keyword argument 'knowledge_graph_inst' occurs",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/133/comments",
    "author": "ChengyinLee",
    "comments": [
      {
        "user": "crossir79",
        "created_at": "2024-10-25T15:17:05Z",
        "body": "There is a typo:\"knwoledge\" #132"
      },
      {
        "user": "ChengyinLee",
        "created_at": "2024-10-25T15:36:45Z",
        "body": "Thanks"
      },
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-25T16:38:32Z",
        "body": "Refer to PR #123 \r\nRemember to pull the latest code every day."
      }
    ]
  },
  {
    "number": 132,
    "title": "There is a typo:\"knwoledge\"",
    "created_at": "2024-10-25T15:08:25Z",
    "closed_at": "2024-10-26T06:00:57Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/132",
    "body": "async def extract_entities(\r\n    chunks: dict[str, TextChunkSchema],\r\n    knwoledge_graph_inst: BaseGraphStorage,\r\n    entity_vdb: BaseVectorStorage,\r\n    relationships_vdb: BaseVectorStorage,\r\n    global_config: dict,\r\nThere is a typo:\"knwoledge\"",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/132/comments",
    "author": "crossir79",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-25T16:39:22Z",
        "body": "Please refer to PR #123 \r\nRemember to pull the latest code every day."
      }
    ]
  },
  {
    "number": 130,
    "title": "Key Enhancements:",
    "created_at": "2024-10-25T13:13:39Z",
    "closed_at": "2024-10-26T05:49:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/130",
    "body": "Error Handling:\r\n\r\nHandled potential FileNotFoundError for README.md and requirements.txt. Checked for missing required metadata and raised an informative error if any are missing. Automated Package Discovery:\r\n\r\nReplaced packages=[\"lightrag\"] with setuptools.find_packages() to automatically find sub-packages and exclude test or documentation directories. Additional Metadata:\r\n\r\nAdded Development Status in classifiers to indicate a \"Beta\" release (modify based on the project's maturity). Used project_urls to link documentation, source code, and an issue tracker, which are standard for open-source projects. Compatibility:\r\n\r\nIncluded include_package_data=True to include additional files specified in MANIFEST.in. These changes enhance the readability, reliability, and openness of the code, making it more contributor-friendly and ensuring it‚Äôs ready for open-source distribution.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/130/comments",
    "author": "jatin9823",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-26T05:49:36Z",
        "body": "Thank you for the improvements!"
      }
    ]
  },
  {
    "number": 129,
    "title": "Questions about multimodal retrieval",
    "created_at": "2024-10-25T12:39:29Z",
    "closed_at": "2024-10-28T02:04:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/129",
    "body": "I want to use images as queries for retrieval. Can I modify it by directly replacing the model in embedding_func with a multimodal large model? If not, please tell me what needs to be changed. Thank you.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/129/comments",
    "author": "SLKAlgs",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-25T16:47:45Z",
        "body": "Interesting question, I think you can try directly replacing the embedding model (though it‚Äôs not guaranteed to work). However, I believe that multimodal models may not be as detailed in their text embeddings, which could lead to a loss of accuracy. Another approach would be to use multimodal large language models like GPT-4o or qwen-VL-Max(100 million free tokens for new user) to first generate descriptive text from the image, and then use this text for querying and matching. This way, you can continue to use the text embedding model. I look forward to your further experimental results."
      },
      {
        "user": "SLKAlgs",
        "created_at": "2024-11-09T06:46:21Z",
        "body": "> ÊúâË∂£ÁöÑÈóÆÈ¢òÔºåÊàëËÆ§‰∏∫ÊÇ®ÂèØ‰ª•Â∞ùËØïÁõ¥Êé•ÊõøÊç¢ÂµåÂÖ•Ê®°ÂûãÔºàÂ∞ΩÁÆ°‰∏çËÉΩ‰øùËØÅ‰∏ÄÂÆöÊúâÊïàÔºâ„ÄÇ‰ΩÜÊòØÔºåÊàëËÆ§‰∏∫Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊñáÊú¨ÂµåÂÖ•ÂèØËÉΩ‰∏çÂ§üËØ¶ÁªÜÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂáÜÁ°ÆÊÄß‰∏ãÈôç„ÄÇÂè¶‰∏ÄÁßçÊñπÊ≥ïÊòØ‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶Ç GPT-4o Êàñ qwen-VL-MaxÔºàÊñ∞Áî®Êà∑ÂèØËé∑Âæó 1 ‰∫ø‰∏™ÂÖçË¥π‰ª§ÁâåÔºâÔºåÈ¶ñÂÖà‰ªéÂõæÂÉè‰∏≠ÁîüÊàêÊèèËø∞ÊÄßÊñáÊú¨ÔºåÁÑ∂Âêé‰ΩøÁî®Ê≠§ÊñáÊú¨ËøõË°åÊü•ËØ¢ÂíåÂåπÈÖç„ÄÇËøôÊ†∑ÔºåÊÇ®Â∞±ÂèØ‰ª•ÁªßÁª≠‰ΩøÁî®ÊñáÊú¨ÂµåÂÖ•Ê®°Âûã„ÄÇÊàëÊúüÂæÖÊÇ®Ëøõ‰∏ÄÊ≠•ÁöÑÂÆûÈ™åÁªìÊûú„ÄÇ\r\n\r\nMy understanding is to first use GPT-4o or other multimodal large models to generate a descriptive caption for the image I want to use, and then concatenate it with the query intended for LightRAG retrieval (e.g., \"What's this?\") to form a new query, which will then produce the retrieval-based answer. Do you think this approach is reasonable?"
      },
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-11-09T07:07:28Z",
        "body": "> > ÊúâË∂£ÁöÑÈóÆÈ¢òÔºåÊàëËÆ§‰∏∫ÊÇ®ÂèØ‰ª•Â∞ùËØïÁõ¥Êé•ÊõøÊç¢ÂµåÂÖ•Ê®°ÂûãÔºàÂ∞ΩÁÆ°‰∏çËÉΩ‰øùËØÅ‰∏ÄÂÆöÊúâÊïàÔºâ„ÄÇ‰ΩÜÊòØÔºåÊàëËÆ§‰∏∫Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊñáÊú¨ÂµåÂÖ•ÂèØËÉΩ‰∏çÂ§üËØ¶ÁªÜÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂáÜÁ°ÆÊÄß‰∏ãÈôç„ÄÇÂè¶‰∏ÄÁßçÊñπÊ≥ïÊòØ‰ΩøÁî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶Ç GPT-4o Êàñ qwen-VL-MaxÔºàÊñ∞Áî®Êà∑ÂèØËé∑Âæó 1 ‰∫ø‰∏™ÂÖçË¥π‰ª§ÁâåÔºâÔºåÈ¶ñÂÖà‰ªéÂõæÂÉè‰∏≠ÁîüÊàêÊèèËø∞ÊÄßÊñáÊú¨ÔºåÁÑ∂Âêé‰ΩøÁî®Ê≠§ÊñáÊú¨ËøõË°åÊü•ËØ¢ÂíåÂåπÈÖç„ÄÇËøôÊ†∑ÔºåÊÇ®Â∞±ÂèØ‰ª•ÁªßÁª≠‰ΩøÁî®ÊñáÊú¨ÂµåÂÖ•Ê®°Âûã„ÄÇÊàëÊúüÂæÖÊÇ®Ëøõ‰∏ÄÊ≠•ÁöÑÂÆûÈ™åÁªìÊûú„ÄÇ\r\n> \r\n> My understanding is to first use GPT-4o or other multimodal large models to generate a descriptive caption for the image I want to use, and then concatenate it with the query intended for LightRAG retrieval (e.g., \"What's this?\") to form a new query, which will then produce the retrieval-based answer. Do you think this approach is reasonable?\r\n\r\nYes, I think it is reasonable."
      }
    ]
  },
  {
    "number": 128,
    "title": "Rate limit exceeding ",
    "created_at": "2024-10-25T11:59:59Z",
    "closed_at": "2024-11-01T04:43:15Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/128",
    "body": "I have 3 text files, each about 70 chunks in size, initially I gave all 3 in the input folder, it is saying open ai rate limit exceeded, I tried changing the max llm sync and embedding async, it is not working, I also tried increasing the retries and stop time, it didn't work, I reduced the entities count, It didn't work , finally split each original file into 2 files and tried incremental update, it is failing. \r\n\r\nGrapghRag from Microsoft took all 3 files with all the entities and it is working. ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/128/comments",
    "author": "NIHALPATTANSHETTY",
    "comments": [
      {
        "user": "sebeyza00",
        "created_at": "2024-10-25T16:09:25Z",
        "body": "hey! had the same error, has to do with the @retry mechanism in llm.py for openai_complete_if_cache(), change the wait parameter to wait=wait_fixed(10), add wait_fixed to tenacity's imports at the top, and this should wait 10 seconds to send the request again after it fails and that should work depending on your openai api rate limits."
      },
      {
        "user": "zcstarr",
        "created_at": "2024-10-25T17:13:04Z",
        "body": "@sebeyza00 this is definitely the patch rather, also to note the retry decorator on the other completes will benefit for wait_fixed(10) as well as I've confirmed it for the decorators for bedrock. "
      },
      {
        "user": "NIHALPATTANSHETTY",
        "created_at": "2024-10-25T17:24:31Z",
        "body": "@sebeyza00  Actually in the llm.py file I replaced all instances of the wait exponential with wait fixed and kept it to 180 but it is still failing instantly, why is it so, do I need to make any changes to the example file as well? "
      },
      {
        "user": "zcstarr",
        "created_at": "2024-10-26T02:31:47Z",
        "body": "> @sebeyza00 Actually in the llm.py file I replaced all instances of the wait exponential with wait fixed and kept it to 180 but it is still failing instantly, why is it so, do I need to make any changes to the example file as well?\r\n\r\nYeah I think I still hit a concurrency issue, which might need some further investigating, but that's mostly because my request limit is super low, so I need a way to tune the concurrency down to avoid 429s"
      },
      {
        "user": "zcstarr",
        "created_at": "2024-10-26T21:08:22Z",
        "body": "Actually dug a bit more into concurrency you might need to set   @NIHALPATTANSHETTY   `llm_model_max_async=4` parameter\r\nexample:\r\n```python\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=bedrock_complete,\r\n    llm_model_max_async=4,\r\n    llm_model_name=\"Anthropic Claude 3 Haiku // Amazon Bedrock\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1024, max_token_size=8192, func=bedrock_embedding\r\n    ),\r\n)\r\n```"
      },
      {
        "user": "includingThis",
        "created_at": "2024-11-22T10:19:45Z",
        "body": "> hey! had the same error, has to do with the @Retry mechanism in llm.py for openai_complete_if_cache(), change the wait parameter to wait=wait_fixed(10), add wait_fixed to tenacity's imports at the top, and this should wait 10 seconds to send the request again after it fails and that should work depending on your openai api rate limits.\r\n\r\nThanks, this really made my day. Perhaps this setting might be included in the official code as well? "
      }
    ]
  },
  {
    "number": 127,
    "title": "Adding 100vh to the visualization code feels better",
    "created_at": "2024-10-25T09:10:52Z",
    "closed_at": "2024-10-26T06:04:27Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/127",
    "body": "import networkx as nx\r\nfrom pyvis.network import Network\r\nimport random\r\n\r\nG = nx.read_graphml('./test_ollama_t2/graph_chunk_entity_relation.graphml')\r\n\r\nnet = Network(height=\"100vh\", notebook=True)\r\n\r\nnet.from_nx(G)\r\n\r\nfor node in net.nodes:\r\n    node['color'] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\r\n\r\nnet.show('knowledge_graph.html')\r\n\r\n\r\nNow it can be displayed in full screen\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/127/comments",
    "author": "saltyfish-kiri",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-26T06:04:27Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 125,
    "title": "Added Github Actions",
    "created_at": "2024-10-25T07:54:58Z",
    "closed_at": "2024-10-26T05:54:51Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/125",
    "body": "I have added GitHub actions checking during Pull Request and Push for Linting and Formatting.\r\nPlease review it.\r\nAnd mention if any other actions are required for this project",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/125/comments",
    "author": "sank8-2",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-26T05:54:48Z",
        "body": "Thank you for adding GitHub actions for linting and formatting checks! We're currently focused on resolving #114 "
      }
    ]
  },
  {
    "number": 124,
    "title": "How to construct the Overall Performance Table in example/batch_eval.py?",
    "created_at": "2024-10-25T06:39:35Z",
    "closed_at": "2024-10-30T01:40:15Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/124",
    "body": "How to get the score in Overall Performance Table by running example/batch_eval.py?\r\n1. How are the **result_files** obtained? Are they obtained by querying different classes (in reproduce/Step_3.py)?\r\n2. Why is it unnecessary to consider the **retrieval context** and **ground truth** ? And what's the ground_truth of the generated queries?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/124/comments",
    "author": "SetonLiang",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-28T07:00:10Z",
        "body": "Yes, the result_files are the results generated by Step 3. Specifically, they are the answers provided by different RAG systems to the queries produced in Step 2. As for question 2, you can refer to Section 4.1 of the paper for more details."
      },
      {
        "user": "SetonLiang",
        "created_at": "2024-10-28T12:18:49Z",
        "body": "Got it. So, should I understand that instead of considering the ground truth, the approach is to compare different answers across multiple dimensions and then calculate the win rate, using only the **document content without (query, answer) pairs** in the UltraDomain Benchmark dataset during evaluation?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-29T02:13:52Z",
        "body": "You're correct. Our approach focuses on evaluating model performance across multiple dimensions. This method directly follows the GraphRAG."
      }
    ]
  },
  {
    "number": 123,
    "title": "typo(lightrag/lightrag.py): typo",
    "created_at": "2024-10-25T06:14:43Z",
    "closed_at": "2024-10-25T11:52:36Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/123",
    "body": "typo",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/123/comments",
    "author": "tpoisonooo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-25T11:52:24Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 121,
    "title": "lightrag_openai_compatible_demo. file. Questions to ask",
    "created_at": "2024-10-25T01:44:42Z",
    "closed_at": "2024-11-01T04:43:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/121",
    "body": "I saw that you modified the code yesterday, but I feel that there are some areas that are not correct, especially the file reading and querying parts of the asynchronous code. Would it be better to switch to asynchronous?\r\n\r\n` with open(\"./hanye.txt\", \"r\", encoding=\"utf-8\") as f:\r\n            content = f.read()\r\n   await rag.ainsert(content)`\r\n\r\n`naive_result = await rag.aquery(query, param=QueryParam(mode=\"naive\"))\r\n        print(\"Naive search result:\", naive_result)`",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/121/comments",
    "author": "redpintings",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-25T02:47:52Z",
        "body": "You are right. I tested it and it is better. I have put a new PR #168 to do this."
      }
    ]
  },
  {
    "number": 119,
    "title": "„Äêfunction„ÄëHow to specify entity types and language types?",
    "created_at": "2024-10-24T08:38:10Z",
    "closed_at": "2024-10-26T06:08:14Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/119",
    "body": "When I'm trying to use LightRAG for knowledge graph generation, I encounter two issues: 1. The disease names extracted from the articles have an entity type of 'event', but I want them to be 'disease'. Also, I don't want it to extract personal names. Can I specify the entity types before the LLM extracts entities? 2. For English input, the output is also in English, but I hope it can be in Chinese.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/119/comments",
    "author": "waHhhHao",
    "comments": [
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-24T10:41:02Z",
        "body": "1. Current enetity type is fixed in function `async def extract_entities` in `operate.py` - `entity_types=\",\".join(PROMPTS[\"DEFAULT_ENTITY_TYPES\"])`,  You can add a paprameter assuming named `customized_entity_types: List[str] = None` and change the default `entity_types` to `entity_types=\",\".join(customized_entity_types or PROMPTS[\"DEFAULT_ENTITY_TYPES\"])`. Then you can customize your entity type if you want.\r\n2. You can refer to prompt.tune in Microsoft Graphrag to specify any language, or you can just modify `prompt.py` to replace the English examples to Chinese examples. "
      }
    ]
  },
  {
    "number": 114,
    "title": "Hosted Vector db and KG.  Pinecone/Neo4j",
    "created_at": "2024-10-23T13:47:50Z",
    "closed_at": "2024-10-28T02:00:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/114",
    "body": "Hi, are you able to share your thoughts or recommendations on how to integrate hosted solutions for the VectorDB (adding support for or replacing Nano with Pinecone) and Graph (adding support for or replacing Networkx with Neo4j).  We'd like to plan a production level release.  I'm happy to contribute but just looking for some helpful tips before we get started.  Don't want to contribute to something thats already in flight or already easily configurable.  I didn't see any options for this in the docs but the \"Base\" classes exist in the code base for adaptation and  seamless integration.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/114/comments",
    "author": "wiltshirek",
    "comments": [
      {
        "user": "rcontesti",
        "created_at": "2024-10-23T14:05:11Z",
        "body": "Plus one to the feature. I would be great to use DB of choice."
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-25T11:24:03Z",
        "body": "All our storage implementations are in the `storage.py`. To be honest, since we‚Äôre not very familiar with databases, we haven't yet set up configuration options for swapping databases. Any help you could provide would be greatly appreciated!"
      },
      {
        "user": "wiltshirek",
        "created_at": "2024-10-25T11:41:32Z",
        "body": "Cool.  I'm on it.  biggest difference so far is with the node_ids.  We are using the entity names as the id's for NetworkX but you can't set the node Ids in Neo4J, and probably quite a few others.  I'll keep the concerns separated and account for that internally by leveraging the node_name property, which may arguably be a slightly cleaner approach.  thanks."
      },
      {
        "user": "spo0nman",
        "created_at": "2024-10-25T15:46:43Z",
        "body": "I'm interested in this as well. Please let me know if you need a hand. "
      },
      {
        "user": "wiltshirek",
        "created_at": "2024-10-25T20:36:40Z",
        "body": "Thanks.  The KG support is a bit of work since adding additional KG platforms should not be disruptive to the code base.  Plus, its using Cypher queries opposed to convenient SDK calls available for NetworkX.  I'm making pretty good progress and should be okay to get this out in a week or two depending on my schedule.  But the Vector integration is a good part for you to pick up if that makes sense. Maybe start with Pinecone or whatever you are comfortable with or need for a requirement and just leave a framework that is easy to extend with future hosted vector DB providers.  Pretty sure other folks will jump in and pick up other Vector integrations from there if its close to plug and play.   I'm going with all non-breaking changes to ease adoption of course and following existing code patterns.  Excited to connect on this @spo0nman if you are up for it.   Would love to hear your thoughts.  "
      }
    ]
  },
  {
    "number": 109,
    "title": "Local, Global, and Hybrid Modes Not Working in HF Example",
    "created_at": "2024-10-23T04:32:56Z",
    "closed_at": "2024-10-25T01:04:27Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/109",
    "body": "I'm using the Hugging Face example from the LightRAG repository, and only the naive mode functions correctly. The local, global, and hybrid modes fail to work.\r\n\r\nLLM Model: llam3.1 8b instruct\r\nEmbed Model: all-MiniLM-L12-V2\r\n\r\nAny advice? ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/109/comments",
    "author": "shmily1012",
    "comments": [
      {
        "user": "TianyuFan0504",
        "created_at": "2024-10-23T06:42:32Z",
        "body": "Hi @shmily1012.\r\nWe have understood your question. Due to the diverse response formats of the HF model, we have not yet added optimizations to all models.\r\nYour problem may arise from the following reasons:\r\n1.The reply format of llama8B is not stable and may generate unreadable JSON, resulting in the query process being unable to proceed\r\n2.Llama8B is not strong enough to find enough entity/relation to handle this type of query.\r\n\r\nAfter our testing, models above 14B can generally perform well"
      },
      {
        "user": "DayanaYuan",
        "created_at": "2024-10-23T07:09:17Z",
        "body": "> Hi @shmily1012. We have understood your question. Due to the diverse response formats of the HF model, we have not yet added optimizations to all models. Your problem may arise from the following reasons: 1.The reply format of llama8B is not stable and may generate unreadable JSON, resulting in the query process being unable to proceed 2.Llama8B is not strong enough to find enough entity/relation to handle this type of query.\r\n> \r\n> After our testing, models above 14B can generally perform well\r\n\r\nmy code is follow:WORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,\r\n    llm_model_name=\"/LightRAG/examples/meta-llama/Llama3-8B-instruct\",\r\n    # llm_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=384,\r\n        max_token_size=5000,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\r\n                \"LightRAG/examples/sentence-transformers/all-MiniLM-L6-v2\"\r\n            ),\r\n            embed_model=AutoModel.from_pretrained(\r\n                \"LightRAG/examples/sentence-transformers/all-MiniLM-L6-v2\"\r\n            ),\r\n        ),\r\n    ),\r\n)\r\n\r\n\r\nwith open(\"./book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)\r\n\r\nbut the answer is follow:agraphs\r\n\r\nassistant\r\n\r\nBased on the provided text, some of the top themes in this story are:\r\n\r\n1. **Redemption and Change**: The story explores the theme of redemption as Ebenezer Scrooge, a miserly and bitter character, undergoes a transformation after being visited by the Ghost of Christmas Past, Present, and Yet to Come. Scrooge's experiences lead him to reevaluate his values and behavior, ultimately changing his ways and becoming a kinder and more generous person.\r\n2. **Kindness and Generosity**: The story highlights the importance of kindness and generosity, particularly around Christmas. The character of Bob Cratchit, a poor but kind and hardworking man, is a prime example of this theme. The story also shows how small acts of kindness, such as the Cratchits' humble celebrations and Scrooge's eventual generosity, can bring joy and warmth to those around them.\r\n3. **Social Class and Poverty**: The story touches on the theme of social class and poverty\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.03it/s]\r\nWARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\r\nSorry, I'm not able to provide an answer to that question.\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.58s/it]\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nSorry, I'm not able to provide an answer to that question.\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.03it/s]\r\nWARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\r\n/LightRAG/lightrag/operate.py:990: UserWarning: High Level context is None. Return empty High entity/relationship/source\r\n  warnings.warn(\r\nLightRAG/lightrag/operate.py:998: UserWarning: Low Level context is None. Return empty Low entity/relationship/source\r\n  warnings.warn(\r\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.58s/it]\r\nsystem\r\n\r\n---Role---\r\n\r\nYou are a helpful assistant responding to questions about data in the tables provided.\r\n\r\n\r\n---Goal---\r\n\r\nGenerate a response of the target length and format that responds to the 's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\r\nIf you don't know the answer, just say so. Do not make anything up.\r\nDo not include information where the supporting evidence for it is not provided.\r\n\r\n---Target response length and format---\r\n\r\nMultiple Paragraphs\r\n\r\n---Data tables---\r\n\r\n\r\n-----Entities-----\r\n```csv\r\n\r\n-----Relationships-----\r\n\r\n-----Sources-----\r\n\r\n\r\n\r\nAdd sections and commentary to the response as appropriate for the length and format. Style the response in markdown.\r\n\r\nassistant\r\n\r\nI'm happy to help! However, I don't see any data tables provided. Could you please provide the tables for \"Entities\", \"Relationships\", and \"Sources\" so I can assist you in identifying the top themes in the story?\r\n\r\nËøõÁ®ãÂ∑≤ÁªìÊùü,ÈÄÄÂá∫‰ª£Á†Å0 \r\n\r\nIs this because of a problem with the 3B model?"
      },
      {
        "user": "shmily1012",
        "created_at": "2024-10-23T14:31:11Z",
        "body": "> Hi @shmily1012. We have understood your question. Due to the diverse response formats of the HF model, we have not yet added optimizations to all models. Your problem may arise from the following reasons: 1.The reply format of llama8B is not stable and may generate unreadable JSON, resulting in the query process being unable to proceed 2.Llama8B is not strong enough to find enough entity/relation to handle this type of query.\r\n> \r\n> After our testing, models above 14B can generally perform well\r\n\r\nHi @TianyuFan0504, thanks for your quick response. I am wondering if you can you provide the full name of your 14B model. I really would like to give it a try.\r\nBR,\r\nAlex"
      },
      {
        "user": "shmily1012",
        "created_at": "2024-10-23T14:40:08Z",
        "body": "I will try Llama3.2 11b and Qwen2 14B today, let's see."
      },
      {
        "user": "TianyuFan0504",
        "created_at": "2024-10-24T10:41:40Z",
        "body": "> > Hi @shmily1012. We have understood your question. Due to the diverse response formats of the HF model, we have not yet added optimizations to all models. Your problem may arise from the following reasons: 1.The reply format of llama8B is not stable and may generate unreadable JSON, resulting in the query process being unable to proceed 2.Llama8B is not strong enough to find enough entity/relation to handle this type of query.\r\n> > After our testing, models above 14B can generally perform well\r\n> \r\n> Hi @TianyuFan0504, thanks for your quick response. I am wondering if you can you provide the full name of your 14B model. I really would like to give it a try. BR, Alex\r\n\r\nThere are some models we have tested:llama1b/3b/7b/11b, qwen 14b, minicpm4b, gemma2b, baichuan2b. \r\nOnly qwen  work. : ("
      },
      {
        "user": "shmily1012",
        "created_at": "2024-10-24T17:09:48Z",
        "body": "Qwen2 works on my test bench as well. However, I noticed that the Hugging Face generator currently has a fixed response length of 200 tokens, which may require adjustment depending on our needs.Sent from my iPhoneOn Oct 24, 2024, at 3:42‚ÄØAM, Tianyu Fan ***@***.***> wrote:Ôªø\r\n\r\n\r\nHi @shmily1012. We have understood your question. Due to the diverse response formats of the HF model, we have not yet added optimizations to all models. Your problem may arise from the following reasons: 1.The reply format of llama8B is not stable and may generate unreadable JSON, resulting in the query process being unable to proceed 2.Llama8B is not strong enough to find enough entity/relation to handle this type of query.\r\nAfter our testing, models above 14B can generally perform well\r\n\r\nHi @TianyuFan0504, thanks for your quick response. I am wondering if you can you provide the full name of your 14B model. I really would like to give it a try. BR, Alex\r\n\r\nThere are some models we have tested:llama1b/3b/7b/11b, qwen 14b, minicpm4b, gemma2b, baichuan2b.\r\nOnly qwen  work. : (\r\n\r\n‚ÄîReply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>"
      },
      {
        "user": "shmily1012",
        "created_at": "2024-10-25T01:04:49Z",
        "body": "Qwen2 14B works with HF mode."
      },
      {
        "user": "amenhere",
        "created_at": "2025-01-17T03:04:47Z",
        "body": "ÊàëÊµãËØï‰∫Üqwen-vl-plusÔºåglm-4-flashÔºågpt-4-turboÂíågpt-4o-mini-2024-07-18ÈÉΩÊúâÂèØËÉΩÂá∫Áé∞Ëøô‰∏™ÈóÆÈ¢òÔºåËÄå‰∏îÈ¢ëÁéáÂæàÈ´ò„ÄÇ\nÊàëÊèêÂá∫ÁöÑÁöÑÈóÆÈ¢òÂ¶Ç‰∏ãÔºö\n\npromptÔºö\nËØ∑Ê†πÊçÆ‰ª•‰∏ãÊØè‰∏™Ê†áÁ≠æÂêéÁöÑÊèèËø∞ÔºåÂáÜÁ°ÆÊèêÂèñÂêàÂêå‰∏≠ÊØè‰∏™ÂÆû‰ΩìÁöÑÂØπÂ∫î‰ø°ÊÅØÔºö\nËßÑÂàôÔºö\n1„ÄÅÂΩìÂâçÂêàÂêå‰∏≠Ê≤°ÊúâËøô‰∏™Ê†áÁ≠æÂÆû‰ΩìÊó∂ÂèØ‰ª•Áî®\"None\"Â°´ÂÜôÔºõ\n2„ÄÅÁî®‰∏≠ÊñáËæìÂá∫Ôºõ\n3„ÄÅÂÆû‰Ωì‰ø°ÊÅØÂøÖÈ°ªÊòØÂêàÂêå‰∏≠ÁöÑÂéüÊñáÔºå‰∏çÂèØ‰ª•ËøõË°åÁºñÈÄ†ÔºÅÔºÅÔºÅ\n4„ÄÅ‰∏çË¶ÅÂá∫Áé∞Êó†ÂÖ≥ÁöÑÊèêÁ§∫ËØçÔºåÂ¶ÇÔºöÊàëËÆ§‰∏∫ÔºåÊÄªÁªìÔºåÂèØËÉΩÔºåÁ≠âÁ≠âÔºõ\n5„ÄÅÁî®jsonÁöÑÊ†ºÂºèËøîÂõûÂÆû‰ΩìÂ≠óÂÖ∏Ë°®Âíå‰ªñÂØπÂ∫îÁöÑÂÆû‰Ωì‰ø°ÊÅØÔºå‰æãÂ¶ÇÔºö{\"ÂÆû‰ΩìÊ†áÁ≠æ\":\"ÂÆû‰ΩìÂÜÖÂÆπ\"}Ôºõ\n\n####Ê†áÁ≠æËß£Èáä\nÂêàÂêåÁºñÂè∑: ÂêàÂêåÁöÑÂîØ‰∏ÄÊ†áËØÜÁ¨¶ÔºåÈÄöÂ∏∏‰∏∫‰∏Ä‰∏≤Êï∞Â≠óÊàñÂ≠óÊØçÁªÑÂêà„ÄÇÁî®‰∫éÂå∫ÂàÜ‰∏çÂêåÂêàÂêå„ÄÇ\nÂêàÂêåÂêçÁß∞: ÂêàÂêåÁöÑÊ≠£ÂºèÂêçÁß∞ÊàñÊ†áÈ¢òÔºåÈÄöÂ∏∏ÂåÖÊã¨ÂêàÂêåÁ±ªÂûã„ÄÅÁ≠æÁ∫¶ÊñπÊàñÈ°πÁõÆÂêçÁß∞Á≠â‰ø°ÊÅØ„ÄÇ\nÁ≠æËÆ¢Êó•Êúü: ÂêàÂêåËææÊàêÁöÑÊó•ÊúüÔºåÈÄöÂ∏∏ÊòØÁî≤ÊñπÂíå‰πôÊñπÂèåÊñπÁ≠æÁΩ≤ÂêàÂêåÁöÑÂÖ∑‰ΩìÊó•Êúü„ÄÇ\nÂêàÂêåÊúâÊïàÊúü: ÂêàÂêåÂú®Ê≥ïÂæã‰∏äÁîüÊïàÂπ∂ÊåÅÁª≠ÊúâÊïàÁöÑÊó∂Èó¥ÊÆµÔºåÈÄöÂ∏∏ÂåÖÊã¨ÂºÄÂßãÊó•ÊúüÂíåÁªìÊùüÊó•Êúü„ÄÇ\nÂêàÂêå‰∏ªË¶ÅÂÜÖÂÆπ: ÂêàÂêåÁöÑÊ†∏ÂøÉÊù°Ê¨æÊàñÊ∂âÂèäÁöÑ‰∏ªË¶Å‰∫ãÈ°πÔºåÁÆÄË¶ÅÊ¶ÇËø∞ÂêàÂêåÁöÑÂü∫Êú¨Á∫¶ÂÆö„ÄÇ\nÊúçÂä°ÂÜÖÂÆπ: ‰πôÊñπÊèê‰æõÁöÑÊúçÂä°ÊàñÁî≤ÊñπË¶ÅÊ±Ç‰πôÊñπÂÆåÊàêÁöÑ‰ªªÂä°ÂíåÂ∑•‰ΩúÂÜÖÂÆπ„ÄÇ\nÁî≤ÊñπÂêçÁß∞: ÂêàÂêå‰∏≠ÁöÑ‰∏ÄÊñπÔºåÈÄöÂ∏∏‰∏∫ÂèëËµ∑ÊñπÊàñÂßîÊâòÊñπÔºåÂ∏∏ËßÅ‰∫éË¥≠‰π∞„ÄÅÂêà‰ΩúÊàñÂßîÊâòÁ≠âÂêàÂêå„ÄÇ\n‰πôÊñπÂêçÁß∞: ÂêàÂêå‰∏≠ÁöÑÂè¶‰∏ÄÊñπÔºåÈÄöÂ∏∏‰∏∫Êé•ÂèóÊñπÊàñÊúçÂä°Êèê‰æõÊñπ„ÄÇ\nÁî≤ÊñπÁ∫≥Á®é‰∫∫ËØÜÂà´Âè∑: Áî≤ÊñπÂú®Á®éÂä°Á≥ªÁªü‰∏≠ÁöÑÂîØ‰∏ÄËØÜÂà´Âè∑Á†ÅÔºåÁî®‰∫éÁ®éÂä°ËØÜÂà´„ÄÇ\n‰πôÊñπÁ∫≥Á®é‰∫∫ËØÜÂà´Âè∑: ‰πôÊñπÂú®Á®éÂä°Á≥ªÁªü‰∏≠ÁöÑÂîØ‰∏ÄËØÜÂà´Âè∑Á†ÅÔºåÁî®‰∫éÁ®éÂä°ËØÜÂà´„ÄÇ\nÁî≤ÊñπÂú∞ÂùÄ: Áî≤ÊñπÁöÑÊ≥®ÂÜåÂú∞ÂùÄÊàñ‰∏ªË¶ÅÂäûÂÖ¨Âú∞ÁÇπÔºåÈÄöÂ∏∏‰∏∫Ê≥ï‰∫∫Ê≥®ÂÜåÂú∞ÊàñËÅîÁ≥ªÂú∞ÂùÄ„ÄÇ\n‰πôÊñπÂú∞ÂùÄ: ‰πôÊñπÁöÑÊ≥®ÂÜåÂú∞ÂùÄÊàñ‰∏ªË¶ÅÂäûÂÖ¨Âú∞ÁÇπÔºåÈÄöÂ∏∏‰∏∫Ê≥ï‰∫∫Ê≥®ÂÜåÂú∞ÊàñËÅîÁ≥ªÂú∞ÂùÄ„ÄÇ\nÁî≤ÊñπÁîµËØù: Áî≤ÊñπÁöÑËÅîÁ≥ªÊñπÂºèÔºåÈÄöÂ∏∏ÊòØÁîµËØùÂè∑Á†ÅÔºåÁî®‰∫éÊó•Â∏∏Ê≤üÈÄö„ÄÇ\n‰πôÊñπÁîµËØù: ‰πôÊñπÁöÑËÅîÁ≥ªÊñπÂºèÔºåÈÄöÂ∏∏ÊòØÁîµËØùÂè∑Á†ÅÔºåÁî®‰∫éÊó•Â∏∏Ê≤üÈÄö„ÄÇ\nËøùÁ∫¶Ë¥£‰ªª: ÂêàÂêå‰∏≠ËßÑÂÆöÁöÑËøùÁ∫¶Êù°Ê¨æÔºåÊèèËø∞ÂΩì‰∫ã‰∏ÄÊñπÊú™Â±•Ë°åÂêàÂêå‰πâÂä°Êó∂Â∫îÊâøÊãÖÁöÑË¥£‰ªª„ÄÇ\n‰∫âËÆÆËß£ÂÜ≥ÊñπÂºè: ÂΩìÂêàÂêåÂèåÊñπÂèëÁîü‰∫âËÆÆÊó∂ÔºåÁ∫¶ÂÆöÁöÑËß£ÂÜ≥ÊñπÂºèÔºåÂ¶Ç‰ª≤Ë£Å„ÄÅËØâËÆºÊàñË∞ÉËß£Á≠â„ÄÇ\n\n####ÂÆû‰ΩìÂ≠óÂÖ∏\n{\n    ÂêàÂêåÁºñÂè∑:\n    ÂêàÂêåÂêçÁß∞:\n    Á≠æËÆ¢Êó•Êúü:\n    ÂêàÂêåÊúâÊïàÊúü:\n    ÂêàÂêå‰∏ªË¶ÅÂÜÖÂÆπ:\n    ÊúçÂä°ÂÜÖÂÆπ:\n    Áî≤ÊñπÂêçÁß∞:\n    ‰πôÊñπÂêçÁß∞:\n    Áî≤ÊñπÁ∫≥Á®é‰∫∫ËØÜÂà´Âè∑:\n    ‰πôÊñπÁ∫≥Á®é‰∫∫ËØÜÂà´Âè∑:\n    Áî≤ÊñπÂú∞ÂùÄ:\n    ‰πôÊñπÂú∞ÂùÄ:\n    Áî≤ÊñπÁîµËØù:\n    ‰πôÊñπÁîµËØù:\n    ËøùÁ∫¶Ë¥£‰ªª:\n    ‰∫âËÆÆËß£ÂÜ≥ÊñπÂºè:\n}"
      }
    ]
  },
  {
    "number": 107,
    "title": "fix(lightrag_siliconcloud_demo.py): max_token_size",
    "created_at": "2024-10-23T03:26:30Z",
    "closed_at": "2024-10-23T03:32:59Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/107",
    "body": "bce Â∫ïÂ±ÇÁî®ÁöÑ 512 token ÔºàËΩ¨‰∏≠ÊñáÂ§ßÁ∫¶ 1.5 ÂÄçÔºâ\r\n\r\nÂèÇÊï∞ assign Èîô‰∫ÜÔºåÁªôÂ§™Èïø‰ºöÊä•Èîô„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/107/comments",
    "author": "tpoisonooo",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T03:32:52Z",
        "body": "Â•ΩÁöÑÔºåÊàëÁé∞Âú®ÂêàÂπ∂‰∏Ä‰∏ã"
      }
    ]
  },
  {
    "number": 106,
    "title": "Add Support for Production Database Storage",
    "created_at": "2024-10-23T00:38:29Z",
    "closed_at": "2024-10-28T01:56:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/106",
    "body": "# Add Support for Production Database Storage \r\n\r\n## Current State\r\nLightRAG stores vectors and documents locally, which limits production deployment capabilities.\r\n\r\n## Request\r\nAdd support for production databases:\r\n- PostgreSQL + pgvector for vector storage\r\n- Document storage with proper DB backend\r\n- Connection pooling and async support\r\n- Basic monitoring\r\n\r\n## Use Case\r\nWe're building a legal research tool and need to:\r\n- Handle large document collections\r\n- Support concurrent users\r\n- Enable cloud deployment\r\n- Ensure data persistence\r\n\r\n## Questions\r\n1. Would you be interested in this feature?\r\n2. Any preferences for database backends?\r\n\r\nHappy to contribute to implementation if there's interest.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/106/comments",
    "author": "proBhavesh",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T04:05:26Z",
        "body": "We actually really need this feature, but since we're not familiar with databases, we haven't implemented it yet. We truly appreciate your offer to help!"
      },
      {
        "user": "wiltshirek",
        "created_at": "2024-10-24T12:51:47Z",
        "body": "> # Add Support for Production Database Storage\r\n> ## Current State\r\n> LightRAG stores vectors and documents locally, which limits production deployment capabilities.\r\n> \r\n> ## Request\r\n> Add support for production databases:\r\n> \r\n> * PostgreSQL + pgvector for vector storage\r\n> * Document storage with proper DB backend\r\n> * Connection pooling and async support\r\n> * Basic monitoring\r\n> \r\n> ## Use Case\r\n> We're building a legal research tool and need to:\r\n> \r\n> * Handle large document collections\r\n> * Support concurrent users\r\n> * Enable cloud deployment\r\n> * Ensure data persistence\r\n> \r\n> ## Questions\r\n> 1. Would you be interested in this feature?\r\n> 2. Any preferences for database backends?\r\n> \r\n> Happy to contribute to implementation if there's interest.\r\n\r\nI'm looking into using Neo4J to replace NetworkX.  Happy to contribute as well.  Just need some advice on directed vs undirected graphs.  Looks like we should require directed for a production scenario using this algo.  Any feedback is helpful.  "
      }
    ]
  },
  {
    "number": 105,
    "title": "AttributeError: module 'ollama' has no attribute 'embeddings'",
    "created_at": "2024-10-22T23:18:40Z",
    "closed_at": "2024-10-25T11:17:23Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/105",
    "body": "I'm running the following adaptation to the ollama example:\r\n\r\n```\r\nimport os\r\n\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\n\r\nWORKING_DIR = \"light_rag/dickens\"\r\nMODEL_NAME = \"llama3.2:3b\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=ollama_model_complete,\r\n    llm_model_name=MODEL_NAME,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=768,\r\n        max_token_size=8192,\r\n        func=lambda texts: ollama_embedding(texts, embed_model=\"nomic-embed-text\"),\r\n    ),\r\n)\r\n\r\n\r\nwith open(\"light_rag/dickens/book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)\r\n```\r\n\r\nUnfortunately, I'm getting the following error:\r\n\r\n```\r\nINFO:lightrag:Logger initialized for working directory: light_rag/dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = light_rag/dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 768, 'max_token_size': 8192, 'func': <function <lambda> at 0x1268d1440>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function ollama_model_complete at 0x142289800>,\r\n  llm_model_name = llama3.2:3b,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x141af6980>\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'light_rag/dickens/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'light_rag/dickens/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'light_rag/dickens/vdb_chunks.json'} 0 data\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 42 chunks\r\nINFO:lightrag:Inserting 42 vectors to chunks\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"light_rag/src/example.py\", line 26, in <module>\r\n    rag.insert(f.read())\r\n  File \"site-packages/lightrag/lightrag.py\", line 162, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"site-packages/lightrag/lightrag.py\", line 206, in ainsert\r\n    await self.chunks_vdb.upsert(inserting_chunks)\r\n  File \"site-packages/lightrag/storage.py\", line 92, in upsert\r\n    embeddings_list = await asyncio.gather(\r\n                      ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/site-packages/lightrag/utils.py\", line 87, in wait_func\r\n    result = await func(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"site-packages/lightrag/utils.py\", line 43, in __call__\r\n    return await self.func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"site-packages/lightrag/llm.py\", line 421, in ollama_embedding\r\n    data = ollama.embeddings(model=embed_model, prompt=text)\r\n           ^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'ollama' has no attribute 'embeddings'\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/105/comments",
    "author": "rcontesti",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T04:01:22Z",
        "body": "You can try creating a new virtual environment and then install LightRAG in it."
      },
      {
        "user": "rcontesti",
        "created_at": "2024-10-23T13:44:06Z",
        "body": "> You can try creating a new virtual environment and then install LightRAG in it.\r\n\r\nThanks, I was able to solve it that way. I'm surprised that it works that way since in both env I have installed the same ollama version. Any clues of what might be going on with ollama? "
      }
    ]
  },
  {
    "number": 98,
    "title": "Fix encoding error when running demo in some occasions",
    "created_at": "2024-10-22T08:53:03Z",
    "closed_at": "2024-10-23T03:01:29Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/98",
    "body": "When running demos under `examples` directory, may encounter `UnicodeDecodeError` at the code below\r\n```python\r\nwith open(\"./book.txt\") as f:\r\n    rag.insert(f.read())\r\n```\r\nthis frustrating error frequently happens on windows, but never happens on Linux.\r\nI've added `encoding` param to avoid it and apply such change to all demos.\r\n```python\r\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\r\n    rag.insert(f.read())\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/98/comments",
    "author": "JavieHush",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T03:01:34Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 95,
    "title": "'NoneType' object is not subscriptable",
    "created_at": "2024-10-22T07:21:24Z",
    "closed_at": "2024-10-28T01:55:37Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/95",
    "body": "/home/horanchen/anaconda3/envs/lightrag/bin/python /home/horanchen/ydy/study/code/LightRAG/examples/lightrag_hf_demo.py\r\nINFO:lightrag:Logger initialized for working directory: ./dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = ./dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 1, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 384, 'max_token_size': 1000, 'func': <function <lambda> at 0x7c8bd0bf7010>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function hf_model_complete at 0x7c8ace6c1510>,\r\n  llm_model_name = /home/horanchen/ydy/study/code/LightRAG/examples/meta-llama/Llama3-8B-instruct,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x7c8ace6b2d40>\r\n\r\nINFO:lightrag:Load KV full_docs with 1 data\r\nINFO:lightrag:Load KV text_chunks with 1 data\r\nINFO:lightrag:Load KV llm_response_cache with 41 data\r\nINFO:lightrag:Loaded graph from ./dickens/graph_chunk_entity_relation.graphml with 18 nodes, 11 edges\r\nINFO:nano-vectordb:Load (18, 384) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_entities.json'} 18 data\r\nINFO:nano-vectordb:Load (11, 384) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_relationships.json'} 11 data\r\nINFO:nano-vectordb:Load (43, 384) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './dickens/vdb_chunks.json'} 43 data\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nWARNING:lightrag:All docs are already in the storage\r\nINFO:lightrag:Writing graph with 18 nodes, 11 edges\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nTraceback (most recent call last):\r\n  File \"/home/horanchen/ydy/study/code/LightRAG/examples/lightrag_hf_demo.py\", line 39, in <module>\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n  File \"/home/horanchen/ydy/study/code/LightRAG/lightrag/lightrag.py\", line 244, in query\r\n    return loop.run_until_complete(self.aquery(query, param))\r\n  File \"/home/horanchen/anaconda3/envs/lightrag/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/home/horanchen/ydy/study/code/LightRAG/lightrag/lightrag.py\", line 278, in aquery\r\n    response = await naive_query(\r\n  File \"/home/horanchen/ydy/study/code/LightRAG/lightrag/operate.py\", line 1052, in naive_query\r\n    maybe_trun_chunks = truncate_list_by_token_size(\r\n  File \"/home/horanchen/ydy/study/code/LightRAG/lightrag/utils.py\", line 172, in truncate_list_by_token_size\r\n    tokens += len(encode_string_by_tiktoken(key(data)))\r\n  File \"/home/horanchen/ydy/study/code/LightRAG/lightrag/operate.py\", line 1054, in <lambda>\r\n    key=lambda x: x[\"content\"],\r\nTypeError: 'NoneType' object is not subscriptable\r\n\r\n\r\nHow can this be changed?\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/95/comments",
    "author": "DayanaYuan",
    "comments": [
      {
        "user": "TerraceCN",
        "created_at": "2024-10-22T07:48:48Z",
        "body": "Try deleting all files in the working directory, reconstructing the graph and vectors"
      },
      {
        "user": "DayanaYuan",
        "created_at": "2024-10-22T08:02:53Z",
        "body": "> Try deleting all files in the working directory, reconstructing the graph and vectors\r\n\r\nWhen I use all the contents of the book, it appears ''out of memory''. What parameter can be modified to make this error disappear without changing the data?"
      },
      {
        "user": "TerraceCN",
        "created_at": "2024-10-22T08:11:18Z",
        "body": "> > Try deleting all files in the working directory, reconstructing the graph and vectors\r\n> \r\n> When I use all the contents of the book, it appears ''out of memory''. What parameter can be modified to make this error disappear without changing the data?\r\n\r\nTry lowering the following parameters:\r\n\r\n- `embedding_batch_num`\r\n- `embedding_func_max_async`\r\n- `llm_model_max_token_size`\r\n- `llm_model_max_async`\r\n\r\nand `top_k` of `QueryParam`"
      },
      {
        "user": "DayanaYuan",
        "created_at": "2024-10-22T08:31:08Z",
        "body": "> > > Try deleting all files in the working directory, reconstructing the graph and vectors\r\n> > \r\n> > \r\n> > When I use all the contents of the book, it appears ''out of memory''. What parameter can be modified to make this error disappear without changing the data?\r\n> \r\n> Try lowering the following parameters:\r\n> \r\n> * `embedding_batch_num`\r\n> * `embedding_func_max_async`\r\n> * `llm_model_max_token_size`\r\n> * `llm_model_max_async`\r\n> \r\n> and `top_k` of `QueryParam`\r\n\r\n''Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.''              Will this ''Default to no truncation'' make a difference?"
      },
      {
        "user": "TerraceCN",
        "created_at": "2024-10-22T09:18:47Z",
        "body": "> ''Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.'' Will this ''Default to no truncation'' make a difference?\r\n\r\nBased on my observations of the code, this could be potentially problematic.\r\n\r\nThe `sentence-transformers/all-MiniLM-L6-v2` model used in the sample only supports a maximum length of 512, whereas the default `chunk_token_size` is 1200, and the encode and decode operations on the tokens are performed using a gpt-40 tokenizer. tokenizer.I think this may cause the input text length to exceed the acceptable length of the model and lead to miscalculation of embedding or loss of information.\r\n\r\nIt is recommended that you replace the embedding model with a longer input length (e.g. bge-m3), or reduce the `chunk_token_size`."
      },
      {
        "user": "DayanaYuan",
        "created_at": "2024-10-23T09:27:46Z",
        "body": "> > ''Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.'' Will this ''Default to no truncation'' make a difference?\r\n> \r\n> Based on my observations of the code, this could be potentially problematic.\r\n> \r\n> The `sentence-transformers/all-MiniLM-L6-v2` model used in the sample only supports a maximum length of 512, whereas the default `chunk_token_size` is 1200, and the encode and decode operations on the tokens are performed using a gpt-40 tokenizer. tokenizer.I think this may cause the input text length to exceed the acceptable length of the model and lead to miscalculation of embedding or loss of information.\r\n> \r\n> It is recommended that you replace the embedding model with a longer input length (e.g. bge-m3), or reduce the `chunk_token_size`.\r\n\r\nThe html knowledge graph is shown below, but this looks like a problem, what is the problem?\r\n![Uploading ÂæÆ‰ø°ÂõæÁâá_20241023172715.png‚Ä¶]()\r\n\r\n"
      },
      {
        "user": "DayanaYuan",
        "created_at": "2024-10-23T09:29:37Z",
        "body": "![Uploading ÂæÆ‰ø°ÂõæÁâá_20241023172715.png‚Ä¶]()\r\n"
      },
      {
        "user": "benx13",
        "created_at": "2024-11-06T02:34:55Z",
        "body": "experiencing the same issue using gpt-4o-mini and text-embeddings3-small. Requesting this issue to be reopened @LarFii "
      }
    ]
  },
  {
    "number": 94,
    "title": "out of meomery",
    "created_at": "2024-10-22T07:11:27Z",
    "closed_at": "2024-10-28T01:55:16Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/94",
    "body": "Change which parameters can keep the program running without running out of memory?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/94/comments",
    "author": "DayanaYuan",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-26T06:06:20Z",
        "body": "You might need to switch to a model with fewer parameters."
      }
    ]
  },
  {
    "number": 93,
    "title": "Stream llm call",
    "created_at": "2024-10-22T07:01:59Z",
    "closed_at": "2024-10-25T11:15:40Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/93",
    "body": "can you add streaming llm call ?\r\n\r\nÊòØÂê¶ËÉΩÊ∑ªÂä†Â§ßÊ®°ÂûãÊµÅÂºèË∞ÉÁî®Ôºü",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/93/comments",
    "author": "illikea",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T03:33:48Z",
        "body": "ÂêéÈù¢ÂèØËÉΩ‰ºöÊ∑ªÂä†Ëøô‰∏™ÂäüËÉΩ"
      }
    ]
  },
  {
    "number": 90,
    "title": "Seperate nano vector DB from LightRAG (hosting solution)",
    "created_at": "2024-10-22T02:37:05Z",
    "closed_at": "2024-10-25T11:15:22Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/90",
    "body": "nano vector db runs in memory just like any db. Its better to have it seperated from LightRAG because we need to host the DB and use LightRAG as a library. LightRAG should be stateless and lightweight..\r\n\r\nIm not sure if nano vector db is actually stateless",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/90/comments",
    "author": "Yazington",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T03:29:36Z",
        "body": "We're working on it‚Äîstay tuned!"
      },
      {
        "user": "wiltshirek",
        "created_at": "2024-10-23T20:15:25Z",
        "body": "Will what you are working on include integrations for alternative vector and graph solutions, preferably hosted (neo4j and pinecone).  Sorry for the similar qry on separate threads.  Happy to pick this up and contribute since its a feature we'll need to take this to prod. "
      }
    ]
  },
  {
    "number": 79,
    "title": "Multifile processing, prompts config",
    "created_at": "2024-10-21T09:41:47Z",
    "closed_at": "2025-01-16T05:15:52Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/79",
    "body": "Currently WIP!\r\n\r\nPrompts moved to toml config files, so later prompts can be easily tuned for concrete task. Source code prompt config added to prompts/code.tom\r\n\r\nThere're some doubts regarding entities capitalization, currently I switched it of, cos for code it harmful I think, mb it's nice to control by parameter, cos it looks like it beneficial for natural language.\r\n\r\nEvery chunk embeded with metadata now, inserted text has !none in that fields\r\n```bash\r\n####\r\n## FILENAME: README.md\r\n## FILEPATH: ./README.md\r\n## CHUNK_NUM: 0\r\n###\r\n```\r\n\r\ninsert accepts either text or Path object, or list of one of them:\r\n```python\r\ndef insert(self, input_data: Union[str, os.PathLike, List[Union[str, os.PathLike]]])\r\n```\r\n\r\nFile name, path and chunk number added in entity and relationship extraction.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/79/comments",
    "author": "ivs",
    "comments": [
      {
        "user": "Jaykumaran",
        "created_at": "2024-11-04T16:16:27Z",
        "body": "You should have synced fork the main branch before creating PR, as there are merge conflicts"
      }
    ]
  },
  {
    "number": 78,
    "title": "Fix example in README",
    "created_at": "2024-10-21T09:35:57Z",
    "closed_at": "2024-10-21T12:38:59Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/78",
    "body": "Fix the example in the README to make it work without any changes.\r\n\r\n- add os import\r\n- rm duplicate working dir definition",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/78/comments",
    "author": "sebastianschramm",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-21T12:39:02Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 77,
    "title": "Does it support streaming output?",
    "created_at": "2024-10-21T09:29:02Z",
    "closed_at": "2024-10-25T09:44:26Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/77",
    "body": "If supported, how should I proceed? Please help me.\r\nIf not supported, will it be supported later?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/77/comments",
    "author": "redpintings",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T03:21:03Z",
        "body": "We do not support streaming output at the moment. It might be added in the future, but it will take some time as we have many other tasks to work on right now."
      }
    ]
  },
  {
    "number": 69,
    "title": "ÁºñÁ†ÅÈóÆÈ¢ò",
    "created_at": "2024-10-20T12:51:20Z",
    "closed_at": "2024-10-21T07:06:59Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/69",
    "body": "ËøêË°åÁéØÂ¢ÉÔºöWin11\r\nPythonÁâàÊú¨Ôºö3.10.10\r\n\r\nÊûÑÂª∫ÂÆåÊàêÂêéÂÜçÊ¨°ËøêË°åËøõË°åÊü•ËØ¢Êó∂‰ºöÊä•ÈîôÔºö\r\n```bash\r\nÂèëÁîüÂºÇÂ∏∏: UnicodeDecodeError\r\n'gbk' codec can't decode byte 0xa5 in position 372: illegal multibyte sequence\r\n  File \"D:\\LLM\\LightRAG\\lightrag\\storage.py\", line 68, in __post_init__\r\n    self._client = NanoVectorDB(\r\n  File \"D:\\LLM\\LightRAG\\lightrag\\lightrag.py\", line 138, in __post_init__\r\n    self.entities_vdb = self.vector_db_storage_cls(\r\n  File \"D:\\LLM\\LightRAG\\examples\\lightrag_openai_compatible_demo.py\", line 36, in <module>\r\n    rag = LightRAG(\r\nUnicodeDecodeError: 'gbk' codec can't decode byte 0xa5 in position 372: illegal multibyte sequence\r\n```\r\nÈîôËØØÁöÑ‰ª£Á†ÅÊòØËøôÈáåÔºö\r\n```python\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1792,\r\n        max_token_size=8192,\r\n        func=embedding_func\r\n    ),\r\n)\r\n```\r\n\r\n‰øÆÊîπÈÉ®ÂàÜjsonÊñá‰ª∂‰∏∫`gbk`ÁºñÁ†ÅÂêéËÉΩÊ≠£Â∏∏Êü•ËØ¢Ôºö\r\n```python\r\n# Â∞Ü‰ª•‚Äúvdb_‚ÄùÂºÄÂ§¥ÁöÑjsonÊñá‰ª∂Êîπ‰∏∫gbkÁºñÁ†Å\r\nfor file in os.listdir(WORKING_DIR):\r\n    if file.startswith(\"vdb_\") and file.endswith(\".json\"):\r\n        with open(os.path.join(WORKING_DIR, file), \"r\", encoding=\"utf-8\") as f:\r\n            data = f.read()\r\n        with open(os.path.join(WORKING_DIR, file), \"w\", encoding=\"gbk\") as f:\r\n            f.write(data)\r\n```\r\n\r\nÊöÇÊó∂‰∏çÊ∏ÖÊ•öÊâãÂä®‰øÆÊîπÂêéÊòØÂê¶‰ºöÂØºËá¥ÂêéÁª≠ÁªßÁª≠Ê∑ªÂä†Êó∂ÂØºËá¥ÂÖ∂‰ªñÈóÆÈ¢ò",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/69/comments",
    "author": "lingyezhixing",
    "comments": [
      {
        "user": "QinQG",
        "created_at": "2024-10-21T01:57:39Z",
        "body": "ÊääÂÜÖÂÆπÂ§çÂà∂Âà∞‰∏Ä‰∏™Êñ∞ÁöÑtxtÊñá‰ª∂ÈáåÈù¢Ôºå‰øùÂ≠òÂç≥ÂèØ"
      },
      {
        "user": "240839785",
        "created_at": "2024-11-29T16:26:48Z",
        "body": "‰ΩøÁî®ubuntuÂèØ‰ª•Ëß£ÂÜ≥"
      }
    ]
  },
  {
    "number": 67,
    "title": "‰∏≠ÊñáÊèêÁ§∫ËØç",
    "created_at": "2024-10-20T12:31:15Z",
    "closed_at": "2024-10-21T07:06:48Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/67",
    "body": "ËÉΩÂê¶Ê∑ªÂä†‰∏≠ÊñáÊèêÁ§∫ËØçÔºåÁõÆÂâçÁöÑËã±ÊñáÊèêÁ§∫ËØçÂú®Â§ÑÁêÜ‰∏≠ÊñáÊó∂Â§ßÈÉ®ÂàÜÊÉÖÂÜµ‰∏ã‰πüËÉΩÊúâ‰∏çÈîôÁöÑÊïàÊûúÔºå‰ΩÜÊòØÂú®‰ΩøÁî®Êú¨Âú∞Â∞èÊ®°ÂûãÊØîÂ¶ÇQwen2.5-7B-InstructÂíåQwen2.5-3B-InstructÊó∂Âá∫Áé∞‰∫ÜÈÅóÊºè‰ø°ÊÅØÁöÑÊÉÖÂÜµÔºåÂèØËÉΩÊòØÂ∞èÊ®°ÂûãËÉΩÂäõÊú¨Êù•Â∞±‰∏çÂº∫ÂÜçÂä†Ë∑®ËØ≠Ë®ÄÂ∞±‰ºöÂá∫ÈóÆÈ¢òÔºåÊàëËá™Â∑±ÊîπÁöÑ‰∏≠ÊñáÊèêÁ§∫ËØçÊïàÊûúÂπ∂‰∏çÂ•ΩÔºåÊúâÊó∂‰ºöÂá∫Áé∞‰∏Ä‰∫õËé´ÂêçÂÖ∂Â¶ôÁöÑÈóÆÈ¢ò„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/67/comments",
    "author": "lingyezhixing",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-21T05:56:19Z",
        "body": "‰∏≠ÊñáÊèêÁ§∫ËØç‰πüÊòØÊàë‰ª¨ÂêéÁª≠Â∑•‰Ωú‰πã‰∏ÄÔºå‰ΩÜÊòØÁî±‰∫é‰∫∫ÊâãÊúâÈôêÂèØËÉΩÈúÄË¶Å‰∏ÄÂÆöÊó∂Èó¥„ÄÇÂú®indexÈò∂ÊÆµÔºåÊõ¥Â§ßÁöÑÂΩ±ÂìçÂõ†Á¥†ËøòÊòØÊ®°ÂûãÊú¨Ë∫´ÁöÑËÉΩÂäõÔºåÂì™ÊÄïÊòØ4oÂíåminiÂ∑ÆË∑ù‰πüÂ∑®Â§ßÔºåÊâÄ‰ª•Êõ¥Êç¢ÊèêÁ§∫ËØçÂæàÂèØËÉΩ‰∏çËÉΩËµ∑Âà∞È¢ÑÊúüÁöÑÊïàÊûú„ÄÇ"
      }
    ]
  },
  {
    "number": 64,
    "title": "'update'",
    "created_at": "2024-10-20T04:01:46Z",
    "closed_at": "2024-10-20T10:06:34Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/64",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/64/comments",
    "author": "hanbin49",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-20T10:06:05Z",
        "body": "Thanks for your demo!"
      }
    ]
  },
  {
    "number": 63,
    "title": "Local ollama service stops responding after ~1 hour - GPU VRAM usage timeout issue",
    "created_at": "2024-10-20T02:37:14Z",
    "closed_at": "2024-10-21T05:56:42Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/63",
    "body": "Description:\r\nWhen using LightRAG with a local ollama service, the responses stop coming in after running for about 1 hour or sometimes less. Upon stopping LightRAG, the ollama service logs the following warning repeatedly:\r\n`time=2024-10-20T10:28:14,237+08:00 level=WARN source=sched.go:646 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.123168046 model=/root/.ollama/models/blobs/sha256-5ee4f07cdhgbeadbbb293e85803c56gbolbd37ed059d2715fa7bb405f3lcaa\r\ntime=2024-10-20T10:28:14.488+08:00 level=WARN source=sched.go:646 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.373610908 model=/root/.ollama/models/blobs/sha256-5ee4f07cdbgbeadbbb293e85803c569bolbd37ed059d2715faa7bb405f3lcaa6IlA\r\ntime=2024-10-20T10:28:14.737+08:00 level=WARN source=sched.go:646 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.623228477 model=/root/.ollama/models/blobs/sha256-5ee4f07cdbcbeadbbb293e85803c56gb01bd37ed059d2715faa7bb405f3lcaa6`\r\nHere's my lightrag_ollama_demo.py:\r\n`import os\r\n\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\n\r\nWORKING_DIR = \"./dickens\"\r\nTEXT_FILES_DIR = \"/llm/mt\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=ollama_model_complete,\r\n    llm_model_name=\"qwen2.5:3b-instruct-max-context\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=768,\r\n        max_token_size=8192,\r\n        func=lambda texts: ollama_embedding(texts, embed_model=\"nomic-embed-text\"),\r\n    ),\r\n)\r\n\r\n# ËØªÂèñ TEXT_FILES_DIR ÁõÆÂΩï‰∏ãÊâÄÊúâÁöÑ .txt Êñá‰ª∂\r\ntexts = []\r\nfor filename in os.listdir(TEXT_FILES_DIR):\r\n    if filename.endswith('.txt'):\r\n        file_path = os.path.join(TEXT_FILES_DIR, filename)\r\n        with open(file_path, 'r', encoding='utf-8') as file:\r\n            texts.append(file.read())\r\n\r\n# ÊâπÈáèÊèíÂÖ•ÊñáÊú¨Âà∞ LightRAG\r\nrag.insert(texts)\r\n#with open(\"./pc.txt\") as f:\r\n#    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)`",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/63/comments",
    "author": "240839785",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-20T10:11:16Z",
        "body": "We add a demo `vram_management_demo.py` in `examples\\`, you can try it."
      },
      {
        "user": "240839785",
        "created_at": "2024-10-20T10:16:41Z",
        "body": "> We add a demo `vram_management_demo.py` in `examples\\`, you can try it.\r\n\r\nI thank you very much for your prompt reply, I am currently using deepseek's api, I will try it later!"
      }
    ]
  },
  {
    "number": 61,
    "title": "Code submission: Working with multiple llms to distribute the load",
    "created_at": "2024-10-19T22:10:48Z",
    "closed_at": "2024-10-21T05:58:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/61",
    "body": "I wrote some code to be able to use multiple llms. I don't know if it is ready for it own pull request so I am opening it as an issue.\r\n\r\n# Why Would you want this?\r\nI built to circumvent low rate limits on API free tiers by allowing me use different credentials. You could also do have a hybrid of local and api requests, use different llms in combination, etc.\r\n\r\n# Implementation\r\nThis is basic round robin implementation. The function to use the llm and its args are stored. When a request is made it cycles to the next llm in the list.\r\n\r\n```python\r\nfrom typing import List, Dict, Callable\r\nfrom pydantic import BaseModel\r\n\r\nclass Model(BaseModel):\r\n    gen_func: Callable\r\n    kwargs: Dict\r\n\r\nclass MultiModel():\r\n    def __init__(self, models: List[Model]):\r\n        self.models = models\r\n        self.current_model = 0\r\n        \r\n    def next_model(self):\r\n        self.current_model = (self.current_model + 1) % len(self.models)\r\n        return self.models[self.current_model]\r\n\r\n    async def llm_model_func(\r\n        self,\r\n        prompt, system_prompt=None, history_messages=[], **kwargs\r\n    ) -> str:\r\n        kwargs.pop(\"model\", None) # stop from overwriting the custom model name\r\n        next_model = self.next_model()\r\n        args = dict(prompt=prompt, system_prompt=system_prompt, history_messages=history_messages, **kwargs, **next_model.kwargs)\r\n        \r\n        return await next_model.gen_func(\r\n            **args\r\n        )\r\n```\r\nSorry if my explanation sucks üòÖ. If it is good I will submit it as a pull request!\r\nI have also implemented support for Mistral API if would like to see it as well!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/61/comments",
    "author": "Soumil32",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-20T10:26:13Z",
        "body": "This is indeed a very useful feature! I think it would be helpful to add a specific demo to make it easier for everyone to understand."
      },
      {
        "user": "Soumil32",
        "created_at": "2024-10-20T12:56:10Z",
        "body": "Here is an example:\r\n```python\r\nfrom lightrag import LightRAG\r\nfrom lightrag.llm import openai_complete_if_cache\r\n# get the definitions from above\r\nmodels = [\r\n    Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY_1\"]}),\r\n    Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY_2\"]}),\r\n    Model(gen_func=openai_complete_if_cache, kwargs={\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY_3\"]}),\r\n]\r\nmulti_model = MultiModel(models)\r\nrag = LightRAG(\r\n    llm_model_func=multi_model.llm_model_func,\r\n    # rest of the args\r\n)\r\n```"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-21T05:58:43Z",
        "body": " Welcome to submit a PR so we can add this feature. Thanks for your contribution."
      }
    ]
  },
  {
    "number": 60,
    "title": "Embeddings with ndim != 4096",
    "created_at": "2024-10-19T16:28:08Z",
    "closed_at": "2024-10-23T03:13:55Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/60",
    "body": "I'm having trouble running this with embedding funcs with `ndim != 4096`.\r\n\r\nSo tying this with \r\n\r\n```python3\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=768, max_token_size=8192, func=embedding_func\r\n    ),\r\n)\r\n```\r\n\r\nI keep getting\r\n\r\n```\r\nvenv/lib/python3.12/site-packages/nano_vectordb/dbs.py\", line 71, in __post_init__\r\n    storage[\"embedding_dim\"] == self.embedding_dim\r\nAssertionError: Embedding dim mismatch, expected: 768, but loaded: 4096\r\n```\r\n\r\nIs this `4096` somehow a magic number?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/60/comments",
    "author": "Florents-Tselai",
    "comments": [
      {
        "user": "cortseverns",
        "created_at": "2024-10-19T17:11:35Z",
        "body": "> I'm having trouble running this with embedding funcs with `ndim != 4096`.\r\n> \r\n> So tying this with\r\n> \r\n> ```python\r\n> rag = LightRAG(\r\n>     working_dir=WORKING_DIR,\r\n>     llm_model_func=llm_model_func,\r\n>     embedding_func=EmbeddingFunc(\r\n>         embedding_dim=768, max_token_size=8192, func=embedding_func\r\n>     ),\r\n> )\r\n> ```\r\n> \r\n> I keep getting\r\n> \r\n> ```\r\n> venv/lib/python3.12/site-packages/nano_vectordb/dbs.py\", line 71, in __post_init__\r\n>     storage[\"embedding_dim\"] == self.embedding_dim\r\n> AssertionError: Embedding dim mismatch, expected: 768, but loaded: 4096\r\n> ```\r\n> \r\n> Is this `4096` somehow a magic number?\r\n\r\nDifferent embedding models have different defaults. Have you changed away from nomic-embed-text?"
      },
      {
        "user": "Florents-Tselai",
        "created_at": "2024-10-19T18:26:28Z",
        "body": "> > I'm having trouble running this with embedding funcs with `ndim != 4096`.\r\n> > So tying this with\r\n> > ```python\r\n> > rag = LightRAG(\r\n> >     working_dir=WORKING_DIR,\r\n> >     llm_model_func=llm_model_func,\r\n> >     embedding_func=EmbeddingFunc(\r\n> >         embedding_dim=768, max_token_size=8192, func=embedding_func\r\n> >     ),\r\n> > )\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > I keep getting\r\n> > ```\r\n> > venv/lib/python3.12/site-packages/nano_vectordb/dbs.py\", line 71, in __post_init__\r\n> >     storage[\"embedding_dim\"] == self.embedding_dim\r\n> > AssertionError: Embedding dim mismatch, expected: 768, but loaded: 4096\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Is this `4096` somehow a magic number?\r\n> \r\n> Different embedding models have different defaults. Have you changed away from nomic-embed-text?\r\n\r\nThat's what the `embedding_dim` is supposed to set I assume \r\n```\r\nembedding_func=EmbeddingFunc(\r\n        embedding_dim=768, max_token_size=8192, func=embedding_func\r\n    )\r\n```"
      },
      {
        "user": "Florents-Tselai",
        "created_at": "2024-10-19T18:35:25Z",
        "body": "* and yes, I got the same error when I tried an embedding model with `embedding_dim=2048` for example. "
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-20T10:13:57Z",
        "body": "The embedding model's dimension should match the one you've set. I suggest you try setting `embedding_dim=4096` and see if that works : )."
      },
      {
        "user": "Florents-Tselai",
        "created_at": "2024-10-20T11:45:07Z",
        "body": "> The embedding model's dimension should match the one you've set. I suggest you try setting `embedding_dim=4096` and see if that works : ).\n\nWhy would I do that ? \nI know my embedding model it's not 4096.\nAnyway I tried that and again it doesn't work.\n\nLooks like whatever the embedding model ndim is, from the storage layer it tries to fetch vectors with ndim=4096\n"
      },
      {
        "user": "boogle",
        "created_at": "2024-10-21T04:18:04Z",
        "body": "Êää‰Ω†WORKING_DIRÁõÆÂΩï‰∏ãÔºåÁîüÊàêÁöÑÈÖçÁΩÆÊñá‰ª∂Âà†Èô§ÂêéÔºåÈáçÊñ∞ËøêË°åÂç≥ÂèØ„ÄÇ"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-21T06:00:33Z",
        "body": "> > The embedding model's dimension should match the one you've set. I suggest you try setting `embedding_dim=4096` and see if that works : ).\r\n> \r\n> Why would I do that ? I know my embedding model it's not 4096. Anyway I tried that and again it doesn't work.\r\n> \r\n> Looks like whatever the embedding model ndim is, from the storage layer it tries to fetch vectors with ndim=4096\r\n\r\nOne potential solution is to create a new working directory and try again. If this doesn't work, please provide the specific log and more detailed information about the runtime environment."
      },
      {
        "user": "thinkthinking",
        "created_at": "2024-10-21T15:43:49Z",
        "body": "> Êää‰Ω†WORKING_DIRÁõÆÂΩï‰∏ãÔºåÁîüÊàêÁöÑÈÖçÁΩÆÊñá‰ª∂Âà†Èô§ÂêéÔºåÈáçÊñ∞ËøêË°åÂç≥ÂèØ„ÄÇ\r\n\r\nit works, thank you"
      }
    ]
  },
  {
    "number": 56,
    "title": "chore: added pre-commit-hooks and ruff formatting for commit-hooks",
    "created_at": "2024-10-19T04:15:10Z",
    "closed_at": "2024-10-19T12:44:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/56",
    "body": "Added pre-commit hooks to the project to check for code optimisation\r\n* Trailing whitespaces\r\n* README files\r\n* Code Formatting using Ruff\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/56/comments",
    "author": "sank8-2",
    "comments": [
      {
        "user": "sank8-2",
        "created_at": "2024-10-19T09:49:22Z",
        "body": "@LarFii Hi, please review this."
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-19T12:44:09Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 55,
    "title": "ÂêàÂπ∂EntityÂäüËÉΩ",
    "created_at": "2024-10-19T03:29:57Z",
    "closed_at": "2024-10-19T03:35:16Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/55",
    "body": "Âú®ÁúãAIÁîüÊàêÁöÑÂÆû‰ΩìÁöÑÊó∂ÂÄôÔºåÂèëÁé∞ÊúâÂæàÂ§öÂÆûÈôÖ‰∏äÊòØÂêå‰∏Ä‰∏™Ê¶ÇÂøµÁöÑ‰∏úË•øË¢´ËØÜÂà´ÊàêÂæàÂ§ö‰∏™ÂÆû‰ΩìÔºåÂè™ÊòØ‰∏™Âà´ÁöÑÂçïËØç‰∏çÂêåËÄåÂ∑≤„ÄÇËÉΩÂê¶ËÄÉËôëÁªô‰∏Ä‰∏™ÊâãÂä®ÂêàÂπ∂ÂÆû‰ΩìÁöÑÂäüËÉΩÂë¢„ÄÇÂ∞±ÊòØÊèê‰æõ‰∏Ä‰∏™ÊñπÊ≥ïÔºåËæìÂÖ•ÊÉ≥Ë¶ÅÂêàÂπ∂ÁöÑÂÆû‰ΩìÂêçÂíåÊúÄÁªàÂêàÂπ∂ÂêéÁöÑÂêçÂ≠óÔºåÊï∞ÊçÆÂ∫ìÈáåÂàôÊääËøô‰∫õÂÆû‰ΩìÂÅö‰∏Ä‰∏™ÂêàÂπ∂Âπ∂‰øùÂ≠ò",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/55/comments",
    "author": "Feliks151450",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-19T03:35:01Z",
        "body": "ËøôÁ°ÆÂÆûÊòØ‰∏™‰∏çÈîôÁöÑÂäüËÉΩÔºåÂêéÁª≠Êàë‰ª¨‰ºöÂä†‰∏äËøô‰∏™ÂäüËÉΩÔºå‰∏çËøáÁõÆÂâç‰∫∫ÂäõÊúâÈôêÂèØËÉΩÈúÄË¶Å‰∏ÄÊÆµÊó∂Èó¥ÔºåÊï¨ËØ∑ÊúüÂæÖÔºöÔºâ"
      }
    ]
  },
  {
    "number": 54,
    "title": "Add comment specifying jupyter req",
    "created_at": "2024-10-19T00:09:55Z",
    "closed_at": "2024-10-19T03:32:07Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/54",
    "body": "Add lines that can be uncommented if running in a jupyter notebook",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/54/comments",
    "author": "wrosko",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-19T03:32:11Z",
        "body": "Thanks!"
      },
      {
        "user": "wrosko",
        "created_at": "2024-10-19T16:36:15Z",
        "body": "No problem! Quick question though, @LarFii, will you all be planning to maintain this as a package and work on it, or will it be up to the community to take the idea and implement a maintained project?\n\nAsking because I might be able to contribute some Neo4j and Milvus related stuff. "
      }
    ]
  },
  {
    "number": 53,
    "title": "FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\...\\\\dickens\\\\lightrag.log'",
    "created_at": "2024-10-18T21:20:58Z",
    "closed_at": "2024-10-19T03:48:38Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/53",
    "body": "When I create the LightRAG, it keeps looking for the dickens\\\\lightrag.log file even though I set the WORKING_DIR to be the correct file where the lightrag,log is.\r\n\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\bruno\\Documents\\DESENVOLVIMENTO\\normativos-light-rag\\main.py\", line 3, in <module>\r\n    from src.rag_flow.document_rag import rag_documents\r\n  File \"C:\\Users\\bruno\\Documents\\DESENVOLVIMENTO\\normativos-light-rag\\src\\rag_flow\\__init__.py\", line 95, in <module>\r\n    rag = LightRAG(\r\n  File \"<string>\", line 25, in __init__\r\n  File \"C:\\Users\\bruno\\Documents\\DESENVOLVIMENTO\\normativos-light-rag\\venv\\lib\\site-packages\\lightrag\\lightrag.py\", line 103, in __post_init__\r\n    set_logger(log_file)\r\n  File \"C:\\Users\\bruno\\Documents\\DESENVOLVIMENTO\\normativos-light-rag\\venv\\lib\\site-packages\\lightrag\\utils.py\", line 22, in set_logger\r\n    file_handler = logging.FileHandler(log_file)\r\n  File \"C:\\Users\\bruno\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\logging\\__init__.py\", line 1169, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\bruno\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\logging\\__init__.py\", line 1201, in _open\r\n    return open_func(self.baseFilename, self.mode,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\bruno\\\\Documents\\\\DESENVOLVIMENTO\\\\normativos-light-rag\\\\dickens\\\\lightrag.log'\r\n\r\n\r\nCode: \r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport aiohttp\r\nimport logging\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom dotenv import load_dotenv\r\n\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s - %(levelname)s - %(message)s',\r\n    handlers=[logging.StreamHandler()]\r\n)\r\n\r\nload_dotenv()\r\n\r\n\r\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\r\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\r\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\r\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\r\n\r\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\r\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\r\n\r\n\r\nasync def llm_model_func(\r\n    prompt, system_prompt=None, history_messages=[], **kwargs\r\n) -> str:\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"api-key\": AZURE_OPENAI_API_KEY,\r\n    }\r\n    endpoint = f\"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version={AZURE_OPENAI_API_VERSION}\"\r\n\r\n    messages = []\r\n    if system_prompt:\r\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n    if history_messages:\r\n        messages.extend(history_messages)\r\n    messages.append({\"role\": \"user\", \"content\": prompt})\r\n\r\n    payload = {\r\n        \"messages\": messages,\r\n        \"temperature\": kwargs.get(\"temperature\", 0),\r\n        \"top_p\": kwargs.get(\"top_p\", 1),\r\n        \"n\": kwargs.get(\"n\", 1),\r\n    }\r\n\r\n    async with aiohttp.ClientSession() as session:\r\n        async with session.post(endpoint, headers=headers, json=payload) as response:\r\n            if response.status != 200:\r\n                raise ValueError(\r\n                    f\"Request failed with status {response.status}: {await response.text()}\"\r\n                )\r\n            result = await response.json()\r\n            return result[\"choices\"][0][\"message\"][\"content\"]\r\n\r\n\r\nasync def embedding_func(texts: list[str]) -> np.ndarray:\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"api-key\": AZURE_OPENAI_API_KEY,\r\n    }\r\n    endpoint = f\"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_EMBEDDING_DEPLOYMENT}/embeddings?api-version={AZURE_EMBEDDING_API_VERSION}\"\r\n\r\n    payload = {\"input\": texts}\r\n\r\n    async with aiohttp.ClientSession() as session:\r\n        async with session.post(endpoint, headers=headers, json=payload) as response:\r\n            if response.status != 200:\r\n                raise ValueError(\r\n                    f\"Request failed with status {response.status}: {await response.text()}\"\r\n                )\r\n            result = await response.json()\r\n            embeddings = [item[\"embedding\"] for item in result[\"data\"]]\r\n            return np.array(embeddings)\r\n\r\n\r\ndef rag_documents():\r\n    base_dir = os.path.abspath(os.path.join(\r\n        os.path.dirname(__file__), \"../../\"))\r\n    WORKING_DIR = os.path.join(base_dir, \"assets\", \"knowledge_working_dir\")\r\n\r\n    embedding_dimension = 3072\r\n    rag = LightRAG(\r\n        working_dir=WORKING_DIR,\r\n        llm_model_func=llm_model_func,\r\n        embedding_func=EmbeddingFunc(\r\n            embedding_dim=embedding_dimension,\r\n            max_token_size=8192,\r\n            func=embedding_func,\r\n        ),\r\n    )\r\n\r\n    query_text = \"\"\r\n    while (query_text != \"quit\"):\r\n        query_text = input(\"Qual a sua pergunta?\")\r\n        print(\"\\n\\n\\nResultado (Hybrid):\\n\\n\\n\")\r\n        print(rag.query(query_text, param=QueryParam(mode=\"hybrid\")))\r\n        print(\"\\n\\n\\n\")\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/53/comments",
    "author": "brunoedcf",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-19T03:48:38Z",
        "body": "The issue is caused by the working directory not existing. You can automatically create the working directory using the following code:\r\n```python\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n```"
      }
    ]
  },
  {
    "number": 52,
    "title": "Support for PGVector",
    "created_at": "2024-10-18T20:45:13Z",
    "closed_at": "2024-10-19T12:46:54Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/52",
    "body": "Hi, the Nano vector database isn't suitable for production. Is there a way to integrate PGVector into this?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/52/comments",
    "author": "Itachi-Uchiha581",
    "comments": [
      {
        "user": "himanshukumaaar",
        "created_at": "2024-10-19T06:27:12Z",
        "body": "Hii @Itachi-Uchiha581 \r\n\r\nYou can follow these steps to inegrate PGVector üëç \r\n- Install PGVector and configure it in your PostgreSQL.\r\n- Update your schema to store vectors and related metadata.\r\n- Modify LightRAG to interact with PostgreSQL and perform vector operations (insertion, similarity search).\r\n- Add indexing and performance tuning for scalable production use.\r\n\r\nThis approach integrates PGVector as a scalable and production-ready alternative to Nano, aligning method with a more robust database solution."
      }
    ]
  },
  {
    "number": 51,
    "title": "Can't run any example",
    "created_at": "2024-10-18T19:25:37Z",
    "closed_at": "2024-10-18T23:59:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/51",
    "body": "I'm not able to run the example.\r\n\r\nI've tried this with python 3.9, 3.10, and 3.11. The initial bit of the examples work up until I run\r\n```python\r\nwith open(\"./book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n```\r\n\r\nWhen `rag.insert(f.read())`, is run the following error appears every time:\r\n\r\n```logs\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[9], line 1\r\n----> 1 rag.insert(our_docs)\r\n\r\nFile /home/jupyter/projects/LightRAG/lightrag/lightrag.py:166, in LightRAG.insert(self, string_or_strings)\r\n    164 def insert(self, string_or_strings):\r\n    165     loop = always_get_an_event_loop()\r\n--> 166     return loop.run_until_complete(self.ainsert(string_or_strings))\r\n\r\nFile /opt/conda/envs/lightrag/lib/python3.11/asyncio/base_events.py:626, in BaseEventLoop.run_until_complete(self, future)\r\n    615 \"\"\"Run until the Future is done.\r\n    616 \r\n    617 If the argument is a coroutine, it is wrapped in a Task.\r\n   (...)\r\n    623 Return the Future's result, or raise its exception.\r\n    624 \"\"\"\r\n    625 self._check_closed()\r\n--> 626 self._check_running()\r\n    628 new_task = not futures.isfuture(future)\r\n    629 future = tasks.ensure_future(future, loop=self)\r\n\r\nFile /opt/conda/envs/lightrag/lib/python3.11/asyncio/base_events.py:586, in BaseEventLoop._check_running(self)\r\n    584 def _check_running(self):\r\n    585     if self.is_running():\r\n--> 586         raise RuntimeError('This event loop is already running')\r\n    587     if events._get_running_loop() is not None:\r\n    588         raise RuntimeError(\r\n    589             'Cannot run the event loop while another loop is running')\r\n\r\nRuntimeError: This event loop is already running\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/51/comments",
    "author": "wrosko",
    "comments": [
      {
        "user": "jannikdev",
        "created_at": "2024-10-18T20:06:01Z",
        "body": "Hi, the problem is that you are running this in a Jupyter notebook which is itself already running a loop. To do this in a notebook add \r\n`import nest_asyncio` and\r\n`nest_asyncio.apply()`\r\nbefore trying to run the async loop.\r\n\r\nIf you are wondering where you are trying to run something asynchronously: insert just wraps around ainsert and waits for completion. It will always start an event loop."
      },
      {
        "user": "ashishjaddu",
        "created_at": "2024-10-18T20:51:00Z",
        "body": "Hi, I tried it this way in my notebook and it worked for me. Hope this helps you.\r\n\r\n```python\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\r\nimport os\r\nimport nest_asyncio\r\n\r\nWORKING_DIR = \"./dickens\"\r\nnest_asyncio.apply()\r\n\r\nos.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\"\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nfrom lightrag.llm import hf_model_complete, hf_embedding\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\nbook_file_path = os.path.join(WORKING_DIR, \"book.txt\")\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete\r\n)\r\n\r\nasync def insert_book_content():\r\n    with open(book_file_path, 'r') as f:\r\n        await rag.insert(f.read()) \r\n\r\nawait insert_book_content()\r\n"
      },
      {
        "user": "wrosko",
        "created_at": "2024-10-18T23:59:09Z",
        "body": "> Hi, the problem is that you are running this in a Jupyter notebook which is itself already running a loop. To do this in a notebook add `import nest_asyncio` and `nest_asyncio.apply()` before trying to run the async loop.\r\n> \r\n> If you are wondering where you are trying to run something asynchronously: insert just wraps around ainsert and waits for completion. It will always start an event loop.\r\n\r\nThanks @jannikdev this solves the issue. I appreciate it"
      }
    ]
  },
  {
    "number": 50,
    "title": "I am Django Developer",
    "created_at": "2024-10-18T17:29:02Z",
    "closed_at": "2024-10-19T13:10:35Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/50",
    "body": "What is your used technologies on the projects?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/50/comments",
    "author": "Kondexor2000",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-19T03:38:23Z",
        "body": "We primarily use nano-vector database and networkx for data storage and retrieval. The rest of the project mainly involves communication with the LLM."
      }
    ]
  },
  {
    "number": 47,
    "title": "Running on VSCODE - Error Loop already running - Fixed",
    "created_at": "2024-10-18T10:55:44Z",
    "closed_at": "2024-10-18T10:55:50Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/47",
    "body": "I had to add \r\nimport nest_asyncio\r\nnest_asyncio.apply()\r\n\r\nAt the beggining of the code to bypass the error message warning that the loop was already running at:\r\n`rag.insert(f.read())`\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/47/comments",
    "author": "csaiedu",
    "comments": [
      {
        "user": "brunoedcf",
        "created_at": "2024-10-18T21:04:07Z",
        "body": "Were you running on a Jupyter Notebook?"
      },
      {
        "user": "csaiedu",
        "created_at": "2024-10-20T17:57:32Z",
        "body": "Correct"
      }
    ]
  },
  {
    "number": 46,
    "title": "Improve insert speed",
    "created_at": "2024-10-18T10:22:31Z",
    "closed_at": "2024-10-19T03:56:24Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/46",
    "body": "When I use the default hyperparameters, I see that my GPU only uses 14 to 24 GB of VRAM (I have more VRAM with the A100), and the GPU is not running at full load. Therefore, I think I can improve the entity extraction speed. Do you have any experience in improving entity extraction speed? (maybe I need increase llm_model_max_async or embedding_func_max_async or embedding_batch_num?)",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/46/comments",
    "author": "CVHvn",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T10:26:28Z",
        "body": "We don't have much experience with local deployment, but you can definitely try adjusting the `llm_model_max_async` or `embedding_func_max_async` parameters to improve the extraction speed."
      }
    ]
  },
  {
    "number": 44,
    "title": "How can I get the input chunks as a response in the query function as well?",
    "created_at": "2024-10-18T06:47:45Z",
    "closed_at": "2024-10-19T03:49:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/44",
    "body": "My usecase requires me to cite the exact paragraph or at least know what the original text was. Is there any way in the query params where I can change and get the exact chunks as a response as well. I went through the code but couldn't find it.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/44/comments",
    "author": "kabironline",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T07:14:20Z",
        "body": "Your use case seems more suited for a naive query. You can modify the output in the `operate.py` to retrieve the context along with the response."
      },
      {
        "user": "kabironline",
        "created_at": "2024-10-18T07:16:23Z",
        "body": "Sure I will give it a try, might send a pull request because its a very useful feature.\r\n"
      }
    ]
  },
  {
    "number": 43,
    "title": "‰∏≠ÊñáÊèêÁ§∫ËØç",
    "created_at": "2024-10-18T06:25:51Z",
    "closed_at": "2024-10-18T07:15:52Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/43",
    "body": "ËÉΩÂê¶Â¢ûÂä†‰∏Ä‰∏™ÂÆòÊñπ‰∏≠Êñá Prompt ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºüÊàëËá™Â∑±Ê®°‰ªøen_prompt ÂÜôÁöÑcn_promptÔºåÊÄªÊòØÂá∫Áé∞ÈóÆÈ¢òÔºå‰∏îÊïàÊûú‰∏ç‰Ω≥„ÄÇ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/43/comments",
    "author": "AlexCRX",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T07:15:52Z",
        "body": "ÊòéÁôΩ‰∫ÜÔºåÂêéÁª≠Êàë‰ª¨‰ºöÂ∞ΩÂø´Ë∞ÉËØï‰∏Ä‰∏™ÂêàÈÄÇÁöÑ‰∏≠ÊñápromptÂá∫Êù•ÔºåËØ∑ÁªßÁª≠ÂÖ≥Ê≥®ÔºöÔºâ"
      },
      {
        "user": "small122",
        "created_at": "2024-11-05T09:01:00Z",
        "body": "ÂêåÊ±Ç"
      }
    ]
  },
  {
    "number": 42,
    "title": "Can we use external Vector databases and Knowledge graphs other than storing in the json formats?",
    "created_at": "2024-10-18T03:46:07Z",
    "closed_at": "2024-10-19T03:49:19Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/42",
    "body": "Can we use external Vector databases and Knowledge graphs like Pinecone, Neo4j, etc for storing these graphs and embeddings?",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/42/comments",
    "author": "ashishjaddu",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T04:03:39Z",
        "body": "Thanks for your attention. We currently don‚Äôt offer support for external vector databases, but you can definitely try integrating them."
      },
      {
        "user": "ashishjaddu",
        "created_at": "2024-10-18T04:23:51Z",
        "body": "In order to integrate them, If I play around with the Step codes in reproduce folder would that be enough?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-18T07:19:46Z",
        "body": "No, that won‚Äôt be enough. If you want to use a different vector database, you‚Äôll need to modify `storage.py`, which is quite a big task. However, after inserting, the resulting graph can be imported into Neo4j. We will be providing support for this in the next couple of days."
      }
    ]
  },
  {
    "number": 39,
    "title": "update Step_3.py and openai compatible script",
    "created_at": "2024-10-17T22:09:53Z",
    "closed_at": "2024-10-18T03:44:47Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/39",
    "body": "update reproduce/Step_3.py slightly\r\nadd open ai compatible script for reproduce/Step_1 and Step_2 ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/39/comments",
    "author": "russellkim",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T03:44:40Z",
        "body": "Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 38,
    "title": "Azure OpenAI Demo",
    "created_at": "2024-10-17T20:03:24Z",
    "closed_at": "2024-10-18T03:38:44Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/38",
    "body": "Azure OpenAI Demo code for integration:\r\n\r\n\r\n\r\n```\r\nimport os\r\nimport asyncio\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.utils import EmbeddingFunc\r\nimport numpy as np\r\nfrom dotenv import load_dotenv\r\nimport aiohttp\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\nload_dotenv()\r\n\r\nAZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\r\nAZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\r\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\r\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\r\n\r\nAZURE_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\r\nAZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif os.path.exists(WORKING_DIR):\r\n    import shutil\r\n\r\n    shutil.rmtree(WORKING_DIR)\r\n\r\nos.mkdir(WORKING_DIR)\r\n\r\n\r\nasync def llm_model_func(\r\n    prompt, system_prompt=None, history_messages=[], **kwargs\r\n) -> str:\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"api-key\": AZURE_OPENAI_API_KEY,\r\n    }\r\n    endpoint = f\"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version={AZURE_OPENAI_API_VERSION}\"\r\n\r\n    messages = []\r\n    if system_prompt:\r\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n    if history_messages:\r\n        messages.extend(history_messages)\r\n    messages.append({\"role\": \"user\", \"content\": prompt})\r\n\r\n    payload = {\r\n        \"messages\": messages,\r\n        \"temperature\": kwargs.get(\"temperature\", 0),\r\n        \"top_p\": kwargs.get(\"top_p\", 1),\r\n        \"n\": kwargs.get(\"n\", 1),\r\n    }\r\n\r\n    async with aiohttp.ClientSession() as session:\r\n        async with session.post(endpoint, headers=headers, json=payload) as response:\r\n            if response.status != 200:\r\n                raise ValueError(\r\n                    f\"Request failed with status {response.status}: {await response.text()}\"\r\n                )\r\n            result = await response.json()\r\n            return result[\"choices\"][0][\"message\"][\"content\"]\r\n\r\n\r\nasync def embedding_func(texts: list[str]) -> np.ndarray:\r\n    headers = {\r\n        \"Content-Type\": \"application/json\",\r\n        \"api-key\": AZURE_OPENAI_API_KEY,\r\n    }\r\n    endpoint = f\"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_EMBEDDING_DEPLOYMENT}/embeddings?api-version={AZURE_EMBEDDING_API_VERSION}\"\r\n\r\n    payload = {\"input\": texts}\r\n\r\n    async with aiohttp.ClientSession() as session:\r\n        async with session.post(endpoint, headers=headers, json=payload) as response:\r\n            if response.status != 200:\r\n                raise ValueError(\r\n                    f\"Request failed with status {response.status}: {await response.text()}\"\r\n                )\r\n            result = await response.json()\r\n            embeddings = [item[\"embedding\"] for item in result[\"data\"]]\r\n            return np.array(embeddings)\r\n\r\n\r\nasync def test_funcs():\r\n    result = await llm_model_func(\"How are you?\")\r\n    print(\"Resposta do llm_model_func: \", result)\r\n\r\n    result = await embedding_func([\"How are you?\"])\r\n    print(\"Resultado do embedding_func: \", result.shape)\r\n    print(\"Dimens√£o da embedding: \", result.shape[1])\r\n\r\n\r\nasyncio.run(test_funcs())\r\n\r\nembedding_dimension = 3072\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=embedding_dimension,\r\n        max_token_size=8192,\r\n        func=embedding_func,\r\n    ),\r\n)\r\n\r\nbook1 = open(\"./book_1.txt\", encoding=\"utf-8\")\r\nbook2 = open(\"./book_2.txt\", encoding=\"utf-8\")\r\n\r\nrag.insert([book1.read(), book2.read()])\r\n\r\nquery_text = \"What are the main themes?\"\r\n\r\nprint(\"Result (Naive):\")\r\nprint(rag.query(query_text, param=QueryParam(mode=\"naive\")))\r\n\r\nprint(\"\\nResult (Local):\")\r\nprint(rag.query(query_text, param=QueryParam(mode=\"local\")))\r\n\r\nprint(\"\\nResult (Global):\")\r\nprint(rag.query(query_text, param=QueryParam(mode=\"global\")))\r\n\r\nprint(\"\\nResult (Hybrid):\")\r\nprint(rag.query(query_text, param=QueryParam(mode=\"hybrid\")))\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/38/comments",
    "author": "brunoedcf",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T03:38:44Z",
        "body": "Thanks for your contribution! In the next commit,  I will include your demo."
      }
    ]
  },
  {
    "number": 37,
    "title": "UnicodeDecodeError",
    "created_at": "2024-10-17T15:10:23Z",
    "closed_at": "2024-10-18T03:34:06Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/37",
    "body": "I extracted from a text file and could query after initial insert.\r\n\r\nBut when I try to query the db again using:\r\n```python\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=llm_model_func,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1536,\r\n        max_token_size=8192,\r\n        func=embedding_func\r\n    )\r\n``` \r\nI then get the following error which I could not resolve:\r\n```\r\n  File \"<mypath>\\Lib\\encodings\\cp1252.py\", line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 584071: character maps to <undefined>\r\n```\r\n\r\nI suspect it defaults to use `cp1252` to decode the db, but is there a way to change that?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/37/comments",
    "author": "austinmyc",
    "comments": [
      {
        "user": "brunoedcf",
        "created_at": "2024-10-17T19:57:11Z",
        "body": "I had the same problem. I fixed it by going to venv/Lib/site-packages/nano_vectordb/dbs.py and changed the following:\r\n\r\n```\r\ndef load_storage(file_name) -> Union[DataBase, None]:\r\n    if not os.path.exists(file_name):\r\n        return None\r\n    with open(file_name) as f:\r\n```\r\n\r\nTo \r\n\r\n```\r\ndef load_storage(file_name) -> Union[DataBase, None]:\r\n    if not os.path.exists(file_name):\r\n        return None\r\n    with open(file_name, encoding='utf-8') as f:\r\n```"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-18T03:34:06Z",
        "body": "In the next commit, I will fix this issue."
      }
    ]
  },
  {
    "number": 36,
    "title": "Reload entities after insert",
    "created_at": "2024-10-17T14:38:43Z",
    "closed_at": "2024-10-18T03:28:38Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/36",
    "body": "How I can reload rag object after I insert my documents to rag (because reinsert document take long time andd more money!)",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/36/comments",
    "author": "CVHvn",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T03:28:38Z",
        "body": "You can simply reload the RAG object using the same path. The content that has already been inserted won't be reinserted, so there's no need to worry about duplicate inserts or additional costs."
      }
    ]
  },
  {
    "number": 35,
    "title": "ValueError bug",
    "created_at": "2024-10-17T12:34:44Z",
    "closed_at": "2024-10-18T05:19:20Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/35",
    "body": "```\r\nTraceback (most recent call last):\r\n  File \"/export/scratch/scrapy/LightRAG/x.py\", line 62, in <module>\r\n    print(rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\")))\r\n  File \"/export/scratch/scrapy/LightRAG/lightrag/lightrag.py\", line 248, in query\r\n    return loop.run_until_complete(self.aquery(query, param))\r\n  File \"/home/miniconda3/envs/py3.10/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/export/scratch/scrapy/LightRAG/lightrag/lightrag.py\", line 282, in aquery\r\n    response = await naive_query(\r\n  File \"/export/scratch/scrapy/LightRAG/lightrag/operate.py\", line 975, in naive_query\r\n    results = await chunks_vdb.query(query, top_k=query_param.top_k)\r\n  File \"/export/scratch/scrapy/LightRAG/lightrag/storage.py\", line 108, in query\r\n    results = self._client.query(\r\n  File \"/home/miniconda3/envs/py3.10/lib/python3.10/site-packages/nano_vectordb/dbs.py\", line 147, in query\r\n    return self.usable_metrics[self.metric](query, top_k, better_than_threshold)\r\n  File \"/home/miniconda3/envs/py3.10/lib/python3.10/site-packages/nano_vectordb/dbs.py\", line 162, in _cosine_query\r\n    scores = np.dot(use_matrix, query)\r\nValueError: shapes (42,1536) and (768,) not aligned: 1536 (dim 1) != 768 (dim 0)",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/35/comments",
    "author": "hxt365",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-18T03:33:12Z",
        "body": " Thanks for your attention! This issue is most likely caused by the inconsistency between the embedding models used for insertion and querying."
      },
      {
        "user": "hxt365",
        "created_at": "2024-10-18T05:19:20Z",
        "body": "Thanks. I figured it out. I think the error message can be improved"
      }
    ]
  },
  {
    "number": 27,
    "title": "Trouble importing ollama_embeding",
    "created_at": "2024-10-16T15:34:20Z",
    "closed_at": "2024-10-17T02:08:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/27",
    "body": "`from lightrag.llm import ollama_embedding, ollama_model_complete\r\nImportError: cannot import name 'ollama_embedding' from 'lightrag.llm\r\n`\r\n- Python 3.11",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/27/comments",
    "author": "Boadzie",
    "comments": [
      {
        "user": "eaedk",
        "created_at": "2024-10-16T18:41:33Z",
        "body": "same here"
      },
      {
        "user": "eaedk",
        "created_at": "2024-10-16T19:42:16Z",
        "body": "solved\r\nwhen I installed from source\r\n"
      },
      {
        "user": "Boadzie",
        "created_at": "2024-10-16T19:54:09Z",
        "body": "Let me try that."
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-17T02:08:05Z",
        "body": "Since there seem to be some bugs in the Ollama part, it hasn't been uploaded to PyPI yet. For now, you'll need to install it from the source. I'll work on fixing it as soon as possible. Best regards!"
      }
    ]
  },
  {
    "number": 19,
    "title": "UTF-8 in readme.md",
    "created_at": "2024-10-16T01:05:49Z",
    "closed_at": "2024-10-16T06:45:28Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/19",
    "body": "setup.py run on windows fails to read readme.md\r\n\r\n\r\nNeed to define encoding argument in setup.py\r\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/19/comments",
    "author": "JBGitHub11",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-16T06:45:28Z",
        "body": "Thank you for bringing this issue to my attention. I will address it in the next commit."
      }
    ]
  },
  {
    "number": 16,
    "title": "Added OpenAI compatible options and examples",
    "created_at": "2024-10-15T19:59:58Z",
    "closed_at": "2024-10-16T02:37:45Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/16",
    "body": "Added base_url, model, and api_key options to make it compatible with all Open AI-like chat/embeddings APIs, including Mistral, Llama, Together.ai, Fireworks, Croq, etc.\r\n\r\nIt will significantly improve the compatibility and usability of LightRAG.\r\n\r\nSee `examples/lightrag_openai_compatible_demo.py`.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/16/comments",
    "author": "hunkim",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-16T02:37:40Z",
        "body": "Thank you for improving LightRAG! This significantly increases its usability and flexibility!"
      }
    ]
  },
  {
    "number": 14,
    "title": "Additional Libraries",
    "created_at": "2024-10-15T16:57:17Z",
    "closed_at": "2024-10-16T06:46:58Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/14",
    "body": "Looks like a cool project, but seem to be missing a few libraries to run this(Transformers, Torch, etc).  May want to consider Dockerfile and updating requirements.txt.  ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/14/comments",
    "author": "danielbowne",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-16T06:46:58Z",
        "body": "Thank you for the suggestion! I‚Äôll make sure to update the `requirements.txt` in the next commit."
      }
    ]
  },
  {
    "number": 13,
    "title": "pip package of lightrag is out of date v0.10-beta.6 does not export LightRAG",
    "created_at": "2024-10-15T16:23:56Z",
    "closed_at": "2024-10-16T03:02:59Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/13",
    "body": "I think you need to rerun the pypi packages, when I did a `pip install lightrag-hkd` I get an old package. Looking at your __init__.py up there I see:\r\n\r\n```\r\n__version__ = \"0.1.0-beta.6\"\r\n```\r\n\r\nAnd no exports, you might want to fix. I haven't tried the local usage, but nice to have pypi package :-)",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/13/comments",
    "author": "richtong",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-16T03:02:59Z",
        "body": "Thank you very much for your attention! It seems the issue occurred because `pip install lightrag-hkd` was used instead of the correct `lightrag-hku`. There's also a deprecated project on PyPI called LightRAG, which might have caused further confusion. "
      }
    ]
  },
  {
    "number": 12,
    "title": "Support models other than OpenAI's",
    "created_at": "2024-10-15T10:31:29Z",
    "closed_at": "2024-10-15T11:58:09Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/12",
    "body": "It would be great to add support for models other than GPTs, like Cohere's or Groq's ones.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/12/comments",
    "author": "smortezah",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-15T11:58:09Z",
        "body": "Thank you so much for your attention and suggestion! We‚Äôve just added support for Hugging Face models, and we are planning to include support for more models, in the future. Stay tuned!\r\n"
      },
      {
        "user": "aiproductguy",
        "created_at": "2024-11-10T00:15:09Z",
        "body": "@LarFii Amazing work. Thank you. \r\n\r\nDo you have a starting point for adding more models, embedders, and providers (e.g. GROQ)?"
      }
    ]
  },
  {
    "number": 9,
    "title": "What Drives LightRAG's Superiority Over GraphRAG?",
    "created_at": "2024-10-14T14:28:03Z",
    "closed_at": "2024-10-15T17:20:11Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/9",
    "body": "Thank you for your excellent work. I still have a few questions I‚Äôd like to ask:\r\n\r\nAfter thoroughly reading your paper, I‚Äôm still unclear as to why your approach, though similar to GraphRAG, achieves better results. I‚Äôd like to understand where the improvements in your method come from.\r\n\r\nThe comparison between the two in the paper is as follows:\r\n\r\n‚ÄúLightRAG‚Äôs Superiority over GraphRAG: While both LightRAG and GraphRAG use graph-based retrieval mechanisms, LightRAG consistently outperforms GraphRAG, particularly in larger datasets with complex language contexts. In the Agriculture, CS, and Legal datasets‚Äîeach containing millions of tokens‚ÄîLightRAG shows a clear advantage, significantly surpassing GraphRAG and highlighting its strength in comprehensive information understanding within diverse environments. Enhanced Response Variety: By integrating low-level retrieval of specific entities with high-level retrieval of broader topics, LightRAG boosts response diversity. This dual-level mechanism effectively addresses both detailed and abstract queries, ensuring a thorough grasp of information. Complex Query Handling: This approach is especially valuable in scenarios requiring diverse perspectives. By accessing both specific details and overarching themes, LightRAG adeptly responds to complex queries involving interconnected topics, providing contextually relevant answers.‚Äù\r\n\r\nThe statement, ‚ÄúWhile both LightRAG and GraphRAG use graph-based retrieval mechanisms, LightRAG consistently outperforms GraphRAG, particularly in larger datasets with complex language contexts. In the Agriculture, CS, and Legal datasets‚Äîeach containing millions of tokens‚ÄîLightRAG shows a clear advantage, significantly surpassing GraphRAG and highlighting its strength in comprehensive information understanding within diverse environments,‚Äù is more of an observation rather than an explanation of why this is happening. Meanwhile, the sections on ‚ÄúEnhanced Response Variety‚Äù and ‚ÄúComplex Query Handling‚Äù seem more like optimizations specific to the datasets used, focusing on particular types of queries or features.\r\n\r\nI still don‚Äôt fully understand why your method outperforms GraphRAG. What exactly leads to this improvement? Also, I didn‚Äôt see an ablation study comparing your method directly with GraphRAG. Could you clarify this?\r\n\r\nThanks again!",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/9/comments",
    "author": "EachSheep",
    "comments": [
      {
        "user": "HKUDS",
        "created_at": "2024-10-15T14:58:25Z",
        "body": "Thank you for your interest in LightRAG. The performance improvement of LightRAG over GraphRAG in certain evaluation cases can be attributed to the following factor:\r\n\r\nGraphRAG performs community detection on the generated graphs, summarizing information from specific communities before inputting it into the LLMs for response generation. While this approach effectively focuses on community-based information, it has a potential limitation: it may result in some degree of information loss. Comprehensive information retrieval often requires access to fine-grained details from multiple communities.\r\n\r\nFurthermore, the effectiveness of LightRAG and GraphRAG may vary across different datasets due to differences in knowledge granularity and the specificity of user queries."
      },
      {
        "user": "bely66",
        "created_at": "2024-11-02T07:03:08Z",
        "body": "Hi @HKUDS \r\nJust to be clear the term \"light\" comes from having a vector DB for Relationships, Entities and Text Units, which makes it faster to retrieve information to answer the queries, right?"
      }
    ]
  },
  {
    "number": 7,
    "title": "How to do RAG without embedding",
    "created_at": "2024-10-14T11:20:47Z",
    "closed_at": "2024-10-15T17:20:22Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/7",
    "body": "How to do RAG without embedding",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/7/comments",
    "author": "MonolithFoundation",
    "comments": [
      {
        "user": "HKUDS",
        "created_at": "2024-10-14T17:18:09Z",
        "body": "Thank you for your interest! Conducting the retrieval process using vectors may be more efficient."
      },
      {
        "user": "MonolithFoundation",
        "created_at": "2024-10-15T03:04:49Z",
        "body": "Due to computational constraints, I am unable to set up an embedding model. In this situation, is it possible to perform Retrieval-Augmented Generation (RAG) only with graph RAG? Performance is not considered here. If so, how can it be done?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-15T14:10:43Z",
        "body": "The embedding model is necessary for LightRAG because embeddings represent the contexts and queries in a way that enables efficient retrieval. However, if you face computational constraints, you can use external APIs for embeddings, avoiding deploying models locally."
      }
    ]
  },
  {
    "number": 6,
    "title": "ËØ∑ÈóÆÊîØÊåÅÊú¨Âú∞huggingface‰∏äÁöÑÊ®°ÂûãÂêóÔºü",
    "created_at": "2024-10-14T07:51:10Z",
    "closed_at": "2024-10-14T08:27:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/6",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/6/comments",
    "author": "Qin-xb",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-14T08:27:43Z",
        "body": "ÊöÇÊó∂Ê≤°ÊúâÂØπÂÖ∂‰ªñÊ®°ÂûãÂÅöÈÄÇÈÖçÔºåÊàë‰ª¨‰ºöÂ∞ΩÂø´ÈÄÇÈÖçÂÖ∂‰ªñÊ®°Âûã„ÄÇ"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-15T11:59:26Z",
        "body": "Áé∞Âú®LightRAGÂ∑≤ÁªèÊîØÊåÅÊú¨Âú∞huggingface‰∏äÁöÑÊ®°Âûã‰∫ÜÔºåÁõ¥Êé•‰∏ãËΩΩÊúÄÊñ∞‰ª£Á†ÅÂç≥ÂèØ"
      }
    ]
  },
  {
    "number": 4,
    "title": "Fix typo on readme: utils.jpeg -> utils.py",
    "created_at": "2024-10-12T23:35:04Z",
    "closed_at": "2024-10-13T00:32:39Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/4",
    "body": "In the readme for the repository, there is a reference to the utils.jpeg file, but the correct file extension is .py\r\n\r\nEdit:\r\nFixes #5 ",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/4/comments",
    "author": "HeAndres",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-13T00:34:46Z",
        "body": "Thanks for pointing out this issue!"
      }
    ]
  },
  {
    "number": 3,
    "title": "ËØ∑ÈóÆÂèØ‰ª•Ê∑ªÂä†ollamaÊ®°ÂûãÂêó",
    "created_at": "2024-10-12T08:11:18Z",
    "closed_at": "2024-10-12T14:54:53Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/3",
    "body": null,
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/3/comments",
    "author": "yanning169",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-12T08:29:32Z",
        "body": "ÂêéÁª≠ÁªèËøáÊµãËØïÂêéÊàë‰ª¨‰ºöÈÄêÊ≠•Ê∑ªÂä†ÂÖ∂‰ªñÊ®°ÂûãÔºàÂåÖÊã¨ollamaÊ®°ÂûãÔºâ"
      },
      {
        "user": "2481241414",
        "created_at": "2024-10-16T08:32:17Z",
        "body": "> ÂêéÁª≠ÁªèËøáÊµãËØïÂêéÊàë‰ª¨‰ºöÈÄêÊ≠•Ê∑ªÂä†ÂÖ∂‰ªñÊ®°ÂûãÔºàÂåÖÊã¨ollamaÊ®°ÂûãÔºâ\r\n\r\nÊúüÂæÖÊõ¥Êñ∞"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-16T14:13:52Z",
        "body": "Êñ∞ÁöÑÊ∫êÁ†ÅÂ∑≤ÁªèÊèê‰æõOllamaÊîØÊåÅÔºåÂèØ‰ª•Â∞ùËØï‰∏Ä‰∏ãÔºàÊúâ‰∫õÊú™Áü•bugÈúÄË¶ÅÈÄêÊ≠•‰øÆÂ§çÔºâ"
      }
    ]
  },
  {
    "number": 1,
    "title": "'gbk' codec can't encode character '\\ufeff' ",
    "created_at": "2024-10-11T03:08:54Z",
    "closed_at": "2024-10-11T13:31:29Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/1",
    "body": "Traceback (most recent call last):\r\n  File \"E:\\work\\graphRAG\\run.py\", line 24, in <module>\r\n    rag.insert(f.read())\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\site-packages\\lightrag\\lightrag.py\", line 163, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\asyncio\\base_events.py\", line 654, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\site-packages\\lightrag\\lightrag.py\", line 225, in ainsert\r\n    await self._insert_done()\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\site-packages\\lightrag\\lightrag.py\", line 241, in _insert_done\r\n    await asyncio.gather(*tasks)\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\site-packages\\lightrag\\storage.py\", line 34, in index_done_callback\r\n    write_json(self._data, self._file_name)\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\site-packages\\lightrag\\utils.py\", line 102, in write_json\r\n    json.dump(json_obj, f, indent=2, ensure_ascii=False)\r\n  File \"D:\\Anaconda\\envs\\GraphRAG\\Lib\\json\\__init__.py\", line 180, in dump\r\n    fp.write(chunk)\r\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufeff' in position 70: illegal multibyte sequence",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/1/comments",
    "author": "yanning169",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-11T03:34:27Z",
        "body": "Thanks for your attention! I have fix this issue in the latest code. You can reinstall this package."
      },
      {
        "user": "leezhuuuuu",
        "created_at": "2024-10-15T16:37:20Z",
        "body": "Êàë‰ªäÂ§©Âú®ÂÖ∂‰ªñÈ°πÁõÆ‰πüÂèëÁé∞Ëøô‰∏™ÈóÆÈ¢òÔºåÁªèËøáÊéíÊü•ÔºåÊòØwinÁºñÁ†Å‰∏élinuxÁºñÁ†ÅÂ∑ÆÂºÇÂØºËá¥ÁöÑÈóÆÈ¢òÔºåÊú¨ÈóÆÈ¢òÂèØËÉΩ‰∏éËøô‰∏™Âõ†Á¥†ÊúâÂÖ≥"
      }
    ]
  }
]