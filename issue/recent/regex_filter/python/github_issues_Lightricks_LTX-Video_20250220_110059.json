[
  {
    "number": 116,
    "title": "Is there any document related to seed number and the video style",
    "created_at": "2025-02-17T05:21:17Z",
    "closed_at": "2025-02-18T13:30:48Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/116",
    "body": "The seed number range is too larger, and I have to try different seeds to see the generation effect. \nIf any of such {seed, video-style} already exists, it would be better.",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/116/comments",
    "author": "xushijie",
    "comments": [
      {
        "user": "yoavhacohen",
        "created_at": "2025-02-18T13:27:59Z",
        "body": "Unfortunately, we don't have a way know which seeds will work better for a given input"
      }
    ]
  },
  {
    "number": 106,
    "title": "Timestep \"starvation at tail\"",
    "created_at": "2025-01-21T11:36:59Z",
    "closed_at": "2025-01-21T14:57:32Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/106",
    "body": "Hi @yoavhacohen, thanks for the repo and your hard work.\n\nCan you please clarify a thing in your paper:\n\n> \"To prevent starvation at the tail of the resolution we clamp the pdf at percentiles 0.5 and 99.9.\"\n\nYou say that you don't want zero probabilities for the tails during training. Why does this matter if the minimum timestep you use during inference is 100 (due to the `shift_terminal` = 0.1)? Or is the the `shift_terminal` also applied during training?",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/106/comments",
    "author": "donthomasitos",
    "comments": [
      {
        "user": "yoavhacohen",
        "created_at": "2025-01-21T13:43:29Z",
        "body": "Hi @donthomasitos, thanks for your interest in our work!\n\nEven at t=0.1￼, there weren’t enough samples when the sequence was long.\nAdditionally, due to generalization from the contiguous timestep range, adding margins around the time steps in use helps make the distribution of samples contributing to each timestep more even."
      },
      {
        "user": "donthomasitos",
        "created_at": "2025-01-21T14:57:32Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 104,
    "title": "Data preparation info",
    "created_at": "2025-01-17T16:22:14Z",
    "closed_at": "2025-01-21T10:15:47Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/104",
    "body": "Hello, \nthanks for sharing your work\n\nA couple of question on the data preparation \n- is the internal video captioner available\n-  how did you measure alignment between generated caption and video ?\n- how did you measure motion of videos?\nThank you ",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/104/comments",
    "author": "segalinc",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-21T10:15:47Z",
        "body": "@segalinc Thank you for your questions. Currently, the internal video captioner is not available. Alignment between the generated captions and the video was measured using a user survey to gather qualitative feedback. Motion in the videos was assessed by measuring differences between frame statistics to quantify changes effectively.\n\n"
      }
    ]
  },
  {
    "number": 94,
    "title": "Approximate release date for LTX 1.0? (full, permissively licensed model)",
    "created_at": "2025-01-10T15:55:41Z",
    "closed_at": "2025-01-15T09:35:35Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/94",
    "body": "hey team, incredible work with LTXV and making one of my favourite AI models to date -- wanted to check if there is an approximate release date (2-3 weeks, 1 month, 2 months etc) for the full version of the model. I've been experimenting with it a ton and think there is a lot of cool use-cases to build on top of it with so looking very forward to the full model release! \r\n\r\nthank you",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/94/comments",
    "author": "nikshepsvn",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-15T09:46:11Z",
        "body": "@nikshepsvn Hi! In a few weeks, stay tuned! "
      }
    ]
  },
  {
    "number": 91,
    "title": "Questions about VAE",
    "created_at": "2025-01-09T03:02:38Z",
    "closed_at": "2025-01-13T11:59:24Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/91",
    "body": "Thank you for your great work! I would like to ask a few questions about VAE:\r\n1. Could you provide more detailed information about the training data for the VAE, such as the resolution, frame range, etc.?\r\n2. Could you share the code for reconstructing videos using the VAE for testing purposes?\r\n3. Do you have plans to open source the code for training the VAE?\r\n\r\nThank you again for your work, and I look forward to your suggestions.",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/91/comments",
    "author": "yiboz2001",
    "comments": [
      {
        "user": "yoavhacohen",
        "created_at": "2025-01-09T12:56:17Z",
        "body": "Thank you for your interest in our work!\r\n\r\n1. At the moment, we are not sharing additional details about the training data for the VAE.\r\n2. Reconstructing videos is straightforward by running the encode and decode steps from the diffusion pipeline.\r\n3. We currently have no plans to open source the code for training the VAE.\r\n\r\nLet me know if you have any further questions, or suggestions for improving the VAE."
      }
    ]
  },
  {
    "number": 89,
    "title": "Any info about compute time for training the base model and plans for dataset release?",
    "created_at": "2025-01-03T14:47:51Z",
    "closed_at": "2025-01-07T11:34:42Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/89",
    "body": "Hi, \r\nI just checked the paper and could not find any info on the compute time required for training of the base model and dataset availability, it would be really helpful if you could give insights for the base model training",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/89/comments",
    "author": "anm-ol",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-07T11:34:42Z",
        "body": "@anm-ol Hi! We don't have any plans to release the dataset."
      }
    ]
  },
  {
    "number": 80,
    "title": "LTX video upscale",
    "created_at": "2024-12-27T22:57:29Z",
    "closed_at": "2025-01-07T11:36:53Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/80",
    "body": "I'm sorry if this sound dumb but is it possible to make a video upscale with LTX model the same way we upscale images with flux and SD 4X or even 12X ?",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/80/comments",
    "author": "mustafaGFX",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-07T11:36:53Z",
        "body": "@mustafaGFX Hi! We've identified an effective upsampling workflow that begins with standard generation parameters (such as 768x512x105). The process then employs a GAN-based upsampler to expand the frames to a larger grid (we've successfully tested up to 1536x resolution). Finally, we use the FlowEdit method to re-render the content at this higher resolution.\"\r\n\r\nWe suggest to join Banadoco LTXV community on discord for more in-depth discussions."
      }
    ]
  },
  {
    "number": 76,
    "title": "Please any update on LTX 0.9.1 diffuser models on huggingface",
    "created_at": "2024-12-23T18:17:47Z",
    "closed_at": "2025-01-07T11:38:59Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/76",
    "body": "PR is already merged fixing the issue related to 0.9.1 diffuser model, waiting for Huggingface model repo.",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/76/comments",
    "author": "nitinmukesh",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-07T11:38:59Z",
        "body": "@nitinmukesh We understand that this issue has now been fixed."
      }
    ]
  },
  {
    "number": 63,
    "title": "When is the paper expected to be released?",
    "created_at": "2024-12-13T06:08:38Z",
    "closed_at": "2025-01-05T20:24:12Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/63",
    "body": "Thank you for sharing such great work. When is the paper expected to be released?",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/63/comments",
    "author": "JINSUBY",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2024-12-13T12:40:37Z",
        "body": "@JINSUBY hopefully soon, stay tuned! "
      }
    ]
  },
  {
    "number": 60,
    "title": "Can I generate video with interpolation method?",
    "created_at": "2024-12-12T12:15:33Z",
    "closed_at": "2025-01-05T20:23:59Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/60",
    "body": "Hi!\r\n\r\nThanks for the cool video generation model :)\r\n\r\nI just want to know is there any way to run this model with two image input (first and last).\r\n\r\nThanks..!",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/60/comments",
    "author": "Suprhimp",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-05T20:23:59Z",
        "body": "@Suprhimp We plan on supporting this soon! "
      }
    ]
  },
  {
    "number": 57,
    "title": "vae reconstruction is slightly poor",
    "created_at": "2024-12-11T10:38:19Z",
    "closed_at": "2025-01-07T11:46:02Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/57",
    "body": "Hello and thank you for sharing your work. I only used vae weights to conduct encoder-decoder for video data and found that the effect was not as good as the previous 16-channel one. I checked the code again and found that the VEVE-encoder reduced the image by 32 times. I think this is probably the reason why the decoder effect is not very good. \r\n1：Do you have 16x reduced vae and corresponding transform weights?\r\n2： how to set the weights based on the current weight file, try to change patch_size from 4 to 2, found that the weights do not match. Is my piece set up wrong?",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/57/comments",
    "author": "lijain",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-07T11:46:02Z",
        "body": "@lijain Hi! We currently don’t offer other Variational Autoencoders (VAEs) as standalone options. Additionally, modifying the VAE architecture, such as changing the patch size or other structural elements, will not be compatible with the pre-trained weights we provide. "
      }
    ]
  },
  {
    "number": 52,
    "title": "diffusers",
    "created_at": "2024-12-10T02:21:03Z",
    "closed_at": "2024-12-11T09:24:00Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/52",
    "body": "Thank you for your open project. Is this integrated into diffusers? Some can send the link, if not, what is your follow-up plan",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/52/comments",
    "author": "lijain",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2024-12-11T07:58:04Z",
        "body": "@lijain Not yet, but it will be very soon. Stay tuned! "
      }
    ]
  },
  {
    "number": 44,
    "title": "Commercial use in YouTube videos",
    "created_at": "2024-12-01T19:12:46Z",
    "closed_at": "2024-12-11T09:29:58Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/44",
    "body": "Do I understand correctly that the videos generated by this model and code can be used in my YouTube videos without any restrictions?",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/44/comments",
    "author": "qo4on",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2024-12-05T14:50:58Z",
        "body": "@qo4on \r\n\r\nThank you for your question! To clarify:\r\n \r\n1. The LTX Video model itself is licensed under the Rail-M license, which is strictly for research and academic use. This means content generated with the model cannot be used for commercial purposes, including monetized YouTube videos.\r\n \r\n2. The custom nodes in ComfyUI are licensed under the Apache license, which allows for commercial use. However, since the LTXV model is under the Rail-M license, the restriction on commercial use still applies when using the model, regardless of the licensing of the nodes.\r\n \r\nWe hope this clears things up."
      },
      {
        "user": "qo4on",
        "created_at": "2024-12-05T18:58:06Z",
        "body": "Thank you!"
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-12-06T06:30:31Z",
        "body": "@Shecht-ltx \r\n\r\nPlease could you confirm if the videos can be used in Youtube as a tutorial for non monetized videos. Basically to show the output from this tool."
      },
      {
        "user": "Shecht-ltx",
        "created_at": "2024-12-11T09:29:41Z",
        "body": "@nitinmukesh \r\nDisplaying results does not constitute commercial use. If no profit is being made from it, there is no issue."
      }
    ]
  },
  {
    "number": 35,
    "title": "When will Training Code be available?",
    "created_at": "2024-11-27T15:17:03Z",
    "closed_at": "2024-12-11T09:30:29Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/35",
    "body": null,
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/35/comments",
    "author": "radna0",
    "comments": [
      {
        "user": "LuthandoMaqondo",
        "created_at": "2024-11-30T15:00:15Z",
        "body": "I'm patiently waiting for this."
      },
      {
        "user": "yoavhacohen",
        "created_at": "2024-12-04T07:14:40Z",
        "body": "Soon"
      }
    ]
  },
  {
    "number": 32,
    "title": "Question : Anybody tried to extend the video ?",
    "created_at": "2024-11-26T22:50:37Z",
    "closed_at": "2025-01-06T20:16:26Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/32",
    "body": "Hi, is it possible to just put the last frame of the video generated so that it can generate more frames but consistently ? So this will be using img2vid then, will the prompt still the same ?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/32/comments",
    "author": "x4080",
    "comments": [
      {
        "user": "x4080",
        "created_at": "2024-11-27T01:27:33Z",
        "body": "I tried it myself using last frame of the txt2vid frame, and put it into img2vid workflow, the 1st frame result is nice but the reset is like over saturated, I'll try again with different seed"
      },
      {
        "user": "x4080",
        "created_at": "2024-11-27T02:48:43Z",
        "body": "Yes using different seed works, the new problem is later the video starts to \"hallucinate\", is there different for img2vid prompt ?"
      },
      {
        "user": "Shecht-ltx",
        "created_at": "2024-12-09T09:23:43Z",
        "body": "@x4080 Given the performance improvements and quality advancements in LTXV this week, we are now able to generate high-quality 12-14 second clips in LTXS. Best of all, this can be achieved while keeping waiting times very competitive."
      },
      {
        "user": "x4080",
        "created_at": "2024-12-09T20:36:22Z",
        "body": "@Shecht-ltx Thanks"
      },
      {
        "user": "MyCroniesSoft",
        "created_at": "2024-12-10T09:20:30Z",
        "body": "so many ai gen models in last 3 months, some have been available since 2023, while i give genmo the crown for the opensource part but 48gig just to load the model, omg thats insane thats 1 ada 6000, then the second for ease is pyramid flow, for complexity on model truncated i give ltx, and lasty the one i use ie none of them its ????\r\n\r\nThat is impressive, while i like midways video generator, but not for 200$ a month with credits.\r\n\r\nsome chinese ones are rising like ive never seen, definitely a threat.\r\n\r\nmidways great for extending the last frame, especially with stability ai svd, then feed it back in, pyramid flows not to bad, i like their technique, if they can get the faces right its a serious contender for low performance hardware, and it can extend its just the temporal consistancy falls apart. "
      },
      {
        "user": "x4080",
        "created_at": "2024-12-10T20:42:15Z",
        "body": "@MyCroniesSoft Is the brightness/contrast of the video changes when extending the video ? I was trying comfy ui ltx and its more efficient because it splits between GPU/CPU memory and with STG getting better result too"
      },
      {
        "user": "bitnom",
        "created_at": "2024-12-12T15:20:16Z",
        "body": "It becomes decoherent for me every time. The saturation, brightness, and colors start to go. Entities blur and it goes bad. I've been using the one on replicate."
      }
    ]
  },
  {
    "number": 31,
    "title": "Fat chance girl",
    "created_at": "2024-11-26T21:37:49Z",
    "closed_at": "2025-01-05T20:26:14Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/31",
    "body": "Beautiful redhead stands on at the helm of a 14th century pirate ship the sails fully filled with wind pushing the ship through the waves hot sunny day in the woman holds the wheel in steers the ship through the rocks nearby her hair blowing in the wind scantly dressed she's determined",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/31/comments",
    "author": "paulcar100",
    "comments": [
      {
        "user": "KT313",
        "created_at": "2024-11-27T07:20:24Z",
        "body": "are you ok"
      }
    ]
  },
  {
    "number": 29,
    "title": "Conditioning on an end image",
    "created_at": "2024-11-26T00:07:34Z",
    "closed_at": "2024-12-11T09:30:56Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/29",
    "body": "Is it possible to condition the model on an end image? Or is it only operating forward in time?",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/29/comments",
    "author": "CorentinJ",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2024-12-09T09:25:44Z",
        "body": "@CorentinJ Currently, the model natively supports first-frame conditioning.\r\n\r\nHowever, it is possible to implement end-frame conditioning by using an inference-time in-painting approach, specifically conditioning on the last 8 frames. This works because the last frame in the latent space corresponds to 8 frames in the pixel space.\r\n\r\nLooking ahead, we plan to include native support for end-frame conditioning in a future update, making this process more seamless."
      },
      {
        "user": "PeterTucker",
        "created_at": "2024-12-19T19:40:30Z",
        "body": "@Shecht-ltx , any way we could get a workflow example?"
      }
    ]
  },
  {
    "number": 27,
    "title": "Docker file is included to containerize LTX-Video",
    "created_at": "2024-11-25T22:01:21Z",
    "closed_at": "2024-12-16T09:54:55Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/pull/27",
    "body": null,
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/27/comments",
    "author": "dannyfiker",
    "comments": [
      {
        "user": "yoavhacohen",
        "created_at": "2024-12-16T09:54:55Z",
        "body": "Thanks for the PR!\r\nWe appreciate your contribution, but to keep this repository concise and flexible, we’ve decided not to include this change. Users can build their own containers in ways that suit their specific needs."
      }
    ]
  },
  {
    "number": 23,
    "title": "vram improvements",
    "created_at": "2024-11-25T18:48:43Z",
    "closed_at": "2024-12-20T07:56:30Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/pull/23",
    "body": "improved vram usage a lot by removing text encoder from vram after generating embeddings and offloading unet before vae decoding to avoid vram spike at the end of generation",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/23/comments",
    "author": "KT313",
    "comments": [
      {
        "user": "yoavhacohen",
        "created_at": "2024-12-20T07:56:30Z",
        "body": "Thank you for the contribution! Please take a look at the latest code - it now supports offloading to the CPU, and bfloat16 is the default precision (although float32 is still supported). If you observe any VRAM improvements, feel free to reopen the PR and rebase it on the latest main branch, or open a new one."
      }
    ]
  },
  {
    "number": 22,
    "title": "docker",
    "created_at": "2024-11-25T09:39:15Z",
    "closed_at": "2025-01-07T09:54:07Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/22",
    "body": "提供docker镜像吗？",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/22/comments",
    "author": "jacky080808",
    "comments": [
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-07T09:54:07Z",
        "body": "@jacky080808 No, unfortunately not! "
      }
    ]
  },
  {
    "number": 8,
    "title": "Relax sentencepiece dependency.",
    "created_at": "2024-11-23T07:00:03Z",
    "closed_at": "2024-11-23T11:22:14Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/pull/8",
    "body": "This commit relaxes sentencepiece dependency. On windows there are no pre-built wheels for sentencepiece satisfying the current version constraints for recent versions of python, and building from a source distribution is unreliable.",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/8/comments",
    "author": "kvochko",
    "comments": [
      {
        "user": "ofirbb",
        "created_at": "2024-11-23T11:21:40Z",
        "body": "Inference script runs as expected with `sentencepiece==0.2.0`."
      }
    ]
  },
  {
    "number": 2,
    "title": "Thank you for this awesome work! I'd love to know what's in the roadmap.",
    "created_at": "2024-11-22T16:07:13Z",
    "closed_at": "2025-01-07T09:55:47Z",
    "labels": [],
    "url": "https://github.com/Lightricks/LTX-Video/issues/2",
    "body": "I see this is a 2b model, will 5b or bigger be trained? Will all your releases be open source? Thank you!",
    "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/2/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "horsten",
        "created_at": "2024-11-24T00:02:54Z",
        "body": "Awesome work indeed, for its size and speed the output quality is mindblowing. Waiting for the paper :)"
      },
      {
        "user": "Shecht-ltx",
        "created_at": "2025-01-07T09:55:47Z",
        "body": "@jpgallegoar We are training a bigger model and plan to open-source it! \r\n\r\n\r\n"
      }
    ]
  }
]