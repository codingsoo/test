[
  {
    "number": 50,
    "title": "Fix langchain integration not present in pypi tar & whl-- pyproject.toml",
    "created_at": "2024-11-03T20:31:39Z",
    "closed_at": "2024-11-04T05:48:42Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/50",
    "body": "The [tool.setuptools.packages.find] configuration with the include pattern include = [\"byaldi*\"] instructs setuptools to automatically discover all packages and subpackages under byaldi. The wildcard asterisk (*) matches any subpackage within byaldi, including structures like byaldi, byaldi.integrations, byaldi.subpackage, and more. This configuration automatically includes all nested subdirectories as Python packages, provided they contain an __init__.py file, which reduces manual maintenance since each subpackage does not need to be listed explicitly.",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/50/comments",
    "author": "DebopamParam",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-11-04T05:48:39Z",
        "body": "Thank you, I was about to set out to do this!"
      }
    ]
  },
  {
    "number": 44,
    "title": "Add custom text for embedding",
    "created_at": "2024-10-16T10:06:38Z",
    "closed_at": "2024-11-04T08:29:01Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/44",
    "body": "Hi everyone, thank you very much for the work !\r\n\r\nFor documents that use a lot of acronyms or specific terminology, it would be useful if we could add custom text to the image during the embedding process. This custom text could serve multiple purposes, such as a summary of the entire document or definitions of acronyms or other key terms present on the page.\r\n\r\nThe goal is to leverage the attention mechanism between this added text and the page. Specifically, the model should be able to focus on this extra text (e.g., acronym definitions) while processing the embedded page, improving retrieval performance.\r\n\r\nI'm not sure if the model has been trained to handle this, or if it's already implemented (in this case I'm sorry for this useless issue, but I didn't find this functionnality).\r\n\r\nThank you !",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/44/comments",
    "author": "GCazottes",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-11-04T08:29:01Z",
        "body": "Hey!\r\n\r\nI don't think this is a bad idea per se, however, it's more of a research topic than within the scope for this lib, so I'll close the issue for now. However, I do encourage you to pursue this as a nice research demo and would be happy to help out if I can :)"
      }
    ]
  },
  {
    "number": 43,
    "title": "CPU indexing",
    "created_at": "2024-10-16T08:35:08Z",
    "closed_at": "2024-11-04T06:40:45Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/43",
    "body": "Hello, how can we run the indexing part strictly on CPU? Currently it defaults to GPU. I want the GPU free for other tasks.",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/43/comments",
    "author": "whoami02",
    "comments": [
      {
        "user": "DebopamParam",
        "created_at": "2024-11-03T19:39:38Z",
        "body": "Just in the place of \r\n`device = \"cuda\"`\r\n\r\nin any parameter you see the term `device`, replace it with:\r\n\r\n`device: str = \"cpu\"`\r\n\r\n> NOTE: I don't know which cpu in the world will be able to do this, but try it out. Best of luck.\r\n\r\nCode:\r\n\r\n```python\r\nfrom byaldi import RAGMultiModalModel\r\n\r\n# Optionally, you can specify an `index_root`, which is where it'll save the index. It defaults to \".byaldi/\".\r\nRAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.2\", device= \"cpu\",) // Changes\r\n\r\nRAG.index(\r\n    input_path=\"docs/\",  # The path to your documents\r\n    index_name=index_name,  # The name you want to give to your index. It'll be saved at `index_root/index_name/`.\r\n    store_collection_with_index=False,  # Whether the index should store the base64 encoded documents.\r\n    doc_ids=[0, 1, 2],  # Optionally, you can specify a list of document IDs. They must be integers and match the number of documents you're passing. Otherwise, doc_ids will be automatically created.\r\n    metadata=[{\"author\": \"John Doe\", \"date\": \"2021-01-01\"}],  # Optionally, you can specify a list of metadata for each document. They must be a list of dictionaries, with the same length as the number of documents you're passing.\r\n    overwrite=True  # Whether to overwrite an index if it already exists. If False, it'll return None and do nothing if `index_root/index_name` exists.\r\n)\r\n"
      },
      {
        "user": "bclavie",
        "created_at": "2024-11-04T06:40:43Z",
        "body": "@DebopamParam is spot-on here! Although with the model being as big as it is, CPU indexing will be awfully slow."
      }
    ]
  },
  {
    "number": 40,
    "title": "[bugfix] Fix logic bug to make  `device` argument be respected when passed in",
    "created_at": "2024-10-13T03:37:20Z",
    "closed_at": "2024-11-11T07:47:34Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/40",
    "body": "Why\r\n===\r\n\r\nCurrently if you pass in the `device` arg to `from_pretrained`, there is a logic bug which makes it get resolved to something else unnecessarily based on the device available. \r\n\r\nFor example, to reproduce, pass in `device=\"cpu\"` on an Mac and it can resolved to \"mps\".\r\n\r\nTurns out its just a bug in terms of logic in  how the device is being set .\r\n\r\n\r\nWhat Changed\r\n===\r\n\r\nFixed in this PR. ",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/40/comments",
    "author": "ms337",
    "comments": [
      {
        "user": "DebopamParam",
        "created_at": "2024-11-03T20:35:36Z",
        "body": "Good Observation bro"
      },
      {
        "user": "bclavie",
        "created_at": "2024-11-11T07:47:32Z",
        "body": "LGTM, thank you! Will ship in tomorrow's release."
      }
    ]
  },
  {
    "number": 39,
    "title": "Where should i put the models?",
    "created_at": "2024-10-11T19:00:56Z",
    "closed_at": "2024-11-04T06:42:23Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/39",
    "body": "I downloaded the models manually, is byaldi does automatically is not working in my case (guess that I don't know where to put the hugging face token).\r\n\r\nSo, where should I put the models?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/39/comments",
    "author": "metantonio",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-11-04T06:42:23Z",
        "body": "> So, where should I put the models?\r\n\r\nYou should be able to pass the model path to byaldi like you would for any other kind of model. The model loading logic is all handed off to `colpali-engine`, which itself hands it off to HF `transformers`. \r\n\r\nAs to where to put the token, the best practice recommended by HF is to store it as an environment variable called `HF_TOKEN`. If this isn't properly being fetched, please do feel free to report back, as it'd be a weird bug in how the library interacts with its dependencies!"
      }
    ]
  },
  {
    "number": 32,
    "title": "load the model in parallel gpu",
    "created_at": "2024-10-02T09:04:20Z",
    "closed_at": "2024-11-13T06:57:33Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/32",
    "body": "can we load the colpali model in parallel gpu?",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/32/comments",
    "author": "bakwankawa",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-11-13T06:57:33Z",
        "body": "What exactly would you want to achieve? There's quite a few meanings to having the model on parallel GPUs:\r\n- Load shards of the model on different GPUs, to minimise memory use\r\n- Load the model on multiple GPUs at once to parallelise processing of a single input as much as possible\r\n- Load multiple instances of the model to different GPUs to break down all the input documents and process them faster on independent model instances.\r\n\r\nIdeally we'd want all 3 to be easy to use, though right now the first two are easiest since we're building on top of HF (and therefore have the `device` argument and `device_map`)"
      }
    ]
  },
  {
    "number": 29,
    "title": "Add support for colqwen2",
    "created_at": "2024-09-28T00:00:01Z",
    "closed_at": "2024-10-03T07:28:19Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/29",
    "body": "Added support for Colqwen2.\r\n1. The changes are currently in colpali.py. Not sure if you would prefer a separate file for ColQwen2.\r\n2. I added a test file for colqwen2 but i didnt modify the all.py or e2e.py. I did however test locally for creating an index and querying it.\r\n\r\nChanges:\r\n1. Model support expansion (Line 67):\r\n   ```python\r\n   if \"colpali\" in pretrained_model_name_or_path.lower():\r\n       self.model = ColPali.from_pretrained(...)\r\n   elif \"colqwen2\" in pretrained_model_name_or_path.lower():\r\n       self.model = ColQwen2.from_pretrained(...)\r\n   ```\r\n\r\n2. Processor initialization update (Line 80):\r\n   ```python\r\n   if \"colpali\" in pretrained_model_name_or_path.lower():\r\n       self.processor = ColPaliProcessor.from_pretrained(...)\r\n   elif \"colqwen2\" in pretrained_model_name_or_path.lower():\r\n       self.processor = ColQwen2Processor.from_pretrained(...)\r\n   ```\r\n\r\n3. colpali-engine update in pyproject.toml:\r\n   ```toml\r\n   dependencies = [\r\n       \"colpali-engine>=0.3.1,<0.4.0\",\r\n       # ...\r\n   ]\r\n   ```\r\n\r\n4. New test file test_colqwen.py:\r\n   ```python\r\n   def test_load_colqwen_from_pretrained(colqwen_rag_model: RAGMultiModalModel):\r\n       assert isinstance(colqwen_rag_model, RAGMultiModalModel)\r\n       assert isinstance(colqwen_rag_model.model, ColPaliModel)\r\n       assert isinstance(colqwen_rag_model.model.model, ColQwen2)\r\n   ```\r\n",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/29/comments",
    "author": "jpetrantoni",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-10-03T07:28:16Z",
        "body": "Sorry, I had (another) bout of illness! I was just about to do this myself, but saw the PR!\r\n\r\nThis looks great. Thank you so much!"
      }
    ]
  },
  {
    "number": 25,
    "title": "cannot import name 'Unpack' from 'transformers.processing_utils'",
    "created_at": "2024-09-22T08:42:48Z",
    "closed_at": "2024-09-23T09:06:31Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/25",
    "body": "The code was working fine till this morning but then it started throwing this error while running this line of code:\r\n\r\nCode:\r\nRAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.2\", verbose=1)\r\n\r\nError:\r\nRuntimeError: Failed to import transformers.models.paligemma.processing_paligemma because of the following error (look up to see its traceback):\r\ncannot import name 'Unpack' from 'transformers.processing_utils' (/usr/local/lib/python3.10/dist-packages/transformers/processing_utils.py)",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/25/comments",
    "author": "akshaychdr",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-09-23T09:06:31Z",
        "body": "This should be fixed in the most recent release (on pypi in a few minutes), likely due to an old version of transformers -- we now enforce at least 4.42"
      }
    ]
  },
  {
    "number": 22,
    "title": "Add metadata filtering support and fix multi-document and metadata issue",
    "created_at": "2024-09-17T08:14:34Z",
    "closed_at": "2024-11-13T07:30:53Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/22",
    "body": "This PR includes the following changes\r\n\r\n1. If we pass multiple metadata for each document in a folder, the `add_to_index` will error out because in the below code, it tries to index the ith element, but it is a dictionary, hence it will raise `KeyError`. Fixed it by renaming `metadata` as `doc_metadata` and removed the indexing\r\n\r\n```python\r\nfor i, item in enumerate(input_items):\r\n     current_doc_id = doc_ids[i] if doc_ids else self.highest_doc_id + 1 + i\r\n     current_metadata = metadata[i] if metadata else None\r\n```\r\n2. Added support for filtering based on metadata, so that users can pass a `filter_metadata` field in the `search` and get ColPali will only search from these documents.\r\n\r\n```python\r\nresults = model.search(query, k=5,filter_metadata={\"file_name\":\"attention.pdf\"})\r\n\r\n\r\nprint(f\"Search results for '{query}':\")\r\nfor result in results:\r\n   print(f\"Doc ID: {result.doc_id}, Page: {result.page_num}, Score: {result.score}\")\r\n```\r\n\r\nResults\r\n```bash\r\nSearch results for 'what's the BLEU score of this new strange method?':\r\nDoc ID: 4, Page: 9, Score: 19.75\r\nDoc ID: 4, Page: 8, Score: 19.5\r\nDoc ID: 4, Page: 11, Score: 17.75\r\nDoc ID: 4, Page: 1, Score: 17.625\r\nDoc ID: 4, Page: 14, Score: 17.375\r\n\r\n```\r\n\r\nNow if we check doc ID to metadata\r\n\r\n```python\r\nmodel.model.doc_id_to_metadata\r\n\r\n```\r\n\r\nResults\r\n```\r\n{0: {'file_name': 'attention_table.png'},\r\n 1: {'file_name': 'product_c.png'},\r\n 2: {'file_name': 'financial_report.pdf'},\r\n 3: {'file_name': 'attention_with_a_mustache.pdf'},\r\n 4: {'file_name': 'attention.pdf'}}\r\n```\r\n\r\nIt only pulled from Doc ID 4, which we intended. \r\n\r\nPlease let me know about your suggestions. Looking forward to your collaboration",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/22/comments",
    "author": "Athe-kunal",
    "comments": [
      {
        "user": "Athe-kunal",
        "created_at": "2024-09-17T08:15:15Z",
        "body": "@bclavie \r\nLet me know about the PR, thanks"
      },
      {
        "user": "bclavie",
        "created_at": "2024-09-23T09:13:18Z",
        "body": "Hey, thanks for this, this PR is helpful!\r\n\r\nI have just a couple change requests\r\n- The policy is to never deprecate/modify an API, unless it is *absolutely critical*. As such, could you revert the changes so `doc_metadata` is back to `metadata`? The slight added clarity isn't worth making a breaking change.\r\n- Could you update the code to be compatible with the rest of the updated codebase (using the newer colpali engine)? Apologies for this, things move fast and the backend API changed slightly for which we acommodated!\r\n\r\nI'll play with this further in the next few days just to make sure nothing's broken. Thank you again!"
      },
      {
        "user": "nuschandra",
        "created_at": "2024-09-28T01:28:04Z",
        "body": "@bclavie @Athe-kunal I was trying to use byaldi for my project and realized that this PR still has an issue with the metadata when there are multiple documents. \r\n\r\nIn line 340, \r\ndoc_metadata = metadata[doc_id] if metadata else None -> We already get the metadata associated with the given doc_id which is a dictionary. \r\n\r\nIn line 406, we once again do current_metadata = metadata[i] if metadata else None -> This will give a key error. \r\n\r\nThis happens specifically in v0.0.4"
      },
      {
        "user": "Athe-kunal",
        "created_at": "2024-09-28T06:46:51Z",
        "body": "Hi @nuschandra and @bclavie \r\nSorry had a busy week. I got back to this issue and will implement the changes"
      },
      {
        "user": "Athe-kunal",
        "created_at": "2024-09-28T09:38:06Z",
        "body": "@bclavie \r\nI have addressed the PR comments, and also updated the tutorial `quick_overview.ipynb` for metadata based filtering. Looking forward to your suggestions."
      },
      {
        "user": "Athe-kunal",
        "created_at": "2024-09-28T09:38:42Z",
        "body": "@nuschandra \r\nThe bug has been fixed in my current fork. Hoping to merge it soon. Thanks"
      },
      {
        "user": "bclavie",
        "created_at": "2024-11-07T08:40:58Z",
        "body": "thanks @Athe-kunal. My apologies for the late review. LGTM at this stage, I'll run some tests and merge afterwards!"
      },
      {
        "user": "bclavie",
        "created_at": "2024-11-13T07:29:15Z",
        "body": "LGTM, works locally. Merging and adding into the next release, thank you so much @Athe-kunal "
      },
      {
        "user": "Athe-kunal",
        "created_at": "2024-11-13T07:50:31Z",
        "body": "That's great!\r\nThanks @bclavie for building this library, it is a great time-saver for my project"
      },
      {
        "user": "fvisconti",
        "created_at": "2024-12-13T18:04:50Z",
        "body": "Hi there, I'm using `byaldi` (good job!) installed with `poetry`.\r\n\r\nI get an error making this search: `results = RAG.search(query, k=10, filter_metadata={\"type\": \"Summaries\"})`. The error is: `AttributeError: 'list' object has no attribute 'items'`, and it is raised by this code:\r\n```python\r\nfor metadata_key,metadata_value in metadata_dict.items():\r\n    ...\r\n```\r\nwhich is in `byaldi/colpali.py` at line 598\r\n\r\nI'm creating the index with this function:\r\n\r\n```python\r\ndef make_index(input_dir, index_name):\r\n    first_subdir = True\r\n\r\n    for root, _, files in os.walk(input_dir):\r\n        pdf_files = [f for f in files if f.endswith('.pdf')]\r\n        if pdf_files:\r\n            subdir_name = os.path.basename(root)\r\n            metadata = [{\"company\": os.path.basename(input_dir), \"type\": subdir_name} for _ in range(len(pdf_files))]\r\n            \r\n            try:\r\n                if first_subdir:\r\n                    RAG.index(\r\n                        input_path=root,\r\n                        index_name=index_name,\r\n                        metadata=metadata,\r\n                        overwrite=True\r\n                    )\r\n                    first_subdir = False\r\n                else:\r\n                    RAG.add_to_index(\r\n                        input_item=root,\r\n                        store_collection_with_index=False,\r\n                        metadata=metadata,\r\n                    )\r\n            except Exception as e:\r\n                print(f\"Error processing directory {root}: {e}\")\r\n```\r\n\r\nIn the `README`, it seems to me that metadata are passed as list, so I don't understand where I'm wrong.\r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 17,
    "title": "Add index_root parameter to from_pretrained",
    "created_at": "2024-09-13T04:16:10Z",
    "closed_at": "2024-09-16T03:26:29Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/17",
    "body": "Currently the index_root cannot be set using RAGMultiModalModel.from_pretrained - the index has to be saved to the .byaldi default. This pull request adds the index_root parameter to RAGMultiModalModel.from_pretrained and ColPaliModel.from_pretrained, and adjusts the wording on the README about index_root as well.",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/17/comments",
    "author": "NathanielLovin",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-09-16T03:26:14Z",
        "body": "Hey, thank you for this! Ideally, the argument would be passed to bot the init, and also the, the `index()` call, so that someone could (if they wanted) override the root in a subsequent indexing process. The current situation where it's passed to neither is indeed *very* suboptimal haha.\r\n\r\nThis LGTM, I'll merge it!"
      }
    ]
  },
  {
    "number": 13,
    "title": "Document classification",
    "created_at": "2024-09-09T19:13:17Z",
    "closed_at": "2024-09-16T03:25:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/13",
    "body": "Thank you for this repo!\r\n\r\nI saw the text based RAG example where a text query returns the most likely page with the possible answer on it.\r\n\r\nIs it possible to use the system for document classification task where the query is not a text but a (page of an) another document? For example with a simple vector similarity of their embeddings? How can we access the embeddings of the pages themselves for this?\r\n\r\nRegards",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/13/comments",
    "author": "sandorkonya",
    "comments": [
      {
        "user": "sandorkonya",
        "created_at": "2024-09-10T13:00:22Z",
        "body": "The solution, if someone finds this:\r\n\r\n\r\n```\r\nfrom pdf2image import convert_from_path\r\nfrom byaldi.colpali import ColPaliModel\r\n\r\nmodel = ColPaliModel.from_pretrained(\"vidore/colpali-v1.2\", device=\"cuda\", verbose=1)\r\n\r\npdf_pages = convert_from_path(\"./docs/attention.pdf\", dpi=300 )\r\n\r\nencoded_result = ColPaliModel.encode_image(model, input_data=pdf_pages[0])\r\n\r\nencoded_result.shape\r\n```\r\n\r\n--> torch.Size([1, 1030, 128])\r\n"
      },
      {
        "user": "LarsAC",
        "created_at": "2024-09-11T07:06:31Z",
        "body": "So is the 1030x128 an embedding then ? Have you successfully clustered / classified documents via similarity on those ? "
      },
      {
        "user": "bclavie",
        "created_at": "2024-09-16T03:24:52Z",
        "body": "Hey,\r\n\r\nI'm not sure if late interaction is very strong as a classifier, or at least, not sure if it's any stronger than just using the base VLM (in this case, PaliGemma) to perform classification. But yes, in terms of using Byaldi, there are (currently undocumented as they're decently un-tested) helper functions `encode_image` and `encode_query` to get just raw embeddings out of the models."
      }
    ]
  },
  {
    "number": 11,
    "title": "External Index for the Data",
    "created_at": "2024-09-09T09:09:45Z",
    "closed_at": "2024-09-09T17:08:42Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/issues/11",
    "body": "First off, great library! I was aware of Copali but hadn't had a chance to try it until now. This library made it easy to experiment, and the results have been impressive. Now, I'm looking to productionize the process, particularly by separating the indexing process from the search function to improve scalability. From what I understand, the index is stored in memory. If I can locate where it‚Äôs stored, I could move it to external storage and load it during search operations.",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/11/comments",
    "author": "satish860",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-09-09T17:08:37Z",
        "body": "> First off, great library! I was aware of Copali but hadn't had a chance to try it until now. This library made it easy to experiment, and the results have been impressive. Now, I'm looking to productionize the process, particularly by separating the indexing process from the search function to improve scalability. From what I understand, the index is stored in memory. If I can locate where it‚Äôs stored, I could move it to external storage and load it during search operations.\r\n\r\nHey. I'll try to make it clearer, especially as I set out to write some documentation soon, but the index is persisted-on-creation, it never really just lives in memory! This is the purpose of `index_root` and `index_name` in the README. By default, index_root is `./.byaldi`, and `index_name` is the same you pass. Your index will live at `{index_root}/{index_name}`, and you can load it into the library with simple calls:\r\n\r\n```python3\r\nRAG = RAGMultiModalModel.from_index(INDEX_NAME, index_root=INDEX_ROOT)\r\n```"
      }
    ]
  },
  {
    "number": 7,
    "title": "added get_doc_ids_to_file_names to ColPaliModel",
    "created_at": "2024-09-06T13:58:29Z",
    "closed_at": "2024-09-06T16:27:19Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/7",
    "body": "Made sure that dict is int:str after reading back in with srsly. The default behaviour of srsly.read_gzip_json reads in str:str dict items which is different to the int:str dicts which get created on initial index creation for the model.",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/7/comments",
    "author": "velaia",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-09-06T16:27:17Z",
        "body": "Thanks! I was just fixing it when I noticed the PR üòÇ "
      }
    ]
  },
  {
    "number": 5,
    "title": "Update pyproject.toml",
    "created_at": "2024-09-06T11:05:01Z",
    "closed_at": "2024-09-06T11:16:36Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/5",
    "body": "Make use of colpali-engine v0.2.2 to enable other accelerators than CUDA",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/5/comments",
    "author": "velaia",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-09-06T11:16:34Z",
        "body": "Thank you, much appreciate the gumption in making this happen so fast!"
      },
      {
        "user": "velaia",
        "created_at": "2024-09-06T11:24:52Z",
        "body": "Thanks a lot for your outstanding work @AnswerDotAI ‚ù§Ô∏è"
      },
      {
        "user": "bclavie",
        "created_at": "2024-09-06T11:30:25Z",
        "body": "Live in 0.0.2! Should be on pypi in a couple minutes once it's propagated :)"
      }
    ]
  },
  {
    "number": 2,
    "title": "chore: typo fixes in readme",
    "created_at": "2024-09-05T16:38:19Z",
    "closed_at": "2024-09-05T19:31:33Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/2",
    "body": null,
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/2/comments",
    "author": "tm17-abcgen",
    "comments": [
      {
        "user": "bclavie",
        "created_at": "2024-09-05T19:31:32Z",
        "body": "Thanks you ü§ó "
      }
    ]
  },
  {
    "number": 1,
    "title": "add example notebook",
    "created_at": "2024-09-05T16:24:36Z",
    "closed_at": "2024-09-05T19:31:51Z",
    "labels": [],
    "url": "https://github.com/AnswerDotAI/byaldi/pull/1",
    "body": "Add a notebook which includes examples of how to chat with an academic paper and financial report.",
    "comments_url": "https://api.github.com/repos/AnswerDotAI/byaldi/issues/1/comments",
    "author": "comhar",
    "comments": [
      {
        "user": "comhar",
        "created_at": "2024-09-05T16:25:07Z",
        "body": "@bclavie "
      },
      {
        "user": "bclavie",
        "created_at": "2024-09-05T19:31:49Z",
        "body": "Thank you, this is excellent!"
      }
    ]
  }
]