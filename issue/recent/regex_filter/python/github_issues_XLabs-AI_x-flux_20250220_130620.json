[
  {
    "number": 140,
    "title": " Has anyone successfully created a ControlNet model?",
    "created_at": "2024-11-28T07:46:37Z",
    "closed_at": "2024-12-16T01:25:41Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/140",
    "body": "I followed the X-Flux code to create a ControlNet model. However, when using the Canny method, it didn’t work as I intended. The generated images didn’t reflect the conditions provided by the input images.\r\n\r\nI attempted to create a ControlNet model with the Canny preprocessor. My training dataset included 6,000 images, and I trained for approximately 30k steps. The learning rate (LR) was set to 2e-5 as provided in the initial configuration, and the image size was 1024x1024.\r\n\r\n\r\n\r\nEven if it's not specific to Canny, I would like to know general recommendations: \r\n\r\nWhat learning rate (LR) works best, how many images are typically needed, and how many steps or epochs should I train for successful results? Any advice or insights would be greatly appreciated.",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/140/comments",
    "author": "Artemis1111",
    "comments": [
      {
        "user": "thethanksforthegod",
        "created_at": "2024-12-02T18:36:43Z",
        "body": "tried and not working also"
      },
      {
        "user": "Artemis1111",
        "created_at": "2024-12-04T05:57:03Z",
        "body": "I conducted experiments with various learning rates, such as 8e-1 and 3e-2, for about 1000 steps each to check if any training progress was being made. However, the resulting safetensors files are identical to the pre-training safetensors files (verified through `sha256sum`). Additionally, the inference results are completely the same as before training.\r\n\r\nIf anyone has managed to make even minimal progress with training using x-flux, I would greatly appreciate it if you could share how you achieved it. Thank you!"
      },
      {
        "user": "Adenialzz",
        "created_at": "2024-12-15T07:48:54Z",
        "body": "How can the controlnet model be exactly the same as the pre-trained model? Which specific models do you mean?"
      },
      {
        "user": "Artemis1111",
        "created_at": "2024-12-16T01:25:39Z",
        "body": "I created `a pretrained ControlNet model` using the `X-Flux code`. However, no training took place at all. I later discovered the cause: I was supposed to use `a transformer model` to initialize `the pretrained ControlNet`, but I mistakenly used `an FP16-saved fine-tuned transformer model`.\r\n\r\nAlthough `X-Flux` anticipated such issues and provided a `mixed_precision` variable in the `test_canny_controlnet.yaml` file within the `train_configs` folder, setting this variable to `fp16` did not resolve the problem, and the loss continued to output `NaN`.\r\n\r\nFinally, I changed the saving format of the fine-tuned transformer model to `BF16`, and I was able to successfully fine-tune `the ControlNet model` (specifically, the `Canny` model)."
      }
    ]
  },
  {
    "number": 136,
    "title": "Can non-square images be trained on ControlNet?",
    "created_at": "2024-11-18T09:45:23Z",
    "closed_at": "2024-12-04T06:05:57Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/136",
    "body": "I have a few questions:\r\n\r\n1. Is it mandatory to use 1024x1024 resolution when fine-tuning ControlNet? For example, is it possible to train using 1280x768 images?\r\n\r\n2. Related to question 1, does ControlNet support training with various bucket resolutions?\r\n\r\n3. Has the pre-trained ControlNet model been trained on images of diverse resolutions?\r\n\r\n4. If the answer to question 3 is no, is the model still capable of performing inference on images with different resolutions?",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/136/comments",
    "author": "Artemis1111",
    "comments": [
      {
        "user": "Artemis1111",
        "created_at": "2024-11-19T04:49:07Z",
        "body": "anyone please answer me ToT"
      },
      {
        "user": "chenlijn",
        "created_at": "2024-11-22T01:32:07Z",
        "body": "Yes, it does support different resolutions, but both width and height should be divisible by 8."
      },
      {
        "user": "Artemis1111",
        "created_at": "2024-11-22T02:44:55Z",
        "body": "oh, thank you. Can a mixed dataset with varying image sizes still be trained in the same way?"
      },
      {
        "user": "oukohou",
        "created_at": "2024-12-02T08:39:30Z",
        "body": "SD3's paper, section5.3 described this in detail"
      }
    ]
  },
  {
    "number": 75,
    "title": "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
    "created_at": "2024-08-23T05:19:32Z",
    "closed_at": "2024-08-26T00:15:25Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/75",
    "body": "  File \"/root/data/x-flux/train_flux_lora_deepspeed.py\", line 301, in <module>\r\n    main()\r\n  File \"/root/data/x-flux/train_flux_lora_deepspeed.py\", line 149, in main\r\n    dit, optimizer, _, lr_scheduler = accelerator.prepare(\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1292, in prepare\r\n    result = tuple(\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1293, in <genexpr>\r\n    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1169, in _prepare_one\r\n    return self.prepare_model(obj, device_placement=device_placement)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1412, in prepare_model\r\n    model = model.to(self.device)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1174, in to\r\n    return self._apply(convert)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 780, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 805, in _apply\r\n    param_applied = fn(param)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1167, in convert\r\n    raise NotImplementedError(\r\nNotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\r\n    args.func(args)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1082, in launch_command\r\n    simple_launcher(args)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 688, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/opt/miniconda/bin/python', 'train_flux_lora_deepspeed.py', '--config', 'train_configs/test_lora.yaml']' returned non-zero exit status 1.",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/75/comments",
    "author": "starinskycc",
    "comments": [
      {
        "user": "whiterm",
        "created_at": "2024-08-25T20:52:46Z",
        "body": "It looks like you are trying to use the quantized model, which is not supported. You need to use the full version, not the quantized one."
      },
      {
        "user": "starinskycc",
        "created_at": "2024-08-26T00:15:22Z",
        "body": "Tks"
      }
    ]
  },
  {
    "number": 57,
    "title": "⭐  Add dev_req.txt for train",
    "created_at": "2024-08-19T08:06:42Z",
    "closed_at": "2024-08-20T17:45:08Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/pull/57",
    "body": null,
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/57/comments",
    "author": "kadirnar",
    "comments": [
      {
        "user": "Anghellia",
        "created_at": "2024-08-20T17:45:08Z",
        "body": "thank you, we will use your update!"
      }
    ]
  },
  {
    "number": 51,
    "title": "How to generate several images at one time by main.py(lora)",
    "created_at": "2024-08-16T13:07:49Z",
    "closed_at": "2024-08-21T07:50:36Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/51",
    "body": "I don't see some parameters represents the number of the image\r\nso how can i modify the `forward` function to generate such as 4 images at one time? ",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/51/comments",
    "author": "arrowonstr",
    "comments": [
      {
        "user": "Anghellia",
        "created_at": "2024-08-20T14:10:22Z",
        "body": "Hi, we added `--num_images_per_prompt` argument, please check"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-21T07:50:35Z",
        "body": "@Anghellia Thanks!"
      }
    ]
  },
  {
    "number": 46,
    "title": "How to train Flux LoRa with a local model dir?",
    "created_at": "2024-08-15T02:34:17Z",
    "closed_at": "2024-08-26T08:50:16Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/46",
    "body": "I download the model to my local dir (following the file structure of the flux repo), so i wanna use the local dir to train flux instead of downloading model by the procedure.\r\nI think it's easier to manage. \r\nHow can I do it? I tried to modify the code but failed\r\nI will be grateful if you can solve the problem",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/46/comments",
    "author": "arrowonstr",
    "comments": [
      {
        "user": "chy-chiu",
        "created_at": "2024-08-15T05:47:43Z",
        "body": "You can do that by specifying environment variables FLUX_DEV, FLUX_DEV_FP8, and AE"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-15T06:13:32Z",
        "body": "Thanks!\r\nI got it \"ckpt_path=os.getenv(\"FLUX_DEV\")\"\r\nI think local dir FLUX_DEV should contain \"flux1-dev.safetensors\"\"model_index.json\"\"ae.safetensors\" the three files\r\n\r\nand these:\r\n```\r\ndef load_t5(device: str | torch.device = \"cuda\", max_length: int = 512) -> HFEmbedder:\r\n    # max length 64, 128, 256 and 512 should work (if your sequence is short enough)\r\n    return HFEmbedder(\"xlabs-ai/xflux_text_encoders\", max_length=max_length, torch_dtype=torch.bfloat16).to(device)\r\n\r\ndef load_clip(device: str | torch.device = \"cuda\") -> HFEmbedder:\r\n    return HFEmbedder(\"openai/clip-vit-large-patch14\", max_length=77, torch_dtype=torch.bfloat16).to(device)\r\n```\r\n```\r\nclass HFEmbedder(nn.Module):\r\n    def __init__(self, version: str, max_length: int, **hf_kwargs):\r\n        super().__init__()\r\n        self.is_clip = version.startswith(\"openai\")\r\n        self.max_length = max_length\r\n        self.output_key = \"pooler_output\" if self.is_clip else \"last_hidden_state\"\r\n\r\n        if self.is_clip:\r\n            self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: CLIPTextModel = CLIPTextModel.from_pretrained(version, **hf_kwargs)\r\n        else:\r\n            self.tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: T5EncoderModel = T5EncoderModel.from_pretrained(version, **hf_kwargs)\r\n```\r\ndownload the tow repo and change the version to local dir, I will try it, hope it works!"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-16T05:07:23Z",
        "body": "It works and make sure to follow these:\r\n\r\n```\r\nexport FLUX_DEV=\"/root/autodl-tmp/project/flux-d/model/FLUX.1-dev/flux1-dev.safetensors\"\r\nexport AE=\"/root/autodl-tmp/project/flux-d/model/FLUX.1-dev/ae.safetensors\"\r\n```\r\nmodify the code\r\n```\r\ndef load_t5(device: str | torch.device = \"cuda\", max_length: int = 512) -> HFEmbedder:\r\n    # max length 64, 128, 256 and 512 should work (if your sequence is short enough)\r\n    return HFEmbedder(\"local_dir/xflux_text_encoders\", max_length=max_length, torch_dtype=torch.bfloat16).to(device)\r\n\r\ndef load_clip(device: str | torch.device = \"cuda\") -> HFEmbedder:\r\n    return HFEmbedder(\"local_dir/clip-vit-large-patch14\", max_length=77, torch_dtype=torch.bfloat16).to(device)\r\n```\r\n```\r\nclass HFEmbedder(nn.Module):\r\n    def __init__(self, version: str, max_length: int, **hf_kwargs):\r\n        super().__init__()\r\n        # from local dir\r\n        #self.is_clip = version.startswith(\"openai\")\r\n        if version == \"/root/autodl-tmp/project/flux-d/model/xflux_text_encoders\":\r\n            self.is_clip= False\r\n        else:\r\n            self.is_clip= True\r\n        self.max_length = max_length\r\n        self.output_key = \"pooler_output\" if self.is_clip else \"last_hidden_state\"\r\n\r\n        if self.is_clip:\r\n            self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: CLIPTextModel = CLIPTextModel.from_pretrained(version, **hf_kwargs)\r\n        else:\r\n            self.tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: T5EncoderModel = T5EncoderModel.from_pretrained(version, **hf_kwargs)\r\n\r\n        self.hf_module = self.hf_module.eval().requires_grad_(False)\r\n```\r\n\r\ncarefully modify the code to specify which version represent a clip model\r\n```\r\n if version == \"/root/autodl-tmp/project/flux-d/model/xflux_text_encoders\":\r\n            self.is_clip= False\r\n        else:\r\n            self.is_clip= True\r\n```"
      }
    ]
  },
  {
    "number": 27,
    "title": "not using autocast?",
    "created_at": "2024-08-09T20:37:59Z",
    "closed_at": "2024-08-09T23:44:59Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/27",
    "body": "hello, i was having a look through the code today and noticed that weight_dtype is initialised but never used. the `accelerator.mixed_precision` value is used to set `args.mixed_precision` but this isn't called anywhere.\r\n\r\nyou probably have to use the context manager from Accelerate, eg. `with accelerator.autocast():`\r\n\r\ncurrently, everything is in fp32, even when the user requests bf16.",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/27/comments",
    "author": "bghira",
    "comments": [
      {
        "user": "AndreRatzenberger",
        "created_at": "2024-08-09T23:40:00Z",
        "body": "DeepSpeed is handling the casting. You have to obviously set bf16 to true in your deepspeed config. \r\n\r\nAlso when calculating model_pred there are some manual casts into weight_dtype"
      },
      {
        "user": "bghira",
        "created_at": "2024-08-09T23:43:22Z",
        "body": "yes, i saw the casting of the model inputs, when i said \"not used\" i mean, to apply to the model at all."
      },
      {
        "user": "bghira",
        "created_at": "2024-08-09T23:44:59Z",
        "body": "thanks for the info. deepspeed is really bad at autocast. it does it like a bull in a china shop. so, any training problems elsewhere are unrelated to precision."
      }
    ]
  }
]