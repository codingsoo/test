[
  {
    "number": 192,
    "title": "大佬们我还想学一下模型部署，可以简单讲解一下吗",
    "created_at": "2025-02-19T05:09:08Z",
    "closed_at": "2025-02-19T05:42:09Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/192",
    "body": "我想知道，比如说自己训练好minimind之后怎样把模型部署在ollama上，可以访问ollama服务器来进行对话呢？",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/192/comments",
    "author": "akie585858",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-19T05:42:09Z",
        "body": "你好，\n不支持llama.cpp\n不支持ollama\n\nthanks"
      }
    ]
  },
  {
    "number": 183,
    "title": "请问为什么保存lora权重前后需要调整模型为eval，保存完成以后再返回train呢？",
    "created_at": "2025-02-15T11:32:12Z",
    "closed_at": "2025-02-18T07:37:20Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/183",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/183/comments",
    "author": "dhr1997",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-15T11:41:11Z",
        "body": "lora训练时候确实不需要eval模式，只是顺手的事~和前面统一"
      }
    ]
  },
  {
    "number": 176,
    "title": "sft_512训练报错",
    "created_at": "2025-02-13T09:48:20Z",
    "closed_at": "2025-02-13T13:01:24Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/176",
    "body": "训练到第三轮的时候报错\npython train_full_sft.py --epochs 3 --batch_size 128 --data_path ./dataset/sft_512.jsonl\n\ntorch                 2.6.0+cu126\ntorchaudio            2.6.0+cu126\ntorchtext             0.18.0\ntorchvision           0.21.0+cu126\ntornado               6.4.2\ntqdm                  4.67.1\ntransformers          4.44.0\n\n```\nEpoch:[2/3](42300/53129) loss:1.552 lr:0.000022369865 epoch_Time:48.0min:\nEpoch:[2/3](42400/53129) loss:1.579 lr:0.000022322955 epoch_Time:47.0min:\nTraceback (most recent call last):\n  File \"D:\\ai\\projects\\minimind-v2\\train_full_sft.py\", line 195, in <module>\n    train_epoch(epoch, wandb)\n  File \"D:\\ai\\projects\\minimind-v2\\train_full_sft.py\", line 56, in train_epoch\n    scaler.scale(loss).backward()\n  File \"C:\\Users\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\_tensor.py\", line 626, in backward\n    torch.autograd.backward(\n  File \"C:\\Users\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"C:\\Users\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 823, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: isDifferentiableType(grad.scalar_type()) INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\pytorch\\\\torch\\\\csrc\\\\autograd\\\\engine.cpp\":982, please report a bug to PyTorch.\n```",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/176/comments",
    "author": "beston123",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T13:01:24Z",
        "body": "error与项目无关\n\n建议按照提示 “please report a bug to PyTorch”，thanks"
      }
    ]
  },
  {
    "number": 175,
    "title": "转gguf报错",
    "created_at": "2025-02-13T09:28:48Z",
    "closed_at": "2025-02-13T13:02:43Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/175",
    "body": "python convert_hf_to_gguf.py ~/MiniMind2 --outtype auto --outfile MiniMind2\nINFO:hf-to-gguf:Loading model: MiniMind2\nERROR:hf-to-gguf:Model MiniMindLM is not supported\n\n请问如何正确转",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/175/comments",
    "author": "fuhao009",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T09:39:51Z",
        "body": "模型结构不兼容不支持llama.cpp"
      },
      {
        "user": "fuhao009",
        "created_at": "2025-02-13T09:48:48Z",
        "body": "目前如果生产想用推荐哪种方式？"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T13:02:43Z",
        "body": "目前只兼容原生transformers方式，不兼容第三方推理框架"
      }
    ]
  },
  {
    "number": 171,
    "title": "预训练报错",
    "created_at": "2025-02-13T07:25:32Z",
    "closed_at": "2025-02-14T11:59:32Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/171",
    "body": "/pretrain/minimind# python 1-pretrain.py   \n\nLLM总参数量：26.878 百万\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [96,0,0] Assertion srcIndex < srcSelectDimSize failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [97,0,0] Assertion srcIndex < srcSelectDimSize failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [98,0,0] Assertion srcIndex < srcSelectDimSize failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [99,0,0] Assertion srcIndex < srcSelectDimSize failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [100,0,0] Assertion srcIndex < srcSelectDimSize failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [101,0,0] Assertion srcIndex < srcSelectDimSize failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [420,0,0], thread: [102,0,0] Assertion srcIndex < srcSelectDimSize failed.\n...\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n  File \"/root/anaconda3/envs/yx_pretrain/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data1/yx/pretrain/minimind/model/model.py\", line 31, in forward\n    output = self._norm(x.float()).type_as(x)\n  File \"/data1/yx/pretrain/minimind/model/model.py\", line 28, in _norm\n    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\nRuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with TORCH_USE_CUDA_DSA to enable device-side assertions.",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/171/comments",
    "author": "shun-qi",
    "comments": [
      {
        "user": "shun-qi",
        "created_at": "2025-02-13T07:26:29Z",
        "body": "你好作者，请问这是什么原因？"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T07:33:07Z",
        "body": "你拉取一下最新的代码\n\ngit clone\n\nor\n\ngit pull origin master"
      },
      {
        "user": "shun-qi",
        "created_at": "2025-02-13T08:37:18Z",
        "body": "最新的分词器训练后发现：\ndecoder和原始文本是否一致： False"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T09:01:03Z",
        "body": "line 142\nresponse = tokenizer.decode(input_ids, skip_special_tokens=False)\n\n改成false\n\n稍后更新这个问题"
      },
      {
        "user": "shun-qi",
        "created_at": "2025-02-13T11:41:06Z",
        "body": "预训练后推理乱码了\n\n👶: 成语\n🤖️: ：中国古典主义\n成语：成语\n意思：通过对古代文物的描绘，传达了中国古代文化的魅力。这个成语出自中国古代文学中的经典名句：“巅峰之夜，七七四五六六六六六六六六六六六六六六六六六六六六六六六\n六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六\n六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六\n六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六六����十六����在古代文学\n中的成之�、十六��\n成文言：十六���、���之计\n成成之文中的征�：�成文写成，统之中贤，和以其三说三国、三、六��十��之后的历代代代，是《古代文的吝，�������五��：�����������������������������������������������\n��������，�������������������来���������，知言：中国三��\n成语之后的之实“古代代称：成。\n成：一�\n成：当元中国之��以�之称道闻道：“二�以�名道"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T12:37:53Z",
        "body": "无更多信息，无法判断\n\n---\n\n暂定预训练流程正确且loss正确收敛的前提下，归因于tokenizer的问题\n\n建议拉取最新代码，根据已有信息观察建议\n\nline 28换成\n\ndata_path = '../dataset/pretrain_hq.jsonl'，重新训练分词器再重新尝试预训练"
      }
    ]
  },
  {
    "number": 169,
    "title": "我看git图片介绍中有代码编写，代码训练数据集用的哪个呢",
    "created_at": "2025-02-13T04:17:52Z",
    "closed_at": "2025-02-13T13:03:11Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/169",
    "body": "我看git图片介绍中有代码编写，代码训练数据集用的哪个呢，没有找到，望指点，谢谢",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/169/comments",
    "author": "lins0621",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-13T04:44:36Z",
        "body": "没有专门对code数据集做训练，全领域都混合在sft_xxx.jsonl里面了"
      }
    ]
  },
  {
    "number": 168,
    "title": "预训练数据集预处理方法",
    "created_at": "2025-02-12T08:56:46Z",
    "closed_at": "2025-02-13T13:03:11Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/168",
    "body": "您好！\n\n非常感谢您构建了这个非常简洁方便的大模型预训练代码库。想请问一下您是否可以release一下生成pretrain_hq.jsonl的预处理脚本？目前提供的封装版本的数据集都非常方便，不过希望可以自己拓展处理一些额外的数据。",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/168/comments",
    "author": "SimingYan",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-12T12:42:10Z",
        "body": "感谢认可\n\norigin_pretrain_hq也没什么好说的，就是普通的把sft数据的\"q+a\"拼起来\n\n这里贴一下把origin_pretrain_hq转换成现在的pretrain_hq.jsonl的脚本就好。\n\n合并原有短文本到尽可能填满`max_len`的更紧凑的新数据集，避免数据长度的稀疏，增加训练效率：\n\n```python\nimport json\n\n\ndef merge_short_texts(input_path, output_path, max_len=512):\n    with open(input_path, 'r', encoding='utf-8') as infile:\n        texts = []\n        for line in infile:\n            data = json.loads(line.strip())\n            text = data.get(\"text\", \"\")\n            texts.append((data, len(text)))\n\n        texts.sort(key=lambda x: x[1])\n        current_group = []\n        current_len = 0\n        group_count = 0\n\n        with open(output_path, 'w', encoding='utf-8') as outfile:\n            for data, text_len in texts:\n                bounded_text_len = text_len + len(\"<s>\") + len(\"</s>\")\n                if current_len + bounded_text_len > max_len:\n                    merged_text = \" \".join([f\"<s>{d['text']}</s>\" for d in current_group])\n                    merged_data = {\n                        \"text\": merged_text,\n                    }\n                    json.dump(merged_data, outfile, ensure_ascii=False)\n                    outfile.write('\\n')\n                    group_count += 1\n                    current_group = [data]\n                    current_len = bounded_text_len\n                else:\n                    current_group.append(data)\n                    current_len += bounded_text_len\n\n            if current_group:\n                merged_text = \" \".join([f\"<s>{d['text']}</s>\" for d in current_group])\n                merged_data = {\n                    \"text\": merged_text,\n                }\n                json.dump(merged_data, outfile, ensure_ascii=False)\n                outfile.write('\\n')\n                group_count += 1\n\n        print(f\"Merged {group_count} items into the final file.\")\n\n\ninput_file = './dataset/origin_pretrain_hq.jsonl'\noutput_file = './dataset/pretrain_hq.jsonl'\n\nmerge_short_texts(input_file, output_file)\n\n```"
      }
    ]
  },
  {
    "number": 165,
    "title": "dpo.init_model 路径问题",
    "created_at": "2025-02-12T06:34:39Z",
    "closed_at": "2025-02-12T13:35:47Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/165",
    "body": "train_dpo.py中，原始代码是对蒸馏模型在做rlhf，如果对sft模型做dpo，需要修改ckp的路径？\n`\ndef init_model(lm_config):\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\n    model = MiniMindLM(lm_config)\n    moe_path = '_moe' if lm_config.use_moe else ''\n    ckp = f'./out/full_dist_{lm_config.dim}{moe_path}.pth'\n\n`",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/165/comments",
    "author": "Ming-Zhou0201",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-12T06:57:29Z",
        "body": "是的修改dist为sft，这个忘记修改了，今晚update一下，thanks"
      }
    ]
  },
  {
    "number": 158,
    "title": "Is it possible to train for a code model?",
    "created_at": "2025-02-10T03:18:25Z",
    "closed_at": "2025-02-10T09:15:12Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/158",
    "body": "I am glad to do some support on code related traning data.",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/158/comments",
    "author": "FFengIll",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T09:15:12Z",
        "body": "This is more of a data collection task rather than the minimal implementation needed for the focus. There are already many code datasets available to enhance performance in the post-training phase.\n\nI am currently unable to cover every niche area due to limited time and energy. Feel free to reach out for any other questions, and thanks."
      }
    ]
  },
  {
    "number": 157,
    "title": "参数量大的模型会爆显存吗？有必要用megatron吗？",
    "created_at": "2025-02-09T13:33:57Z",
    "closed_at": "2025-02-10T09:00:29Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/157",
    "body": "想问下，这个项目支持大点模型的训练吗，比如7b或者十几b，会不会爆显存 ？ ",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/157/comments",
    "author": "yankuo111",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T09:00:29Z",
        "body": "支持；会爆；有必要；\n\n但是超过1B的模型就没有必要参考这个项目了，这个项目只负责完成LLM的最小实现，不做AI Infra和高性能训练框架拓展。\n\n其他问题欢迎继续交流，thanks"
      }
    ]
  },
  {
    "number": 156,
    "title": "关于“”更高效的kvcache“”的疑问",
    "created_at": "2025-02-08T15:15:34Z",
    "closed_at": "2025-02-10T08:43:52Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/156",
    "body": "我看代码中其实只保留了xk和xv。但是每次在generate的时候，只传递进去了最后一个token。是不是少了点什么，在我的理解当中，缓存中应该是前面所有token的QKT和V。但是里面只保留了xk和xv。换句话说，是不是前面token的q没有参与到注意力的计算里面。这个前面token的q既没有缓存，也没有添加到输入里面。\n\n还有另一个问题，为什么我单步调式的时候，generate这个方法进不去呢？有遇到过类似的情况吗？",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/156/comments",
    "author": "dhr1997",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-09T05:25:45Z",
        "body": "缓存中应该是前面所有token的QKT和V\n不是的，cache中保存k和v即可\n\n推理到第n个序列的时候\n\n- **Q (Query)**: 形状为 `(1, dim)`\n- **K (Key)**: 形状为 `(n, dim)`\n- **V (Value)**: 形状为 `(n, dim)`\n\n`Q` 会与 `K^T` 进行点积操作，得到一个形状为 `attn=(1, n)` 的矩阵（复杂度o(n*dim)）\n\n接着，归一化后的attn分数与 `V=(n, dim)` 相乘，最终得到一个形状为 `(1, dim)` 的输出向量。（复杂度o(n*dim)）\n\n前面的 `Q` 是不会被用到的。\n\n---\n\n没有遇到过，你是否在什么子线程中debug，或者找错了函数呢"
      },
      {
        "user": "David-19940718",
        "created_at": "2025-02-11T06:13:47Z",
        "body": "> 我看代码中其实只保留了xk和xv。但是每次在generate的时候，只传递进去了最后一个token。是不是少了点什么，在我的理解当中，缓存中应该是前面所有token的QKT和V。但是里面只保留了xk和xv。换句话说，是不是前面token的q没有参与到注意力的计算里面。这个前面token的q既没有缓存，也没有添加到输入里面。\n> \n> 还有另一个问题，为什么我单步调式的时候，generate这个方法进不去呢？有遇到过类似的情况吗？\n\n你猜猜为什么叫 `kvcache`? 😄 "
      }
    ]
  },
  {
    "number": 141,
    "title": "dpo强化学习报错'generator' object has no attribute 'generate'",
    "created_at": "2025-02-06T08:08:48Z",
    "closed_at": "2025-02-10T08:40:33Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/141",
    "body": "具体报错内容：\n```python\nTraceback (most recent call last):\n  File \"/root/train_about/llm_from_zero/my_minimind/5-dpo-train.py\", line 74, in <module>\n    dpo_trainer.train()\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/trainer.py\", line 2171, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/trainer.py\", line 2480, in _inner_training_loop\n    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py\", line 1508, in get_batch_samples\n    policy_output = model.generate(\n                    ^^^^^^^^^^^^^^\nAttributeError: 'generator' object has no attribute 'generate'\n```\n\n查看模型定义中，generate方法，是yield出去结果，把yield修改成return后还是会有这个错误，查看huggingface中开源的代码，里面的model定义，generate方法中也是yield出去，应该不是这个问题的异常。\n\n目前是用的包版本：\n```text\ntorch==2.4.0\ntransformers==4.48.2\ntrl=0.11.3\n\n```\n\n暂时还没找到具体问题",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/141/comments",
    "author": "ykallan",
    "comments": [
      {
        "user": "ykallan",
        "created_at": "2025-02-06T08:13:46Z",
        "body": "现在尝试一下是用huggingface开源的模型权重来进行dpo，看看能否跑通"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-06T08:22:03Z",
        "body": "transformers==4.44.0\n\n降一下版本"
      },
      {
        "user": "ykallan",
        "created_at": "2025-02-06T08:37:43Z",
        "body": "> transformers==4.44.0\n> \n> 降一下版本\n\n降低了版本到4.44.0，但是我前面训练的tokenizer权重就不能加载了，这个就不太理解，我试试重新训练一下分词器权重再看看，谢谢up"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-06T08:57:44Z",
        "body": "暂时不用重新训练，否则你前面前功尽弃了\n\n现在的error是什么呢"
      },
      {
        "user": "ykallan",
        "created_at": "2025-02-06T09:03:17Z",
        "body": "```text\nTraceback (most recent call last):\n  File \"/root/train_about/llm_from_zero/my_minimind/5-dpo-train.py\", line 49, in <module>\n    model, tokenizer = init_model()\n                       ^^^^^^^^^^^^\n  File \"/root/train_about/llm_from_zero/my_minimind/5-dpo-train.py\", line 23, in init_model\n    tokenizer = AutoTokenizer.from_pretrained(my_tokenizer_pretrained)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 897, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2271, in from_pretrained\n    return cls._from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2505, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py\", line 115, in __init__\n    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nException: data did not match any variant of untagged enum ModelWrapper at line 31025 column 3\n```"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T08:40:33Z",
        "body": "尝试新的pytorch原生强化学习代码，仓库已大更新\n\n先关闭了～"
      }
    ]
  },
  {
    "number": 133,
    "title": "考虑使用MLA吗",
    "created_at": "2025-02-01T19:01:50Z",
    "closed_at": "2025-02-10T09:11:31Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/133",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/133/comments",
    "author": "GlancerZ",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T09:11:31Z",
        "body": "无计划，项目聚焦于最小实现，无法兼顾实现每天都在更新的「所有」LLM变化\n\nMLA复杂实现换取的微弱推理速度提升在小模型中性价比太低\n\n当出现更「炸裂」更有意义的结构或者技术`minimind`都会尝试复现\n\n其他问题欢迎继续交流，thanks"
      }
    ]
  },
  {
    "number": 131,
    "title": "考虑加入蒙特卡洛搜索吗",
    "created_at": "2025-01-30T03:40:41Z",
    "closed_at": "2025-02-10T08:54:07Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/131",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/131/comments",
    "author": "Gera001",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T08:54:08Z",
        "body": "无～\n\n随着DeepSeek-R1的爆火\n\n事实证明，现存的Reasoning-LLM并没有采用MCTS\n\n这是一个基于闭源o1几乎「错误且复杂」的猜测方向"
      }
    ]
  },
  {
    "number": 130,
    "title": "why use BOS and EOS token and not only the EOS?",
    "created_at": "2025-01-28T10:12:04Z",
    "closed_at": "2025-02-10T08:43:50Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/130",
    "body": "In the `model/dataset.py`\n\nline 29\n\n` text = f\"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}\"` \n\nyou prepend bos token and then append the eos token to the end of text and pad the tokens if shorter than the context length. Is there any reason for doing that? \n\nand why don't you just append the EOS token to the end of the text and fill it with the next sequence rather than padding it?",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/130/comments",
    "author": "CohleM",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-01-28T12:24:54Z",
        "body": "1. In pre-training, there are practices that use both [BOS] and [EOS] tokens to define text boundaries, as well as approaches that omit [BOS] and only use text + [EOS]. We chose the latter, as there is no strict rule. On the other hand, in the instruction fine-tuning template for chat, dialogue roles start with [BOS] + user/assistant/system. Therefore, we maintained the consistency of starting with [BOS] during pre-training.\n\n2. The padding after [EOS] is masked to avoid participating in loss calculation, preventing the model from learning invalid information. If unrelated segments are dynamically concatenated, it introduces complexity and may create false contextual associations.\n\n3. Independent sample processing (non-concatenation) aligns better with real-world inference scenarios (e.g., single-document generation), avoiding cross-text semantic contamination."
      },
      {
        "user": "CohleM",
        "created_at": "2025-01-28T12:35:50Z",
        "body": "Thank you for the response. My concern was with the addition of pad tokens, as it simply wastes the extra computation which could have been used if the sequences were simply concatenated. \n\n\nFor\n\n> 3. Independent sample processing (non-concatenation) aligns better with real-world inference scenarios (e.g., single-document generation), avoiding cross-text semantic contamination.\n\ndo you have any data that backs this claim? I'm currently exploring which process (concatenation or non-concatenation) better suites pre-training and would really appreciate your help.\n"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-01-28T13:38:07Z",
        "body": "First, the consensus is that the computational waste caused by padding is unavoidable, with only the degree of severity varying.  \nIf it is [Document A content...<EOS>][Document B content...<EOS>][Document C content...<EOS>] ➜ processed as a single sample during training.\n\nI believe that if the input sequence contains unrelated texts (e.g., news + code), the model will simultaneously learn false associations between them (e.g., \"stock rise\" followed by `print(\"Hello\")`), leading to confusion in distribution modeling. However, since EOS serves as a delimiter, it allows the model to infer explicit boundaries.\n\nGPT-3's *\"Language Models are Few-Shot Learners\"* mentions that using a reasonable context window and clear separators indeed helps the model better understand document boundaries and reduce perplexity. *\"Scaling Laws for Neural Language Models\"* points out that noise or irrelevant information within long contexts may increase perplexity, so it is more beneficial to avoid concatenating documents during training and keep each sample independent.\n\nAlthough EOS exists, it still inevitably slightly increases the model's perplexity (only a matter of degree compared to independent data). However, I have yet to find objective data supporting this view, as all papers only provide subjective evaluations.\n\nNevertheless, compared to the increase in training efficiency, the slight cost is, in my opinion, entirely acceptable."
      }
    ]
  },
  {
    "number": 128,
    "title": "您好，我想了解下Seq-Monkey通用文本数据集的构建",
    "created_at": "2025-01-27T19:03:38Z",
    "closed_at": "2025-02-10T08:49:42Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/128",
    "body": "Readme中提到Seq-Monkey通用文本数据集是由多种公开来源的数据（如网页、百科、博客、开源代码、书籍等）汇总清洗而成。整理成统一的JSONL格式，并经过了严格的筛选和去重，确保数据的全面性、规模、可信性和高质量。\n问题：我想咨询的是这里数据的筛选和去重用的是什么方式？能给笔记和代码学习一下吗",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/128/comments",
    "author": "pengfan7758258",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T08:49:42Z",
        "body": "“并经过了严格的筛选和去重，确保数据的全面性、规模、可信性和高质量。”\n\n此话摘自seq-monkey官方说明，不是我说的😄\n\n数据筛选不是本项目的主要内容，抱歉无法给出更多案例\n\n如有需要可以后面继续讨论，thanks\n"
      }
    ]
  },
  {
    "number": 127,
    "title": "mac book pro m1能用吗",
    "created_at": "2025-01-26T09:38:11Z",
    "closed_at": "2025-02-10T09:12:03Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/127",
    "body": "mac book pro m1能用吗",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/127/comments",
    "author": "skyrain888",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-01-27T16:00:08Z",
        "body": "可以哦，训练的话会比较耗时"
      }
    ]
  },
  {
    "number": 125,
    "title": "考虑加一个Reasoning模型么？",
    "created_at": "2025-01-24T08:47:54Z",
    "closed_at": "2025-02-10T08:47:27Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/125",
    "body": "蒸馏一下Deepseek的R1",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/125/comments",
    "author": "kexul",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T08:47:27Z",
        "body": "updated"
      }
    ]
  },
  {
    "number": 122,
    "title": "如何训练一个垂直领域的模型",
    "created_at": "2025-01-22T22:43:41Z",
    "closed_at": "2025-02-10T08:47:52Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/122",
    "body": "比如我有一个数据集，使用自然语言描述数据结构，模型的任务是还原出数据结构。\n是否需要从头构建tokenizer和预训练数据集呢，以及tokenizer和预训练数据集是否要完全基于我的数据集构建呢，望解惑。",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/122/comments",
    "author": "lyw02",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-01-23T01:59:34Z",
        "body": "`tokenizer` 在任意数据集中都不需要重新构建\n\n`minimind2` 将会给出新的数据集格式，可以用于构建自己的垂直任务\n\n未来几天很快发布"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T08:47:52Z",
        "body": "updated\n\n> `tokenizer` 在任意数据集中都不需要重新构建\n> \n> `minimind2` 将会给出新的数据集格式，可以用于构建自己的垂直任务\n> \n> 未来几天很快发布\n\n"
      }
    ]
  },
  {
    "number": 120,
    "title": "Is it possible to update the model to DeepSeek-V3 architecture?",
    "created_at": "2025-01-21T04:48:28Z",
    "closed_at": "2025-02-10T08:46:58Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/120",
    "body": "Just wondering if it id possible to update the model to DeepSeek-V3 architecture?\n\nIt claimed to be more computational efficient.",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/120/comments",
    "author": "mw66",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-02-10T08:46:58Z",
        "body": "Sorry, I don't have a schedule. Minimind is a minimal implementation of streamlined LLM and cannot include 100% of the world's content"
      }
    ]
  },
  {
    "number": 119,
    "title": "这个只能训练中文Model吗，英文model可以训练吗，求大佬讲解",
    "created_at": "2025-01-20T14:47:25Z",
    "closed_at": "2025-02-10T08:43:49Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/119",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/119/comments",
    "author": "quiteqiang",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-01-23T01:58:05Z",
        "body": "可以的哦，有对应的数据集即可\n`minimind2` 支持英文（但能力较中文很弱），会同步给出英文版本数据集\n未来几天即将更新"
      }
    ]
  },
  {
    "number": 118,
    "title": "训练时,输入的token会补全到最大长度.那是不是意味着attention计算时,是按照满token的时间复杂度计算的",
    "created_at": "2025-01-20T08:55:04Z",
    "closed_at": "2025-02-10T08:43:49Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/118",
    "body": "推理时,不需要补全,所以计算时间与输入token长度有关?而训练时,则是满token的时间?我理解的对吗",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/118/comments",
    "author": "lightning0016",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-01-20T09:23:58Z",
        "body": "☑️ 推理时，不需要补全，所以计算时间与输入token长度有关\n☑️ 而训练时，则是满token的时间。\n\n但是需要补充的几点：\n1、有kv_cache的情况下，推理耗时「几乎」和token长度无关，线性增长。（之所以用「几乎」，是因为随着长度增长内存读写开销也要考虑）\n2、训练时确实是padding满最大max-len长度的tokens计算。只不过相较于推理还多出了「反向传播」、「优化器更新」、「loss计算」等时间，相同长度tokens的训练是推理时间的n倍。"
      },
      {
        "user": "cqcracked",
        "created_at": "2025-01-23T02:21:48Z",
        "body": "有kv_cache的情况下，推理耗时「几乎」和token长度无关?  1,2,3推理出4之后，这个4也要同1，2，3做自注意力，然后1，2，3，4推理出5.依次这样，做自注意力还是和token长度有关，所以推理耗时也同token长度有关？谢谢\n@jingyaogong "
      },
      {
        "user": "HEroKuma",
        "created_at": "2025-02-08T13:43:21Z",
        "body": "我的理解是有kv_cache的話, 可以把123的推理結果保留不用重新計算, 只要計算新的token(4的部分)的KV, 然後合併到123的KV一起跟Q去計算響應程度, 這樣可以大幅減少計算量\n雖然耗時還是回隨著token長度增加, 但是不會是token^2的增長幅度"
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-02-09T05:28:07Z",
        "body": "推理到第n个序列的时候\n\n- **Q (Query)**: 形状为 `(1, dim)`\n- **K (Key)**: 形状为 `(n, dim)`\n- **V (Value)**: 形状为 `(n, dim)`\n\n`Q` 会与 `K^T` 进行点积操作，得到一个形状为 `attn=(1, n)` 的矩阵（复杂度o(n*dim)）\n\n接着，归一化后的attn分数与 `V=(n, dim)` 相乘，最终得到一个形状为 `(1, dim)` 的输出向量。（复杂度o(n*dim)）"
      }
    ]
  },
  {
    "number": 113,
    "title": "Githubpage.io ✖️✖️",
    "created_at": "2025-01-12T08:39:06Z",
    "closed_at": "2025-02-10T08:43:47Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/113",
    "body": "GitHub Pages 不再支持部署，您能否在其他供应商上进行部署？",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/113/comments",
    "author": "Audran-wol",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2025-01-13T14:31:00Z",
        "body": "> GitHub Pages 不再支持部署\r\n\r\n请问 “不再支持部署” 具体指的是什么呢？🫡"
      }
    ]
  },
  {
    "number": 108,
    "title": "测试数据的问题",
    "created_at": "2025-01-02T09:03:13Z",
    "closed_at": "2025-01-04T07:41:37Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/108",
    "body": "在执行0-eval_pretrain.py进行测试时，如果模型在训练的时候没有见过测试数据，一些事实性的问题（世界上最高的山峰是），模型应该是回答不了的吧？对于一些事实的东西，模型只是把它记住了，没办法推理得到。\r\n谢谢",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/108/comments",
    "author": "cqcracked",
    "comments": [
      {
        "user": "cqcracked",
        "created_at": "2025-01-04T07:41:53Z",
        "body": "."
      },
      {
        "user": "jingyaogong",
        "created_at": "2025-01-04T15:27:34Z",
        "body": "这和\r\n是`eval_pretrain`测试预训练模型\r\n或是`eval_chat`测试指令微调后的聊天模型没有必然关系\r\n\r\n\"一些事实性的问题（世界上最高的山峰是），模型只是把它记住了，没办法推理得到\"\r\n不只是记住了，模型可以推理得到\r\n例如，如果模型在训练中见过“A比B高，B比C高”，它可能会推断出“A比C高”\r\n模型基于大量的训练数据统计内在模式生成文本，不只是存储知识\r\n你如果发现`0-eval_pretrain.py`效果差，目前是预训练数据质量太低的原因"
      }
    ]
  },
  {
    "number": 105,
    "title": "想问一下作者大大是怎么做到模型大小这么小的，通过模型压缩或是参数共享吗，还是只是减少了layer、dim这些参数呢？",
    "created_at": "2024-12-27T06:41:01Z",
    "closed_at": "2025-02-10T08:44:35Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/105",
    "body": "最近有在看MobileLLM那篇论文，不知道作者是不是用了论文里面的技术呢？如embedding share、GQA等呢",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/105/comments",
    "author": "ChinanBoys",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-12-27T13:43:33Z",
        "body": "更少的layer+hidden dim/dim，这些readme有写。\r\n\r\n另外GQA和linear share大约一年前就普及了，minimind里同样标配。"
      }
    ]
  },
  {
    "number": 99,
    "title": "如果训练过程中中断了，如何从中断的地方开始训练呢",
    "created_at": "2024-12-19T06:51:06Z",
    "closed_at": "2024-12-25T11:25:11Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/99",
    "body": "跑的过程中，中断了，可是再运行源文件时，会从头给你跑，作者大大写了这一块吗",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/99/comments",
    "author": "ChinanBoys",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-12-19T07:16:08Z",
        "body": "你指的是从中断的数据开始训练吗？\r\n可以加载中断之前的权重继续训练，保存到 `./out/**.pth` 了\r\n可以在1-pretrain.py的第111行加载到\r\n\r\n```\r\n        model = Transformer(lm_config)\r\n        moe_path = '_moe' if lm_config.use_moe else ''\r\n        ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\r\n        state_dict = torch.load(ckp, map_location=args.device)\r\n        unwanted_prefix = '_orig_mod.'\r\n        for k, v in list(state_dict.items()):\r\n            if k.startswith(unwanted_prefix):\r\n                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\r\n        model.load_state_dict(state_dict, strict=False)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 96,
    "title": "做完预训练之后测试问答会出现很多重复，调高temperture会减轻，是共性问题吗？",
    "created_at": "2024-12-12T13:01:09Z",
    "closed_at": "2024-12-25T11:25:11Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/96",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/96/comments",
    "author": "jy007",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-12-13T11:55:56Z",
        "body": "是的，预训练数据质量低，有大量重复句子或者段落，或者没什么逻辑的上下文。相比于sft数据，质量低的多，这很大程度是数据质量问题，之后寻求更高的预训练数据来解决。"
      }
    ]
  },
  {
    "number": 93,
    "title": "咨询如何处理超长的pretrain数据集",
    "created_at": "2024-11-30T13:41:18Z",
    "closed_at": "2024-12-13T12:04:43Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/93",
    "body": "当我pretrain 使用 pd.read_json 的时候\r\n\r\n```\r\n    model, tokenizer = init_model()\r\n    df = pd.****read_json****(args.data_path)\r\n    df = df.sample(frac=1.0)\r\n    train_ds = PretrainDataset(df, tokenizer, max_length=max_seq_len)\r\n    train_sampler = DistributedSampler(train_ds) if ddp else None\r\n    train_loader = DataLoader(\r\n        train_ds,\r\n        batch_size=args.batch_size,\r\n        pin_memory=True,\r\n        drop_last=False,\r\n        shuffle=False,\r\n        num_workers=args.num_workers,\r\n        sampler=train_sampler\r\n    )\r\n\r\n```\r\n\r\n使用 minimind 筛选出来的数据，（text length < 512）约4.4G ，500万行，\r\n当读取(pd.read_json)的时候，内存占用峰值会到24G左右，\r\n最终读取完的内存占用大约在6G左右，\r\n使用pd.read_csv应该也是类似的占用，一大部分空间会被重复使用而浪费掉，\r\n\r\n训练wenge-research/yayi2_pretrain_data （500G，筛选text length < 512之后还剩25G数据）或者更大一点的SkyPile数据集则会导致内存过大无法训练\r\n\r\n有没有办法削减这一步的内存占用，以及削减后面GPU的内存占用？\r\n或者说，在针对大预言处理超大数据集的时候，一般采取怎么样的方案？\r\n（如果是按照数据大小进行分片处理，那么处理完成后的pth文件最终应该怎样合并在一起？）",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/93/comments",
    "author": "qiukeren",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-12-13T12:04:43Z",
        "body": "这个问题就是一些偏工程方面的优化，读取大型数据集是软件/数据处理行业内任意场景都会遇到的，这并非LLM独家专属\r\n\r\n例如读取的时候可以用chunksize分片\r\n```\r\nchunksize = 10 ** 6  # 每次读取100万行\r\nfor chunk in pd.read_json(args.data_path, lines=True, chunksize=chunksize):\r\n    # 对每个块进行处理，例如筛选、预处理等\r\n    chunk = chunk[chunk['text'].str.len() < 512]\r\n    # 将处理后的数据保存到磁盘或内存中（视具体情况而定）\r\n```\r\n\r\ndask并行处理和延迟计算（没试过）\r\n```\r\nimport dask.dataframe as dd\r\n\r\ndf = dd.read_json(args.data_path, lines=True)\r\n```\r\n\r\n应对超大规模数据集，用更高效的数据加载器 `IterableDataset` （这里用法不展开）\r\n\r\n....等等方案\r\n\r\n所以MiniMind给出的demo还存在着无数的方向可以深入优化"
      }
    ]
  },
  {
    "number": 89,
    "title": "这是分词器不行吗，是不是得去换别的",
    "created_at": "2024-11-09T08:57:33Z",
    "closed_at": "2024-11-11T10:45:17Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/89",
    "body": "生成的回答：迟疑惋怔��下去？/安迟疑惋？？？/安迟？？？？？？/安迟？？？？？/安迅速走过去眼前面前不知道眼前面前面前台边上边上边上边上边上边上边上边上床上边上边上边上边上的一张口中间图书架着？”安迅速走过身边上的脸颊一只是一个个身上边画着食指尸体型盆…？/安迅速走边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上边上楼层次再度瘦���道道道体内里面部臀部部部臀部臀部部臀部部部部瞬间图道号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号称号号称号称号称号\r\n\r\n还有这种一直重复是怎么回事，已经loss1以下了，可还是欠拟合",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/89/comments",
    "author": "Enter10000",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-11-10T06:06:30Z",
        "body": "有几个问题：prompt是什么？训练数据是什么？怎么训练得来的这个模型？直接预训练？小说数据微调？基于哪种模型微调？学习率，轮次？loss 1以下为什么会得出“欠拟合”结论？测试方法是什么？\r\n\r\n根据提问和现象，没有任何定量数据我不得而知，请补齐以上基本信息。\r\n\r\n根据目前已知信息：\r\n1.首先标题所谓的 “分词器不行...” 和这条issue内容没有任何任何关系\r\n2.猜测在低质量自定义（小说？）数据集上用和预训练相同的大学习率把模型（本来具备通用能力的）参数全洗没了，变成过拟合低质数据集的完完全全残疾模型，基本回复能力全部丧失。这是过拟合而恰恰非欠拟合..."
      }
    ]
  },
  {
    "number": 87,
    "title": "我有个比较粗浅疑问：PT、SFT、DPO这些不同的训练方法在本质上是一样的？",
    "created_at": "2024-11-08T00:46:15Z",
    "closed_at": "2024-11-11T10:45:16Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/87",
    "body": "我有个比较粗浅疑问：PT、SFT、DPO这些不同的训练方法在本质上是一样的？\r\n当我在问这个问题的时候就在想，他们都在计算loss，以一定的学习率去梯度下降。那么他们在这个层面上是不是一样的。\r\n只是设计了不同的loss函数，不同的学习率优化器。\r\n",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/87/comments",
    "author": "chuanzhubin",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-11-08T03:36:17Z",
        "body": "基本上是的，区别只有数据和loss的目标"
      }
    ]
  },
  {
    "number": 85,
    "title": "5-dpo_train运行报错，是版本问题吗？",
    "created_at": "2024-11-07T17:06:54Z",
    "closed_at": "2024-11-07T23:56:25Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/85",
    "body": "D:\\Anaconda3\\envs\\minni\\python.exe D:/work/qyQwen/minimind-master/5-dpo_train.py\r\nTraceback (most recent call last):\r\n  File \"D:\\work\\qyQwen\\minimind-master\\5-dpo_train.py\", line 49, in <module>\r\n    dpo_trainer = DPOTrainer(\r\n  File \"D:\\Anaconda3\\envs\\minni\\lib\\site-packages\\**transformers\\utils\\deprecation.py**\", line 165, in wrapped_func\r\n    return func(*args, **kwargs)\r\n**TypeError: __init__() got an unexpected keyword argument 'beta'**\r\n\r\nProcess finished with exit code 1\r\ntransformer=4.46.0和4.44.0都试过，trl=0.11.3",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/85/comments",
    "author": "srconly",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-11-07T23:56:25Z",
        "body": "版本问题，你需要确认 `所谓\"transformer=4.46.0和4.44.0都试过，trl=0.11.3\"` 是否来自 `minni` 环境？\r\n\r\n召集多人测试了所有不同平台的机器，不可复现，暂时close"
      },
      {
        "user": "srconly",
        "created_at": "2024-11-08T13:44:02Z",
        "body": "感谢感谢，我的trl确实不来自minni环境，我本地clone了最新的trl项目，它和mind同级目录，造成了这个问题，已解决"
      }
    ]
  },
  {
    "number": 84,
    "title": "请教如何优化内存使用",
    "created_at": "2024-11-06T15:31:23Z",
    "closed_at": "2024-11-07T23:57:31Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/84",
    "body": "首先感谢大佬的分享。我这里想请教一下在训练过程有什么方法可以提高CPU和显存的使用率吗？我使用的是阿里云的NVIDIA V100，在训练的过程中GPU的使用率基本是满的，但是其他的利用率挺低的",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/84/comments",
    "author": "ltc0",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-11-07T01:40:26Z",
        "body": "谢谢认可哈！  \r\n\r\n不知道“GPU的使用率基本是满的”指的是不是 `Pwr: Usage/Cap` 和 `Volatile GPU-Util` 这两个指标。第一个表示功率是否拉满，第二个表示显存的利用率是否已经饱和。\r\n\r\n如果只是简单为了拉满显存和CPU，可以尝试增大 `batch_size=16/32/64/128` 和 `num_workers=4/8/16/32`\r\n不过，如果GPU的利用率已经接近满负荷，即使这些超参成倍增加，速度提升也不会很显著\r\n\r\n举个不完全恰当的例子：麦芽糖体积小，占用“嘴存”（相当于显存）不多，但嚼起来又硬又黏牙（相当于GPU计算），即便手上同时递2块3块4块麦芽糖，塞满“嘴存”也不会明显加快吃糖的速度，因为嘴巴已经100%在努力咀嚼，这时手（类比CPU）的递糖速度就变得不重要了。相比之下，棉花糖虽然占用“嘴存”较多，但入口即化，对嘴的负担（GPU计算）几乎没有，此时提高递糖频率以提升速度就显得重要了。"
      },
      {
        "user": "ltc0",
        "created_at": "2024-11-07T01:40:57Z",
        "body": "这是来自QQ邮箱的假期自动回复邮件。\r\n \r\n您好，我最近正在休假中，无法亲自回复您的邮件。我将在假期结束后，尽快给您回复。"
      }
    ]
  },
  {
    "number": 82,
    "title": "请教Pretrian中断后继续训练的疑问",
    "created_at": "2024-11-04T03:47:03Z",
    "closed_at": "2024-11-07T23:57:30Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/82",
    "body": "在执行Pretrain.py时如果没有训练完就中断了\r\n想要在中断前的checkpoint的基础上继续训练，本项目的代码请问应该修改哪个部分呀。\r\n谢谢各位大佬",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/82/comments",
    "author": "VictorSun1996",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-11-04T04:49:52Z",
        "body": "可以看一下你 `./out/pretrain_xx_xx.pth` 的修改时间，就是你中断之前被保存的最后一次checkpoint时间\r\n\r\n所以把 `1-pretrain.py` 第105行换成\r\n```\r\ndef init_model():\r\n    def count_parameters(model):\r\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\r\n\r\n    model = Transformer(lm_config).to(args.device)\r\n    moe_path = '_moe' if lm_config.use_moe else ''\r\n    ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\r\n\r\n    state_dict = torch.load(ckp, map_location=args.device)\r\n\r\n    # 处理不需要的前缀\r\n    unwanted_prefix = '_orig_mod.'\r\n    for k, v in list(state_dict.items()):\r\n        if k.startswith(unwanted_prefix):\r\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\r\n\r\n    for k, v in list(state_dict.items()):\r\n        if 'mask' in k:\r\n            del state_dict[k]\r\n\r\n    # 加载到模型中\r\n    model.load_state_dict(state_dict, strict=False)\r\n    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\r\n    return model, tokenizer\r\n```\r\n加载它即可"
      },
      {
        "user": "VictorSun1996",
        "created_at": "2024-11-04T07:40:28Z",
        "body": "> 可以看一下你 `./out/pretrain_xx_xx.pth` 的修改时间，就是你中断之前被保存的最后一次checkpoint时间\r\n> \r\n> 所以把 `1-pretrain.py` 第105行换成\r\n> \r\n> ```\r\n> def init_model():\r\n>     def count_parameters(model):\r\n>         return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n> \r\n>     tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')\r\n> \r\n>     model = Transformer(lm_config).to(args.device)\r\n>     moe_path = '_moe' if lm_config.use_moe else ''\r\n>     ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\r\n> \r\n>     state_dict = torch.load(ckp, map_location=args.device)\r\n> \r\n>     # 处理不需要的前缀\r\n>     unwanted_prefix = '_orig_mod.'\r\n>     for k, v in list(state_dict.items()):\r\n>         if k.startswith(unwanted_prefix):\r\n>             state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\r\n> \r\n>     for k, v in list(state_dict.items()):\r\n>         if 'mask' in k:\r\n>             del state_dict[k]\r\n> \r\n>     # 加载到模型中\r\n>     model.load_state_dict(state_dict, strict=False)\r\n>     Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\r\n>     return model, tokenizer\r\n> ```\r\n> \r\n> 加载它即可\r\n\r\n谢谢，明白了。"
      }
    ]
  },
  {
    "number": 81,
    "title": "Padding Mask部分的疑惑",
    "created_at": "2024-11-02T09:28:27Z",
    "closed_at": "2024-11-11T10:45:34Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/81",
    "body": "您好，感谢您的项目。\r\n我在您的项目中发现模型中的Attention模块中的mask只是包含 attention mask，而没有通过传入padding部分的padding mask结合作为mask。我发现您在模型输出完logits后计算loss时（例如pretrain代码中计算entropy loss），通过padding mask对loss计算结果mask掉了padding部分。我想请教下这两种padding mask的实现方法上在效果上有什么区别。\r\n谢谢，希望您能回复我的疑惑！",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/81/comments",
    "author": "skygreygrey",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-11-02T11:51:08Z",
        "body": "## 1、Attention Mask的作用\r\n假设模型需要学习一句话：\"我来自地球<终止符>\"\r\n\r\n\"我来自地球\" 输入一次，模型只能够学习一次，也就是学会预测最后的 \"<终止符>\"\r\n然而，这样太慢了，现在希望它只需要接收1次输入，就能等价于学5次。\r\n\r\n自然可以想到将输入的 “我来自地球” 展开成一个矩阵，就可以愉快地并行训练了，每一行被学习的时候，我们的目的是：\r\n\r\n```\r\n* 希望它看到 “我” 能学会预测 “来” \r\n* 希望它看到 “我来” 能学会预测 “自”\r\n* 希望它看到 “我来自” 能学会预测“地”\r\n* .......\r\n```\r\n\r\n但是完全展开成矩阵：\r\n```\r\n我来自地球\r\n我来自地球\r\n我来自地球\r\n我来自地球\r\n我来自地球\r\n```\r\n在第 i 行，无论 i 取几，模型都可以看到一整行的内容，下一个要预测啥，下下一个要预测啥，答案都一眼看到了，显然答案被开卷就没有意义了。\r\n\r\n因此这里需要把每一行不该看到的内容给加上Mask矩阵，使之变成负无穷（相当于注意力分数变成0，把这个位置的token给藏起来即可）：\r\n```\r\n我负负负负\r\n我来负负负\r\n我来自负负\r\n我来自地负\r\n我来自地球\r\n```\r\n这样的话，每行各司其职，预测它该预测的值\r\n\r\n```\r\n* 第一行学习“来” (模型只参考\"我\")\r\n* 第二行学习“自” (模型只参考\"我来\")\r\n* 第三行学习“地” (模型只参考\"我来自\")\r\n* 第四行学习“球” (模型只参考\"我来自地\")\r\n* 第五行学习“<终止符>” (模型只参考\"我来自地球\")\r\n```\r\n\r\n完全做到各司其职，互不干扰，过去的token无法看见未来的token。\r\n\r\n这里的Attention Mask达到**只输入一次，实际上潜在地并行训练了N次的目的**\r\n\r\n## 2、logit的mask\r\n\r\n这里没什么特殊的含义，如果设定训练长度为512。\r\n\r\n但是一段话只有200个有效token需要被学习，那么相当于需要padding 312个无实际含义的token使之统一长度方便输入训练。\r\n\r\n那么模型的推理过程并不需要关注这312个无意义的token，所以直接在最后计算loss时去除这部分的损失即可。\r\n\r\n至于Attention中无论是否计算这312个token都没有关系，因为输出的loss反正都会屏蔽掉这部分损失，所以Attention模块只需要面无表情地麻木工作即可，它并不需要知道token数量到底是512还是312还是200。因为这部分工作由loss完成。\r\n\r\n举个不恰当的例子，\r\n\r\n高考数学时，监考老师并不会要求学生格式化大脑中的 **语英政史地生物化** 知识只集中Attention Mask矩阵计算数学题。\r\n因为无论经历怎样惊涛骇浪的天文地理文学历史头脑风暴，这张试卷高考Loss只计算数学答案..."
      },
      {
        "user": "skygreygrey",
        "created_at": "2024-11-03T11:34:19Z",
        "body": "感谢您的回复。"
      },
      {
        "user": "cpp2016",
        "created_at": "2024-11-04T03:54:39Z",
        "body": "至于Attention中无论是否计算这312个token都没有关系，因为输出的loss反正都会屏蔽掉这部分损失，所以Attention模块只需要面无表情地麻木工作即可，它并不需要知道token数量到底是512还是312还是200。因为这部分工作由loss完成。\r\n\r\n这句话中，后边312个token可能会对前边200个token产生贡献吧，特别是在上层，注意力中是不是应该屏蔽掉"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-11-04T04:45:18Z",
        "body": "> 至于Attention中无论是否计算这312个token都没有关系，因为输出的loss反正都会屏蔽掉这部分损失，所以Attention模块只需要面无表情地麻木工作即可，它并不需要知道token数量到底是512还是312还是200。因为这部分工作由loss完成。\r\n> \r\n> 这句话中，后边312个token可能会对前边200个token产生贡献吧，特别是在上层，注意力中是不是应该屏蔽掉\r\n\r\n不会，GPT属于Causal Model，前N个位置的Attention计算不会考虑N+1及以后的未来token，只会关注当前位置之前的token。\r\n\r\n相反，如果312个空token padding在前面而不是后面，会对剩下200个原有token产生影响。"
      }
    ]
  },
  {
    "number": 80,
    "title": "求助，sft数据处理完后是不会使用分词器转码的吗？",
    "created_at": "2024-10-31T08:59:22Z",
    "closed_at": "2024-11-07T23:57:30Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/80",
    "body": "我自己制作了一份数据集，不怎么大，我的目的不是让它会答题，而是希望能模仿我给的q和a的风格，生成类似的答案，哪怕不通顺都行。我按照公开sft数据集的格式制作的，数据处理也正常，可想要训练的时候指定要pretrain文件，我修改文件名后，运行又加载不出来，报错识别错误。\r\n代码我是用git本地部署的，也问过gpt解决了一些问题，但现在就卡在这了，是必须要用pretrain数据集吗\r\n",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/80/comments",
    "author": "Enter10000",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-10-31T09:02:37Z",
        "body": "@Enter10000 \r\n具体错误是什么，肯定是不要求 \"必须要用pretrain数据集\"\r\n可能是你数据里有一些解析错误的问题，需要看到具体的信息再确定"
      },
      {
        "user": "Enter10000",
        "created_at": "2024-11-01T08:00:38Z",
        "body": "好像是我想错了，我是想直接只用sft数据训练大模型，不是微调的那种，所以一直不行。"
      }
    ]
  },
  {
    "number": 79,
    "title": "想问下pretrain_data(24-09-27后弃用).bin 需要下载吗？ 谢谢",
    "created_at": "2024-10-30T14:51:07Z",
    "closed_at": "2024-10-31T02:42:33Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/79",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/79/comments",
    "author": "chenzk1993",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-10-30T15:38:31Z",
        "body": "不需要下载了哦"
      }
    ]
  },
  {
    "number": 72,
    "title": "sft_data.csv数据生成报错",
    "created_at": "2024-10-23T03:15:30Z",
    "closed_at": "2024-10-28T09:21:44Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/72",
    "body": "当 data_process.py 执行 sft_process(contain_history=True) 报错。\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/xxxx/Documents/AI/minimind/data_process.py\", line 147, in <module>\r\n    sft_process(contain_history=True)\r\n  File \"/Users/xxxx/Documents/AI/minimind/data_process.py\", line 101, in sft_process\r\n    process_and_write_data(data)\r\n  File \"/Users/xxxx/Documents/AI/minimind/data_process.py\", line 78, in process_and_write_data\r\n    df.to_csv(f'./dataset/{file_name}', mode='a', header=False, index=False, lineterminator='\\r\\n')\r\n  ......\r\n    libwriters.write_csv_rows(\r\n  File \"pandas/_libs/writers.pyx\", line 72, in pandas._libs.writers.write_csv_rows\r\n_csv.Error: need to escape, but no escapechar set\r\n\r\n是需要处理 sft_data_zh.jsonl 的数据还是 导出sft_data.csv的时候，添加分割符。 如果添加了，对后面的训练是否有影响？",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/72/comments",
    "author": "liyu98",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-10-23T04:22:23Z",
        "body": "pandas版本是多少，我无法复现这个问题"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-10-23T04:27:24Z",
        "body": "可以加的，强行转义字符频率很低，应该基本不会受影响\r\n\r\n可以pull最新代码process一下\r\n\r\n训练后comment效果😊"
      }
    ]
  },
  {
    "number": 69,
    "title": "模型的输入输出长度",
    "created_at": "2024-10-17T07:09:06Z",
    "closed_at": "2024-10-18T06:16:05Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/69",
    "body": "1.这个模型训练好后，上下文输入的长度最长能到多少，是由max_seq_len: int = 512 来控制的吗？\r\n2.这个模型训练好后，输出的最大长度是由哪里控制的呢？谢谢",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/69/comments",
    "author": "cqcracked",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-10-17T07:39:03Z",
        "body": "> 1.这个模型训练好后，上下文输入的长度最长能到多少，是由max_seq_len: int = 512 来控制的吗？\n> 2.这个模型训练好后，输出的最大长度是由哪里控制的呢？谢谢\n\n是的由它决定"
      },
      {
        "user": "cqcracked",
        "created_at": "2024-10-17T08:05:59Z",
        "body": "谢谢，我看了下代码，minimind比较清楚了。其它大模型的输入输出长度也是这么直接限制吗？麻烦解释一下呢？话说rope也有外推能力呢\r\n\r\n\r\n\r\n\r\n------------------&nbsp;原始邮件&nbsp;------------------\r\n发件人:                                                                                                                        \"jingyaogong/minimind\"                                                                                    ***@***.***&gt;;\r\n发送时间:&nbsp;2024年10月17日(星期四) 下午3:39\r\n***@***.***&gt;;\r\n***@***.******@***.***&gt;;\r\n主题:&nbsp;Re: [jingyaogong/minimind] 模型的输入输出长度 (Issue #69)\r\n\r\n\r\n\r\n\r\n\r\n  \r\n1.这个模型训练好后，上下文输入的长度最长能到多少，是由max_seq_len: int = 512 来控制的吗？\r\n 2.这个模型训练好后，输出的最大长度是由哪里控制的呢？谢谢\r\n  \r\n是的由它决定\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***&gt;"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-10-17T09:54:13Z",
        "body": "max_seq_len只起到截断作用，具体从哪截 ，怎么截，截多少并不重要，只是长度为了模型质量服务而已。设置到10000万也没任何关系，输入输出至少'不会报错'。\n\n那么输入太长影响的是：模型训练只见过的512以内编码长度，以外的文本在训练之外，从没见过，性能会大打折扣罢了。rope靠插值做长度外推，缓解这种影响而已。\n\n「外不外推」影响的是质量\n\n和你问题「怎样限制输入输出长度」本身没有任何关系。"
      }
    ]
  },
  {
    "number": 67,
    "title": "768爆显存",
    "created_at": "2024-10-15T08:13:56Z",
    "closed_at": "2024-10-18T06:16:20Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/67",
    "body": "使用两个A800（80G）训练768的模型，在加载数据的时候，经常出现爆显存的问题，偶尔不报能正常训练起来，只占30G显存，不知道这个问题有没有遇到过，如何解决？",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/67/comments",
    "author": "yhl41001",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-10-15T09:43:53Z",
        "body": " 没有遇到过，有没有更多可参考的日志或其他信息。\r\n\r\n另外模型是768*16\r\n\r\n还是768*16*N的MoE"
      },
      {
        "user": "yhl41001",
        "created_at": "2024-10-15T13:13:20Z",
        "body": "模型是768*16，我观察到了显存增长超出显卡的显存了。"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-10-15T14:23:19Z",
        "body": "> 模型是768*16，我观察到了显存增长超出显卡的显存了。\r\n\r\n`max_seq_len` 有没有改过\r\n\r\n降低 `batch_size` 直到可以运行"
      },
      {
        "user": "yhl41001",
        "created_at": "2024-10-16T07:49:48Z",
        "body": "> > 模型是768*16，我观察到了显存增长超出显卡的显存了。\r\n> \r\n> `max_seq_len` 有没有改过\r\n> \r\n> 降低 `batch_size` 直到可以运行\r\n\r\n这两个都没改过，现在模型在训练，等训练玩了我有时间看一下是怎么回事！"
      }
    ]
  },
  {
    "number": 58,
    "title": "小哥哥再把量化加入进去，整个生命周期就完整了",
    "created_at": "2024-10-03T03:42:23Z",
    "closed_at": "2024-10-05T01:47:18Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/58",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/58/comments",
    "author": "cqcracked",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-10-05T01:47:18Z",
        "body": "OK，我尝试一下"
      }
    ]
  },
  {
    "number": 56,
    "title": "如何微调用于下游任务？",
    "created_at": "2024-10-01T09:14:33Z",
    "closed_at": "2024-10-18T06:17:18Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/56",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/56/comments",
    "author": "h2h2h",
    "comments": [
      {
        "user": "h2h2h",
        "created_at": "2024-10-01T09:19:15Z",
        "body": "训练集是csv文件，有history,q和a三列，其中history是空列表。将其输入4-lora_sft.py中，因为训练集比较小，就把--save_interval由1000改成了100，训练完成后将得到的safetensors放到2-eval.py中输出直接胡言乱语了...本人是小白，诚恳地请教该如何去微调呢？问题出在了哪里？谢谢🙏"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-10-01T15:50:55Z",
        "body": "无法从以上描述获取任何定量信息，例如\r\n数据集条数？学习率？下游任务类型？“胡言乱语”指的是什么程度？LoRA微调前的基座效果如何？用full_sft有没有试过？\r\n\r\n等等\r\n\r\n希望进一步提供分析问题😊"
      },
      {
        "user": "h2h2h",
        "created_at": "2024-10-02T04:20:39Z",
        "body": "谢谢回复！我的描述确实太业余的。。。数据集大概只有1000条左右，学习率是default的1e-4，想微调用于特定领域的三元组抽取，model和tokenizer的路径填的是使用huggingface-cli下载下来的minimind-v1-small。再次抱歉，这是我第一次发issues，确实太业余了🤦\r\n"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-10-02T06:27:10Z",
        "body": "> 谢谢回复！我的描述确实太业余的。。。数据集大概只有1000条左右，学习率是default的1e-4，想微调用于特定领域的三元组抽取，model和tokenizer的路径填的是使用huggingface-cli下载下来的minimind-v1-small。再次抱歉，这是我第一次发issues，确实太业余了🤦\r\n\r\n没任何关系，feel free，有问题随意交流即可\r\n\r\n现在的问答是什么样的效果，有没有样例\r\n\r\n尝试一下把学习率改成1e-6及以下"
      }
    ]
  },
  {
    "number": 51,
    "title": "1-pretrain中新增的loss mask与现有F.cross_entropy参数不匹配问题",
    "created_at": "2024-09-29T07:19:41Z",
    "closed_at": "2024-10-01T16:02:43Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/jingyaogong/minimind/issues/51",
    "body": "1.如题，作者新加的mask处理不足一个max_len的情况下的loss计算问题，但model.py计算loss还是reduce模式，这个loss计算出来已经是一个标量了，后续mask就无效了，\r\n2.是不是可以将F.cross_entropy增加上（size_average=False, reduce=False）这两参数",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/51/comments",
    "author": "lesterlee89",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-29T08:59:04Z",
        "body": "是的，完全正确，不加, reduction='none' 的话它直接返回一个平均标量损失，已更新bug\r\n\r\n谢谢你的细心发现！！！\r\n"
      }
    ]
  },
  {
    "number": 46,
    "title": "邮件回复",
    "created_at": "2024-09-25T03:45:51Z",
    "closed_at": "2024-09-25T06:35:29Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/46",
    "body": "您好，尊敬的MiniMind所有者，我已经向您的邮箱gongjy.cs@gmail.com发送了关于Accelerate的邮件，非常期待您的回复。",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/46/comments",
    "author": "iomgaa-ycz",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-25T04:31:21Z",
        "body": "抱歉我查看了一下，\r\n`453***883@qq.com` 是你吗\r\n如果不是，可能gmail丢邮件了，貌似没发现其它邮件，可以发到gongjy.cs@qq.com重新联系我\r\n如果是，我马上回复"
      },
      {
        "user": "iomgaa-ycz",
        "created_at": "2024-09-25T06:19:10Z",
        "body": "453***883@qq.com并不是我的邮箱，我是一个gmail邮箱。现已经重新向[gongjy.cs@qq.com](mailto:gongjy.cs@qq.com)发送了邮件。"
      }
    ]
  },
  {
    "number": 44,
    "title": "修复wandb bug & 添加了argparse",
    "created_at": "2024-09-24T04:49:02Z",
    "closed_at": "2024-09-24T06:10:06Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/pull/44",
    "body": "1. 修复了在torchrun命令下wandb会执行两次初始化的bug\r\n2. 添加了argparse，以方便在终端控制超参数\r\n\r\n接下来我希望添加Acclerate，以实现比torchrun更好的多gpu训练，但有一个细节希望与您确认，我的微信号ycz050516-110720",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/44/comments",
    "author": "iomgaa-ycz",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-24T06:15:31Z",
        "body": "> 1. 修复了在torchrun命令下wandb会执行两次初始化的bug\r\n> 2. 添加了argparse，以方便在终端控制超参数\r\n> \r\n> 接下来我希望添加Acclerate，以实现比torchrun更好的多gpu训练，但有一个细节希望与您确认，我的微信号ycz050516-110720\r\n\r\ngongjy.cs@qq.com\r\n@iomgaa-ycz 抱歉微信号用于其它事务，工作和交流个人偏向使用邮件沟通，欢迎讨论任何事宜"
      },
      {
        "user": "iomgaa-ycz",
        "created_at": "2024-09-25T02:22:05Z",
        "body": "您好，请问。可不可以回复一下邮件，不然我没办法开展进一步的工作。非常感谢您的帮助！"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-09-25T04:57:51Z",
        "body": "> 您好，请问。可不可以回复一下邮件，不然我没办法开展进一步的工作。非常感谢您的帮助！\r\n\r\n@iomgaa-ycz 抱歉我查看了一下，\r\n`453***883@qq.com` 是你吗\r\n如果不是，可能gmail丢邮件了，貌似没发现其它邮件，可以发到gongjy.cs@qq.com重新联系我\r\n如果是，已经回复"
      }
    ]
  },
  {
    "number": 33,
    "title": "微调需要至少多少条数据？",
    "created_at": "2024-09-18T02:51:59Z",
    "closed_at": "2024-09-18T05:04:34Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/33",
    "body": "微调需要至少多少条数据？",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/33/comments",
    "author": "charmowen",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-18T05:00:00Z",
        "body": "看目的，全参SFT不用来增加领域知识，这部分理应在PT阶段。\r\n\r\nSFT应该是针对某种问法怎么回答，或者下游任务。\r\n\r\nLora-FT是一种外挂，可以不损失原有的回答能力。\r\n\r\n但如果要全量SFT的话，需要分阶段混合，比如一开始10%的领域加90的通用，一点点增加到50%的领域（数字是猜的）否则一定会损失基准能力。\r\n\r\n此外，如果有百万数据量的话，一个epoch足够了。如果只有几千上万的数据量，可以尝试1~3个epoch，不要太多，容易过拟合。"
      }
    ]
  },
  {
    "number": 32,
    "title": "大佬，发现一个小问题，3-full_sft.py中 加载预训练权重的代码被注释掉了，所以可能导致有其他大佬微调之后结果也不太好。",
    "created_at": "2024-09-17T15:25:57Z",
    "closed_at": "2024-09-18T05:04:19Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/issues/32",
    "body": "if model_from == 1:\r\n        model = Transformer(lm_config)\r\n        # 加载预训练权重的代码被注释掉了\r\n        # ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\r\n        # state_dict = torch.load(ckp, map_location=device)\r\n        # unwanted_prefix = '_orig_mod.'\r\n        # for k, v in list(state_dict.items()):\r\n        #     if k.startswith(unwanted_prefix):\r\n        #         state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\r\n        # model.load_state_dict(state_dict, strict=False)\r\n    else:\r\n        model = AutoModel.from_pretrained('./minimind', trust_remote_code=True)",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/32/comments",
    "author": "sunxinti",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-18T05:03:20Z",
        "body": "这个是怕很多人没有对应的.pth权重文件就直接做SFT，应该不会有人把这里给漏掉吧😂\r\n\r\n还是解开注释吧~"
      }
    ]
  },
  {
    "number": 29,
    "title": "Update my_openai_api.py 忽略客户端传来的未知字段",
    "created_at": "2024-09-15T07:40:23Z",
    "closed_at": "2024-09-15T09:34:57Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/pull/29",
    "body": "像dify这样的客户端，会传入一个用户序列号, 需要兼容一下,避免报错.\r\n```\r\nrequest.json={'model': 'minimind-small-T', 'stream': True, 'temperature': 0.7, 'messages': [{'role': 'user', 'content': '1'}], 'user': 'cdc1f28b-4bb3-4326-ac4b-1cf78e0a6a3b'}\r\n```",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/29/comments",
    "author": "chuanzhubin",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-15T09:35:14Z",
        "body": "Thank you, merged"
      }
    ]
  },
  {
    "number": 23,
    "title": "关于PretrainDataset读取训练数据的问题",
    "created_at": "2024-09-14T07:17:14Z",
    "closed_at": "2024-09-16T15:14:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jingyaogong/minimind/issues/23",
    "body": "\r\n```python\r\nclass PretrainDataset(Dataset):\r\n    def __init__(self, data_path_lst, max_length=512, memmap=False):\r\n        super().__init__()\r\n        #\r\n        if memmap:\r\n            with open(data_path_lst[0], 'r') as f:\r\n                nbytes = f.seek(0, 2)\r\n                flen = f.tell() // np.dtype('uint16').itemsize\r\n            self.data = np.memmap(data_path_lst[0], dtype=np.dtype('uint16'), shape=(flen // max_length, max_length))\r\n        else:\r\n            data_lst = []\r\n            for data_path in data_path_lst:\r\n                with open(data_path, 'rb') as f:\r\n                    data = np.fromfile(f, dtype=np.uint16)\r\n                    data_lst.append(data)\r\n            data = np.concatenate(data_lst)\r\n            data = data[:max_length * int(len(data) / max_length)]\r\n            # np.random.shuffle(data)\r\n            self.data = data.reshape(-1, max_length)\r\n```\r\n \r\n\r\n从代码逻辑上看，PretrainDataset读取bin数据的时候，按照512的长度作为一条数据了好像。\r\n但是在数据存储的时候，各条数据并没有格式化成512的程度再存入bin文件中，如果按照512长度读取出来，训练数据可能没有严格按照训练数据集的条目进行划分吧。\r\n",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/23/comments",
    "author": "jasonheyh",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-14T07:42:44Z",
        "body": "是的没有按照长度划分：\r\n```\r\n文本1</s>文本2</s>文本3</s>文本4</s>...\r\n```\r\n\r\n例如\r\n```\r\n床前明月光</s>疑是地上霜</s>举头望明月</s>低头思故乡</s>\r\n```\r\n\r\n训练如果`context_length=10`\r\n用这10个token：\r\n```\r\n床前明月光</s>疑是地上\r\n```\r\n\r\n或者是这10个token：\r\n```\r\n地上霜</s>举头望明月</s>\r\n```\r\n\r\n虽然看起来不是一个完整的句子，但输入做预训练都是可以的，会损失一些文本首尾的信息。\r\n在这些窗口内捕捉局部上下文中的依赖关系。即使窗口之间没有重叠，模型在学文本中局部的语言模式。因为有 </s> 分隔符，可以明确每个句子或段落的边界。说白了就是直接练习接龙。\r\n\r\n当然了，这里如果严格确保句子之间padding max_length的空缺部分，相当于补上句头句尾的内容，效果会更好一些，但很有限。\r\n\r\n句子如果只有100长度，补齐412个空白token凑到512，甚至补924到1024，文件就冗余得比较大了。\r\n\r\n这里就是为了节省空间牺牲轻微的预训练效果了。"
      },
      {
        "user": "jasonheyh",
        "created_at": "2024-09-14T08:08:03Z",
        "body": "谢谢，明白了，解释的非常明白。"
      },
      {
        "user": "jingyaogong",
        "created_at": "2024-09-14T08:19:12Z",
        "body": "> 谢谢，明白了，解释的非常明白。\r\n\r\n好的，No Thanks\r\n\r\n有什么问题随时继续交流~"
      }
    ]
  },
  {
    "number": 17,
    "title": "虽然很有想法，但是这种模型只能当教学用了，实际生产谁敢用这个？",
    "created_at": "2024-09-13T08:55:19Z",
    "closed_at": "2024-09-13T13:48:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jingyaogong/minimind/issues/17",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/17/comments",
    "author": "GZ315200",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-13T09:12:55Z",
        "body": "对，只能是用来学习，它也不可能出现在实际生产里。\r\n无论是参数体量还是训练数据量都差了上千倍。\r\n基本上所有小于4B的模型还不具备逻辑，不谈生产力。"
      }
    ]
  },
  {
    "number": 3,
    "title": "这项目很好，是我感兴趣的",
    "created_at": "2024-09-07T04:36:58Z",
    "closed_at": "2024-09-12T06:03:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jingyaogong/minimind/issues/3",
    "body": null,
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/3/comments",
    "author": "844704781",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-07T04:47:28Z",
        "body": "感谢支持"
      }
    ]
  },
  {
    "number": 2,
    "title": "Update requirements.txt",
    "created_at": "2024-09-05T07:10:55Z",
    "closed_at": "2024-09-05T07:28:54Z",
    "labels": [],
    "url": "https://github.com/jingyaogong/minimind/pull/2",
    "body": "Fix conflict with flask and jinja",
    "comments_url": "https://api.github.com/repos/jingyaogong/minimind/issues/2/comments",
    "author": "MuWinds",
    "comments": [
      {
        "user": "jingyaogong",
        "created_at": "2024-09-05T07:30:03Z",
        "body": "Okay, thank you very much for your submission."
      }
    ]
  }
]