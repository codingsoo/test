[
  {
    "number": 116,
    "title": "llava-next issue. ",
    "created_at": "2025-01-17T17:28:32Z",
    "closed_at": "2025-01-18T14:49:20Z",
    "labels": [],
    "url": "https://github.com/PKU-Alignment/align-anything/pull/116",
    "body": "dpo, reward model, cost model all works on llava-v1.6-vicuna-7b-hf (llava-next model)\r\nHowever, I met the same issue for ppo, and ppo-lag (rlhf) algorithm no matter what dataset I used for training. \r\nError: \r\nFile \"/home/user/anaconda3/envs/align-anything/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 566, in forward\r\n[rank0]:     attn_output = torch.nn.functional.scaled_dot_product_attention(\r\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: RuntimeError: The expanded size of the tensor (669) must match the existing size (93) at non-singleton dimension 3.  Target sizes: [8, 32, 1, 669].  Tensor sizes: [8, 1, 1, 93]",
    "comments_url": "https://api.github.com/repos/PKU-Alignment/align-anything/issues/116/comments",
    "author": "Happy-Corpse",
    "comments": [
      {
        "user": "Gaiejj",
        "created_at": "2025-01-18T14:49:18Z",
        "body": "What this PR does has been fixed by the previous PR. So we temporarily closed it."
      }
    ]
  }
]