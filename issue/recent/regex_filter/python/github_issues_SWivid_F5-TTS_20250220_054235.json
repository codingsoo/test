[
  {
    "number": 792,
    "title": "FileNotFoundError: [WinError 2]",
    "created_at": "2025-02-17T06:21:27Z",
    "closed_at": "2025-02-17T17:13:51Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/792",
    "body": "### Checks\n\n- [x] This template is only for usage issues encountered.\n- [x] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [x] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nwhen i click synthesize i get this error :(\n\n\n\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n### Steps to Reproduce\n\ni did everything the documentation said exactly and everything was going fine until i actually tried to run the synthesize button\n\n### ✔️ Expected Behavior\n\nWorking, generating audio\n\n### ❌ Actual Behavior\n\nError, nothing happens",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/792/comments",
    "author": "SulaimanBasal",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:23:18Z",
        "body": "> FileNotFoundError: [WinError 2] The system cannot find the file specified\n\nmake sure you have fully upload the audio"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:26:20Z",
        "body": "@SWivid i did, i can even listen to it in the ui... (thanks for the fast reply)\n"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:29:37Z",
        "body": "you could probably provide more details as suggest by the issue template.\notherwise we could hardly help cuz no info"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:31:25Z",
        "body": "@SWivid yea i'm sorry about that, i'm a total noob at this... \n\nthis is the full message when i upload, will that help?\n\n\nC:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\nTraceback (most recent call last):\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n    response = await route_utils.call_process_api(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 2051, in process_api\n    result = await self.call_function(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 1598, in call_function\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\utils.py\", line 883, in wrapper\n    response = f(*args, **kwargs)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 241, in basic_tts\n    audio_out, spectrogram_path, ref_text_out = infer(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 131, in infer\n    ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, ref_text, show_info=show_info)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\utils_infer.py\", line 294, in preprocess_ref_audio_text\n    aseg = AudioSegment.from_file(ref_audio_orig)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\audio_segment.py\", line 728, in from_file\n    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py\", line 274, in mediainfo_json\n    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\subprocess.py\", line 971, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\subprocess.py\", line 1456, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n\n\n\n"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:36:24Z",
        "body": "@SWivid also when starting the gradio app  i get this before given the local link:\nStarting app...\nINFO: Could not find files for the given pattern(s).\n\nwould that be what's causing the issue?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:52:06Z",
        "body": "> C:\\Users\\SUL.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n\nmay try #739 \ninstall ffmpeg"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T17:13:51Z",
        "body": "@SWivid Yep!!! that was the issue hahaha, it's weird that it didn't tell me that it was missing as an error but anyway thanks for the fast reply, installing the ffmpeg and the ffmpeg python fixed it and now works like a charm! thank you again, and great job on the tool, much appreciate it!"
      }
    ]
  },
  {
    "number": 784,
    "title": "Added IPEX support",
    "created_at": "2025-02-14T15:10:41Z",
    "closed_at": "2025-02-19T07:32:33Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/784",
    "body": "Improved compatibility with Intel GPU support\r\n\r\nSupport IPEX (Intel® Extension for PyTorch)\r\n\r\nThis method allows you to just use the Python environment without installing additional development kits\r\n\r\nIt continues to be compatible with Pytorch+XPU",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/784/comments",
    "author": "DDXDB",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2025-02-15T04:53:27Z",
        "body": "LGTM, but you need to pre-commit run --all-files"
      },
      {
        "user": "DDXDB",
        "created_at": "2025-02-15T13:30:36Z",
        "body": "> LGTM, but you need to pre-commit run --all-files\r\n\r\nno, `pre-commit run --all-files`will override the necessary code"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-19T07:32:33Z",
        "body": "instruction of ipex usage added in d457c3e2450ee83a8a27b6f41808716c7763311b\r\nsanity check not involved yet\r\n\r\nthanks again~"
      }
    ]
  },
  {
    "number": 772,
    "title": "Improve prepare_csv_wavs.py",
    "created_at": "2025-02-07T20:13:05Z",
    "closed_at": "2025-02-09T06:34:11Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/772",
    "body": "This PR enhances the dataset preparation pipeline with major performance and reliability improvements. \r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/772/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "hcsolakoglu",
        "created_at": "2025-02-08T04:35:46Z",
        "body": "I actually wrote this for my own use. Goal here is to efficiently prepare large datasets for training. For example, in a dataset with 116,180 samples, the old version of the code took 4 hours, but now it has been reduced to 1 hour. If you're interested, I can finish adding it for everyone to use. @SWivid "
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-08T06:06:48Z",
        "body": "Yes, if you are willing to share"
      },
      {
        "user": "justinjohn0306",
        "created_at": "2025-02-08T06:57:45Z",
        "body": "> I actually wrote this for my own use. Goal here is to efficiently prepare large datasets for training. For example, in a dataset with 116,180 samples, the old version of the code took 4 hours, but now it has been reduced to 1 hour. If you're interested, I can finish adding it for everyone to use. @SWivid\r\n\r\nI really love your PR. Please share the final thing, it would be much appreciated. Thank you :)"
      },
      {
        "user": "atlonxp",
        "created_at": "2025-02-08T19:01:24Z",
        "body": "looking forward to the merging"
      },
      {
        "user": "hcsolakoglu",
        "created_at": "2025-02-09T16:04:34Z",
        "body": "Since I was working, I couldn't return, but I see that @SWivid merged it for me. I wish everyone fast and successful training runs! @justinjohn0306 @atlonxp"
      }
    ]
  },
  {
    "number": 770,
    "title": "Adding tensorboard dependency in requirements and save tensorboard events runs inside checkpoint directory",
    "created_at": "2025-02-06T17:35:20Z",
    "closed_at": "2025-02-06T17:38:30Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/770",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/770/comments",
    "author": "MuruganR96",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-02-06T17:38:30Z",
        "body": "Sure, you could do it locally~"
      }
    ]
  },
  {
    "number": 765,
    "title": "Add Per-Epoch Batch Shuffling to DynamicBatchSampler",
    "created_at": "2025-02-04T19:26:48Z",
    "closed_at": "2025-02-05T07:10:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/765",
    "body": "This PR implements deterministic per-epoch shuffling for DynamicBatchSampler while preserving the efficient length-based batch grouping. The changes allow training to see different batch orders each epoch while maintaining padding efficiency and reproducibility, which helps prevent the model from learning potential epoch-level patterns. The implementation integrates seamlessly with both single-GPU and distributed training scenarios using PyTorch's Generator for deterministic shuffling based on seed + epoch number.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/765/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-02-05T07:10:21Z",
        "body": "Thanks~~"
      }
    ]
  },
  {
    "number": 758,
    "title": "configuration files, vocab.txt",
    "created_at": "2025-01-30T21:45:02Z",
    "closed_at": "2025-02-01T11:40:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/758",
    "body": "### Checks\n\n- [x] This template is only for question, not feature requests or bug reports.\n- [x] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [x] I have searched for existing issues, including closed ones, no similar questions.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI have installed the .safetensor file in the english/chinese e2-f5-tts pinokio-installation. Don't I need to provide the vocab.txt as well? Where do I change the path to that, when I copy the project it still points to the same file. \nWhere is the model defined? I try to integrate another language and there are more than one model file and I don't know how to integrate them ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/758/comments",
    "author": "sirdir1972",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-31T02:22:43Z",
        "body": "have you successfully launched infer_gradio interface?\njust filling the infos of other model(s) in right places."
      },
      {
        "user": "sirdir1972",
        "created_at": "2025-01-31T11:38:11Z",
        "body": "well I have successfully launched the e2-f5-tts module for pinokio, so I suppose that launches infer_gradio (I honestly don’t know the internals very well), so that’s all automated and I don’t know really where to start looking. "
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-01T04:16:40Z",
        "body": "any screenshots? could also search for tutorials or go through the project readme first"
      },
      {
        "user": "sirdir1972",
        "created_at": "2025-02-01T10:50:05Z",
        "body": "A screenshot of what? I don’t know where to change the config files… And sure I would do a tutorial, but I couldn’t find any. The read me didn’t help me either, as the model I’m referring to probably isn’t meant to be integrated into pinokio. "
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-01T11:13:39Z",
        "body": "> A screenshot of what? I don’t know where to change the config files…\n\nyou could record a video (or some screenshots)  to let us know where you stuck, we have no info now so we cannot provide any help.\n\n> The read me didn’t help me either, as the model I’m referring to probably isn’t meant to be integrated into pinokio.\n\nwhich lines of command you have executed and failed? and if you don't describe in detail the model you refer to, how could we help"
      },
      {
        "user": "sirdir1972",
        "created_at": "2025-02-01T11:23:27Z",
        "body": "I’m not executing any commands and none are failing. I’m installing the e2-f5-tts module on Pinokio. It works. I’m trying to understand the format of the e2-f5-tts module that goes into pinokio to be able to modify the configuration because I would like it to load another model and another vocab.txt. In the end it probably doesn’t matter as I found out everybody seems just to be redistributing the same vocab.txt and it seems to be relevant for chinese only and I can just replace the .safetensor file with the one I want, but I still would have preferred modifing the config. "
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-01T11:25:22Z",
        "body": "so you have finetuned or trained your own model or stuff?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-01T11:40:34Z",
        "body": "anyway you can find some tutorials on youtube simply search for f5-tts pinokio\nor you just follow the readme and elaborate on detailed usage problem(s) if stuck with help-wanted issue template"
      }
    ]
  },
  {
    "number": 756,
    "title": "Implement atomic save and validation for model checkpoints",
    "created_at": "2025-01-29T10:26:14Z",
    "closed_at": "2025-01-29T18:01:27Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/756",
    "body": "Introduces atomic checkpoint saving to eliminate the risk of checkpoint corruption caused by interrupted training. It ensures that checkpoints are either fully written or not present at all by using temporary files, atomic renaming, and integrity validation before loading. \r\n\r\nNote: I tested whether it solves the issue before sending the PR, but since I didn't have much time, I couldn't check if it broke anything else. If you review it in detail and leave feedback, I can make corrections. If you think it's not necessary, you can approve it. Resolves #754 ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/756/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-29T11:11:26Z",
        "body": "Hi @hcsolakoglu , from these points of view, we can just keep it simple like any other project:\r\n1. \".tmp\" suffix might have conflict with other unknown project and make it hard to debug in that case\r\n2. would be possible to delete important checkpoints if do it automatically, e.g. E2-TTS or other customed structure model weights may be deleted if someone just missuse F5-TTS config yaml file (which will raise an error caught and ckpt deleted)\r\n3. it's also important to have users themselves to control otherwise pytorch would just introduce that atomic operations in their load code."
      },
      {
        "user": "hcsolakoglu",
        "created_at": "2025-01-29T11:40:59Z",
        "body": "Alright, except for this issue, the project is in good shape with no critical problems. I'll check what I can do when I have time. @SWivid "
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-29T11:50:41Z",
        "body": "Sure, your contribution always appreciated~"
      }
    ]
  },
  {
    "number": 742,
    "title": "dataset preprocess",
    "created_at": "2025-01-22T10:51:07Z",
    "closed_at": "2025-02-02T07:39:27Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/742",
    "body": "### Checks\n\n- [x] This template is only for usage issues encountered.\n- [x] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [x] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\npython=3.10\n\n### Steps to Reproduce\n\nHi there, thanks for your amazing work on open-sourcing F5-TTS! While training F5 on my own dataset, I noticed an issue: the generated audio quality drops significantly when the dataset contains some audio clips longer than 20 seconds.\n\nI saw there's a slice function in src/f5_tts/train/finetune_gradio.py that can split audio based on silent segments. So, my question is: when working with longer audio clips (e.g., over 20 seconds), do I need to slice them into shorter segments, translate them, and then use them for training? Or is it okay to include a small number of long audio clips directly? I'm planning to use a large dataset later, and slicing and translating all long audio files might be too much work.\n\n### ✔️ Expected Behavior\n\n_No response_\n\n### ❌ Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/742/comments",
    "author": "CreepJoye",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-27T11:38:44Z",
        "body": "Hi @CreepJoye ,\nit's ok to do with audios of <30 total length, mainly for #719 "
      }
    ]
  },
  {
    "number": 740,
    "title": "size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512])",
    "created_at": "2025-01-21T17:29:39Z",
    "closed_at": "2025-01-27T11:31:47Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/740",
    "body": "### Checks\n\n- [x] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [x] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [x] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nwidow's10 , python 3.9.5 ,\n\n\n### Steps to Reproduce\n\nsize mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\n        size mismatch for transformer.input_embed.proj.weight: copying a param with shape torch.Size([1024, 300]) from checkpoint, the shape in current model is torch.Size([1024, 712]).\n\n### ✔️ Expected Behavior\n\ntraining must be started \n\n### ❌ Actual Behavior\n\n size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512])",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/740/comments",
    "author": "T-When",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2025-01-22T04:54:17Z",
        "body": "You need to change the text dim from 512 to 100 to load this checkpoint"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-27T11:31:47Z",
        "body": "Hi @T-When make sure you are using checkpoint of F5 (instead of E2)"
      }
    ]
  },
  {
    "number": 729,
    "title": "Exclude pretrained models from the checkpoint rotation logic",
    "created_at": "2025-01-17T16:44:27Z",
    "closed_at": "2025-01-27T11:57:05Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/729",
    "body": "There was an issue due to the pretrained models having the same name as the checkpoints saved during model training. Suppose `keep_last_n_checkpoints` was set to 5 since the pretrained model was also included in this count, only 4 training checkpoints + 1 pretrained model checkpoint were actually kept. Additionally, in long enough training sessions, there was a risk of the pretrained model being deleted. These changes address this issue. \r\n\r\nIf there are any modifications I might have overlooked, it would be great if you could review them and provide feedback.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/729/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-27T11:57:17Z",
        "body": "@hcsolakoglu Thanks~"
      }
    ]
  },
  {
    "number": 723,
    "title": "All other things being equal, is the longer the inference audio (0 to 15s) the better the clone result?",
    "created_at": "2025-01-15T09:44:55Z",
    "closed_at": "2025-01-15T10:32:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/723",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI have a 1 minute reasoning audio, how to get the best cloning results, would using the first 15 seconds be better than the first 8 seconds?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/723/comments",
    "author": "BobReal0822",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-15T10:32:01Z",
        "body": "there are many aspects:\r\n\r\nif all good quality 15 sec better\r\nif 8 sec quality better than other 7 sec, 8 better\r\n\r\njust try it out"
      }
    ]
  },
  {
    "number": 714,
    "title": "My finetuned model is unable to clone voices like the base f5_1200000 model.",
    "created_at": "2025-01-13T10:39:34Z",
    "closed_at": "2025-01-27T14:30:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/714",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nso what i want to do is that i have finetuned a model with 15 hours of a single person on Urdu Language, i have trained the model for 500k steps and it is working good. Now when i open the same model in the inference_gradio and i want to give another person's voice as reference voice. it is unable to clone that voice.\r\n\r\nI expected that by finetuning the model we can teach it the language and later we can just clone the voices, or just train the model for another 10k steps on that new voice so it will work as the model now knows the language and only need to pick on voice characteristic, but it is unable to where as in the base model F5_1200000 it is able to clone voice. \r\n\r\nSo i have a simple question, why is the base model f5_1200000 able to clone voices but my finetuned one cant clone.\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/714/comments",
    "author": "masooddahmedd",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2025-01-13T14:15:24Z",
        "body": "Have you changed your vocab?"
      },
      {
        "user": "masooddahmedd",
        "created_at": "2025-01-13T14:56:03Z",
        "body": "yes i have"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-27T14:30:03Z",
        "body": "> So i have a simple question, why is the base model f5_1200000 able to clone voices but my finetuned one cant clone.\n\nthe base model is trained on multi-speaker dataset, so it could learn to generalize to unseen voice"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-27T14:30:21Z",
        "body": "welcome to share or discuss if further questions~"
      }
    ]
  },
  {
    "number": 713,
    "title": "Inference speed with different devices, A100, RTX 3090, RTX 4090 ",
    "created_at": "2025-01-13T02:07:20Z",
    "closed_at": "2025-01-15T05:37:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/713",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\n@SWivid \r\nHI, I would like to ask about the inference speed. In the same CUDA, cudnn, and torch versions:\r\n\r\n+ The inference speed of NVIDIA A100-PCIE 40G is `2.67`\r\n+ The inference speed of NVIDIA GeForce RTX 3090 is `2.75`\r\n+ The inference speed of NVIDIA GeForce RTX 4090 is `1.46`\r\n\r\nI would like to ask why the A100 is reasoning at about the same speed as the 3090?\r\n\r\nThank you very much for the reply!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/713/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-13T03:58:33Z",
        "body": "Yes, we have similar test result with RTF metric.\r\nIt's reasonable cuz 3090 and A100 have similar fp32 fp16 performance.\r\n\r\nA100 is especially good for training cuz larger GPU mem."
      }
    ]
  },
  {
    "number": 709,
    "title": "Clarification on Commercial Use of F5-TTS When Retrained on a New Dataset",
    "created_at": "2025-01-12T04:54:52Z",
    "closed_at": "2025-01-12T15:54:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/709",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n**Hi,**\r\nFirst of all, I want to express my gratitude for all the work on **F5-TTS!**\r\n\r\nI have a question about the commercial use of the model. I understand that starting from **2024-10-14**, the checkpoints and the pretrained model are under **CC-BY-NC 4.0** due to the **Emilia dataset**, which limits commercial use.\r\n\r\nHowever, I need to use F5-TTS commercially and plan to retrain the model using my own dataset, which permits commercial use. Since the codebase is licensed under MIT, would I be able to use the retrained model commercially without the restrictions of the **CC-BY-NC 4.0** license, given that I won't be using the Emilia dataset or any of its derivatives?\r\n\r\n### To clarify:\r\n\r\n- **I intend to use only the codebase (MIT license) and my own dataset for retraining.**\r\n- **I will not use any pretrained weights or data from the Emilia dataset.**\r\n- **Does this mean the resulting model can be used for commercial purposes?**\r\n- **How would I go about doing/starting this?**\r\n\r\nThank you in advance for your assistance and clarification!\r\n\r\nBest regards,\r\n**Totaie Cyrus :)**",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/709/comments",
    "author": "Totaie",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-12T07:58:42Z",
        "body": "> I intend to use only the codebase (MIT license) and my own dataset for retraining.\r\nI will not use any pretrained weights or data from the Emilia dataset.\r\nDoes this mean the resulting model can be used for commercial purposes?\r\n\r\nyes, for certain if you own your training dataset or it's a public set commercially allowed"
      },
      {
        "user": "Totaie",
        "created_at": "2025-01-12T15:54:17Z",
        "body": "Great! Thank you for the quick response and clarification, I'll get started training right away!"
      }
    ]
  },
  {
    "number": 704,
    "title": "Anyone can share a 44k pretrain or gives some guide for training 44k from scratch by tiny dataset?",
    "created_at": "2025-01-09T10:57:01Z",
    "closed_at": "2025-01-17T02:07:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/704",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\nI want to train a 44k model to get a better voice quality, but failed to train. My dataset is about 10 hours, after about 300k updates, the learning rate has deceased to 1e-13, seems not update, increase to 400k still not improved, the voice is clear, But the content is still a mess. I think the model can not learn alignment with tiny dataset. Anyone has a success example?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/704/comments",
    "author": "ILG2021",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-09T11:15:31Z",
        "body": "> the learning rate has deceased to 1e-13\r\n\r\nset a large epoch number and train longer"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-09T11:37:12Z",
        "body": "> > the learning rate has deceased to 1e-13\r\n> \r\n> set a large epoch number and train longer\r\n\r\nThank. Should I use small model not base model? @SWivid "
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-09T11:46:14Z",
        "body": "> My dataset is about 10 hours,\r\n\r\nsmaller size is fine"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-10T21:59:14Z",
        "body": "I have set epoch to 100000, and use F5 small model arch.\r\nAfter 710k updates, the content is still a mess, and the sound become noisy, may be F5 is not suitable for tiny dataset from scratch."
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-11T07:22:16Z",
        "body": "> I have set epoch to 100000, and use F5 small model arch.\r\nAfter 710k updates\r\n\r\nhave you reset the epoch and restart the training or continue from previous one?\r\nhow is the learning rate curve"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-11T10:54:04Z",
        "body": "I create a new project, not resume to train. the learning rate curve is ok. because I set epoch to 100000"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-11T10:57:03Z",
        "body": "how is the batchsize, e.g. batch_size_per_gpu and gpu numbers?\r\n\r\nfor reference: we use default setting as in yaml file for small model to train with 24 hours LJSpeech"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-12T04:57:03Z",
        "body": "{\r\n    \"exp_name\": \"F5TTS_Small\",\r\n    \"learning_rate\": 7.5e-05,\r\n    \"batch_size_per_gpu\": 4800,\r\n    \"batch_size_type\": \"frame\",\r\n    \"max_samples\": 64,\r\n    \"grad_accumulation_steps\": 1,\r\n    \"max_grad_norm\": 1,\r\n    \"epochs\": 100000,\r\n    \"num_warmup_updates\": 300,\r\n    \"save_per_updates\": 10000,\r\n    \"last_per_steps\": 10000,\r\n    \"finetune\": false,\r\n    \"file_checkpoint_train\": \"\",\r\n    \"tokenizer_type\": \"char\",\r\n    \"tokenizer_file\": \"\",\r\n    \"mixed_precision\": \"fp16\",\r\n    \"logger\": \"tensorboard\",\r\n    \"bnb_optimizer\": false\r\n}\r\n\r\nThis is the settings, I have only 1 4080 GPU"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-12T08:07:24Z",
        "body": "> {\r\n\"exp_name\": \"F5TTS_Small\",\r\n\"batch_size_per_gpu\": 4800,\r\n\"batch_size_type\": \"frame\",\r\n\"grad_accumulation_steps\": 1,\r\n}\r\nThis is the settings, I have only 1 4080 GPU\r\n\r\nequals to 10k updates with `default setting as in yaml file for small model to train with 24 hours LJSpeech`\r\n\r\nfor reference, under `batch_size_per_gpu: 38400  # 8 GPUs, 8 * 38400 = 307200` setting, 100k updates to get ok results, 200k updates really good ones.\r\n\r\nit surely takes some time to train from scratch"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-12T10:13:13Z",
        "body": "> > {\r\n> > \"exp_name\": \"F5TTS_Small\",\r\n> > \"batch_size_per_gpu\": 4800,\r\n> > \"batch_size_type\": \"frame\",\r\n> > \"grad_accumulation_steps\": 1,\r\n> > }\r\n> > This is the settings, I have only 1 4080 GPU\r\n> \r\n> equals to 10k updates with `default setting as in yaml file for small model to train with 24 hours LJSpeech`\r\n> \r\n> for reference, under `batch_size_per_gpu: 38400 # 8 GPUs, 8 * 38400 = 307200` setting, 100k updates to get ok results, 200k updates really good ones.\r\n> \r\n> it surely takes some time to train from scratch\r\n\r\nyou mean I need 100k*307200/4800 = 6400k?"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-14T23:08:49Z",
        "body": "@SWivid "
      },
      {
        "user": "ILG2021",
        "created_at": "2025-01-15T00:49:17Z",
        "body": "> how is the batchsize, e.g. batch_size_per_gpu and gpu numbers?\r\n> \r\n> for reference: we use default setting as in yaml file for small model to train with 24 hours LJSpeech\r\n\r\nHello, how many steps do you train and how many hours do you cost?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-15T05:35:44Z",
        "body": "check previous response\r\neach 100k 8 hours on 8*h100"
      },
      {
        "user": "ILG2021",
        "created_at": "2025-02-08T16:01:22Z",
        "body": "> check previous response\n> each 100k 8 hours on 8*h100\n\nHello @SWivid If I have only one H100, how many steps should I train a 44k model. I have 16 hours data.\nHow many updates/s of your H100 when you train? I have only 2.6updates/s. Is it regular?\nOn a 4080 GPU, it can achieve 4.2 updates/s very strange. but batch size on 4080 is 4800.\n\n"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-08T16:33:55Z",
        "body": "base model on a100: 1~2updates/s (setting see paper)\nof course it depends on your actual batch size per gpu (frames_per_gpu * grad_accum)"
      }
    ]
  },
  {
    "number": 699,
    "title": "Character level duration",
    "created_at": "2025-01-08T07:27:27Z",
    "closed_at": "2025-01-09T12:52:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/699",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nCan we somehow get the duration of each character of `gen_text` in the final audio or `out`(mel)?\r\n\r\nAlthough the duration is calculated using `average frames per text X chars in gen_text`, the duration of each char is different in the output audio. How to map each char to duration in output audio?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/699/comments",
    "author": "nityanandmathur",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-08T07:31:22Z",
        "body": "#688 "
      },
      {
        "user": "nityanandmathur",
        "created_at": "2025-01-08T07:38:33Z",
        "body": "So, if I understand correctly, extracting durations is not possible without a duration predictor?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-08T07:40:58Z",
        "body": "so far cannot achieve high naturalness if you assign rigid duration"
      },
      {
        "user": "nityanandmathur",
        "created_at": "2025-01-08T09:35:57Z",
        "body": "Let's say I don't want to assign rigid duration but, only need to find the extent of a character in the final audio, i.e., if the text is \"I am Nits.\", what is the duration of 'I', 'am' and 'Nits' in the final audio?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-01-08T12:43:07Z",
        "body": "try with a VAD model"
      },
      {
        "user": "nityanandmathur",
        "created_at": "2025-01-09T12:52:03Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 698,
    "title": "long_skip_connection",
    "created_at": "2025-01-08T04:50:06Z",
    "closed_at": "2025-01-08T06:11:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/698",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n请教一个问题，我看到在dit当中看到有long_skip_connection这个可选参数，这个参数设置为true时对模型收敛会有帮助吗？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/698/comments",
    "author": "awei669",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-08T06:11:57Z",
        "body": ">  I have thoroughly reviewed the project documentation and read the related paper(s).\r\n\r\ncheck for our arxiv preprint, has ablation there\r\nin short, no help or even worse"
      }
    ]
  },
  {
    "number": 695,
    "title": "FIne-tuning/Training  on Urdu Language",
    "created_at": "2025-01-07T08:09:38Z",
    "closed_at": "2025-01-07T08:13:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/695",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [x] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nThere are 2 basic questions:\r\n1: I have around 80 hours of data for urdu language. Is fine-tuning a better option or Shall I train it from scratch? Also for how much time shall I train?\r\n\r\n2: Will the vocab size remain same 2346 or will it change for Urdu language ? ALso if it changes for new language, do I need to make some changes in the code?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/695/comments",
    "author": "Faisal11241992",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-01-07T08:13:52Z",
        "body": "As you have checked on these\r\n\r\n> I have thoroughly reviewed the project documentation and read the related paper(s).\r\n I have searched for existing issues, including closed ones, no similar questions.\r\n \r\n I believe these questions are already answered. Take some time to go through the readme, corresponding discussion boards, and just search with key words in existing issues.\r\n \r\n We would be happy to help if you have further questions"
      }
    ]
  },
  {
    "number": 676,
    "title": "Bug Fix: Parsing Argument --finetune always True",
    "created_at": "2024-12-28T11:19:46Z",
    "closed_at": "2024-12-29T13:29:50Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/676",
    "body": "I use f5-tts_finetune-gradio and found out that the finetune checkbox will always send finetune=true to the argument, eventhought the checkbox are off.\r\n\r\nAfter some investigation turns out using type=bool on ArgumentParser always return true because in python:\r\n`bool(\"False\")` are `True`.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/676/comments",
    "author": "hndrbrm",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-28T11:36:47Z",
        "body": "@hndrbrm seems bugs in first commit\r\n\r\nfeel free to test if works, if so will merge\r\nthanks again~"
      },
      {
        "user": "hndrbrm",
        "created_at": "2024-12-30T16:22:48Z",
        "body": "Its working, thanks."
      }
    ]
  },
  {
    "number": 668,
    "title": "多语言版本大概什么时候出来呀",
    "created_at": "2024-12-25T07:20:36Z",
    "closed_at": "2024-12-28T12:43:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/668",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n多语言版本大概什么时候出来呀",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/668/comments",
    "author": "wchuan163",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-25T11:48:47Z",
        "body": "现在支持中英双语，我们会尽快放出来支持更多语言的版本"
      }
    ]
  },
  {
    "number": 666,
    "title": "target_sample_rate must be 24000?",
    "created_at": "2024-12-24T06:45:15Z",
    "closed_at": "2024-12-25T14:50:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/666",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI try to change the target_sample_rate to 16000, and then the audio would be distorted. So can I use the model to produce audio with 24000 sample rate rather then convert the generated audio？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/666/comments",
    "author": "YTWeng176",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-24T09:24:44Z",
        "body": "> I try to change the target_sample_rate to 16000, and then the audio would be distorted. So can I use the model to produce audio with 24000 sample rate rather then convert the generated audio？\r\n\r\nas you have mentioned, you can first generate with 24khz then resample to 16khz\r\nor you could train from scratch new model checkpoints set up with 16khz the hyper param"
      }
    ]
  },
  {
    "number": 658,
    "title": "Leaving the ref_text blank via the cli doesn't use whisper ASR anymore",
    "created_at": "2024-12-21T15:05:14Z",
    "closed_at": "2024-12-21T15:19:19Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/658",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nWindows 11, RTX 3090\n\n### Steps to Reproduce\n\nRun the inference using the ``5-tts_infer-cli`` command with the ``--ref_text`` argument left blank.\n\n### ✔️ Expected Behavior\n\nLeaving ``--ref_text`` blank should invoke the Whisper ASR to transcribe the reference audio automatically.\n\n### ❌ Actual Behavior\n\nWith the recent updates, the CLI inference script appears to be broken. Leaving ``--ref_text`` blank no longer triggers Whisper ASR for transcription of the reference audio, as it previously did.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/658/comments",
    "author": "justinjohn0306",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-21T15:20:14Z",
        "body": "thanks for report, fixed"
      }
    ]
  },
  {
    "number": 649,
    "title": "支持口语化的语气词吗？",
    "created_at": "2024-12-19T08:18:47Z",
    "closed_at": "2024-12-19T08:25:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/649",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n作者团队你们好！\r\n\r\n我想请问：是否在生成音频的时候指定哪个地方有口语化的词？\r\n\r\n比如：你的号码是八八九八八九八八九吗？\r\n\r\n加入固定的口语化可能是：你的号码是八八九 嗯 八八九八八九吗？\r\n\r\n就是更加拟人一些，有人为的语气词，如 嗯、额、啊之类的\r\n\r\n感谢回复！",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/649/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-19T08:21:54Z",
        "body": "训练数据没有这些口语化的表达，您可以在有这种标签的数据上微调"
      },
      {
        "user": "WGS-note",
        "created_at": "2024-12-19T08:25:28Z",
        "body": "> 训练数据没有这些口语化的表达，您可以在有这种标签的数据上微调\r\n\r\n好的，非常感谢！"
      }
    ]
  },
  {
    "number": 643,
    "title": "Do you have to set ref_text",
    "created_at": "2024-12-18T08:32:26Z",
    "closed_at": "2024-12-21T02:05:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/643",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nDo you have to set ref_text? It's so troublesome. Can the program automatically recognize the corresponding subtitles? Thank you",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/643/comments",
    "author": "jxaujunjun",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-18T09:57:29Z",
        "body": "You can add an ASR model to recognize the corresponding subtitles automatically by yourself."
      }
    ]
  },
  {
    "number": 642,
    "title": "Custom vocab size mismatch ",
    "created_at": "2024-12-18T06:45:26Z",
    "closed_at": "2025-01-10T15:11:54Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/642",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nEnv same as defined\n\n### Steps to Reproduce\n\n1. Used the train.py script.\n\n### ✔️ Expected Behavior\n\nI have trained the model for 17 epochs and stopped for some purpose to debug, but when i want to train model previous pretrain checkpoint, it raises a shape mismatch error. I have vocab size 134, it always show 135 shape.\n\n### ❌ Actual Behavior\n\nTrain the scarch vocab file 134, but from the training checkpoint, it shows 135.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/642/comments",
    "author": "saiful9379",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-18T06:47:24Z",
        "body": "the filler token \\<F\\> is also a vocab\r\nand normally if you have not rerun the script to prepare dataset (i.e. not overwrite previous vocab.txt), will not have mismatch error"
      },
      {
        "user": "saiful9379",
        "created_at": "2024-12-21T19:51:24Z",
        "body": "@SWivid Thank you for your reply, but I want to know where to add extra tokens when loading a pretrained checkpoint. I have changed the data loader without preparing the dataset. Could you please mention where to add extra token when loading pretrain."
      }
    ]
  },
  {
    "number": 640,
    "title": "Update card: Add Russian model",
    "created_at": "2024-12-17T19:17:23Z",
    "closed_at": "2024-12-17T19:23:49Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/640",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/640/comments",
    "author": "HotDro4illa",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-17T19:23:45Z",
        "body": "Thanks for sharing ~"
      }
    ]
  },
  {
    "number": 635,
    "title": "The size of tensor a (512) must match the size of tensor b (521) at non-singleton dimension 2",
    "created_at": "2024-12-16T03:44:59Z",
    "closed_at": "2024-12-16T06:57:50Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/635",
    "body": "### Checks\r\n\r\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\r\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\r\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Environment Details\r\n\r\ntorch                         2.3.0\r\n\r\n### Steps to Reproduce\r\n\r\n我的运行代码：\r\n\r\n```python\r\nf5-tts_infer-cli \\\r\n--model \"F5-TTS\" \\\r\n--ref_audio \"/home/wangguisen/projects/tts/test_audio/wgs-f5tts_test.wav\" \\\r\n--ref_text \"那到时候再给你打电话，麻烦你注意接听。\" \\\r\n--gen_text \"这点请您放心，估计是我的号码被标记了，请问您是XX吗？\" \\\r\n--vocoder_name \"vocos\" \\\r\n--load_vocoder_from_local \\\r\n--ckpt_file \"/home/wangguisen/projects/tts/F5-TTS/ckpts/F5TTS_Base/model_1200000.pt\" \\\r\n--remove_silence false \\\r\n--speed 1.0 \\\r\n--output_dir \"/home/wangguisen/projects/tts/test_audio/output\" \\\r\n--output_file \"wgs_out.wav\"\r\n```\r\n\r\n### ✔️ Expected Behavior\r\n\r\n之前的代码并没有问题，但是我重新pull后就出现了这个问题\r\n\r\n### ❌ Actual Behavior\r\n\r\n报错：\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/home/wangguisen/miniconda3/envs/f5_tts/bin/f5-tts_infer-cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/wangguisen/projects/tts/F5-TTS/src/f5_tts/infer/infer_cli.py\", line 223, in main\r\n    main_process(ref_audio, ref_text, gen_text, ema_model, mel_spec_type, remove_silence, speed)\r\n  File \"/home/wangguisen/projects/tts/F5-TTS/src/f5_tts/infer/infer_cli.py\", line 203, in main_process\r\n    audio, final_sample_rate, spectragram = infer_process(\r\n  File \"/home/wangguisen/projects/tts/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 384, in infer_process\r\n    return infer_batch_process(\r\n  File \"/home/wangguisen/projects/tts/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 469, in infer_batch_process\r\n    generated_wave = vocoder.decode(generated_mel_spec)\r\n  File \"/home/wangguisen/miniconda3/envs/f5_tts/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/wangguisen/miniconda3/envs/f5_tts/lib/python3.10/site-packages/vocos/pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n  File \"/home/wangguisen/miniconda3/envs/f5_tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/wangguisen/miniconda3/envs/f5_tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/wangguisen/miniconda3/envs/f5_tts/lib/python3.10/site-packages/vocos/models.py\", line 80, in forward\r\n    x = self.norm.weight.data * x / torch.norm(x, p=2, dim=1, keepdim=True) + self.norm.bias.data\r\nRuntimeError: The size of tensor a (512) must match the size of tensor b (521) at non-singleton dimension 2\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/635/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "WGS-note",
        "created_at": "2024-12-16T06:57:47Z",
        "body": "我重新构建了Conda环境，已解决。"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-16T08:45:53Z",
        "body": "@WGS-note 解决了就好~\r\n\r\nnote：\r\n新更新的版本里`--remove_silence`也变成store_true了\r\n如果toml里没有设置true或者不传，默认就是false，和`--load_vocoder_from_local`一样了"
      }
    ]
  },
  {
    "number": 630,
    "title": "Checkpoint saving differences",
    "created_at": "2024-12-15T09:30:49Z",
    "closed_at": "2024-12-17T02:16:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/630",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\nWhen having `grad_accumulation_steps` set to a different value than `1`, the checkpoint saving is a bit unexpected:\r\n\r\nIn `trainer.py` for setting `save_per_updates`:\r\n\r\n```python\r\nif global_step % (self.save_per_updates * self.grad_accumulation_steps) == 0:\r\n    self.save_checkpoint(global_step)\r\n```\r\n\r\nfor setting `last_per_steps`:\r\n\r\n```python\r\nif global_step % self.last_per_steps == 0:\r\n    self.save_checkpoint(global_step, last=True)\r\n```\r\n\r\nConsequently, for my global batch-size of `19200*8` the value of `save_per_updates` needs to be divided by `8` to be comparable to the setting of `last_per_steps` and the overall variable `global_step` shown via `tqdm`.\r\n\r\nIs this intended ?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/630/comments",
    "author": "lumpidu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T10:11:43Z",
        "body": "yes, intended\r\nstep and update are different stuffs"
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-16T23:46:34Z",
        "body": "Similar subject as in #632, closing"
      }
    ]
  },
  {
    "number": 624,
    "title": "Update SHARED.md",
    "created_at": "2024-12-13T10:36:31Z",
    "closed_at": "2024-12-13T10:57:36Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/624",
    "body": "TYPO, HF Links... Sorry for that...",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/624/comments",
    "author": "AsmoKoskinen",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-13T10:57:28Z",
        "body": "lol, it's fine~"
      }
    ]
  },
  {
    "number": 623,
    "title": "Added Italian model card",
    "created_at": "2024-12-12T23:27:41Z",
    "closed_at": "2024-12-13T08:00:13Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/623",
    "body": "Added Italian model card ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/623/comments",
    "author": "MithrilMan",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-13T08:00:02Z",
        "body": "Many thanks~"
      },
      {
        "user": "Darux86",
        "created_at": "2025-01-26T08:12:36Z",
        "body": "how do I add it in comfyui?"
      }
    ]
  },
  {
    "number": 622,
    "title": "F5 Word swallowing and missing reading",
    "created_at": "2024-12-12T16:30:16Z",
    "closed_at": "2025-01-10T15:04:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/622",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nIn the process of text to speech in F5 model, the phenomenon of word swallowing and missing reading often occur. However, in the phoneme conversion stage, F5 has entered the corresponding text phonemes, but the speech is not read out",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/622/comments",
    "author": "kitty359",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-13T01:02:01Z",
        "body": "can you share some example, including your text and generation"
      },
      {
        "user": "kitty359",
        "created_at": "2024-12-23T15:41:15Z",
        "body": "> Quote reply\r\nI'm sorry I didn't get back to you in time. Through my experiments, I found that for Chinese, F5 could not pronounce some words because they were soft Chinese sounds.（The tone of 0 and 1 in Chinese intonation）\r\n\r\nFor example, t \"那个\"  ，“个” phoneme is \"ge\",It wouldn't have said \"ge.\"。 '简朴' is also impossible to pronounce correctly. I wonder if this situation can be improved by fine-tuning?"
      },
      {
        "user": "kitty359",
        "created_at": "2024-12-23T15:43:36Z",
        "body": "In addition, I would like to ask, what is the main role of fine tuning? If I just use one person's voice, can I make them sound more similar?"
      }
    ]
  },
  {
    "number": 618,
    "title": "Numeric digits can only be pronounced using Chinese pronunciation, not English pronunciation.",
    "created_at": "2024-12-12T02:26:11Z",
    "closed_at": "2024-12-13T07:41:03Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/618",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\nWhen synthesizing numeric digits from an English text, these digits can only be pronounced using Chinese pronunciation, not English pronunciation. Date texts (e.g., Mar. 12, 2024) also have a similar problem.\n\n### 2. What is your suggested solution?\n\nSince I am using the API, I need a parameter like language=\"en\" and language=\"ch\" to identify how to pronounce the numeric digits.\n\n### 3. Additional context or comments\n\nno.\n\n### 4. Can you help us with this feature?\n\n- [ ] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/618/comments",
    "author": "likangkao",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-12T07:59:42Z",
        "body": "you need to transform the numeric digits into english words, it can works well"
      },
      {
        "user": "likangkao",
        "created_at": "2024-12-12T21:02:30Z",
        "body": "Ok. Thanks for your reply."
      }
    ]
  },
  {
    "number": 612,
    "title": "Training with IPA + English chars",
    "created_at": "2024-12-10T17:06:15Z",
    "closed_at": "2024-12-15T18:31:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/612",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nif I'd fine-tune the base model for a new language and at the same time also integrate English data, could I use `IPA` phones for the new language and use additionally normal text as before for the English training part ?\r\nThe idea is to have more control over pronunciation of the target language while preserving the ability to speak English and to guide the model better, when to apply which code.\r\n\r\n\r\nI have seen, that `vocab.txt` already contains most of the `IPA` phone-set (but not all IPA characters are integrated and also no secondary stress symbol).\r\n\r\nI am confused about the meaning of the used vocabulary **for the trained base-model**:\r\n\r\n- what is the meaning of `vocab.txt` ?\r\n- is one free to choose any UTF-8 character for training/finetuning or only what is inside `vocab.txt` ?\r\n- what is the consequence of keeping the character sets of already trained/ to be finetuned datasets separate ? Does this help code-switching, e.g. for English/Chinese ?\r\n- have you already tried using phones instead of normalized text ? This could also simply be done via `SAMPA`, if `IPA` symbols cannot be supported by the base-model",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/612/comments",
    "author": "lumpidu",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-12T07:58:42Z",
        "body": "1. The vocab contains character tokens (for modeling English) and pinyin initials and finals (for modeling Chinese).\r\n2. You can freely use any UTF-8 characters.\r\n3. This means you need to retrain the word embeddings.\r\n4. We have tried using IPA, and the results were also very good."
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-13T14:29:18Z",
        "body": "To 3.) what I meant: if one uses characters of the existing `vocab.txt`of a base model and remaps the characters for a fine-tune for a different language to unused characters of this `vocab.txt` as used so far in the base model (i.e. no pinyin + standard latin characters), does this help code switching, so that the model better learns to distinguish the new and the existing languages ?\r\n\r\nTo 4.) Has this helped in better controllability of OOD words or is there no difference for the model ? I could imagine that homographs could be better supported via phonemes instead of graphemes."
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-14T01:54:00Z",
        "body": "3）Why would this approach help improve code-switching? Could you provide a detailed example? However, this approach might reduce your training costs.\r\n\r\n4）Using phonemes results in a smaller performance drop for English, but a larger performance drop for Chinese, as it cannot effectively handle polyphonic characters (characters with multiple pronunciations)."
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-14T15:50:35Z",
        "body": "3.) Mapping new language via existing unused characters of vocab.txt\r\n\r\nLet's add German to the mix: so far latin characters are used for English + the Pinyin characters for Chinese. There are unique German characters: `Umlauts` (ä, ö, ü, Ä, Ö, Ü) + a longish sharp s `ß`, none of them is used in both trained languages, therefore the model will be able to learn their individual role in the new language. But German has a lot of words that are written equally but pronounced differently than in English (e.g. **Bank, Ball, Boss, Rat, Ring**). Furthermore, all other German characters besides the exceptions mentioned above are the same as in English.\r\n\r\nThe sentence: **mein boss gab mir den rat, den ball flach zu halten** (i.e. \"My boss advised me to keep a low profile\") would be a challenge for the model to synthesize, if code switching is also aimed for in the new model, e.g. via adding German + English datasets to the fine-tune.\r\n\r\nBut let's assume, we don't change anything inside `vocab.txt`, but simply remap all German characters besides the special characters to e.g. the Greek alphapet which is part of the vocab.txt, but not used so far in the English/Chinese training. Then the above sentence would be like:\r\n\r\n*μειν βοσσ γαβ μιρ δεν ρατ, δεν βαλλ φλακχ ζυ χαλτεν*\r\n\r\nMy idea would be then to choose the English pronunciation / code switching e.g. like the following:\r\n\r\n*μειν **boss** γαβ μιρ δεν ρατ, δεν **ball** φλακχ ζυ χαλτεν*\r\n\r\nI hope my idea gets clearer via the above example. I want to better control, when the model should speak what code.\r\n\r\n4.) I don't really understand, why training via phones degrades the model, as pronunciation can follow more straight-forward principles than via graphemes alone, where you even need the full context for the correct pronunciation (like an LM).\r\n\r\nE.g. *\"Umfahren\"*: *\"du musst das zentrum umfahren\"* (\"You must drive around the center.\"), *\"du musst das schild umfahren\"* (\"You must run over the sign.\")\r\n\r\n- `/ʊmˈfaːʁən/`- \"to drive around sth.\"\r\n- `/ˈʊmfaːʁən/` - \"to run over\"\r\n\r\nThe above example shows the difference for syllable stresses, but should generalize for the IPA characters as well - which simply are much closer to pronunciation. Btw. not all IPA symbols are part of the current `vocab.txt`.\r\n\r\nI don't think this approach would help for code switching, as the same IPA symbols are used with different pronuncations in different languages, but the hope is to better control the pronunciation without being dependent on too much training data and/or training data variety. E.g. in our case, we have enough training data with many speakers, but almost always the same texts."
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-15T01:12:23Z",
        "body": "3. Indeed, this approach can preserve the ability for code-switching.\r\n4. The performance in English doesn't degrade significantly, but the main issue is a considerable decline in Chinese performance, primarily due to the problem of polyphonic characters (the same character having different pronunciations)."
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-15T18:31:11Z",
        "body": "3.) Understood. I'll try that out and compare with the normal grapheme approach\r\n4.) Understood, IPA doesn't preserve the nuances for Chinese, therefore not applicable here / degradation consequences.\r\n\r\nThanks for your clarifications."
      },
      {
        "user": "lbdave94",
        "created_at": "2024-12-18T13:42:00Z",
        "body": "@lumpidu Hi,\r\n\r\nat the end did you try to fine-tuned the model using phonemes? If yes, what are your results?"
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-20T11:15:36Z",
        "body": "Not yet, I am training first different rounds of datasets with some different changes (pauses removed, punctuations, etc.) to learn how the model behaves."
      }
    ]
  },
  {
    "number": 611,
    "title": "Update infer_cli.py",
    "created_at": "2024-12-10T15:53:48Z",
    "closed_at": "2024-12-15T13:57:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/611",
    "body": "i add --cross_fade_duration argument ,which was missing ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/611/comments",
    "author": "djallalzoldik",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T13:57:39Z",
        "body": "thanks, will add with #628 "
      }
    ]
  },
  {
    "number": 609,
    "title": "Audio  degrading when using Transcribe Data",
    "created_at": "2024-12-08T23:01:52Z",
    "closed_at": "2024-12-13T07:47:57Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/609",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nWindows 10, RTX 3060, Python 3.11.9, torch==2.3.0+cu118, cuda 12.1\n\n### Steps to Reproduce\n\nLaunch f5-tts_finetune-gradio\r\nCreate new project\r\nPut either separate audio clips or full lengh audio file, either .wav or .mp3 into the data folder\r\nTranscribe on the Trascribe Data page\r\nCheck the prept audio in the wavs folder and compare to original\n\n### ✔️ Expected Behavior\n\nAudio keeps the same\r\nNo quality loss\n\n### ❌ Actual Behavior\n\nQuality loss\r\n\r\nBut only sometimes... 2/6 projects are fine.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/609/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "AlpacaManAlpha",
        "created_at": "2024-12-08T23:03:40Z",
        "body": "Audio sound like the base was dialed up a bit. Slightly hazy, mostly clean. Still, I would like to avoid this. Is there anything I can do?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-08T23:55:47Z",
        "body": "maybe because slicer or stuff, feel free to check; or try use dataset/ scripts to prepare and use train.py"
      }
    ]
  },
  {
    "number": 608,
    "title": "Gradio app subsequent gens don't redo the transcribing correctly",
    "created_at": "2024-12-08T17:29:50Z",
    "closed_at": "2024-12-09T00:11:41Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/608",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nWin11\n\n### Steps to Reproduce\n\nLatest git pull, \r\nUse Gradio,\r\nTry different voices (without transcription), get app to auto-transcribe,\r\nApp messes up transacriptions on new audio clips,\r\nHave to refresh window in (chrome) in order for it to reset\n\n### ✔️ Expected Behavior\n\nShould transcribe audio each new clip you put in, and clear the cache\n\n### ❌ Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/608/comments",
    "author": "bbecausereasonss",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-08T23:49:06Z",
        "body": "as suggested by this template\r\nany detailed info, e.g. screenshots?"
      },
      {
        "user": "bbecausereasonss",
        "created_at": "2024-12-09T00:11:40Z",
        "body": "My bad, I didn't realize I was adding the text to the wrong window."
      }
    ]
  },
  {
    "number": 605,
    "title": "Can we train based on float16?",
    "created_at": "2024-12-08T06:02:48Z",
    "closed_at": "2024-12-09T01:08:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/605",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nWe train based on dtype=float32  4*NVIDIA A100 80GB， 330h english voice datasets， 1 weeks，190k steps。\r\nIf we train on dtype=float16 ,Will it affect the effectiveness of training?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/605/comments",
    "author": "xwan07017",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-08T12:26:55Z",
        "body": "Our vocos-based F5TTS checkpoint is trained on A100 80GB with fp16"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-09T01:08:31Z",
        "body": "> Will it affect the effectiveness of training with fp16?\r\n\r\nlikely no degradation on performance\r\ntrain faster for sure"
      }
    ]
  },
  {
    "number": 604,
    "title": "Please include Custom Vocab maker Or Custom Vocab Option.",
    "created_at": "2024-12-08T04:49:09Z",
    "closed_at": "2024-12-09T01:09:13Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/604",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\nI find much complications while implementing custom\r\nVocab. Even though changing paths, the vocab mismatch error arises while training from scratch. Please help.\n\n### 2. What is your suggested solution?\n\nIncluding a custom vocab path modifier \n\n### 3. Additional context or comments\n\n_No response_\n\n### 4. Can you help us with this feature?\n\n- [X] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/604/comments",
    "author": "sam4muzix",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-09T01:09:13Z",
        "body": "thought already there?\r\nboth cli and local gradio"
      }
    ]
  },
  {
    "number": 601,
    "title": "Finnish Model added at Hugging Face",
    "created_at": "2024-12-06T18:24:03Z",
    "closed_at": "2024-12-07T05:38:32Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/601",
    "body": "Here is my first finnish model at hugging face.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/601/comments",
    "author": "AsmoKoskinen",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-07T05:38:29Z",
        "body": "@AsmoKoskinen thanks a lot!"
      }
    ]
  },
  {
    "number": 600,
    "title": "Russian language support for f5 tts",
    "created_at": "2024-12-06T16:25:21Z",
    "closed_at": "2024-12-06T16:28:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/600",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nWill there be Russian language support for cloning and training your own model on f5 and when? Can you do an f5 launch on colab?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/600/comments",
    "author": "mikhail2013ru",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T16:28:23Z",
        "body": "pls read the issue template:\r\n\r\n> I have thoroughly reviewed the project documentation and read the related paper(s).\r\n\r\nalready support finetuning\r\n\r\n> I have searched for existing issues, including closed ones, no similar questions.\r\n\r\nthere are issue for colab usage, you can just search `colab`"
      }
    ]
  },
  {
    "number": 597,
    "title": "生成的语音开头有噪音，人生比较快。",
    "created_at": "2024-12-06T10:50:35Z",
    "closed_at": "2024-12-06T12:16:55Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/597",
    "body": "### Checks\n\n- [ ] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [ ] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nRTX 3090\n\n### Steps to Reproduce\n\n1. 最新的代码\r\n2. f5-tts_infer-cli \\\r\n--model \"F5-TTS\" \\\r\n--ref_audio \"test.wav\" \\\r\n--ref_text \"大家都准备好了吗？大可大可，咱们开始上课吧。\" \\\r\n--gen_text \"面条是一种形状细长的面食。面条可以很短，如使头发，也可以又长又粗，如荞麦面，还可以同样长但更细，如意大利面，或极细，如玻璃面。无论是哪种形状和形态的面条，都是用某种面粉，小麦粉、大米粉、大豆粉、豌豆粉等）制作而成，不加发酵剂。面条在世界各地的许多菜系中都有制作，但主要是在东亚和地中海菜系中。四千年前在中国青海省发现的面条化石被认为是世界上最古老的面条，研究表明最早的面条是在中国新石器时代发展起来的。面条有新鲜的和干的两种，是多种菜肴的基础。特别值得一提的是汤面，它几乎广泛存在于世界各地的所有文化中。\"\n\n### ✔️ Expected Behavior\n\n能够修复这个问题\n\n### ❌ Actual Behavior\n\n开头有噪音。有了人声后 速度比较快。\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/597/comments",
    "author": "dttlgotv",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T12:16:55Z",
        "body": "please review the **Checks**\r\n\r\nuse a better reference audio, (~10s good quality)"
      },
      {
        "user": "dttlgotv",
        "created_at": "2024-12-09T08:28:03Z",
        "body": "> please review the **Checks**\r\n> \r\n> use a better reference audio, (~10s good quality)\r\n\r\n多谢，有效果。\r\n\r\n如果我的生成文本是这个， 合成的语音挺好。\r\n\r\n--gen_text \"面条是一种形状细长的面食。面条可以很短，如使头发，也可以又长又粗，如荞麦面，还可以同样长但更细，如意大利面，或极细，如玻璃面。无论是哪种形状和形态的面条，都是用某种面粉，小麦粉、大米粉、大豆粉、豌豆粉等）制作而成，不加发酵剂。\"\r\n\r\n但是如果我的生成文本是上面文本的两次重复（如下），合成的声音就不清晰，且有快进的感觉。\r\n--gen_text \"面条是一种形状细长的面食。面条可以很短，如使头发，也可以又长又粗，如荞麦面，还可以同样长但更细，如意大利面，或极细，如玻璃面。无论是哪种形状和形态的面条，都是用某种面粉，小麦粉、大米粉、大豆粉、豌豆粉等）制作而成，不加发酵剂。面条是一种形状细长的面食。面条可以很短，如使头发，也可以又长又粗，如荞麦面，还可以同样长但更细，如意大利面，或极细，如玻璃面。无论是哪种形状和形态的面条，都是用某种面粉，小麦粉、大米粉、大豆粉、豌豆粉等）制作而成，不加发酵剂。\"\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-09T12:34:06Z",
        "body": "长的文本会被自动切分，所以可能会切到不理想的断句导致语速不均匀。"
      },
      {
        "user": "dttlgotv",
        "created_at": "2024-12-09T12:54:16Z",
        "body": "> 长的文本会被自动切分，所以可能会切到不理想的断句导致语速不均匀。\r\n\r\n\r\n\r\n> 长的文本会被自动切分，所以可能会切到不理想的断句导致语速不均匀。\r\n\r\n哦，还以为是按照标点符号来断句的，没看那么仔细。"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-09T13:03:29Z",
        "body": "是按照标点切的，但double之后可能会切到逗号，如果整句很长"
      }
    ]
  },
  {
    "number": 596,
    "title": "在推理的时候，能将这个人的音色记录下来，并且不断推理吗？",
    "created_at": "2024-12-06T08:58:42Z",
    "closed_at": "2024-12-06T12:20:57Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/596",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\n能将这个人的音色记录下来，并且不断推理吗？\n\n### 2. What is your suggested solution?\n\n直接克隆下来\n\n### 3. Additional context or comments\n\n1\n\n### 4. Can you help us with this feature?\n\n- [X] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/596/comments",
    "author": "23332323",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T12:20:57Z",
        "body": "请看一下这个模板里Checks的内容以及4里面的内容\r\n\r\n传的参考音频是什么样，就克隆什么样"
      },
      {
        "user": "23332323",
        "created_at": "2024-12-06T12:21:25Z",
        "body": "您好，您的邮件已收到，谢谢！  余清"
      }
    ]
  },
  {
    "number": 594,
    "title": "Does the vocabulary size impact the amount of data required for training?",
    "created_at": "2024-12-06T04:19:55Z",
    "closed_at": "2024-12-06T16:27:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/594",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI've been experimenting with this recently, there seems to be some great potential here.  It does clearly take a lot of data to work well.  I was wondering then, if it would train more quickly or with less data if the vocab.txt file is smaller?  Has anyone tried this out?  I realize it would reduce the multi-language capability of the resultant model.  ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/594/comments",
    "author": "DrBrule",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T04:58:31Z",
        "body": "no idea,\r\nthought the hard part is to learn speech-text alignment but word embed seems no conflict with that\r\nand if we use big vocab size but just use part of it, the gradient will just go for the used ones, thus the only bad thing is wasted params and sparse weights (low utilization rate)?"
      },
      {
        "user": "DrBrule",
        "created_at": "2024-12-06T16:27:25Z",
        "body": "That's clear, thank you!  Seems like it wouldn't make much of a difference in practice. "
      }
    ]
  },
  {
    "number": 590,
    "title": "Can I use my own language IPA for getting perfect model ?",
    "created_at": "2024-12-05T16:02:20Z",
    "closed_at": "2024-12-14T01:54:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/590",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI trained my dataset for about 40 hours with a single speaker using F5 for 35 epochs. My model synthesizes short words well, but when synthesizing longer text, it produces speech with a different accent. For example, Uzbek is similar to Turkish, so sometimes my model synthesizes Uzbek text with a Turkish accent. Additionally, there is some noise at the end of the output audio.\r\n\r\nHow can I resolve this issue?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/590/comments",
    "author": "RifatMamayusupov",
    "comments": [
      {
        "user": "Mustaphajudi",
        "created_at": "2024-12-05T21:55:11Z",
        "body": "Bigger dataset,and longer audio samples"
      },
      {
        "user": "RifatMamayusupov",
        "created_at": "2024-12-06T03:23:24Z",
        "body": "@Mustaphajudi can you tell me length of audio ? such as 10s or 15 s ?\r\nand total 50 hours dataset is enough ?"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-12-12T08:01:34Z",
        "body": "> @Mustaphajudi can you tell me length of audio ? such as 10s or 15 s ? and total 50 hours dataset is enough ?\r\n\r\nI think the setting is ok"
      }
    ]
  },
  {
    "number": 587,
    "title": "Voice Control ideas (brainstorming)",
    "created_at": "2024-12-05T00:21:03Z",
    "closed_at": "2024-12-12T14:24:00Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/587",
    "body": "Since this is a diffusion model, I'm wondering if would be possible to implement something like ControlNet did for StableDiffusion/Flux\r\n\r\nBasically is it possible to build models that drive the audio generation by, for example, extracting the emotion from a voice reference (different from the reference used to \"inpaint\"), or the roughness, etc...\r\n\r\nI'm not asking now to implement this but I just wanted to understand the possibilities given by a diffusion model for voices",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/587/comments",
    "author": "MithrilMan",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-05T03:59:05Z",
        "body": "yes, possible. the reason we are open-sourcing is exactly to make it easier for all people to take a try for any idea"
      }
    ]
  },
  {
    "number": 586,
    "title": "Some short sentences cannot be synthesized.",
    "created_at": "2024-12-04T13:46:17Z",
    "closed_at": "2024-12-05T03:37:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/586",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n`Unit 1`\r\n\r\nThis sentecen can not work, why? \r\n\r\nDoes there a way to make it work?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/586/comments",
    "author": "lucasjinreal",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-05T03:37:46Z",
        "body": "the duration is simply linearly estimated, so not that accurate for extremely short sentences\r\n#460 would help, for try fix duration (the total length of ref_audio + gen_audio in second)"
      }
    ]
  },
  {
    "number": 583,
    "title": "returned non-zero exit status 1 when run Train data",
    "created_at": "2024-12-04T12:43:20Z",
    "closed_at": "2024-12-04T14:23:11Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/583",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nWindows 11, Python 3.11\n\n### Steps to Reproduce\n\nRun Train Data with Auto Settings\n\n### ✔️ Expected Behavior\n\nRun Train data\n\n### ❌ Actual Behavior\n\nvocab :  2545\r\nUsing logger: tensorboard\r\nTraceback (most recent call last):\r\n  File \"C:\\F5-TTS\\src\\f5_tts\\train\\finetune_cli.py\", line 161, in <module>\r\n    main()\r\n  File \"C:\\F5-TTS\\src\\f5_tts\\train\\finetune_cli.py\", line 132, in main\r\n    trainer = Trainer(\r\n              ^^^^^^^^\r\n  File \"C:\\F5-TTS\\src\\f5_tts\\model\\trainer.py\", line 90, in __init__\r\n    from torch.utils.tensorboard import SummaryWriter\r\n  File \"C:\\F5-TTS\\venv\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\", line 1, in <module>\r\n    import tensorboard\r\nModuleNotFoundError: No module named 'tensorboard'\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"C:\\F5-TTS\\venv\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\F5-TTS\\venv\\Lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"C:\\F5-TTS\\venv\\Lib\\site-packages\\accelerate\\commands\\launch.py\", line 1168, in launch_command\r\n    simple_launcher(args)\r\n  File \"C:\\F5-TTS\\venv\\Lib\\site-packages\\accelerate\\commands\\launch.py\", line 763, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['C:\\\\F5-TTS\\\\venv\\\\Scripts\\\\python.exe', 'src/f5_tts/train/finetune_cli.py', '--exp_name', 'F5TTS_Base', '--learning_rate', '1e-05', '--batch_size_per_gpu', '3200', '--batch_size_type', 'frame', '--max_samples', '64', '--grad_accumulation_steps', '1', '--max_grad_norm', '1', '--epochs', '431', '--num_warmup_updates', '900', '--save_per_updates', '1802', '--last_per_steps', '450', '--dataset_name', 'swedish', '--finetune', 'True', '--tokenizer', 'pinyin', '--log_samples', 'True', '--logger', 'tensorboard']' returned non-zero exit status 1.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/583/comments",
    "author": "CJ468",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-04T14:23:12Z",
        "body": "> import tensorboard\r\n> ModuleNotFoundError: No module named 'tensorboard'\r\n\r\npip install tensorboard"
      },
      {
        "user": "CJ468",
        "created_at": "2024-12-04T14:42:01Z",
        "body": "Hi, thanks just to be sure I reinstalled Tensorboard but getting the same error.\r\nThe program doesn't seem to find tensorboard? \r\nModuleNotFoundError: No module named 'tensorboard'"
      },
      {
        "user": "CJ468",
        "created_at": "2024-12-04T15:57:27Z",
        "body": "Found the problem, fixed now. "
      },
      {
        "user": "promptpirate",
        "created_at": "2024-12-23T06:31:19Z",
        "body": "> Found the problem, fixed now.\r\n\r\nHow did you fix this by the way?"
      }
    ]
  },
  {
    "number": 581,
    "title": "Training longer vs more NFE steps",
    "created_at": "2024-12-04T07:45:42Z",
    "closed_at": "2024-12-04T08:28:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/581",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nFrom the paper, training longer (figure 2) and using more NFE steps (table 2) both improve inference output WER and SIM-o. I am wondering if training for much longer (for example, several million steps on a single language) will allow us to reduce NFE steps significantly, which will help inference speed.\r\n\r\nTable 2 seems to be saying that more NFE steps are crucial for MOS subjective evaluations. Does this mean longer training can never fully replace having a high number of NFE steps?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/581/comments",
    "author": "kmn1024",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-04T08:02:06Z",
        "body": "number of updates and nfe steps are different and no conflict with each other\r\n\r\ntraining too long will overfit;\r\nsamely too large nfe will have marginal effect\r\n\r\nnfe steps is basically for efficiency trade-off (speed / performance)"
      }
    ]
  },
  {
    "number": 572,
    "title": "Can you specify polyphonic words when you encounter them in your reasoning?",
    "created_at": "2024-12-03T08:32:36Z",
    "closed_at": "2024-12-03T12:09:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/572",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nIf there is a polyphonic word in the given text, can the pronunciation of this word be fixed while reasoning?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/572/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-03T12:09:52Z",
        "body": "> I have searched for existing issues, including closed ones, no similar questions.\r\n\r\nyou can refer to previous issue\r\n\r\npolyphonic 多音字 etc."
      }
    ]
  },
  {
    "number": 571,
    "title": "Update Speech Edit Gradio",
    "created_at": "2024-12-03T08:31:03Z",
    "closed_at": "2024-12-15T10:51:16Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/571",
    "body": "To make audio editing more convenient, a new Gradio interface for audio editing has been added.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/571/comments",
    "author": "Happenmass",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T10:51:16Z",
        "body": "thanks for pr~\r\n\r\nthe crucial part is to do vad and match the content\r\ni.e. to determine `editable_time_line = gr.Textbox(label=\"Editable Time Line[[], []]\", lines=2)`\r\n\r\nwe will see if there's an easy way to do this later, and will add this with your contributed code if got this function mature"
      }
    ]
  },
  {
    "number": 570,
    "title": "[macOS issue] RuntimeError: Placeholder storage has not been allocated on MPS device!",
    "created_at": "2024-12-03T08:19:52Z",
    "closed_at": "2024-12-03T11:49:30Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/570",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nRuntimeError: Placeholder storage has not been allocated on MPS device!\r\n\r\nHad tried both device. cpu or mps all not work\n\n### Steps to Reproduce\n\nRuntimeError: Placeholder storage has not been allocated on MPS device!\n\n### ✔️ Expected Behavior\n\nworkable.\n\n### ❌ Actual Behavior\n\nRuntimeError: Placeholder storage has not been allocated on MPS device!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/570/comments",
    "author": "MonolithFoundation",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-03T11:49:30Z",
        "body": "the purpose of the template is to make the process of solving problem easier\r\n\r\nif you just skip and provide no details, we cannot help"
      }
    ]
  },
  {
    "number": 569,
    "title": "Can reasoning be accelerated?",
    "created_at": "2024-12-03T07:42:04Z",
    "closed_at": "2024-12-12T08:02:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/569",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI synthesized the following text:\r\n\r\n`这点请您放心，估计是我的号码被标记了，请问您是沈沈吗？`\r\n\r\nIt takes 3 seconds to reason.\r\n\r\nCan millisecond-level reasoning be achieved?\r\n\r\nVery much looking forward to hearing back from the author team!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/569/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-03T11:37:54Z",
        "body": "you can do some engineering optimization to boost speed (will surely speed up in magnitude\r\nand welcome pr"
      }
    ]
  },
  {
    "number": 563,
    "title": "Bad synthesis results when reading phone numbers（读取电话号码时合成结果不佳）",
    "created_at": "2024-12-02T03:33:19Z",
    "closed_at": "2024-12-06T05:03:35Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/563",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\ntorch                         2.3.0\r\ntorchaudio                    2.3.0\n\n### Steps to Reproduce\n\nWhen there is a phone number in `gen_text`, there is an undesirable pause in the synthesized result, hopefully the phone number should be concatenated.\r\n\r\n当 `gen_text` 中有电话号码的时候，合成结果中会出现不应该有的停顿，希望的是电话号码应该是连起来读的\r\n\r\n```shell\r\n! python inference-cli.py \\\r\n--model \"F5-TTS\" \\\r\n--ref_audio \"tests/lyn-f5tts_test.wav\" \\\r\n--ref_text \"那到时候再给你打电话，麻烦你注意接听\" \\\r\n--gen_text \"你好，我这边是XXX的工作人员，请问你的号码是幺幺幺八八八七三三七七吗？\" \\\r\n--ckpt_file \"/home/repository/audio/F5TTS_Base/model_1200000.safetensors\" \\\r\n--output_dir \"tests/lyn_out.wav\"\r\n```\n\n### ✔️ Expected Behavior\n\nSmooth reading of phone numbers\r\n\r\n流畅的读电话号码\n\n### ❌ Actual Behavior\n\nPhone numbers are not synthesized well and there are unwanted pauses\r\n\r\n电话号码合成效果不好，会出现不应该有的停顿",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/563/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T05:03:35Z",
        "body": "Yes, the problem exists. finetuning will help or train with more phone number reading data\r\nthe model still lacks these practices.\r\n\r\n是的，电话号码模型还不太6，需要在相关数据上再训下微调啥的\r\n另外我试了下，不同的参考音频可能不一样，有些会读得好些，有些差些\r\n可以尝试指定成`幺幺幺，八八八七，三三七七`这种，如果要一直连读就没什么想法了，换换参考音频试试"
      }
    ]
  },
  {
    "number": 562,
    "title": "The output content may be mixed with other content",
    "created_at": "2024-12-02T03:27:37Z",
    "closed_at": "2024-12-13T07:48:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/562",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nThis model has failed to generate speech according to Text to Generate multiple times, and it will automatically add some other content. What is the reason for this and is there any solution",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/562/comments",
    "author": "zyq-zzz",
    "comments": [
      {
        "user": "hequnbai",
        "created_at": "2024-12-02T06:18:47Z",
        "body": "I have the same issue. I use mandarin, sometimes it generates unexpected replica of some words, sometimes skips a few ones. Not very stable. I also finetuned using private dataset, the timbre and prosody and speaking style get really close to the training and reference set, but still the error remains, and becomes even worse. \r\n\r\nAnyone has a clue of the issue and give some solutions ? ..."
      },
      {
        "user": "liuhui881125",
        "created_at": "2024-12-02T06:52:25Z",
        "body": "Open advanced settings and clear reference text"
      },
      {
        "user": "zyq-zzz",
        "created_at": "2024-12-02T07:35:04Z",
        "body": "> Open advanced settings and clear reference text\r\nThat's not the reason, that text cleans up every time it's opened\r\n"
      },
      {
        "user": "thesandi99",
        "created_at": "2024-12-02T14:42:17Z",
        "body": "Guys, this is not a high-end parameter model like 3B, 9B, or 90B, so these problems are bound to occur."
      }
    ]
  },
  {
    "number": 560,
    "title": "How to train a 48khz model",
    "created_at": "2024-12-01T07:35:27Z",
    "closed_at": "2024-12-01T07:56:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/560",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nThe default output sample rate is 24k, which is too low quality, I want to train a 48k model, anyone can gives a guide?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/560/comments",
    "author": "ILG2021",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-01T07:56:42Z",
        "body": "1. use 48khz dataset\r\n2. set the target_sample_rate to 48khz\r\n3. use trained 48khz model and a 48khz vocoder"
      }
    ]
  },
  {
    "number": 554,
    "title": "EMA True ignores letters other than English letters.",
    "created_at": "2024-11-29T22:02:50Z",
    "closed_at": "2024-11-30T15:07:09Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/554",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nWindows, RTX3070, Python 3.10, torch==2.3.0, cuda 11.8\n\n### Steps to Reproduce\n\n1. Clone the rep, and prepare env as in documentation.\r\n2. Finetune a non-English model using gradle webui - `f5-tts_finetune-gradio`\r\n3. In the finetune webui, in Test Model tab, check \"Use EMA\".\r\n4. Select the latest or saved update of your non-English model.\r\n5. Add reference audio and text that has letters that doesn't exist in English.\r\n6. Click Infer button\n\n### ✔️ Expected Behavior\n\nExpecting the model to generate audio speaking non-english letters while EMA is turned on.\n\n### ❌ Actual Behavior\n\nThe model skips non-english letters and generates without saying it.\r\n\r\nFor example, the words like \"Ştraüs\" is being transcribed as \"tras\".\r\n\r\nWhen I turn off EMA, then it doesn't ignore and describes well. The issue happens only when I turn on EMA.\r\nWhat might be the issue? Does EMA ignore the vocab?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/554/comments",
    "author": "Alykasym",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-30T15:07:09Z",
        "body": "it is mainly because the finetuning process just goes few steps, ema weights still dominated by previous pretrained weight\r\nif you just train few steps, turn ema off; if trained longer, could try with it"
      }
    ]
  },
  {
    "number": 546,
    "title": "Add functionality to the Transcription section",
    "created_at": "2024-11-28T04:53:46Z",
    "closed_at": "2024-12-13T07:49:04Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/546",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\nI can't upload a large volume of audio files in the \"Transcription data\" section to create a dataset through the audio file upload window, everything freezes. My file size is 20 GB\n\n### 2. What is your suggested solution?\n\nCan you add a window in \"Gradio\" in the \"Transcription data\" section where I can specify the path to the folder where the audio files are located, this will eliminate the freeze and make it possible to upload a large volume of files to create a dataset.\r\n\r\nThank you\n\n### 3. Additional context or comments\n\n_No response_\n\n### 4. Can you help us with this feature?\n\n- [ ] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/546/comments",
    "author": "nikish3",
    "comments": [
      {
        "user": "danielw97",
        "created_at": "2024-11-28T05:21:53Z",
        "body": "I don't have the interface in front of me at the moment, but there is an audio from path checkbox in the gradio training interface if memory serves.\r\nYou can then place your files into the dataset subdirectory of your dataset.\r\ni.e. if your dataset is called voice1, the directory would be voice1/dataset\r\nHope this helps a bit."
      },
      {
        "user": "nikish3",
        "created_at": "2024-11-28T05:29:03Z",
        "body": "Thank"
      }
    ]
  },
  {
    "number": 545,
    "title": "issue training F5tts small model after new training code",
    "created_at": "2024-11-28T04:22:22Z",
    "closed_at": "2024-11-28T04:36:55Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/545",
    "body": "### Checks\r\n\r\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\r\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\r\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Environment Details\r\n\r\npython 3.10.12 (wsl)\r\n\r\n### Steps to Reproduce\r\n\r\n1. initialize training using the new config-based training code following the readme.\r\nIn my case I'm trying to train the f5tts small model and have modified the training config to point to my dataset.\r\nI receive the following error:\r\n\r\n```\r\nError executing job with overrides: []                                                                                  \r\nTraceback (most recent call last):                                                                                      \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/train/train.py\", line 35, in main                                                \r\n    model = CFM(                                                                                                        \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/model/cfm.py\", line 55, in __init__                                              \r\n    self.mel_spec = default(mel_spec_module, MelSpec(**mel_spec_kwargs))                                                \r\nTypeError: MelSpec.__init__() got an unexpected keyword argument 'is_local_vocoder'                                     \r\n```\r\nAnother issue I'm experiencing when using the new prepare_ljspeech script and trying to train using that dataset with char is the following, let me know if I should use a different tokenizer in this senario though.\r\n\r\n```\r\nTraceback (most recent call last):                                                                                      \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/train/train.py\", line 26, in main                                                \r\n    vocab_char_map, vocab_size = get_tokenizer(tokenizer_path, tokenizer)                                               \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/model/utils.py\", line 118, in get_tokenizer                                      \r\n    assert vocab_char_map[\" \"] == 0, \"make sure space is of idx 0 in vocab.txt, cuz 0 is used for unknown char\"         \r\nAssertionError: make sure space is of idx 0 in vocab.txt, cuz 0 is used for unknown char                                \r\n```\r\n\r\nThanks for all of your work on these models.\r\n\r\n\r\n### ✔️ Expected Behavior\r\n\r\nmodel training should proceed.\r\n\r\n### ❌ Actual Behavior\r\n\r\ntracebacks as above, which I assume are related to the new training code.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/545/comments",
    "author": "danielw97",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-28T04:37:54Z",
        "body": "Hi @danielw97\r\nThanks for reporting the bug!\r\nCheck if it works now"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T04:45:21Z",
        "body": "Thanks a lot for the quick fix!\r\nThat fixed my first issue with training, however am still having the second one with the prepare_ljspeech vocab.txt that I also mentioned.\r\nIf it's easier I can open another issue for that.\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-28T04:53:23Z",
        "body": "It would be fine with this issue, we will check for the LJSpeech vocab"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T04:55:01Z",
        "body": "Thanks, in my tests I'm using ljspeech 1.1\r\nIf there's any more info you need, let me know."
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-28T05:22:00Z",
        "body": "Hi @danielw97 \r\nthe second one is because the ljspeech has /n at end, which is not expected to include in the vocab\r\nfixed with adding strip() during processing\r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T05:24:25Z",
        "body": "Thanks for the quick resolution, and explanation of my second issue."
      }
    ]
  },
  {
    "number": 540,
    "title": "Can the 150M parameter small model handle multiple languages?",
    "created_at": "2024-11-27T13:57:58Z",
    "closed_at": "2024-12-13T07:46:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/540",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI am planning to train the 150M parameter small model and would like to know if it can handle multiple languages. Is it feasible to train this model for TTS tasks in two or more languages, or are there any limitations regarding multilingual support with this model size?\r\n\r\nIf anyone has experience or insights into training this model for multilingual tasks, your input would be greatly appreciated!\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/540/comments",
    "author": "hamees-sayed",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-29T08:19:22Z",
        "body": "Good question, maybe you can mix wenetspeech4tts premium and LibriTTS to test. The small model can perform well in each of them"
      }
    ]
  },
  {
    "number": 534,
    "title": "Adding support for F5-small models, sharing a Hindi checkpoint trained from scratch and adding NFE steps in inference scripts",
    "created_at": "2024-11-27T10:18:23Z",
    "closed_at": "2024-12-16T09:32:19Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/534",
    "body": "In this PR I am contributing three things as given in title.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/534/comments",
    "author": "rumourscape",
    "comments": [
      {
        "user": "thesandi99",
        "created_at": "2024-11-27T13:22:11Z",
        "body": "Oh hindi is nice how much hour you train model , becuas im train 60H and is not result like your's model and wht setting"
      },
      {
        "user": "rumourscape",
        "created_at": "2024-11-27T14:12:56Z",
        "body": "> Oh hindi is nice how much hour you train model , becuas im train 60H and is not result like your's model and wht setting\n\nI have trained the small config model with about 80 hrs of data for 2.5 million steps. \n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-27T14:50:24Z",
        "body": "Thanks a lot!\r\nWill check after #518 , which enables using yaml for better config"
      },
      {
        "user": "Aunali321",
        "created_at": "2024-11-28T03:51:24Z",
        "body": "@rumourscape Hi, Can you share training code for this model? "
      },
      {
        "user": "justinjohn0306",
        "created_at": "2024-11-28T12:59:46Z",
        "body": "I tested the model and it sounds trash "
      },
      {
        "user": "thesandi99",
        "created_at": "2024-11-28T13:07:19Z",
        "body": "> I tested the model and it sounds trash\r\n\r\nactually is better compare to  80H of  training"
      },
      {
        "user": "rumourscape",
        "created_at": "2024-11-28T13:40:08Z",
        "body": "> I tested the model and it sounds trash\r\n\r\nYou clearly haven't used it properly then. The ref_audio must be in Hindi and the ref_text & gen_text must be in Devanagari script. I have showed this model to many colleagues and they all believe that this is probably the current SOTA for Hindi TTS."
      },
      {
        "user": "rumourscape",
        "created_at": "2024-11-28T13:42:07Z",
        "body": "> @rumourscape Hi, Can you share training code for this model?\r\n\r\nI have used the same training script as given in the F5 repo. The only change I made was to skip the preprocess step and directly plug in the Huggingface datasets using the HFDataset class. \r\nPS: I also avoided the ```convert_char_to_pinyin``` function to prevent extra spaces in between characters."
      },
      {
        "user": "Aunali321",
        "created_at": "2024-11-28T14:59:45Z",
        "body": "> > @rumourscape Hi, Can you share training code for this model?\r\n> \r\n> I have used the same training script as given in the F5 repo. The only change I made was to skip the preprocess step and directly plug in the Huggingface datasets using the HFDataset class. PS: I also avoided the `convert_char_to_pinyin` function to prevent extra spaces in between characters.\r\n\r\nThat's a good change to add upstream. Would make it much easier to train on other languages."
      },
      {
        "user": "rumourscape",
        "created_at": "2024-12-09T09:21:49Z",
        "body": "@SWivid @ZhikangNiu Hi, are you looking into this?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-09T12:36:51Z",
        "body": "@rumourscape yes, but busy currently, will modified as mentioned in #591 \r\nwill come back and add this feature asap "
      },
      {
        "user": "sudeep333",
        "created_at": "2024-12-11T15:25:14Z",
        "body": "It would be great if you add Hindi language support. Thank you in advance."
      },
      {
        "user": "SWivid",
        "created_at": "2024-12-16T08:37:25Z",
        "body": "Hi @rumourscape , feel free to check the update\r\nThanks again for sharing and adding Hindi model~"
      },
      {
        "user": "rumourscape",
        "created_at": "2024-12-16T09:32:19Z",
        "body": "> Hi @rumourscape , feel free to check the update \r\n\r\nLooks good!"
      }
    ]
  },
  {
    "number": 529,
    "title": "Update infer_cli.py",
    "created_at": "2024-11-26T16:02:22Z",
    "closed_at": "2024-12-15T10:10:56Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/529",
    "body": "Save generated chunks in the output_dir folder; it helps with the multi-generation process.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/529/comments",
    "author": "thesandi99",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-26T16:10:20Z",
        "body": "1. was saving to different path then the final one, better same path; and better named with same prefix as final one with postfix {chunk_num}_{chunk_gen_text} e.g.\r\n2. not accurate annotation e.g. mkdir but annotated as save file\r\n3. better introduce with a flag to control whether save chunks\r\n\r\nas new feature introduced, also need\r\n1. properly update readme\r\n2. add script to merge chunk with same logic (e.g. fade-in fade out with factor to control and possible silence removal); could be add under scripts/ dir"
      },
      {
        "user": "thesandi99",
        "created_at": "2024-11-26T16:20:54Z",
        "body": "🫡"
      }
    ]
  },
  {
    "number": 525,
    "title": "How to use the API function",
    "created_at": "2024-11-26T02:41:43Z",
    "closed_at": "2024-11-26T02:43:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/525",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nThank you to the author, a great project.\r\nI have read the user manual, but it does not cover how to use the API. I hope you can add instructions on how to use the API. Thank you again",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/525/comments",
    "author": "Jandown",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-26T02:43:36Z",
        "body": "this issue better suit for a \"help wanted\" or \"Feature Request\" template, will close here~"
      }
    ]
  },
  {
    "number": 515,
    "title": "Protobuf downgrade required",
    "created_at": "2024-11-22T21:02:44Z",
    "closed_at": "2024-11-25T02:24:29Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/515",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nPop_OS! Deb. 22.04, CUDA 12.6, RTX 3040, Conda python 3.10, Jupyter Notebook VS Code. \n\n### Steps to Reproduce\n\nEverything as instructed for cloning editable repo. and running the CLI inference, including 15 second wav refrence audio with accurate transcript.\n\n### ✔️ Expected Behavior\n\nGet tests/infer_cli_out.wav of voice\n\n### ❌ Actual Behavior\n\nFirst protobu and several package dependency mismatch with it. \r\n-Downgraded to protobuf==3.19.6\r\n\r\nThen export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (As instructed, after several tries to find the right version) and got this \r\n____\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ngoogleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n____\r\n\r\nSo I tried googleapis-common-protos-1.56.0 and then found the needed version 1.56.2\r\n\r\nNot sure if anyone else needs help, and don't know where to goto ... post this... so here you go.\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/515/comments",
    "author": "WillRegister",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-23T06:18:48Z",
        "body": "```\r\n### Installation\r\n# Create a python 3.10 conda env (you could also use virtualenv)\r\nconda create -n f5-tts python=3.10\r\nconda activate f5-tts\r\n```\r\n\r\nmight help\r\nuse a separate env"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-25T02:24:27Z",
        "body": "Will close this issue, feel free to ask question."
      },
      {
        "user": "Vektor369",
        "created_at": "2024-12-08T12:12:20Z",
        "body": "Didn't work for me. I'm still getting \"ImportError: cannot import name 'builder' from 'google.protobuf.internal'\".\r\nI did the following:\r\n```\r\npip install protobuf==3.19.6\r\npip install googleapis-common-protos==1.56.0\r\n```"
      }
    ]
  },
  {
    "number": 507,
    "title": "Concerting checkpoints",
    "created_at": "2024-11-21T23:15:42Z",
    "closed_at": "2024-11-25T02:24:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/507",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n1. When I reduce my checkpoints, will f5 use these for further training? Does it need the .pt or will .safetensors also do?\r\nSince I plan to train an assortment of speakers, storage will be an issue for me eventually.\r\n2. Is there a difference between model_last and the latest checkpoint? They are not created at the same time with minutes apart.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/507/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-22T02:17:56Z",
        "body": "For Q2, you can load the checkpoint and check the keys, especially train_step"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-22T07:49:58Z",
        "body": "> When I reduce my checkpoints, will f5 use these for further training?\r\n\r\nyes, but start from step=0, and all optimizer scheduler states reset\r\n\r\n> Does it need the .pt or will .safetensors also do?\r\n\r\njust .pt is fine\r\n\r\n> Since I plan to train an assortment of speakers, storage will be an issue for me eventually.\r\n\r\nyou are using a mixed a dataset or train separate models? set a larger saving interval is fine\r\n\r\n> Is there a difference between model_last and the latest checkpoint?\r\n\r\nthe `model_last` is the lastest while the ckpt like `model_200000.pt` maybe not\r\nit is quite straightforward, that `last` is literally the last, 200000 is literally the ckpt for 200k step ckpt"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-25T02:24:43Z",
        "body": "Will close this issue, feel free to ask question."
      }
    ]
  },
  {
    "number": 506,
    "title": "Change download folder when generating audio in f5-tts_infer-gradio",
    "created_at": "2024-11-21T22:57:03Z",
    "closed_at": "2024-11-22T07:51:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/506",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI want to change the export path, preferably for f5-tts_infer-gradio and f5-tts_finetune-gradio at the same time.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/506/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-22T07:51:11Z",
        "body": "will close here since `enhancement` or `help wanted` template better suit"
      }
    ]
  },
  {
    "number": 503,
    "title": "feat: solve the problem of number in Chinese text",
    "created_at": "2024-11-21T12:18:56Z",
    "closed_at": "2024-12-15T02:46:54Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/503",
    "body": "This PR introduces several improvements to the text normalization and processing pipeline, ensuring a more robust conversion of mixed English and Chinese text to Pinyin with tone marks. The changes enhance text compatibility and address edge cases in the input data.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/503/comments",
    "author": "boomyao",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-21T12:26:15Z",
        "body": "> ensuring a more robust conversion of mixed English and Chinese text\r\n\r\nThe implementation seems to convert all numbers to Chinese text, what if code-switching numbers with English?"
      },
      {
        "user": "boomyao",
        "created_at": "2024-11-21T12:38:21Z",
        "body": "You are right, I will try to optimize it."
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-26T12:19:27Z",
        "body": "the simplest way is detect language -> zh -> use this function.\r\nYou'd better use this function in prepare dataset"
      }
    ]
  },
  {
    "number": 497,
    "title": "Huggingface Demo和本地部署合成的音频差距非常大",
    "created_at": "2024-11-20T07:12:40Z",
    "closed_at": "2024-11-20T07:46:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/497",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n我本地部署的是F5TTS_Base，除了之前有人提到的，合成的音频会夹杂输入音频中的某些短语；现在我发现了更差的情况就是不知道合成后的音频在说什么(输入的是中文音频，合成的也是中文，但是输出感觉在鬼扯)，这个现象在huggingface的demo上没出现过，不知道是不是模型的原因？使用F5TTS_Base_bigvgan会更好？我的音频都是16khz的",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/497/comments",
    "author": "Rex-Liu1027",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-20T07:46:14Z",
        "body": "hf demo same as local infer_gradio (both vocos as default)\r\n\r\nwill close this issue cuz\r\n> I confirm that I am using English to submit this report in order to facilitate communication."
      }
    ]
  },
  {
    "number": 496,
    "title": "How epoch is perfect for 16 hours of data",
    "created_at": "2024-11-19T18:10:24Z",
    "closed_at": "2024-11-22T07:31:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/496",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI can't understand the difference between epochs and steps. Which one is more important? Based on the number of hours of audio, how many steps are required to achieve a minimum quality output?\r\n\r\nFor example, if I have 16 or 200 hours of audio data, how many steps are needed to produce a perfect output?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/496/comments",
    "author": "Chiyan200",
    "comments": [
      {
        "user": "atlonxp",
        "created_at": "2024-11-19T19:52:06Z",
        "body": "### **Epochs and Training Time**  \r\nIn TTS training:\r\n- **1 Epoch**: Means the model has seen the entire training dataset once. If your dataset has 16 hours of speech, 1 epoch corresponds to the model processing all 16 hours of data.\r\n- **2 Epochs**: Means the model has seen the dataset twice, equivalent to 32 hours of data processed in training (assuming no data augmentation or changes).\r\n\r\nThis cumulative processing helps the model learn, but it doesn't guarantee convergence or high-quality results.\r\n\r\n### **Determining the Number of Epochs Needed**\r\n- **No Definite Answer**: It's true that there's no fixed number of epochs to produce a perfect output. This depends on:\r\n  - **Dataset Quality and Size**: Larger and higher-quality datasets often require more training epochs to generalize well.\r\n  - **Model Complexity**: More complex architectures might need more iterations to converge.\r\n  - **Learning Rate Scheduling**: An effective learning rate schedule can accelerate convergence.\r\n  - **Objective**: \"Perfect output\" depends on your quality criteria (e.g., MOS scores, intelligibility, naturalness).\r\n\r\nYou'll often need to monitor metrics like loss curves, validation results, and perceptual quality checks to determine when the model has sufficiently trained.\r\n\r\n### **Steps for High-Fidelity TTS**\r\n- It is common for TTS training to require **multiple epochs (5, 10, 20, or more)** to reach a point where generated voice quality is high fidelity.\r\n- However, models can also start overfitting if trained too long. Regularly evaluating generated samples is essential.\r\n- The exact number of steps or epochs depends on your goal:\r\n  - **Fine-Tuning**: Might take fewer epochs.\r\n  - **Training from Scratch**: Often requires more epochs, especially on large datasets.\r\n"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-20T13:17:14Z",
        "body": "> ### Checks\r\n> * [x]  This template is only for question, not feature requests or bug reports.\r\n> * [x]  I have thoroughly reviewed the project documentation and read the related paper(s).\r\n> * [x]  I have searched for existing issues, including closed ones, no similar questions.\r\n> * [x]  I confirm that I am using English to submit this report in order to facilitate communication.\r\n> \r\n> ### Question details\r\n> I can't understand the difference between epochs and steps. Which one is more important? Based on the number of hours of audio, how many steps are required to achieve a minimum quality output?\r\n> \r\n> For example, if I have 16 or 200 hours of audio data, how many steps are needed to produce a perfect output?\r\n\r\nAs LJSpeech, I think 20w step the quality will be better when batch is 38400 and train on 8 GPU"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-22T07:31:35Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 493,
    "title": "读数字有问题",
    "created_at": "2024-11-19T07:57:04Z",
    "closed_at": "2024-11-19T08:02:22Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/493",
    "body": "### Checks\n\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\n比如说一段文字里边带数字，在读数字的时候就会鬼畜\n\n### Steps to Reproduce\n\n一段文字里边带数字，在读数字的时候就会鬼畜\n\n### ✔️ Expected Behavior\n\n_No response_\n\n### ❌ Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/493/comments",
    "author": "martjay",
    "comments": [
      {
        "user": "martjay",
        "created_at": "2024-11-19T07:58:42Z",
        "body": "我发现它会把数字读为英文"
      },
      {
        "user": "martjay",
        "created_at": "2024-11-19T07:59:27Z",
        "body": "还有在长句子会莫名其妙的被分开读，语气很奇怪不流畅"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-19T08:02:23Z",
        "body": "@martjay #244 #390 关于数字\r\n\r\n关于长句子readme里有说明的，最长30s，过长的会被切开，如果想要理想的切分可以手动处理\r\n\r\n确认下这个issue模板里你勾选的内容：\r\n>  This template is only for bug reports, usage problems go with 'Help Wanted'. （这个issue应该不包含bug）\r\n I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.（关于长句子的readme有说明）\r\n I have searched for existing issues, including closed ones, and couldn't find a solution.（关于中文数字有不少issue）\r\n I confirm that I am using English to submit this report in order to facilitate communication.（希望用英文发issue便于不说中文的人寻找问题解决方案）"
      },
      {
        "user": "atlonxp",
        "created_at": "2024-11-19T08:30:33Z",
        "body": "@martjay would be good if write this issue in English. This might be of use to Thai that I'm working on as well. I might have this problem is reading number as well. "
      },
      {
        "user": "martjay",
        "created_at": "2024-11-19T09:37:04Z",
        "body": "> @martjay #244 #390 关于数字\r\n> \r\n> 关于长句子readme里有说明的，最长30s，过长的会被切开，如果想要理想的切分可以手动处理\r\n> \r\n> 确认下这个issue模板里你勾选的内容：\r\n> \r\n> > This template is only for bug reports, usage problems go with 'Help Wanted'. （这个issue应该不包含bug）\r\n> > I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.（关于长句子的readme有说明）\r\n> > I have searched for existing issues, including closed ones, and couldn't find a solution.（关于中文数字有不少issue）\r\n> > I confirm that I am using English to submit this report in order to facilitate communication.（希望用英文发issue便于不说中文的人寻找问题解决方案）\r\n\r\n我说的长句问题可能不是你描述的，可能这个小句连着二十个字，然后就会中间停一下继续念而不是连贯的念完整句，很不自然"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-19T09:48:26Z",
        "body": "> 我说的长句问题可能不是你描述的，可能这个小句连着二十个字，然后就会中间停一下继续念而不是连贯的念完整句，很不自然\r\n\r\n那应该是目前base模型训练集里没太见过这样的长句，会自行划短；考虑在二十几个字中想要小停顿的地方加逗号，或者speed slider速度拉快一点看看"
      },
      {
        "user": "martjay",
        "created_at": "2024-11-22T05:25:12Z",
        "body": "> > 我说的长句问题可能不是你描述的，可能这个小句连着二十个字，然后就会中间停一下继续念而不是连贯的念完整句，很不自然\r\n> \r\n> 那应该是目前base模型训练集里没太见过这样的长句，会自行划短；考虑在二十几个字中想要小停顿的地方加逗号，或者speed slider速度拉快一点看看\r\n\r\n我又测试了很多遍，发现就算不是长句，有时候也会在中间停顿。\r\n\r\n你可以试试下面这段：\r\n\r\n从天涯海角飘来两朵彩云，\r\n无人知晓究竟是来自何方。\r\n突然中止遨游，驻足天心，\r\n初四的月光下含情的对望，\r\n微光里依稀觉得昔日相识。\r\n记起绿色海岛，雾绕的山峦，\r\n黄昏的海滨一度过从甚密。\r\n面对面却怀天各一方的离愁，\r\n正欲交合，因乍遇又害羞。\r\n交汇的视线上高悬一弯新月，\r\n笑的羞涩妨碍亲吻的密切;\r\n春梦的绸缪将倦眼紧紧连结。\r\n叙罢韵事，蓦闻青曦的足音，\r\n无语作别，身带摩挲的温存。"
      },
      {
        "user": "martjay",
        "created_at": "2024-11-22T06:44:08Z",
        "body": "这种诗歌生成也很不自然。\r\n\r\n\r\n我情愿化成一片落叶，\r\n让风吹雨打到处飘零；\r\n或流云一朵，在澄蓝天，\r\n和大地再没有些牵连。\r\n\r\n但抱紧那伤心的标志，\r\n去触遇没着落的怅惘；\r\n在黄昏，夜班，蹑着脚走，\r\n全是空虚，再莫有温柔；\r\n\r\n忘掉曾有这世界；有你；\r\n哀悼谁又曾有过爱恋；\r\n落花似的落尽，忘了去\r\n这些个泪点里的情绪。\r\n\r\n到那天一切都不存留，\r\n比一闪光，一息风更少\r\n痕迹，你也要忘掉了我\r\n曾经在这世界里活过。"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-22T09:26:27Z",
        "body": "@martjay 感谢反馈~\r\n\r\n我测试了下\r\n\r\n> 初四的月光下含情的对望，\r\n\r\n这个会读成`初四的月光下 含情的对望，` 还行\r\n\r\n> 交汇的视线上高悬一弯新月，\r\n\r\n这个会读成`交汇的视线 上 高悬一弯新月，` 有问题\r\n\r\n因为模型没有显式音素强制对齐，模型是自行确定怎么念的，比如断句、腔调啥的\r\n确实还有很多问题，之后结构和训练的设计以及数据上迭代会努力改进这个~"
      },
      {
        "user": "martjay",
        "created_at": "2024-11-23T09:50:10Z",
        "body": "> @martjay 感谢反馈~\r\n> \r\n> 我测试了下\r\n> \r\n> > 初四的月光下含情的对望，\r\n> \r\n> 这个会读成`初四的月光下 含情的对望，` 还行\r\n> \r\n> > 交汇的视线上高悬一弯新月，\r\n> \r\n> 这个会读成`交汇的视线 上 高悬一弯新月，` 有问题\r\n> \r\n> 因为模型没有显式音素强制对齐，模型是自行确定怎么念的，比如断句、腔调啥的 确实还有很多问题，之后结构和训练的设计以及数据上迭代会努力改进这个~\r\n\r\n加油，如果能处理好这些问题，你们将会成为最好的AI克隆TTS"
      }
    ]
  },
  {
    "number": 492,
    "title": "long text generate bad result",
    "created_at": "2024-11-19T07:26:18Z",
    "closed_at": "2024-11-19T07:48:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/492",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nin my ubuntu cuda 4090 environment ,when i use a short gen_text , it generate a good result ;but when i use a long text ,>10 words ,it generate a very bad result ,why?the paper say long text also can generate a good result.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/492/comments",
    "author": "wwbnjsace",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-19T07:48:21Z",
        "body": "Hi @wwbnjsace , maybe it suits for a help wanted issue template?\r\nneed more info to figure out~\r\n\r\nwill close here, feel free to open with a help wanted template and provide more info with actual problem (e.g. the prompt text and audio, used gen_text prompt etc.)"
      }
    ]
  },
  {
    "number": 487,
    "title": "Fixes both issues from #480",
    "created_at": "2024-11-18T19:16:10Z",
    "closed_at": "2024-11-19T07:39:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/487",
    "body": "Wotking fix for path issues described in #480 (tested). Suggested solution before issue was closed did not correctly address the problems.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/487/comments",
    "author": "mame82",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-19T07:39:36Z",
        "body": "Hi @mame82 , many thanks!"
      }
    ]
  },
  {
    "number": 482,
    "title": "Update. Added print statement for debugging of transcription.",
    "created_at": "2024-11-17T23:26:29Z",
    "closed_at": "2024-11-19T12:18:15Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/482",
    "body": "I had an error indicating that that the installed version of Keras (Keras 3) is incompatible with the Transformers library used by Hugging Face. Transformers currently requires a backwards-compatible version of tf-keras instead of the latest Keras version.\n\nI fixed the issue with:\npip uninstall keras -y\npip install tf-keras\npip install tensorflow\npip show tensorflow transformers tf-keras",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/482/comments",
    "author": "realgermosen",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-18T05:00:03Z",
        "body": "> I had an error indicating that that the installed version of Keras (Keras 3) is incompatible with the Transformers library used by Hugging Face. Transformers currently requires a backwards-compatible version of tf-keras instead of the latest Keras version.\r\n> \r\n> I fixed the issue with: pip uninstall keras -y pip install tf-keras pip install tensorflow pip show tensorflow transformers tf-keras\r\n\r\nSeems off topic of PR and not related with this repo.\r\nAre you willing to solve the possible edge cases of transcription failures? We are expecting some helpful PR, thanks~"
      }
    ]
  },
  {
    "number": 481,
    "title": "Fine-tuning F5-TTS for Persian with 10k Hours of Data",
    "created_at": "2024-11-17T18:42:14Z",
    "closed_at": "2024-11-29T03:00:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/481",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI'm interested in fine-tuning your base model for Persian. I have a high-quality dataset of approximately 10,000 hours and access to substantial compute resources. I'd appreciate your guidance on the best approach and training strategy.\r\n\r\nSpecifically, I have the following questions:\r\n\r\nFine-tuning vs. Training from Scratch: Given my 10k-hour dataset, would fine-tuning the existing multilingual base model be more effective than training from scratch? My primary goal is high-quality Persian speech synthesis.\r\n\r\nFine-tuning Concerns: If fine-tuning is recommended, how well can I expect the model to capture the nuances of Persian pronunciation? Are there any specific aspects of Persian phonetics that might be challenging for the model to learn?\r\n\r\nFine-tuning Settings: Assuming fine-tuning is the best approach, what would be recommended settings for a 10k-hour dataset? I'm particularly interested in guidance on learning rate schedules, batch size, and other relevant hyperparameters.\r\n\r\nTraining from Scratch (If Necessary): If training from scratch is deemed necessary, could you please clarify the process? Can this be done through the fine-tuning Gradio demo, or is there a separate procedure? What would be suitable training settings in this scenario?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/481/comments",
    "author": "Mustaphajudi",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-18T05:31:22Z",
        "body": "Thanks for your interest~\r\n\r\n> Fine-tuning vs. Training from Scratch\r\n\r\nboth ok given 10khrs dataset, \r\n- if only need high-quality Persian & no vocab conflict with English, fine-tuning could leverage already learned speech-text alignment; if conflict, train from scratch\r\n- if wanna keep some English ability & no vocab conflict, finetune while mixing in some high-quality English data\r\n\r\n> Concerns: how well can expect the nuances of Persian pronunciation; Any specific aspects challenging to learn\r\n\r\nI dunno Persian thus sry I may not give useful info with possible challenge on its vocab, but it's sure chjar will work and g2p may have it easier for model to learn though need extra text frontend (and should be correct). Since you have 10khrs high-quality data, you could expect pretty well nuanced Persian pronunciation.\r\n\r\n> Settings for a 10k-hour dataset\r\n\r\nSame as our base model is fine for 10khrs, all details in our paper~\r\n\r\n> Training from Scratch script\r\n\r\nWe use the `train.py` while having not tested `finetune-xx` on large-scale corpus; \r\nSome possible issues #440 "
      },
      {
        "user": "Mustaphajudi",
        "created_at": "2024-11-18T12:00:19Z",
        "body": "> Thanks for your interest~\r\n> \r\n> > Fine-tuning vs. Training from Scratch\r\n> \r\n> both ok given 10khrs dataset,\r\n> \r\n> * if only need high-quality Persian & no vocab conflict with English, fine-tuning could leverage already learned speech-text alignment; if conflict, train from scratch\r\n> * if wanna keep some English ability & no vocab conflict, finetune while mixing in some high-quality English data\r\n> \r\n> > Concerns: how well can expect the nuances of Persian pronunciation; Any specific aspects challenging to learn\r\n> \r\n> I dunno Persian thus sry I may not give useful info with possible challenge on its vocab, but it's sure chjar will work and g2p may have it easier for model to learn though need extra text frontend (and should be correct). Since you have 10khrs high-quality data, you could expect pretty well nuanced Persian pronunciation.\r\n> \r\n> > Settings for a 10k-hour dataset\r\n> \r\n> Same as our base model is fine for 10khrs, all details in our paper~\r\n> \r\n> > Training from Scratch script\r\n> \r\n> We use the `train.py` while having not tested `finetune-xx` on large-scale corpus; Some possible issues #440\r\n\r\nThank you sir!\r\nOne last questions,i removed convert_char_to_pinyin function while training and inference Persian language since i will use Char and not pinyin,does it help improving the accuracy of pronunciation?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-18T12:10:26Z",
        "body": "pinyin and char is same for alphabets and so on, thus no influence on accuracy for language other than Mandarin. Maybe even speed up since less complicated text front end"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-29T03:00:51Z",
        "body": "will close this issue and if you have other question you can reopen this issue"
      }
    ]
  },
  {
    "number": 473,
    "title": "Removed redundant final chunk logic from socket_server.py",
    "created_at": "2024-11-16T04:42:56Z",
    "closed_at": "2024-11-18T08:25:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/473",
    "body": "generate_stream() was processing the final chunk twice, causing it to repeat the last word.  I removed the redundancy, but needed to add logic to handle an edge case that would occur if the full audio chunk was smaller than the chunk size.  I'm not sure if such an edge case is even possible but I threw it in just in case.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/473/comments",
    "author": "tjb4578",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-16T04:57:44Z",
        "body": "Hi @tjb4578 many thanks~\r\n\r\nThought probably will have edge case encountered in actual usage.\r\nSince there are many people using this, will see if someone is willing to test.\r\n\r\ncc @kunci115 @moseshu @amabilee @duanx123 @atlonxp @hjj-lmx\r\nThanks in advance!"
      }
    ]
  },
  {
    "number": 471,
    "title": "Moving storage, but f5 still load from old location",
    "created_at": "2024-11-15T23:48:34Z",
    "closed_at": "2024-11-16T02:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/471",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI cloned my f5 installation to a new drive (E:\\ -> Y:\\) and ran f5-tts_finetune-gradio from the new location.\r\nSeems the path for my data checkpoints is still mapped to E:\\. I need help to re-map, so I can delete the data on the source drive.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/471/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-16T01:36:20Z",
        "body": "`pip uninstall f5-tts`\r\nDo `pip install -e .` at new path."
      },
      {
        "user": "AlpacaManAlpha",
        "created_at": "2024-11-16T02:00:54Z",
        "body": "This worked, thank you!"
      }
    ]
  },
  {
    "number": 470,
    "title": "long text generate a bad result",
    "created_at": "2024-11-15T10:08:34Z",
    "closed_at": "2024-11-15T10:10:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/470",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\ni read the paper that the F5 can have a good and robustness result ;but when is use a long text, for example the text is \"In this paper, we introduce me,i will be a good boy\" ,the generated audio sounds very bad\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/470/comments",
    "author": "wwbnjsace",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-15T10:10:52Z",
        "body": "Hi @wwbnjsace , need some examples~\r\nFeel free to open with the `help wanted` template."
      }
    ]
  },
  {
    "number": 469,
    "title": "Allow for local path specification of HF models/repos",
    "created_at": "2024-11-15T09:49:01Z",
    "closed_at": "2024-11-15T10:22:06Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/469",
    "body": "Makes `local_path` more robust by downloading the models to path specified instead of needing to have the files already.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/469/comments",
    "author": "JarodMica",
    "comments": [
      {
        "user": "JarodMica",
        "created_at": "2024-11-15T10:11:10Z",
        "body": "Also added JA model to `SHARED.md` not related to PR"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-15T10:21:54Z",
        "body": "@JarodMica Thanks~\r\nThe logic of using `is_local` is to avoid failure with connection to hf, while snap_shot will still try confirming hf once (will modify this after merge)\r\n\r\nThanks again for JA model sharing!\r\nLooking forward to a final version"
      }
    ]
  },
  {
    "number": 467,
    "title": "Possible cast needed",
    "created_at": "2024-11-14T13:10:52Z",
    "closed_at": "2024-11-22T07:22:17Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/467",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nUbuntu LTS 24; Python 3.11.10\n\n### Steps to Reproduce\n\n1. Create a conda environment with Python 3.11.10\r\n2. install requirements via `pip install -e .`\r\n3. Download and unarchive Emilia dataset from EN-B000000 to EN-B000019 (so just the first 20 folders for English) and the first 10 from Chinese (ZH-B000000 to ZH-B000010). \r\n4. Run train/datasets/prepare_emmilia.py script to obtain the duration.json, raw.arrow and vocab.txt files (with a modification: account only for the included subset, not all Emilia)\r\n5. Replace in the dataset directory vocab.txt with the pretrained model's vocab.txt\r\n6. Run train/datasets/train.py with the default hyperparameters and modfiied:\r\n- batch_size_per_gpu = 384 (batch_size_type = \"frame\").\r\n- num_warmup_updates = 20 \r\n- save_per_updates = 50\r\n- last_per_steps = 50\n\n### ✔️ Expected Behavior\n\nTrain\n\n### ❌ Actual Behavior\n\nAfter 49 batches, I get `TypeError: can only concatenate str (not \"list\") to str` at ` text=[text_inputs[0] + [\" \"] + text_inputs[0]]` in src/train/train.py\r\n\r\nAfter inspecting the types, test_inputs[0] is a string and [\"\"] is a list. Because so many succeeded training I guess that test_inputs[0] shouldn't actually be a string but a list.\r\n\r\nIs this a problem with the dataset formatting? ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/467/comments",
    "author": "RaduBolbo",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-15T01:08:54Z",
        "body": "```python\r\ntext = [\" \".join([text_inputs[0], text_inputs[0]])]\r\n```"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-22T07:22:17Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 459,
    "title": "load finetune data",
    "created_at": "2024-11-13T09:26:17Z",
    "closed_at": "2024-11-13T11:59:53Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/459",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\ntrain on 4 a800\n\n### Steps to Reproduce\n\nI encountered an issue when trying to load my fine-tuned checkpoint. During the load_state_dict() call, I received the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 151, in <module>\r\n    infer_tts()\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 72, in infer_tts\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 216, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, dtype=dtype, use_ema=use_ema)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 168, in load_checkpoint\r\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\r\n  File \"/remote-home1/ycyuan/conda/anaconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n    Missing key(s) in state_dict: \"transformer.time_embed.time_mlp.0.weight\", \"transformer.time_embed.time_mlp.0.bias\", \"transformer.time_embed.time_mlp.2.weight\", \"transformer.time_embed.time_mlp.2.bias\", \"transformer.text_embed.text_embed.weight\", \"transformer.text_embed.text_blocks.0.dwconv.weight\", \"transformer.text_embed.text_blocks.0.dwconv.bias\", \"transformer.text_embed.text_blocks.0.norm.weight\", \"transformer.text_embed.text_blocks.0.norm.bias\", \"transformer.text_embed.text_blocks.0.pwconv1.weight\", \"transformer.text_embed.text_blocks.0.pwconv1.bias\", \"transformer.text_embed.text_blocks.0.grn.gamma\", \"transformer.text_embed.text_blocks.0.grn.beta\", \"transformer.text_embed.text_blocks.0.pwconv2.weight\", \"transformer.text_embed.text_blocks.0.pwconv2.bias\", \"transformer.text_embed.text_blocks.1.dwconv.weight\", \"transformer.text_embed.text_blocks.1.dwconv.bias\", \"transformer.text_embed.text_blocks.1.norm.weight\", \"transformer.text_embed.text_blocks.1.norm.bias\"...\r\n    Unexpected key(s) in state_dict: \"module.transformer.time_embed.time_mlp.0.weight\", \"module.transformer.time_embed.time_mlp.0.bias\", \"module.transformer.time_embed.time_mlp.2.weight\", \"module.transformer.time_embed.time_mlp.2.bias\", \"module.transformer.text_embed.text_embed.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.bias\", \"module.transformer.text_embed.text_blocks.0.norm.weight\", \"module.transformer.text_embed.text_blocks.0.norm.bias\", \"module.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"module.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"module.transformer.text_embed.text_blocks.0.grn.gamma\", \"module.transformer.text_embed.text_blocks.0.grn.beta\",...\r\n```\r\nHowever, when I print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected:\r\n```\r\nOrderedDict([('transformer.time_embed.time_mlp.0.weight', tensor([[-0.0007, -0.0009, -0.0007,  ..., -0.0355,  0.0119,  0.0061], ...], device='cuda:3')),\r\n            ('transformer.time_embed.time_mlp.0.bias', tensor([ 0.0139,  0.0171,  0.0399,  ..., -0.0508,  0.0258, -0.0191], device='cuda:3'))]),....\r\n```\r\nHowever, when I attempt to load the checkpoint without any wrapping, the model loading fails due to the error above. If I wrap the model with DataParallel before loading the checkpoint (like this: model = torch.nn.DataParallel(model)), the loading works fine, but this leads to subsequent errors during inference.\r\n\r\nAdditionally, I have tried to remove the module prefix using the following function:\r\n```\r\ndef remove_module_prefix(state_dict):\r\n    new_state_dict = {}\r\n    for k, v in state_dict.items():\r\n        # Remove the 'module.' prefix\r\n        print(k)\r\n        new_k = k.replace('module.', '') if 'module.' in k else k\r\n        new_state_dict[new_k] = v\r\n    return new_state_dict\r\n```\r\nHowever, this approach does not work because, when I print the keys, they don't actually contain the module. prefix. Despite this, when loading the checkpoint, I still encounter a mismatch error regarding the module. prefix.\r\n\r\nI have not modified the checkpoint saving function. Here’s the relevant code that I use to save checkpoints:\r\n```\r\ndef save_checkpoint(self, step, last=False):\r\n    self.accelerator.wait_for_everyone()\r\n    if self.is_main:\r\n        checkpoint = dict(\r\n            model_state_dict=self.accelerator.unwrap_model(self.model).state_dict(),\r\n            optimizer_state_dict=self.accelerator.unwrap_model(self.optimizer).state_dict(),\r\n            ema_model_state_dict=self.ema_model.state_dict(),\r\n            scheduler_state_dict=self.scheduler.state_dict(),\r\n            step=step,\r\n        )\r\n        if not os.path.exists(self.checkpoint_path):\r\n            os.makedirs(self.checkpoint_path)\r\n        rank = self.accelerator.process_index\r\n\r\n        if rank == 0:\r\n            if last:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_last.pt\")\r\n                print(f\"Saved last checkpoint at step {step}\")\r\n            else:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_{step}.pt\")\r\n```\r\n\r\nIs there a way to handle the mismatch of the module. prefix in the checkpoint file?\n\n### ✔️ Expected Behavior\n\n_No response_\n\n### ❌ Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/459/comments",
    "author": "southwindyong",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-13T09:33:23Z",
        "body": "maybe you could provide the additional `File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\"` for more info to figure out the issue."
      },
      {
        "user": "southwindyong",
        "created_at": "2024-11-13T10:31:33Z",
        "body": "This is the file of ``/remote-home1/ycyuan/TTS/F5-TTS/inference.py``\r\n```\r\nimport os\r\nimport random\r\nimport json\r\nimport codecs\r\nimport numpy as np\r\nimport soundfile as sf\r\nimport tomli\r\nfrom pathlib import Path\r\nfrom f5_tts.infer.utils_infer import (\r\n    infer_process,\r\n    load_model,\r\n    load_vocoder,\r\n    preprocess_ref_audio_text,\r\n    remove_silence_for_generated_wav,\r\n)\r\nfrom f5_tts.model import DiT, UNetT\r\nimport re\r\n\r\ndef infer_tts():\r\n    # Directly specify the variables without argparse\r\n    print(\"开始 TTS 推理过程...\")\r\n\r\n    config_path = Path(\"f5_tts\") / \"infer/examples/basic\" / \"basic.toml\"  \r\n    model = \"F5-TTS\"  # Choose \"F5-TTS\" or \"E2-TTS\"\r\n    ckpt_file = \"/remote-home1/ycyuan/TTS/F5-TTS/ckpts/F5TTS_Base/libritts_25s_1/model_last.pt\"  # Provide the path to checkpoint if necessary\r\n    vocab_file = \"\"  # Provide the vocab file path if necessary\r\n    text_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_text_2-6\"  # Text directory\r\n    audio_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_prompt_2_10s\"  # Audio directory\r\n    output_dir = \"./generate_data/libritts_25s_1\"  # Path to output folder\r\n    remove_silence = False  # Whether to remove silence from generated audio\r\n    vocoder_name = \"vocos\"  # Choose between \"vocos\" or \"bigvgan\"\r\n    load_vocoder_from_local = False  # Whether to load vocoder from local path\r\n    speed = 1.0  # Adjust audio generation speed (default is 1.0)\r\n\r\n    # Load configuration from the specified TOML file\r\n    print(f\"加载配置文件 {config_path}...\")\r\n    config = tomli.load(open(config_path, \"rb\"))\r\n    \r\n    # Use provided values or defaults from the config file\r\n    output_dir = output_dir if output_dir else config[\"output_dir\"]\r\n    model = model if model else config[\"model\"]\r\n    ckpt_file = ckpt_file if ckpt_file else \"\"\r\n    vocab_file = vocab_file if vocab_file else \"\"\r\n    remove_silence = remove_silence if remove_silence else config[\"remove_silence\"]\r\n    speed = speed\r\n    wave_path = Path(output_dir) / \"infer_cli_out.wav\"\r\n    \r\n    # Set vocoder local path based on the selected vocoder\r\n    print(f\"选择的 vocoder 模型: {vocoder_name}...\")\r\n    if vocoder_name == \"vocos\":\r\n        vocoder_local_path = \"../checkpoints/vocos-mel-24khz\"\r\n    elif vocoder_name == \"bigvgan\":\r\n        vocoder_local_path = \"../checkpoints/bigvgan_v2_24khz_100band_256x\"\r\n    \r\n    # Load the vocoder model\r\n    print(\"加载 vocoder 模型...\")\r\n    vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=load_vocoder_from_local, local_path=vocoder_local_path)\r\n\r\n    # Load the TTS model\r\n    print(f\"加载 TTS 模型: {model}...\")\r\n    if model == \"F5-TTS\":\r\n        model_cls = DiT\r\n        model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"F5-TTS\"\r\n            exp_name = \"F5TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    elif model == \"E2-TTS\":\r\n        model_cls = UNetT\r\n        model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"E2-TTS\"\r\n            exp_name = \"E2TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    \r\n    # Initialize the model\r\n    print(f\"使用 {model} 模型进行推理...\")\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n    print(\"模型加载成功。\")\r\n\r\n    # Load a random JSON file containing text\r\n    print(f\"从 {text_dir} 中加载随机 JSON 文件...\")\r\n    json_files = [f for f in os.listdir(text_dir) if f.endswith('.json')]\r\n    random_json_file = random.choice(json_files)\r\n    json_file_path = os.path.join(text_dir, random_json_file)\r\n\r\n    with open(json_file_path, 'r') as file:\r\n        json_data = json.load(file)\r\n        text = json_data[\"完整文本\"]\r\n        text_turns = json_data[\"文件数\"]\r\n\r\n    # From the audio directory, select a random folder and corresponding audio file\r\n    print(f\"从 {audio_dir} 中选择一个随机文件夹...\")\r\n    folders = [f for f in os.listdir(audio_dir) if os.path.isdir(os.path.join(audio_dir, f))]\r\n    while True:\r\n        random_folder = random.choice(folders)\r\n        folder_path = os.path.join(audio_dir, random_folder)\r\n        lab_files = [f for f in os.listdir(folder_path) if f.endswith('.lab')]\r\n        if not lab_files:\r\n            continue  # Skip folders without `.lab` files\r\n\r\n        random_lab_file = random.choice(lab_files)\r\n        lab_file_path = os.path.join(folder_path, random_lab_file)\r\n\r\n        # Read the corresponding `.lab` file to get prompt text\r\n        with open(lab_file_path, 'r') as lab_file:\r\n            prompt_text = lab_file.read().strip()\r\n        prompt_turns = len(prompt_text.splitlines())  # Number of lines = number of turns\r\n\r\n        if text_turns % 2 == prompt_turns % 2 and prompt_turns <= 4:  # Ensure matching turns and prompt_turns <= 4\r\n            break\r\n\r\n    # Choose the corresponding audio file\r\n    print(f\"选择的音频文件: {random_lab_file}...\")\r\n    audio_file = random_lab_file.replace('.lab', '.wav')\r\n    ref_audio = os.path.join(folder_path, audio_file)\r\n    if not os.path.exists(ref_audio):\r\n        raise FileNotFoundError(f\"未找到音频文件 {ref_audio}\")\r\n\r\n    # Call the inference process\r\n    def main_process(ref_audio, text, gen_text, model_obj, mel_spec_type, remove_silence, speed):\r\n        print(f\"正在生成音频: {text}\")\r\n        generated_audio_segments = []\r\n        reg1 = r\"(?=\\[\\w+\\])\"\r\n        chunks = re.split(reg1, gen_text)\r\n        reg2 = r\"\\[(\\w+)\\]\"\r\n\r\n        # Process each chunk of the generated text\r\n        for text_chunk in chunks:\r\n            if not text_chunk.strip():\r\n                continue\r\n            match = re.match(reg2, text_chunk)\r\n            if match:\r\n                voice = match[1]\r\n            else:\r\n                print(\"没有找到语音标签，使用默认语音。\")\r\n                voice = \"main\"\r\n            \r\n            print(f\"语音选择: {voice}\")\r\n            gen_text = text_chunk.strip()\r\n            audio, final_sample_rate, spectrogram = infer_process(\r\n                ref_audio, text, gen_text, model_obj, vocoder, mel_spec_type=mel_spec_type, speed=speed\r\n            )\r\n            generated_audio_segments.append(audio)\r\n\r\n        if generated_audio_segments:\r\n            final_wave = np.concatenate(generated_audio_segments)\r\n\r\n            if not os.path.exists(output_dir):\r\n                os.makedirs(output_dir)\r\n\r\n            with open(wave_path, \"wb\") as f:\r\n                sf.write(f.name, final_wave, final_sample_rate)\r\n                if remove_silence:\r\n                    print(\"正在去除生成音频中的静音部分...\")\r\n                    remove_silence_for_generated_wav(f.name)\r\n                print(f\"生成的音频已保存到 {f.name}\")\r\n\r\n    main_process(ref_audio, text, text, ema_model, vocoder_name, remove_silence, speed)\r\n\r\n# Call the infer_tts function to start the process\r\nif __name__ == \"__main__\":\r\n    infer_tts()\r\n```\r\nI think the text and prompt wav file is not important because it's all right"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-13T10:58:49Z",
        "body": "> print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected\r\n\r\nmaybe you have changed the code related with ema_model setup?\r\n\r\ntry do the removal of the module prefix using the function you got to `checkpoint['ema_model_state_dict']` see if works"
      },
      {
        "user": "southwindyong",
        "created_at": "2024-11-13T11:59:51Z",
        "body": "problem solved！ Thanks！\r\n"
      }
    ]
  },
  {
    "number": 458,
    "title": "Question : anybody got \"PytorchStreamWriter failed writing file data.pkl\" while training ?",
    "created_at": "2024-11-13T04:36:52Z",
    "closed_at": "2024-11-13T21:18:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/458",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nError look like this\r\n```\r\nEpoch 4/5:   0%|                 | 0/33 [00:01<?, ?step/s, loss=0.688, step=100]\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/opt/conda/envs/venv/lib/python3.10/site-packages/torch/serialization.py\", line 652, in save\r\n[rank0]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\r\n[rank0]:   File \"/opt/conda/envs/venv/lib/python3.10/site-packages/torch/serialization.py\", line 866, in _save\r\n[rank0]:     zip_file.write_record('data.pkl', data_value, len(data_value))\r\n[rank0]: RuntimeError: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data.pkl: file write failed\r\n```\r\n\r\nBefore was working fine just before finishing epoch ex target is 5 epoch it will slow down and maybe memory leak ? So I just canceled the training\r\n\r\nhere's the parameter of training I did :\r\n\r\n```\r\naccelerate launch --mixed_precision=fp16 src/f5_tts/train/finetune_cli.py --exp_name F5TTS_Base --learning_rate 7.5e-05 --batch_size_per_gpu 2742 --batch_size_type frame --max_samples 64 --grad_accumulation_steps 1 --max_grad_norm 1 --epochs 5 --num_warmup_updates 2 --save_per_updates 300 --last_per_steps 100 --dataset_name my_speak --tokenizer char --log_samples True --logger wandb\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/458/comments",
    "author": "x4080",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-13T05:06:45Z",
        "body": "check if have sufficient disk space"
      },
      {
        "user": "x4080",
        "created_at": "2024-11-13T20:52:15Z",
        "body": "@SWivid Yes, thats what I thought earlier but it have lot of space, the strange thing is it works before and running it to 80 epochs"
      },
      {
        "user": "x4080",
        "created_at": "2024-11-13T21:18:44Z",
        "body": "I retried it again the next day and the error is no more dont know what happened (its a rented gpu), I guess I just close it for now"
      }
    ]
  },
  {
    "number": 456,
    "title": "what happen to gradio.app file ? cant seem to run anymore ",
    "created_at": "2024-11-12T22:32:10Z",
    "closed_at": "2024-11-14T06:29:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/456",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nsuddenly my bat file stop running. its missing gradio.py",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/456/comments",
    "author": "jonnytracker",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-14T06:29:22Z",
        "body": "Try update the bat with new repo structure, which was updated week ago.\r\nJust easier to use.\r\n\r\nFree feel to open if further questions~"
      }
    ]
  },
  {
    "number": 438,
    "title": "After fine-tuning the original model with other languages, I can no longer speak Chinese and English properly when reasoning in both languages. However, my newly tuned language is becoming more and more similar. I want to keep both Chinese and English with my newly added language. Did I do something wrong?",
    "created_at": "2024-11-09T07:11:56Z",
    "closed_at": "2024-11-14T06:32:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/438",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nAfter fine-tuning the original model with other languages, I can no longer speak Chinese and English properly when reasoning in both languages. However, my newly tuned language is becoming more and more similar. I want to keep both Chinese and English with my newly added language. Did I do something wrong?\r\nBTW The audio I used is 16000Hz, and the tokenizer I selected is custom. I don't know if this has any impact.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/438/comments",
    "author": "0x1001u",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-09T09:29:14Z",
        "body": "@0x1001u You could keep some English and Chinese data while finetuning, do such a mixed training would help mitigate catastrophic forgetting problem"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-14T06:32:43Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 430,
    "title": "How to  customizable Voices?",
    "created_at": "2024-11-08T04:06:47Z",
    "closed_at": "2024-11-08T04:24:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/430",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nDoes it support some custom parameters, such as tone, pitch, softened voice, etc.?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/430/comments",
    "author": "ldgoooo",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-08T04:24:51Z",
        "body": "> I have thoroughly reviewed the project documentation and read the related paper(s).\r\n\r\ncould use ref_audio to control"
      }
    ]
  },
  {
    "number": 428,
    "title": "Finetune - exit status 3221225477",
    "created_at": "2024-11-07T17:50:37Z",
    "closed_at": "2024-11-15T11:51:57Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/428",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nHi,\r\n\r\ni try to get the training running. \r\nYesterday it was working fine, but I reinstalled after continue training wasnt working as expected.\r\n\r\nNow i cant get this thing running at all. I get until the Train Data Tab and there it wont work at all.\r\n\r\n\r\nThe code from CMD (Im running CMD as admin)\r\n\r\n```\r\nvocab :  2545\r\nUsing logger: None\r\nLoading dataset ...\r\nDownload Vocos from huggingface charactr/vocos-mel-24khz\r\n\r\nSorting with sampler... if slow, check whether dataset is provided with duration:   0%|          | 0/30863 [00:00<?, ?it/s]\r\nSorting with sampler... if slow, check whether dataset is provided with duration: 100%|##########| 30863/30863 [00:00<00:00, 2681043.11it/s]\r\n\r\nCreating dynamic batches with 2400 audio frames per gpu:   0%|          | 0/30863 [00:00<?, ?it/s]\r\nCreating dynamic batches with 2400 audio frames per gpu: 100%|##########| 30863/30863 [00:00<00:00, 3082481.35it/s]\r\n\r\nEpoch 1/370:   0%|          | 0/6140 [00:00<?, ?step/s]Traceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"C:\\F5\\F5-TTS\\venv\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\r\n  File \"C:\\F5\\F5-TTS\\venv\\Lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"C:\\F5\\F5-TTS\\venv\\Lib\\site-packages\\accelerate\\commands\\launch.py\", line 1168, in launch_command\r\n    simple_launcher(args)\r\n  File \"C:\\F5\\F5-TTS\\venv\\Lib\\site-packages\\accelerate\\commands\\launch.py\", line 763, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['C:\\\\F5\\\\F5-TTS\\\\venv\\\\Scripts\\\\python.exe', 'src/f5_tts/train/finetune_cli.py', '--exp_name', 'F5TTS_Base', '--learning_rate', '1e-05', '--batch_size_per_gpu', '2400', '--batch_size_type', 'frame', '--max_samples', '64', '--grad_accumulation_steps', '1', '--max_grad_norm', '1', '--epochs', '370', '--num_warmup_updates', '1544', '--save_per_updates', '10000', '--last_per_steps', '772', '--dataset_name', 'german_tts', '--finetune', 'True', '--tokenizer', 'pinyin', '--log_samples', 'True', '--logger', 'wandb']' returned non-zero exit status 3221225477.\r\naccelerate launch --mixed_precision=bf16 src/f5_tts/train/finetune_cli.py --exp_name F5TTS_Base --learning_rate 1e-05 --batch_size_per_gpu 1200 --batch_size_type frame --max_samples 64 --grad_accumulation_steps 1 --max_grad_norm 1 --epochs 370 --num_warmup_updates 1544 --save_per_updates 5000 --last_per_steps 772 --dataset_name german_tts --finetune True --tokenizer pinyin  --log_samples True --logger wandb\r\n```\n\n### Steps to Reproduce\n\n1. Start venv and finetune\r\n2. Transcribe, Vocab Check, Prepare Data\r\n3. Train Data -> Error\n\n### ✔️ Expected Behavior\n\n_No response_\n\n### ❌ Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/428/comments",
    "author": "fabianrossbach",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:13:01Z",
        "body": "no clear idea, seems with memory issue.\r\nmaybe check `accelerate config` first"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-15T11:51:57Z",
        "body": "will close as no more info received~"
      }
    ]
  },
  {
    "number": 413,
    "title": "Training the model from scratch, pronunciation is unintelligible",
    "created_at": "2024-11-07T03:18:13Z",
    "closed_at": "2024-11-15T11:54:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/413",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nUsing my own phone sequence, I trained the model from scratch, with about 200 hours of Chinese data and a 155M model. The synthesized speech is completely incomprehensible.\r\nHow much data is generally needed to train a model from scratch?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/413/comments",
    "author": "yygg678",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-07T05:20:36Z",
        "body": "> I have thoroughly reviewed the project documentation and read the related paper(s).\r\n\r\nAll details are given in our paper, including used training corpus for small model, batchsize, evaluation results from 400~800k updates.\r\nTrain with same batchsize to approx. 200K updates will hear something intelligible."
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-15T11:54:34Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 412,
    "title": "加载西班牙和日语模型的时候，报错：RuntimeError: Error(s) in loading state_dict for CFM:",
    "created_at": "2024-11-07T02:20:30Z",
    "closed_at": "2024-11-07T05:19:54Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/412",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n加载Discussions中提供的训练好的日语和西班牙语模型的时候报错：\r\n日语报错：\r\nmodel :  /root/F5-TTS/ckpts/.../model_1108224.pt\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\n西班牙语报错：\r\nmodel :  /root/F5-TTS/ckpts/jpgallegoar/model_1200000.safetensors\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        Missing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\". ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/412/comments",
    "author": "liuhui881125",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-07T05:18:36Z",
        "body": "> RuntimeError: Error(s) in loading state_dict for CFM:\r\n> size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\nUse corresponding vocab.txt\r\n\r\n> RuntimeError: Error(s) in loading state_dict for CFM:\r\nMissing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\".\r\n\r\npull latest repo commit"
      },
      {
        "user": "liuhui881125",
        "created_at": "2024-11-07T05:19:54Z",
        "body": "ok.fix .good"
      }
    ]
  },
  {
    "number": 408,
    "title": "如何自定义分词停顿，目前好像不支持，导致有的停顿位置不合适",
    "created_at": "2024-11-06T09:32:04Z",
    "closed_at": "2024-11-08T10:19:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/408",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n请问应该如何自定义分词，好像用空格+分词+空格方式似乎没有效果，我在convert_char_to_pinyin方法中，用jieba.add_word(words, freq=None, tag=None)似乎也没有起到效果。",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/408/comments",
    "author": "japleak",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:19:11Z",
        "body": "> using English to submit this report in order to facilitate communication.\r\n\r\ntry add comma e.g. to introduce pause explicitly\r\n\r\nwill close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 407,
    "title": "ModuleNotFoundError: No module named 'f5_tts'",
    "created_at": "2024-11-06T07:08:54Z",
    "closed_at": "2024-11-06T07:17:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/407",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nGreat project, thank you to the author.\r\nI encountered an error while running my reasoning. May I ask what the problem is?\r\nDeploy in Windows system environment",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/407/comments",
    "author": "Jandown",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-06T07:17:03Z",
        "body": "> I have thoroughly reviewed the project documentation and read the related paper(s).\r\n\r\nFollow the installation in README.md first."
      }
    ]
  },
  {
    "number": 406,
    "title": "Questions about base models' training time, loss, etc.",
    "created_at": "2024-11-06T03:38:47Z",
    "closed_at": "2024-11-08T10:19:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/406",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nhello, 非常棒的工作！我在基础模型上进行了一些模型结构的调整，并在Emilia数据集上重新开始训练底模，想问下，loss到多少的时候模型开始可以正常发声呢？你们在训练开源的这个底模的时候，用了多少机器和时间能达到正常发声呢？多少时间能到收敛呢？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/406/comments",
    "author": "ruby11dog",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-06T04:11:49Z",
        "body": "Hi @ruby11dog , we could `use English to submit this report in order to facilitate communication.` (Checks 4.)\r\n\r\nLoss is not significant to see how training process goes as pred and gt boundries are mismatched. #9 \r\n\r\nWe have posed all results and detailes of training and evaluation in our paper. For base model 8*A100 80G over one week to reach 1.2M updates, 200~400k to hear some aligned speech (say, intelligible)"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:19:34Z",
        "body": "using English to submit this report in order to facilitate communication."
      }
    ]
  },
  {
    "number": 403,
    "title": "Improve inference speed on CPU while keeping flow adherence and accuracy",
    "created_at": "2024-11-05T15:16:58Z",
    "closed_at": "2024-11-22T07:24:21Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/403",
    "body": "### Checks\r\n\r\n- [X] This template is only for feature request.\r\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\r\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\r\n\r\nI was trying to create speech on CPU for a finetuned and reduced safetensors model, but encountered very slow generation: 7 minutes and 40 seconds for a sentence with 8 words with 40 literals. This was frustrating because my goal is to use it as a replacement for coqui-ai/TTS / TTSv2 but with that speed, it's hopeless.\r\nCoqui-ai/TTS / TTSv2 is fast in cloning and generation but hallucinates every single time and they are unable to fix it, which made me switch to F5-TTS.\r\n\r\n### 2. What is your suggested solution?\r\n\r\nIncrease generation speed to be competitive in speed with coqui-ai/TTS but make sure to not sacrifice on flow and accuracy.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/403/comments",
    "author": "ErfolgreichCharismatisch",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-22T07:24:21Z",
        "body": "will close this issue, feel free to open if further questions\r\n\r\nwill definitely consider efficiency problem in future plan"
      }
    ]
  },
  {
    "number": 402,
    "title": "Inference with socket example have unexpected high speaking rate",
    "created_at": "2024-11-05T12:23:25Z",
    "closed_at": "2024-11-26T02:21:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/402",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nfollow the script python socket_server.py.\r\n使用下面的代码去调用，生成的audio语速很快，和离线的效果差距很大，是配置的问题，还是其他问题？\r\n```python\r\nimport socket\r\nimport numpy as np\r\nimport asyncio\r\nimport pyaudio\r\n\r\nasync def listen_to_voice(text, server_ip='localhost', server_port=7777):\r\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    client_socket.connect((server_ip, server_port))\r\n\r\n    async def play_audio_stream():\r\n        buffer = b''\r\n        p = pyaudio.PyAudio()\r\n        stream = p.open(format=pyaudio.paFloat32,\r\n                        channels=1,\r\n                        rate=24000,  # Ensure this matches the server's sampling rate\r\n                        output=True,\r\n                        frames_per_buffer=2048)\r\n\r\n        try:\r\n            while True:\r\n                chunk = await asyncio.to_thread(client_socket.recv, 1024)\r\n                if not chunk:  # End of stream\r\n                    break\r\n                if b\"END_OF_AUDIO\" in chunk:\r\n                    buffer += chunk.replace(b\"END_OF_AUDIO\", b\"\")\r\n                    if buffer:\r\n                        audio_array = np.frombuffer(buffer, dtype=np.float32).copy()  # Make a writable copy\r\n                        stream.write(audio_array.tobytes())\r\n                    break\r\n                buffer += chunk\r\n                if len(buffer) >= 4096:\r\n                    audio_array = np.frombuffer(buffer[:4096], dtype=np.float32).copy()  # Make a writable copy\r\n                    stream.write(audio_array.tobytes())\r\n                    buffer = buffer[4096:]\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n            p.terminate()\r\n\r\n    try:\r\n        # Send only the text to the server\r\n        await asyncio.to_thread(client_socket.sendall, text.encode('utf-8'))\r\n        await play_audio_stream()\r\n        print(\"Audio playback finished.\")\r\n    except Exception as e:\r\n        print(f\"Error in listen_to_voice: {e}\")\r\n    finally:\r\n        client_socket.close()\r\n\r\n# Example usage: Replace this with your actual server IP and port\r\nasync def main():\r\n    await listen_to_voice(\"春天的江潮水势浩荡，与大海连成一片，一轮明月从海上升起，好像与潮水一起涌出来。月光照耀着春江，随着波浪闪耀千万里，所有地方的春江都有明亮的月光！江水曲曲折折地绕着花草丛生的原野流淌，月光照射着开遍鲜花的树林好像细密的雪珠在闪烁。月色如霜，所以霜飞无从觉察，洲上的白沙和月色融合在一起，看不分明。江水、天空成一色，没有一点微小灰尘，明亮的天空中只有一轮孤月高悬空中。江边上什么人最初看见月亮？江上的月亮哪一年最初照耀着人？人生一代代地无穷无尽，只有江上的月亮一年年地总是相像。不知江上的月亮等待着什么人，只见长江不断地一直运输着流水。游子像一片白云缓缓地离去，只剩下思妇站在离别的青枫浦不胜忧愁。哪家的游子今晚坐着小船在漂流？\", server_ip='localhost', server_port=32023)\r\n\r\n# Run the main async function\r\nif __name__ == '__main__':\r\n\r\n    import nest_asyncio\r\n    nest_asyncio.apply()\r\n    asyncio.run(main())\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/402/comments",
    "author": "moseshu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-05T12:30:32Z",
        "body": "> using English to submit this report in order to facilitate communication.\r\n\\\r\n使用下面的代码去调用，生成的audio语速很快，和离线的效果差距很大，是配置的问题，还是其他问题？\r\nUsing the following code to call, the generated audio speech speed is very fast, and the effect of offline is very large, is the problem of configuration, or other problems?\r\n\r\nWe haven't closely check this, may be with sampling rate and stuff.\r\n@kunci115 might help"
      },
      {
        "user": "amabilee",
        "created_at": "2024-11-05T12:35:07Z",
        "body": "It can be:\r\nThat the sampling rate and audio format used by both the server and client aren't consistent.\r\nor\r\nThe buffer size might be too small, causing the audio to be played faster than intended"
      },
      {
        "user": "kunci115",
        "created_at": "2024-11-05T13:24:57Z",
        "body": "I've just debug in short time, so far from what i checked, it only happen with ch language text >50 token, make it short for immediate solution, since I don't really understand CH preprocessing\r\n\r\n```\r\nimport socket\r\nimport numpy as np\r\nimport asyncio\r\nimport pyaudio\r\nimport re\r\n\r\ndef chunk_text(text, max_length=50):\r\n    \"\"\"\r\n    Splits the input text into smaller chunks based on punctuation and length.\r\n    Adjust max_length to control chunk size.\r\n    \"\"\"\r\n    sentences = re.split(r'(?<=[。！？])', text)  # Adjust for Chinese punctuation\r\n    chunks = []\r\n    current_chunk = \"\"\r\n\r\n    for sentence in sentences:\r\n        if len(current_chunk) + len(sentence) > max_length:\r\n            chunks.append(current_chunk)\r\n            current_chunk = sentence\r\n        else:\r\n            current_chunk += sentence\r\n\r\n    if current_chunk:\r\n        chunks.append(current_chunk)\r\n    \r\n    return chunks\r\n\r\nasync def listen_to_voice(text, server_ip='localhost', server_port=9998):\r\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    client_socket.connect((server_ip, server_port))\r\n\r\n    async def play_audio_stream():\r\n        buffer = b''\r\n        p = pyaudio.PyAudio()\r\n        stream = p.open(format=pyaudio.paFloat32,\r\n                        channels=1,\r\n                        rate=24000,\r\n                        output=True,\r\n                        frames_per_buffer=2048)\r\n\r\n        try:\r\n            while True:\r\n                chunk = await asyncio.get_event_loop().run_in_executor(None, client_socket.recv, 1024)\r\n                if not chunk:  # End of stream\r\n                    break\r\n                if b\"END_OF_AUDIO\" in chunk:\r\n                    buffer += chunk.replace(b\"END_OF_AUDIO\", b\"\")\r\n                    if buffer:\r\n                        audio_array = np.frombuffer(buffer, dtype=np.float32).copy()\r\n                        stream.write(audio_array.tobytes())\r\n                    break\r\n                buffer += chunk\r\n                if len(buffer) >= 4096:\r\n                    audio_array = np.frombuffer(buffer[:4096], dtype=np.float32).copy()\r\n                    stream.write(audio_array.tobytes())\r\n                    buffer = buffer[4096:]\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n            p.terminate()\r\n\r\n    try:\r\n        # Split text into chunks\r\n        text_chunks = chunk_text(text)\r\n        \r\n        # Send each chunk, waiting for playback to finish before proceeding\r\n        for chunk in text_chunks:\r\n            await asyncio.get_event_loop().run_in_executor(None, client_socket.sendall, chunk.encode('utf-8'))\r\n            await play_audio_stream()  # Play the current chunk fully before sending the next\r\n            print(f\"Finished playing chunk: {chunk}\")\r\n\r\n        print(\"Audio playback finished.\")\r\n\r\n    except Exception as e:\r\n        print(f\"Error in listen_to_voice: {e}\")\r\n\r\n    finally:\r\n        client_socket.close()\r\n\r\n# Example usage\r\nasync def main():\r\n    await listen_to_voice(\r\n        \"春天的江潮水势浩荡，与大海连成一片，一轮明月从海上升起，好像与潮水一起涌出来。月光照耀着春江，随着波浪闪耀千万里，所有地方的春江都有明亮的月光！\"\r\n        \"江水曲曲折折地绕着花草丛生的原野流淌，月光照射着开遍鲜花的树林好像细密的雪珠在闪烁。月色如霜，所以霜飞无从觉察，洲上的白沙和月色融合在一起，看不分明。\"\r\n        \"江水、天空成一色，没有一点微小灰尘，明亮的天空中只有一轮孤月高悬空中。江边上什么人最初看见月亮？江上的月亮哪一年最初照耀着人？人生一代代地无穷无尽，只有江上的月亮一年年地总是相像。\"\r\n        \"不知江上的月亮等待着什么人，只见长江不断地一直运输着流水。游子像一片白云缓缓地离去，只剩下思妇站在离别的青枫浦不胜忧愁。哪家的游子今晚坐着小船在漂流？\",\r\n        server_ip='localhost', server_port=9998\r\n    )\r\n\r\n# Run the main async function\r\nasyncio.run(main())\r\n```"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-26T02:21:39Z",
        "body": "Since this issue has been inactive for a long time, it will be closed. Feel free to reopen this issue and ask questions at any time."
      }
    ]
  },
  {
    "number": 401,
    "title": "Add --bnb_optimizer argument to CLI and pass it to Trainer initialization",
    "created_at": "2024-11-05T12:11:38Z",
    "closed_at": "2024-11-05T12:20:32Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/401",
    "body": "Add `--bnb_optimizer` argument to CLI and pass it to Trainer initialization.\n\n* Add `--bnb_optimizer` argument to `parse_args()` function in `src/f5_tts/train/finetune_cli.py`.\n* Pass `bnb_optimizer` argument to `Trainer` initialization in the `main()` function of `src/f5_tts/train/finetune_cli.py`.\n\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/401/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-05T12:19:23Z",
        "body": "Hi @hcsolakoglu , we added 8bit-Adam yet not tried.\r\nWould you like to share some test results (e.g. the mem usage and performance) ?"
      }
    ]
  },
  {
    "number": 399,
    "title": "Why is Gradient Checkpointing Not Implemented in Training?",
    "created_at": "2024-11-05T06:31:30Z",
    "closed_at": "2024-12-16T08:48:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/399",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nIt appears that gradient checkpointing is not implemented in the current training pipeline. Gradient checkpointing can significantly reduce memory usage by trading off computation, making it valuable for large models and resource-limited environments. This raises the question:\r\n\r\nIs there a specific reason for not implementing gradient checkpointing?\r\nIf possible, could it be integrated in future updates, or are there known limitations that prevent its integration? If there is no compatibility issue, I would be open to exploring the possibility of adding it via a PR.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/399/comments",
    "author": "kostum123",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-05T06:34:21Z",
        "body": "Yeah, I think you can explore the gradient checkpointing in F5 and add it via a PR."
      }
    ]
  },
  {
    "number": 392,
    "title": "Keep top_k checkpoint",
    "created_at": "2024-11-04T13:19:59Z",
    "closed_at": "2024-11-04T13:31:35Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/392",
    "body": "Keep the top k checkpoints for those with limited disk space",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/392/comments",
    "author": "HuuHuy227",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-04T13:24:04Z",
        "body": "Hi @HuuHuy227 , if just want to save disk space, could simply set `save_per_updates=10000000000`\r\nthe intermediate ckpts is mainly for evaluation usage and backup"
      },
      {
        "user": "HuuHuy227",
        "created_at": "2024-11-04T13:31:35Z",
        "body": "Oh, I see. I thought that keeping the top_k checkpoints would make evaluation and checkpoint backups easier when using a small save_per_updates interval, especially with limited disk space"
      }
    ]
  },
  {
    "number": 390,
    "title": "Chinese Number Pronunciation",
    "created_at": "2024-11-04T09:28:04Z",
    "closed_at": "2024-11-04T09:39:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/390",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nIn a Chinese sentence, all numbers are pronounced in English. \r\nCan it be arranged so that the entire output is in Chinese instead?\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/390/comments",
    "author": "skyboooox",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-04T09:39:49Z",
        "body": "#138 #244 etc.\r\nTurn Chinese number to words."
      }
    ]
  },
  {
    "number": 385,
    "title": "Hi, how can I specify the download directory for the model ",
    "created_at": "2024-11-04T00:31:53Z",
    "closed_at": "2024-11-08T10:23:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/385",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nHi, how can I specify the download directory for the model ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/385/comments",
    "author": "zhaojigang",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-04T02:20:11Z",
        "body": "huggingface-cli download --resume-download SWivid/F5-TTS --local-dir <YOUR PATH>"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:23:17Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 384,
    "title": "Why is the new duration predictor model not being merged in this repository?",
    "created_at": "2024-11-03T20:03:48Z",
    "closed_at": "2024-11-08T10:26:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/384",
    "body": "### Checks\n\n- [x] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI saw there's a new duration predictor that is supposedly better than the current one, why is it not merged here?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/384/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-04T04:26:38Z",
        "body": "Discussions at #363 \r\nIt's generally having output speech read faster, and other considerations mentioned there.\r\nIf interested, could also help with organizing modules.py in JM's fork repo which is helpful for those need a duration predictor"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:26:46Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 381,
    "title": "ValueError: Unknown scheme for proxy URL URL('socks://127.0.0.1:1089/')",
    "created_at": "2024-11-03T11:43:13Z",
    "closed_at": "2024-11-03T11:59:51Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/381",
    "body": "\r\n(f5)$ f5-tts_infer-gradio \r\nTraceback (most recent call last):\r\n  File \"/home/jk/miniconda3/envs/f5/bin/f5-tts_infer-gradio\", line 5, in <module>\r\n    from f5_tts.infer.infer_gradio import main\r\n  File \"/media/jk/000BCAF400087F74/F5-TTS/F5-TTS/src/f5_tts/infer/infer_gradio.py\", line 8, in <module>\r\n    import gradio as gr\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/__init__.py\", line 3, in <module>\r\n    import gradio._simple_templates\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/_simple_templates/__init__.py\", line 1, in <module>\r\n    from .simpledropdown import SimpleDropdown\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/_simple_templates/simpledropdown.py\", line 6, in <module>\r\n    from gradio.components.base import Component, FormComponent\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/components/__init__.py\", line 1, in <module>\r\n    from gradio.components.annotated_image import AnnotatedImage\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/components/annotated_image.py\", line 13, in <module>\r\n    from gradio import processing_utils, utils\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/gradio/processing_utils.py\", line 99, in <module>\r\n    sync_client = httpx.Client(transport=sync_transport)\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_client.py\", line 693, in __init__\r\n    proxy_map = self._get_proxy_map(proxies or proxy, allow_env_proxies)\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_client.py\", line 218, in _get_proxy_map\r\n    return {\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_client.py\", line 219, in <dictcomp>\r\n    key: None if url is None else Proxy(url=url)\r\n  File \"/home/jk/miniconda3/envs/f5/lib/python3.10/site-packages/httpx/_config.py\", line 338, in __init__\r\n    raise ValueError(f\"Unknown scheme for proxy URL {url!r}\")\r\nValueError: Unknown scheme for proxy URL URL('socks://127.0.0.1:1089/')\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/381/comments",
    "author": "jakeytan",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-03T11:50:40Z",
        "body": "are you using system proxy?"
      },
      {
        "user": "jakeytan",
        "created_at": "2024-11-03T11:56:13Z",
        "body": "> are you using system proxy?\r\n\r\nyes ! qv2ray\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T11:58:14Z",
        "body": "so it's a network problem, try using a proxy that will not take over all system (e.g. not using tunnel mode)\r\nand if you could connect to hf directly, not using proxy."
      },
      {
        "user": "jakeytan",
        "created_at": "2024-11-03T11:59:19Z",
        "body": "ok,it is going on already, thanks a lot !\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T11:59:51Z",
        "body": "ok, that's great~"
      },
      {
        "user": "jakeytan",
        "created_at": "2024-11-03T11:59:59Z",
        "body": "huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T12:01:22Z",
        "body": "> huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\r\n\r\nyeh, so cannot connect directly, then try using a proxy that will not take over all system (e.g. not using tunnel mode)\r\nor you can manually download the ckpt files, and following instruction in readme to load the local ckpt file."
      }
    ]
  },
  {
    "number": 372,
    "title": "Batched inference with multiple texts at once",
    "created_at": "2024-11-02T01:47:35Z",
    "closed_at": "2024-11-08T10:16:45Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/372",
    "body": "The current inference script only supports generating one audio per pass, which is slow and inefficient for massive speech generation. However, the evaluation script supports batched inference where a list of text is passed to the model and the inference  process is split among multiple GPUs. It would be nice if the inference script supported batched inference across multiple GPUs  too, just like the evaluation script.\r\n\r\nI wonder if there are any plans to implement this? Thanks for the great work.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/372/comments",
    "author": "singularity-s0",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-02T05:20:51Z",
        "body": "Hi @singularity-s0 , batch inference with ddp generally needs more gpu thus serves more as an advanced operation. In this case, there would be many complicated and different scenarios of batch inference compared to single inference (so we just pose it for eval for academic usage). It is very welcome some PR for it."
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:16:45Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 368,
    "title": "trained voice playback gui",
    "created_at": "2024-11-01T20:04:19Z",
    "closed_at": "2024-11-08T10:16:28Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/368",
    "body": "Hello\r\n\r\nI finetuned a voice using the finetune_gradio.py succesfully, now how am im using it as the model with the infer_gradio normal gui and not only in test tab of the finetune gui?\r\n\r\nis it a way to load the trained checkpoint with the normal infer_gradio gui? also why do i need a voice reference if i already trained a model which i want that specific voice?\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/368/comments",
    "author": "dpp-user",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-11-01T20:11:20Z",
        "body": "An easy way to do it is to launch the app and look for this in the terminal:\r\n\r\n\"\r\nvocab :  D:\\AI\\F5-TTS\\src\\f5_tts\\infer\\examples\\vocab.txt\r\ntokenizer :  custom\r\nmodel :  C:\\Users\\thega\\.cache\\huggingface\\hub\\models--SWivid--F5-TTS\\snapshots\\995ff41929c08ff968786b448a384330438b5cb6\\F5TTS_Base\\model_1200000.safetensors\r\n\"\r\n\r\nnow go to that model path and put your new model with the name \"model_1200000.safetensors\". Make sure to reduce the model selecting safetensors type in the finetune_gradio app"
      },
      {
        "user": "dpp-user",
        "created_at": "2024-11-01T21:54:18Z",
        "body": "> An easy way to do it is to launch the app and look for this in the terminal:\r\n> \r\n> \" vocab : D:\\AI\\F5-TTS\\src\\f5_tts\\infer\\examples\\vocab.txt tokenizer : custom model : C:\\Users\\thega.cache\\huggingface\\hub\\models--SWivid--F5-TTS\\snapshots\\995ff41929c08ff968786b448a384330438b5cb6\\F5TTS_Base\\model_1200000.safetensors \"\r\n> \r\n> now go to that model path and put your new model with the name \"model_1200000.safetensors\". Make sure to reduce the model selecting safetensors type in the finetune_gradio app\r\n\r\nThanks i will try it. it would be awesome if we could get an option on the gui to switch models on the fly like in rvc. also i really \r\nliked the chat option if only we could use other models than the qwen. \r\n\r\nf5-tts is really good has a great potential if it could integrate with other platforms"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-11-01T23:39:03Z",
        "body": "I'm planning on implementing something like that when the open source community releases some finetunes. I'm doing spanish, which did you do?\n\nAbout Chat, which model would you like to use? I also thought about making the model swappable"
      },
      {
        "user": "dpp-user",
        "created_at": "2024-11-01T23:56:34Z",
        "body": "i did some english finetune not other lang\r\ni thought a more powerful models like lllama 3.2 and more uncensored models as well. "
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-11-01T23:58:57Z",
        "body": "oh nice. I mean if you want you can change what model you want in the code it's really easy. it's not trivial implementing the dropdown because for example huggingface spaces can't do that (they have a single instance for many users using the same model)"
      },
      {
        "user": "dpp-user",
        "created_at": "2024-11-02T00:18:54Z",
        "body": "i tried but i couldnt make it to work. i even tried to load it from my local pc and not hf and still didnt work"
      },
      {
        "user": "dpp-user",
        "created_at": "2024-11-04T07:44:00Z",
        "body": "Anyone knows how to swap the model for the chat section? like use other models and not the qwen"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-11-04T08:37:53Z",
        "body": "> Anyone knows how to swap the model for the chat section? like use other models and not the qwen\n\nyou can change it in the code, where it loads Qwen in infer_gradio.py, you can put any huggingface LLM."
      },
      {
        "user": "dpp-user",
        "created_at": "2024-11-04T09:47:58Z",
        "body": "> > Anyone knows how to swap the model for the chat section? like use other models and not the qwen\r\n> \r\n> you can change it in the code, where it loads Qwen in infer_gradio.py, you can put any huggingface LLM.\r\n\r\ni couldnt do it, i tried using chatgpt but i get an error. can help with what to change and how? like example\r\n"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-11-04T09:49:19Z",
        "body": "> > > Anyone knows how to swap the model for the chat section? like use other models and not the qwen\n> \n> > \n> \n> > you can change it in the code, where it loads Qwen in infer_gradio.py, you can put any huggingface LLM.\n> \n> \n> \n> i couldnt do it, i tried using chatgpt but i get an error. can help with what to change and how? like example\n> \n> \n\nWell ChatGPT isn't in huggingface obviously. You'd need to integrate the API yourself, that's currently out of scope. "
      },
      {
        "user": "dpp-user",
        "created_at": "2024-11-04T10:06:28Z",
        "body": "> > > > Anyone knows how to swap the model for the chat section? like use other models and not the qwen\r\n> > \r\n> > \r\n> > > \r\n> > \r\n> > \r\n> > > you can change it in the code, where it loads Qwen in infer_gradio.py, you can put any huggingface LLM.\r\n> > \r\n> > \r\n> > i couldnt do it, i tried using chatgpt but i get an error. can help with what to change and how? like example\r\n> \r\n> Well ChatGPT isn't in huggingface obviously. You'd need to integrate the API yourself, that's currently out of scope.\r\n\r\ni meant chatgpt guidance, well i thought it is something simple like only directing to hugging face model page.\r\n\r\nThanks anyway"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-08T10:16:28Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 362,
    "title": "English Pronunciation Issues with Vowels in F5-TTS",
    "created_at": "2024-11-01T12:10:24Z",
    "closed_at": "2024-11-03T15:17:04Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/362",
    "body": "### English Pronunciation Issues with Vowels in F5-TTS\r\n\r\n#### Description\r\nWhen processing English text without phoneme input, F5-TTS shows inconsistent pronunciation patterns, particularly with vowel sounds. The model appears to have difficulty distinguishing between certain vowel phonemes, leading to pronunciation confusion.\r\n\r\n#### Specific Issues\r\n- Confusion between different vowel sounds (AEI vowels)\r\n- Example: \"time\" may be pronounced as:\r\n  - \"t em\" (incorrect)\r\n  - \"t ey m\" (correct pronunciation should be /taɪm/)\r\n  \r\n- Example 2: \"I\" may be pronounced as:\r\n  - \"yee\" (incorrect)\r\n  - \"eye\" (correct pronunciation should be /aɪ/)",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/362/comments",
    "author": "bensonbs",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T12:50:38Z",
        "body": "Yes, thanks for mentioning this.\r\n\r\nThe current model is trained with Emilia which has many non-native English speakers in corpus.\r\nCompare with models trained with Audio books corpus (native speakers and standard pronunciations), e.g. VALL-E series, Voicebox, MS's official E2 TTS, etc, F5 repo models currently have problems with it.\r\nAlso, as the current base models are using characters rather than phonemes, it might be harder to learn proper pronunciation.\r\n\r\nWIP~"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T15:17:04Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 359,
    "title": "small update gradio finetune",
    "created_at": "2024-11-01T09:23:40Z",
    "closed_at": "2024-11-01T10:22:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/359",
    "body": "@SWivid just small fix stuff \r\nso now when audio stereo always get duraction mono and resample\r\nafter train take case stereo to mono and resample\r\nand just fix error speling the bf16",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/359/comments",
    "author": "lpscr",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T10:22:32Z",
        "body": "@lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification.\r\nJust remove the num_channel in eq is fine."
      },
      {
        "user": "lpscr",
        "created_at": "2024-11-01T10:28:24Z",
        "body": "> @lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification. Just remove the num_channel in eq is fine.\r\n\r\nok so the first comit was ok then , yes after i confuse and chnage again thank you the fix"
      }
    ]
  },
  {
    "number": 355,
    "title": "Issues with multiple GPUs when using the chat function.",
    "created_at": "2024-11-01T03:33:19Z",
    "closed_at": "2024-11-03T15:21:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/355",
    "body": "I'm having issues with the voice chat function in F5. I can use the TTS just fine, but the voice chat is errors with an incompatibility with the GPU. I have both an RTX 3090 and a 2080ti. It is not clear what went wrong when trying to use the right GPU or if I can get it to use just my 3090. I have the correct cuda version install which is 12.4. I am using a conda environment with python 3.10.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/gradio/queueing.py\", line 536, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/gradio/blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/gradio/blocks.py\", line 1520, in call_function\r\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\r\n    result = context.run(func, *args)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/gradio/utils.py\", line 826, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"/home/jvanderzee/F5-TTS/src/f5_tts/infer/infer_gradio.py\", line 584, in process_audio_input\r\n    response = generate_response(conv_state, chat_model_state, chat_tokenizer_state)\r\n  File \"/home/jvanderzee/F5-TTS/src/f5_tts/infer/infer_gradio.py\", line 68, in generate_response\r\n    generated_ids = model.generate(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2215, in generate\r\n    result = self._sample(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3206, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/accelerate/hooks.py\", line 170, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1164, in forward\r\n    outputs = self.model(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 895, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/accelerate/hooks.py\", line 170, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 623, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/accelerate/hooks.py\", line 170, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/jvanderzee/miniconda3/envs/f5-tts/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 544, in forward\r\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\r\nRuntimeError: FlashAttention only supports Ampere GPUs or newer.\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/355/comments",
    "author": "JAAYapps",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T03:58:41Z",
        "body": "> if I can get it to use just my 3090\r\n\r\ncmd run \"nvidia-smi\" see at which rank is 3090\r\n\r\nif 3090 is rank0, do `CUDA_VISIBLE_DEVICES=0 f5-tts_infer-gradio`"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T15:21:53Z",
        "body": "will close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 354,
    "title": "[add] socket stream",
    "created_at": "2024-11-01T03:15:21Z",
    "closed_at": "2024-11-03T09:24:58Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/354",
    "body": "- [add] socket server\r\n- [edit] readme inference to use socket in stream ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/354/comments",
    "author": "kunci115",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T06:20:15Z",
        "body": "@kunci115 Thanks for PR~\r\nWill check after #345 "
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-01T07:28:26Z",
        "body": "cc @lpscr @ZhikangNiu \r\nI'm not familiar with this, might help check if ready for merge"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-03T09:23:58Z",
        "body": "Merging, cc @ZhikangNiu .\r\n\r\nThanks a lot @kunci115 ."
      }
    ]
  },
  {
    "number": 352,
    "title": "Why is the size of the checkpoint saved during training large?",
    "created_at": "2024-11-01T03:07:27Z",
    "closed_at": "2024-11-01T04:02:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/352",
    "body": "I fine tune the F5-TTS model, however I see that the weight size of the model is more than 5GB while your weight size is only 1.26GB. What is the reason?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/352/comments",
    "author": "haubui17062019",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-01T03:14:59Z",
        "body": "because we re-clean the train checkpoint and only save ema_model_state_dict"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-01T04:02:10Z",
        "body": "during training, the ckpt is saving also the states of optimizer, scheduler, model, ema_model, etc.\r\n\r\nwill close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 350,
    "title": "ne",
    "created_at": "2024-11-01T01:42:51Z",
    "closed_at": "2024-11-01T03:06:49Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/350",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/350/comments",
    "author": "JohnSmithToYou",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-01T03:06:47Z",
        "body": "Unclear question and I will close this issue"
      }
    ]
  },
  {
    "number": 348,
    "title": "Multi-speaker small bug",
    "created_at": "2024-10-31T17:36:33Z",
    "closed_at": "2024-11-01T06:11:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/348",
    "body": "When using infer-cli for multi-speaker operations, I discovered that if gen_text begins with a character tag, such as [main]This is a speaker speaks., it throws an error. However, if it doesn't start with a character tag, no error occurs. The error message is:\r\nF5-TTS/src/f5_tts/infer/utils_infer.py\", line 380, in infer_batch_process\r\nfinal_wave = generated_waves[0]\r\n~~~~~~~~~~~~~~~^^^\r\nIndexError: list index out of range",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/348/comments",
    "author": "Huowuge",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T06:11:08Z",
        "body": "@Huowuge Thanks for reporting the bug ! fixed"
      }
    ]
  },
  {
    "number": 342,
    "title": "accelerate with vllm、imdeploy、trt-llm",
    "created_at": "2024-10-31T02:59:37Z",
    "closed_at": "2024-11-16T11:46:04Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/342",
    "body": "Does the algorithm currently support the use of these engines to accelerate? like vllm？imdeploy？or TensorRT-LLM",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/342/comments",
    "author": "Bigfishering",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-01T03:08:15Z",
        "body": "In fact, we also want to speed up F5TTS, but this is not our area of ​​expertise. If possible, we hope more people can help us."
      },
      {
        "user": "arthursunbao",
        "created_at": "2024-11-15T03:02:02Z",
        "body": "+1. we encountered the same problem"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-11-16T11:46:00Z",
        "body": "In fact, the vllm framework is suitable for the generation of large language models based on autoregressive models, but is not suitable for dit series. I will close this issue and if you have some insight you can reopen this issue"
      },
      {
        "user": "arunvisw123",
        "created_at": "2025-01-07T20:43:52Z",
        "body": "@SWivid why didn't we combine the predictions for null and regular..        def fn(t, x):\r\n            combined_x = torch.cat([x, x], dim=0)\r\n            combined_cond = torch.cat([step_cond, torch.zeros_like(step_cond)], dim=0)\r\n            combined_text = torch.cat([text, torch.zeros_like(text)], dim=0)\r\n            # Perform the forward pass with concatenated inputs\r\n            combined_pred = self.transformer(\r\n                x=combined_x, cond=combined_cond,\r\n                text=combined_text, time=t,\r\n                mask=mask, drop_audio_cond=False, drop_text=False\r\n            )\r\n\r\n            # Split the predictions back into pred and null_pred\r\n            pred, null_pred = combined_pred.chunk(2, dim=0)\r\n            # Apply cfg_strength logic\r\n            if cfg_strength < 1e-5:\r\n                return pred\r\n            return pred + (pred - null_pred) * cfg_strength. this gives me a good boost with performance without losing any quality (i think so). Do you see any problems with this approach?\r\n"
      }
    ]
  },
  {
    "number": 338,
    "title": "Fix error in Gradio app",
    "created_at": "2024-10-30T17:07:07Z",
    "closed_at": "2024-12-16T08:47:38Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/338",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/338/comments",
    "author": "fakerybakery",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-31T03:14:00Z",
        "body": "Hi @fakerybakery , it was intended to do like that, cuz gr.info will pull the page up frequently while the conversation interface is at lower space.\r\nAre there any solutions enabling gr.info showing on the current position of page rather than top?\r\nThanks~"
      }
    ]
  },
  {
    "number": 337,
    "title": "RuntimeError: Error(s) in loading state_dict for EMA",
    "created_at": "2024-10-30T17:01:15Z",
    "closed_at": "2024-10-31T01:26:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/337",
    "body": "求助，是加载模型的代码不对吗，我只改了cache_path为本地的绝对路径\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 161, in <module>\r\n    main()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 154, in main\r\n    trainer.train(\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 248, in train\r\n    start_step = self.load_checkpoint()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 168, in load_checkpoint\r\n    self.ema_model.load_state_dict(checkpoint[\"ema_model_state_dict\"])\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 512]) from checkpoint, the shape in current model is torch.Size([3413, 512]).\r\nTraceback (most recent call last):\r\n  File \"/data/env/f5-tts/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1168, in launch_command\r\n    simple_launcher(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 763, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/data/env/f5-tts/bin/python', 'src/f5_tts/train/finetune_cli.py', '--exp_name', 'F5TTS_Base', '--learning_rate', '1e-05', '--batch_size_per_gpu', '2400', '--batch_size_type', 'frame', '--max_samples', '64', '--grad_accumulation_steps', '1', '--max_grad_norm', '1', '--epochs', '18107', '--num_warmup_updates', '14', '--save_per_updates', '26', '--last_per_steps', '6', '--dataset_name', 'yn_1030', '--finetune', 'True', '--tokenizer', 'pinyin', '--log_samples', 'True', '--logger', 'wandb']' returned non-zero exit status 1.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/337/comments",
    "author": "russell-shu",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T00:51:44Z",
        "body": "vocab大小不对，如果想finetune的话，不要更改vocab"
      },
      {
        "user": "russell-shu",
        "created_at": "2024-10-31T01:11:57Z",
        "body": "非常感谢，果然是我改了vocab的原因\r\n\r\n> vocab大小不对，如果想finetune的话，不要更改vocab\r\n\r\n"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T01:26:16Z",
        "body": "great"
      }
    ]
  },
  {
    "number": 336,
    "title": "trained",
    "created_at": "2024-10-30T14:50:46Z",
    "closed_at": "2024-10-31T01:28:41Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/336",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/336/comments",
    "author": "wallaceblaia",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-30T15:07:00Z",
        "body": "Could you elaborate? Do you have any question?"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T01:28:40Z",
        "body": "It's an unclear question and I will close this issue. "
      }
    ]
  },
  {
    "number": 335,
    "title": "Multiple Speech-Type Generation broken?",
    "created_at": "2024-10-30T13:05:43Z",
    "closed_at": "2024-10-30T13:56:04Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/335",
    "body": "I din´t know if i am stupid but no mater what i do, as soon as i put text into the Text to Generate field the button to generate audio grays out and can´t be used.\r\nHow does this function work now? \r\nThe description text at the top also doesn´t make me smarter ^^\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/335/comments",
    "author": "0dram",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-30T13:31:05Z",
        "body": "Hi @0dram , will need to assure the text input has a reference audio matched.\r\ne.g. \r\nif you input text as : \"{happy} happy happy, {sad} sad sad\"\r\nyou should have uploaded two reference audio and mark them as `happy` and `sad`"
      },
      {
        "user": "0dram",
        "created_at": "2024-10-30T13:42:03Z",
        "body": "hey there thanks for the quick reply <3\r\nIt just doesn't work in Pinocchio interface. I tried it with the browser and it worked just fine. I'm sorry. \r\nThanks again "
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-30T13:56:04Z",
        "body": "If work is great~\r\nWill close this issue, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 330,
    "title": "针对训练和推理mask操作的一些疑问",
    "created_at": "2024-10-30T07:42:35Z",
    "closed_at": "2024-10-30T09:11:48Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/330",
    "body": "我在做onnx的导出和尝试训练的时候发现一些问题，虽然我已经解决了一些mask的问题，但是还是想请教一下，我的问题如下：\r\n\r\n1、在训练阶段，没有存在text_mask和mel_mask的情况，这样模型不会注意到很多无用的信息么？\r\n例如：\r\n```\r\n        # lens and mask\r\n        if not exists(lens):\r\n            lens = torch.full((batch,), seq_len, device=device)\r\n\r\n        mask = lens_to_mask(lens, length=seq_len)  # useless here, as collate_fn will pad to max length in batch\r\n\r\n        # get a random span to mask out for training conditionally\r\n        frac_lengths = torch.zeros((batch,), device=self.device).float().uniform_(*self.frac_lengths_mask)\r\n        rand_span_mask = mask_from_frac_lengths(lens, frac_lengths)\r\n\r\n        if exists(mask):\r\n            rand_span_mask &= mask\r\n```\r\n注释提到了：useless here, as collate_fn will pad to max length in batch，倘若我传入lens = torch.tensor([245, 356])，inp = torch.rand([2, 500, 100])，代码会报错。所以我认为咱们并没有进行mel_mask。\r\n\r\n2、第二段代码是，text_len会padding到seq_len的长度，再去过convnextv2的结构。\r\n```\r\n        batch, text_len = text.shape[0], text.shape[1]\r\n        text = text + 1  # use 0 as filler token. preprocess of batch pad -1, see list_str_to_idx()\r\n        text = text[:, :seq_len]  # curtail if character tokens are more than the mel spec tokens\r\n        text = F.pad(text, (0, seq_len - text_len), value = 0)\r\n```\r\n这段代码我也有一些费解，没有text_mask的遮掩，convnextv2的计算是否正确？因为我在onnx定长方案导出的过程中，明显发现会语意捕捉失败，导致合成失效。我更改增加了许多mask的操作，才使得onnx定长推理成功。\r\n\r\n综上：请问作者是如何考量text_mask和mel_mask的问题的？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/330/comments",
    "author": "QingliangMeng",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-30T08:04:03Z",
        "body": "关于1.：\r\n一方面我们sampler直接从小到大排序了，所以padding得很少，尤其数据量大的时候\r\n另一方面loss计算有个mask，只会算TTS infilling task要重建部分的mse loss\r\n另一个点是forward那里有提，虽然实际上是写了的，只要传mask=mask进transformer()就行，但加mask会吃更多显存\r\n\r\n关于2：\r\n这个我觉得是有更好做法的。我们还没有非常深入研究refinement的过程，仅做了model structure search\r\n如果方便的欢迎分享加了怎么样的mask能work的"
      },
      {
        "user": "QingliangMeng",
        "created_at": "2024-10-30T08:39:27Z",
        "body": "感谢您的回复。\r\n针对1：确实sort可以减小padding的错误信息，我也会在我的data streaming中进行该操作。在大数据上确实这种误差不是那么重要了。\r\n\r\n针对2：其实我为了保证onnx导出没有误差，进行大范围的mask操作，可能会有一些冗余，但是可以指出一些结论，希望可以帮到：\r\n1、主要受到padding信息干扰最严重的位置是：ConvNeXtV2Block。最开始根据理论，我以为self.dwconv会产生较大padding错误干扰，但是后来发现后边的计算也会导致误差错误，所以我基本是一个操作一个mask（这个不严谨，只是为了偷懒争取不错）。\r\n2、Transformer DiT block那个mask确保attention不会造成误差。但是其他DiT Block的算子不知道会不会，为了保险我在每层输出位置进行mask纠正。"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-30T09:19:01Z",
        "body": "@QingliangMeng 感谢share~\r\n我大概有一些关于conv误差的猜测：\r\n\r\n1. 这里的padding其实不像mel的batch padding，这里更多是filler tokens，会起到一定语义上作用的；这也是我们先加了filler tokens然后做refinement的部分原因。因此dwconv由于是局部的，kernel_size也小，不会有特别多因为batch padding带来的误差？\r\n\r\n3. 后面linear的计算误差，很有可能是因为我们先对添了filler tokens后的text sequence 加了绝对位置编码，而这个位置编码是在整个sequence上加的，会隐式有关于对应到语音序列（因为处理到和mel等长了）的长度、位置等信息在？"
      },
      {
        "user": "QingliangMeng",
        "created_at": "2024-10-30T10:13:40Z",
        "body": "十分感谢！确实解决了我很多疑惑哈哈哈。我仔细思考了一下，很赞同这两个猜测（确实有可能）。我会在接下来实验中，操作观测一下。"
      },
      {
        "user": "awei669",
        "created_at": "2025-01-08T08:50:19Z",
        "body": "> 感谢您的回复。 针对1：确实sort可以减小padding的错误信息，我也会在我的data streaming中进行该操作。在大数据上确实这种误差不是那么重要了。\r\n> \r\n> 针对2：其实我为了保证onnx导出没有误差，进行大范围的mask操作，可能会有一些冗余，但是可以指出一些结论，希望可以帮到： 1、主要受到padding信息干扰最严重的位置是：ConvNeXtV2Block。最开始根据理论，我以为self.dwconv会产生较大padding错误干扰，但是后来发现后边的计算也会导致误差错误，所以我基本是一个操作一个mask（这个不严谨，只是为了偷懒争取不错）。 2、Transformer DiT block那个mask确保attention不会造成误差。但是其他DiT Block的算子不知道会不会，为了保险我在每层输出位置进行mask纠正。\r\n\r\n请问您是如何添加text_mask的，是直接将filler token部分当成mask吗？"
      },
      {
        "user": "QingliangMeng",
        "created_at": "2025-01-08T09:21:41Z",
        "body": "> > 感谢您的回复。 针对1：确实sort可以减小padding的错误信息，我也会在我的data streaming中进行该操作。在大数据上确实这种误差不是那么重要了。\r\n> > 针对2：其实我为了保证onnx导出没有误差，进行大范围的mask操作，可能会有一些冗余，但是可以指出一些结论，希望可以帮到： 1、主要受到padding信息干扰最严重的位置是：ConvNeXtV2Block。最开始根据理论，我以为self.dwconv会产生较大padding错误干扰，但是后来发现后边的计算也会导致误差错误，所以我基本是一个操作一个mask（这个不严谨，只是为了偷懒争取不错）。 2、Transformer DiT block那个mask确保attention不会造成误差。但是其他DiT Block的算子不知道会不会，为了保险我在每层输出位置进行mask纠正。\r\n> \r\n> 请问您是如何添加text_mask的，是直接将filler token部分当成mask吗？\r\n\r\n那不能够。。。。训练阶段我遵循作者讲数据进行sort，减少了无用的padding信息，并且换了显存大的gpu。工程化如果要定长推理的话，我区分了padding token和filler token，filler token不需要mask，是需要将多余适配定长的padding token mask就行了"
      }
    ]
  },
  {
    "number": 325,
    "title": "Using a temp directory inside of F5 instead of clogging %temp%",
    "created_at": "2024-10-30T03:39:35Z",
    "closed_at": "2024-10-30T03:40:35Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/325",
    "body": "So I have notice that F5 is exporting everything to %temp% on windows and I would rather make it run everything from within the local directory to make users systems cleaner.\r\nAlready it calls to users/user/.cache when an updated gradio.py could solve this problem.\r\n\r\nOriginally with infer_gradio.py I added \r\n\r\nimport os\r\n\r\n# Set up the temp directory at the top of your script\r\napp_root = os.path.dirname(os.path.abspath(__file__))\r\ntemp_dir = os.path.join(app_root, 'temp')\r\nos.makedirs(temp_dir, exist_ok=True)\r\ntempfile.tempdir = temp_dir\r\n\r\n\r\nThis makes generations appear in F5-TTS\\src\\f5_tts\\infer\\temp which can be cleaner.\r\nI don't know much about python so I can't make it better, but including something like this will be a step towards making f5 more 'portable' which should be the end goal.\r\n\r\nI love the project by the way!\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/325/comments",
    "author": "spike4379",
    "comments": [
      {
        "user": "spike4379",
        "created_at": "2024-10-30T03:40:35Z",
        "body": "Closing and moving to discussions. apologies"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-30T04:41:26Z",
        "body": "@spike4379 Thanks~\r\nIf further questions, feel free to open an issue."
      }
    ]
  },
  {
    "number": 322,
    "title": "no such file f5-tts_infer-gradio",
    "created_at": "2024-10-30T01:20:47Z",
    "closed_at": "2024-10-30T11:16:59Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/322",
    "body": "# Launch a Gradio app (web interface)\r\nf5-tts_infer-gradio\r\nThere is no such file in the repo. cannot run it.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/322/comments",
    "author": "michaeltran33",
    "comments": [
      {
        "user": "justinjohn0306",
        "created_at": "2024-10-30T02:26:23Z",
        "body": "> # Launch a Gradio app (web interface)\r\n> f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n\r\ncd F5-TTS\r\npip install -e ."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T10:42:55Z",
        "body": "> > # Launch a Gradio app (web interface)\r\n> > f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n> \r\n> cd F5-TTS pip install -e .\r\nno such file regardless. Best to review the repo. \r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-10-30T10:46:01Z",
        "body": "Please note that this command is available when f5tts is installed as a pip package, not before.\r\nHope this helps a bit."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T10:47:21Z",
        "body": "lemme try. found it. it is exe form. thx. "
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T11:17:00Z",
        "body": "it works now. thanks in a bunch."
      }
    ]
  },
  {
    "number": 319,
    "title": "Integration with Openai Advance Audio Mode",
    "created_at": "2024-10-29T16:12:28Z",
    "closed_at": "2024-10-31T10:41:54Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/319",
    "body": "Could we look into this?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/319/comments",
    "author": "Adesoji1",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-30T09:42:05Z",
        "body": "What's the use case?"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-31T10:41:54Z",
        "body": "Closing until you give more context"
      }
    ]
  },
  {
    "number": 314,
    "title": "finetune-gradio 转写的时候耗时很长，并且每次都会失败",
    "created_at": "2024-10-29T08:10:31Z",
    "closed_at": "2024-10-29T08:24:12Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/314",
    "body": "transcribe complete samples : 0\r\npath : /root/F5-TTS/src/f5_tts/../../data/my_spe_pinyin/wavs\r\nerror files : 6",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/314/comments",
    "author": "liuhui881125",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-29T08:14:38Z",
        "body": "check network to huggingface, or manully download `whisper-large-v3-turbo` ckpt files and put under `.cache/huggingface/hub`"
      },
      {
        "user": "liuhui881125",
        "created_at": "2024-10-29T08:24:12Z",
        "body": "fix .nice "
      }
    ]
  },
  {
    "number": 313,
    "title": "How to train the model effectively?",
    "created_at": "2024-10-29T04:02:19Z",
    "closed_at": "2024-10-29T08:31:49Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/313",
    "body": "I’m unsure whether I should train this TTS model with multiple voices or just one to achieve the best results. Should the text be normalized to all lowercase, or is it okay to keep uppercase letters? Also, should punctuation marks like '. , ? !' be removed from sentences? I would appreciate any advice. Thank you!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/313/comments",
    "author": "PhamDangNguyen",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-29T04:31:44Z",
        "body": "All kept in our training, you may refer to Emilia dataset, and you can see there are punctuations and capitalizations, and our tokenizer has no operations of lowercase or punc removal."
      },
      {
        "user": "PhamDangNguyen",
        "created_at": "2024-10-29T08:01:47Z",
        "body": "@SWivid Is using multiple speakers better than using just one? What do you think?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-29T08:17:48Z",
        "body": "> @SWivid Is using multiple speakers better than using just one? What do you think?\r\n\r\nUp to your need, if you only do inference with the typical one speaker style, one for training is fine.\r\nOtherwise more is better."
      }
    ]
  },
  {
    "number": 311,
    "title": "Generation broken when using reference clips longer than 15 sec",
    "created_at": "2024-10-29T02:11:55Z",
    "closed_at": "2024-11-01T15:00:36Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/311",
    "body": "Not sure when this started happening, but if you use a reference clip of--for example 30 seconds--the generated result is 10 second gibberish.\r\n\r\nFor comparison I trimmed the exact same reference clip to 12 second and generated with the exact same text, and it works perfectly.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/311/comments",
    "author": "cocktailpeanut",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-29T04:54:52Z",
        "body": "Hi @cocktailpeanut ~\r\n\r\nThanks for reporting this, mainly because some edge cases not properly considered when clipping long ref_audio input.\r\nFix in 85089a276beca7fb853e0f77d91197ee7e0a1358\r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 305,
    "title": "Control: Silence, Pronunciation, Interrogatives, Fine-tuning",
    "created_at": "2024-10-28T11:02:32Z",
    "closed_at": "2024-11-01T07:23:50Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/305",
    "body": "Great program and love the way that you are handling long context and multiple speakers!\r\n\r\n\r\n**Background**\r\n\r\nI noticed that the colon is ignored, so my option was to replace the colon with a comma.\r\nSentences are immediate spoken, back to back. There are often times when you need to inject or extend silence.\r\nSometimes the wrong emphasis is placed on a word.\r\nThere are times when I dont hear the difference between a statement and a question\r\nThere are other times when the clone doesnt match my prosidy or the way I would pronounce from my clone.\r\n\r\n**Questions**\r\n\r\n- Is there a way to add a breath sound, for naturalness?\r\n- Is there a way to extend the silence for a pause (in general), a colon (\":\") and after a period?\r\n- Is there a way to address mispronunciations or to adjust the emphasis on a pronunciation?\r\n- How is silence handled in the output when you **need** to introduction silence, given that introducing complete silence is unnatural?\r\n- How are interrogatives (questions) handled to support inflections on questions?\r\n- How much data is required to fine-tune a model so the outcome more accurately reflects a speakers style?\r\n- Does F5 try to copy the room dynamics or noise of the reference audio for cloning **_or_** in the dataset for fine-tuning, in order to match the dynamics of the provided data?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/305/comments",
    "author": "MrEdwards007",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-28T11:35:41Z",
        "body": "Hi @MrEdwards007 ~\r\n\r\n> Is there a way to add a breath sound, for naturalness?\r\n\r\nNo explicit control yet. WIP\r\n\r\n> Is there a way to extend the silence for a pause (in general), a colon (\":\") and after a period?\r\n\r\nWe take in all punc when training. So adding blank, comma, period would help; colon probably not seen much in training, thus yes `to replace the colon with a comma`\r\n\r\n> Is there a way to address mispronunciations or to adjust the emphasis on a pronunciation?\r\n\r\nMispronunciation: e.g. \"phenix\" -> \"feenix\". Emphasis: not sure, maybe try uppercase the first letter of word\r\n\r\n> How is silence handled in the output when you need to introduction silence, given that introducing complete silence is unnatural?\r\n\r\nUnlike autoregressive models, F5 is NAR and we haven't explicitly introduced special non-verbal tokens like \\<laughter\\> \\<silence\\> yet, it would be nice and hope F5 could serve as a base model boosting future work on it.\r\n\r\n> How are interrogatives (questions) handled to support inflections on questions?\r\n\r\nWe noticed that a more clean and neat reference audio could help model with generating interrogatives with \"?\" at end.\r\n\r\n> How much data is required to fine-tune a model so the outcome more accurately reflects a speakers style?\r\n\r\nF5/E2 structures are without phoneme-level force-alignment, so more is better. \r\n\r\n> Does F5 try to copy the room dynamics or noise of the reference audio for cloning or in the dataset for fine-tuning, in order to match the dynamics of the provided data?\r\n\r\nNot quite get this question. F5 is taking in whole reference audio as prompt, no matter noisy or not. So noisy ref -> noisy output, clean ref -> clean output, leveraging a denoising tool to do preprocess of ref_audio would probably get better results"
      },
      {
        "user": "MrEdwards007",
        "created_at": "2024-10-28T13:01:13Z",
        "body": "This is outstanding and greatly appreciated.\r\n\r\n >>Question:  How much data is required to fine-tune a model so the outcome more accurately reflects a speakers style?\r\n Answer: F5/E2 structures are without phoneme-level force-alignment, so more is better.\r\n\r\nMaybe I should have asked, what is a good starting amount of data?  I have 2.5 hours of high-quality audio.\r\nI haven't seen information on how to fine-tuning for a specific voice.  I saw one person state there isn't fine-tuning but there is only training from scratch for a specific voice, which may take days or weeks.  That sounded wrong but didnt have a way of verifying that statement.\r\n\r\n\r\n>>Question: Does F5 try to copy the room dynamics or noise of the reference audio for cloning or in the dataset for fine-tuning, in order to match the dynamics of the provided data?\r\n\r\n>>Answer: Not quite get this question. F5 is taking in whole reference audio as prompt, no matter noisy or not. So noisy ref -> noisy output, clean ref -> clean output, leveraging a denoising tool to do preprocess of ref_audio would probably get better results\r\n\r\nMy question from the following idea.  I was curious as to whether F5 can be used to support fixing existing audio?  Would the room acoustics (noise, distance from microphone, echo) be replicated in the final product.  This would mean that you could fix existing audio by using a reference audio (room acoustics), produce an the replacement audio and drop that into the the your audio file, either as a replacement segment or you could use it to extend your content, while maintaining the original sound quality."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-28T13:10:43Z",
        "body": "> fine-tuning for a specific voice\r\n\r\nWill have gain. Probably work if the voice is spoken in a language supported by pretrained model.\r\n\r\n> fix existing audio by using a reference audio (room acoustics)\r\n\r\nSo it's something like speech editing? Maybe check the `speech_edit.py` script, a very preliminary application exploration.\r\nProviding a ref_audio with room acoustics -> generate for the part to edit with similar room acoustics, thought OK, not sure about results' consistency"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-01T07:23:50Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 303,
    "title": "Generated audio speak repeated words, or wrong order of words",
    "created_at": "2024-10-28T10:21:00Z",
    "closed_at": "2024-10-28T10:39:00Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/303",
    "body": "Hi,\r\nI've finetuned F5TTS_Base model with my ~300h dataset and tokenizer for Vietnamese language.\r\nThe model can speak intelligible Vietnamese now, but it has problem of repeating words, missing words, or wrong order of words, compared to the input gen_text.\r\nFor example:\r\n- input gen_text: \"Chất lượng cuộc sống của người dân sau khi sắp xếp đơn vị hành chính là yếu tố then chốt\"\r\n- output audio as heard by ear: \"Chất lượng cuộc sống của người dân **_sau khin_**(<--wrong and repeated words) sau khi sắp xếp **_đị_**(<--wrong word) hành chính là yếu tố **_viều_**(<--added word) then chốt\"\r\n\r\nFrom 1M steps to currently at 1M6 steps, I haven't yet to see any improvement.\r\nShould I train more or what else should I do to improve this problem?\r\n\r\nMy train setting:\r\n- bf16 precision on 1x GPU A5000 24GB\r\n- learning_rate = 7.5e-5\r\n- batch_size_per_gpu = 3000\r\n- batch_size_type = \"frame\"\r\n- max_sample = 64\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/303/comments",
    "author": "vu-the-dung",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-28T10:34:04Z",
        "body": "> The model can speak intelligible Vietnamese now, but it has problem of repeating words, missing words, or wrong order of words, compared to the input gen_text.\r\n\r\nIt is nice that it can speak some Vietnamese.\r\nThe robustness issue is mainly for insufficient data, as you can compare with the pretrained data amount.\r\nTwo possible way out:\r\n1. Try get more Vietnamese data.\r\n2. Wait for our more powerful pretrained model."
      },
      {
        "user": "vu-the-dung",
        "created_at": "2024-10-28T10:39:00Z",
        "body": "Thanks,\r\nI'll continue training and try to get more data. Let's see if it can improve the issue"
      },
      {
        "user": "manhcuong17072002",
        "created_at": "2024-11-06T04:01:13Z",
        "body": "@vu-the-dung  Could you please provide more updates on the results? What dataset did you train it on, and how long did the training process take? Thank you.\r\n"
      },
      {
        "user": "vu-the-dung",
        "created_at": "2024-11-19T09:27:18Z",
        "body": "> @vu-the-dung Could you please provide more updates on the results? What dataset did you train it on, and how long did the training process take? Thank you.\r\n\r\nSorry for late update. I trained on my company's private dataset, total ~400h of speech from multiple speakers, from multiple regions (mainly Northern VN, about 3/4 of dataset).\r\n\r\nMy train setting:\r\n- finetune from F5TTS_Base pre-trained model\r\n- bf16 precision on 1x GPU A5000 24GB\r\n- learning_rate = 7.5e-5\r\n- batch_size_per_gpu = 9000\r\n- batch_size_type = \"frame\"\r\n- max_sample = 64\r\n\r\nI trained to 3M3 steps, which took ~3 weeks. The robustness is much better now. The model can almost read correctly on the first generation. So far I see the model can clone the style, speed, dialect of Northern and Southern Vietnamese (my dataset don't have many Central Vietnamese data) based on the ref audio. And the speaker similarity is very impressive"
      },
      {
        "user": "manhcuong17072002",
        "created_at": "2024-11-19T14:12:06Z",
        "body": "@vu-the-dung Thank you for the useful information from you"
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-16T13:45:52Z",
        "body": "@vu-the-dung : which epoch-value have you used ? > 1000 ?"
      }
    ]
  },
  {
    "number": 292,
    "title": "Used with audio instead of text?",
    "created_at": "2024-10-27T10:38:50Z",
    "closed_at": "2024-10-27T11:06:24Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/292",
    "body": "I am amazed at how faithful the voices are with just 15 seconds of audio. Is it possible to use this cloned voice with audio instead of writing text?\r\n\r\nThank you.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/292/comments",
    "author": "AIisCool",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-27T10:52:21Z",
        "body": "What do you mean exactly? You would like your voice to be transcribed into the reference text for batched generation?"
      },
      {
        "user": "AIisCool",
        "created_at": "2024-10-27T10:55:58Z",
        "body": "I mean something like RVC, where the cloned voice can be used with uploaded audio or even real-time. This sounds much better than any RVC model I've heard by miles."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-27T10:57:19Z",
        "body": "I understand now, unfortunately speech to speech is not possible right now due to the model architecture, although it's something they are looking into."
      },
      {
        "user": "AIisCool",
        "created_at": "2024-10-27T11:06:24Z",
        "body": "Ah I see. Sounds like perhaps one day it'll be possible. Thank you for you time."
      }
    ]
  },
  {
    "number": 285,
    "title": "Podcast generation",
    "created_at": "2024-10-26T11:00:31Z",
    "closed_at": "2024-10-26T12:10:26Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/285",
    "body": "I love this but a bit disappointed that the podcast generation has been taken out of the update really used that a lot, how can I get it back is there code for it I could include in the script.\r\nthanks for your work!!!!!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/285/comments",
    "author": "Cal-DCosta",
    "comments": [
      {
        "user": "samni728",
        "created_at": "2024-10-26T11:18:18Z",
        "body": "in  Multiple Speech-Type Generation \r\n\r\nExample Input 2:\r\n{Speaker1_Happy} Hello, I'd like to order a sandwich please.\r\n{Speaker2_Regular} Sorry, we're out of bread.\r\n{Speaker1_Sad} I really wanted a sandwich though...\r\n{Speaker2_Whisper} I'll give you the last one I was hiding."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T11:26:25Z",
        "body": "Hi @Cal-DCosta ~\r\nMultiple Speech-Type Generation is just for it, and you can generate for a talk (podcast) with more guests invited now lol"
      },
      {
        "user": "Cal-DCosta",
        "created_at": "2024-10-26T11:39:02Z",
        "body": "Ah! sorry did not realise, thanks!!!!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T12:10:26Z",
        "body": "@Cal-DCosta Yeah, have fun~\r\nWill close as solved, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 281,
    "title": "Possible to infer with an audio file hosted on the web?",
    "created_at": "2024-10-26T06:06:48Z",
    "closed_at": "2024-10-26T17:44:50Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/281",
    "body": "See title",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/281/comments",
    "author": "platform-kit",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-26T17:13:09Z",
        "body": "I would consider this out of scope, it can be done through the API"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-26T17:44:50Z",
        "body": "Feel free to reopen this issue if need be."
      }
    ]
  },
  {
    "number": 277,
    "title": "fix path in finetune_gradio for finetune-cli",
    "created_at": "2024-10-26T01:56:35Z",
    "closed_at": "2024-10-26T02:12:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/277",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/277/comments",
    "author": "justinjohn0306",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-26T02:12:13Z",
        "body": "Thanks~"
      }
    ]
  },
  {
    "number": 275,
    "title": "Removed Requirments.txt and nodel folders",
    "created_at": "2024-10-26T00:16:58Z",
    "closed_at": "2024-10-27T04:53:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/275",
    "body": "Amazing app! Just curious, whats the reasoning for removing requirements.txt? and the models folder? Youve made the app uninstallable\r\n\r\nthanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/275/comments",
    "author": "gjnave",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-26T01:02:49Z",
        "body": "Hi @gjnave , you could uninstall the pkg simply with `pip uninstall f5-tts`\r\nThe `pyproject.toml` just fulfilled the role of `requirements.txt`, and the models folder is still there -> `src/f5-tts/`\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-27T04:53:39Z",
        "body": "will closed as solved, feel free to open if further questions"
      }
    ]
  },
  {
    "number": 274,
    "title": "Add backward compatibility",
    "created_at": "2024-10-25T22:42:44Z",
    "closed_at": "2024-10-26T01:06:34Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/274",
    "body": "Fixes #260",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/274/comments",
    "author": "fakerybakery",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-25T23:03:22Z",
        "body": "Hi @fakerybakery ~\r\nSaw pinokio updated! Maybe we could take the current version as v1.0.0 lol \r\nWe will ensure backward compatibility to this start point (and it's very possible that no change to whole structure will be made after)"
      },
      {
        "user": "fakerybakery",
        "created_at": "2024-10-26T01:06:34Z",
        "body": "Makes sense, thanks!"
      }
    ]
  },
  {
    "number": 273,
    "title": "Any way to make voice more expressive?",
    "created_at": "2024-10-25T20:31:02Z",
    "closed_at": "2024-10-29T04:57:27Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/273",
    "body": "Is there any value that can be tweaked to sacrifice stability but add more random expression? I'd love that if that's possible.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/273/comments",
    "author": "Likkkez",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-26T13:20:31Z",
        "body": "Currently the control mechanism is just with reference audio.\r\nAs we are not fixing with a default value of random seed during the process of generation, you could simply get various (diverse to some extent) results.\r\n\r\nUnlike autoregressive way of TTS modeling which could leverage various sampling strategies directly, it is not that straightforward for non-autoregressive model to explicitly introduce \"more\" randomness."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-29T04:57:28Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 269,
    "title": "Update README: must be lowercase docker",
    "created_at": "2024-10-25T16:20:06Z",
    "closed_at": "2024-10-25T17:48:17Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/269",
    "body": "## Corrected version\r\n\r\n```bash\r\n# Build from Dockerfile\r\ndocker build -t f5tts:v1 .\r\n\r\n# Or pull from GitHub Container Registry\r\ndocker pull ghcr.io/swivid/f5-tts:main\r\n```\r\n\r\n## Error on docker pull\r\n\r\n```bash\r\n# Build from Dockerfile\r\ndocker build -t f5tts:v1 .\r\n\r\n# Or pull from GitHub Container Registry\r\ndocker pull ghcr.io/SWivid/F5-TTS:main\r\n\r\n[+] Building 0.1s (1/1) FINISHED                                                                                                                                           docker:orbstack\r\n => [internal] load build definition from Dockerfile                                                                                                                                  0.0s\r\n => => transferring dockerfile: 2B                                                                                                                                                    0.0s\r\nERROR: failed to solve: failed to read dockerfile: open Dockerfile: no such file or directory\r\ninvalid reference format: repository name (SWivid/F5-TTS) must be lowercase\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/269/comments",
    "author": "eleijonmarck",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-25T17:48:10Z",
        "body": "Thanks~"
      }
    ]
  },
  {
    "number": 266,
    "title": "convert_char_to_pinyin(text_list) 生成的拼音和最终生成语音结果不符",
    "created_at": "2024-10-25T12:26:42Z",
    "closed_at": "2024-10-29T05:00:00Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/266",
    "body": "> 我们的技术完全没有被全世界落下。\r\n> [['wo3', ' ', 'men', ' ', 'de', ' ', 'ji4', ' ', 'shu4', ' ', 'wan2', ' ', 'quan2', ' ', 'mei2', ' ', 'you3', ' ', 'bei4', ' ', 'quan2', ' ', 'shi4', ' ', 'jie4', ' ', 'la4', ' ', 'xia4', '。']]\r\n\r\n上一个F5-TTS打死读成luo4 xia4\r\n\r\n这个版本的F5-TTS打死读成le4 xia4\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/266/comments",
    "author": "JiajunDou",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-25T12:53:14Z",
        "body": "@JiajunDou 我试了下这个case确实有问题，而且暂时没有在**不改变文本的情况下**好的解决方案\r\n\r\n应该是训练集里`落下`基本只有`luo4 xia4`读法的语音（尽管文本在转拼音时候会既有`luo4 xia4`也有`la4 xia4`），所以模型在这两个词一起出现时候，都会对应成`luo4 xia4`的读法\r\n而像是`蜡烛`->`la4 zhu2`就可以正常读对`la4`\r\n\r\n这种需要在包含正确读法对应的数据上微调或重训\r\n或者考虑改变文本为`我们的技术，完全没有被全世界辣在后面。`"
      },
      {
        "user": "JiajunDou",
        "created_at": "2024-10-25T13:30:13Z",
        "body": "@SWivid 感谢回复，看来遇到类似问题只能自己微调了， 关于落下我又测试了几遍，这个版本的确是读le4 xia4"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-29T05:00:00Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 264,
    "title": "有可能实现多个句子在一个batch中同时推理么",
    "created_at": "2024-10-25T09:49:15Z",
    "closed_at": "2024-10-29T05:03:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/264",
    "body": "对于一些长文本，代码中是先将文本分割，然后在循环中推理这些分割的句子，但这样速度很慢，当前的模型结构有可能做到在一个batch中同时推理多个句子么，这样速度应该会提升很多。",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/264/comments",
    "author": "justzmq",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-25T09:51:32Z",
        "body": "可以，但是需要加mask，不一定会快"
      },
      {
        "user": "justzmq",
        "created_at": "2024-10-25T10:03:41Z",
        "body": "\r\n> 可以，但是需要加口罩，不一定会快\r\n\r\n感谢大佬回复！那么之后有测试或者添加这个功能的计划么，有的话我会持续关注的😊"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T10:41:24Z",
        "body": "@justzmq 现有的code已经支持这种形式的inference（即batch，可以看`cfm.py`的`sample()`函数\r\n只是如上面所说，因为一个batch里长短不一样所以要加mask有额外开销\r\n\r\n目前用fp16推理速度已经还不错的，这些更多是工程上的优化。欢迎PR~~"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-29T05:03:10Z",
        "body": "will close this issue, feel free to open if further questions."
      },
      {
        "user": "WGS-note",
        "created_at": "2024-12-19T07:12:48Z",
        "body": "@SWivid \r\n\r\n你好，非常感谢回复！看到你上面所说的可以支持 batch 的推理，我有一个问题：\r\n\r\n在 `infer_ cli.py` 中，`utils_infer.py` 中的 `infer_process`，它会通过 `chunk_text` 将传入的文本进行切分，然后以 \"batches\" 的形式传入 `infer_batch_process`。\r\n\r\n这其实就是对一个文本啊，我能不能传入的是一个列表呢？比如一个句子数为 3 的列表 gen_texts，然后这个对这个 batches 生成3次推理结果，我得到的是3个音频。\r\n\r\n再次感谢！"
      }
    ]
  },
  {
    "number": 255,
    "title": " Calculated padded input size per channel",
    "created_at": "2024-10-24T21:43:22Z",
    "closed_at": "2024-11-01T04:04:57Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/255",
    "body": "While using gradio, I can not synthesize output. I get the error log below.\r\n\r\n`gen_text 1 Test text\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\r\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\r\n    result = context.run(func, *args)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 90, in infer\r\n    final_wave, final_sample_rate, combined_spectrogram = infer_process(\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\f5_tts\\infer\\utils_infer.py\", line 263, in infer_process\r\n    return infer_batch_process(\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\f5_tts\\infer\\utils_infer.py\", line 343, in infer_batch_process\r\n    generated_wave = vocos.decode(generated_mel_spec.cpu())\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\vocos\\pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\vocos\\models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 310, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"C:\\Users\\alper\\.conda\\envs\\f5-tts\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 306, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\nRuntimeError: Calculated padded input size per channel: (6). Kernel size: (7). Kernel size can't be greater than actual input size`",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/255/comments",
    "author": "gezeralperen",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-25T05:28:14Z",
        "body": "> `gen_text 1 Test text`\r\n\r\nMaybe try with a more reasonable input e.g. Test text for F5-TTS generation, test test."
      },
      {
        "user": "br0ze",
        "created_at": "2024-10-28T15:44:09Z",
        "body": "gen_text 0 Once upon a time, there was a chicken who dreamed of becoming an artist.\r\nGenerating audio in 1 batches...\r\nConverting audio...\r\nUsing cached reference text...\r\ngen_text 0 It wanted to perform on a big stage, but no one believed a chicken could do that.\r\nGenerating audio in 1 batches...\r\nConverting audio...\r\nUsing cached reference text...\r\ngen_text 0 However, the chicken didn’t give up.\r\nGenerating audio in 1 batches...\r\nConverting audio...\r\nAudio is over 18s, clipping short.\r\nNo reference text provided, transcribing reference audio...\r\nFinished transcription\r\ngen_text 0 Once upon a time,\r\ngen_text 1 there was a chicken who dreamed of becoming an artist.\r\nGenerating audio in 2 batches...\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\r\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\r\n    result = context.run(func, *args)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"D:\\F5-TTS\\src\\f5_tts\\infer\\infer_gradio.py\", line 406, in generate_multistyle_speech\r\n    audio, _ = infer(\r\n  File \"D:\\F5-TTS\\src\\f5_tts\\infer\\infer_gradio.py\", line 92, in infer\r\n    final_wave, final_sample_rate, combined_spectrogram = infer_process(\r\n  File \"D:\\F5-TTS\\src\\f5_tts\\infer\\utils_infer.py\", line 263, in infer_process\r\n    return infer_batch_process(\r\n  File \"D:\\F5-TTS\\src\\f5_tts\\infer\\utils_infer.py\", line 343, in infer_batch_process\r\n    generated_wave = vocos.decode(generated_mel_spec.cpu())\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\vocos\\pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\vocos\\models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 310, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"C:\\Users\\Henry Huynh\\miniconda3\\envs\\f5\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 306, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\nRuntimeError: Calculated padded input size per channel: (6). Kernel size: (7). Kernel size can't be greater than actual input size\r\nI got this error too"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-01T04:04:58Z",
        "body": "will close this issue, feel free to open if further questions"
      },
      {
        "user": "jason-kane",
        "created_at": "2024-11-04T03:09:00Z",
        "body": "Same error, but I'm calling infer_process directly.\r\n```\r\n                        audio, final_sample_rate, spectragram = infer_process(                                                                                                                                     \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/f5_tts/infer/utils_infer.py\", line 331, in infer_process                                                                          \r\n                        return infer_batch_process(                                                                                                                                                                \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/f5_tts/infer/utils_infer.py\", line 416, in infer_batch_process                                                                    \r\n                        generated_wave = vocoder.decode(generated_mel_spec)                                                                                                                                        \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context                                                                        \r\n                        return func(*args, **kwargs)                                                                                                                                                               \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/vocos/pretrained.py\", line 112, in decode                                                                                         \r\n                        x = self.backbone(features_input, **kwargs)                                                                                                                                                \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl                                                                     \r\n                        return self._call_impl(*args, **kwargs)                                                                                                                                                    \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl                                                                             \r\n                        return forward_call(*args, **kwargs)                                                                                                                                                       \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/vocos/models.py\", line 79, in forward                                                                                             \r\n                        x = self.embed(x)                                                                                                                                                                          \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl                                                                     \r\n                        return self._call_impl(*args, **kwargs)                                                                                                                                                    \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl                                                                             \r\n                        return forward_call(*args, **kwargs)                                                                                                                                                       \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 308, in forward                                                                                   \r\n                        return self._conv_forward(input, self.weight, self.bias)                                                                                                                                   \r\n                      File \"/home/jkane/books/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward                                                                             \r\n                        return F.conv1d(input, weight, bias, self.stride,                                                                                                                                          \r\n                    RuntimeError: Calculated padded input size per channel: (6). Kernel size: (7). Kernel size can't be greater than actual input size\r\n```"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-04T04:18:53Z",
        "body": "@jason-kane how is the input output and used command"
      }
    ]
  },
  {
    "number": 250,
    "title": "Added a Load button for the chat tab",
    "created_at": "2024-10-24T09:42:09Z",
    "closed_at": "2024-10-24T09:43:38Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/250",
    "body": "This will speed up loading times and optimize memory usage for users who don't want to use that feature",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/250/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-24T09:43:29Z",
        "body": "@jpgallegoar is it ready for merge?\r\ndont mind for the formatter, i'll just take care of it"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-24T10:02:43Z",
        "body": "> @jpgallegoar is it ready for merge? dont mind for the formatter, i'll just take care of it\r\n\r\nYes merged and formatted "
      }
    ]
  },
  {
    "number": 249,
    "title": "Added audio caching for faster inference",
    "created_at": "2024-10-24T09:05:10Z",
    "closed_at": "2024-10-24T09:05:51Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/249",
    "body": "Added audio caching in utils_infer so if the same audio is used twice, it will not be transcribed again.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/249/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-24T09:06:08Z",
        "body": "@SWivid This speeds inference a lot, good idea"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-24T09:30:44Z",
        "body": "Flash PR⚡"
      }
    ]
  },
  {
    "number": 244,
    "title": "读取阿拉伯数字出错",
    "created_at": "2024-10-24T06:24:17Z",
    "closed_at": "2024-10-25T06:40:09Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/244",
    "body": "中文文本里面带阿拉伯数字会自动转英文，怎么解决",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/244/comments",
    "author": "KelvinHuang66",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-24T06:48:30Z",
        "body": "先把阿拉伯数字转中文\r\n比如用 cn2an 这个包"
      }
    ]
  },
  {
    "number": 243,
    "title": "可以支持多少种音色？",
    "created_at": "2024-10-24T06:19:12Z",
    "closed_at": "2024-10-25T22:02:22Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/243",
    "body": "请问：可以支持多少种音色？我如何切换不同的音色？谢谢！",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/243/comments",
    "author": "WGS-note",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-24T06:49:55Z",
        "body": "给不同音色的参考音频就行了\r\n口音特别独特或者风格太强（超出训练集分布太多）的效果可能没那么好\r\n一般的都还行的"
      },
      {
        "user": "WGS-note",
        "created_at": "2024-10-24T09:07:19Z",
        "body": "谢谢！"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T22:02:22Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 240,
    "title": "请问为啥读英语有口音啊，没有地道点的发音吗？",
    "created_at": "2024-10-24T03:54:10Z",
    "closed_at": "2024-10-25T22:02:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/240",
    "body": "我是克隆了一个中文声音，但是读英文的时候，不地道啊，怎么才能读地道的呢？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/240/comments",
    "author": "lishu44275",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-24T05:19:32Z",
        "body": "可以提供一个口音轻的中文声音作为参考音频"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T22:02:03Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 230,
    "title": "4060 8 gb",
    "created_at": "2024-10-23T15:02:51Z",
    "closed_at": "2024-10-25T06:38:02Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/230",
    "body": "4060 8 gb可以部署F5-TTS吗",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/230/comments",
    "author": "blazerui",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T15:08:02Z",
        "body": "可以 用`inference-cli.py` 在手动输入参考音频对应的文本（即不使用whisper转录文本）的情况下只要2G不到\r\n也可以修改`gradio_app.py` prune掉不需要的部分"
      }
    ]
  },
  {
    "number": 225,
    "title": "I was not impressed. I just tried it. Used the gradio and did an inference with a video game character and got halluination and bad voice result. What I am missing?",
    "created_at": "2024-10-23T08:30:18Z",
    "closed_at": "2024-10-23T08:41:42Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/225",
    "body": "I just installed this\r\nOpened gradio, inseretd a 20 sec audio from a video game character talking (without music)\r\nGave it a text\r\nand the result had an imaginary word at the beginning (hallucination), then the rest was correct but bad quality.\r\n\r\n\r\nam I doing something wrong?\r\nWhy is everybody talking about this?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/225/comments",
    "author": "GPU-server",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T08:41:04Z",
        "body": "Maybe you should at least follow our instructions to do inference.\r\nEverything is just there in readme, solved issues and discussions with all people's efforts."
      },
      {
        "user": "GPU-server",
        "created_at": "2024-10-23T08:43:43Z",
        "body": "> Maybe you should at least follow our instructions to do inference. Everything is just there in readme, solved issues and discussions with all people's efforts.\r\n\r\nI moved it (re did a post) in Discussion section, maybe it is more fit there.\r\ncould you show me an example with the voide of \"Geralt\" please? I would like to see how good it can reproduce it (Geralt = voice from the game: The Witcher3) I would LOVE to see an actual example I can reproduce .\r\nPlease."
      }
    ]
  },
  {
    "number": 223,
    "title": "CUDA OOM during training, after some steps",
    "created_at": "2024-10-23T06:56:53Z",
    "closed_at": "2024-10-26T02:34:09Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/223",
    "body": "Hi, I notice GPU mem usage keeps increasing during training\r\nI'm training on 1x A5000 24GB, dataset ~300h contains of 0.2-49s audio files\r\n\r\nMy train.py settings: \r\nbatch_size_per_gpu = 1500\r\nbatch_size_type = \"frame\"\r\nmax_samples = 64\r\n\r\nAt start, GPU mem usage ~9GB, after some x10000 steps, GPU mem usage increases to ~16GB, after some another x10000 steps, GPU mem usage increases to ~22GB, then eventually leads to OOM\r\nIf I resume training, this phenomenon repeat\r\n\r\nIs this intended or a bug somewhere?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/223/comments",
    "author": "vu-the-dung",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T07:11:22Z",
        "body": "cuz attention has a quadratic mem cost,\r\nneed to adjust current rough batchsampler taking longer samples into consideration, e.g. dynamically adjust threshold, smaller for longer samples."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T22:00:51Z",
        "body": "#233 might help"
      },
      {
        "user": "vu-the-dung",
        "created_at": "2024-10-26T02:34:09Z",
        "body": "i changed constraint < 0.3 to < 0.2, use bf16 precision. So far VRAM usage seems stable, no OOM yet. So I'll close this issue"
      }
    ]
  },
  {
    "number": 222,
    "title": "Solve the problem of writing Chinese garbled code to csv training set…",
    "created_at": "2024-10-23T05:41:20Z",
    "closed_at": "2024-10-24T06:51:29Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/222",
    "body": "When reading csv files, Excel identifies the encoding by reading the BOM on the file header. If the file header has no BOM information, it will read according to the Unicode encoding by default. The default csv is ANSI. When we use utf-8 encoding to generate a csv file, no BOM information is generated, and Excel will automatically read it according to the Unicode encoding, resulting in garbled code problems.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/222/comments",
    "author": "v3ucn",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:52:47Z",
        "body": "Hi @v3ucn , thanks for PR~\r\n\r\nWill it also be compatible with `.read()` operations, or better serve as an option for compatibility of current users.\r\n\r\nThanks again."
      },
      {
        "user": "v3ucn",
        "created_at": "2024-10-23T06:49:15Z",
        "body": "> Hi @v3ucn , thanks for PR~\r\n> \r\n> Will it also be compatible with `.read()` operations, or better serve as an option for compatibility of current users.\r\n> \r\n> Thanks again.\r\n\r\nyes sir,It also works with the read function, and I tested it with both Chinese and English data sets\r\n\r\nIn \"uft-8-sig\", sig is spelled\" signature \", that is, \"utf-8 with signature \". When \"UTF-8-SIG\" is used to read \"utf-8 file with BOM \", the BOM is processed separately and separated from the text content"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T07:17:41Z",
        "body": "Hi @v3ucn . \r\n\r\nYes, what I mean is: \r\nwill we need also change the parts in code\r\n```\r\n    with open(file_metadata, \"r\", encoding=\"utf-8\") as f:\r\n        data = f.read()\r\n```\r\nto `utf-8-sig`, as you only change for `.save()` part,\r\nand will the merge operation influences people already have run this script before, block on-going finetuning or not.\r\n\r\nOr we could provide `utf-8-sig` as an option for those interested in checking specific csv content in the future."
      },
      {
        "user": "v3ucn",
        "created_at": "2024-10-23T09:33:31Z",
        "body": "> Hi @v3ucn .\r\n> \r\n> Yes, what I mean is: will we need also change the parts in code\r\n> \r\n> ```\r\n>     with open(file_metadata, \"r\", encoding=\"utf-8\") as f:\r\n>         data = f.read()\r\n> ```\r\n> \r\n> to `utf-8-sig`, as you only change for `.save()` part, and will the merge operation influences people already have run this script before, block on-going finetuning or not.\r\n> \r\n> Or we could provide `utf-8-sig` as an option for those interested in checking specific csv content in the future.\r\n\r\ngot it,just update the script"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T12:50:11Z",
        "body": "Hi @lpscr , maybe you could help check if compatible with current version, or a re-run of dataset preparation process needed.\r\nWill merge if no big conflicts, together with the repo reorganization."
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-23T19:55:08Z",
        "body": "> Hi @lpscr , maybe you could help check if compatible with current version, or a re-run of dataset preparation process needed. Will merge if no big conflicts, together with the repo reorganization.\r\n\r\ni see tomorrow i want also make some other update and add stuff "
      }
    ]
  },
  {
    "number": 220,
    "title": "convert_char_to_pinyin doesn't tokenize the input string?",
    "created_at": "2024-10-22T17:09:00Z",
    "closed_at": "2024-10-23T22:13:13Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/220",
    "body": "I noticed the function convert_char_to_pinyin wasn't correctly tokenizing my text according to my vocab. This only affects custom vocabs, but the function was returning every word as a token, even though those words aren't tokens in my vocab.\r\n\r\nI had to create my own tokenization script.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/220/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T04:40:43Z",
        "body": "Yes, customed tokenizer is needed to match for changes."
      }
    ]
  },
  {
    "number": 219,
    "title": "Error in podcast：AttributeError: 'int' object has no attribute 'export'",
    "created_at": "2024-10-22T16:45:30Z",
    "closed_at": "2024-10-23T14:27:36Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/219",
    "body": "Traceback (most recent call last):\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\r\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\r\n    result = context.run(func, *args)\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\env\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\gradio_app.py\", line 257, in podcast_generation\r\n    return generate_podcast(\r\n  File \"G:\\AI\\AI\\tool\\F5-TTS\\gradio_app.py\", line 141, in generate_podcast\r\n    final_podcast.export(podcast_path, format=\"wav\")\r\nAttributeError: 'int' object has no attribute 'export'\r\n\r\n\r\nAn error will occur when running, it seems that the type is wrong?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/219/comments",
    "author": "yhqqxq",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T06:10:07Z",
        "body": "@yhqqxq tried and it works from my side\r\ncheck if you are just passing blank `Podcast Script`? Make sure you get something into it."
      },
      {
        "user": "yhqqxq",
        "created_at": "2024-10-23T13:13:26Z",
        "body": "thanks"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T14:27:36Z",
        "body": "will close as resolved\r\nfeel free to open if further questions"
      }
    ]
  },
  {
    "number": 218,
    "title": "Tracking validation loss? ",
    "created_at": "2024-10-22T12:27:47Z",
    "closed_at": "2024-10-25T22:04:35Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/218",
    "body": "Hello, and thanks a lot for open sourcing this work.\r\n\r\nI am using the `finetune-cli.py` script, which works great! I'm only able to track the training loss, and don't see validation loss being calculated anywhere? Is this supported at all? Or is there a reason why it's not?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/218/comments",
    "author": "rlenain",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-22T12:57:02Z",
        "body": "No specific reason lol, we just had not organized a val set.\r\nWe basically check by generating few samples to hear during training #9 "
      },
      {
        "user": "rlenain",
        "created_at": "2024-10-22T13:47:33Z",
        "body": "okay that makes sense! thanks "
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-22T15:18:18Z",
        "body": "@rlenain this script **finetune-cly.py** make for the **gradio finetune**  so please change your value you need do not use default \r\nsomething good working last test i make it's \r\n\r\n\r\n```\r\nlearning_rate = 1e-5 \r\n\r\nbatch_size_per_gpu = 3200 # for 3090 and 4090 if you have other card see memory ! you need change lower \r\nbatch_size_type = \"frame\"  # \"frame\" or \"sample\"\r\nmax_samples = 64  # max sequences per batch if use frame-wise batch_size. we set 32 for small models, 64 for base models\r\ngrad_accumulation_steps = 1  # note: updates = steps / grad_accumulation_steps\r\nmax_grad_norm = 1.\r\n\r\nepochs = 100  # i use 100 to stop manual the train this not mean you need do 100 epochs stop when you think it's good\r\nnum_warmup_updates = 300 # warmup steps for this for 4500 samples about\r\nsave_per_updates = 2000  # save checkpoint per steps\r\nlast_per_steps = 10000 # save last checkpoint per steps \r\n```\r\n\r\ni try find some good values i change soon  "
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T22:04:36Z",
        "body": "will close this issue, feel free to open if further questions~"
      }
    ]
  },
  {
    "number": 215,
    "title": "对中文的使用拼音写的文字，转义效果不好",
    "created_at": "2024-10-22T09:54:41Z",
    "closed_at": "2024-10-23T05:19:06Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/215",
    "body": "比如，“采用jieba分词工具进行分词”，中对“jieba“ TTS完全识别不到是中文的读法jieba， 二是使用英文读法了。\r\n这个有什么方式改善吗？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/215/comments",
    "author": "sc-lj",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-22T10:03:11Z",
        "body": "可以在输入文本时把`jieba`换成`结巴`"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:19:06Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 203,
    "title": "小数据训练",
    "created_at": "2024-10-21T11:56:03Z",
    "closed_at": "2024-10-23T05:18:02Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/203",
    "body": "我想问一下你们有没有用小数据集训练出的模型，我想问一下效果大概是怎么样的",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/203/comments",
    "author": "1750585724",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T13:13:31Z",
        "body": "> 我想问一下你们有没有用小数据集训练出的模型，我想问一下效果大概是怎么样的\r\n\r\n小模型客观指标参考arxiv paper\r\n小模型在部分细节上（比如timestep embedding的维度）和目前repo不兼容，整体结构一致的"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:18:02Z",
        "body": "will close this issue, feel free to open if further questions."
      },
      {
        "user": "justzmq",
        "created_at": "2024-10-23T07:14:13Z",
        "body": "我想问下，如果我只有半个小时的语音数据(中文角色)，可以微调出具有该语音特色的模型么？如果可以的话，我大概需要设置怎样的微调参数？期待回复！感谢！"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T08:36:04Z",
        "body": "@justzmq 可以看看discussion里的讨论，他们会比较有经验"
      }
    ]
  },
  {
    "number": 197,
    "title": "Minimal GPU Memory Requirements for Running the Model??",
    "created_at": "2024-10-21T07:10:33Z",
    "closed_at": "2024-10-21T11:24:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/197",
    "body": "I would like to know:\r\n\r\nWhat is the minimal GPU memory requirement for running this model smoothly?\r\nIs there a recommended batch size or other configurations (like precision or memory optimizations) I should use to reduce memory usage?\r\ni managed to change batch_size of trainer.py script, utils_infer.py still encountering cuda oom!! can you tell me the minimal requirements of GPU mmeory??",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/197/comments",
    "author": "sachin-seisei",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:05:56Z",
        "body": "the minimal GPU memory is like 2G (just loading F5/E2 TTS model, and not leverage ASR model to do transcription)\r\nbtw you were to do training or inference.\r\n"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:08:13Z",
        "body": "@SWivid so inference only i am doing as of now!! thats why i want to know in inference also why its showing cuda oom!!\r\nPS: i am not leveraging ASR even i am providing ref text also and not an empty string!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:15:49Z",
        "body": "which script are you using? gradio_app.py or inference-cli.py"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:16:16Z",
        "body": "inference-cli.py \r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:48:26Z",
        "body": "how many gpu mem do you have.\r\ni just tweak the `inference-cli.py` script, make sure it is not loading unneeded asr pipeline.\r\nit takes 1622M gpu mem for me running it"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T11:24:08Z",
        "body": "the issue has been resolved that's why i am closing this.. thanks @SWivid "
      }
    ]
  },
  {
    "number": 192,
    "title": "RuntimeError: Input type (c10::Half) and bias type (float) should be the same",
    "created_at": "2024-10-21T01:57:10Z",
    "closed_at": "2024-10-21T17:55:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/192",
    "body": "I just started getting this error with the latest build:\r\n```\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/ttspod/speech/f5.py\", line 214, in infer_batch\r\n    generated_wave = self.vocos.decode(generated_mel_spec.cpu())\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n        ^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 308, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/192/comments",
    "author": "ajkessel",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T02:32:21Z",
        "body": "`generated = generated.to(torch.float32)` should be added as we done along with .half() for model and input\r\n```\r\n# Final result\r\ngenerated = generated.to(torch.float32)\r\ngenerated = generated[:, ref_audio_len:, :]\r\ngenerated_mel_spec = generated.permute(0, 2, 1)\r\ngenerated_wave = vocos.decode(generated_mel_spec.cpu())\r\n```"
      },
      {
        "user": "ajkessel",
        "created_at": "2024-10-21T17:55:39Z",
        "body": "Thanks, that fixed it."
      }
    ]
  },
  {
    "number": 190,
    "title": "Broken on MPS",
    "created_at": "2024-10-20T19:29:57Z",
    "closed_at": "2024-10-20T20:06:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/190",
    "body": "Not sure what happened, but looks like the app is broken on Macs at the moment.\r\n\r\nJust did a fresh install and the app itself runs, but the resulting audio is empty.\r\n\r\nAlso I am not sure if the logs are useful but pasting just in case:\r\n\r\n```\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n  warnings.warn(\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n\r\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\ngen_text 0 Reference text will be automatically transcribed with Whisper if not provided. For best results, keep your reference clips short\r\nBuilding prefix dict from the default dictionary ...\r\nLoading model from cache /Users/x/pinokio/cache/TMPDIR/jieba.cache\r\nLoading model cost 0.426 seconds.\r\nPrefix dict has been built successfully.\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:574: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\r\n  warnings.warn(warning.format(data.dtype))\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:577: RuntimeWarning: invalid value encountered in cast\r\n  data = data.astype(np.int16)\r\n```\r\n\r\nI am not completely sure but I don't remember seeing this many warning messages previously.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/190/comments",
    "author": "cocktailpeanut",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:35:37Z",
        "body": "a default fp16 inference setting was added.\r\nsee if the last commit works d3badb95cf1b97a61472d65d4787a35cddf9c908"
      },
      {
        "user": "cocktailpeanut",
        "created_at": "2024-10-20T19:48:06Z",
        "body": "@SWivid this worked, thank you!\r\n\r\nCould you share what the switch to fp16 means from end user's point of view (performance, etc.)? Appreciate it!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-20T20:05:25Z",
        "body": "> Could you share what the switch to fp16 means from end user's point of view (performance, etc.)? Appreciate it!\r\n\r\nA bit faster than using fp32, ~half graphics card usage (the %), and more environmentally friendly maybe lol\r\nCompared to a more aggressive int8 quantization, it can be seen as no performance (quality) penalty."
      },
      {
        "user": "cocktailpeanut",
        "created_at": "2024-10-20T20:06:10Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 188,
    "title": "Request to split based on what word is being used for the Gradio app",
    "created_at": "2024-10-20T18:30:44Z",
    "closed_at": "2024-10-20T19:15:13Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/188",
    "body": "Can you include the option to split the chunk based on what word is being used? Like say for example, I want the split to start with the word cheese and split from this\r\n\r\n```\r\nI like to eat cheese. I like to eat pizza.\r\n```\r\n\r\nto this\r\n\r\n```\r\nI like to eat cheese.\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/188/comments",
    "author": "GUUser91",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T18:45:17Z",
        "body": "Is it like explicitly do chunking?\r\n`I like to eat cheese. I like to eat pizza.` -> `I like to eat cheese.`    `I like to eat pizza.`\r\nI thought it could be realized with multi-style generation (to do explicit chunking) but you just pass in same reference audio (for each chunk), lol\r\nAnd maybe remove_silence would be needed in this case (it is turned down by default)."
      }
    ]
  },
  {
    "number": 181,
    "title": "How to run in docker?",
    "created_at": "2024-10-20T03:43:28Z",
    "closed_at": "2024-10-20T03:59:20Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/181",
    "body": "I build the image, but any instructions on how to run it?\r\nFrom youtube, it seems there is a web UI?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/181/comments",
    "author": "bilogic",
    "comments": [
      {
        "user": "bilogic",
        "created_at": "2024-10-20T03:59:21Z",
        "body": "Run this\r\n\r\n```\r\ndocker run f5tts:v1 python inference-cli.py \\\r\n--model \"F5-TTS\" \\\r\n--ref_audio \"tests/ref_audio/test_en_1_ref_short.wav\" \\\r\n--ref_text \"Some call me nature, others call me mother nature.\" \\\r\n--gen_text \"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring. Respect me and I'll nurture you; ignore me and you shall face the consequences.\"\r\n```"
      },
      {
        "user": "platform-kit",
        "created_at": "2024-10-26T05:53:54Z",
        "body": "@bilogic what is the right command now that the code has been refactored?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T06:14:54Z",
        "body": "thought `python inference-cli.py` -> `f5-tts_infer-cli`\r\nor `python inference-cli.py` -> `python src/f5_tts/infer/infer_cli.py`"
      }
    ]
  },
  {
    "number": 177,
    "title": "TypeError: 'type' object is not subscriptable",
    "created_at": "2024-10-19T12:38:31Z",
    "closed_at": "2024-10-23T05:18:46Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/177",
    "body": "Traceback (most recent call last):\r\n  File \"inference-cli.py\", line 19, in <module>\r\n    from model import CFM, DiT, MMDiT, UNetT\r\n  File \"/opt/SWivid/F5-TTS/F5-TTS/model/__init__.py\", line 7, in <module>\r\n    from model.trainer import Trainer\r\n  File \"/opt/SWivid/F5-TTS/F5-TTS/model/trainer.py\", line 22, in <module>\r\n    from model.dataset import DynamicBatchSampler, collate_fn\r\n  File \"/opt/SWivid/F5-TTS/F5-TTS/model/dataset.py\", line 127, in <module>\r\n    class DynamicBatchSampler(Sampler[list[int]]):\r\nTypeError: 'type' object is not subscriptable\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/177/comments",
    "author": "ThinkInFuture",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:17:23Z",
        "body": "Thought it was for using a early python version.\r\nTry with python least 3.9,  python>=3.10(recommended)"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:18:46Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 175,
    "title": "how to control speed or emotion?",
    "created_at": "2024-10-19T10:53:52Z",
    "closed_at": "2024-10-19T13:27:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/175",
    "body": "how to control speed or emotion?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/175/comments",
    "author": "yiluzhuimeng",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-19T12:24:00Z",
        "body": "See `advanced settings` in gradio interface for speed option.\r\nEmotion is currently with a emotional ref_audio transfering to the generated to some extent.\r\nThanks~"
      }
    ]
  },
  {
    "number": 174,
    "title": "demo using google colab nootbook",
    "created_at": "2024-10-19T08:59:44Z",
    "closed_at": "2024-10-25T05:45:59Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/174",
    "body": "This Colab notebook sets up and runs F5-TTS on Google Colab using T4 Runtime. It automates the installation process and provides a user-friendly progress and launches the Gradio app for exploring F5-TTS's capabilities.\r\n\r\nIdeal for researchers, developers, content creators, and TTS enthusiasts, this notebook offers a convenient way to test F5-TTS's advanced features without complex setup or specialized hardware requirements.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/174/comments",
    "author": "labKnowledge",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-19T13:26:17Z",
        "body": "@labKnowledge Thanks for your pr, I think you can add a description on the README"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T05:45:59Z",
        "body": "Will close as no response"
      }
    ]
  },
  {
    "number": 171,
    "title": "Stays at \"Processing\" and never converts.",
    "created_at": "2024-10-19T01:19:44Z",
    "closed_at": "2024-10-20T10:06:04Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/171",
    "body": "Hi everyone. I've deleted the folder and started all over again just to see if that was the case, but it is not. I have everything in stalled, even ffmpeg and python version as well. The link opens, i am able to upload an audio sample and even synthesize, but i never ever get the conversion. Right now, it is literally at the 380-second mark. I understand it shouldn't take this long, so what am I doing incorrectly, or what am I missing?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/171/comments",
    "author": "BMKBAI",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-19T07:05:40Z",
        "body": "maybe you could help with checking your torch version, see if it's -cpu or -cuda.\r\nand make sure your connection to huggingface is fine, otherwise using a local ckpt is better"
      }
    ]
  },
  {
    "number": 169,
    "title": "ModuleNotFoundError: No module named 'encodec.model'",
    "created_at": "2024-10-18T20:54:33Z",
    "closed_at": "2024-10-23T05:19:41Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/169",
    "body": "I am running on Windows.  Using pyenv to set up a virtual environment and Python 3.10.11.   I ran the two commands for installing pytorch and then ran the command to install requirements.txt.   All of that went without errors.   \r\n\r\n```\r\n(venv) C:\\ai\\F5-TTS>python gradio_app.py\r\nTraceback (most recent call last):\r\n  File \"C:\\ai\\F5-TTS\\gradio_app.py\", line 8, in <module>\r\n    from vocos import Vocos\r\n  File \"C:\\ai\\F5-TTS\\venv\\lib\\site-packages\\vocos\\__init__.py\", line 1, in <module>\r\n    from vocos.pretrained import Vocos\r\n  File \"C:\\ai\\F5-TTS\\venv\\lib\\site-packages\\vocos\\pretrained.py\", line 9, in <module>\r\n    from vocos.feature_extractors import FeatureExtractor, EncodecFeatures\r\n  File \"C:\\ai\\F5-TTS\\venv\\lib\\site-packages\\vocos\\feature_extractors.py\", line 5, in <module>\r\n    from encodec import EncodecModel\r\n  File \"C:\\ai\\F5-TTS\\venv\\lib\\site-packages\\encodec\\__init__.py\", line 12, in <module>\r\n    from .model import EncodecModel\r\nModuleNotFoundError: No module named 'encodec.model'\r\n```\r\n\r\nIt does hesitate for a few seconds before outputting this error.\r\n\r\nHere are the list of installed packages:\r\n\r\n```\r\n(venv) C:\\ai\\F5-TTS>pip list\r\nPackage                  Version\r\n------------------------ ------------\r\naccelerate               1.0.1\r\naiofiles                 23.2.1\r\naiohappyeyeballs         2.4.3\r\naiohttp                  3.10.10\r\naiosignal                1.3.1\r\nannotated-types          0.7.0\r\nanyio                    4.6.2.post1\r\nasync-timeout            4.0.3\r\nattrs                    24.2.0\r\naudioread                3.0.1\r\nboto3                    1.35.44\r\nbotocore                 1.35.44\r\ncached_path              1.6.3\r\ncachetools               5.5.0\r\ncertifi                  2024.8.30\r\ncffi                     1.17.1\r\ncharset-normalizer       3.4.0\r\nclick                    8.1.7\r\ncolorama                 0.4.6\r\ncontourpy                1.3.0\r\ncycler                   0.12.1\r\ndatasets                 3.0.1\r\ndecorator                5.1.1\r\ndill                     0.3.8\r\ndocker-pycreds           0.4.0\r\neinops                   0.8.0\r\neinx                     0.3.0\r\nema-pytorch              0.7.0\r\nencodec                  0.1.1\r\nexceptiongroup           1.2.2\r\nfastapi                  0.115.2\r\nffmpy                    0.4.0\r\nfilelock                 3.13.4\r\nfonttools                4.54.1\r\nfrozendict               2.4.6\r\nfrozenlist               1.4.1\r\nfsspec                   2024.6.1\r\ngitdb                    4.0.11\r\nGitPython                3.1.43\r\ngoogle-api-core          2.21.0\r\ngoogle-auth              2.35.0\r\ngoogle-cloud-core        2.4.1\r\ngoogle-cloud-storage     2.18.2\r\ngoogle-crc32c            1.6.0\r\ngoogle-resumable-media   2.7.2\r\ngoogleapis-common-protos 1.65.0\r\ngradio                   4.44.1\r\ngradio_client            1.3.0\r\nh11                      0.14.0\r\nhttpcore                 1.0.6\r\nhttpx                    0.27.2\r\nhuggingface-hub          0.23.5\r\nidna                     3.10\r\nimportlib_resources      6.4.5\r\nintel-openmp             2021.4.0\r\njieba                    0.42.1\r\nJinja2                   3.1.4\r\njmespath                 1.0.1\r\njoblib                   1.4.2\r\nkiwisolver               1.4.7\r\nlazy_loader              0.4\r\nlibrosa                  0.10.2.post1\r\nllvmlite                 0.43.0\r\nmarkdown-it-py           3.0.0\r\nMarkupSafe               2.1.5\r\nmatplotlib               3.9.2\r\nmdurl                    0.1.2\r\nmkl                      2021.4.0\r\nmpmath                   1.3.0\r\nmsgpack                  1.1.0\r\nmultidict                6.1.0\r\nmultiprocess             0.70.16\r\nnetworkx                 3.4.1\r\nnumba                    0.60.0\r\nnumpy                    1.26.4\r\norjson                   3.10.7\r\npackaging                24.1\r\npandas                   2.2.3\r\npillow                   10.4.0\r\npip                      23.0.1\r\nplatformdirs             4.3.6\r\npooch                    1.8.2\r\npropcache                0.2.0\r\nproto-plus               1.24.0\r\nprotobuf                 5.28.2\r\npsutil                   6.1.0\r\npyarrow                  17.0.0\r\npyasn1                   0.6.1\r\npyasn1_modules           0.4.1\r\npycparser                2.22\r\npydantic                 2.9.2\r\npydantic_core            2.23.4\r\npydub                    0.25.1\r\nPygments                 2.18.0\r\npyparsing                3.2.0\r\npypinyin                 0.53.0\r\npython-dateutil          2.9.0.post0\r\npython-multipart         0.0.12\r\npytz                     2024.2\r\nPyYAML                   6.0.2\r\nregex                    2024.9.11\r\nrequests                 2.32.3\r\nrich                     13.9.2\r\nrsa                      4.9\r\nruff                     0.7.0\r\ns3transfer               0.10.3\r\nsafetensors              0.4.5\r\nscikit-learn             1.5.2\r\nscipy                    1.14.1\r\nsemantic-version         2.10.0\r\nsentry-sdk               2.17.0\r\nsetproctitle             1.3.3\r\nsetuptools               65.5.0\r\nshellingham              1.5.4\r\nsix                      1.16.0\r\nsmmap                    5.0.1\r\nsniffio                  1.3.1\r\nsoundfile                0.12.1\r\nsoxr                     0.5.0.post1\r\nstarlette                0.40.0\r\nsympy                    1.13.3\r\ntbb                      2021.13.1\r\nthreadpoolctl            3.5.0\r\ntokenizers               0.20.1\r\ntomli                    2.0.2\r\ntomlkit                  0.12.0\r\ntorch                    2.3.0+cu118\r\ntorchaudio               2.3.0+cu118\r\ntorchdiffeq              0.2.4\r\ntqdm                     4.66.5\r\ntransformers             4.45.2\r\ntyper                    0.12.5\r\ntyping_extensions        4.12.2\r\ntzdata                   2024.2\r\nurllib3                  2.2.3\r\nuvicorn                  0.32.0\r\nvocos                    0.1.0\r\nwandb                    0.18.5\r\nwebsockets               12.0\r\nx-transformers           1.40.2\r\nxxhash                   3.5.0\r\nyarl                     1.15.5\r\n\r\n[notice] A new release of pip is available: 23.0.1 -> 24.2\r\n[notice] To update, run: python.exe -m pip install --upgrade pip\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/169/comments",
    "author": "dzehme",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-19T04:29:06Z",
        "body": "Maybe you can use Dockerfile"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:19:41Z",
        "body": "will close this issue, feel free to open if further questions."
      },
      {
        "user": "zhazhao23",
        "created_at": "2024-10-27T06:50:27Z",
        "body": "Same problem , help me , thanks "
      }
    ]
  },
  {
    "number": 165,
    "title": "I get a mismatch error from using a finetune model while using the inference file",
    "created_at": "2024-10-18T17:51:27Z",
    "closed_at": "2024-10-18T20:22:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/165",
    "body": "I edit the inference-cli.py file to include the checkpoint path\r\n\r\n```\r\ndef load_model(repo_name, exp_name, model_cls, model_cfg, ckpt_step):\r\n    ckpt_path = f\"/home/user/F5-TTS/ckpts/Finetune/model_last.pt\" # .pt | \r\n.safetensors\r\n```\r\nI left this as it is\r\n\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Emilia_ZH_EN\", \"pinyin\")\r\n```\r\n\r\nThen I changed it after I got a message message\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Finetune\", \"pinyin\")\r\n```\r\n\r\nThen I use the inference file\r\n```\r\npython inference-cli.py \\\r\n--ref_audio \"/home/user/F5-TTS Test/Template/Input.wav\" \\\r\n--ref_text \"Robinson Industries, the world's leading scientific research and design factory. My dad runs the company. They mass produce his inventions. His motto, keep moving forward. It's what he does.\" \\\r\n--gen_text \"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring. Respect me and I'll nurture you; ignore me and you shall face the consequences.\"\r\n```\r\n\r\nThen I get this error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 392, in <module>\r\n    process(ref_audio, ref_text, gen_text, model, remove_silence)\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 374, in process\r\n    audio, spectragram = infer(ref_audio, ref_text, gen_text, model, remove_silence)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 346, in infer\r\n    return infer_batch((audio, sr), ref_text, gen_text_batches, model, remove_silence, cross_fade_duration)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 190, in infer_batch\r\n    ema_model = load_model(model, \"F5TTS_Base\", DiT, F5TTS_model_cfg, 1200000)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 148, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, use_ema = True)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/model/utils.py\", line 575, in load_checkpoint\r\n    ema_model.load_state_dict(checkpoint['ema_model_state_dict'])\r\n  File \"/home/user/F5-TTS/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\r\n    raise RuntimeError(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        Missing key(s) in state_dict: \"ema_model.transformer.text_embed.text_blocks.0.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.0.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.0.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.0.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.0.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.0.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.1.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.1.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.1.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.1.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.2.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.2.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.2.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.2.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.3.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.3.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.3.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.3.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.bias\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.bias\", \"ema_model.transformer.norm_out.linear.weight\", \"ema_model.transformer.norm_out.linear.bias\". \r\n        Unexpected key(s) in state_dict: \"ema_model.transformer.layers.0.1.g\", \"ema_model.transformer.layers.0.2.to_q.weight\", \"ema_model.transformer.layers.0.2.to_q.bias\", \"ema_model.transformer.layers.0.2.to_k.weight\", \"ema_model.transformer.layers.0.2.to_k.bias\", \"ema_model.transformer.layers.0.2.to_v.weight\", \"ema_model.transformer.layers.0.2.to_v.bias\", \"ema_model.transformer.layers.0.2.to_out.0.weight\", \"ema_model.transformer.layers.0.2.to_out.0.bias\", \"ema_model.transformer.layers.0.3.g\", \"ema_model.transformer.layers.0.4.ff.0.0.weight\", \"ema_model.transformer.layers.0.4.ff.0.0.bias\", \"ema_model.transformer.layers.0.4.ff.2.weight\", \"ema_model.transformer.layers.0.4.ff.2.bias\", \"ema_model.transformer.layers.1.1.g\", \"ema_model.transformer.layers.1.2.to_q.weight\", \"ema_model.transformer.layers.1.2.to_q.bias\", \"ema_model.transformer.layers.1.2.to_k.weight\", \"ema_model.transformer.layers.1.2.to_k.bias\", \"ema_model.transformer.layers.1.2.to_v.weight\", \"ema_model.transformer.layers.1.2.to_v.bias\", \"ema_model.transformer.layers.1.2.to_out.0.weight\", \"ema_model.transformer.layers.1.2.to_out.0.bias\", \"ema_model.transformer.layers.1.3.g\", \"ema_model.transformer.layers.1.4.ff.0.0.weight\", \"ema_model.transformer.layers.1.4.ff.0.0.bias\", \"ema_model.transformer.layers.1.4.ff.2.weight\", \"ema_model.transformer.layers.1.4.ff.2.bias\", \"ema_model.transformer.layers.2.1.g\", \"ema_model.transformer.layers.2.2.to_q.weight\", \"ema_model.transformer.layers.2.2.to_q.bias\", \"ema_model.transformer.layers.2.2.to_k.weight\", \"ema_model.transformer.layers.2.2.to_k.bias\", \"ema_model.transformer.layers.2.2.to_v.weight\", \"ema_model.transformer.layers.2.2.to_v.bias\", \"ema_model.transformer.layers.2.2.to_out.0.weight\", \"ema_model.transformer.layers.2.2.to_out.0.bias\", \"ema_model.transformer.layers.2.3.g\", \"ema_model.transformer.layers.2.4.ff.0.0.weight\", \"ema_model.transformer.layers.2.4.ff.0.0.bias\", \"ema_model.transformer.layers.2.4.ff.2.weight\", \"ema_model.transformer.layers.2.4.ff.2.bias\", \"ema_model.transformer.layers.3.1.g\", \"ema_model.transformer.layers.3.2.to_q.weight\", \"ema_model.transformer.layers.3.2.to_q.bias\", \"ema_model.transformer.layers.3.2.to_k.weight\", \"ema_model.transformer.layers.3.2.to_k.bias\", \"ema_model.transformer.layers.3.2.to_v.weight\", \"ema_model.transformer.layers.3.2.to_v.bias\", \"ema_model.transformer.layers.3.2.to_out.0.weight\", \"ema_model.transformer.layers.3.2.to_out.0.bias\", \"ema_model.transformer.layers.3.3.g\", \"ema_model.transformer.layers.3.4.ff.0.0.weight\", \"ema_model.transformer.layers.3.4.ff.0.0.bias\", \"ema_model.transformer.layers.3.4.ff.2.weight\", \"ema_model.transformer.layers.3.4.ff.2.bias\", \"ema_model.transformer.layers.4.1.g\", \"ema_model.transformer.layers.4.2.to_q.weight\", \"ema_model.transformer.layers.4.2.to_q.bias\", \"ema_model.transformer.layers.4.2.to_k.weight\", \"ema_model.transformer.layers.4.2.to_k.bias\", \"ema_model.transformer.layers.4.2.to_v.weight\", \"ema_model.transformer.layers.4.2.to_v.bias\", \"ema_model.transformer.layers.4.2.to_out.0.weight\", \"ema_model.transformer.layers.4.2.to_out.0.bias\", \"ema_model.transformer.layers.4.3.g\", \"ema_model.transformer.layers.4.4.ff.0.0.weight\", \"ema_model.transformer.layers.4.4.ff.0.0.bias\", \"ema_model.transformer.layers.4.4.ff.2.weight\", \"ema_model.transformer.layers.4.4.ff.2.bias\", \"ema_model.transformer.layers.5.1.g\", \"ema_model.transformer.layers.5.2.to_q.weight\", \"ema_model.transformer.layers.5.2.to_q.bias\", \"ema_model.transformer.layers.5.2.to_k.weight\", \"ema_model.transformer.layers.5.2.to_k.bias\", \"ema_model.transformer.layers.5.2.to_v.weight\", \"ema_model.transformer.layers.5.2.to_v.bias\", \"ema_model.transformer.layers.5.2.to_out.0.weight\", \"ema_model.transformer.layers.5.2.to_out.0.bias\", \"ema_model.transformer.layers.5.3.g\", \"ema_model.transformer.layers.5.4.ff.0.0.weight\", \"ema_model.transformer.layers.5.4.ff.0.0.bias\", \"ema_model.transformer.layers.5.4.ff.2.weight\", \"ema_model.transformer.layers.5.4.ff.2.bias\", \"ema_model.transformer.layers.6.1.g\", \"ema_model.transformer.layers.6.2.to_q.weight\", \"ema_model.transformer.layers.6.2.to_q.bias\", \"ema_model.transformer.layers.6.2.to_k.weight\", \"ema_model.transformer.layers.6.2.to_k.bias\", \"ema_model.transformer.layers.6.2.to_v.weight\", \"ema_model.transformer.layers.6.2.to_v.bias\", \"ema_model.transformer.layers.6.2.to_out.0.weight\", \"ema_model.transformer.layers.6.2.to_out.0.bias\", \"ema_model.transformer.layers.6.3.g\", \"ema_model.transformer.layers.6.4.ff.0.0.weight\", \"ema_model.transformer.layers.6.4.ff.0.0.bias\", \"ema_model.transformer.layers.6.4.ff.2.weight\", \"ema_model.transformer.layers.6.4.ff.2.bias\", \"ema_model.transformer.layers.7.1.g\", \"ema_model.transformer.layers.7.2.to_q.weight\", \"ema_model.transformer.layers.7.2.to_q.bias\", \"ema_model.transformer.layers.7.2.to_k.weight\", \"ema_model.transformer.layers.7.2.to_k.bias\", \"ema_model.transformer.layers.7.2.to_v.weight\", \"ema_model.transformer.layers.7.2.to_v.bias\", \"ema_model.transformer.layers.7.2.to_out.0.weight\", \"ema_model.transformer.layers.7.2.to_out.0.bias\", \"ema_model.transformer.layers.7.3.g\", \"ema_model.transformer.layers.7.4.ff.0.0.weight\", \"ema_model.transformer.layers.7.4.ff.0.0.bias\", \"ema_model.transformer.layers.7.4.ff.2.weight\", \"ema_model.transformer.layers.7.4.ff.2.bias\", \"ema_model.transformer.layers.8.1.g\", \"ema_model.transformer.layers.8.2.to_q.weight\", \"ema_model.transformer.layers.8.2.to_q.bias\", \"ema_model.transformer.layers.8.2.to_k.weight\", \"ema_model.transformer.layers.8.2.to_k.bias\", \"ema_model.transformer.layers.8.2.to_v.weight\", \"ema_model.transformer.layers.8.2.to_v.bias\", \"ema_model.transformer.layers.8.2.to_out.0.weight\", \"ema_model.transformer.layers.8.2.to_out.0.bias\", \"ema_model.transformer.layers.8.3.g\", \"ema_model.transformer.layers.8.4.ff.0.0.weight\", \"ema_model.transformer.layers.8.4.ff.0.0.bias\", \"ema_model.transformer.layers.8.4.ff.2.weight\", \"ema_model.transformer.layers.8.4.ff.2.bias\", \"ema_model.transformer.layers.9.1.g\", \"ema_model.transformer.layers.9.2.to_q.weight\", \"ema_model.transformer.layers.9.2.to_q.bias\", \"ema_model.transformer.layers.9.2.to_k.weight\", \"ema_model.transformer.layers.9.2.to_k.bias\", \"ema_model.transformer.layers.9.2.to_v.weight\", \"ema_model.transformer.layers.9.2.to_v.bias\", \"ema_model.transformer.layers.9.2.to_out.0.weight\", \"ema_model.transformer.layers.9.2.to_out.0.bias\", \"ema_model.transformer.layers.9.3.g\", \"ema_model.transformer.layers.9.4.ff.0.0.weight\", \"ema_model.transformer.layers.9.4.ff.0.0.bias\", \"ema_model.transformer.layers.9.4.ff.2.weight\", \"ema_model.transformer.layers.9.4.ff.2.bias\", \"ema_model.transformer.layers.10.1.g\", \"ema_model.transformer.layers.10.2.to_q.weight\", \"ema_model.transformer.layers.10.2.to_q.bias\", \"ema_model.transformer.layers.10.2.to_k.weight\", \"ema_model.transformer.layers.10.2.to_k.bias\", \"ema_model.transformer.layers.10.2.to_v.weight\", \"ema_model.transformer.layers.10.2.to_v.bias\", \"ema_model.transformer.layers.10.2.to_out.0.weight\", \"ema_model.transformer.layers.10.2.to_out.0.bias\", \"ema_model.transformer.layers.10.3.g\", \"ema_model.transformer.layers.10.4.ff.0.0.weight\", \"ema_model.transformer.layers.10.4.ff.0.0.bias\", \"ema_model.transformer.layers.10.4.ff.2.weight\", \"ema_model.transformer.layers.10.4.ff.2.bias\", \"ema_model.transformer.layers.11.1.g\", \"ema_model.transformer.layers.11.2.to_q.weight\", \"ema_model.transformer.layers.11.2.to_q.bias\", \"ema_model.transformer.layers.11.2.to_k.weight\", \"ema_model.transformer.layers.11.2.to_k.bias\", \"ema_model.transformer.layers.11.2.to_v.weight\", \"ema_model.transformer.layers.11.2.to_v.bias\", \"ema_model.transformer.layers.11.2.to_out.0.weight\", \"ema_model.transformer.layers.11.2.to_out.0.bias\", \"ema_model.transformer.layers.11.3.g\", \"ema_model.transformer.layers.11.4.ff.0.0.weight\", \"ema_model.transformer.layers.11.4.ff.0.0.bias\", \"ema_model.transformer.layers.11.4.ff.2.weight\", \"ema_model.transformer.layers.11.4.ff.2.bias\", \"ema_model.transformer.layers.12.0.weight\", \"ema_model.transformer.layers.12.1.g\", \"ema_model.transformer.layers.12.2.to_q.weight\", \"ema_model.transformer.layers.12.2.to_q.bias\", \"ema_model.transformer.layers.12.2.to_k.weight\", \"ema_model.transformer.layers.12.2.to_k.bias\", \"ema_model.transformer.layers.12.2.to_v.weight\", \"ema_model.transformer.layers.12.2.to_v.bias\", \"ema_model.transformer.layers.12.2.to_out.0.weight\", \"ema_model.transformer.layers.12.2.to_out.0.bias\", \"ema_model.transformer.layers.12.3.g\", \"ema_model.transformer.layers.12.4.ff.0.0.weight\", \"ema_model.transformer.layers.12.4.ff.0.0.bias\", \"ema_model.transformer.layers.12.4.ff.2.weight\", \"ema_model.transformer.layers.12.4.ff.2.bias\", \"ema_model.transformer.layers.13.0.weight\", \"ema_model.transformer.layers.13.1.g\", \"ema_model.transformer.layers.13.2.to_q.weight\", \"ema_model.transformer.layers.13.2.to_q.bias\", \"ema_model.transformer.layers.13.2.to_k.weight\", \"ema_model.transformer.layers.13.2.to_k.bias\", \"ema_model.transformer.layers.13.2.to_v.weight\", \"ema_model.transformer.layers.13.2.to_v.bias\", \"ema_model.transformer.layers.13.2.to_out.0.weight\", \"ema_model.transformer.layers.13.2.to_out.0.bias\", \"ema_model.transformer.layers.13.3.g\", \"ema_model.transformer.layers.13.4.ff.0.0.weight\", \"ema_model.transformer.layers.13.4.ff.0.0.bias\", \"ema_model.transformer.layers.13.4.ff.2.weight\", \"ema_model.transformer.layers.13.4.ff.2.bias\", \"ema_model.transformer.layers.14.0.weight\", \"ema_model.transformer.layers.14.1.g\", \"ema_model.transformer.layers.14.2.to_q.weight\", \"ema_model.transformer.layers.14.2.to_q.bias\", \"ema_model.transformer.layers.14.2.to_k.weight\", \"ema_model.transformer.layers.14.2.to_k.bias\", \"ema_model.transformer.layers.14.2.to_v.weight\", \"ema_model.transformer.layers.14.2.to_v.bias\", \"ema_model.transformer.layers.14.2.to_out.0.weight\", \"ema_model.transformer.layers.14.2.to_out.0.bias\", \"ema_model.transformer.layers.14.3.g\", \"ema_model.transformer.layers.14.4.ff.0.0.weight\", \"ema_model.transformer.layers.14.4.ff.0.0.bias\", \"ema_model.transformer.layers.14.4.ff.2.weight\", \"ema_model.transformer.layers.14.4.ff.2.bias\", \"ema_model.transformer.layers.15.0.weight\", \"ema_model.transformer.layers.15.1.g\", \"ema_model.transformer.layers.15.2.to_q.weight\", \"ema_model.transformer.layers.15.2.to_q.bias\", \"ema_model.transformer.layers.15.2.to_k.weight\", \"ema_model.transformer.layers.15.2.to_k.bias\", \"ema_model.transformer.layers.15.2.to_v.weight\", \"ema_model.transformer.layers.15.2.to_v.bias\", \"ema_model.transformer.layers.15.2.to_out.0.weight\", \"ema_model.transformer.layers.15.2.to_out.0.bias\", \"ema_model.transformer.layers.15.3.g\", \"ema_model.transformer.layers.15.4.ff.0.0.weight\", \"ema_model.transformer.layers.15.4.ff.0.0.bias\", \"ema_model.transformer.layers.15.4.ff.2.weight\", \"ema_model.transformer.layers.15.4.ff.2.bias\", \"ema_model.transformer.layers.16.0.weight\", \"ema_model.transformer.layers.16.1.g\", \"ema_model.transformer.layers.16.2.to_q.weight\", \"ema_model.transformer.layers.16.2.to_q.bias\", \"ema_model.transformer.layers.16.2.to_k.weight\", \"ema_model.transformer.layers.16.2.to_k.bias\", \"ema_model.transformer.layers.16.2.to_v.weight\", \"ema_model.transformer.layers.16.2.to_v.bias\", \"ema_model.transformer.layers.16.2.to_out.0.weight\", \"ema_model.transformer.layers.16.2.to_out.0.bias\", \"ema_model.transformer.layers.16.3.g\", \"ema_model.transformer.layers.16.4.ff.0.0.weight\", \"ema_model.transformer.layers.16.4.ff.0.0.bias\", \"ema_model.transformer.layers.16.4.ff.2.weight\", \"ema_model.transformer.layers.16.4.ff.2.bias\", \"ema_model.transformer.layers.17.0.weight\", \"ema_model.transformer.layers.17.1.g\", \"ema_model.transformer.layers.17.2.to_q.weight\", \"ema_model.transformer.layers.17.2.to_q.bias\", \"ema_model.transformer.layers.17.2.to_k.weight\", \"ema_model.transformer.layers.17.2.to_k.bias\", \"ema_model.transformer.layers.17.2.to_v.weight\", \"ema_model.transformer.layers.17.2.to_v.bias\", \"ema_model.transformer.layers.17.2.to_out.0.weight\", \"ema_model.transformer.layers.17.2.to_out.0.bias\", \"ema_model.transformer.layers.17.3.g\", \"ema_model.transformer.layers.17.4.ff.0.0.weight\", \"ema_model.transformer.layers.17.4.ff.0.0.bias\", \"ema_model.transformer.layers.17.4.ff.2.weight\", \"ema_model.transformer.layers.17.4.ff.2.bias\", \"ema_model.transformer.layers.18.0.weight\", \"ema_model.transformer.layers.18.1.g\", \"ema_model.transformer.layers.18.2.to_q.weight\", \"ema_model.transformer.layers.18.2.to_q.bias\", \"ema_model.transformer.layers.18.2.to_k.weight\", \"ema_model.transformer.layers.18.2.to_k.bias\", \"ema_model.transformer.layers.18.2.to_v.weight\", \"ema_model.transformer.layers.18.2.to_v.bias\", \"ema_model.transformer.layers.18.2.to_out.0.weight\", \"ema_model.transformer.layers.18.2.to_out.0.bias\", \"ema_model.transformer.layers.18.3.g\", \"ema_model.transformer.layers.18.4.ff.0.0.weight\", \"ema_model.transformer.layers.18.4.ff.0.0.bias\", \"ema_model.transformer.layers.18.4.ff.2.weight\", \"ema_model.transformer.layers.18.4.ff.2.bias\", \"ema_model.transformer.layers.19.0.weight\", \"ema_model.transformer.layers.19.1.g\", \"ema_model.transformer.layers.19.2.to_q.weight\", \"ema_model.transformer.layers.19.2.to_q.bias\", \"ema_model.transformer.layers.19.2.to_k.weight\", \"ema_model.transformer.layers.19.2.to_k.bias\", \"ema_model.transformer.layers.19.2.to_v.weight\", \"ema_model.transformer.layers.19.2.to_v.bias\", \"ema_model.transformer.layers.19.2.to_out.0.weight\", \"ema_model.transformer.layers.19.2.to_out.0.bias\", \"ema_model.transformer.layers.19.3.g\", \"ema_model.transformer.layers.19.4.ff.0.0.weight\", \"ema_model.transformer.layers.19.4.ff.0.0.bias\", \"ema_model.transformer.layers.19.4.ff.2.weight\", \"ema_model.transformer.layers.19.4.ff.2.bias\", \"ema_model.transformer.layers.20.0.weight\", \"ema_model.transformer.layers.20.1.g\", \"ema_model.transformer.layers.20.2.to_q.weight\", \"ema_model.transformer.layers.20.2.to_q.bias\", \"ema_model.transformer.layers.20.2.to_k.weight\", \"ema_model.transformer.layers.20.2.to_k.bias\", \"ema_model.transformer.layers.20.2.to_v.weight\", \"ema_model.transformer.layers.20.2.to_v.bias\", \"ema_model.transformer.layers.20.2.to_out.0.weight\", \"ema_model.transformer.layers.20.2.to_out.0.bias\", \"ema_model.transformer.layers.20.3.g\", \"ema_model.transformer.layers.20.4.ff.0.0.weight\", \"ema_model.transformer.layers.20.4.ff.0.0.bias\", \"ema_model.transformer.layers.20.4.ff.2.weight\", \"ema_model.transformer.layers.20.4.ff.2.bias\", \"ema_model.transformer.layers.21.0.weight\", \"ema_model.transformer.layers.21.1.g\", \"ema_model.transformer.layers.21.2.to_q.weight\", \"ema_model.transformer.layers.21.2.to_q.bias\", \"ema_model.transformer.layers.21.2.to_k.weight\", \"ema_model.transformer.layers.21.2.to_k.bias\", \"ema_model.transformer.layers.21.2.to_v.weight\", \"ema_model.transformer.layers.21.2.to_v.bias\", \"ema_model.transformer.layers.21.2.to_out.0.weight\", \"ema_model.transformer.layers.21.2.to_out.0.bias\", \"ema_model.transformer.layers.21.3.g\", \"ema_model.transformer.layers.21.4.ff.0.0.weight\", \"ema_model.transformer.layers.21.4.ff.0.0.bias\", \"ema_model.transformer.layers.21.4.ff.2.weight\", \"ema_model.transformer.layers.21.4.ff.2.bias\", \"ema_model.transformer.layers.22.0.weight\", \"ema_model.transformer.layers.22.1.g\", \"ema_model.transformer.layers.22.2.to_q.weight\", \"ema_model.transformer.layers.22.2.to_q.bias\", \"ema_model.transformer.layers.22.2.to_k.weight\", \"ema_model.transformer.layers.22.2.to_k.bias\", \"ema_model.transformer.layers.22.2.to_v.weight\", \"ema_model.transformer.layers.22.2.to_v.bias\", \"ema_model.transformer.layers.22.2.to_out.0.weight\", \"ema_model.transformer.layers.22.2.to_out.0.bias\", \"ema_model.transformer.layers.22.3.g\", \"ema_model.transformer.layers.22.4.ff.0.0.weight\", \"ema_model.transformer.layers.22.4.ff.0.0.bias\", \"ema_model.transformer.layers.22.4.ff.2.weight\", \"ema_model.transformer.layers.22.4.ff.2.bias\", \"ema_model.transformer.layers.23.0.weight\", \"ema_model.transformer.layers.23.1.g\", \"ema_model.transformer.layers.23.2.to_q.weight\", \"ema_model.transformer.layers.23.2.to_q.bias\", \"ema_model.transformer.layers.23.2.to_k.weight\", \"ema_model.transformer.layers.23.2.to_k.bias\", \"ema_model.transformer.layers.23.2.to_v.weight\", \"ema_model.transformer.layers.23.2.to_v.bias\", \"ema_model.transformer.layers.23.2.to_out.0.weight\", \"ema_model.transformer.layers.23.2.to_out.0.bias\", \"ema_model.transformer.layers.23.3.g\", \"ema_model.transformer.layers.23.4.ff.0.0.weight\", \"ema_model.transformer.layers.23.4.ff.0.0.bias\", \"ema_model.transformer.layers.23.4.ff.2.weight\", \"ema_model.transformer.layers.23.4.ff.2.bias\", \"ema_model.transformer.norm_out.g\". \r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n        size mismatch for ema_model.transformer.input_embed.proj.weight: copying a param with shape torch.Size([1024, 300]) from checkpoint, the shape in current model is torch.Size([1024, 712]).\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/165/comments",
    "author": "GUUser91",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-18T18:37:27Z",
        "body": "make sure you use corrrect model_type  also **F5TTS_Base** or **E2TTS_Base** this you finetune\r\n\r\n"
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T19:02:32Z",
        "body": "@lpscr\r\nI used the finetune_gradio.py file and selected E2TTS_Base for finetuning. Then I clicked on auto settings. Then I click on start training.\r\nI also used finetune-cli.py and the same thing happened."
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:10:36Z",
        "body": "@lpscr\r\nI used the finetune_gradio.py file again and finetuned with the F5TTS_Base model and now I don't have the error if I used this finetuned F5TTS_Base model. But I want to finetune with E2TTS_Base, can you try finetuning with E2TTS_Base on your end to see if it works?"
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-18T20:11:49Z",
        "body": "yes because you need to change in\r\nstart the interface-cli -m E2-TTS\r\n\r\n\r\nparser.add_argument(\r\n    \"-m\",\r\n    \"--model\",\r\n    help=\"F5-TTS | E2-TTS\",\r\n)"
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-18T20:13:30Z",
        "body": "what gpu you have and memory  ? because i working in auto setting , "
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:22:54Z",
        "body": "@lpscr\r\nThat did it. Thank you. I'm using a 4090. I have 64GB Ram."
      }
    ]
  },
  {
    "number": 159,
    "title": "What changed recently? Seems to run even slower now, and doesn't sound as good",
    "created_at": "2024-10-18T12:36:49Z",
    "closed_at": "2024-10-20T19:13:41Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/159",
    "body": "I pulled latest GIT (Oct 18 2024) and really the performance and quality are worse now.\r\n\r\nI also puts audio statements from the input reference audio at the beginning of the clip. It has like zero emotion now, reading very robotically. \r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/159/comments",
    "author": "nikolatesla20",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-18T14:49:30Z",
        "body": "> I also puts audio statements from the input reference audio at the beginning of the clip\r\n\r\nnot quite understand for this part.\r\n\r\nand for the changes made, the remove_silence option is default now, and you may turn it on in advance_setting, also with other controls"
      },
      {
        "user": "sonicnerd14",
        "created_at": "2024-10-19T02:03:29Z",
        "body": "> > I also puts audio statements from the input reference audio at the beginning of the clip\r\n> \r\n> not quite understand for this part.\r\n> \r\n> and for the changes made, the remove_silence option is default now, and you may turn it on in advance_setting, also with other controls\r\n\r\nI noticed this as well. Generations are taking much longer, and the output out is mostly just gibberish and nonsense artifacts. Not sure if they are doing something in the backend with the model, or the hugging face servers are getting hit hard. I'm able to generate better quality right now on my local machine with the same clips and text, then the servers. Only thing is the generation speed is much slower locally on my 3070."
      }
    ]
  },
  {
    "number": 158,
    "title": "请问能分析出文本与 Mel 频谱帧的映射时间关系吗？",
    "created_at": "2024-10-18T10:14:24Z",
    "closed_at": "2024-10-23T05:17:36Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/158",
    "body": "就是生成的Mel 频谱与输入的字符之间的映射关系。",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/158/comments",
    "author": "ABC0408",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:17:34Z",
        "body": "整个模型在做这件事\r\n神经网络可解释性分析是单独的topic，欢迎分享可能的发现"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:17:36Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 157,
    "title": "合成的音频缺失和夹杂参考音频内容",
    "created_at": "2024-10-18T10:11:30Z",
    "closed_at": "2024-10-20T19:22:30Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/157",
    "body": "遇到的几个问题\r\ngradio界面测试\r\n1 合成的音频会在开头缺失几个字的发音(参考音频1s到3s试了几个)\r\n2 以及开头会携带一点参考音频的尾部\r\n3 合成音频中间会夹杂一些参考音频的内容（25s参考音频+长文本）\r\n如题的3个问题，是什么原因呢，有没有什么解决方式",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/157/comments",
    "author": "Ustinianer",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:22:30Z",
        "body": "我们在README里有说明，issue里也有类似的解答\r\n\r\n1. 太短的参考音频会导致简单估算的时长不准，建议单独训练一个时长预测模型以应对该情况\r\n2. 不太建议使用戛然而止的参考音频，最好留有一些空白静音在尾部\r\n3. 过长的参考音频其实会被截断到15秒，参考2.\r\n\r\n一般来说用10秒左右的参考音频，末尾留有一定空白余地会得到比较好的结果\r\n我们的模型还很粗糙，带来使用不便敬请见谅~"
      },
      {
        "user": "Ustinianer",
        "created_at": "2024-10-21T01:07:09Z",
        "body": "> 我们在README里有说明，issue里也有类似的解答\r\n> \r\n> 1. 太短的参考音频会导致简单估算的时长不准，建议单独训练一个时长预测模型以应对该情况\r\n> 2. 不太建议使用戛然而止的参考音频，最好留有一些空白静音在尾部\r\n> 3. 过长的参考音频其实会被截断到15秒，参考2.\r\n> \r\n> 一般来说用10秒左右的参考音频，末尾留有一定空白余地会得到比较好的结果 我们的模型还很粗糙，带来使用不便敬请见谅~\r\n\r\n感谢解答，这对我很有帮助"
      }
    ]
  },
  {
    "number": 156,
    "title": "If I want to customize a model with high sound quality and similar timbre, do I need to fine tune the base model",
    "created_at": "2024-10-18T08:23:17Z",
    "closed_at": "2024-10-23T05:15:13Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/156",
    "body": "If I want to customize a model with high sound quality and similar timbre, do I need to fine tune the base model",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/156/comments",
    "author": "xwan07017",
    "comments": [
      {
        "user": "AndrewBeniston",
        "created_at": "2024-10-18T16:42:11Z",
        "body": "bump"
      },
      {
        "user": "Toolfolks",
        "created_at": "2024-10-18T20:46:22Z",
        "body": "I think I do ... But new to all this cloning lark...\r\nIs this training a model to make it sound more like the original ?????"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:15:13Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 146,
    "title": "Multivoice CLI Similar to Gradio App",
    "created_at": "2024-10-17T16:26:28Z",
    "closed_at": "2024-10-17T17:03:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/146",
    "body": "Inspired by the Gradio app, my attempt to make CLI to generate with multiple voices.\r\nIt's a combination of both podcast and emotion tabs.\r\nNot 100% done, missing the combined spectrogram, but I just wanted to check what you guys think first.\r\n\r\nRun: `python inference-cli.py -c samples/story.toml`\r\nConfiguration looks like this with three voices: main, town, and country.\r\nThe sample files are from vctk dataset.\r\n\r\n```toml\r\nmodel = \"F5-TTS\"\r\nref_audio = \"samples/main.flac\"\r\n# If an empty \"\", transcribes the reference audio automatically.\r\nref_text = \"\"\r\ngen_text = \"\"\r\ngen_file = \"samples/story.txt\"\r\nremove_silence = true\r\noutput_dir = \"samples\"\r\n\r\n[voices.town]\r\nref_audio = \"samples/town.flac\"\r\nref_text = \"\"\r\n\r\n[voices.country]\r\nref_audio = \"samples/country.flac\"\r\nref_text = \"\"\r\n```\r\n\r\nYou mark the voice with [main] [town] [country] whenever you want to change voice.\r\n\r\n```txt\r\nA Town Mouse and a Country Mouse were acquaintances, and the Country Mouse one day invited his friend to come and see him at his home in the fields. The Town Mouse came, and they sat down to a dinner of barleycorns and roots, the latter of which had a distinctly earthy flavour. The fare was not much to the taste of the guest, and presently he broke out with [town] “My poor dear friend, you live here no better than the ants. Now, you should just see how I fare! My larder is a regular horn of plenty. You must come and stay with me, and I promise you you shall live on the fat of the land.” [main] So when he returned to town he took the Country Mouse with him, and showed him into a larder containing flour and oatmeal and figs and honey and dates. The Country Mouse had never seen anything like it, and sat down to enjoy the luxuries his friend provided: but before they had well begun, the door of the larder opened and someone came in. The two Mice scampered off and hid themselves in a narrow and exceedingly uncomfortable hole. Presently, when all was quiet, they ventured out again; but someone else came in, and off they scuttled again. This was too much for the visitor. [country] “Goodbye,” [main] said he, [country] “I’m off. You live in the lap of luxury, I can see, but you are surrounded by dangers; whereas at home I can enjoy my simple dinner of roots and corn in peace.”\r\n```\r\n\r\nI'd appreciate your thoughts.\r\nThanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/146/comments",
    "author": "chigkim",
    "comments": [
      {
        "user": "chigkim",
        "created_at": "2024-10-17T22:20:55Z",
        "body": "@SWivid, there's a probably easy answer, but any idea on how to combined spectrogram?\r\nThe process function loops through each voice tag with:\r\n`audio, spectragram = infer(ref_audio, ref_text, gen_text, model, remove_silence)`\r\nRight now I'm just ignoring the spectragram from infer function.\r\nIs there a way to combine them with numpy? Or, should it just generate new spectragram based on final_wave?\r\n`final_wave = np.concatenate(generated_audio_segments)`\r\nHere are some additional thoughts:\r\nWould it be possible for someone to refactor and come up with common functions that both cli and gradio app share, before they diverged too much?\r\nOther than the fact that podcast tab only works with two voices, is there difference between Podcast and multi-style tabs on Gradio app? They just have different way of marking voices on script, and You should be able to achieve podcast with multi-style as well, right? Am I missing something?\r\nThanks!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-18T05:09:07Z",
        "body": "Yes, maybe someone could help!\r\nwe are just focusing on developping v2 model"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-18T05:27:46Z",
        "body": "@chigkim @jpgallegoar @JarodMica @lpscr @cocktailpeanut \r\nHi there ~\r\n\r\nWe trust your submission very much, because we have limited manpower, so we may not always be able to merge in time.\r\nSo I just sent an invitation to you all to collaborate together.\r\n\r\nFeel free just merge ! I think you are far more familiar with these modules than we are !"
      },
      {
        "user": "JarodMica",
        "created_at": "2024-10-18T07:16:56Z",
        "body": "> @chigkim @jpgallegoar @JarodMica @lpscr @cocktailpeanut Hi there ~\r\n> \r\n> We trust your submission very much, because we have limited manpower, so we may not always be able to merge in time. So I just sent an invitation to you all to collaborate together.\r\n> \r\n> Feel free just merge ! I think you are far more familiar with these modules than we are !\r\n\r\nAppreciate the trust! I don't have the knowledge to fully understand all the architectural stuff going on, but do understand the fine-tuning and training stuff generally so glad to be of help where I can."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-18T23:00:00Z",
        "body": "Dear @SWivid. Have you explored any form of quantization of the model? I'd like to give it a shot but I need the config.json architecture file. I think it could be interesting to explore since current model is full precision 32 bits. Maybe Q8 retains quality being much faster."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-19T07:01:52Z",
        "body": "Hi @jpgallegoar , we have seen through some existing tools to do quantization e.g. gptq, awq, though we felt a little at a loss since we are new to this, and these tools seems mostly take text llm for usage example.\r\nIf you are interested in it, what kind of config.json we could help provide~"
      }
    ]
  },
  {
    "number": 145,
    "title": "Run time error while fine tuning",
    "created_at": "2024-10-17T15:26:06Z",
    "closed_at": "2024-10-21T07:00:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/145",
    "body": "I am trying to fine tune the model. And I am stuck with this error \r\n\r\n>`Epoch 1/10:  23% 34/145 [00:24<01:21,  1.36step/s, loss=1.92, step=34]\r\nTraceback (most recent call last):\r\n  File \"/content/F5-TTS/train.py\", line 94, in <module>\r\n    main()\r\n  File \"/content/F5-TTS/train.py\", line 88, in main\r\n    trainer.train(train_dataset,\r\n  File \"/content/F5-TTS/model/trainer.py\", line 229, in train\r\n    loss, cond, pred = self.model(mel_spec, text=text_inputs, lens=mel_lengths, noise_scheduler=self.noise_scheduler)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 820, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/cfm.py\", line 273, in forward\r\n    pred = self.transformer(x = φ, cond = cond, text = text, time = time, drop_audio_cond = drop_audio_cond, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 162, in forward\r\n    text_embed = self.text_embed(text, seq_len, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 57, in forward\r\n    text = self.text_embed(text) # b n -> b n d\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\", line 164, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2267, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)`\r\n\r\n\r\n\r\n\r\nthese are the inputs I am using.\r\n\r\ntarget_sample_rate = 24000\r\nn_mel_channels = 100\r\nhop_length = 256\r\n\r\ntokenizer = \"pinyin\" # 'pinyin', 'char', or 'custom'\r\ntokenizer_path = None # if tokenizer = 'custom', define the path to the tokenizer you want to use (should be vocab.txt)\r\ndataset_name = \"My_Dataset\"\r\n\r\n# -------------------------- Training Settings -------------------------- #\r\n\r\nexp_name = \"E2TTS_Base\"  # F5TTS_Base | E2TTS_Base\r\n\r\nlearning_rate = 5e-06\r\n\r\nbatch_size_per_gpu = 38400  # 8 GPUs, 8 * 38400 = 307200\r\nbatch_size_type = \"frame\"  # \"frame\" or \"sample\"\r\nmax_samples = 2  # max sequences per batch if use frame-wise batch_size. we set 32 for small models, 64 for base models\r\ngrad_accumulation_steps = 1  # note: updates = steps / grad_accumulation_steps\r\nmax_grad_norm = 1.\r\n\r\nepochs = 10  # use linear decay, thus epochs control the slope\r\nnum_warmup_updates = 20  # warmup steps\r\nsave_per_updates = 500  # save checkpoint per steps\r\nlast_per_steps = 5000  # save last checkpoint per steps\r\n\r\n\r\n\r\nIs this error something related to my dataset or my input? Can anyone help?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/145/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-19T02:12:00Z",
        "body": "'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\r\n\r\nmaybe you need to check the tensor's type"
      },
      {
        "user": "MilanaShhanukova",
        "created_at": "2024-10-19T19:48:36Z",
        "body": "@rasheed-aidetic I may guess that you have an empty text sample, check it out. If it is intended, you may add the type converter in TextEmbedding\r\n\r\n```\r\n        if text.dtype is not torch.long:\r\n            text = text.long()\r\n```\r\n\r\n"
      },
      {
        "user": "rasheed-aidetic",
        "created_at": "2024-10-21T07:00:01Z",
        "body": "Thank you so much @MilanaShhanukova"
      }
    ]
  },
  {
    "number": 144,
    "title": "Shape mismatch error while fine tuning (non-singleton dimension 1)",
    "created_at": "2024-10-17T12:37:13Z",
    "closed_at": "2024-10-17T14:56:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/144",
    "body": "Hi This project is incredible guys. Great work.\r\n\r\nI am trying to finetune this model and facing an error on shape mismatch. Can anyone help?\r\n\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\r\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/content/F5-TTS/model/dataset.py\", line 117, in __getitem__\r\n    mel_spec = rearrange(mel_spec, '1 d t -> d t')\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\r\n    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\r\n    raise EinopsError(message + \"\\n {}\".format(e))\r\neinops.EinopsError:  Error while processing rearrange-reduction pattern \"1 d t -> d t\".\r\n Input tensor shape: torch.Size([2, 100, 1407]). Additional info: {}.\r\n Shape mismatch, 2 != 1`\r\n\r\nI am trying to fine tune the model with my own voice model. I have created the dataset using the prepare_csv_wavs.py script. Please let me know if I need to make any changes in the input section of train.py\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/144/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-17T12:57:14Z",
        "body": "i think the problem is you have stereo you need to  resample your audio to **mono** **24000 hz**\r\n\r\nrun this script please first make back up because replace your original files !\r\n\r\n```\r\nimport os\r\nimport glob\r\nfrom pydub import AudioSegment\r\n\r\ndef convert_wav_to_mono(folder_path):\r\n    wav_files = glob.glob(os.path.join(folder_path, '*.wav'))\r\n    for file_path in wav_files:\r\n        filename = os.path.basename(file_path)\r\n        audio = AudioSegment.from_wav(file_path)\r\n        mono_audio = audio.set_channels(1)\r\n        mono_audio = mono_audio.set_frame_rate(24000)\r\n        new_file_path = os.path.join(folder_path, f\"mono_{filename}\")\r\n        mono_audio.export(new_file_path, format=\"wav\")\r\n\r\nfolder_path = 'your_folder_path'\r\nconvert_wav_to_mono(folder_path)\r\n```\r\n"
      },
      {
        "user": "rasheed-aidetic",
        "created_at": "2024-10-17T14:56:13Z",
        "body": "Working after the change. Thank you so much for the quick reply."
      }
    ]
  },
  {
    "number": 142,
    "title": "Output consistency",
    "created_at": "2024-10-17T10:22:51Z",
    "closed_at": "2024-10-21T13:21:02Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/142",
    "body": "I'm developing a tool to create audiobooks, and I've been working on replacing my existing backend for speech generation with F5-TTS.\r\n\r\nWhen comparing it to what I'm currently using (XTTS), the speech quality of F5-TTS is just as good, if not better. The performance is also acceptable, though F5-TTS is around 50% slower than XTTS in the generation process when using CUDA.\r\n\r\nHowever, I'm facing some issues with output consistency, which is preventing me from moving forward:\r\n\r\n- Occasionally, there's random noise at the start of the generated speech.\r\n- At times, the start of the speech is delayed by a random amount (between 0.3 and 0.5 seconds).\r\n\r\nSince I'm creating audiobooks with my tool, I need to process thousands of speech generations. I can't afford to manually adjust each one, as it would make the process too time-consuming. XTTS offers strong consistency in its outputs, and this is crucial for my workflow.\r\n\r\nCurrently, I've set up a Flask endpoint service using your \"inference-cli.py\" script. Is there anything I can do to improve the output consistency?\r\n\r\nP.S. Congrats on this fantastic project! The field of speech synthesis really needed more strong players.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/142/comments",
    "author": "EstebanGameDevelopment",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-17T13:11:59Z",
        "body": "about the  speed up \r\nyou can  change in **inference-cli.py**  the **nfe_step = 32** to something low and see what working good for me also **24** it's good and  **16** lower you lost a alot quality"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T13:21:00Z",
        "body": "will close this issue, feel free to open if further questions ~"
      }
    ]
  },
  {
    "number": 138,
    "title": "中文中带有数字时的中文数字发音为英文, 请问如何解决。",
    "created_at": "2024-10-17T08:03:05Z",
    "closed_at": "2024-10-19T02:13:11Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/138",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/138/comments",
    "author": "pengyong94",
    "comments": [
      {
        "user": "ABC0408",
        "created_at": "2024-10-17T08:49:18Z",
        "body": "数字转成汉字"
      },
      {
        "user": "pengyong94",
        "created_at": "2024-10-17T13:20:28Z",
        "body": "感谢，我试下看"
      }
    ]
  },
  {
    "number": 135,
    "title": "Finetune has error when using accelerate",
    "created_at": "2024-10-17T02:45:01Z",
    "closed_at": "2024-10-17T06:08:57Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/135",
    "body": "Thanks you for great works.\r\nI am trying to finetune F5TTS_Base model with model_1200000.pt pretrained model. I followed your steps but i found that I cannot load pretrained model with \"accelerate launch train.py\" cmd, all layers of the model have shape: torch.Size([0]). It causes mismatch size between current model and pretrained model. \r\nBut when I use \"python train.py\", it run correctly on single GPU.\r\nCould you please help me with this problem? \r\nThank you in advance!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/135/comments",
    "author": "zaidato",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-17T03:20:19Z",
        "body": "no clear idea for it, maybe check accelerate config first, see if it's all right\r\ngenerally if `python train.py` works, `accelerate launch train.py` will work"
      },
      {
        "user": "zaidato",
        "created_at": "2024-10-17T03:28:01Z",
        "body": "This is my default_config.yaml \r\n\r\ncompute_environment: LOCAL_MACHINE\r\ndebug: true\r\ndistributed_type: FSDP\r\ndowncast_bf16: 'no'\r\nenable_cpu_affinity: false\r\nfsdp_config:\r\n  fsdp_activation_checkpointing: false\r\n  fsdp_auto_wrap_policy: NO_WRAP\r\n  fsdp_backward_prefetch: BACKWARD_PRE\r\n  fsdp_cpu_ram_efficient_loading: true\r\n  fsdp_forward_prefetch: true\r\n  fsdp_offload_params: false\r\n  fsdp_sharding_strategy: FULL_SHARD\r\n  fsdp_state_dict_type: FULL_STATE_DICT\r\n  fsdp_sync_module_states: true\r\n  fsdp_use_orig_params: true\r\nmachine_rank: 0\r\nmain_training_function: main\r\nmixed_precision: bf16\r\nnum_machines: 1\r\nnum_processes: 2\r\nrdzv_backend: static\r\nsame_network: true\r\ntpu_env: []\r\ntpu_use_cluster: false\r\ntpu_use_sudo: false\r\nuse_cpu: false\r\n\r\nDo you have any idea?\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-17T03:58:20Z",
        "body": "not quite sure if FSDP works, we haven't tried it.\r\nmaybe just multi-gpu"
      },
      {
        "user": "zaidato",
        "created_at": "2024-10-17T04:16:04Z",
        "body": "thank you for your help. FSDP causes the error.\r\nIt works fine now"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-17T06:08:54Z",
        "body": "I will close this issue. If you have other questions, you can reopen this issue.\r\nBTW, here is my config:\r\n```yaml\r\ncompute_environment: LOCAL_MACHINE\r\ndebug: false\r\ndistributed_type: MULTI_GPU\r\ndowncast_bf16: 'no'\r\nenable_cpu_affinity: true\r\ngpu_ids: all\r\nmachine_rank: 0\r\nmain_training_function: main\r\nmixed_precision: fp16\r\nnum_machines: 1\r\nnum_processes: 2\r\nrdzv_backend: static\r\nsame_network: true\r\ntpu_env: []\r\ntpu_use_cluster: false\r\ntpu_use_sudo: false\r\nuse_cpu: false\r\n```"
      }
    ]
  },
  {
    "number": 121,
    "title": "效果问题",
    "created_at": "2024-10-16T09:41:31Z",
    "closed_at": "2024-10-16T15:52:59Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/121",
    "body": "1、reference audio 如果超过20s ，生成的结果会胡言乱语\r\n2、-gen_text中如果出现了 时间， 不能正确读出来",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/121/comments",
    "author": "Thekey756",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T09:54:55Z",
        "body": "在readme有说明，训练集只见过30秒\r\n所以生成的时候只支持总时长<30s\r\n\r\n没怎么做细致的文本正则，可以考虑自定义一个把时间做下预处理"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T15:52:59Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      },
      {
        "user": "walkingwithGod2017",
        "created_at": "2024-10-17T11:14:38Z",
        "body": "> 在readme有说明，训练集只见过30秒 所以生成的时候只支持总时长<30s\r\n> \r\n> 没怎么做细致的文本正则，可以考虑自定义一个把时间做下预处理\r\n\r\n朋友您好，感谢你们在这个项目的出色工作。针对这个项目，我测试了一些音色，包括马保国和星爷的，整体上感觉短音频的效果非常好，声音相似度和语气都非常不错，尤其是相似度。但是一旦稍微长点之后，音色就会非常不像，就是那种标准的剪映风格的音色，语气会稍微差一点儿，主要是音色不像了。\r\n整体上还是优秀的，刚开始，祝后续能越来越好！"
      }
    ]
  },
  {
    "number": 120,
    "title": "how to make it run faster",
    "created_at": "2024-10-16T09:13:26Z",
    "closed_at": "2024-10-21T13:08:20Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/120",
    "body": "Inferring that 30 seconds of audio takes about 8 seconds, how to make it faster?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/120/comments",
    "author": "xwan07017",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-16T14:43:29Z",
        "body": "hi @xwan07017 you can change setting inside to script **[inference-cli.py]** , **[gradio_app.py]**\r\n\r\n**nfe_step** = **32**  # to **24**  or **16** see what best working for you"
      },
      {
        "user": "CreepJoye",
        "created_at": "2024-10-20T12:13:52Z",
        "body": "@lpscr Thank you for your great job,I also want to make it faster and I have tried to set nfe_step = 16 and it works.However,I want to know why the F5-TTS base model has 22 layers?Can simply I reduce it for a faster inference?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-20T12:17:39Z",
        "body": "> why the F5-TTS base model has 22 layers\r\n\r\nit is to scale total params ~335M, while E2 has 24 layers lol\r\nin order to have a fair comparison"
      },
      {
        "user": "CreepJoye",
        "created_at": "2024-10-20T14:11:44Z",
        "body": "> > why the F5-TTS base model has 22 layers\r\n> \r\n> it is to scale total params ~335M, while E2 has 24 layers lol in order to have a fair comparison\r\n\r\nI see，thank you very much！"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T13:08:20Z",
        "body": "will close this issue, feel free to open if any questions."
      }
    ]
  },
  {
    "number": 117,
    "title": "Python version",
    "created_at": "2024-10-16T08:06:20Z",
    "closed_at": "2024-10-16T08:13:26Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/117",
    "body": "It would be a good idea to specify the version of python. From my tests the project works for` >3.9`",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/117/comments",
    "author": "Mateleo",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T08:11:48Z",
        "body": "yes, we have the badge in readme showing that python 3.10 is recommended"
      },
      {
        "user": "Mateleo",
        "created_at": "2024-10-16T08:13:26Z",
        "body": "Oh thks !"
      }
    ]
  },
  {
    "number": 116,
    "title": "Update train.py",
    "created_at": "2024-10-16T07:30:52Z",
    "closed_at": "2024-10-16T07:35:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/116",
    "body": "Hello,\r\ni tried adding some changes that are,\r\nCommand-Line Argument Parsing which Allows flexible configuration without modifying the script directly.\r\nError Handling which Added checks to ensure paths exist and catch exceptions when loading the tokenizer and dataset.\r\nImproved Logging to Provides clear messages about the training process.\r\nParameterization for More parameters are passed through the command line for ease of use.\r\nplease update me if there will be anything,\r\nThank You.\r\nRajat Mishra.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/116/comments",
    "author": "mst-rajatmishra",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T07:35:47Z",
        "body": "thought some key hyper params are just fixed in your modified version.\r\nmaybe we could just keep it simple as we haven't reached to a phase with critical need of e.g. `omegaconf`"
      },
      {
        "user": "mst-rajatmishra",
        "created_at": "2024-10-16T09:04:52Z",
        "body": "cool, Thanks,\r\nAll The Best!!\r\n"
      }
    ]
  },
  {
    "number": 115,
    "title": "Problems In Fine Tuning",
    "created_at": "2024-10-16T05:56:32Z",
    "closed_at": "2024-10-16T08:46:30Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/115",
    "body": "Help, when fine-tuning, I have the following problems:\r\n\r\n RuntimeError: Error(s) in loading state_dict for EMA:\r\nsize mismatch for ema_model.transformer.text_embed.text_embed.weight:  copying a param with shape torch.Size([2546, 512]) from checkpoint, the shape in current model is torch.Size([259, 512])",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/115/comments",
    "author": "wen0320",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T06:07:43Z",
        "body": "remember to reuse the `vocab.txt` compatible with pretrained model\r\nyou could simply get it at `data/Emilia_ZH_EN_pinyin/vocab.txt`"
      },
      {
        "user": "wen0320",
        "created_at": "2024-10-16T08:46:28Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 114,
    "title": "HuggingFace App Problem",
    "created_at": "2024-10-16T04:39:50Z",
    "closed_at": "2024-10-16T15:53:58Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/114",
    "body": "I tried the demo in your hugging face, but the result of F5 TTS can't speak the text in a right way, but E5 TTS can, may be the duration preditor and the text prompt part has some bugs to fix.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/114/comments",
    "author": "JohnFengNeumann",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T04:44:51Z",
        "body": "maybe you could provide us some generated samples to figure out"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T15:53:58Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      }
    ]
  },
  {
    "number": 110,
    "title": "Offloading Whisper to CPU",
    "created_at": "2024-10-16T02:18:29Z",
    "closed_at": "2024-10-16T05:08:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/110",
    "body": "I'm using the Gradio app on a laptop with 6 GB VRAM.\n\nWith the current settings, the model overflows into system RAM reducing generation speed significantly, but if I offload Whisper to CPU, it fits in about 5.2 GB. Can we have a cmdline swich / config file setting / in-app setting to do this automatically? Or perhaps unload Whisper when not in use?\n\nThe model itself is amazing by the way.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/110/comments",
    "author": "athu16",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T03:02:54Z",
        "body": "Thanks for using. `inference-cli.py` may with your need"
      },
      {
        "user": "athu16",
        "created_at": "2024-10-16T05:08:40Z",
        "body": "Thanks, just what I needed!"
      }
    ]
  },
  {
    "number": 107,
    "title": "cli interface seems to stammer",
    "created_at": "2024-10-15T22:00:25Z",
    "closed_at": "2024-10-20T19:14:51Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/107",
    "body": "gradio version works well, but cli version seems to  have problems. \r\n```\r\npython inference-cli.py --model \"F5-TTS\" --ref_audio ./voices/fromTort/favorites/patrick_stewart_omicron.mp3 --gen_text \"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring.\"\r\n```\r\ngithub won't let me upload voice sample. \r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/107/comments",
    "author": "iplayfast",
    "comments": [
      {
        "user": "chigkim",
        "created_at": "2024-10-15T23:24:55Z",
        "body": "> github won't let me upload voice sample.\r\n\r\nMaybe link to Dropbox, WeTransfer, etc?"
      }
    ]
  },
  {
    "number": 104,
    "title": "Mimic real human like conversation ",
    "created_at": "2024-10-15T17:49:06Z",
    "closed_at": "2024-10-17T06:16:52Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/104",
    "body": "Include something like the following to mimic human like conversation like bark you can add the following to make conversations real, [laughter] [laughs] [sighs] [music] [gasps] [clears throat] — or ... for hesitations ♪ for song lyrics CAPITALIZATION for emphasis of a word ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/104/comments",
    "author": "kentyler2",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-15T18:45:43Z",
        "body": "This is not possible without retraining or fine-tuning, so it's out of scope for now"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-16T00:32:53Z",
        "body": "Our training data not include this non-verbal token, eg:[laughter] [laughs] [sighs] [music] [gasps] [clears throat]. But you can annotate some data and finetune F5TTS"
      },
      {
        "user": "sidharthrajaram",
        "created_at": "2024-10-16T03:02:20Z",
        "body": "When I include non-verbal tokens like that like \"[laughter] The sky is indeed blue.\", the generated audio produced sounds like \"laughter, the sky is indeed blue\". The non-verbal token is being spoken aloud. Any way to better avoid that?\r\n\r\nThanks "
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T03:25:41Z",
        "body": "> When I include non-verbal tokens like that like \"[laughter] The sky is indeed blue.\", the generated audio produced sounds like \"laughter, the sky is indeed blue\". The non-verbal token is being spoken aloud. Any way to better avoid that?\r\n> \r\n> Thanks\r\n\r\nwe have not explicitly include non-verbal control in training, but there exists such acoustic features in dataset.\r\nso it needs finetuning"
      },
      {
        "user": "sidharthrajaram",
        "created_at": "2024-10-16T21:17:12Z",
        "body": "Ah, I see. thank you @SWivid "
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-17T06:16:50Z",
        "body": "I will close this issue. If you have other questions, you can reopen this issue."
      }
    ]
  },
  {
    "number": 101,
    "title": "Make sure to provide either `input_features` or `encoder_outputs` to `generate`",
    "created_at": "2024-10-15T14:46:35Z",
    "closed_at": "2024-10-16T15:56:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/101",
    "body": "Traceback (most recent call last):\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\gradio\\queueing.py\", line 622, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\gradio\\blocks.py\", line 2016, in process_api\r\n    result = await self.call_function(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\gradio\\blocks.py\", line 1569, in call_function\r\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\r\n    result = context.run(func, *args)\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"C:\\Users\\谦以自强\\F5-TTS\\gradio_app.py\", line 326, in infer\r\n    ref_text = pipe(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 284, in __call__\r\n    return super().__call__(inputs, **kwargs)\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1260, in __call__\r\n    return next(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 269, in __next__\r\n    processed = self.infer(next(self.iterator), **self.params)\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1175, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 512, in _forward\r\n    tokens = self.model.generate(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py\", line 507, in generate\r\n    batch_size, total_input_frames = self._retrieve_total_input_frames(\r\n  File \"D:\\Aniconda\\envs\\f5-tts\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py\", line 1126, in _retrieve_total_input_frames\r\n    raise ValueError(\"Make sure to provide either `input_features` or `encoder_outputs` to `generate`.\")\r\nValueError: Make sure to provide either `input_features` or `encoder_outputs` to `generate`.   How to solve?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/101/comments",
    "author": "crush119",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-15T14:55:47Z",
        "body": "what is your version of gradio and transformer\r\nand also python"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T15:56:08Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      }
    ]
  },
  {
    "number": 98,
    "title": "这简直就是一个学术造假。",
    "created_at": "2024-10-15T12:08:15Z",
    "closed_at": "2024-10-15T13:01:19Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/98",
    "body": "放出来的模型和论文的例子就是两个东西。要么是没有开源，要么就是造假。\r\n丢交大的脸啊。",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/98/comments",
    "author": "dnlzsy",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-15T12:30:07Z",
        "body": "确实 看看你的😀"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-15T13:01:19Z",
        "body": "Closing this issue as it is consuming resources that could be better utilized elsewhere."
      },
      {
        "user": "heiway",
        "created_at": "2024-10-21T08:49:18Z",
        "body": "假在哪里，效果相当不错，生成速度也很快。感谢作者分享"
      }
    ]
  },
  {
    "number": 97,
    "title": "Split dependencies, late-import bits",
    "created_at": "2024-10-15T11:43:12Z",
    "closed_at": "2024-10-15T12:47:06Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/97",
    "body": "This PR allows installing this repository without having to install as many dependencies.\r\n\r\nNamely, doing `pip install -r requirements-infer.txt -r requirements-en.txt` will be enough to install inference dependencies for English. (I tried this out with a fresh venv on my laptop.)\r\n\r\nTrying to do `zh` inference, or training, with this set of requirements installed, will fail with an `ImportError` sooner or later.\r\n\r\n`pip install -r requirements.txt` should still work as before.\r\n\r\nThank you for your work, the results are impressive!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/97/comments",
    "author": "akx",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-15T12:47:06Z",
        "body": "Thanks a lot for your careful consideration and we appreciate your recognition of our efforts~\r\n\r\nWe have not received much feedback regarding installation failures, so we would like to maintain the current installation process as simple for main branch.\r\n\r\nIf you have any questions, welcome to open an issue~"
      },
      {
        "user": "akx",
        "created_at": "2024-10-15T12:53:26Z",
        "body": "Hi, thank you for the reply.\r\n\r\nThis does not change the default installation instructions at all. You can still install requirements with `pip install -r requirements.txt`, just as before. It will install approximately 165 packages on Python 3.12.\r\n\r\nInstalling the inference requirements only, `pip install -r requirements-infer.txt -r requirements-en.txt`, installs 104 packages.\r\n\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-15T13:18:58Z",
        "body": "Hi @akx, we just split some eval pkgs from `requirements.txt` to `requirements_eval.txt`\r\nHope it would be better that doesn't install all packages or split many txt files"
      }
    ]
  },
  {
    "number": 96,
    "title": "Why convnext instead of transformer?",
    "created_at": "2024-10-15T11:36:03Z",
    "closed_at": "2024-10-16T15:56:28Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/96",
    "body": "it seems transformer architectural difference is small(alibi -> rotary, concat time emb -> AdaLN-Zero, no u-net connection)\r\n\r\nwhy not transformer instead of ConvNeXt? did you guys. test it, and convnext was better?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/96/comments",
    "author": "sphmel",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-15T12:25:50Z",
        "body": "welcome to make this ablation\r\n\r\nwe have tested with mmdit and results in failures, which is a double-stream transformer\r\nmaybe something like fluxmusic modification suits more for tts task\r\n\r\nthought a minor difference in architecture will change a lot, otherwise it's just as simple as you could imagine to reproduce e2"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T15:56:28Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      }
    ]
  },
  {
    "number": 95,
    "title": "Possible to bump numpy<=1.26?",
    "created_at": "2024-10-15T10:53:21Z",
    "closed_at": "2024-10-15T11:15:35Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/95",
    "body": "The current requirements won't install with MacOS 15 Python 3.12 because numpy==1.23.5 requirement.\r\nIf there's no specific problem, can we increase to numpy<=1.26?\r\nIt seems to run fine from my end.\r\nThanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/95/comments",
    "author": "chigkim",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-15T11:15:35Z",
        "body": "Thx~ 423fe4a0a5aab14488df397bf25b0d6f413e6d0a"
      }
    ]
  },
  {
    "number": 94,
    "title": "default to weights_only=True for safer loading",
    "created_at": "2024-10-15T07:39:30Z",
    "closed_at": "2024-10-16T09:03:57Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/94",
    "body": "Given that people may be sharing around models, weights_only=True may be a safer way to ensure only weights are loaded as it is not a default torch option yet.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/94/comments",
    "author": "JarodMica",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-15T07:52:50Z",
        "body": "@JarodMica Thanks, not sure if something like 'step' in the dict will be dropped or not.\r\nmay you could help us check, if is fine with normal training process, will just merge~"
      },
      {
        "user": "JarodMica",
        "created_at": "2024-10-15T08:25:02Z",
        "body": "> @JarodMica Thanks, not sure if something like 'step' in the dict will be dropped or not. may you could help us check, if is fine with normal training process, will just merge~\r\n\r\nSure, I'll check it out and report back after the epoch on my latest training run finishes through"
      },
      {
        "user": "JarodMica",
        "created_at": "2024-10-16T08:58:32Z",
        "body": "Alright, so training is running with weight_only=True.  It seems to be chugging along as normal.\r\n\r\nHowever, two things to note: I have not tried to inference with a model out of this training yet and I am running this on an older version of the main branch, though, I don't think any of the training functions have changed.  "
      }
    ]
  },
  {
    "number": 79,
    "title": "Reorganize Gradio app",
    "created_at": "2024-10-14T20:30:59Z",
    "closed_at": "2024-10-14T20:34:33Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/79",
    "body": "Hi,\r\nThis PR reorganizes the Gradio app, by moving the separate features into tabs.\r\nThanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/79/comments",
    "author": "fakerybakery",
    "comments": [
      {
        "user": "fakerybakery",
        "created_at": "2024-10-14T20:38:15Z",
        "body": "Also, do you think it would make sense to directly sync the GitHub repo to the HF Space?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T20:46:50Z",
        "body": "it would be nice.\r\nI just send you an invite to collaborate, since I'm not familiar with it haha"
      }
    ]
  },
  {
    "number": 78,
    "title": "Weird Voice Change",
    "created_at": "2024-10-14T19:21:21Z",
    "closed_at": "2024-10-16T15:53:11Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/78",
    "body": "Any idea why the pitch/voice changes for the following sentence? It works fine with everything else. \r\n\r\n```\r\npython inference-cli.py \\\r\n--model \"F5-TTS\" \\\r\n--ref_audio \"tests/ref_audio/test_en_1_ref_short.wav\" \\\r\n--ref_text \"Some call me nature, others call me mother nature.\" \\\r\n--gen_text \"Thanks for having me\\!\"\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/78/comments",
    "author": "imomin",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T20:03:39Z",
        "body": "maybe some generated samples for us to figure out ~"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T15:53:12Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      },
      {
        "user": "imomin",
        "created_at": "2024-10-16T15:54:50Z",
        "body": "@SWivid any idea, if the model needs to be fine-tuned for the above phase with a better audio? "
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-16T16:30:35Z",
        "body": "if you mean high resolution audio or a tailored speaker style, yes."
      }
    ]
  },
  {
    "number": 76,
    "title": "Numpy version",
    "created_at": "2024-10-14T19:00:04Z",
    "closed_at": "2024-10-14T19:29:46Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/76",
    "body": "Why isn't the numpy version set to 1.23.5 in the requirements?\r\n1.22.0 is mentioned in the readme, but the one that seems compatible is 1.23.5\r\n\r\nAlso prolly better to install torch first, then the requirements, as they will install torch as well but likely the wrong version",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/76/comments",
    "author": "AWAS666",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T19:29:46Z",
        "body": "Thanks for mentioning.\r\ntorch installation instruct placed to first now.\r\nnot sure if numpy==1.23.5 compatible for everyone, as may exist other pkg conflicts"
      },
      {
        "user": "AWAS666",
        "created_at": "2024-10-14T19:33:52Z",
        "body": "On windows 10 at least it installs numpy 2 which is probably the least compatible.\n\n1.22 had some incompatibility with one library but I didn't check which and went with it's required 1.23.5\n\nMight be good to have at least some version in there else it might be bad for everyone"
      }
    ]
  },
  {
    "number": 74,
    "title": "Audio splitting is broken since the \"minor fix\" commit.",
    "created_at": "2024-10-14T17:48:32Z",
    "closed_at": "2024-10-14T18:18:19Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/74",
    "body": "The changes made in the \"minor fix\" commit have broken the audio splitting for batch processing. Now small prompt get split up in lots of batches, ruining inference quality.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/74/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T17:54:37Z",
        "body": "Ah yes, mistaken the sampling rate.\r\nwill it be fine with the lastest pull?\r\n\r\nwas trying to rule the max_chars with a max length of 30s generation rather than counting on characters"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-14T17:57:46Z",
        "body": "Yes, it is solved :D"
      }
    ]
  },
  {
    "number": 71,
    "title": "Added multiple speech types generation",
    "created_at": "2024-10-14T16:08:41Z",
    "closed_at": "2024-10-14T16:20:23Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/71",
    "body": "I added a new \"Emotional Text-to-Speech Generation\" feature to the Gradio app, which lets users upload different audio clips for various speech styles.\r\n\r\nThe feature includes options to dynamically add and remove custom speech types. For example, users can upload reference audios for emotions like Regular, Whisper, Scream, Angry, Sad, Surprised, and more. Each type includes a reference audio, text input, and a delete option for easy management. You can also add new speech types using the \"Add Speech Type\" button.\r\n\r\nTo ensure smooth functionality, I implemented input validation that disables the \"Generate Emotional Speech\" button if the text contains emotions not defined by the user. The input text format looks like (Emotion) Text, and the system will generate speech based on the audio associated with that emotion. If no emotion is specified, it defaults to \"Regular.\"\r\n\r\nThroughout this process, I made sure everything stays consistent with the existing TTS and Podcast generation features for seamless integration into the app.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/71/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T16:20:20Z",
        "body": "Thanks !"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T17:28:26Z",
        "body": "@jpgallegoar seems missing `parse_emotional_text` func in gradio_app.py, maybe you could help with it"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-14T17:34:37Z",
        "body": "> @jpgallegoar seems missing `parse_emotional_text` func in gradio_app.py, maybe you could help with it\r\n\r\nI'm very sorry, don't know how that slipped. I added it back in a new PR\r\n"
      }
    ]
  },
  {
    "number": 70,
    "title": "Will not work without reference text.",
    "created_at": "2024-10-14T15:54:26Z",
    "closed_at": "2024-10-25T06:39:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/70",
    "body": "Any time I try to generate audio without a reference text (using gradio) I get the following message:\r\n\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n\r\nAnd that's it we live here now, until we shutdown.   Not really a big deal to give it reference text, but cleary it is intended to work without.  \r\n\r\nWindows 10, AMD 5700xt, python 3.10.6  ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/70/comments",
    "author": "KungFuFurniture",
    "comments": [
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-14T16:13:10Z",
        "body": "I don't get this error. Please try deleting and re-cloning the repository."
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-14T16:41:58Z",
        "body": "Just tried complete re-clone, re-install, no difference...  Below is the full warning/error message. \r\n\r\nD:\\Games\\F5\\F5-TTS\\env\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n  warnings.warn(\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n\r\nI am more than happy to try any ideas.\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T16:51:26Z",
        "body": "As it seems like a warning rather than error, could you get a proper generation result or fail with it"
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-14T20:35:08Z",
        "body": "What I pasted in the last message is it.  That's all it says.  Big generation fail, it will just stay on that message forever.  Gradio still thinks its working, but it never does work.  The pc is not working, nothing.  Again, if I supply the reference text, it seems to work. Takes a min or 2 on cpu but it works."
      },
      {
        "user": "chrisbward",
        "created_at": "2024-10-15T21:20:33Z",
        "body": "Also got the same issue, the warning comes up and then \"segmentation fault\", the program exits.\r\n\r\nIf I add the transcribed text, it works - some other issues arise with the generated audio though"
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-15T22:16:38Z",
        "body": "> Also got the same issue, the warning comes up and then \"segmentation fault\", the program exits.\r\n> \r\n> If I add the transcribed text, it works - some other issues arise with the generated audio though\r\n\r\nWhat other issues are you getting.  I also noticed with transcribed text, the F5 model gets to a point where it says converting from fp32 to fp16 and never moves on.  The E5 model does great with transcribed audio, however if I use a full 15 seconds, and matching text, the audio is quite fast, it slows down a bit at 12 seconds of source audio.  \r\n\r\nI also installed pinikio cause I saw this was on there.  Installed this through Pinikio as well, got the exact same results."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-18T15:46:26Z",
        "body": "Hello, are you still getting the same issues?"
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-18T16:18:37Z",
        "body": "> Hello, are you still getting the same issues?\r\n\r\nYes, as you ask, I did a git pull, and tried again. (Not the pinikio version I mentioned), and still the same result.  \r\n```\r\ngeneration_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n  warnings.warn(\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n```\r\nonce here there is no cpu/gpu usage at all.  We are just hanging."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-18T16:28:04Z",
        "body": "Maybe there is a problem with the whisper model, did you try deleting it and letting the apo redownload it? The model is in another folder so it doesnt get deleted when you redownload the repo "
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-18T16:35:17Z",
        "body": "Where would I find that model?"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-18T16:39:14Z",
        "body": "In my computer it's at:\r\n\r\nC:\\Users\\thega\\.cache\\huggingface\\hub"
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-18T20:47:57Z",
        "body": "That's exactly where mine was,  Why do you have my user name...  Just kidding.  It was there, started fresh, same exact results.   However, as I started fresh I got all sorts of dependency issues.  The version of faster-whisper recomended requires a lower version of tokenizers. (0.15.2) but the the app won't even start with that version of tokenizers, and the up to date version of transformers won't work with 0.15.2.  I am not sure how far back in transformers we need to go.  I gave up after a few tries before I broke everything.  Transfomers installs the tokenizers it needs, but each of my attempts was the wrong tokenizer.   If someone else thinks I am down the right rabbit hole I'll keep trying.  But, it feels like I might be making a bigger mess than is actually present."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-18T20:52:55Z",
        "body": "OMG that sounds painful. The easiest method is to factory reset your pc. jk lol. This is my pip list in case you need it:\r\n\r\naccelerate               1.0.1\r\naiofiles                 23.2.1\r\naiohappyeyeballs         2.4.3\r\naiohttp                  3.10.10\r\naiosignal                1.3.1\r\naliyun-python-sdk-core   2.16.0\r\naliyun-python-sdk-kms    2.16.5\r\nannotated-types          0.7.0\r\nantlr4-python3-runtime   4.9.3\r\nanyio                    4.6.0\r\nasync-timeout            4.0.3\r\nattrs                    24.2.0\r\naudioread                3.0.1\r\nav                       12.3.0\r\nboto3                    1.35.39\r\nbotocore                 1.35.39\r\ncached_path              1.6.3\r\ncachetools               5.5.0\r\ncertifi                  2024.8.30\r\ncffi                     1.17.1\r\ncharset-normalizer       3.4.0\r\nclick                    8.1.7\r\ncolorama                 0.4.6\r\ncoloredlogs              15.0.1\r\ncontourpy                1.3.0\r\ncrcmod                   1.7\r\ncryptography             43.0.1\r\nctranslate2              4.4.0\r\ncycler                   0.12.1\r\ndatasets                 3.0.1\r\ndecorator                5.1.1\r\ndill                     0.3.8\r\ndocker-pycreds           0.4.0\r\neditdistance             0.8.1\r\neinops                   0.8.0\r\neinx                     0.3.0\r\nema-pytorch              0.7.0\r\nencodec                  0.1.1\r\nexceptiongroup           1.2.2\r\nfastapi                  0.115.2\r\nfaster-whisper           1.0.3\r\nffmpy                    0.4.0\r\nfilelock                 3.13.4\r\nflatbuffers              24.3.25\r\nfonttools                4.54.1\r\nfrozendict               2.4.5\r\nfrozenlist               1.4.1\r\nfsspec                   2024.6.1\r\nfunasr                   1.1.12\r\ngitdb                    4.0.11\r\nGitPython                3.1.43\r\ngoogle-api-core          2.21.0\r\ngoogle-auth              2.35.0\r\ngoogle-cloud-core        2.4.1\r\ngoogle-cloud-storage     2.18.2\r\ngoogle-crc32c            1.6.0\r\ngoogle-resumable-media   2.7.2\r\ngoogleapis-common-protos 1.65.0\r\ngradio                   5.0.2\r\ngradio_client            1.4.0\r\nh11                      0.14.0\r\nhttpcore                 1.0.6\r\nhttpx                    0.27.2\r\nhuggingface-hub          0.25.2\r\nhumanfriendly            10.0\r\nhydra-core               1.3.2\r\nidna                     3.10\r\njaconv                   0.4.0\r\njamo                     0.4.1\r\njieba                    0.42.1\r\nJinja2                   3.1.4\r\njiwer                    3.0.4\r\njmespath                 0.10.0\r\njoblib                   1.4.2\r\nkaldiio                  2.18.0\r\nkiwisolver               1.4.7\r\nlazy_loader              0.4\r\nlibrosa                  0.10.2.post1\r\nllvmlite                 0.43.0\r\nmarkdown-it-py           3.0.0\r\nMarkupSafe               2.1.5\r\nmatplotlib               3.9.2\r\nmdurl                    0.1.2\r\nmpmath                   1.3.0\r\nmsgpack                  1.1.0\r\nmultidict                6.1.0\r\nmultiprocess             0.70.16\r\nnetworkx                 3.4.1\r\nnumba                    0.60.0\r\nnumpy                    1.26.4\r\nomegaconf                2.3.0\r\nonnxruntime              1.19.2\r\norjson                   3.10.7\r\noss2                     2.19.0\r\npackaging                24.1\r\npandas                   2.2.3\r\npillow                   10.4.0\r\npip                      24.2\r\nplatformdirs             4.3.6\r\npooch                    1.8.2\r\npropcache                0.2.0\r\nproto-plus               1.24.0\r\nprotobuf                 5.28.2\r\npsutil                   5.9.8\r\npyarrow                  17.0.0\r\npyasn1                   0.6.1\r\npyasn1_modules           0.4.1\r\npycparser                2.22\r\npycryptodome             3.21.0\r\npydantic                 2.9.2\r\npydantic_core            2.23.4\r\npydub                    0.25.1\r\nPygments                 2.18.0\r\npynndescent              0.5.13\r\npyparsing                3.1.4\r\npypinyin                 0.53.0\r\npyreadline3              3.5.4\r\npython-dateutil          2.9.0.post0\r\npython-multipart         0.0.12\r\npytorch-wpe              0.0.1\r\npytz                     2024.2\r\nPyYAML                   6.0.2\r\nRapidFuzz                3.10.0\r\nregex                    2024.9.11\r\nrequests                 2.32.3\r\nrich                     13.9.2\r\nrsa                      4.9\r\nruff                     0.6.9\r\ns3transfer               0.10.3\r\nsafetensors              0.4.5\r\nscikit-learn             1.5.2\r\nscipy                    1.14.1\r\nsemantic-version         2.10.0\r\nsentencepiece            0.2.0\r\nsentry-sdk               2.16.0\r\nsetproctitle             1.3.3\r\nsetuptools               75.1.0\r\nshellingham              1.5.4\r\nsix                      1.16.0\r\nsmmap                    5.0.1\r\nsniffio                  1.3.1\r\nsoundfile                0.12.1\r\nsoxr                     0.5.0.post1\r\nstarlette                0.39.2\r\nsympy                    1.13.3\r\ntensorboardX             2.6.2.2\r\nthreadpoolctl            3.5.0\r\ntokenizers               0.20.1\r\ntomlkit                  0.12.0\r\ntorch                    2.4.0+cu124\r\ntorch-complex            0.4.4\r\ntorchaudio               2.4.0+cu124\r\ntorchdiffeq              0.2.4\r\ntorchvision              0.19.0+cu124\r\ntqdm                     4.66.5\r\ntransformers             4.45.2\r\ntyper                    0.12.5\r\ntyping_extensions        4.12.2\r\ntzdata                   2024.2\r\numap-learn               0.5.6\r\nurllib3                  2.2.3\r\nuvicorn                  0.31.1\r\nvocos                    0.1.0\r\nwandb                    0.18.3\r\nwebsockets               12.0\r\nwheel                    0.44.0\r\nx-transformers           1.39.1\r\nxxhash                   3.5.0\r\nyarl                     1.15.0\r\nzhconv                   1.4.3\r\nzhon                     2.0.2"
      },
      {
        "user": "KungFuFurniture",
        "created_at": "2024-10-18T21:06:00Z",
        "body": "So I think the issue is cuda and whisper.  Note I am AMD (Old AMD)  So my torch is different, and I am willing to believe that is the issue.   I can get faster whisper to work without cuda (I have in my own program project, but abandoned it for a faster smaller implementation) In your case you have torch cu124   which is not an option for my system.   There is a way to move faster-whisper to cpu, but I don't recall the process.  And that would take some diving into these scripts, calling it, etc.    And if I am being totally honest, the max 15 sec of audio sample, is not asking for a whole bunch of manual transcribing.  I can't truly say it's worth that much effort (because I do not know what I am doing).   I'll install your pip list (less torch) and update any major events.   If you have anything you want me to try, I am happy to."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-18T21:42:10Z",
        "body": "If you can get any version of whisper running and that's your only thing holding you back, it's an easy fix!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-25T06:39:03Z",
        "body": "will close this issue, if further questions feel free to open"
      },
      {
        "user": "TrevorAj",
        "created_at": "2025-02-04T11:06:57Z",
        "body": "Saved me a lot of trouble"
      }
    ]
  },
  {
    "number": 69,
    "title": "Error when installing dependencies on Mac",
    "created_at": "2024-10-14T15:34:11Z",
    "closed_at": "2024-10-14T16:23:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/69",
    "body": "```\r\nUsing Python 3.10.0 environment at .direnv/python-3.10\r\nResolved 142 packages in 36ms\r\nerror: Failed to prepare distributions\r\n  Caused by: Failed to fetch wheel: llvmlite==0.36.0\r\n  Caused by: Build backend failed to determine requirements with `build_wheel()` (exit status: 1)\r\n\r\n[stderr]\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 14, in <module>\r\n  File \"/Users/ahmetkca/.cache/uv/builds-v0/.tmpxYn69B/lib/python3.10/site-packages/setuptools/build_meta.py\", line 332, in get_requires_for_build_wheel\r\n    return self._get_build_requires(config_settings, requirements=[])\r\n  File \"/Users/ahmetkca/.cache/uv/builds-v0/.tmpxYn69B/lib/python3.10/site-packages/setuptools/build_meta.py\", line 302, in _get_build_requires\r\n    self.run_setup()\r\n  File \"/Users/ahmetkca/.cache/uv/builds-v0/.tmpxYn69B/lib/python3.10/site-packages/setuptools/build_meta.py\", line 503, in run_setup\r\n    super().run_setup(setup_script=setup_script)\r\n  File \"/Users/ahmetkca/.cache/uv/builds-v0/.tmpxYn69B/lib/python3.10/site-packages/setuptools/build_meta.py\", line 318, in run_setup\r\n    exec(code, locals())\r\n  File \"<string>\", line 55, in <module>\r\n  File \"<string>\", line 52, in _guard_py_ver\r\nRuntimeError: Cannot install on Python version 3.10.0; only versions >=3.6,<3.10 are supported.\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/69/comments",
    "author": "ahmetkca",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T15:50:17Z",
        "body": "maybe \r\n```\r\npip install -U numba\r\npip install -U llvmlite\r\n```\r\nor try with python 3.9\r\n"
      },
      {
        "user": "ahmetkca",
        "created_at": "2024-10-14T16:15:18Z",
        "body": "> maybe \n> \n> ```\n> \n> pip install -U numba\n> \n> pip install -U llvmlite\n> \n> ```\n> \n> or try with python 3.9\n> \n> \n\nWe can close this issue. It has nothing to do with anything in the repo. I used uv to install packages and it happens to have a problem building llvmlite with python >3.10\n\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T16:23:18Z",
        "body": "@ahmetkca sure, hope it goes well now"
      }
    ]
  },
  {
    "number": 66,
    "title": "Added backward compatibility for Python 3.9",
    "created_at": "2024-10-14T11:46:35Z",
    "closed_at": "2024-10-14T15:46:56Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/66",
    "body": "Changed union type (introduced in Python 3.10) to typing.Union to make the dataset.py script work in Python 3.9.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/66/comments",
    "author": "gpercem",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T15:46:56Z",
        "body": "Thanks a lot ~\r\nfixed in 9d2b8cb3dab227f2e6286b9aac2c6ca0c077efac"
      }
    ]
  },
  {
    "number": 64,
    "title": "包含阿拉伯数字会输出语音会异常",
    "created_at": "2024-10-14T09:04:10Z",
    "closed_at": "2024-10-14T10:17:05Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/64",
    "body": "非常棒的项目，但我发现了一个问题，就是文本中包含阿拉伯数字的话，输出的语音会异常？所以是必须将阿拉伯数字预处理成对应的中文或英文吗。",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/64/comments",
    "author": "alants56",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T09:46:15Z",
        "body": "我们的文本前端还比较简陋，我建议将阿拉伯数字进行预处理\r\n虽然模型能够一定程度上对阿拉伯数字进行生成，但是转化成中文或英文来明确指定读法会更稳定"
      },
      {
        "user": "alants56",
        "created_at": "2024-10-14T10:17:05Z",
        "body": "明白了，感谢您的回复"
      }
    ]
  },
  {
    "number": 63,
    "title": "Does this model support Chinese, English, and a mix of Chinese and English?",
    "created_at": "2024-10-14T08:39:08Z",
    "closed_at": "2024-10-14T17:13:46Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/63",
    "body": "Does this great work support Chinese, English, and a mix of Chinese and English?\r\n\r\nHow about the efficiency?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/63/comments",
    "author": "cwr250",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T09:47:54Z",
        "body": "Yes, code-switched generation is just as fluent, I thought."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T17:13:46Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      }
    ]
  },
  {
    "number": 61,
    "title": "Could this be used for singing ?",
    "created_at": "2024-10-14T06:40:12Z",
    "closed_at": "2024-10-23T05:12:02Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/61",
    "body": "Hi, awesome work! \r\nI was wondering if we could use this method to have a powerful singing model as well ?\r\nEither text-to-singing or direct voice conversion ?\r\nThanks",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/61/comments",
    "author": "davidseroussi",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T06:48:10Z",
        "body": "SVC is also a specialized research topic. I may not be that professional to answer this question.\r\nJust hope our work could help in some minor aspects."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-14T07:53:11Z",
        "body": "I tried inputting a singing reference audio and the output *tried* to sing, but it's pretty buggy. That would probably require a fine-tune"
      },
      {
        "user": "leoiania",
        "created_at": "2024-10-14T09:33:42Z",
        "body": "Ciao, I would like to help in case you start a project to fine-tune these models for text-to-singing task.\r\n@jpgallegoar @davidseroussi "
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-14T09:42:07Z",
        "body": "@leoiania Best to wait until LORAs are supported :)"
      },
      {
        "user": "leoiania",
        "created_at": "2024-10-15T12:59:31Z",
        "body": "@jpgallegoar  don't you think that fine-tuning can help? Especially by using english and chinese singing datasets only ?"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-15T13:10:56Z",
        "body": "> @jpgallegoar don't you think that fine-tuning can help? Especially by using english and chinese singing datasets only ?\r\n\r\nYeah, 100%. It's just more computationally expensive"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-23T05:12:02Z",
        "body": "will close this issue, feel free to open if further questions."
      }
    ]
  },
  {
    "number": 59,
    "title": "A reminder for using numpy<2.x，部署的时候python最好选择3.10以下，不包括310",
    "created_at": "2024-10-14T03:04:54Z",
    "closed_at": "2024-10-14T04:37:01Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/59",
    "body": "提醒一下，部署的时候python最好选择3.10以下，不包括310，numpy不支持2.x",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/59/comments",
    "author": "hotdogarea",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T03:14:24Z",
        "body": "我们具体有使用过\r\nPython 3.10.14\r\n\r\nnumpy 1.22.0\r\nnumpy 1.26.4\r\n\r\ntorch 2.3.0+cu118\r\ntorch 2.4.0+cu121\r\n\r\n应该在比较广泛的版本都可用，感谢提醒 numpy<2.x"
      },
      {
        "user": "hotdogarea",
        "created_at": "2024-10-14T03:53:51Z",
        "body": "确实，降级python之后\r\npython gradio_app.py\r\nTraceback (most recent call last):\r\n  File \"H:\\AIaudio_Live\\F5-TTS\\gradio_app.py\", line 11, in <module>\r\n    from model import CFM, UNetT, DiT, MMDiT\r\n  File \"H:\\AIaudio_Live\\F5-TTS\\model\\__init__.py\", line 7, in <module>\r\n    from model.trainer import Trainer\r\n  File \"H:\\AIaudio_Live\\F5-TTS\\model\\trainer.py\", line 22, in <module>\r\n    from model.dataset import DynamicBatchSampler, collate_fn\r\n  File \"H:\\AIaudio_Live\\F5-TTS\\model\\dataset.py\", line 191, in <module>\r\n    ) -> CustomDataset | HFDataset:\r\nTypeError: unsupported operand type(s) for |: 'type' and 'type'\r\n\r\n而这个特性又只能在3.10以上运行。。作者应该在文档里将成功运行的python版本和numpy版本写出来。。。已经部署两次了。。。\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T04:37:01Z",
        "body": "感谢指出 在 408075fa58d80a63ee6bc426c2480f63df9e8bd1 中添加了关于python版本和numpy的说明\r\n如果有另外问题 随时欢迎open issue~"
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-10-14T10:35:44Z",
        "body": "> As a reminder, when deploying, it is best to choose Python 3.10 or below, excluding 310, and numpy does not support 2.x\r\n\r\nPython 3.10 is supported with lower numpy version."
      }
    ]
  },
  {
    "number": 58,
    "title": "Audio prompt is repeated at the start",
    "created_at": "2024-10-14T02:01:13Z",
    "closed_at": "2024-10-14T17:15:32Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/58",
    "body": "Thanks for open sourcing this model, it seems very promising.\r\n\r\nThe model seems to regenerate the last part of the reference audio at the beginning of the generated audio. It also seems like the last part of the generated audio is always better than the first part. Any ideas why is that, how to fix? ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/58/comments",
    "author": "tammam1998",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T02:18:58Z",
        "body": "Generally because we just simply estimate the duration rather than using a delicate duration predictor.\r\n\r\n1. Consider fix duration\r\n2. If long silence at start and end of prompt audio, do a pre-cut would help\r\n\r\nMaybe also try with local Gradio APP, there are some features e.g. removing silence applied."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T17:15:32Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      }
    ]
  },
  {
    "number": 55,
    "title": "Providing a reference audio longer than 20 seconds produces truncated output.",
    "created_at": "2024-10-14T00:17:54Z",
    "closed_at": "2024-10-14T14:55:17Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/55",
    "body": "First of all, great model!\r\nIf I provide a reference audio that's about 10-15 seconds long, it produces decent output.\r\nI was hoping that providing longer audio reference would improve the output quality. However, if I provide a reference audio that's longer than 20 seconds, it produces an output with a lot of words missing from gen_text variable.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/55/comments",
    "author": "chigkim",
    "comments": [
      {
        "user": "chigkim",
        "created_at": "2024-10-14T00:19:00Z",
        "body": "I'm using the python script for single inference."
      },
      {
        "user": "AznamirWoW",
        "created_at": "2024-10-14T00:46:25Z",
        "body": "or instead of truncating it may just produce a greatly speed-up read.\r\n\r\nAs I understand, it concatenates the reference text and the text it is supposed to produce, uses the reference audio as a starting point and then continues the audio using inference. Then it cuts out the original audio from the result.\r\n\r\nUnfortunately the reference can not be too long, anything more than 6 seconds and 100 characters resulted in corrupted output, as well as the text to generate can not be too long either, not more than 300 characters? Otherwise the output corrupts completely, half of it gets replaced by white noise and the rest is a random inferred text at ~5x speed."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T01:34:25Z",
        "body": "Yes, indeed there would be failure with long input >30s. As the model is trained with infilling task, which is masking out some part and letting model restore.\r\nCurrently support a total length of 30s (reference + to_generate), cuz the longest sampled met in training is 30s."
      },
      {
        "user": "chigkim",
        "created_at": "2024-10-14T02:48:56Z",
        "body": "Is this model's architecture limitation, or can you finetune with longer dataset? It would be amazing to be able to keep generating without time limitation. :)"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T02:53:07Z",
        "body": "> Is this model's architecture limitation, or can you finetune with longer dataset? It would be amazing to be able to keep generating without time limitation. :)\r\n\r\nTry with local Gradio APP, which enables batch inference with chunks ~"
      },
      {
        "user": "chigkim",
        "created_at": "2024-10-14T14:55:17Z",
        "body": "Thanks for pointing out the batch inference in Gradio app.\r\nAlso thanks for accepting #67 for CLI, modified from Gradio app."
      }
    ]
  },
  {
    "number": 48,
    "title": "Wonderful app, impressed with the output quality.",
    "created_at": "2024-10-13T18:04:37Z",
    "closed_at": "2024-10-14T01:25:46Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/48",
    "body": "Thanks you devs for sharing this tool with us. I am impressed with the output quality. I have so far tried atleast 10 TTS but this surpasses them all. Appreciate your hard work on this app.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/48/comments",
    "author": "nitinmukesh",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T01:25:46Z",
        "body": "Thanks for using ~\r\nIf you have any questions about this repo, feel free to open an issue"
      }
    ]
  },
  {
    "number": 47,
    "title": "Hello bro!",
    "created_at": "2024-10-13T18:01:49Z",
    "closed_at": "2024-10-14T01:51:42Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/47",
    "body": "One question... WhY sO SeRious?!?! *beatboxing...*\r\nAgh, alr..sry...\r\n\r\nWill u add Russian language?... What specifications are needed for a computer to process everything quickly? On my laptop with a 3060 6GB, it takes a very long time to process. Will there be optimization for this in the future?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/47/comments",
    "author": "Juranik",
    "comments": [
      {
        "user": "XLED-bd",
        "created_at": "2024-10-13T22:30:47Z",
        "body": "Same problem: I need to train the model with 50k hours of English speech, and with a 3060 12 GB it takes 3 years... and then add 50k hours of Russian speech, which is another 3 years, for a total of 6 years 🤗\r\n"
      },
      {
        "user": "SeraDreams",
        "created_at": "2024-10-13T22:55:14Z",
        "body": "I have a 1660ti with 6GB and I couldn't even start the training, it would be really cool if they added Russian"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-14T00:58:47Z",
        "body": "We'll try our best to train on broader corpus! So, be patient！And our model was trained on A100 and you'd better train the model on > 24G machine. "
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-14T01:51:41Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue. @SeraDreams @Juranik @XLED-bd "
      },
      {
        "user": "nitinmukesh",
        "created_at": "2024-10-14T10:38:43Z",
        "body": "@ZhikangNiu \r\n\r\nYou have created an amazing tool like I mentioned in one of the earlier post.\r\n\r\nSupporting more languages like e.g. Hindi, Russian will be very helpful for end users who don't know anything about code forget training."
      }
    ]
  },
  {
    "number": 45,
    "title": "Newest batch commit broke inference",
    "created_at": "2024-10-13T16:55:09Z",
    "closed_at": "2024-10-13T17:48:15Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/45",
    "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\gradio\\queueing.py\", line 622, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\gradio\\blocks.py\", line 2016, in process_api\r\n    result = await self.call_function(\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\gradio\\blocks.py\", line 1569, in call_function\r\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2405, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 914, in run\r\n    result = context.run(func, *args)\r\n  File \"C:\\Users\\thega\\miniconda3\\envs\\tts\\lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"D:\\AI\\F5-TTS\\gradio_app.py\", line 326, in infer\r\n    return infer_batch(ref_audio, ref_text, gen_text_batches, exp_name, remove_silence)\r\n  File \"D:\\AI\\F5-TTS\\gradio_app.py\", line 252, in infer_batch\r\n    final_wave = np.concatenate(generated_waves)\r\n    \r\n    Newest batch commit broke inference in some cases. I got this error with one specific prompt, and by adding a \".\" at the end of the prompt, the error went away and inference worked again. I think it's better if you disable batching for generations smaller than 200 chars, or fix this issue.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/45/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T17:12:29Z",
        "body": "Thanks for report.\r\nWould it be fine with current version ?"
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-13T17:39:44Z",
        "body": "Hello, thank you very much for the quick response and commit, it has been fixed."
      }
    ]
  },
  {
    "number": 41,
    "title": "Finetune?",
    "created_at": "2024-10-13T15:32:06Z",
    "closed_at": "2024-10-14T01:00:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/41",
    "body": "Can this be used to finetune with own voice or only train entire model from scratch?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/41/comments",
    "author": "helikopterodaktyl",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T15:39:39Z",
        "body": "Finetune is fine, refer to #27 "
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-14T01:00:01Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue. @helikopterodaktyl "
      }
    ]
  },
  {
    "number": 40,
    "title": "Plan for other languages?",
    "created_at": "2024-10-13T14:58:17Z",
    "closed_at": "2024-10-13T16:35:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/40",
    "body": "Hi there,\r\n\r\nThank you for the release, you did such a great job, voice cloning quality is unbelievable.\r\n\r\nI was wondering if by any chance you planned to extend the model to other languages beyond English and Chinese; I saw that you used Emilia dataset for pretraining so I guess that for the Emilia’s languages should be doable.\r\n\r\nAnd what about languages not included in the Emilia dataset like Italian? Do you have any plan about it?\r\n\r\nHere in Italy we’re really suffering the lack of high quality open source alternative for text to speech.\r\n\r\nthank you and great work again.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/40/comments",
    "author": "imthebilliejoe",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T15:44:49Z",
        "body": "We'll try our best to train on broader corpus."
      }
    ]
  },
  {
    "number": 39,
    "title": "about frame batch size OOM",
    "created_at": "2024-10-13T14:38:44Z",
    "closed_at": "2024-10-14T06:06:56Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/39",
    "body": "hello @SWivid \r\n\r\n\r\ni want to reproduce your train,\r\n\r\ni am using 8xH100 \r\nwith amount 5k hours data\r\n\r\nbut when i use 38400 batch size i got OOM\r\n\r\nright now i am using half of it 19200 seems working\r\n\r\nmy question :\r\nis 38400 depend on the dataset we use ?\r\n(i am also make sure that duration.json and data have same index in dataset collate)\r\n\r\nalso if i am about scaling it to 16 gpu what hyper parameter that you would recommend to change?\r\n\r\nthanks",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/39/comments",
    "author": "acul3",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T15:07:58Z",
        "body": "1. We use torch==2.3.0 (native flash_attn for torch>=2.2), and fp16/bf16\r\n2. 38400 is just the exact batchsize in E2 TTS paper (8*38400=307200). If OOM 19200 is fine, set grad_accumuation_steps=2 bs is just the same, while saved ckpt e.g. model_10000.pt is actually for 5000 updates.\r\n3. The hyper parameter is generally with used batchsize, thought no big change with 16\\*19200 to 8\\*38400\r\n\r\nAlso the current batchsampler is not delicate, might consider dynamically lower the threshold for long sequence batch"
      }
    ]
  },
  {
    "number": 38,
    "title": "Batch Inference & Podcast Generation",
    "created_at": "2024-10-13T14:36:36Z",
    "closed_at": "2024-10-13T15:20:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/38",
    "body": "Here's what the Batch Inference part does:\r\n\r\n- Try to put as much characters as possible into one batch (200 max)\r\n- If it's not possible, it'll try to do a cut whenever there's a semicolon character\r\n- If it's not possible, it'll try to do a cut whenever there's a comma character\r\n- If it's not possible, it'll try to do a cut after the most logical word (thus, therefore etc.) --> There's a list at the top of the Gradio script, and it's possible to modify it in Advanced Settings\r\n- If nothing above worked, it's just going to go past that 200 line (realistically, if your text isn't gibberish, this shouldn't happen :D)\r\n\r\nThe Podcast Generation feature has these features built in:\r\n- Takes two reference speeches and two reference texts (or empty and then transcribed automatically)\r\n- You have to give a name to each of the two speakers\r\n- You can then paste the podcast script, with one speaker's name followed by a semicolon and then their text, you can do the same with the other speaker, all as long as you want (because it's using the same batch inference as before)\r\n\r\nAll in all, the batch inference feature allow for a little bit more than real-time inference. (I might do another pull request with real-time streaming)\r\n\r\nImmense thanks to all of those who worked on this project, it's really great. There's of course still room for improvement, but I think this is a step forward in terms of OSS TTS, so thanks !",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/38/comments",
    "author": "RootingInLoad",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T15:20:30Z",
        "body": "Many thanks ~"
      }
    ]
  },
  {
    "number": 36,
    "title": "german language please",
    "created_at": "2024-10-13T13:33:47Z",
    "closed_at": "2024-10-13T14:03:05Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/36",
    "body": "thank you",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/36/comments",
    "author": "NeverBeLazyG",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T14:03:05Z",
        "body": "👌"
      }
    ]
  },
  {
    "number": 34,
    "title": "ModuleNotFoundError: No module named 'encodec.model'",
    "created_at": "2024-10-13T13:20:25Z",
    "closed_at": "2024-10-14T17:16:37Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/34",
    "body": "Keep getting this error even after installing all dependencies. Not sure what it's stemming from",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/34/comments",
    "author": "llkj11",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T14:04:02Z",
        "body": "Can you provide more specific information, such as a full error message"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T17:16:37Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      }
    ]
  },
  {
    "number": 33,
    "title": "precompute_max_pos = 4096 目前最大只能支持到4096token的文本长度么？",
    "created_at": "2024-10-13T13:20:08Z",
    "closed_at": "2024-10-14T02:46:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/33",
    "body": "      if conv_layers > 0:\r\n          self.extra_modeling = True\r\n          self.precompute_max_pos = 4096  # ~44s of 24khz audio\r\n          self.register_buffer(\"freqs_cis\", precompute_freqs_cis(text_dim, self.precompute_max_pos), persistent=False)\r\n          self.text_blocks = nn.Sequential(*[ConvNeXtV2Block(text_dim, text_dim * conv_mult) for _ in range(conv_layers)])\r\n\r\n请问后续是否有计划支持更大的长度。",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/33/comments",
    "author": "pengyong94",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T13:56:45Z",
        "body": "实际上目前训练集最长是30秒，所以4096 ~44s完全覆盖了（因为text是pad到和mel一样长，而mel最长就是30*24000/256目前）\r\n目前通过分块生成，30s应该够用，之后大概率会往流式的方向走"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T02:46:10Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue."
      },
      {
        "user": "pengyong94",
        "created_at": "2024-10-14T04:21:20Z",
        "body": "ok, thanks for your reply ,and your work is great !"
      }
    ]
  },
  {
    "number": 30,
    "title": "Multi-machine training quesion",
    "created_at": "2024-10-13T10:49:00Z",
    "closed_at": "2024-10-15T08:20:44Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/30",
    "body": "Hi, Is this code suitable for multi-machine training? Seems to be stuck all the time. @SWivid ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/30/comments",
    "author": "yangyyt",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T15:57:01Z",
        "body": "we tried `accelerate config` with multi-machine setting.\r\nseems ok with our expr. maybe check for the port used"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-15T08:20:44Z",
        "body": "will close, if any questions welcome to open it~"
      },
      {
        "user": "yangyyt",
        "created_at": "2024-10-15T14:05:26Z",
        "body": "> we tried `accelerate config` with multi-machine setting. seems ok with our expr. maybe check for the port used\r\n\r\nI noticed a phenomenon during my training. The training uses batch_size_per_gpu=20000, max_samples=64, the data only uses 0.3-20s of audio, training on 80G A100, in the middle of training, cuda oom, printing cuda memory occupation in training, found that in the middle of a batch, memory directly increased from 30G to 70G, and then kept at 70G, continue to train some steps, The OOM error is reported. \r\nHave you ever encountered it， or Do you have any suggestions？ @SWivid "
      },
      {
        "user": "yangyyt",
        "created_at": "2024-10-15T14:11:31Z",
        "body": "Another question is why you use frame-wise batch_size with 32 for small models and 64 for base models. Why not use 32 for the base model and 64 for the small model?  @SWivid "
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-15T14:19:38Z",
        "body": "Hi @yangyyt , the increased gpu memory is normal. Once all ranks are prepared and batches with audio samples starts to swarm into gpu mem.\r\nOur experience is that once figure out a oom-free batchsize, the mem usage will just keep the same all along training.\r\n\r\nFor the max_sample setting, we just intuitively have small model with smaller batchsize, and not squeeze too much short samples into one batch :D"
      },
      {
        "user": "yangyyt",
        "created_at": "2024-10-15T14:43:04Z",
        "body": "> the increased gpu memory is normal. Once all ranks are prepared and batches with audio samples starts to swarm into gpu mem.\r\n\r\nThank you for your reply.\r\nemmm,  step 3 to step 3566: gpu mem_30G,   but step 3567 to step 6454: gpu mem_70G, \r\nand training then oom. Isn't that a little strange?  "
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-15T14:53:54Z",
        "body": "> step 3 to step 3566: gpu mem_30G, but step 3567 to step 6454: gpu mem_70G\r\n\r\nah, this is strange. we just thought all gpu would reach 70g once step from 0 to 1.\r\nwill test for it."
      }
    ]
  },
  {
    "number": 28,
    "title": "Which languages does this support?",
    "created_at": "2024-10-13T09:46:54Z",
    "closed_at": "2024-10-13T10:50:16Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/28",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/28/comments",
    "author": "AlgorithmicKing",
    "comments": [
      {
        "user": "wangfeng35",
        "created_at": "2024-10-13T10:34:06Z",
        "body": "English and Chinese \r\n"
      }
    ]
  },
  {
    "number": 24,
    "title": "Add missing gradio dependency to requirements_gradio.txt",
    "created_at": "2024-10-13T03:42:40Z",
    "closed_at": "2024-10-13T04:58:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/24",
    "body": null,
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/24/comments",
    "author": "Tradunsky",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T04:58:18Z",
        "body": "Thanks~ fixed in 68b4ce0f2be711a448934f62ad591e244588c77d"
      }
    ]
  },
  {
    "number": 23,
    "title": "chore: update trainer.py",
    "created_at": "2024-10-13T01:40:12Z",
    "closed_at": "2024-10-13T02:04:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/23",
    "body": "recieved -> received",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/23/comments",
    "author": "eltociear",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-13T02:04:50Z",
        "body": "thanks, we have fix this typo"
      }
    ]
  },
  {
    "number": 21,
    "title": "Support for the persian language ",
    "created_at": "2024-10-12T23:38:06Z",
    "closed_at": "2024-10-13T02:08:14Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/21",
    "body": "Hi, I wonder how I can add Persian to this awesome project. Is there any manual for training? Making dataset or aid for training? \nI would like to contribute despite there being no good open source tts for Persian. ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/21/comments",
    "author": "tavallaie",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-13T01:31:51Z",
        "body": "We are very pleased to have you participate in this project.\r\nYou may also refer to #5 #12 #16 for some guidance."
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-13T02:08:11Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue. @tavallaie "
      }
    ]
  },
  {
    "number": 16,
    "title": "finetune single speaker",
    "created_at": "2024-10-12T11:36:02Z",
    "closed_at": "2024-10-13T02:10:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/16",
    "body": "is it possible to finetune for a single speaker, if so could you elaborate? (lora maybe)\r\nthank you for your time!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/16/comments",
    "author": "kunibald413",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-12T12:33:26Z",
        "body": "Full finetune is currently supported, lora or adapter not yet.\r\n1. Set `checkpoint_path` to pretrained model dir in `test_train.py`, `model/trainer.py` will load from there to resume. Reuse the `vocab.txt` under `data\r\n/Emilia_ZH_EN_pinyin` (Emilia_ZH_EN_pinyin <- tokenizer = \"pinyin\"; dataset_name = \"Emilia_ZH_EN\" in `test_train.py` setting) \r\n2. For preparing finetune data, see `model/dataset.py`. Just need e.g. the **audio path**, **text** (tokenized, leverage `convert_char_to_pinyin` func in `model/utils.py` see script/prepare_xxxx.py), **duration** of audio in seconds.\r\n```\r\ndef __getitem__(self, index):\r\n    row = self.data[index]\r\n    audio_path = row[\"audio_path\"]\r\n    text = row[\"text\"]\r\n    duration = row[\"duration\"]\r\n```\r\n3. Set a smaller batchsize according to your GPU mem. The `grad_accumulation_steps` could be used to simulate a large batchsize. Also other settings, e.g. few warmup steps, 1e-4 lr, etc.\r\n\r\nWe didn't specifically experiment with finetuning, so if you get positive results, welcome to share :)"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-13T02:10:37Z",
        "body": "I'll close this issue, if you have other questions you can reopen this issue. @kunibald413 "
      }
    ]
  },
  {
    "number": 13,
    "title": "采用utf8每个字符一个时长的编码，必有深意",
    "created_at": "2024-10-11T15:04:26Z",
    "closed_at": "2024-10-12T13:16:13Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/13",
    "body": "如果简单地修改文本前端，是否会影响对齐效果？\r\n我看你简单粗暴地将文本时长pad0到mel帧长，是不是因为每个字符一个时长的编码比较适合？",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/13/comments",
    "author": "splinter21",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-11T15:09:17Z",
        "body": "不会的 既然简单粗暴纯用character pad0到mel帧长能work，就说明模型在别的上面也肯定work\r\n这样搞是为了完全复刻E2 TTS的pipeline（突出一个什么附加的都没有，embarrassingly easy）\r\n\r\n完全可以用音素或者bpe，应该会更好训的"
      },
      {
        "user": "splinter21",
        "created_at": "2024-10-11T16:28:36Z",
        "body": "> 不会的 既然简单粗暴纯用character pad0到mel帧长能work，就说明模型在别的上面也肯定work 这样搞是为了完全复刻E2 TTS的pipeline（突出一个什么附加的都没有，embarrassingly easy）\r\n> \r\n> 完全可以用音素或者bpe，应该会更好训的\r\n\r\n为什么diffusion能自己学会文本和mel的对齐呢，我不明白"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-12T01:35:38Z",
        "body": "> 为什么diffusion能自己学会文本和mel的对齐呢，我不明白\r\n\r\n这个逻辑可以参考文生图，虽然没有像TTS要求严格的对齐，但也是从prompt的风格和描述里做对齐\r\n本身是让模型学习分布之间的关系，只是force-align之后学习难度更低一些，而没有force-align难学但如果学出来效果更好\r\n这是我粗浅的理解"
      }
    ]
  },
  {
    "number": 12,
    "title": "Language Support",
    "created_at": "2024-10-11T12:46:57Z",
    "closed_at": "2024-10-11T15:54:24Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/12",
    "body": "Hi\r\nWould this model support more languages than English and Chinese?\r\nShall we wait for the new checkpoints with a wider language selection?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/12/comments",
    "author": "Nik-Kras",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-11T12:58:00Z",
        "body": "In progress...\r\nAnd welcome to PR for sharing more language supported ckpts.\r\nmay refer to #5 "
      }
    ]
  },
  {
    "number": 11,
    "title": "linearly estimated duration may have unstable generation",
    "created_at": "2024-10-11T07:24:11Z",
    "closed_at": "2024-10-11T10:32:12Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/11",
    "body": "fix_duration=None, speed=1.0 ，设置后，漏字重复问题很明显，如何解决这个问题呢",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/11/comments",
    "author": "kunyao2015",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-11T09:14:48Z",
        "body": "有具体例子 比如使用的prompt吗（如果整体时长超过30s，超过的部分会不正常）\r\n如果prompt的语速异常快或慢，或者开头结尾包含很长的静音，线性预估应该是不太行\r\n最好的方式就是训练一个单独的时长预测器来提供全句时长，我们为了简单就没训；如果有计划训练欢迎PR ckpt\r\n\r\nYou may provide some specific examples of used prompt to locate the problem. (If the overall duration exceeds 30s, the generation for part >30s will be abnormal)\r\nIf the prompt is spoken at an unusually fast or slow rate, or if the (beginning/end of) prompt contains long silences, the linear estimate not work for sure.\r\nThe best approach is to train a separate duration predictor that provides the full sentence duration, which we left out for simplicity; Welcome to PR the ckpt if you plan to train such a dp model"
      },
      {
        "user": "kunyao2015",
        "created_at": "2024-10-11T09:52:30Z",
        "body": "发现确实整体不超过30秒会好很多； 另一个问题， 跨语言的duration 如何合理预估，比如prompt是中文，需要合成英文，按test_infer_single中方式无需改动吗"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-11T09:59:01Z",
        "body": "需要改的，因为训练中Emilia里遇到的顶多是code-switch，但是在inference里可能会用英文prompt来生成中文的\r\n这两种语言语速是不同的，所以我们在注释写最好fix duration\r\n\r\n如果要预估比较建议以不同语言的语速为基准，例如英文一个词是多少秒，中文一个词多少秒，然后算一下\r\n此外因为两种语言分别扎堆，例如英文prompt+中文生成，可能会吞掉中文前几个字\r\n这种情况可以通过在前面英文prompt对应的转录text加句号解决，因为模型其实并不“那么”在意转录对应文本，只需要让他知道前面有这么多字是prompt就行了（我们也试过把prompt对应的转录全换成等长的空格，其实也OK的）\r\n\r\nIf want to inference with code-switch, it is recommended to fix_duraion (different languages has a different speed). If to simply estimate, e.g. consider the the common per word duration for English, and for Chinese, then do a simple calcul.\r\n\r\nAs such inference way is e.g. stacking all EN words in prompt, and to generate all ZH; there exists a speaking rate gap; may cause skipping first several words in to_generate ZH. You may add some 'full stop' marks to ref_text to solve temporally (cuz model just need to know the length occupied by ref, could even just replace ref_text with equal-length blanks, and the generation goes well)."
      },
      {
        "user": "kunyao2015",
        "created_at": "2024-10-11T10:32:07Z",
        "body": "感谢回复， 很有帮助！"
      }
    ]
  },
  {
    "number": 10,
    "title": "Does it able to integrate a LLM to train e2e?",
    "created_at": "2024-10-11T02:29:44Z",
    "closed_at": "2024-10-11T15:54:41Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/10",
    "body": "I am thinking of using whisper-large-v3 and F5-TTS stack, to make a e2e voice 2 voice model, for tss part, do u think it possiable intergate F5 with an LLM?\r\n\r\nThis means, it should be streamable.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/10/comments",
    "author": "luohao123",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-11T02:40:19Z",
        "body": "Such system is very meaningful.\r\nF5-TTS cannot do 'Truly' steamable generation at present, but somehow in a sequence.\r\n\r\nThat's part of the reason we're open sourcing, to move faster in this direction with the community"
      },
      {
        "user": "luohao123",
        "created_at": "2024-10-11T08:51:39Z",
        "body": "Is because of  the flow-matching way in generation?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-11T09:20:53Z",
        "body": "> Is because of the flow-matching way in generation?\r\n\r\nfor all I know, compare to the naturally streamable next-token prediction modeling,\r\n    a compromise may be 'just chunk the generation' for diffusion models.\r\nOr a brand new training task (enabling streaming) in stead of in-filling is required"
      },
      {
        "user": "luohao123",
        "created_at": "2024-10-11T09:45:27Z",
        "body": "What potential approaches might F5 evolve through if we have the objective of integrating it into a Large Language Model?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-11T10:14:17Z",
        "body": "In my opinion, integrating 'into' a LLM might be more of the way Seed-TTS_ICL does, or MELLE, ARDiT-TTS, etc.\r\nTo integrating 'with' a text LLM, we are trying to make F5 streamable first."
      },
      {
        "user": "luohao123",
        "created_at": "2024-10-11T12:01:20Z",
        "body": "The later is what I mean, using an adaptor or something concat togther, some work already showed promising progress, like MiniOmni.\r\n\r\nIn your mind, what's the first step could be to make F5 streamable"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-11T12:21:48Z",
        "body": "We will keep on open-sourcing if once done something concrete w.r.t. this.\r\nAnd it would be welcomed if you could share your insights ~"
      }
    ]
  },
  {
    "number": 7,
    "title": "Questions about the baseline models in the paper",
    "created_at": "2024-10-10T12:46:01Z",
    "closed_at": "2024-10-11T04:40:01Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/7",
    "body": "Hi,\r\n\r\nThanks for your nice work and open source!\r\nCan I ask how did you get the results of baseline models in your paper, like VALL-E 2, MELLE, Voicebox and NaturalSpeech 3 in Table 1? You mentioned the score reported in baseline papers were from different subsets for evaluation. When I came to the VALL-E 2 paper, I did not see the corresponding results listed in their paper. Could you please explain more about the details of the baselines? Thanks!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/7/comments",
    "author": "WangHelin1997",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-10T14:16:21Z",
        "body": "Yes, we have mentioned in Appendix-A Baseline Details.\r\ne.g. for VALL-E 2, we compared with results reported in [29], which is MELLE paper\r\nas MELLE paper contains most VALL-E series scores\r\n\r\nAlso in MaskGCT, they compared with NS3, thus we report the scores with also NS3 paper\r\n\r\nResults in 2 subset (as you can see from Tab.1)"
      }
    ]
  },
  {
    "number": 6,
    "title": "Can it be used to edit speech?",
    "created_at": "2024-10-10T10:06:20Z",
    "closed_at": "2024-10-11T01:28:09Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/6",
    "body": "Can this model to be used to edit speech?\r\n\r\nIt follow VoiceBox, but why E2-TTS do not mention its ability of redact speech, or it just can not. Or, I missed some thing?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/6/comments",
    "author": "chenht2021",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-10T10:16:30Z",
        "body": "As it was trained with in-filling task, editing ability should be inherited.\r\nSimply mask out the part for edit, synthesize and replace into this part.\r\n\r\nI'll try it out and if it works I'll add it to the script."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-10T16:43:40Z",
        "body": "@chenht2021 It works well\r\nsee `python test_infer_single_edit.py`\r\nhave fun~"
      },
      {
        "user": "chenht2021",
        "created_at": "2024-10-11T01:28:02Z",
        "body": "Thanks, I figure out I made a mistake sending wrong mask to model.\r\nThanks for open source this model. Nice work!"
      }
    ]
  },
  {
    "number": 5,
    "title": "Is it possible to train TTS for a new language?",
    "created_at": "2024-10-10T06:13:08Z",
    "closed_at": "2024-10-10T09:09:15Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/5",
    "body": "Thank you for your work. I would like to inquire about the possibility of training for a new language. If this is feasible, could you please provide more details on the following:\r\n\r\n- How much data is required?\r\n- In what format should the data be?\r\n- What resources are approximately needed to achieve results comparable to those for the English language?\r\n\r\nYour insights on this matter would be greatly appreciated. Thank you in advance for your assistance.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/5/comments",
    "author": "AigizK",
    "comments": [
      {
        "user": "ScottishFold007",
        "created_at": "2024-10-10T06:33:35Z",
        "body": "### 1. Required Data Volume\r\n- **Data Volume**: Approximately 95K hours of English and Chinese data were used to train the base model. To achieve a high-quality TTS system, a large amount of data is required. For stable performance, at least 10,000 hours of voice data for a certain language is needed.\r\n\r\n### 2. Data Format\r\n- **Audio Format**: The audio files should be in mono WAV format, with a sampling rate of 24kHz, using 100-dimensional log Mel spectrogram features, and a frame hop length of 256.\r\n- **Text Processing**: English uses letters and symbols directly; Chinese characters are processed into complete pinyin through Jieba segmentation and pypinyin.\r\n\r\n### 3. Required Resources\r\n- **Computational Resources**: Training of the base model was conducted on 8 NVIDIA A100 80G GPUs, lasting more than a week. This demonstrates the significant computational resources required to train a high-quality TTS model.\r\n- **Model Configuration**: The base model includes 22 layers, 16 attention heads, and embedding/feed-forward network (FFN) dimensions of 1024/2048. The configuration of the small model is detailed in Appendix B.1.\r\n\r\n### 4. Results Comparable to English\r\n- **Model Performance**: To ensure high-quality TTS comparable to English, a large amount of training data and advanced model architecture were used. In addition, various test sets were used for evaluation, including LibriSpeech-PC test-clean, Seed-TTS test-en, and Seed-TTS test-zh, to ensure the model's wide applicability and fairness of comparison.\r\n\r\n### 5. Training Details\r\n- **Optimizer**: The AdamW optimizer was used, with a peak learning rate of 7.5e-5, linear warm-up for the first 20K updates, followed by linear decay.\r\n- **Regularization**: Attention and FFN use a dropout rate of 0.1 to prevent overfitting.\r\n- **Data Augmentation**: Random masking of 70% to 100% of Mel spectrogram frames for fill-in-the-blank training tasks."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-10T06:35:43Z",
        "body": "All training details is mentioned in our paper.\r\n\r\nAnd you could simply train your own model for a new language:\r\n1. Leverage Emilia Dataset (DE EN FR JA KO ZH), as we have include script for it (NOTE. download the mentioned version of Emilia in script, cuz it's currently updated to a WebDataset ver.)\r\n2. or prepare your own data pairs if not covered, just tailor a Dataset Class in model/dataset.py to your need\r\n\r\nFor Base model (multilingual, ~300M), we use <50K hours for each language (EN ZH)\r\nFor Small model (e.g. Chinese-only, ~150M), we have made it work with just 1K hours data, config. mentioned in our paper also\r\n\r\nJust one thing, the training would take a long time, especially for E2 TTS (if you choose)\r\nAnd be patient, 8 x RTX3090 small model for one week (200~400K updates to hear something reasonable)\r\n8 x A100 for base model similarly.\r\n"
      },
      {
        "user": "ukemamaster",
        "created_at": "2024-10-14T07:21:13Z",
        "body": "@SWivid For each language a separate model has to be trained ? or a joint model can be trained for multiple additional languages?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-14T07:30:57Z",
        "body": "Just make sure your tokenizer can deal with multiple languages, as you can see our F5 and reproduced E2 support both ZH and EN in one model rather than separated."
      },
      {
        "user": "ukemamaster",
        "created_at": "2024-10-14T07:35:23Z",
        "body": "Do you plan to add Spanish language in the near future?"
      },
      {
        "user": "wkd88",
        "created_at": "2024-11-02T07:17:36Z",
        "body": "Could you please provide specific hyperparameters for training a small model? For example, batch_size_per_gpu and grad_accumulation_steps ?"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-02T07:30:24Z",
        "body": "> Could you please provide specific hyperparameters for training a small model? For example, batch_size_per_gpu and grad_accumulation_steps ?\r\n\r\ncheck `src/f5_tts/scripts/count_params_gflops.py` and appendix in our paper"
      }
    ]
  },
  {
    "number": 3,
    "title": "Is the generated audio length limited to only 15 seconds?",
    "created_at": "2024-10-10T03:10:52Z",
    "closed_at": "2024-10-10T03:48:24Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/3",
    "body": "Can the generated audio be made longer, such as 30 seconds or more? I've found that when there's more text, the speech rate becomes faster, trying to squeeze into 15 seconds.\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/3/comments",
    "author": "ScottishFold007",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-10T03:15:08Z",
        "body": "modify the `fix_duration` or set to None for simple estimation\r\ni will add this mentioning in README\r\n\r\nthe trainset is up to 30s, thus will do for 30s inference (prompt + to_generate, total length)"
      }
    ]
  },
  {
    "number": 1,
    "title": "windows运行test_infer_single卡了好久不动弹",
    "created_at": "2024-10-09T08:25:30Z",
    "closed_at": "2024-10-09T08:55:42Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/1",
    "body": "显存8G，log信息如下：\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\github\\F5-TTS-main\\test_infer_single.py\", line 142, in <module>\r\n    generated, trajectory = model.sample(\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"D:\\github\\F5-TTS-main\\model\\cfm.py\", line 187, in sample\r\n    trajectory = odeint(fn, y0, t, **self.odeint_kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torchdiffeq\\_impl\\odeint.py\", line 79, in odeint\r\n    solution = solver.integrate(t)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torchdiffeq\\_impl\\solvers.py\", line 114, in integrate\r\n    dy, f0 = self._step_func(self.func, t0, dt, t1, y0)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torchdiffeq\\_impl\\fixed_grid.py\", line 10, in _step_func\r\n    f0 = func(t0, y0, perturb=Perturb.NEXT if self.perturb else Perturb.NONE)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torchdiffeq\\_impl\\misc.py\", line 197, in forward\r\n    return self.base_func(t, y)\r\n  File \"D:\\github\\F5-TTS-main\\model\\cfm.py\", line 158, in fn\r\n    pred = self.transformer(x = x, cond = step_cond, text = text, time = t, mask = mask, drop_audio_cond = False, drop_text = False)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\github\\F5-TTS-main\\model\\backbones\\dit.py\", line 150, in forward\r\n    x = block(x, t, mask = mask, rope = rope)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\github\\F5-TTS-main\\model\\modules.py\", line 479, in forward\r\n    attn_output = self.attn(x=norm, mask=mask, rope=rope)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\github\\F5-TTS-main\\model\\modules.py\", line 306, in forward\r\n    return self.processor(self, x, mask = mask, rope = rope)\r\n  File \"D:\\github\\F5-TTS-main\\model\\modules.py\", line 327, in __call__\r\n    key = attn.to_k(x)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\funasr\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 116, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\nKeyboardInterrupt\r\n```\r\n为什么会在这里卡住，是因为显存不够？但是我看显存占用也没增加，也没爆oom的错误..",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/1/comments",
    "author": "yuyun2000",
    "comments": [
      {
        "user": "yuyun2000",
        "created_at": "2024-10-09T08:34:42Z",
        "body": "我去 我pc的torch咋变成cpu版本了..."
      },
      {
        "user": "yuyun2000",
        "created_at": "2024-10-09T08:55:39Z",
        "body": "运行完了，占用显存7.5G，英语示例完美，但是中文声音速度非常奇怪，很慢，感谢作者开源这么优秀的作品"
      },
      {
        "user": "yuyun2000",
        "created_at": "2024-10-09T08:56:56Z",
        "body": "不是语速慢...是停顿太久了"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-09T08:57:39Z",
        "body": "> 运行完了，占用显存7.5G，英语示例完美，但是中文声音速度非常奇怪，很慢，感谢作者开源这么优秀的作品\r\n\r\n能用就好哈哈，可以调整下fix_duration（是包含prompt和要生成的总时长），如果设置None就是按照字符个数线性估算的\r\n过几天会把paper挂出来，应该包含大部分的细节"
      },
      {
        "user": "yuyun2000",
        "created_at": "2024-10-09T09:00:38Z",
        "body": "太强了太强了~"
      }
    ]
  }
]