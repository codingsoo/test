[
  {
    "number": 302,
    "title": "Allow pdf files to be processed directly by LLM calls via attachments",
    "created_at": "2025-02-01T20:55:46Z",
    "closed_at": "2025-02-10T05:19:19Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/302",
    "body": "Some LLM apis (e.g., claude, gemini) support pdf files as attachments in the API request. That is, we don't need to use OCR to convert the pdf to text and inject the text in the prompt. We should explore this in more detail and determine how to support this direct pdf attachment.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/302/comments",
    "author": "shreyashankar",
    "comments": [
      {
        "user": "staru09",
        "created_at": "2025-02-02T08:20:21Z",
        "body": "Is this open to work?"
      }
    ]
  },
  {
    "number": 301,
    "title": "🚀 feat: add docker-compose support",
    "created_at": "2025-02-01T11:34:11Z",
    "closed_at": "2025-02-04T15:57:54Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/301",
    "body": "This pull request introduces docker-compose support by adding necessary port mappings in .env.sample and a new docker-compose.yml file to run the services. Please review the changes and let me know if further adjustments are needed.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/301/comments",
    "author": "Sunwood-ai-labs",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2025-02-04T15:52:26Z",
        "body": "thanks for this! tested locally 💯 \r\n\r\nadded a health check in case people are running in -d mode"
      },
      {
        "user": "shreyashankar",
        "created_at": "2025-02-04T15:57:27Z",
        "body": "tests pass locally"
      }
    ]
  },
  {
    "number": 294,
    "title": "Support for DeepSeek-R1 and Distills in Doc ETL",
    "created_at": "2025-01-29T09:41:41Z",
    "closed_at": "2025-01-30T01:28:23Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/294",
    "body": "#### **Description:**  \nDeepSeek-R1 has taken the AI world by storm, and it would be great to integrate it with **Doc ETL**. However, at the moment, **DeepSeek-R1 does not support structured output nor function calling**.  \n\nI am specifically working with **DeepSeek Distills**, which are available on two platforms:  \n1. **Qwen 2.5-based distill**  \n2. **Meta Llama-based distill**  \n\nThese base models **do** support structured output and function calling, but when Doc ETL processes queries using these models, it automatically transforms them into a function call. This transformation **skips the model's \"thinking step\"**, leading to subpar responses.  \n\n### **Proposed Solution:**  \nTo ensure optimal performance when using **DeepSeek-R1** with **Doc ETL**, we should:  \n1. **Detect when DeepSeek-R1 (or its distills) is being used.**  \n2. **Set `use_tools` to False** so the model does not skip the reasoning step.  \n3. Modify the response handling:\n   - The model’s reasoning process should be **explicitly separated** in the response.  \n   - The reasoning should be parsed from the `<think>` tags.  \n   - Extract and display the reasoning step separately from the final answer.  \n\nThis adjustment will allow us to preserve the model's reasoning process and improve the quality of its responses when integrated with **Doc ETL**.  \n\n",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/294/comments",
    "author": "lemig",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2025-01-29T16:55:10Z",
        "body": "Thanks for doing this! I'll take a look today & merge by end of day :-)"
      }
    ]
  },
  {
    "number": 290,
    "title": "Updated reduce.py to add caching mechanism for scratchpad",
    "created_at": "2025-01-24T12:36:15Z",
    "closed_at": "2025-01-28T17:20:03Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/290",
    "body": "1) Added a new attribute store the cache\r\n2) Added a method to clear the cache in case of performance issues for a future date\r\n3) Modified _increment_fold, _merge_results, _batch_reduce methods to use the cache.\r\n\r\nI was unable to test the new implementation. so please so go through it once.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/290/comments",
    "author": "rrawatt",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2025-01-28T17:20:03Z",
        "body": "Closing since the scratchpad is already stored in memory and doesn't benefit from caching"
      }
    ]
  },
  {
    "number": 265,
    "title": "OSS support added",
    "created_at": "2025-01-06T18:19:47Z",
    "closed_at": "2025-01-09T06:22:42Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/265",
    "body": "Support for OSS Model added.\r\nsolves #127.\r\n",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/265/comments",
    "author": "staru09",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2025-01-08T06:59:01Z",
        "body": "DM'ed on Discord offline"
      }
    ]
  },
  {
    "number": 264,
    "title": "Fixed bugs, improved compatibility, adjusted backend, and enhanced UI",
    "created_at": "2025-01-05T08:59:55Z",
    "closed_at": "2025-01-07T02:50:30Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/264",
    "body": "- Bugfixes and compatibility: Fixed issues with docling-serve return values and URL backslashes.\r\n- Backend request handling: Changed to backend for sending requests.\r\n- Markdown image format: Removed base64 images from Markdown.\r\n- UI adjustments: Reduced file selection component height and ensured processing methods display on one line.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/264/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2025-01-07T02:50:24Z",
        "body": "Excellent. Thank you!"
      }
    ]
  },
  {
    "number": 229,
    "title": "local GPU's conversion of PDFs is really too slow. Just can't bear it…",
    "created_at": "2024-12-04T10:02:19Z",
    "closed_at": "2024-12-07T23:57:04Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/229",
    "body": "… anymore\r\n\r\ntime consumption has been reduced from around 200s to around 15s\r\n\r\n-----\r\n* turn off OCR\r\n* switch to Pdfium for the backend\r\n*  turn on cell matching",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/229/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-12-06T04:49:28Z",
        "body": "Thanks for making the PR--I'm traveling until tomorrow; then will get to this over the weekend :-) "
      }
    ]
  },
  {
    "number": 215,
    "title": "Add variable descriptions that limit the number of concurrent threads",
    "created_at": "2024-11-28T00:40:37Z",
    "closed_at": "2024-11-29T05:56:42Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/215",
    "body": null,
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/215/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-29T05:56:38Z",
        "body": "TY!"
      }
    ]
  },
  {
    "number": 206,
    "title": "ensure the value is converted to a string when it's not an object",
    "created_at": "2024-11-21T12:36:07Z",
    "closed_at": "2024-11-21T23:17:31Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/206",
    "body": "\r\nCould there be something wrong with my local configuration? Why do I always encounter compilation problems... 🤣",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/206/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-21T23:18:17Z",
        "body": "Feel free to DM on discord what your TS config is...I can check it against mine. anyways I'm adding CI for the web app now, so these errors should me minimized moving forward "
      }
    ]
  },
  {
    "number": 188,
    "title": "Sample key only works for first step in pipeline when loading previous steps from cache",
    "created_at": "2024-11-15T14:26:53Z",
    "closed_at": "2024-11-18T02:27:57Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/188",
    "body": "Take this pipeline:\r\n\r\n```\r\ndatasets:\r\n  unprocessed:\r\n    path: input.json\r\n    type: file\r\n\r\ndefault_model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0\r\n\r\npipeline:\r\n  steps:\r\n    - name: cool\r\n      input: unprocessed\r\n      operations:\r\n        - cool_thing_1\r\n        - cool_thing_2\r\n\r\n  output:\r\n    type: file\r\n    path: result.json\r\n    intermediate_dir: intermediate_results\r\n\r\noperations:\r\n   - name: cool_thing_1\r\n    type: map\r\n    output:\r\n      schema:\r\n        processed: str\r\n    prompt: |\r\n      Look at this thing:\r\n      {{ input.unprocessed }}\r\n      Do something\r\n\r\n  - name: cool_thing_2\r\n    type: map\r\n    output:\r\n      schema:\r\n        processed: str\r\n    # Note that this prompt is taking as input the output from the first operation\r\n    prompt: |\r\n      Look at this thing:\r\n      {{ input.processed }}\r\n      Do something\r\n\r\n```\r\n\r\nIf you do `docetl run this_pipeline.yaml` and cancel the operation once cool_thing_2 starts, if you rerun the docetl command, everything loads from cache and all is well. \r\n\r\nHowever, lets say you want to just sample 10 outputs from that cool_thing_2 step so that you can verify that the output is doing what you want before you spend $$ to run the whole pipeline. If you add `sample: 10` to the `cool_thing_2` stage and run docetl, it starts processing but {{input.processed}} is an empty string!\r\n\r\nI think this is some sort of issue with caching or sampling, where sampling later on in the pipeline no longer has the outputs from the previous operation available.\r\n\r\nLet me know if this isn't clear.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/188/comments",
    "author": "njbrake",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-15T20:30:21Z",
        "body": "Thanks for opening this issue! I'll debug & push a fix over the weekend :-)"
      }
    ]
  },
  {
    "number": 185,
    "title": "hotfix: Import declaration conflicts with local declaration of 'Operation'.",
    "created_at": "2024-11-15T02:12:26Z",
    "closed_at": "2024-11-15T07:08:04Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/185",
    "body": "```typescript\r\n\r\nType error: Import declaration conflicts with local declaration of 'Operation'.\r\n\r\n  1 | import React from \"react\";\r\n> 2 | import { Operation, SchemaItem } from \"'app/types\"' (see below for file content);\r\n    |          ^\r\n  3 | import { OutputSchema, PromptInput, CodeInput } from \"./args\";\r\n  4 | import { useMemo } from \"react\";\r\n  5 | import { Input } from \"'components/ui/input\"' (see below for file content);\r\nmake: *** [Makefile:42: run-ui] Error 1\r\n```",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/185/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-15T07:07:55Z",
        "body": "Thanks for this -- my bad for pushing broken code!"
      }
    ]
  },
  {
    "number": 180,
    "title": "Execution of docetl on AWS lambda fails",
    "created_at": "2024-11-14T15:13:20Z",
    "closed_at": "2024-11-15T07:32:17Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/180",
    "body": "Error message:\r\n`[ERROR] OSError: [Errno 30] Cache directory \"/home/sbx_user1051/.docetl/llm_cache\" does not exist and could not be created`\r\n\r\nDeploying docetl to AWS lambda fails, because AWS lambda does not allow write to home directory. AWS lambda writes are only supported to the /tmp directory.\r\n\r\nDOCETL_HOME_DIR is always created at users home directory, can this be configured through the env?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/180/comments",
    "author": "yogitha2023",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-14T18:21:24Z",
        "body": "Thanks for raising this! Does the PR you made fully address you issue? I'll close it if so :)"
      },
      {
        "user": "yogitha2023",
        "created_at": "2024-11-15T05:17:17Z",
        "body": "Yes, the PR addresses it. You may close this. "
      }
    ]
  },
  {
    "number": 172,
    "title": "AttributeError: 'Console' object has no attribute 'post_optimizer_status'",
    "created_at": "2024-11-13T17:37:21Z",
    "closed_at": "2024-11-14T05:59:12Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/172",
    "body": "when i use this , pipeline = pipeline.optimize(model=\"gpt-4o-mini\")\r\nit gives me this error",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/172/comments",
    "author": "sarveshshh8",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-13T18:38:19Z",
        "body": "Ah this was a bug we fixed a week ago; any chance you need to rebase your fork?"
      },
      {
        "user": "sarveshshh8",
        "created_at": "2024-11-14T05:26:30Z",
        "body": "thanks it works now"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-11-14T05:59:12Z",
        "body": "Awesome; closing!"
      }
    ]
  },
  {
    "number": 169,
    "title": " We need to come up with a way to present the code_map, code_reduce, and code_filter operations on the web UI.[web-ui]",
    "created_at": "2024-11-13T09:08:01Z",
    "closed_at": "2024-11-14T19:34:31Z",
    "labels": [
      "enhancement",
      "UI"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/169",
    "body": "* The preliminary requirement is to have a simple UI that allows editing code snippets and defining the output schema.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/169/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-13T18:39:10Z",
        "body": "Agreed!"
      }
    ]
  },
  {
    "number": 164,
    "title": "The information of validating guardrails and gleaning cannot be saved.[Web-UI]",
    "created_at": "2024-11-12T06:59:38Z",
    "closed_at": "2024-11-12T23:37:40Z",
    "labels": [
      "bug",
      "UI"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/164",
    "body": "As stated in the title, the validating information cannot be saved and executed correctly through the web-ui.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/164/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-12T17:09:02Z",
        "body": "Thanks for flagging! I'll take a look later today"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-11-12T23:37:59Z",
        "body": "Hope it works now; please let me know!"
      },
      {
        "user": "plpycoin",
        "created_at": "2024-11-13T00:16:34Z",
        "body": "LGTM, Thanks!👍👍👍"
      }
    ]
  },
  {
    "number": 159,
    "title": "Unable to perform reduce operation on list",
    "created_at": "2024-11-11T16:31:22Z",
    "closed_at": "2024-11-11T21:17:41Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/159",
    "body": "I am using the map operation  in the pipeline.yaml to generate the following o/p\r\n{\r\n\"content\": \"abcde\",\r\n\"classification\": [\"CLASS1\"]\r\n},\r\n{\r\n\"content\": \"qwerty\",\r\n\"classification\": [\"CLASS1\",\"CLASS2\"]\r\n}\r\n\r\nI used the reduce operation to aggregate the content based on the classification key (using reduce_key as \"classification\") in pipeline.yaml . However getting error \r\n\"TypeError: unhashable type: 'list'\"\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/159/comments",
    "author": "gcoolgithub",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-11T21:18:49Z",
        "body": "Thanks for flagging this! I just added support for list-type reduce_keys, which will sort the values in the list, convert to a tuple, and use this tuple for the reduce_key. LMK if this is not what you intended; feel free to re-open the issue."
      },
      {
        "user": "gcoolgithub",
        "created_at": "2024-11-12T16:31:23Z",
        "body": "The aggregation is working in reduce operation now , but if the same classification is present in multiple lists, then it is getting aggregated separately.\r\nexample\r\nIf this is the input\r\n{\r\n            \"content\": \"Document about AI and ML\",\r\n            \"classifications\": [\"AI\", \"ML\"]\r\n        },\r\n       {\r\n            \"content\": \"Document about AI only\",\r\n            \"classifications\": [\"AI\"]\r\n        }\r\n\r\n**then the expected aggregation  should be**\r\nAI -> Document about AI and ML. Document about AI only\r\nML -> Document about AI and ML\r\n\r\n**The actual aggregation is**\r\nAI -> Document about AI only\r\nAI,ML -> Document about AI and ML\r\n"
      }
    ]
  },
  {
    "number": 152,
    "title": "hotfix: an error occurred when running make run-ui",
    "created_at": "2024-11-07T01:20:49Z",
    "closed_at": "2024-11-07T16:31:09Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/152",
    "body": "## ENV:\r\n\r\n```bash\r\n$ node --version\r\nv18.20.4\r\n\r\n$ npm --version\r\n10.7.0\r\n\r\n```\r\n\r\n## Err Info:\r\n```bash\r\n./src/components/Output.tsx:254:43\r\nType error: Type '{ children: Element; source: string; className: string; }' is not assignable to type 'IntrinsicAttributes & BookmarkableTextProps'.\r\n  Property 'className' does not exist on type 'IntrinsicAttributes & BookmarkableTextProps'.\r\n\r\n  252 |         </div>\r\n  253 |       ) : outputs.length > 0 ? (\r\n> 254 |         <BookmarkableText source=\"output\" className=\"h-full\">\r\n      |                                           ^\r\n  255 |           <ResizableDataTable\r\n  256 |             data={outputs}\r\n  257 |             columns={columns}\r\n```\r\n\r\n```bash\r\n./src/components/ResizableDataTable.tsx:240:5\r\nType error: Object literal may only specify known properties, and 'onColumnSizingStart' does not exist in type 'TableOptions<T>'.\r\n\r\n  238 |       },\r\n  239 |     },\r\n> 240 |     onColumnSizingStart: () => setIsResizing(true),\r\n      |     ^\r\n  241 |     onColumnSizingEnd: () => debouncedSetIsResizing(false),\r\n  242 |   });\r\n  243 |\r\n```",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/152/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-07T16:31:05Z",
        "body": "thanks for catching this! LGTM on my end"
      }
    ]
  },
  {
    "number": 147,
    "title": "Fix: Default max comparison pairs in resolve.py",
    "created_at": "2024-11-05T14:30:17Z",
    "closed_at": "2025-01-09T05:35:50Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/147",
    "body": "This PR closes #130 \r\nI have added a custom rate limit for different models which can be edited by the user.\r\nThe OpenAI limits need to be updated though.\r\n\r\nSample usage is as follows:\r\n```\r\npipeline = Pipeline(\r\n    name=\"resolution_pipeline\",\r\n    datasets={...},\r\n    operations=[...],\r\n    steps=[...],\r\n    output=...,\r\n    rate_limits={\r\n        \"claude-3.5-sonnet\": 300,  # Custom lower limit\r\n        \"gpt-4o\": 800,\r\n        \"my-custom-model\": 100\r\n    }\r\n)\r\n\r\n# Check different models\r\nmodels = [\"claude-3-sonnet\", \"gpt-4o\", \"my-custom-model\", \"unknown-model\"]\r\nfor model in models:\r\n    limit_info = pipeline.get_rate_limits(model)\r\n    print(f\"{model}: {limit_info}\")\r\n\r\n# Output:\r\nclaude-3-sonnet: {'requests_per_minute': 300, 'source': 'custom'}\r\ngpt-4o: {'requests_per_minute': 800, 'source': 'custom'}\r\nmy-custom-model: {'requests_per_minute': 100, 'source': 'custom'}\r\nunknown-model: {'requests_per_minute': 200, 'source': 'fallback'}\r\n```",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/147/comments",
    "author": "staru09",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-05T18:37:10Z",
        "body": "Discussing offline in Discord"
      }
    ]
  },
  {
    "number": 146,
    "title": "chore: Use environment variable configuration files to set host, port",
    "created_at": "2024-11-05T07:37:49Z",
    "closed_at": "2024-11-06T00:05:55Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/146",
    "body": "* Customize host and port.\r\n* Specify the OpenAI base URL, OpenAI API key and model of the assistant in UI.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/146/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-05T18:37:40Z",
        "body": "Thanks for this; will take a look later today or tomorrow!"
      }
    ]
  },
  {
    "number": 142,
    "title": "chore, load envs from current directory",
    "created_at": "2024-11-01T08:45:18Z",
    "closed_at": "2024-11-01T15:24:34Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/142",
    "body": "* fastapi uses the environment variables in. env under the current directory",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/142/comments",
    "author": "plpycoin",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-11-01T15:24:28Z",
        "body": "Thank you!!"
      }
    ]
  },
  {
    "number": 126,
    "title": "Fix Resolve and Map progress bars",
    "created_at": "2024-10-24T08:57:50Z",
    "closed_at": "2024-10-24T19:11:12Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/126",
    "body": "The `RichLoopBar` in `Map` and `Resolve` gets updated beyond the number of expected items. `for i in pbar` should suffice for updating it, `pbar.update(i)` can be removed.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/126/comments",
    "author": "michielree",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-24T19:11:09Z",
        "body": "cool; thank you!"
      }
    ]
  },
  {
    "number": 108,
    "title": "New operation: GraphResolve",
    "created_at": "2024-10-15T10:43:59Z",
    "closed_at": "2024-10-23T05:36:02Z",
    "labels": [
      "New operation"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/108",
    "body": "After my first few steps of extraction, I have something along the lines of\r\n\r\n```\r\n[\r\n  {\"name\": \"Steam engine\", \"related_to\": [\"Boiler\", \"Turbine\", \"Ship\", \"Locomotive\"]},\r\n  {\"name\": \"Steam boiler\", \"related_to\": [\"Turbine\", \"Steam engine\", \"Steam ship\", \"Locomotive\"]},\r\n  {\"name\": \"Locomotive\", \"related_to\": [\"Steam engine\", \"Steam boiler\", \"Train\", \"Track\"]},\r\n]\r\n```\r\n\r\nAs can be seen, the names listed in the `related_to` array are not always the exact same as the ones listed in `name` of the items. It'd like to somehow run a resolve operation to unify this set of names, so `related_to` links don't point out into the void!\r\n\r\nBut how can such an operation be structured in docetl? I believe this isn't possible with the current set of operations. So this is mostly a question of what new operation(s) we could add that would make this possible to solve using docetl.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/108/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-15T16:11:24Z",
        "body": "Hmm interesting. If they were all names (or stored in a name field that was exploded/unnested), resolve could at least create the distinct set of names, and somehow you'd have to wrangle the join of the names back on the `related_to`. But this is quite tedious.\r\n\r\nI wonder if it makes sense to write a GraphResolve Operation for this, which takes in an `edge_key` (which will be a list, like `related_to`) and a `node_key` (e.g., `name` in your example), with the same interface as Resolve (without defining the output schema, since we know the unit of resolution), but the operation will make edits to the `node_key` and `edge_key` fields.\r\n\r\nLet me think about other ideas today. I'm at the conference so my thoughts are a bit all over the place :-)"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-15T16:34:47Z",
        "body": "It's the wrangling that's a bit... hard. GraphResolve is simple, straight forward, but a bit non-generic. Could we do something like providing an operation that runsa sub-pipeline (where you'd just run resolve)?"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-16T04:45:24Z",
        "body": "Wouldn't the sub-pipeline also be non-generic here? Why not just include the sub-pipeline's ops in the main pipeline? Maybe i don't fully understand what you mean\r\n\r\nI don't think GraphResolve is so bespoke, i think creating knowledge graphs can be a common pattern maybe?"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-16T08:41:17Z",
        "body": "The idea would be that there's a generic \"split out this data and run this set of ops on it, then merge it again\" operation, where the ops are configurable, and the difference from just sticking them in the main pipeline would be what items they operate on. But maybe it's overkill?"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-17T21:37:31Z",
        "body": "yeah it seems like this GraphResolve use case is actually common. we don't have any operators to help create graphs"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-18T15:27:16Z",
        "body": "Ok, changed the title to reflect that a new op is how we'll solve this :) I'll see what I can cook up!"
      }
    ]
  },
  {
    "number": 107,
    "title": "Come up with a better way to sync python api and the execution engine",
    "created_at": "2024-10-14T22:43:03Z",
    "closed_at": "2024-11-14T17:40:39Z",
    "labels": [
      "refactor",
      "medium"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/107",
    "body": "Currently, one has to manually update `schemas.py` and `api.py` to match operations and executors. I wonder if there's a way we can automatically generate the python API, or at least, do a sync and come up with a better plan to sync.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/107/comments",
    "author": "shreyashankar",
    "comments": [
      {
        "user": "redhog",
        "created_at": "2024-10-15T10:38:22Z",
        "body": "If you generate json-schema from the operation signatures (the parameter type annotations etc), you could generate the dataclasses you use for the python API on the fly from that.\r\n\r\nHowever, I think it might be worth it to reevaluate what the python API should actually _do_. In my mind, it doesn't need those classes. It's fine to have dictionaries and lists (parsed yaml) be the input - the pipeline already does a syntax check.\r\n\r\nHowever, what would be useful would be for the input and output to not be files, but be python variables.\r\n\r\nE.g. an api like this:\r\n\r\n```\r\np = docetl.Pipeline({\r\n  \"operarations\": [...],\r\n  \"pipeline\": {\r\n    \"steps\": [\"extract-stuff\", \"unnest-stuff\", \"resolve-stuff\"]\r\n  }\r\n}) # This does the syntax check\r\n\r\noutput = p.run([\r\n  {\"text\": \"Some text...\"}, {\"text\": \"Another text...\"}, ...\r\n])\r\nprint(output)\r\n```\r\n"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-15T16:03:31Z",
        "body": "This makes a ton of sense"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-15T16:40:59Z",
        "body": "And for loading/saving, you'd add:\r\n\r\n`datasets = p.load()`\r\n\r\nand\r\n\r\n`p.save(data)`\r\n\r\nthat would load/save according to the `dataset` and `output` sections (and throw an exception if they are not present, they'd have to be optional for the above API to work).\r\n\r\nThat would also mean that the main cli would just do\r\n\r\n```\r\np = docetl.Pipeline(yaml.load(filename))\r\np.save(p.run(p.load()))\r\n```\r\n\r\n(Hm, this would mean `run()` takes a dictionary of `{datasetname: [items...]}`, not just `[items...]` as parameter. Not as obvious, but given that the pipeline can have multiple inputs I guess we have to)"
      }
    ]
  },
  {
    "number": 106,
    "title": "docs: add 'output' argument to ResolveOp code eg",
    "created_at": "2024-10-14T21:03:49Z",
    "closed_at": "2024-10-14T22:16:09Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/106",
    "body": null,
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/106/comments",
    "author": "goutham794",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-14T22:14:08Z",
        "body": "thank you!!"
      }
    ]
  },
  {
    "number": 100,
    "title": "Sample (+ Outlier Functionality) Operation",
    "created_at": "2024-10-12T19:44:34Z",
    "closed_at": "2024-10-14T12:14:37Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/100",
    "body": "I've combined the Sample and Outliers Operators into one (Sample). LMK what you think.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/100/comments",
    "author": "shreyashankar",
    "comments": [
      {
        "user": "redhog",
        "created_at": "2024-10-13T18:12:43Z",
        "body": "So, you have\r\n\r\n```\r\n- type: sample\r\n  outliers:\r\n    samples: 0.9\r\n```\r\n\r\nfor removing 10% outliers vs\r\n\r\n```\r\n- type: sample\r\n  samples: 0.9\r\n```\r\n\r\nfor removing 10% random samples.\r\n\r\nWouldn't it be better to specify size with `samples` at the toplevel, and then say how you want to sample separately:\r\n\r\n* `sample: randomly`\r\n* `sample: stratified`\r\n* `sample: outliers`\r\n* `sample: non-outliers`\r\n\r\nor some such?\r\n\r\nWhat about specifying an explicit center btw? You want that part of another operation, or you don't want it at all?\r\n\r\n"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-13T20:38:08Z",
        "body": "Good catch; I forgot the center logic! "
      }
    ]
  },
  {
    "number": 86,
    "title": "Question: Inneficient map",
    "created_at": "2024-10-08T17:51:39Z",
    "closed_at": "2024-10-30T16:05:11Z",
    "labels": [
      "efficiency",
      "hard"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/86",
    "body": "So... I have a large dataset I want to run a map operation over, where the relevant key for each item is small. For example, the key might contain some single line of text, and the map might use a prompt to determine if this text is likely to be the name of a person.\r\n\r\nThe cost and time of running many llm calls ends up pretty high, while it should theoretically be possible to batch the values into say 100 values at a time, and use a slightly more complex prompt that outputs a list of true/false instead of just a single boolean value.\r\n\r\nI can't see a way to do this in docetl currently, but it shouldn't be too hard to implement as a pair of operations: One that batches items into groups of N items, and one that takes such a group that might also have other keys with lists of the same length, merges in the values of those extra keys, and then flattens the whole thing into just the list of items again.\r\n\r\nThis could potentially be used by an optimization rule to mape map operations faster if the above case is detected.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/86/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-09T05:28:46Z",
        "body": "Agreed, this is what I was trying to get at with #7 but was not clear enough. \r\n\r\nI wonder if a cleaner solution is to implement a batchmap operation, which takes in a prompt template and batch size, and does the batching & flattening you mentioned. Two operations may be excessive...\r\n\r\nI really like the idea of automatically determining this in the optimizer (e.g., ideal batch size)"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-09T11:30:27Z",
        "body": "A batchmap is probably the way to go yes. Unfortunately, the same problem does come up in filter and cluster... So what about them?"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-09T18:10:18Z",
        "body": "Good point. maybe there's a way to set `batch` to true (or a size) in each of the 3 operation, and treat the prompt as a batch prompt if batching is detected? No need to introduce new operators then"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-10T18:35:21Z",
        "body": "Dug a bit more, and what I'd like to propose is to replace \r\n\r\n```\r\n        with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\r\n            futures = [executor.submit(_process_map_item, item) for item in input_data]\r\n```\r\n\r\nin each operation, with a single call to a `APIWrapper.call_llm_with_batching()` that takes a list of dicts with arguments like what `call_llm` does now.\r\n\r\nThat way all the batching code can be generalize inside the `APIWrapper`, and the operations just need to send on the batch size parameter...\r\n\r\nHow does that sound? Would you be able to do this? I'm a bit unsure how to handle the interaction between this and gleaning (and validation / parsing for that sake)..."
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-11T03:20:37Z",
        "body": "The reason we didn't bake in validation into call_llm is because validation is at the operation output level, not the LLM call level (e.g., reduce and resolve have multiple llm calls orchestrated).\r\n\r\nI like having a `batch_call_llm` method in API wrapper, but we may want different batching & parsing logic for each operation (e.g., for filter, we might instruct the LLM to return IDs or numbers of the documents that pass the filter; for map we may instruct the LLM to give an output per document).\r\n\r\nFor validation + gleaning:\r\nI'll refactor the gleaning function to to operate directly on the outputs. then if gleaning is enabled, i'll send  document + call_llm or batch_call_llm output pairs (with the gleaning config) to the gleaning function, for each document. Similarly, i'll create a validation + retry function that operates directly on outputs, for each output/document instead of batch.\r\n\r\nI'll create a PR in the next couple of days with this proposal."
      }
    ]
  },
  {
    "number": 83,
    "title": "Suggestion: A new sampler operation for debugging",
    "created_at": "2024-10-08T08:34:10Z",
    "closed_at": "2024-10-14T12:15:22Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/83",
    "body": "An operation that samples a subset of the items, to be used when developing a pipeline. You stick it right before any part of the pipeline that you are developing on right now, making running that much cheaper while you try it out by only running it on a subset of the data.\r\n\r\nNote: The random seed used for the sampling must be possible to set so that which items the subsequent operations work on stays the same even when you rerun the pipeline.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/83/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-09T05:37:06Z",
        "body": "Currently we allow operations to take in a `sample` parameter (just added to the docs in #87 )"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-09T11:29:33Z",
        "body": "Hm, I guess that solves the problem, but is less explicit to the user, and requires code in all operations..."
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-10T00:10:52Z",
        "body": "Hm why might someone add the sample param to each operator? If you sample the first operator, then there are fewer docs going into successive operators"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-10T16:17:42Z",
        "body": "Because you (or at least me) build a pipeline iteratively. I'd read in the full dataset, sample some and write the first map. If that works, I'd move the sampling and put it after the map, running it and checking the full intermediate map output. Then repeat for each stage until the whole pipeline is built... That way, you can at any point change your sampling to a wider selection, without having to rerun the whole pipeline, and once you're done with the last stage, you have the whole output for your original input."
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-10T18:25:56Z",
        "body": "Ahh i see. so having a sample operation allows you to just move around the operation order in the pipeline steps...ok that makes sense. i'm on board\r\n\r\nit can also allow specific ids to be selected in the sample"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-10T18:30:45Z",
        "body": "Hm, that'd be interesting yes. I'll see if I can hack something together."
      }
    ]
  },
  {
    "number": 72,
    "title": "Parsers / loaders are a bit limited",
    "created_at": "2024-10-05T20:19:32Z",
    "closed_at": "2024-10-08T04:34:08Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/72",
    "body": "Limitations:\r\n\r\n* Can not use multiple input fields / the entire item\r\n* Can not take additional parameters from config\r\n* Can only output a single field\r\n  * Use-cases for multiple fields:\r\n    * Parse some text out of a document and additionally some metadata (set of name/value pairs)\r\n    * Parse all columns of e.g. a csv into separate fields\r\n \r\nHow can we redesign this to make it more flexible?",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/72/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-05T22:17:49Z",
        "body": "Agreed. One idea is for the custom parsers to accept a document and return a list of documents/dictionaries, which get merged into the original documents, instead of a list of strings. Then the user doesn't have to specify `input_key` or `output_key`."
      },
      {
        "user": "redhog",
        "created_at": "2024-10-06T07:54:20Z",
        "body": "I think that, coupled with arbitrary arguments from the config yaml to the parser, would be good enough. Parsers that wants to use an input key, would just take that as an argument.\r\n\r\nWe'd just change the yaml to be:\r\n\r\n```\r\ndatasets:\r\n  my_dataset:\r\n    type: file\r\n    source: local\r\n    path: \"data.json\"\r\n    parsing:\r\n      - function: my_parser\r\n        argument_name_1: value\r\n        argument_name_2: value\r\n        argument_name_3: value\r\n```\r\n\r\nAnd for the existing parsers, we add the parameters `input_key` and `output_key` and it will be compatible with old pipeline yaml files too :)\r\n\r\nSignature for a parser function would then be:\r\n\r\n```\r\ndef my_parser(\r\n    item: dict[str, Any],\r\n    **kw) -> list[dict[str, Any]]:\r\n```\r\n"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-07T06:10:04Z",
        "body": "I like this idea! I'm on board.\r\n\r\nFeel free to take it if you'd like--I'm trying to get a v0 of the UI out this week :-) But i can also get to it later no worries"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-07T09:46:53Z",
        "body": "I'd be happy to do this one, but I want the rate_limit stuff merged first (with the runner argument), to limit merge conflicts..."
      }
    ]
  },
  {
    "number": 58,
    "title": "Outliers-filter",
    "created_at": "2024-10-03T18:12:59Z",
    "closed_at": "2024-10-14T12:15:30Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/issues/58",
    "body": "* Embed some field\r\n* Calculate the average embedding across all entries\r\n* Remove entries whose embeddings are farthest from the average\r\n  * Remove the N farthest away\r\n  * Remove the M farthest away percentiles\r\n  * Remove those more than X standard deviations away\r\n ",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/58/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "ahmedtadde",
        "created_at": "2024-10-05T12:54:08Z",
        "body": "hey @redhog, this is an interesting proposal. Do you have 1-2 concrete cases where this would be useful?"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-05T15:33:16Z",
        "body": "I have one use case right now: Ingest a bunch of books or papers on a subject, extract all concepts and how they relate to each other. Unfortunately, cited authors and works end up being added as concepts! But I'm sure their embeddings would be quite far from the rest of the concepts..."
      }
    ]
  },
  {
    "number": 48,
    "title": "Cluster operation",
    "created_at": "2024-10-02T22:48:43Z",
    "closed_at": "2024-10-15T13:27:40Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/48",
    "body": "A bit similar to Resolve in aim, but without actually reducing the data:\r\n\r\n* Embed a field in each item\r\n* Cluster the embeddings\r\n* Generate a name for each cluster based on the field values\r\n* Add the cluster name as a new field\r\n\r\nExtension: Do this using hierarchical clustering, and add the path of clusters from the entry all the way to the top cluster encompassing the entire dataset, as an array of cluster names in a new field.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/48/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-03T07:28:10Z",
        "body": "I think this is a nice idea. We can implement this as a type of resolve, actually. The current resolve operator takes in the following:\r\n- comparison_prompt (to determine equivalence classes of documents)\r\n- resolution_prompt (to canonicalize a set of keys that are equivalent)\r\n\r\nA semantic cluster-based resolve would require the following parameters:\r\n- k (number of clusters)\r\n- resolution_prompt (same as before, to canonicalize a group/cluster of document keys)\r\n- optional: number of levels for hierarchical clustering? what API would you want here?"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-03T15:27:55Z",
        "body": "So I played around with hierarchical clustering with llama-index before, using scikit-learn AgglomerativeClustering as the clusterer. There, you can get all clusters, at all levels, as a tree, including cluster distances at all levels (pairwise). I then ran a recursive resolution-prompt llm to generate labels for all clusters. In fact, I generated both labels and descriptions, and used the descriptions, not the labels, as input to the next level resolution prompt to generate both description and label (as labels can be short enough to not be uniquely identifying the concept). It would be nice to represent all of that somehow.\r\n\r\nI was thinking it would create something like:\r\n\r\n```\r\ncluster_on: \"{{title}} - {{description}}\" # Input to embedding model\r\noutput_key: categories\r\nresolution_prompt: |\r\n  Summarize the following descriptions of a concept into a single description and also provide a short title:\r\n\r\n  {% for entry in inputs %}\r\n    {{ entry.title }}: {{entry.description}}\r\n  {% endfor %}\r\n```\r\n\r\nExample output\r\n```\r\n{\r\n  \"title\": \"Zebra\",\r\n  \"description\": \"African equines with distinctive black-and-white striped coats. There are three living species: Grévy's zebra (Equus grevyi), the plains zebra (E. quagga),...\",\r\n  categories: [\r\n    {\r\n      \"title\": \"Equus\",\r\n      \"description\": \"A genus of mammals in the family Equidae, which includes horses, asses...\",\r\n      \"distance\": 0.01\r\n    },\r\n    {\r\n      \"title\": \"Equidae\",\r\n      \"description\": \"The horse family is the taxonomic family of horses and related animals, including the extant horses...\",\r\n      \"distance\": 0.05\r\n    },\r\n    ...\r\n    {\r\n       \"title\": \"Perissodactyla\",\r\n       \"description\": \"An order of ungulates. The order includes about 17 living species divided into three families: Equidae , Rhinocerotidae, and Tapiridae ...\"\r\n       \"distance\": 0.12\r\n    },\r\n    ...\r\n    {\r\n       \"title\": \"Eukaryote\",\r\n       \"description\": \"Organisms whose cells have a membrane-bound nucleus...\",\r\n       \"distance\": 0.98\r\n    }\r\n  ]\r\n}\r\n"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-03T15:29:35Z",
        "body": "I think the comparison prompt is kinda unnecessary btw: once the clusters are big enough, it's not gonna do anything really useful (how do you know if these concepts are too far from each other at this particular clustering level, in the llm?)."
      },
      {
        "user": "redhog",
        "created_at": "2024-10-03T15:30:24Z",
        "body": "Nice thing with agglomerative clustering is that you do not have to provide a k: You get pairs of nodes as the lowest level clusters, and then the pairs are paired up, all the way to the top."
      }
    ]
  },
  {
    "number": 45,
    "title": "Entrypoints",
    "created_at": "2024-10-02T22:26:48Z",
    "closed_at": "2024-10-03T06:44:28Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/45",
    "body": "This PR makes it possible for third party packages to register new parser tools and operations using the standard python entrypoint / plugin system.\r\n\r\nIn particular, this means that a project that uses docetl that needs a custom parser, does not need to write the parser code inline in the yaml pipeline file, but can implement it in a normal python package and just reference it in the yaml, as long as the python package is installed using pyproject.toml or setup.py and registers the parser function there.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/45/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-03T06:44:16Z",
        "body": "Awesome -- thank you!"
      }
    ]
  },
  {
    "number": 40,
    "title": "Bugfix for sqlite3 operation error in cache",
    "created_at": "2024-10-02T10:11:20Z",
    "closed_at": "2024-10-02T15:42:10Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/40",
    "body": "diskcache isn't 100% threadsafe and can under certain conditions end up unable to open the sqlite database with an `OperationalError`. This only happens if threads do not close their cache when not using it. Closing the cache and reopening it is slightly slower, but fixes this problem. Solution copied from the documentation of `diskcache`.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/40/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-02T15:42:06Z",
        "body": "Thank you! Merging into another staging branch to run tests."
      }
    ]
  },
  {
    "number": 39,
    "title": "RateLimit error",
    "created_at": "2024-10-02T10:09:23Z",
    "closed_at": "2024-10-02T16:13:38Z",
    "labels": [],
    "url": "https://github.com/ucbepic/docetl/pull/39",
    "body": "OpenAI API / litellm  can throw a RateLimit error if you hit your rate limit for the API. This is a bugfix to make that not case the pipeline to fail, but just wait a bit and retry (which will suceed). Very similar to a timeout error in handling.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/39/comments",
    "author": "redhog",
    "comments": [
      {
        "user": "shreyashankar",
        "created_at": "2024-10-02T16:13:19Z",
        "body": "Thank you! Sorry for totally mucking up the rebase"
      },
      {
        "user": "redhog",
        "created_at": "2024-10-02T22:28:12Z",
        "body": "No problem :) I generally recommend staying far away from rebase and doing traditional merges (preserving history), as that makes git bisect a better debugging tool :P"
      }
    ]
  },
  {
    "number": 11,
    "title": "Support Audio File Inputs as Documents",
    "created_at": "2024-09-23T00:37:30Z",
    "closed_at": "2024-10-01T05:26:12Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/11",
    "body": "We need to add support for audio file inputs as documents in our pipeline system. This will allow users to process audio files (e.g., MP3) and automatically transcribe them using services like OpenAI's Whisper API.\r\n\r\n## Current Situation\r\nCurrently, our system supports text-based documents and JSON files as inputs. We need to extend this functionality to include audio files.\r\n\r\n## Proposed Solutions\r\n\r\n1. Augment Dataset Types:\r\n   - Add a new dataset type called \"audio\" or \"speech\"\r\n   - Example:\r\n     ```yaml\r\n     datasets:\r\n       debate_audio:\r\n         type: audio\r\n         path: \"/path/to/audio/files/*.mp3\"\r\n         transcription_service: whisper  # Optional, default to Whisper\r\n     ```\r\n\r\n2. JSON Object with Audio Keys:\r\n   - For JSON datasets, introduce a convention where keys prefixed with `_audio` are treated as paths to audio files\r\n   - Example:\r\n     ```yaml\r\n     datasets:\r\n       mixed_data:\r\n         type: file\r\n         path: \"/path/to/mixed_data.json\"\r\n     ```\r\n     Where the JSON file might contain:\r\n     ```json\r\n     {\r\n       \"title\": \"Presidential Debate 2023\",\r\n       \"date\": \"2023-10-15\",\r\n       \"_audio\": \"/path/to/debate_audio.mp3\"\r\n     }\r\n     ```\r\n\r\n3. Audio Transcription Operation:\r\n   - Add a new operation type for audio transcription\r\n   - Example:\r\n     ```yaml\r\n     operations:\r\n       - name: transcribe_audio\r\n         type: transcribe\r\n         input_key: audio_path\r\n         output_key: transcript\r\n         service: whisper\r\n     ```\r\n\r\n## Implementation Details\r\n\r\n1. Add audio file detection and handling in the dataset loading process\r\n2. Implement audio transcription functionality, potentially using OpenAI's Whisper API\r\n3. Update the pipeline execution engine to handle audio files and transcription\r\n4. Modify relevant schema definitions to include audio-related fields\r\n5. Update documentation to reflect new audio input capabilities",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/11/comments",
    "author": "shreyashankar",
    "comments": [
      {
        "user": "elecnix",
        "created_at": "2024-09-30T01:09:19Z",
        "body": "As a first step, adding support for \"_file\": \"/path/to/file.txt\" would enable multi-file input."
      }
    ]
  },
  {
    "number": 6,
    "title": "Support UDFs",
    "created_at": "2024-09-18T05:19:04Z",
    "closed_at": "2024-11-04T20:42:44Z",
    "labels": [
      "good first research issue"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/6",
    "body": "Not all operations need to be LLM operations :-)\r\n\r\nUsers should be able to define Python functions for map and reduce operations, instead of LLM calls.\r\n\r\nThis will also enable a code synthesis optimization: if an operation can be accurately executed via code than an LLM call that operates directly on the document, then we should prefer the code implementation.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/6/comments",
    "author": "shreyashankar",
    "comments": [
      {
        "user": "staru09",
        "created_at": "2024-10-02T17:58:23Z",
        "body": "Is this available to work?"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-02T18:00:51Z",
        "body": "yes!! feel free to take it! we might want a proposal first though, lmk what you think"
      }
    ]
  },
  {
    "number": 1,
    "title": "Adaptive `compare_batch_size` for Resolve Operator",
    "created_at": "2024-09-17T17:52:38Z",
    "closed_at": "2024-10-28T02:17:34Z",
    "labels": [
      "good first research issue",
      "efficiency"
    ],
    "url": "https://github.com/ucbepic/docetl/issues/1",
    "body": "## Current Implementation\r\nOur `ResolveOperation` class uses a Union-Find (aka Disjoint Set Union) algorithm for grouping similar items efficiently. Here's how it works:\r\n\r\nWe've got two main data structures:\r\n1. `clusters`: A list of sets, each representing a cluster of items.\r\n2. `cluster_map`: A dictionary that maps each item to its current cluster representative.\r\n\r\nThe key functions are:\r\n- `find_cluster(item)`: Finds the representative of an item's cluster.\r\n- `merge_clusters(item1, item2)`: When two items match, this merges their clusters. It finds the reps for both items' clusters, then combines the smaller cluster into the larger one for efficiency. The `cluster_map` gets updated to reflect this merge.\r\n\r\nThe resolution process goes like this:\r\n1. Each item starts in its own cluster.\r\n2. We generate all possible item pairs to compare.\r\n3. We process these pairs in batches (controlled by `compare_batch_size`, default is 100).\r\n4. For each batch:\r\n   a. We use an LLM to do pairwise comparisons and see if items match.\r\n   b. For each matching pair, we call `merge_clusters` to combine their clusters.\r\n5. We keep doing this until we've compared all pairs.\r\n6. Finally, we collect all non-empty clusters as the result.\r\n\r\nThis approach lets us do efficient, incremental clustering as we find matches, without rebuilding the whole cluster structure after each match. Processing comparisons in batches means we can parallelize LLM calls, which helps with overall performance.\r\n\r\n## Problem\r\nThe fixed `compare_batch_size` we're using now can lead to some performance issues, especially with large datasets. Here's what can happen:\r\n1. If the batch size is too small, we end up making too many LLM API calls, which slows things down and can get expensive.\r\n2. If it's too large, we might overwhelm system memory or hit API rate limits.\r\n3. Our one-size-fits-all approach doesn't adapt to different dataset sizes or system capabilities.\r\n\r\nThis lack of flexibility can make execution times unnecessarily slow for large datasets, or lead to inefficient resource use for smaller ones.\r\n\r\n## Proposed Enhancement\r\nWe should automatically configure `compare_batch_size` based on the number of pairwise comparisons, but only if the user hasn't specified it themselves. This would help optimize performance for datasets of all sizes.\r\n\r\n## Tasks\r\n1. Implement a function to calculate an appropriate `compare_batch_size` based on the number of pairwise comparisons. We need to consider factors like:\r\n   - Total number of comparisons\r\n   - Available system resources (e.g., CPU cores, memory)\r\n   - Typical LLM response times\r\n2. Modify the `execute` method to use this function when `compare_batch_size` isn't user-specified.\r\n3. Add appropriate logging to let users know what batch size we've automatically selected.\r\n4. Update our documentation to explain this new adaptive behavior.\r\n5. Implement unit tests to verify that the automatic configuration works as expected.\r\n6. (Optional) Consider adding a configuration option to enable/disable this automatic sizing.\r\n\r\n## Expected Outcome\r\n- Better performance for large datasets without needing manual tuning.\r\n- More efficient resource utilization across different hardware setups.\r\n- Maintained or improved efficiency for smaller datasets.\r\n\r\n## Additional Considerations\r\n- We need to make sure the automatic configuration doesn't negatively impact smaller datasets.\r\n- We should think about setting upper and lower bounds for the batch size to prevent extreme values.\r\n- It'd be good to evaluate how this affects total execution time and resource usage across various dataset sizes.",
    "comments_url": "https://api.github.com/repos/ucbepic/docetl/issues/1/comments",
    "author": "shreyashankar",
    "comments": [
      {
        "user": "sushruth2003",
        "created_at": "2024-10-12T23:05:22Z",
        "body": "Is this available to work on? "
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-13T04:01:58Z",
        "body": "Yes!"
      },
      {
        "user": "shreyashankar",
        "created_at": "2024-10-28T02:17:18Z",
        "body": "#128 closes this"
      }
    ]
  }
]