[
  {
    "number": 129,
    "title": "Dedicate more uv area for specific part (i.e. human face)?",
    "created_at": "2025-01-08T04:23:12Z",
    "closed_at": "2025-01-16T06:03:48Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/129",
    "body": "I am having a hard time figuring out how to turn a picture of a human to a 3D model with clear facial texture. I tried changing the texture resolution to 8192, My 24gb 3090 was almost out of ram but it's still a blur.\n\nI have very little knowledge about gaussian splatting, is there any options or parameters I can tweak for to improve the facial quality?\n\nNormally in conventional 3D modelling we dedicate more uv area of more important parts, how can we do it here? Or did anyone tried any \"hack\" to work around this?\n\nThank you very much!\n\n(Edit: I don't want to show my face so I will provide my current output results later tonight, sorry. Again, thanks for anyone who can provide guidance or help!)",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/129/comments",
    "author": "zaqxs123456",
    "comments": [
      {
        "user": "denis130",
        "created_at": "2025-01-08T05:01:41Z",
        "body": "Trellis currently is very bad for faces, hair, animals, repeating patterns or text, no resolution hack will fix that."
      }
    ]
  },
  {
    "number": 127,
    "title": "Install on Apple M series device, using MPS (Metal Performance Shaders) instead of CUDA",
    "created_at": "2025-01-07T13:17:54Z",
    "closed_at": "2025-01-08T11:02:53Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/127",
    "body": "Hi, I know this is not an issue, I just wanted to know if it would be possible somehow to get this working on MacOS.\nUnfortunately I'm not very experienced with this stack :/\nIf there's any solution/workaround, please let us know here.\n\nThank you!",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/127/comments",
    "author": "tamas-torjek",
    "comments": [
      {
        "user": "8bitgentleman",
        "created_at": "2025-01-10T20:22:09Z",
        "body": "Did you figure this out @tamas-torjek ?"
      }
    ]
  },
  {
    "number": 112,
    "title": "How to implement this model in nvidia tesla v100 gpu?",
    "created_at": "2025-01-03T09:03:22Z",
    "closed_at": "2025-01-06T10:09:14Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/112",
    "body": "It throws an error saying that \"FlashAttention only supports ampere gpus or newer\", then how can I use this model since my server has a nvidia tesla v100 gpu? ",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/112/comments",
    "author": "Sakthi691",
    "comments": [
      {
        "user": "0lento",
        "created_at": "2025-01-03T11:31:36Z",
        "body": "You can use xformers on older gpu's (set ATTN_BACKEND to xformers, there's example on the readme).\n"
      }
    ]
  },
  {
    "number": 81,
    "title": "How to convert a 3D asset into Structured LATents (SLAT) ?",
    "created_at": "2024-12-20T04:43:02Z",
    "closed_at": "2024-12-25T03:45:59Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/81",
    "body": "Hello, author! I am very interested in your work, especially the design of Structured LATents. I want to know how to convert a 3D asset (mesh、NeRF、3DGS) into Structured LATents? According to your paper, this process includes steps such as voxelization, 2D feature extraction, and encoding. Will you open source this part of the code?\n\nLooking forward to your reply！",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/81/comments",
    "author": "XiaokunSun",
    "comments": [
      {
        "user": "YuDeng",
        "created_at": "2024-12-25T02:45:02Z",
        "body": "Converting a 3D asset into SLAT includes four steps:\n1. Extract active voxels that intersect with object's surface.\n2. Render multi-view images of the object, and extract Dino-v2 features of each image.\n3. Back-project Dino-v2 features to the active voxels, and take average of the multiview features within each voxel.\n4.  Compress the above voxelized features into SLAT via a shallow transformer encoder.\n\nWe plan to release all the code for reproduction, involving data preprocessing, training for SLAT encoder and decoder, as well as training for rectified flow transformers. Please stay tuned!"
      },
      {
        "user": "XiaokunSun",
        "created_at": "2024-12-25T03:45:59Z",
        "body": "Thanks for your reply, looking forward to your update!"
      },
      {
        "user": "melo513",
        "created_at": "2024-12-25T10:25:39Z",
        "body": "> Converting a 3D asset into SLAT includes four steps:\n> \n> 1. Extract active voxels that intersect with object's surface.\n> 2. Render multi-view images of the object, and extract Dino-v2 features of each image.\n> 3. Back-project Dino-v2 features to the active voxels, and take average of the multiview features within each voxel.\n> 4. Compress the above voxelized features into SLAT via a shallow transformer encoder.\n> \n> We plan to release all the code for reproduction, involving data preprocessing, training for SLAT encoder and decoder, as well as training for rectified flow transformers. Please stay tuned!\n\nabout 4. step, how to convert the voxelied features into sparseTensor ?"
      },
      {
        "user": "YuDeng",
        "created_at": "2024-12-26T02:55:45Z",
        "body": "> > Converting a 3D asset into SLAT includes four steps:\n> > \n> > 1. Extract active voxels that intersect with object's surface.\n> > 2. Render multi-view images of the object, and extract Dino-v2 features of each image.\n> > 3. Back-project Dino-v2 features to the active voxels, and take average of the multiview features within each voxel.\n> > 4. Compress the above voxelized features into SLAT via a shallow transformer encoder.\n> > \n> > We plan to release all the code for reproduction, involving data preprocessing, training for SLAT encoder and decoder, as well as training for rectified flow transformers. Please stay tuned!\n> \n> about 4. step, how to convert the voxelied features into sparseTensor ?\n\nWe just discard the inactive voxels and serialize the active ones as a 1-D sequence with their voxel indices as positional encodings, as described in Sec. 3.2 in our paper."
      },
      {
        "user": "FishWoWater",
        "created_at": "2024-12-27T02:16:23Z",
        "body": "In case anyone needs a single script for dataset processing \n``` shell \nset -e \nset -x \n# install requirements and depdendencies \n. ./dataset_toolkits/setup.sh\n\nexport DATASET_NAME=ObjaverseXL\nexport DATASET_SOURCE=sketchfab\nexport OUTPUT_DIR=datasets/ObjaverseXL_sketchfab\nexport RANK=0\nexport WORLD_SIZE=1600\nexport MAX_WORKERS=6\n\n# initially build meta data of the specified dataset\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\necho \"Init building metadata done.\"\n\n# download dataset\npython dataset_toolkits/download.py ${DATASET_NAME} --output_dir ${OUTPUT_DIR} --world_size ${WORLD_SIZE}\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\necho \"Dataset Downloaded.\"\n\n# render multi-view images with blender (e.g. 150 views)\npython dataset_toolkits/render.py ${DATASET_NAME} --output_dir ${OUTPUT_DIR} --max_workers ${MAX_WORKERS}\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\n\n# voxelization (based on open3d)\npython dataset_toolkits/voxelize.py ${DATASET_NAME} --output_dir ${OUTPUT_DIR}\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\n\n# extract DINOv2 features\npython dataset_toolkits/extract_feature.py --output_dir ${OUTPUT_DIR}\n# update meta data\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\n\n# extract SparseStructure Latents\npython dataset_toolkits/encode_ss_latent.py --output_dir ${OUTPUT_DIR}\n# update meta data\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\n\n# encode to SLATS\npython dataset_toolkits/encode_latent.py --output_dir ${OUTPUT_DIR}\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR} \n\n# render condition images\npython dataset_toolkits/render_cond.py ${DATASET_NAME} --output_dir ${OUTPUT_DIR}\npython dataset_toolkits/build_metadata.py ${DATASET_NAME} --source ${DATASET_SOURCE} --output_dir ${OUTPUT_DIR}\n\n```"
      }
    ]
  },
  {
    "number": 77,
    "title": "Details of decoder training",
    "created_at": "2024-12-19T16:33:26Z",
    "closed_at": "2024-12-26T20:39:32Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/77",
    "body": "Thanks for your great work! May I know the training resources required for training Mesh Decoder? Specifically, number of GPUs and days required to train. I see from #52 that number of steps is 270K",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/77/comments",
    "author": "vidit98",
    "comments": [
      {
        "user": "YuDeng",
        "created_at": "2024-12-25T03:10:10Z",
        "body": "For the mesh decoder, we use 32 V100 GPUs with 32 GB memory, and train it for 7 days in total. "
      }
    ]
  },
  {
    "number": 71,
    "title": "Kind inquiry about objaverse-500k",
    "created_at": "2024-12-17T01:12:15Z",
    "closed_at": "2024-12-26T12:06:50Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/71",
    "body": "Thanks for the great work! I wonder if you can share the UUID list of the objaverse objects that were used for training. Thank you! ",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/71/comments",
    "author": "hansongfang",
    "comments": [
      {
        "user": "YuDeng",
        "created_at": "2024-12-18T02:15:45Z",
        "body": "We plan to release the training data soon. Stay tuned!"
      },
      {
        "user": "JeffreyXiang",
        "created_at": "2024-12-26T05:03:04Z",
        "body": "Hi, dataset is released. Checkout DATASET.md"
      }
    ]
  },
  {
    "number": 52,
    "title": "Detailed Training Configurations",
    "created_at": "2024-12-10T23:27:29Z",
    "closed_at": "2024-12-12T00:42:42Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/52",
    "body": "The work is awesome and thank you for open-source!  I’m interested in training the model from scratch and could you please provide some instructions on the following:\n\nTraining Configurations:\n- What optimizer and hyper parameters (learning rate, batch size, etc.) were used?\n- How many steps were trained?  \n- For Slat VAE-gs/raidanceField/flexicubes, how many views is used per object per training step? \n\nThank you so much!\n",
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/52/comments",
    "author": "hansongfang",
    "comments": [
      {
        "user": "JeffreyXiang",
        "created_at": "2024-12-11T02:32:11Z",
        "body": "Hi! \n- We use AdamW optimizer with `decay_rate=0.0` (equals Adam maybe?). The LR is set to constant `1e-4` and batch size is `256` for all the models.\n- The VAE for GS/RF/Mesh is trained for 750K, 500K and 290K steps, respectively\n- 1 view per object"
      },
      {
        "user": "hansongfang",
        "created_at": "2024-12-12T00:42:42Z",
        "body": "Thank you for the information! "
      },
      {
        "user": "rfeinman",
        "created_at": "2025-01-17T15:32:05Z",
        "body": "@JeffreyXiang thanks for providing these training details! I have a question about your comment:\n\n> The VAE for GS/RF/Mesh is trained for 750K, 500K and 290K steps, respectively\n\nDoes this mean that you are training a separate encoder for each of the 3 decoders? I was under the assumption that there is 1 shared encoder and 3 distinct decoders. But, if that is the case, I'm not sure how 3-stage training would look. My best guess would be that they train in this sequence:\n\n1. Train encoder + GS decoder for 750K steps\n2. Train RF decoder for 500K steps with fixed encoder\n3. Train mesh decoder for 290K steps with fixed encoder\n\nIs is something along these lines?"
      }
    ]
  },
  {
    "number": 44,
    "title": "That's really great work! Is there a specific timeline for releasing the training code?",
    "created_at": "2024-12-10T09:18:21Z",
    "closed_at": "2024-12-27T08:43:00Z",
    "labels": [],
    "url": "https://github.com/microsoft/TRELLIS/issues/44",
    "body": null,
    "comments_url": "https://api.github.com/repos/microsoft/TRELLIS/issues/44/comments",
    "author": "ChengMacsaewe",
    "comments": [
      {
        "user": "YuDeng",
        "created_at": "2024-12-10T09:29:05Z",
        "body": "Hi, we're currently working on refining the training code and can't provide a specific timeline just yet. Please stay tuned!"
      }
    ]
  }
]