[
  {
    "number": 509,
    "title": "åœ¨å•å¼ A100å¤ç°æˆåŠŸï¼Œä½†æ˜¯ggufæƒé‡åŠ è½½éå¸¸ç¼“æ…¢ï¼Œæœ‰äººé‡åˆ°å—ï¼Ÿ",
    "created_at": "2025-02-19T12:16:09Z",
    "closed_at": "2025-02-20T04:12:37Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/509",
    "body": "attentionä¸€å±‚åŠ è½½å¤§çº¦è¦èŠ±10S",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/509/comments",
    "author": "jerryrual",
    "comments": [
      {
        "user": "3wweiweiwu",
        "created_at": "2025-02-19T13:48:47Z",
        "body": "æˆ‘ä¹Ÿé‡åˆ°åŒæ ·çš„é—®é¢˜ï¼Œæˆ‘çš„ç¯å¢ƒæ˜¯åœ¨äº‘ä¸Šï¼Œ2XA10ï¼Œç¬¬ä¸€æ¬¡è¯»å–è¦ä¸€ä¸ªå°æ—¶ï¼Œä¹‹ååªè¦ä¸é‡å¯ï¼Œå°±å¿«å¾ˆå¤šï¼Œåªè¦2åˆ†é’Ÿã€‚ä½†æ˜¯æˆ‘åœ¨ç”¨ollamaçš„æ—¶å€™ä¹Ÿæœ‰ç±»ä¼¼é—®é¢˜ï¼Œæ‰€ä»¥ä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿è¥å•†é‚£çš„é™åˆ¶ã€‚"
      },
      {
        "user": "edisonchan",
        "created_at": "2025-02-20T01:30:52Z",
        "body": "æ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯æ¯æ¬¡éƒ½é‡æ–°ä¸‹è½½æ¨¡å‹ã€‚ã€‚"
      },
      {
        "user": "codedoves",
        "created_at": "2025-02-20T01:34:01Z",
        "body": "æˆ‘ä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ï¼Œr1Q3çš„æ¨¡å‹åŠ è½½å®Œéœ€è¦30min"
      },
      {
        "user": "3wweiweiwu",
        "created_at": "2025-02-20T02:00:06Z",
        "body": "> æ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯æ¯æ¬¡éƒ½é‡æ–°ä¸‹è½½æ¨¡å‹ã€‚ã€‚\n\nåº”è¯¥ä¸æ˜¯é‡æ–°ä¸‹è½½æ¨¡å‹ï¼Œæˆ‘çš„gguféƒ½æ˜¯æœ¬åœ°çš„ï¼Œè€Œä¸”æ˜¯åœ¨æ˜¯è¯»gpuçš„æ—¶å€™ç‰¹åˆ«æ…¢ã€‚è€Œä¸”å°±ç®—æ˜¯ä¸‹è½½æ¨¡å‹ï¼Œä¸€ä¸ªå°æ—¶ä¹Ÿä¸å¤Ÿå§ã€‚"
      },
      {
        "user": "johnray22",
        "created_at": "2025-02-20T02:14:30Z",
        "body": "æ¨¡å‹è¦ä»ç¡¬ç›˜åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¦‚æœæ˜¯SSDå°±å¿«ä¸€äº›ï¼Œä¸€èˆ¬2000M/Sã€‚æœºæ¢°ç›˜å°±æ…¢äº†ï¼Œæ‹‰æ»¡ä¹Ÿå°±200M"
      },
      {
        "user": "johnray22",
        "created_at": "2025-02-20T02:14:45Z",
        "body": "å¯ä»¥è®¾ç½®æŸä¸ªå‚æ•°å°†æ¨¡å‹å¸¸é©»å†…å­˜"
      },
      {
        "user": "jzw02",
        "created_at": "2025-02-20T02:19:29Z",
        "body": "æˆ‘ä¹Ÿé‡åˆ°äº†è¿™æ ·çš„æƒ…å†µï¼Œload blkéå¸¸æ…¢ï¼Œå¹¶ä¸”chatçš„æ—¶å€™ä¹Ÿå¾ˆæ…¢"
      },
      {
        "user": "codedoves",
        "created_at": "2025-02-20T02:47:45Z",
        "body": "**ä½ ç”¨çš„æ˜¯sddè¿˜æ˜¯æœºæ¢°ç¡¬ç›˜ï¼Ÿæˆ‘ä¹Ÿæ˜¯å•å¼ A100ï¼Œä½¿ç”¨æœºæ¢°ä¹Ÿæ˜¯å¾ˆæ…¢ï¼Œåæ¥æ¢æˆssdä¼šå¥½å¾ˆå¤š"
      },
      {
        "user": "neichuanzhang",
        "created_at": "2025-02-20T03:23:56Z",
        "body": "ç”¨htopçœ‹ä¸€ä¸‹åŠ è½½æ¨¡å‹çš„è¿›ç¨‹ï¼Œå¦‚æœè¿›ç¨‹æ˜¯DçŠ¶æ€ï¼Œçœ‹ä¸‹iostat -x 1çœ‹çœ‹æ˜¯ä¸æ˜¯ç£ç›˜çš„iowaitå¤ªé«˜äº†ã€‚\n\nå¦‚æœæ˜¯ä¸€èˆ¬æ˜¯ç£ç›˜çš„æ€§èƒ½é—®é¢˜"
      },
      {
        "user": "Wilbur0626",
        "created_at": "2025-02-20T03:49:58Z",
        "body": "æˆ‘è¿™é‡Œæ˜¯ç£ç›˜çš„é—®é¢˜ï¼Œéœ€è¦æŠŠå…¶ä»–è¯»å†™æ“ä½œå…³æ‰\n"
      }
    ]
  },
  {
    "number": 503,
    "title": "V0.2.1 å®˜æ–¹é¢„å¡«å……é€Ÿåº¦80å¤šå’‹æµ‹å‡ºæ¥çš„ï¼Ÿ",
    "created_at": "2025-02-19T09:41:39Z",
    "closed_at": "2025-02-20T09:28:11Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/503",
    "body": "æµ‹è¯•ç¯å¢ƒï¼š\n* CPUï¼šIntel(R) Xeon(R) Gold 6430, 29 cores per socket, 2 sockets, 2 numa nodes\n* GPU:  4090 24G VRAM\n* test after enough warm up\n* 944G DRAM\n* System: Ubuntu 22.04.3 LTS \n* Python 3.10.12\n* CUDAï¼šBuild cuda_12.3.r12.3 \n* NVIDIA Driverï¼š12.2  -  NVIDIA GeForce RTX 4090 x 8 \n* PyTorch version: 2.6.0+cu124 \n* flash-attn: 2.7.1.post4\n\npython -m ktransformers.local_chat --model_path /workspace/Deepseek-models/DeepSeek-R1 --gguf_path /workspace/Deepseek-models/DeepSeek-R1-Q4_K_M --cpu_infer 65 --max_new_tokens 1000\næµ‹è¯•äº†ä¸åŒçš„ cpu_infer ï¼ˆ65ã€58ã€56ã€20ï¼‰ï¼Œ65æ•ˆæœæœ€å¥½\n\næµ‹è¯•ç»“æœï¼š\n* Prefill token/sï¼ˆæœ€é«˜ï¼‰ï¼š13.35\n* Decode token/sï¼ˆæœ€é«˜ï¼‰ï¼š10.11\n* å†…å­˜å ç”¨:  903.6G/944G = 95.7%\n* æ˜¾å­˜å ç”¨ï¼š14881MiB / 24564MiB = 14.54G/24G = 60.51%\n\nå†…å­˜ç†è®ºå¸¦å®½æ˜¯563 GB/sï¼Œå®æµ‹å‡ºæ¥çš„å¤§çº¦352GB/s\n\nå®˜æ–¹é€Ÿåº¦ï¼šV0.2.1\n  * Prefill token/sï¼š88.2\n  * Decode token/sï¼š13.5\n\n é¢„å¡«å……é€Ÿåº¦å·®è·è¾ƒå¤§ï¼Œå…«åå¤šå’‹æµ‹å‡ºæ¥çš„ï¼Ÿ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/503/comments",
    "author": "yansiyu550",
    "comments": [
      {
        "user": "dachengai",
        "created_at": "2025-02-20T07:28:44Z",
        "body": "æµ‹é•¿ prompt   prefill é€Ÿåº¦å°±ä¸Šæ¥äº† "
      },
      {
        "user": "yansiyu550",
        "created_at": "2025-02-20T08:23:43Z",
        "body": "> æµ‹é•¿ prompt prefill é€Ÿåº¦å°±ä¸Šæ¥äº†\n\nå¥½çš„å¥½çš„ï¼Œæˆ‘è¯•è¯•"
      },
      {
        "user": "seanzhang-zhichen",
        "created_at": "2025-02-20T09:45:20Z",
        "body": "ä¸çŸ¥é“ä½ ä»¬æ˜¯æ€ä¹ˆæµ‹å‡ºæ¥é€Ÿåº¦è¿™ä¹ˆå¿«çš„ï¼Œæˆ‘å‘ç°æœ‰ä¸å°‘äººï¼Œéƒ½å’Œæˆ‘ä¸€æ ·ï¼Œéƒ½æ²¡æœ‰ è¶…è¿‡1token/s \n\nä¸çŸ¥é“é—®é¢˜å‡ºåœ¨å“ªé‡Œ"
      },
      {
        "user": "yansiyu550",
        "created_at": "2025-02-20T09:51:40Z",
        "body": "æˆ‘æµ‹å‡ºæ¥Prefill 50.53  token/sï¼ŒDecode 10.11  token/sï¼Œæµ‹è¯•æœºå™¨ç†è®ºå¸¦å®½ 563 GB/sï¼Œå®æµ‹å¸¦å®½å¤§æ¦‚æ˜¯ 352 GB/sï¼Œå®˜æ–¹æä¾›çš„æœºå™¨ç†è®ºå¸¦å®½ 600 GB/sï¼Œæˆ‘è¿™ä¸ªç»“æœåº”è¯¥æ˜¯ç¬¦åˆé¢„æœŸçš„\n\n    * å†…å­˜å ç”¨ï¼š810.5G/944G = 85.9%\n    * æ˜¾å­˜å ç”¨ï¼š15375MiB / 24564MiB = 15.01G/24G = 62.6%"
      }
    ]
  },
  {
    "number": 501,
    "title": "Necessary tips for Node.js related issues",
    "created_at": "2025-02-19T08:38:51Z",
    "closed_at": "2025-02-19T08:46:48Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/501",
    "body": "Necessary tips for Node.js related issues",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/501/comments",
    "author": "Zhoneym",
    "comments": [
      {
        "user": "Atream",
        "created_at": "2025-02-19T08:46:38Z",
        "body": "Thank you for your contribution!"
      }
    ]
  },
  {
    "number": 494,
    "title": "è¿™èƒ½åœ¨4090è·‘èµ·æ¥r1 ? æœ‰æ²¡æœ‰é«˜æ‰‹çœŸçš„èƒ½è·‘çš„",
    "created_at": "2025-02-19T07:54:46Z",
    "closed_at": "2025-02-20T04:15:37Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/494",
    "body": "æƒ³é—®é—®äº‹deepseek-r1 ä»€ä¹ˆé‡çº§çš„ï¼Œ ç¡®å®šå¯ä»¥è·‘å—ï¼Œ æ„Ÿè°¢ğŸ™ï¼Œ å¦‚æœå¯ä»¥æƒ³å»æä¸ªæœºå™¨è¯•è¯•",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/494/comments",
    "author": "652994331",
    "comments": [
      {
        "user": "yansiyu550",
        "created_at": "2025-02-19T09:50:25Z",
        "body": "1T DRAMï¼Œ14GB VRAMï¼Œå•å¡4090å°±èƒ½è·‘ï¼Œæ˜¯Q4é‡åŒ–ç‰ˆæœ¬çš„ï¼Œä¸æ˜¯æ»¡è¡€ç‰ˆï¼ŒCPUè¦æ”¯æŒAMX"
      },
      {
        "user": "paranoiagu",
        "created_at": "2025-02-19T23:58:51Z",
        "body": "ä¸€ç‚¹è¦æ”¯æŒ AMX å—ï¼Ÿ"
      },
      {
        "user": "VinayaLee",
        "created_at": "2025-02-20T01:18:21Z",
        "body": "> ä¸€ç‚¹è¦æ”¯æŒ AMX å—ï¼Ÿ\n\nv0.3.0éœ€è¦ï¼Œä¹‹å‰çš„ç‰ˆæœ¬ä¸éœ€è¦"
      }
    ]
  },
  {
    "number": 481,
    "title": "Modify and add any incorrect or missing content in the `install.md`",
    "created_at": "2025-02-19T02:04:44Z",
    "closed_at": "2025-02-19T03:19:12Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/481",
    "body": "Modify and add any incorrect or missing content in the `install.md`",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/481/comments",
    "author": "Zhoneym",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-19T03:19:47Z",
        "body": "Thank you for your contribution! ğŸŒ¹"
      }
    ]
  },
  {
    "number": 471,
    "title": "æƒé‡æ–‡ä»¶åŠ è½½å¾ˆæ…¢æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ",
    "created_at": "2025-02-18T12:03:04Z",
    "closed_at": "2025-02-18T12:45:29Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/471",
    "body": "CPUå‹å·æ˜¯Intel(R) Xeon(R) Platinum 8179M CPU @ 2.40GHz \næœ‰ä¸€å¼ 3090æ˜¾å¡ï¼Œæ˜¾å­˜24G\nå†…å­˜DDR 4 ï¼Œ2666 MT/sï¼Œ8æ ¹64G=512G\nè¿è¡ŒQ4æ»¡è¡€ç‰ˆçš„gguf\nåŠ è½½æƒé‡æ–‡ä»¶éå¸¸æ…¢ï¼Œå‡ ååˆ†é’Ÿäº†è¿˜æ²¡åŠ è½½å®Œ\nlogæ˜¾ç¤ºå¤§æ¦‚ä¸€åˆ†å¤šé’Ÿæ‰èƒ½å‡ºç°æ–°çš„blkæ•°å­—ï¼š\n```\nloading blk.48.attn_q_a_norm.weight to cuda:0\nloading blk.48.attn_kv_a_norm.weight to cuda:0\nloading blk.48.attn_kv_b.weight to cuda:0\nloading blk.48.attn_norm.weight to cuda:0\nloading blk.48.ffn_norm.weight to cuda:0\nloading blk.49.attn_q_a_norm.weight to cuda:0\nloading blk.49.attn_kv_a_norm.weight to cuda:0\nloading blk.49.attn_kv_b.weight to cuda:0\nloading blk.49.attn_norm.weight to cuda:0\nloading blk.49.ffn_norm.weight to cuda:0\nloading blk.50.attn_q_a_norm.weight to cuda:0\nloading blk.50.attn_kv_a_norm.weight to cuda:0\nloading blk.50.attn_kv_b.weight to cuda:0\n```\nåŠ è½½è¿‡ç¨‹ä¸­è¿›ç¨‹æ˜¾ç¤ºå†…å­˜å ç”¨70%\n```    \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n2577178 being     20   0  419.5g 363.2g 358.8g R  2257  72.1 417:19.23 python\n```\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/471/comments",
    "author": "burugo",
    "comments": [
      {
        "user": "burugo",
        "created_at": "2025-02-18T12:45:29Z",
        "body": "å¤§æ¦‚çŸ¥é“åŸå› äº†ï¼Œæ–‡ä»¶æ”¾åœ¨æœºæ¢°ç¡¬ç›˜ä¸Š"
      },
      {
        "user": "jzw02",
        "created_at": "2025-02-19T08:56:10Z",
        "body": "æˆ‘ä¹Ÿæ˜¯å¾ˆæ…¢ï¼Œæœ‰è§£å†³åŠæ³•å—"
      }
    ]
  },
  {
    "number": 457,
    "title": "è°ƒç”¨OpenAI ChatCompletion APIè¾“å…¥è¶…è¿‡ä¸€ä¸ªå•è¯åå°±ä¼šå‡ºç°Segmentation faulté—®é¢˜",
    "created_at": "2025-02-18T08:07:24Z",
    "closed_at": "2025-02-19T07:21:21Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/457",
    "body": "2025-02-19 00:16:02,186 DEBUG /home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/transformers.py[193]: get input ids of shape torch.Size([1, 102])\n<think>\n2025-02-19 00:16:02,186 DEBUG /home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py[125]: input_ids: torch.Size([1, 104])\n2025-02-19 00:16:02,187 DEBUG /home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py[151]: cache position: 0 to 104\nFatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation faultFatal Python error: \n\nFatal Python error: Segmentation faultFatal Python error: \n\nFatal Python error: Fatal Python error: Thread 0xFatal Python error: Segmentation faultFatal Python error: Segmentation fault\n\nFatal Python error: Fatal Python error: Fatal Python error: Segmentation faultFatal Python error: Fatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation fault00007f740b100200Segmentation faultFatal Python error: \n\nFatal Python error: Segmentation fault\n\nFatal Python error: Fatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation faultSegmentation fault\n\nSegmentation faultSegmentation faultFatal Python error: Segmentation faultFatal Python error: Fatal Python error: Segmentation faultFatal Python error: \n\n (most recent call first):\n\n\n\n\nSegmentation faultSegmentation fault\n\nSegmentation faultSegmentation faultSegmentation fault\n\nSegmentation fault\n\næœ‰ä»€ä¹ˆè§£å†³åŠæ³•å—ï¼Ÿ\n\n\n\n\n\n\nSegmentation fault\n\nSegmentation fault\n\nSegmentation faultSegmentation fault  File \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/cpuinfer.py\", line 740 in sync\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 222 in forward\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 579 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 849 in moe_kexperts\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 828 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/models/modeling_deepseek_v3.py\", line 1220 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/models.py\", line 722 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/models/modeling_deepseek_v3.py\", line 1688 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py\", line 160 in prefill\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36 in generator_context\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/transformers.py\", line 340 in inference\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py\", line 181 in inference\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/api/openai/endpoints/chat.py\", line 40 in chat_completion\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 210 in run_endpoint_function\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 297 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 73 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 51 in wrapped_app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 76 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 288 in handle\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 735 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 715 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 51 in wrapped_app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/applications.py\", line 113 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/applications.py\", line 1054 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406 in run_asgi\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/events.py\", line 84 in _run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/base_events.py\", line 1936 in _run_once\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/base_events.py\", line 608 in run_forever\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/base_events.py\", line 641 in run_until_complete\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/runners.py\", line 118 in run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/runners.py\", line 190 in run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/server.py\", line 65 in run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/main.py\", line 577 in run\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/main.py\", line 91 in run_api\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/main.py\", line 118 in main\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/main.py\", line 128 in <module>\nSegmentation fault (core dumped)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/457/comments",
    "author": "GodXuxilie",
    "comments": [
      {
        "user": "GodXuxilie",
        "created_at": "2025-02-19T07:21:21Z",
        "body": "å·²ç»è§£å†³ï¼Œåº”è¯¥æ˜¯æ²¡æœ‰é…ç½®å¥½çš„é—®é¢˜ã€‚é‡æ–°pip install wheelåï¼Œå°±å¯ä»¥æ­£å¸¸ä½¿ç”¨å—ã€‚"
      },
      {
        "user": "jinec",
        "created_at": "2025-02-20T09:34:36Z",
        "body": "@GodXuxilie é‡æ–°å®‰è£…å“ªä¸ªè½®å­å‘¢ï¼Ÿè€å“¥"
      }
    ]
  },
  {
    "number": 448,
    "title": "è¯·æ•™ä¸€ä¸‹ï¼ŒæœåŠ¡å™¨ä¸Šä¸¤å¼ 3090ï¼Œ251Gå†…å­˜ï¼Œèƒ½å¦r1çš„é‡åŒ–æ»¡è¡€ç‰ˆ",
    "created_at": "2025-02-18T05:42:58Z",
    "closed_at": "2025-02-20T05:03:46Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/448",
    "body": "æˆ‘æœ‰ä¸€å°æœåŠ¡å™¨ï¼Œ40æ ¸CPUï¼Œæœ‰251Gå†…å­˜ï¼Œ18Tå­˜å‚¨ï¼Œç°åœ¨æƒ³é€šè¿‡dockerå®¹å™¨æµ‹è¯•ä¸€ä¸‹r1çš„æ»¡è¡€ç‰ˆï¼Œçœ‹çœ‹è¿™ä¸ªèµ„æºèƒ½ä¸èƒ½æ‹‰èµ·è¿™ä¸ªæœåŠ¡ã€‚",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/448/comments",
    "author": "johnhillcn",
    "comments": [
      {
        "user": "xialang2012",
        "created_at": "2025-02-18T06:00:02Z",
        "body": "ä¸èƒ½"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T06:22:05Z",
        "body": "å¯ä»¥è¯•ä¸€ä¸‹q2kxlæ ¼å¼çš„unsloth"
      }
    ]
  },
  {
    "number": 447,
    "title": "æœ‰æ”¯æŒARMæ¶æ„çš„è®¡åˆ’ä¹ˆï¼Ÿ",
    "created_at": "2025-02-18T04:40:55Z",
    "closed_at": "2025-02-18T10:48:41Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/447",
    "body": "æœ‰æ”¯æŒarmæ¶æ„çš„è®¡åˆ’ä¹ˆï¼Ÿå¤§æ¦‚ä»€ä¹ˆæ—¶é—´å‘¢ï¼Ÿ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/447/comments",
    "author": "hexinlei",
    "comments": [
      {
        "user": "xialang2012",
        "created_at": "2025-02-18T06:00:30Z",
        "body": "æ²¡æœ‰ï¼Œç›®å‰æä¸å®š"
      },
      {
        "user": "hexinlei",
        "created_at": "2025-02-18T10:48:41Z",
        "body": "å¥½çš„ï¼Œæ„Ÿè°¢å›å¤ã€‚"
      }
    ]
  },
  {
    "number": 443,
    "title": "ä¸æ”¯æŒAMXçš„CPUä¹Ÿèƒ½è¿è¡Œå—",
    "created_at": "2025-02-18T03:38:29Z",
    "closed_at": "2025-02-18T06:25:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/443",
    "body": "æœ‰æ²¡æœ‰å¤§ä½¬ä½¿ç”¨ä¸æ”¯æŒAMXçš„CPUæˆåŠŸè¿è¡Œäº†KT",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/443/comments",
    "author": "zhudongwork",
    "comments": [
      {
        "user": "ningpengtao-coder",
        "created_at": "2025-02-18T03:46:15Z",
        "body": "å‘å¸ƒé¡µæœ‰å‘å¸ƒavx2å’Œavx512åŒ…ï¼Œç›´æ¥ä¸‹å°±è¡Œã€‚12ä»£é…·ç¿åintelç¦ç”¨äº†avx512åªæ”¯æŒavx2äº†ã€‚"
      },
      {
        "user": "xialang2012",
        "created_at": "2025-02-18T06:01:12Z",
        "body": "AMXæ”¯æŒä¸å¦ä¸å½±å“ï¼Œåªæ˜¯é€Ÿåº¦ä¸Šä¸å»"
      }
    ]
  },
  {
    "number": 432,
    "title": "ä¸ºå•¥ç”¨ä¸¤é¢—cpuåªæœ‰ç”¨ä¸€é¢—cpu ååˆ†ä¹‹ä¸€çš„é€Ÿåº¦",
    "created_at": "2025-02-18T01:53:52Z",
    "closed_at": "2025-02-18T06:51:42Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/432",
    "body": "# env\nIntel(R) Xeon(R) Gold 6462C x2\n1024GB RAM(Hynix 64GB DDR5-4800 HMCG94MEBRA123N)\nNVIDIA L20 48GB x8\nUbuntu 22.04.5 LTS\nktransformers v0.2.1\n\n\n# single cpu\npython ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/ --cpu_infer 65 --max_new_tokens 1000\n## resource usage\n13986 MB VRAM\n390483 MB RAM\nCPU 6554%\nsingle gpu 10%\n## rate\nprompt eval count:    6 token(s)\nprompt eval duration: 1.7646489143371582s\nprompt eval rate:     3.4001097619203957 tokens/s\neval count:           408 token(s)\neval duration:        77.80391049385071s\neval rate:            5.24395235933863 tokens/s\n\n# rebuild with export USE_NUMA=1 to use two cpu\n\npython ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/ --cpu_infer 126 --max_new_tokens 1000\n## resource usage\n13986 MB VRAM\n904183 MB RAM\nCPU 10554%\nsingle gpu 0%\n## rate\nprompt eval count:    6 token(s)\nprompt eval duration: 12.765036821365356s\nprompt eval rate:     0.4700338968045559 tokens/s\neval count:           332 token(s)\neval duration:        640.3727271556854s\neval rate:            0.5184480630126604 tokens/s\n\nä¸¤é¢—cpuåªæœ‰ç”¨ä¸€é¢—cpu ååˆ†ä¹‹ä¸€çš„é€Ÿåº¦",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/432/comments",
    "author": "yourchanges",
    "comments": [
      {
        "user": "chk4991",
        "created_at": "2025-02-18T06:18:14Z",
        "body": "ç”¨äº†ä¸€é¢—cpué€Ÿåº¦å¾ˆæ…¢ï¼ŒåŸºæœ¬1tokenè¦10ç§’ï¼Œä½ è§£å†³äº†å—"
      },
      {
        "user": "yourchanges",
        "created_at": "2025-02-18T06:32:13Z",
        "body": "cpu infer =ã€‹65 ä»5tokenæ¯ç§’åˆ°13tokenæ¯ç§’äº†\n\n# rebuild with export USE_NUMA=1 to use two sockets\npython ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/ --cpu_infer 65 --max_new_tokens 1000\n## resource usage\n13986 MB VRAM\n984183 MB RAM\nCPU 6554%\nsingle gpu 42%\n## rate\nprompt eval count:    10 token(s)\nprompt eval duration: 0.4073047637939453s\nprompt eval rate:     24.551640169519306 tokens/s\neval count:           1000 token(s)\neval duration:        72.73280572891235s\neval rate:            13.748953996456175 tokens/s"
      },
      {
        "user": "yydslc",
        "created_at": "2025-02-18T06:35:36Z",
        "body": "å¤§ä½¬ï¼Œä½ çš„ktç‰ˆæœ¬æ˜¯v0.2.1è¿˜æ˜¯v0.3.0?"
      },
      {
        "user": "yourchanges",
        "created_at": "2025-02-18T06:38:19Z",
        "body": "> å¤§ä½¬ï¼Œä½ çš„ktç‰ˆæœ¬æ˜¯v0.2.1è¿˜æ˜¯v0.3.0?\n\nä¸Šé¢ç¯å¢ƒå†™äº† ç¼–è¯‘çš„master æ˜¯0.2.1"
      }
    ]
  },
  {
    "number": 414,
    "title": "AttributeError: 'KDeepseekV2Attention' object has no attribute 'q_lora_rank'",
    "created_at": "2025-02-17T09:50:22Z",
    "closed_at": "2025-02-17T10:13:17Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/414",
    "body": "é…ç½®:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\næ¨¡å‹:unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF\næ‰§è¡Œçš„æŒ‡ä»¤python3 -m ktransformers.local_chat ./DeepSeek-R1-Distill-Qwen-32B --gguf_path ./DeepSeek-R1-Distill-Qwen-32B-GGUF --optimize_rule_path ./DeepSeek-V3-Chat.yaml \nè¿›å…¥èŠå¤©ç•Œé¢å‘å®Œæ¶ˆæ¯æŠ¥è¯¥é”™è¯¯",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/414/comments",
    "author": "AndrewBoom",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T10:13:12Z",
        "body": "æˆ‘ä»¬æ²¡æœ‰æµ‹è¯•è¿‡qwen32bçš„æ¨¡å‹ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„å®‰è£…æŒ‡å—é‡Œæ”¯æŒçš„æ¨¡å‹â€œsupported modelsâ€"
      }
    ]
  },
  {
    "number": 411,
    "title": "èƒ½å‡ºä¸€ä¸ª çº¯CPU åŠ  å¤§å†…å­˜ï¼Œæ— æ˜¾å¡çš„ å¯ä»¥è¿è¡Œçš„æ•™ç¨‹ä¹ˆã€‚",
    "created_at": "2025-02-17T08:56:29Z",
    "closed_at": "2025-02-17T10:15:42Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/411",
    "body": "èƒ½å‡ºä¸€ä¸ª çº¯CPU åŠ  å¤§å†…å­˜ï¼Œæ— æ˜¾å¡çš„ å¯ä»¥è¿è¡Œçš„æ•™ç¨‹ä¹ˆã€‚",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/411/comments",
    "author": "clywm520",
    "comments": [
      {
        "user": "clywm520",
        "created_at": "2025-02-17T09:09:12Z",
        "body": "æˆ–è€…å‡ºä¸€æ•™ç¨‹ï¼Œä½¿ç”¨ 16Gå†…å­˜èƒ½è¿è¡Œ  deepseek-r1:70bï¼Œæˆ–  deepseek-r1:32b çš„æ•™ç¨‹å“ˆã€‚åªè¦èƒ½æå‡tokené€Ÿåº¦å°±å¥½äº†ã€‚æœ€å¥½æ˜¯dockerè¿è¡Œ"
      },
      {
        "user": "kissint8",
        "created_at": "2025-02-17T09:39:20Z",
        "body": "çº¯CPUå¯åœ¨issueåŒºæ‰¾æ‰¾çœ‹ã€‚\n\ndeepseek-r1 70bå’Œ32bå®è´¨ä¸Šå¹¶ä¸æ˜¯deepseek-moeæ¨¡å‹ï¼Œæ˜¯llamaå’Œqwenæ¨¡å‹çš„ç»“æ„ï¼Œæˆ‘ä¹‹å‰çš„æµ‹è¯•ä¸­llama70bå­˜åœ¨è¿è¡Œé—®é¢˜ï¼Œå°šæœªå¾—çŸ¥æ˜¯å¦å·²ä¿®å¤ï¼Œqwenæœªæµ‹è¯•ã€‚\n\nç›®å‰deepseekï¼Œæ¨èç”¨671bçš„å¤šç§é‡åŒ–ç‰ˆæœ¬ã€‚\n\næ­¤å¤–ï¼ŒKtransformerså¯¹äºå‡å°‘æ¨¡å‹å†…å­˜å ç”¨æ•ˆæœæ˜¾è‘—ï¼Œä½†çº¯CPUåŠ é€Ÿæ–¹é¢å¦‚æœæœ‰è·å¾—æ˜¾è‘—åŠ é€Ÿæ•ˆæœæ¬¢è¿è´´å‡ºã€‚"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T10:14:39Z",
        "body": "çº¯cpuæ¨ç†æ¨¡å‹å¹¶ä¸äº«å—ktransformerså¸¦æ¥çš„å„ç§åŠ é€Ÿï¼Œè€ƒè™‘åˆ°æ¡†æ¶çš„æ˜“ç”¨ç¨‹åº¦ï¼Œå¦‚æœæƒ³çº¯cpuæ¨ç†è¯·è€ƒè™‘ollamaæˆ–è€…llamacppï½"
      }
    ]
  },
  {
    "number": 406,
    "title": "Fix cmake error caused by lack of environment variables in Windows environment",
    "created_at": "2025-02-17T07:59:03Z",
    "closed_at": "2025-02-18T12:10:00Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/406",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/406/comments",
    "author": "hoshinohikari",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T07:44:36Z",
        "body": "Hi, thanks for your contribution to KTransformers. Can you further explain what type of error this pr would solve?"
      },
      {
        "user": "hoshinohikari",
        "created_at": "2025-02-18T08:02:17Z",
        "body": "Of course. when compiling the project using the Developer Command for Visual Studio, the cmake generator used is ninja, but the environment variable is empty. If the cmake platform parameter is added, it will prompt that ninja does not support this parameter."
      }
    ]
  },
  {
    "number": 401,
    "title": "feat: refactor local_chat to openai server mode",
    "created_at": "2025-02-17T06:41:29Z",
    "closed_at": "2025-02-18T13:14:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/401",
    "body": "together with A9 in wechat-group",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/401/comments",
    "author": "zhaoyukoon",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T12:13:33Z",
        "body": "Local chat is a lightweight tool that we offer for experimentation, and we aim to keep it simple and easy to use. If you need access to the OpenAI API, we recommend using the server instead."
      },
      {
        "user": "zhaoyukoon",
        "created_at": "2025-02-18T13:04:49Z",
        "body": "i will commit to another `server.py`"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T13:13:39Z",
        "body": "> i will commit to another `server.py`\r\n\r\nThank you for your understanding! Looking forward to seeing your new PR."
      }
    ]
  },
  {
    "number": 378,
    "title": "Does it supports IQ1_S and Q2_K_XS?",
    "created_at": "2025-02-16T15:31:03Z",
    "closed_at": "2025-02-17T10:26:48Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/378",
    "body": "æ”¯æŒIQ1_Så’ŒQ2_K_XSè¿™ä¸¤ç§é‡åŒ–æ ¼å¼å—ï¼Ÿèƒ½åšåˆ°å†…å­˜æ˜¾å­˜åŒæ—¶ç”¨å—ï¼Ÿå¦‚æœæ”¯æŒè¿™äº›é‡åŒ–æ ¼å¼é‚£ä¼šå¾ˆæœ‰å‘å±•å‰æ™¯",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/378/comments",
    "author": "exumeOAO",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T10:26:18Z",
        "body": "We did not support iq_x fomat, except iq4; Q2_K_XS is supported; i didn't test Q2_K_XS but i saw someone say it is runnable"
      }
    ]
  },
  {
    "number": 362,
    "title": "Warning: orig_module is not set, but has in_features or out_features equals to 1, can't get in_features and out_features from GGUF",
    "created_at": "2025-02-16T08:14:24Z",
    "closed_at": "2025-02-16T10:26:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/362",
    "body": "ä½¿ç”¨ktransformers --model_path Qwen/Qwen2-57B-A14B-Instruct --gguf_path E:\\models\\Qwen\\qwen2-57b-a14b-instruct-q3_k_m  --port 12345 --web Trueå‘½ä»¤å¯åŠ¨æ¨¡å‹æŠ¥é”™å¦‚ä¸‹ï¼š\nWarning: orig_module is not set, but has in_features or out_features equals to 1, can't get in_features and out_features from GGUF\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"E:\\anaconda3\\envs\\deepseek\\Scripts\\ktransformers.exe\\__main__.py\", line 7, in <module>\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\server\\main.py\", line 114, in main\n    create_interface(config=cfg, default_args=cfg)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\server\\utils\\create_interface.py\", line 27, in create_interface\n    GlobalInterface.interface = BackendInterface(default_args)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\server\\backend\\interfaces\\ktransformers.py\", line 47, in __init__\n    optimize_and_load_gguf(self.model, optimize_rule_path, gguf_path, config)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 128, in optimize_and_load_gguf\n    inject(module, optimize_config, model_config, gguf_loader)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 42, in inject\n    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 42, in inject\n    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 42, in inject\n    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n  [Previous line repeated 1 more time]\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 33, in inject\n    inject_module=module_cls(key = inject_module_meta[\"key\"], gguf_loader = gguf_loader, config = model_config, orig_module=child, **inject_module_meta[\"kwargs\"])\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\operators\\linear.py\", line 378, in __init__\n    KLinearBase.__init__(self, key, gguf_loader, config, orig_module, generate_device, **kwargs)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\operators\\linear.py\", line 65, in __init__\n    self.out_features = self.gguf_loader.tensor_info[key + \".weight\"][\"shape\"][1]\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/362/comments",
    "author": "zerozhengsi",
    "comments": [
      {
        "user": "zerozhengsi",
        "created_at": "2025-02-16T08:21:11Z",
        "body": "æˆ‘æ˜¯windwos10ç³»ç»Ÿï¼Œé…ç½®32Gå†…å­˜ï¼Œ16Gæ˜¾å­˜4060 TIæ˜¾å¡ï¼Œè¿è¡Œçš„æ¨¡å‹æ˜¯qwen2-57b-a14b-instruct-q3_k_m"
      },
      {
        "user": "zerozhengsi",
        "created_at": "2025-02-16T10:25:38Z",
        "body": "ä¿®æ”¹ä»£ç self.out_features = self.gguf_loader.tensor_info[key + \".weight\"][\"shape\"][1]ä¸ºself.out_features = self.gguf_loader.tensor_info[key + \".weight\"][\"shape\"][-1]å¯åŠ¨æˆåŠŸ"
      },
      {
        "user": "Zha-Miku",
        "created_at": "2025-02-17T09:28:35Z",
        "body": "æœ€åçš„æ•ˆæœå’‹æ ·å•Š"
      }
    ]
  },
  {
    "number": 351,
    "title": "è¿™ä¸ªæ¡†æ¶èƒ½æ”¯æŒå¤šå°‘å¹¶å‘ï¼Ÿ",
    "created_at": "2025-02-16T03:26:11Z",
    "closed_at": "2025-02-16T06:12:55Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/351",
    "body": "æ ¹æ®v0.3çš„é…ç½®ï¼Œèƒ½æ”¯æŒå¤šå°‘å¹¶å‘æ¨ç†å‘€ï¼Ÿ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/351/comments",
    "author": "edoserbia",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-16T06:12:55Z",
        "body": "ç›®å‰ä¸æ”¯æŒå¹¶å‘ï¼Œåªèƒ½å•batch"
      },
      {
        "user": "zengqingfu1442",
        "created_at": "2025-02-17T08:45:48Z",
        "body": "m"
      }
    ]
  },
  {
    "number": 344,
    "title": "Infinite Loop when CPU Infer is Low",
    "created_at": "2025-02-16T00:26:20Z",
    "closed_at": "2025-02-16T06:12:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/344",
    "body": "I encountered infinite loop issue today, in both v0.2 and v0.2.1.\n\n## Hardware Spec\n**NUMA Cores**: 4\n**CPU**: AMD EPYC 74F3 24-Core Processor\n**Socket**: 2\n**Cores per socket**: 18\n**Threads per Core**: 2\n\n## Software Env\n**KTransformers Version**: V0.2.0\n**Model**: DeepSeek-R1-Q4_K_M & DeepSeek-R1-UD-Q2_K_XL\n\n## Problem Statement\nDepends on the cpu_infer parameter, the DeepSeek Model will behave differently. When value is low, it falls into infinite loop. As we increase cpu infer count, the problem is resolved.\n\nFollowing table is measured against Q2 model, but similar behavior is observed in Q4_K_M\n\n| CPU Infer | Observation    | Token/s |\n| --------- | -------------- | ------- |\n| 18        | Infintely loop4/5 times | 6.857   |\n| 38        | OK             | 6.85    |\n| 50        | OK             | 6.72    |\n| 65        | OK             | 6.1     |\n\n\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/344/comments",
    "author": "3wweiweiwu",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-16T06:12:25Z",
        "body": "It seems due to triton mla kernel. #354 "
      }
    ]
  },
  {
    "number": 338,
    "title": "qwen2_5_vl ä¸æ”¯æŒ",
    "created_at": "2025-02-15T16:52:55Z",
    "closed_at": "2025-02-16T06:21:27Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/338",
    "body": "- ç³»ç»Ÿä¿¡æ¯ï¼šUbuntu24.04\n- æ˜¾å¡ï¼š3080\n- cuda: 12.4\né”™è¯¯ä¿¡æ¯ï¼š\n```shell\nktransformers --model_path Qwen/Qwen2.5-VL-7B-Instruct --gguf_path /data/work/huggingface/Qwen/Qwen2.5-VL-7B-Instruct --port 10002 --web True\n2025-02-16 00:44:49,940 INFO /home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/main.py[29]: Creating SQL tables\n2025-02-16 00:44:49,942 INFO /home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/api/openai/assistants/assistants.py[75]: Creating default assistant\nTraceback (most recent call last):\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 989, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 691, in getitem\nraise KeyError(key)\nKeyError: 'qwen2_5_vl'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/bin/ktransformers\", line 8, in <module>\nsys.exit(main())\n^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/main.py\", line 114, in main\ncreate_interface(config=cfg, default_args=cfg)\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/utils/create_interface.py\", line 27, in create_interface\nGlobalInterface.interface = BackendInterface(default_args)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 29, in init\nconfig = AutoConfig.from_pretrained(args.model_dir, trust_remote_code=args.trust_remote_code)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 991, in from_pretrained\nraise ValueError(\nValueError: The checkpoint you are trying to load has model type qwen2_5_vl but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date. ```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/338/comments",
    "author": "javaDer",
    "comments": [
      {
        "user": "KMSorSMS",
        "created_at": "2025-02-16T06:21:27Z",
        "body": "> Qwen2.5-VL-7B-Instruct\n\nç›®å‰æš‚ä¸æ”¯æŒ"
      }
    ]
  },
  {
    "number": 334,
    "title": "stable-diffusion-webui èƒ½ä¸èƒ½ç”¨é€™å€‹??",
    "created_at": "2025-02-15T15:35:56Z",
    "closed_at": "2025-02-15T16:09:49Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/334",
    "body": "æƒ³å•ä¸€ä¸‹,å¦‚æœå¯ä»¥ç”¨é‚£éº¼sdxl å°±èƒ½æ›´å¿«ç•«åœ–äº†!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/334/comments",
    "author": "upright2003",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-15T16:09:49Z",
        "body": "ä¸èƒ½ï½"
      }
    ]
  },
  {
    "number": 324,
    "title": "Update DeepseekR1_V3_tutorial.md",
    "created_at": "2025-02-15T11:43:51Z",
    "closed_at": "2025-02-15T14:23:09Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/324",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/324/comments",
    "author": "BiFangKNT",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-15T14:26:32Z",
        "body": "æ„Ÿè°¢æ‚¨çš„ä¿®æ­£ï¼ğŸ’"
      }
    ]
  },
  {
    "number": 321,
    "title": "NUMAæ¨¡å¼ä¸èƒ½å·¥ä½œ",
    "created_at": "2025-02-15T10:06:40Z",
    "closed_at": "2025-02-15T16:26:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/321",
    "body": "ç³»ç»Ÿé…ç½®\n  CPU: EPYC 7742 x2\n  DRAM: 64GB x 16\n  GPU: 3090 x 8\n  ä½¿ç”¨çš„æ¨¡å‹: Ollamaé‚£è¾¹æ‹‰ä¸‹æ¥çš„8bité‡åŒ–çš„gguf\n\nå¦‚æœä¸ä½¿ç”¨NUMAæ¨¡å¼, install.shå®‰è£…, ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨\npython -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V3 --cpu_infer 64 --max_new_tokens 1000 --gguf_path /home/adonis/temp/ds_v3\n\nå¯ä»¥æ­£å¸¸å·¥ä½œ, ç”¨é™„åŠ çš„testæ–‡ä»¶é‡Œçš„ä¿¡æ¯, è¦æ±‚è¿›è¡Œæ€»ç»“, æ€§èƒ½æ•°æ®å¦‚ä¸‹, å¦å¤–åœ¨è¿™ä¸ªæ—¶å€™cpué‚£è¾¹çš„å†…å­˜å ç”¨åªæœ‰19G\n\nprompt eval count:    966 token(s)\nprompt eval duration: 17.906471252441406s\nprompt eval rate:     53.9469774017197 tokens/s\neval count:           1000 token(s)\neval duration:        222.02748823165894s\neval rate:            4.5039468219206285 tokens/s\n\nå¦‚æœä½¿ç”¨NUMAæ¨¡å¼, ä¹Ÿå°±æ˜¯export NUMA=1; install.shå®‰è£…, ä½¿ç”¨ä¸Šè¿°å‘½ä»¤å¯åŠ¨\nç¨‹åºåœ¨è¾“å‡ºä¸€ç³»åˆ—åŠ è½½ä¿¡æ¯, ç›´åˆ°ä»¥ä¸‹ä¿¡æ¯\n\nloading blk.46.ffn_norm.weight to cuda:0\nloading blk.47.attn_q_a_norm.weight to cuda:0\nloading blk.47.attn_kv_a_norm.weight to cuda:0\nloading blk.47.attn_kv_b.weight to cuda:0\n\nç„¶åå°±killedäº†\n\nçœ‹äº†ä¸€ä¸‹htopä¿¡æ¯, æ˜¯1Tå†…å­˜éƒ½è¢«å æ»¡äº†, ç„¶åswapç”¨å…‰(swapå°±é…ç½®äº†8GB), ç„¶åå°±killedäº†\n\nbtw, éå¸¸æ„Ÿè°¢ä½œè€…çš„å·¥ä½œ, éå¸¸éå¸¸æ£’çš„å·¥ä½œ!!!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/321/comments",
    "author": "adonishong",
    "comments": [
      {
        "user": "NeilAlfred",
        "created_at": "2025-02-15T12:20:19Z",
        "body": "é‡åˆ°äº†åŒæ ·çš„é—®é¢˜"
      },
      {
        "user": "Radohead",
        "created_at": "2025-02-15T15:14:32Z",
        "body": "ç›®å‰æ¶æ„ä¸‹ç”¨numaæ„å‘³ç€æœ‰å‡ è·¯cpuï¼Œå†…å­˜å°±å¤åˆ¶å‡ ä»½ï¼Œæ‰€ä»¥2è·¯cpuè·‘q8 1tæ˜¯ä¸å¤Ÿçš„å¤§æ¦‚è¦1.5t"
      },
      {
        "user": "BiFangKNT",
        "created_at": "2025-02-15T16:06:17Z",
        "body": "è®¤çœŸçœ‹æ–‡æ¡£å•Šï¼Œä½ éƒ½çŸ¥é“numaäº†ï¼Œéš¾é“ä¸çŸ¥é“åŒè·¯ä¼šæŠŠæ¨¡å‹å¤åˆ¶ä¸€ä»½å—\n\nä¸€ä»½æ¨¡å‹å 713GBï¼Œä¸¤ä»½å°±æ˜¯ 713*2=1426GB å•Š"
      },
      {
        "user": "adonishong",
        "created_at": "2025-02-15T16:25:26Z",
        "body": "æˆ‘é”™äº†, ç¡®å®çœ‹æ–‡æ¡£ä¸å¤Ÿä»”ç»†"
      }
    ]
  },
  {
    "number": 293,
    "title": "æ˜¯å¦æ”¯æŒ A6000 æˆ–è€… A100 å•å¡ï¼Ÿ",
    "created_at": "2025-02-14T11:39:20Z",
    "closed_at": "2025-02-16T12:01:45Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/293",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/293/comments",
    "author": "Liangdi",
    "comments": [
      {
        "user": "vc5409ftu",
        "created_at": "2025-02-15T23:45:47Z",
        "body": "å¯ä»¥"
      },
      {
        "user": "Liangdi",
        "created_at": "2025-02-16T12:01:45Z",
        "body": "> å¯ä»¥\n\nè°¢è°¢"
      }
    ]
  },
  {
    "number": 290,
    "title": "ensure that gguf_path argument is a directory.",
    "created_at": "2025-02-14T10:31:07Z",
    "closed_at": "2025-02-15T14:48:57Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/290",
    "body": "When pass gguf file to `--gguf_path`\r\n```\r\npython ktransformers/local_chat.py --model_path /workspace/models/Qwen2-57B-A14B-Instruct --gguf_path /workspace/models/Qwen2-57B-A14B-Instruct-GGUF/qwen2-57b-a14b-instruct-q4_k_m.gguf\r\n```\r\n`GGUFLoader.__init__` does not check whether gguf_path is a directory, and the error output is\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 179, in <module>\r\n    fire.Fire(local_chat)\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 110, in local_chat\r\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\r\n  File \"/workspace/ktransformers/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\r\n    load_weights(module, gguf_loader)\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 85, in load_weights\r\n    module.load()\r\n  File \"/workspace/ktransformers/ktransformers/operators/base_operator.py\", line 60, in load\r\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 81, in load_weights\r\n    load_cur_state_dict(module, gguf_loader, prefix)\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 76, in load_cur_state_dict\r\n    raise Exception(f\"can't find {translated_key} in GGUF file!\")\r\nException: can't find token_embd.weight in GGUF file!\r\n```\r\n\r\nAfter this patch, the output will be as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 179, in <module>\r\n    fire.Fire(local_chat)\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 110, in local_chat\r\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\r\n  File \"/workspace/ktransformers/ktransformers/optimize/optimize.py\", line 126, in optimize_and_load_gguf\r\n    gguf_loader=GGUFLoader(gguf_path)\r\n                ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ktransformers/ktransformers/util/custom_gguf.py\", line 170, in __init__\r\n    raise NotADirectoryError(f\"GGUF is not a dir: {gguf_path}\")\r\nNotADirectoryError: GGUF is not a dir: /workspace/models/Qwen2-57B-A14B-Instruct-GGUF/qwen2-57b-a14b-instruct-q4_k_m.gguf\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/290/comments",
    "author": "ZhangShuaiyi",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-14T19:17:38Z",
        "body": "Thanks for your contribution! Have you tested this PR to ensure everything works as expected?\r\n\r\nI have a suggestion: You could check the provided path and handle it more flexibly.\r\n\tâ€¢\tIf the input is a file, use its containing directory.\r\n\tâ€¢\tIf the input is already a directory, use it directly.\r\n\r\nThis approach would make it more user-friendly and adaptable. Let me know what you think."
      },
      {
        "user": "ZhangShuaiyi",
        "created_at": "2025-02-15T07:15:47Z",
        "body": "@Azure-Tang  Thanks, done. And raise an error if no gguf file is found in the directory."
      }
    ]
  },
  {
    "number": 277,
    "title": "Linuxç¯å¢ƒä¸‹å®‰è£…ktransformerså¤±è´¥",
    "created_at": "2025-02-14T08:18:04Z",
    "closed_at": "2025-02-17T01:34:12Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/277",
    "body": "æ„Ÿè°¢ä½ ä»¬å¼€æºäº†ä»£ç \n\næˆ‘åœ¨æœ¬åœ°ç¯å¢ƒä¸‹éƒ¨ç½²ktransformersé‡åˆ°äº†éš¾é¢˜ï¼Œéƒ¨ç½²çš„ç¯å¢ƒå¦‚ä¸‹\nOSï¼šUbuntu22.04\nCUDAç‰ˆæœ¬ï¼š12.4.1\ntorchç‰ˆæœ¬ï¼š2.6.0\n\nå°è¯•äº†ä¸¤ç§å®‰è£…æ–¹å¼ï¼Œé¦–å…ˆæ˜¯pipå®‰è£…ï¼Œé‡åˆ°çš„é”™è¯¯\n`WARNING: Generating metadata for package ktransformers produced metadata for project name unknown. Fix your #egg=ktransformers fragments.`\nåæ¥å°è¯•äº†æºç å®‰è£…çš„æ–¹å¼ï¼Œèƒ½æ­£å¸¸å®‰è£…ï¼Œä½†æ˜¯æ— æ³•é€šè¿‡`python -m ktransformers`ä½¿ç”¨ï¼Œé€šè¿‡pip freezeæŸ¥çœ‹èƒ½çœ‹åˆ°`UNKNOWN @ file:///home/ubuntu/ktransformers`\n\nè¯·é—®è¿™ä¸ªé—®é¢˜åº”è¯¥å¦‚ä½•è§£å†³å‘¢ï¼Ÿ\npythonçš„ç‰ˆæœ¬æ˜¯ï¼š3.10.12\npipçš„ç‰ˆæœ¬æ˜¯ï¼š25.0.1ï¼ˆå·²æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ï¼‰\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/277/comments",
    "author": "zbljz98",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-14T08:24:04Z",
        "body": "pip show ktransformerså¯ä»¥æ‰¾åˆ°åŒ…ä¹ˆ"
      },
      {
        "user": "SongXiaoMao",
        "created_at": "2025-02-15T02:08:49Z",
        "body": "çœ‹ä¸€ä¸‹ nvcc -Væ˜¯å¤šå°‘ï¼Ÿ"
      },
      {
        "user": "bigben446",
        "created_at": "2025-02-15T14:39:07Z",
        "body": "> pip show ktransformerså¯ä»¥æ‰¾åˆ°åŒ…ä¹ˆ\n\nlinuxä¸‹èƒ½æä¾›pipxå‘½ä»¤å®‰è£…å—ï¼Œpipå®‰è£…ç³»ç»ŸæŠ¥é”™\npipå®‰è£…æŠ¥é”™ï¼šerror: externally-managed-environment\nè¿™ä¸ªé”™è¯¯ä¿¡æ¯æ˜¯ç”±äº Python çš„æ–°ç­–ç•¥ï¼ˆPEP 668ï¼‰æ‰€å¯¼è‡´çš„ï¼Œè¯¥ç­–ç•¥æ—¨åœ¨é˜²æ­¢ç”¨æˆ·åœ¨ç³»ç»Ÿçº§ Python ç¯å¢ƒä¸­éšæ„å®‰è£…åŒ…ï¼Œä»¥é¿å…ç ´åç³»ç»Ÿçš„ Python å®‰è£…æˆ–æ“ä½œç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-15T14:52:16Z",
        "body": "> > pip show ktransformerså¯ä»¥æ‰¾åˆ°åŒ…ä¹ˆ\n> \n> linuxä¸‹èƒ½æä¾›pipxå‘½ä»¤å®‰è£…å—ï¼Œpipå®‰è£…ç³»ç»ŸæŠ¥é”™ pipå®‰è£…æŠ¥é”™ï¼šerror: externally-managed-environment è¿™ä¸ªé”™è¯¯ä¿¡æ¯æ˜¯ç”±äº Python çš„æ–°ç­–ç•¥ï¼ˆPEP 668ï¼‰æ‰€å¯¼è‡´çš„ï¼Œè¯¥ç­–ç•¥æ—¨åœ¨é˜²æ­¢ç”¨æˆ·åœ¨ç³»ç»Ÿçº§ Python ç¯å¢ƒä¸­éšæ„å®‰è£…åŒ…ï¼Œä»¥é¿å…ç ´åç³»ç»Ÿçš„ Python å®‰è£…æˆ–æ“ä½œç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚\n\nå…ˆè£…ä¸€ä¸ªcondaç”¨æ¥ç®¡ç†ç¯å¢ƒå§ï¼Œæˆ‘æ„Ÿè§‰æ˜¯ä½ ç›´æ¥ç”¨ç³»ç»Ÿpythonçš„pipè£…äº†"
      },
      {
        "user": "youde2000",
        "created_at": "2025-02-16T09:11:52Z",
        "body": "ä½ è§£å†³äº†ä¹ˆ"
      },
      {
        "user": "zbljz98",
        "created_at": "2025-02-17T01:32:40Z",
        "body": "> pip show ktransformerså¯ä»¥æ‰¾åˆ°åŒ…ä¹ˆ\n\né—®é¢˜è§£å†³äº†ï¼Œä¹‹å‰ç”¨çš„ç³»ç»Ÿçš„pipå®‰è£…çš„ï¼Œæ¢äº†condaæ­£å¸¸äº†"
      }
    ]
  },
  {
    "number": 258,
    "title": "AssertionError: assert self.gate_type == GGMLQuantizationType.BF16",
    "created_at": "2025-02-14T03:46:22Z",
    "closed_at": "2025-02-15T04:01:04Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/258",
    "body": "V0.3: \nktransformers --model_path /workspace/Deepseek-models/DeepSeek-R1 --gguf_path /workspace/Deepseek-models/DeepSeek-R1-Q4_K_M --port 10002\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/ktransformers\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/main.py\", line 114, in main\n    create_interface(config=cfg, default_args=cfg)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 27, in create_interface\n    GlobalInterface.interface = BackendInterface(default_args)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 47, in __init__\n    optimize_and_load_gguf(self.model, optimize_rule_path, gguf_path, config)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\n    load_weights(module, gguf_loader)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/experts.py\", line 520, in load\n    self.generate_experts.load(w)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/experts.py\", line 201, in load\n    assert self.gate_type == GGMLQuantizationType.BF16\nAssertionError",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/258/comments",
    "author": "yansiyu550",
    "comments": [
      {
        "user": "xuxiaoou",
        "created_at": "2025-02-14T06:09:00Z",
        "body": "+1\n v0.3 + 671b q4 same error\n v0.2 + 671b q4 ok"
      },
      {
        "user": "dtlzhuangz",
        "created_at": "2025-02-14T07:59:59Z",
        "body": "The same error."
      },
      {
        "user": "DearPlanet",
        "created_at": "2025-02-14T11:30:32Z",
        "body": "Same, failed at ktransformers/operators/experts.py when using prebuilt wheel ktransformers-0.3.0rc0+cu126torch26fancy-cp311-cp311-linux_x86_64.whl.\n```\nelif self.backend == \"AMXInt8\":\n            # Here\n            assert self.gate_type == GGMLQuantizationType.BF16\n            assert self.up_type == GGMLQuantizationType.BF16\n            assert self.down_type == GGMLQuantizationType.BF16\n            moe_config = AMX_MOEConfig(\n                n_routed_experts,\n                self.config.num_experts_per_tok,\n                self.config.hidden_size,\n                self.config.moe_intermediate_size,\n                25600,\n                gate_ptr,\n                up_ptr,\n                down_ptr,\n            )\n"
      },
      {
        "user": "hustwyk",
        "created_at": "2025-02-14T11:57:51Z",
        "body": "+1 The same error."
      },
      {
        "user": "Atream",
        "created_at": "2025-02-15T04:01:04Z",
        "body": "Version 0.3 is currently just a preview, and its features are not yet fully developed. It currently only supports on-the-fly quantization from BF16 in GGUF format. Version 0.3 is faster in prefill speed, but decoding can sometimes be slower than Version 0.2. It is recommended to use Version 0.2 for now."
      },
      {
        "user": "yansiyu550",
        "created_at": "2025-02-18T03:17:56Z",
        "body": "> Version 0.3 is currently just a preview, and its features are not yet fully developed. It currently only supports on-the-fly quantization from BF16 in GGUF format. Version 0.3 is faster in prefill speed, but decoding can sometimes be slower than Version 0.2. It is recommended to use Version 0.2 for now.\n\nHere is my test data. Why is V0.2 faster than V0.3 in both prefill and decode stages?\n# V0.3\nPerformance(T/s): prefill 1.9163943328367548, decode 1.6059428791832788. Time(s): tokenize 0.055808305740356445, prefill 5.218132734298706, decode 196.76914048194885\n# V0.2\n Performance(T/s): prefill 4.045294075735022, decode 3.956256483823671. Time(s): tokenize 0.01299285888671875, prefill 1.9776065349578857, decode 284.10695934295654"
      }
    ]
  },
  {
    "number": 247,
    "title": "[Doc]Fix dead link problem",
    "created_at": "2025-02-14T02:00:50Z",
    "closed_at": "2025-02-14T02:37:32Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/247",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/247/comments",
    "author": "liugddx",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-14T02:37:47Z",
        "body": "Thx for your fix~"
      }
    ]
  },
  {
    "number": 220,
    "title": "Add optimization config for Deepseek V3/R1 with 4 GPUs",
    "created_at": "2025-02-13T08:36:30Z",
    "closed_at": "2025-02-13T08:41:39Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/220",
    "body": "Add a new optimization config for Deepseek V3/R1 with 4 GPUs.\r\n\r\nWe tested it on our server with the following config:\r\n- CPU: 2 x AMD EPYC 7543 32-Core Processor\r\n- RAM: 16 Ã— 64GB Hynix PC4-25600 3200MHz Dual Rank DDR4 RDIMM\r\n- GPU: 8 x Nvidia RTX 3080 10G\r\n- Motherboard: ASUS KMPG-D32 Series\r\n\r\nThe decoding speed is ~5 tokens/s.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/220/comments",
    "author": "MengshiZhang",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T08:52:48Z",
        "body": "Thank you so much for your valuable contribution! Your PR is greatly appreciated and will be shared to everyone. Keep up, and we look forward to more contributions from you!"
      },
      {
        "user": "chuangzhidan",
        "created_at": "2025-02-14T06:42:49Z",
        "body": "> Thank you so much for your valuable contribution! Your PR is greatly appreciated and will be shared to everyone. Keep up, and we look forward to more contributions from you!\r\n\r\nhow to reproduce your result with the right server,which type ,model and configuration did you use?\r\nthank u so much if you can help"
      },
      {
        "user": "yydslc",
        "created_at": "2025-02-17T14:23:52Z",
        "body": "å¤§ä½¬æ˜¯å¦æœ‰å¼€AVXåŠ é€Ÿï¼Ÿ"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T16:00:42Z",
        "body": "> > Thank you so much for your valuable contribution! Your PR is greatly appreciated and will be shared to everyone. Keep up, and we look forward to more contributions from you!\r\n> \r\n> how to reproduce your result with the right server,which type ,model and configuration did you use? thank u so much if you can help\r\n\r\nPlz check our tutorial."
      }
    ]
  },
  {
    "number": 213,
    "title": "No module named 'ktransformers.models.modeling_deepseek_v3'",
    "created_at": "2025-02-13T07:27:25Z",
    "closed_at": "2025-02-13T08:18:42Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/213",
    "body": "\n<details><summary>cpu info</summary>\n<p>\n\n```shell\nprocessor       : 127\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 106\nmodel name      : Intel(R) Xeon(R) Platinum 8338C CPU @ 2.60GHz\nstepping        : 6\nmicrocode       : 0xd0003e7\ncpu MHz         : 800.000\ncache size      : 49152 KB\nphysical id     : 1\nsiblings        : 64\ncore id         : 31\ncpu cores       : 32\napicid          : 191\ninitial apicid  : 191\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 27\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nvmx flags       : vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling\nbugs            : spectre_v1 spectre_v2 spec_store_bypass swapgs mmio_stale_data eibrs_pbrsb gds bhi\nbogomips        : 5200.00\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 46 bits physical, 57 bits virtual\n``` \n\n\n</p>\n</details> \n\n<details><summary>pip list</summary>\n<p>\n```sh\n(ktransformers) root@user-B7129F83AV8E4HR-N:/home/user/app/ktransformers/ktransformers/optimize/optimize_rules# pip list\nPackage                  Version\n------------------------ -----------\naccelerate               1.3.0\naiohappyeyeballs         2.4.6\naiohttp                  3.11.12\naiosignal                1.3.2\nannotated-types          0.7.0\nanyio                    4.8.0\nattrs                    25.1.0\nblessed                  1.20.0\nbuild                    1.2.2.post1\ncertifi                  2025.1.31\ncharset-normalizer       3.4.1\nclick                    8.1.8\ncolorlog                 6.9.0\ncpufeature               0.2.1\neinops                   0.8.1\nfastapi                  0.115.8\nfilelock                 3.17.0\nfire                     0.7.0\nflash_attn               2.7.4.post1\nfrozenlist               1.5.0\nfsspec                   2025.2.0\ngreenlet                 3.1.1\nh11                      0.14.0\nhttpcore                 1.0.7\nhttpx                    0.28.1\nhuggingface-hub          0.28.1\nidna                     3.10\nJinja2                   3.1.5\njsonpatch                1.33\njsonpointer              3.0.0\nktransformers            0.1.4\nlangchain                0.3.18\nlangchain-core           0.3.35\nlangchain-text-splitters 0.3.6\nlangsmith                0.3.8\nMarkupSafe               3.0.2\nmpmath                   1.3.0\nmultidict                6.1.0\nnetworkx                 3.4.2\nninja                    1.11.1.3\nnumpy                    1.26.4\nnvidia-cublas-cu12       12.4.5.8\nnvidia-cuda-cupti-cu12   12.4.127\nnvidia-cuda-nvrtc-cu12   12.4.127\nnvidia-cuda-runtime-cu12 12.4.127\nnvidia-cudnn-cu12        9.1.0.70\nnvidia-cufft-cu12        11.2.1.3\nnvidia-curand-cu12       10.3.5.147\nnvidia-cusolver-cu12     11.6.1.9\nnvidia-cusparse-cu12     12.3.1.170\nnvidia-cusparselt-cu12   0.6.2\nnvidia-nccl-cu12         2.21.5\nnvidia-nvjitlink-cu12    12.4.127\nnvidia-nvtx-cu12         12.4.127\norjson                   3.10.15\npackaging                24.2\npip                      25.0\npropcache                0.2.1\nprotobuf                 5.29.3\npsutil                   6.1.1\npydantic                 2.10.6\npydantic_core            2.27.2\npyproject_hooks          1.2.0\nPyYAML                   6.0.2\nregex                    2024.11.6\nrequests                 2.32.3\nrequests-toolbelt        1.0.0\nsafetensors              0.5.2\nsentencepiece            0.2.0\nsetuptools               75.8.0\nsix                      1.17.0\nsniffio                  1.3.1\nSQLAlchemy               2.0.38\nstarlette                0.45.3\nsympy                    1.13.1\ntenacity                 9.0.0\ntermcolor                2.5.0\ntokenizers               0.19.1\ntorch                    2.6.0\ntqdm                     4.67.1\ntransformers             4.43.2\ntriton                   3.2.0\ntyping_extensions        4.12.2\nurllib3                  2.3.0\nuvicorn                  0.34.0\nwcwidth                  0.2.13\nwheel                    0.45.1\nyarl                     1.18.3\nzstandard                0.23.0\n```\n\n</p>\n</details> \n\n<details><summary>run cli</summary>\n<p>\n\n```sh\npython -m ktransformers.local_chat \\\n    --model_path deepseek-ai/DeepSeek-R1  \\\n    --gguf_path ./gguf_file \\\n    --optimize_rule_path /home/user/app/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml\n\n```\n\n</p>\n</details> \n\n<details><summary>gpu info</summary>\n<p>\n\n```sh\nroot@user-B7129F83AV8E4HR-N:/home/user/app/models/deepseek-R1# nvidia-smi\nThu Feb 13 15:25:32 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40                     On  |   00000000:15:00.0 Off |                    0 |\n| N/A   29C    P8             54W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA L40                     On  |   00000000:18:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA L40                     On  |   00000000:19:00.0 Off |                    0 |\n| N/A   27C    P0             72W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA L40                     On  |   00000000:1C:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   4  NVIDIA L40                     On  |   00000000:1D:00.0 Off |                    0 |\n| N/A   29C    P0             69W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   5  NVIDIA L40                     On  |   00000000:96:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   6  NVIDIA L40                     On  |   00000000:99:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   7  NVIDIA L40                     On  |   00000000:9A:00.0 Off |                    0 |\n| N/A   27C    P0             70W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   8  NVIDIA L40                     On  |   00000000:9D:00.0 Off |                    0 |\n| N/A   25C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   9  NVIDIA L40                     On  |   00000000:9E:00.0 Off |                    0 |\n| N/A   27C    P8             26W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n\n```\n\n</p>\n</details> \n\n<details><summary>Ram info</summary>\n<p>\n\n```sh\nroot@user-B7129F83AV8E4HR-N:/home/user/app/models/deepseek-R1# free -g\n               total        used        free      shared  buff/cache   available\nMem:             503         102         356           0          44         397\nSwap:              0           0           0\n```\n\n</p>\n</details> \n\n# Error show :\n\n```sh\n\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/local_chat.py\", line 159, in <module>\n    fire.Fire(local_chat)\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/local_chat.py\", line 106, in local_chat\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/optimize/optimize.py\", line 122, in optimize_and_load_gguf\n    gen_optimize_config(module, optimize_config, rule_list, default_device = default_device)\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/optimize/optimize.py\", line 68, in gen_optimize_config\n    module_cls=getattr(__import__(import_module_name, fromlist=[\"\"]), import_class_name)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'ktransformers.models.modeling_deepseek_v3'\n```\n\n\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/213/comments",
    "author": "Zha-Miku",
    "comments": [
      {
        "user": "Linzwcs",
        "created_at": "2025-02-13T07:39:01Z",
        "body": " ktransformers 0.1.4ç‰ˆæœ¬ä½äº†ï¼Œ ç°åœ¨å¾—ä»æºç å®‰è£…ï¼Œpipæºé‚£è¾¹è¿˜æ²¡æ›´æ–°\n"
      }
    ]
  },
  {
    "number": 212,
    "title": "500Gå†…å­˜åªç”¨äº†18G",
    "created_at": "2025-02-13T07:23:23Z",
    "closed_at": "2025-02-14T08:53:00Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/212",
    "body": "500Gå†…å­˜\nå¿—å¼ºCPUï¼Œå•å¡è¿è¡Œï¼Œé€Ÿåº¦è¿˜å¯ä»¥8 tokenæ¯ç§’ï¼Œä½¿ç”¨äº†2.5é‡åŒ–æ¨¡å‹ã€‚\n\nä½†åªæœ‰18gå†…å­˜ç”¨åˆ°ï¼ŒçŒœæµ‹éœ€è¦å¼€å¯å†…å­˜è½½å…¥ï¼ŒåŒæ ·æƒ…å†µæˆ‘åœ¨llama cppä¸Šé‡åˆ°è¿‡ï¼Œè§£å†³äº†ï¼Œä½†ä¼¼ä¹å¯¹ktransformersæ— æ•ˆã€‚\n\nå®‰è£…è½¯ä»¶ï¼šktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n\nå‘½ä»¤ï¼š\npython -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n\n\n\n\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/212/comments",
    "author": "lymanzhao",
    "comments": [
      {
        "user": "murongweibo",
        "created_at": "2025-02-13T07:26:00Z",
        "body": "> 500Gå†…å­˜ å¿—å¼ºCPUï¼Œå•å¡è¿è¡Œï¼Œé€Ÿåº¦è¿˜å¯ä»¥8 tokenæ¯ç§’ï¼Œä½¿ç”¨äº†2.5é‡åŒ–æ¨¡å‹ã€‚\n> \n> ä½†åªæœ‰18gå†…å­˜ç”¨åˆ°ï¼ŒçŒœæµ‹éœ€è¦å¼€å¯å†…å­˜è½½å…¥ï¼ŒåŒæ ·æƒ…å†µæˆ‘åœ¨llama cppä¸Šé‡åˆ°è¿‡ï¼Œè§£å†³äº†ï¼Œä½†ä¼¼ä¹å¯¹ktransformersæ— æ•ˆã€‚\n> \n> å®‰è£…è½¯ä»¶ï¼šktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> \n> å‘½ä»¤ï¼š python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n\nä½ è¿™ä¸ªç”¨ä»€ä¹ˆgpuï¼Ÿæˆ‘çš„æŠ¥äº†é”™\n```\nRuntimeError: CUDA error: invalid device function\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```"
      },
      {
        "user": "shuchong",
        "created_at": "2025-02-13T07:26:56Z",
        "body": "> > 500Gå†…å­˜ å¿—å¼ºCPUï¼Œå•å¡è¿è¡Œï¼Œé€Ÿåº¦è¿˜å¯ä»¥8 tokenæ¯ç§’ï¼Œä½¿ç”¨äº†2.5é‡åŒ–æ¨¡å‹ã€‚\n> > ä½†åªæœ‰18gå†…å­˜ç”¨åˆ°ï¼ŒçŒœæµ‹éœ€è¦å¼€å¯å†…å­˜è½½å…¥ï¼ŒåŒæ ·æƒ…å†µæˆ‘åœ¨llama cppä¸Šé‡åˆ°è¿‡ï¼Œè§£å†³äº†ï¼Œä½†ä¼¼ä¹å¯¹ktransformersæ— æ•ˆã€‚\n> > å®‰è£…è½¯ä»¶ï¼šktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > å‘½ä»¤ï¼š python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> \n> ä½ è¿™ä¸ªç”¨ä»€ä¹ˆgpuï¼Ÿæˆ‘çš„æŠ¥äº†é”™\n> \n> ```\n> RuntimeError: CUDA error: invalid device function\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> ```\n\nè¿™ä¸ªæŠ¥é”™æ˜¯ä¸æ˜¯å’Œå¡æœ‰å…³ç³»ï¼Œæˆ‘ä½¿ç”¨v100ä¹Ÿæœ‰è¿™ä¸ªæŠ¥é”™"
      },
      {
        "user": "lymanzhao",
        "created_at": "2025-02-13T07:52:47Z",
        "body": "> > 500Gå†…å­˜ å¿—å¼ºCPUï¼Œå•å¡è¿è¡Œï¼Œé€Ÿåº¦è¿˜å¯ä»¥8 tokenæ¯ç§’ï¼Œä½¿ç”¨äº†2.5é‡åŒ–æ¨¡å‹ã€‚\n> > ä½†åªæœ‰18gå†…å­˜ç”¨åˆ°ï¼ŒçŒœæµ‹éœ€è¦å¼€å¯å†…å­˜è½½å…¥ï¼ŒåŒæ ·æƒ…å†µæˆ‘åœ¨llama cppä¸Šé‡åˆ°è¿‡ï¼Œè§£å†³äº†ï¼Œä½†ä¼¼ä¹å¯¹ktransformersæ— æ•ˆã€‚\n> > å®‰è£…è½¯ä»¶ï¼šktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > å‘½ä»¤ï¼š python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> \n> ä½ è¿™ä¸ªç”¨ä»€ä¹ˆgpuï¼Ÿæˆ‘çš„æŠ¥äº†é”™\n> \n> ```\n> RuntimeError: CUDA error: invalid device function\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> ```\n\næˆ‘æ˜¯4090ï¼Œä½ è¿™ä¸ªçœ‹ç€æ˜¯torchçš„é—®é¢˜ï¼Œæ£€æŸ¥æ˜¯ä¸æ˜¯ç‰ˆæœ¬é—®é¢˜å‘¢"
      },
      {
        "user": "Xxianna",
        "created_at": "2025-02-13T08:02:38Z",
        "body": "åŒï¼ŒåŠ è½½deepseek r1 q4kmååªæœ‰å¤§æ¦‚15gå†…å­˜å’Œ15gæ˜¾å­˜å ç”¨\nä½†æ˜¯åŠ è½½è¿‡ç¨‹äº§ç”Ÿäº†å·¨å¤§çš„cache"
      },
      {
        "user": "futureltj",
        "created_at": "2025-02-13T08:06:43Z",
        "body": "åŒæ ·çš„æƒ…å†µï¼Œæ¨ç†é€Ÿåº¦å¾ˆæ…¢ã€‚æœ‰è§£å—ï¼Ÿ"
      },
      {
        "user": "shiontao",
        "created_at": "2025-02-13T08:33:30Z",
        "body": "> > > 500Gå†…å­˜ å¿—å¼ºCPUï¼Œå•å¡è¿è¡Œï¼Œé€Ÿåº¦è¿˜å¯ä»¥8 tokenæ¯ç§’ï¼Œä½¿ç”¨äº†2.5é‡åŒ–æ¨¡å‹ã€‚\n> > > ä½†åªæœ‰18gå†…å­˜ç”¨åˆ°ï¼ŒçŒœæµ‹éœ€è¦å¼€å¯å†…å­˜è½½å…¥ï¼ŒåŒæ ·æƒ…å†µæˆ‘åœ¨llama cppä¸Šé‡åˆ°è¿‡ï¼Œè§£å†³äº†ï¼Œä½†ä¼¼ä¹å¯¹ktransformersæ— æ•ˆã€‚\n> > > å®‰è£…è½¯ä»¶ï¼šktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > > å‘½ä»¤ï¼š python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> > \n> > \n> > ä½ è¿™ä¸ªç”¨ä»€ä¹ˆgpuï¼Ÿæˆ‘çš„æŠ¥äº†é”™\n> > ```\n> > RuntimeError: CUDA error: invalid device function\n> > CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> > For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> > Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> > ```\n> \n> è¿™ä¸ªæŠ¥é”™æ˜¯ä¸æ˜¯å’Œå¡æœ‰å…³ç³»ï¼Œæˆ‘ä½¿ç”¨v100ä¹Ÿæœ‰è¿™ä¸ªæŠ¥é”™\n\nä¸€æ ·çš„ï¼Œåº”è¯¥æ˜¯æ¶æ„å¤ªè€äº†çš„åŸå› "
      },
      {
        "user": "murongweibo",
        "created_at": "2025-02-13T09:18:13Z",
        "body": "> > > 500Gå†…å­˜ å¿—å¼ºCPUï¼Œå•å¡è¿è¡Œï¼Œé€Ÿåº¦è¿˜å¯ä»¥8 tokenæ¯ç§’ï¼Œä½¿ç”¨äº†2.5é‡åŒ–æ¨¡å‹ã€‚\n> > > ä½†åªæœ‰18gå†…å­˜ç”¨åˆ°ï¼ŒçŒœæµ‹éœ€è¦å¼€å¯å†…å­˜è½½å…¥ï¼ŒåŒæ ·æƒ…å†µæˆ‘åœ¨llama cppä¸Šé‡åˆ°è¿‡ï¼Œè§£å†³äº†ï¼Œä½†ä¼¼ä¹å¯¹ktransformersæ— æ•ˆã€‚\n> > > å®‰è£…è½¯ä»¶ï¼šktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > > å‘½ä»¤ï¼š python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> > \n> > \n> > ä½ è¿™ä¸ªç”¨ä»€ä¹ˆgpuï¼Ÿæˆ‘çš„æŠ¥äº†é”™\n> > ```\n> > RuntimeError: CUDA error: invalid device function\n> > CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> > For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> > Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> > ```\n> \n> è¿™ä¸ªæŠ¥é”™æ˜¯ä¸æ˜¯å’Œå¡æœ‰å…³ç³»ï¼Œæˆ‘ä½¿ç”¨v100ä¹Ÿæœ‰è¿™ä¸ªæŠ¥é”™\n\nåº”è¯¥æ˜¯å¡çš„æ¡†æ¶åŸå› ï¼Œæˆ‘è¿™é‡Œç”¨çš„è°·æ­Œcolabé‡Œé¢çš„åˆ†çš„T4ï¼Œæˆ‘çœ‹åˆ«äººè¯´V100ä¹Ÿä¸è¡Œ"
      },
      {
        "user": "ipfgao",
        "created_at": "2025-02-13T13:53:12Z",
        "body": "> åŒï¼ŒåŠ è½½deepseek r1 q4kmååªæœ‰å¤§æ¦‚15gå†…å­˜å’Œ15gæ˜¾å­˜å ç”¨ ä½†æ˜¯åŠ è½½è¿‡ç¨‹äº§ç”Ÿäº†å·¨å¤§çš„cache\n\nè¿™æ ·çš„è¯ï¼Œä¸ç”¨32gå†…å­˜ä¹Ÿæœ‰å¸Œæœ›è¿è¡Œdeepseek r1 q4kmäº†ï¼Ÿ"
      },
      {
        "user": "lymanzhao",
        "created_at": "2025-02-13T14:31:25Z",
        "body": "> > åŒï¼ŒåŠ è½½deepseek r1 q4kmååªæœ‰å¤§æ¦‚15gå†…å­˜å’Œ15gæ˜¾å­˜å ç”¨ ä½†æ˜¯åŠ è½½è¿‡ç¨‹äº§ç”Ÿäº†å·¨å¤§çš„cache\n> \n> è¿™æ ·çš„è¯ï¼Œä¸ç”¨32gå†…å­˜ä¹Ÿæœ‰å¸Œæœ›è¿è¡Œdeepseek r1 q4kmäº†ï¼Ÿ\n\nä½ æ»¡è¶³è¿™ç‚¹é€Ÿåº¦å—ï¼Œè¿™æ˜¯ç¡¬ç›˜ç¼“å­˜é™åˆ¶é€Ÿåº¦"
      },
      {
        "user": "lymanzhao",
        "created_at": "2025-02-14T08:52:56Z",
        "body": "topå¯ä»¥çœ‹åˆ°çœŸå®ä½¿ç”¨æƒ…å†µ"
      }
    ]
  },
  {
    "number": 209,
    "title": "ä»€ä¹ˆæ—¶å€™æ”¯æŒamdæ˜¾å¡å•Šï¼Œ rocmç”Ÿæ€è¿™ä¹ˆå·®å—ï¼Ÿ",
    "created_at": "2025-02-13T06:39:11Z",
    "closed_at": "2025-02-13T06:44:57Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/209",
    "body": "ä»€ä¹ˆæ—¶å€™æ”¯æŒamdæ˜¾å¡å•Šï¼Œ rocmç”Ÿæ€è¿™ä¹ˆå·®å—ï¼Ÿ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/209/comments",
    "author": "carsonfly",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T06:44:49Z",
        "body": "PRé‡Œæœ‰ä¸ªå¤§ä½¬æ­£åœ¨æ”¯æŒï¼Œæ‚¨å¯ä»¥å…³æ³¨ä¸€ä¸‹\n"
      }
    ]
  },
  {
    "number": 193,
    "title": "è¯·é—®æ”¯æŒHç³»åˆ—æ˜¾å¡å•å¡è¿è¡ŒDeepSeek V3/R1å—",
    "created_at": "2025-02-13T02:56:16Z",
    "closed_at": "2025-02-13T04:17:31Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/193",
    "body": "æ˜¾å¡ç³»åˆ—ï¼šH20",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/193/comments",
    "author": "CarsonSo",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T04:05:06Z",
        "body": "è¿™ä¸ªä¸»è¦å–å†³äºæ˜¾å¡çš„æ¶æ„å’Œæ˜¾å­˜ï¼Œéœ€è¦ampereåŠä»¥ä¸Šæ¶æ„ä»¥åŠ16GBä»¥ä¸Šæ˜¾å­˜ã€‚Hç³»åˆ—æ˜¯å¯ä»¥çš„"
      }
    ]
  },
  {
    "number": 189,
    "title": "fix typo in README.md",
    "created_at": "2025-02-13T02:16:27Z",
    "closed_at": "2025-02-13T02:24:01Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/189",
    "body": "replace misspelling 'ktransfermor' with 'ktransformers'",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/189/comments",
    "author": "Kattos",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T08:53:45Z",
        "body": "Thank you so much for your valuable contribution~"
      }
    ]
  },
  {
    "number": 180,
    "title": "doc: fix clerical error",
    "created_at": "2025-02-12T23:27:47Z",
    "closed_at": "2025-02-13T02:25:30Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/180",
    "body": "fix clerical error: Fed -> Feb",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/180/comments",
    "author": "lusipad",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T08:54:15Z",
        "body": "Thank you so much for your correction~"
      }
    ]
  },
  {
    "number": 169,
    "title": "è¯·é—®æ”¯æŒçš„pythonæœ€ä½ç‰ˆæœ¬æ˜¯3.11åŠä»¥ä¸Šå—ï¼Ÿ",
    "created_at": "2025-02-12T09:47:40Z",
    "closed_at": "2025-02-13T02:27:52Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/169",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/169/comments",
    "author": "bachelor-dou",
    "comments": [
      {
        "user": "KMSorSMS",
        "created_at": "2025-02-12T12:20:14Z",
        "body": "> No description provided.\n\næˆ‘ä»¬éƒ½æ˜¯åœ¨3.11ç‰ˆæœ¬ä»¥ä¸Šè·‘çš„"
      },
      {
        "user": "Atream",
        "created_at": "2025-02-13T02:27:52Z",
        "body": "åªè¦torchèƒ½æ­£å¸¸è¿è¡Œå°±å¯ä»¥ï¼Œpythonç‰ˆæœ¬åˆ«å¤ªè€æ—§å°±è¡Œï¼Œæœ‰äººç”¨python 3.10è·‘é€šè¿‡ï¼Œæ›´ä½çš„ç‰ˆæœ¬æ²¡æœ‰è¯•è¿‡ï¼Œä½†ç†è®ºä¸Šæ²¡å•¥é—®é¢˜"
      }
    ]
  },
  {
    "number": 154,
    "title": "v0.2 run DeepSeek-R1-Q4_K_M failed.",
    "created_at": "2025-02-12T05:24:03Z",
    "closed_at": "2025-02-12T06:36:03Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/154",
    "body": "```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 278, in <module>\n    fire.Fire(local_chat)\n  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 215, in local_chat\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\n    load_weights(module, gguf_loader)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/gate.py\", line 110, in load\n    if w is None: w = self.load_weights(device=device)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/gate.py\", line 72, in load_weights\n    tensors = self.load_multi(key, targets, device=device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/gate.py\", line 85, in load_multi\n    tensors[k] = self.gguf_loader.load_gguf_tensor(key + k, device=device)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/custom_gguf.py\", line 300, in load_gguf_tensor\n    values = values.view(shape[::-1])\n             ^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[256, 7168]' is invalid for input of size 0\n```\n```\npython3.11 -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-UD-Q2_K_XL  --cpu_infer 24 --max_new_tokens 1000\n```\nHi, UD-Q2_K_XL works pretty well on my server. But when I switch the model to Q4_K_M, the above RuntimeError was thrown.\nCould you please clarify for me?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/154/comments",
    "author": "ArYuZzz",
    "comments": [
      {
        "user": "Bob99uk",
        "created_at": "2025-02-12T06:37:56Z",
        "body": "Can i ask how you resolved this?"
      },
      {
        "user": "wangfan008",
        "created_at": "2025-02-17T08:07:25Z",
        "body": "+1"
      },
      {
        "user": "EBGU",
        "created_at": "2025-02-19T08:46:13Z",
        "body": "+10086"
      },
      {
        "user": "HuDi2018",
        "created_at": "2025-02-20T09:29:01Z",
        "body": "> Can i ask how you resolved this?\n\nCheck if the GGUF weights have been downloaded completely."
      }
    ]
  },
  {
    "number": 134,
    "title": "èƒ½å¦æ”¯æŒSM75çš„è®¾å¤‡",
    "created_at": "2025-02-11T06:45:02Z",
    "closed_at": "2025-02-11T06:55:24Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/134",
    "body": "å¦‚æœä¸€å®šä½¿ç”¨flash-attenttionçš„ï¼Œæ˜¯å¦å¯ä»¥é€‚é…flash-attenttion1.0ç‰ˆæœ¬",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/134/comments",
    "author": "bltcn",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-11T06:55:06Z",
        "body": "ä¸æ”¯æŒsm75ï¼Œæˆ‘ä»¬ç”¨çš„marlinç®—å­åªæ”¯æŒampereåŠä»¥ä¸Šæ¶æ„"
      }
    ]
  },
  {
    "number": 133,
    "title": "è£…ä¸äº†å•Š",
    "created_at": "2025-02-11T06:03:47Z",
    "closed_at": "2025-02-11T16:02:08Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/133",
    "body": "é‚£ä¸ªdeepseekr1æ˜¯æ€ä¹ˆè·‘èµ·æ¥çš„ã€‚\n./install.sh ç›´æ¥UNKNOW",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/133/comments",
    "author": "qixing-ai",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-11T06:52:24Z",
        "body": "ä¸¥æ ¼follow readmeäº†å—ï¼Œè´´ä¸€ä¸‹æŠ¥é”™ï¼Ÿ"
      },
      {
        "user": "Atream",
        "created_at": "2025-02-11T06:58:28Z",
        "body": "å¦‚æœæ˜¯è¦æºç å®‰è£…çš„è¯ï¼Œè¯•ä¸€ä¸‹bash install.shï¼Œå¦‚æœå·²ç»å®‰è£…æˆåŠŸè¦è·‘çš„è¯ï¼Œçœ‹ä¸‹é¢çš„python -m ktransformers.local_chat"
      }
    ]
  },
  {
    "number": 118,
    "title": "use HF cli to download ggufs instead of wget",
    "created_at": "2024-12-29T16:34:57Z",
    "closed_at": "2025-02-11T10:20:50Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/118",
    "body": "its easier to use HF cli to pull gulfs or any file/files directly.\r\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/118/comments",
    "author": "lzumot",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-11T10:20:44Z",
        "body": "Access to Hugging Face is limited in certain regions, so we have provided a more flexible solution to ensure consistent usage."
      }
    ]
  },
  {
    "number": 103,
    "title": "How to infer quantized models on CPU&GPU",
    "created_at": "2024-10-20T06:33:50Z",
    "closed_at": "2024-10-23T07:19:10Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/103",
    "body": "I found that ktransformers first performs a dequantize operation when loading the weights. Due to DRAM limitations, I want to directly infer the model with quantized weights on CPU&GPU. How can I implement this?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/103/comments",
    "author": "shuzhang-pku",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-10-23T07:18:37Z",
        "body": "The dequantised weight will be quantised again into marlin format to use marlin op (which is fast). So if you are using optimising rules that we provided, you are directly using quantized weights."
      }
    ]
  },
  {
    "number": 100,
    "title": "Does ktransformers support deepseek V2.5ï¼Ÿ",
    "created_at": "2024-10-12T11:34:10Z",
    "closed_at": "2024-10-15T06:37:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/100",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/100/comments",
    "author": "huliangbing",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-10-14T06:58:02Z",
        "body": "Yesï½you can run v2.5 with deepseekv2â€˜s yaml."
      },
      {
        "user": "huliangbing",
        "created_at": "2024-10-14T17:39:28Z",
        "body": "Thank you very much!"
      }
    ]
  },
  {
    "number": 95,
    "title": "Suggestion to add DeepSeek v2.5 support",
    "created_at": "2024-09-24T08:40:58Z",
    "closed_at": "2024-09-25T08:49:30Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/95",
    "body": "Recently DeepSeek 2.5 MoE model was released. Please consider adding a support for it. (Q5_K_M or Q4_K_M)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/95/comments",
    "author": "arisau",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-24T10:31:11Z",
        "body": "It appears to be supported automatically, as some users have already tested it successfully."
      },
      {
        "user": "arisau",
        "created_at": "2024-09-24T21:04:31Z",
        "body": "Perfect. Thank you. \r\njust to confirm, only Q4_K_M works?"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-25T04:45:04Z",
        "body": "We have supported all Qx_km format~"
      },
      {
        "user": "arisau",
        "created_at": "2024-09-25T08:49:30Z",
        "body": "Understood. Thank you once again. "
      }
    ]
  },
  {
    "number": 90,
    "title": "Installation Problem",
    "created_at": "2024-09-15T04:32:18Z",
    "closed_at": "2024-09-24T06:33:07Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/90",
    "body": "My GPU device is L20, the two installation ways are not work for me, could you give me some suggestions?\r\n1. What I run 'bash install.sh'. The error is:\r\n      /root/ktransformers/ktransformers/ktransformers_ext/cpu_backend/cpuinfer.h:45:43: error: using invalid field â€˜CPUInfer::enqueue(Func, Obj*, Args ...)::__lambda1::__argsâ€™\r\n      ninja: build stopped: subcommand failed.\r\n      Traceback (most recent call last):\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\r\n          return _build_backend().build_wheel(wheel_directory, config_settings,\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/build_meta.py\", line 415, in build_wheel\r\n          return self._build_with_temp_dir(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/build_meta.py\", line 397, in _build_with_temp_dir\r\n          self.run_setup()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 293, in <module>\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/__init__.py\", line 108, in setup\r\n          return distutils.core.setup(**attrs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 184, in setup\r\n          return run_commands(dist)\r\n                 ^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\r\n          dist.run_commands()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\r\n          self.run_command(cmd)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"<string>\", line 153, in run\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/wheel/_bdist_wheel.py\", line 378, in run\r\n          self.run_command(\"build\")\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\r\n          self.run_command(cmd_name)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 93, in run\r\n          _build_ext.run(self)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 866, in build_extensions\r\n          build_ext.build_extensions(self)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 479, in build_extensions\r\n          self._build_extensions_serial()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 505, in _build_extensions_serial\r\n          self.build_extension(ext)\r\n        File \"<string>\", line 288, in build_extension\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/subprocess.py\", line 571, in run\r\n          raise CalledProcessError(retcode, process.args,\r\n      subprocess.CalledProcessError: Command '['cmake', '--build', '.']' returned non-zero exit status 1.\r\n      [end of output]\r\n  \r\n      note: This error originates from a subprocess, and is likely not a problem with pip.\r\n      ERROR: Failed building wheel for ktransformers\r\n      Failed to build ktransformers\r\n      ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\r\n\r\n2. When I run pip install ktransformers --no-build-isolation, there is no error, but when I run 'python -m ktransformers.local_chat --model_name deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ./DeepSeek-V2-Chat-0628-GGUFA', the error is:\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/root/ktransformers/ktransformers/local_chat.py\", line 25, in <module>\r\n    from ktransformers.optimize.optimize import optimize_and_load_gguf\r\n  File \"/root/ktransformers/ktransformers/optimize/optimize.py\", line 15, in <module>\r\n    from ktransformers.util.custom_gguf import GGUFLoader, translate_name_to_gguf\r\n  File \"/root/ktransformers/ktransformers/util/custom_gguf.py\", line 27, in <module>\r\n    import KTransformersOps\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.32' not found (required by /root/.conda/envs/ktransformers/lib/python3.11/site-packages/KTransformersOps.cpython-311-x86_64-linux-gnu.so)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/90/comments",
    "author": "Chain-Mao",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-18T00:44:44Z",
        "body": "This issue seems to be very similar to #37 , both involving the absence of libstdc++.so.6. Perhaps the methods mentioned there could useful?"
      }
    ]
  },
  {
    "number": 83,
    "title": "Use cond var to avoid busy loop",
    "created_at": "2024-09-11T09:12:11Z",
    "closed_at": "2024-10-09T10:57:17Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/83",
    "body": "Fix #80 ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/83/comments",
    "author": "sayap",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-13T00:58:00Z",
        "body": "I will merge the code after completing the tests on the Windows platform."
      }
    ]
  },
  {
    "number": 82,
    "title": "Seg Fault on long replies",
    "created_at": "2024-09-10T14:44:09Z",
    "closed_at": "2024-09-11T13:43:47Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/82",
    "body": "I am trying to use this as a llama.cpp replacement as it is a lot faster.  I did modify the backend args file (`ktransformers/server/backend/args.py`) as max_new_tokens isn't an option for the sever:\r\n```\r\n-    max_new_tokens: int = Field(500, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n+    max_new_tokens: int = Field(2040, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n```\r\n\r\nto allow for longer responses as 500 tokens was causing a lot of my stuff to get cut off half way through.\r\nIt does generate some tokens (feels like about the 500 limit) and then hard crashes.  So maybe there is some other limit that I need to adjust?\r\n\r\nI am using DeekSeek-V2.5 in Q4_K_M, I did also try it with WizardLM 8x22B and the same thing happens.\r\n\r\nHardware: 1x 3090, Epyc 7402, 512 Gb Ram\r\n\r\n```\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../ktransformers.sh: line 12: 2152563 Segmentation fault      (core dumped) ktransformers --model_path /nvmes/models/DeepSeek-V2.5/ --gguf_path /nvmes/models/DeepSeek-V2.5-GGUF2/ --port 8081 --cpu_infer 32\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/82/comments",
    "author": "matthusby",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-11T06:38:22Z",
        "body": "Hi, maybe this one will help #73 "
      },
      {
        "user": "matthusby",
        "created_at": "2024-09-11T13:43:47Z",
        "body": "Thank you, yeah changing the `cache_lens` param is what was causing my problem.  I guess my chat was at just the right length where the longer max tokens would trigger that error.\r\n\r\nFor reference I doubled it and now when I try to run DeepSeek-V2.5 I get a out of memory crash, but on Wizard 8x22 its working great.\r\n\r\nIt looks like there are plans to address some of this soon, so I will close this issue."
      }
    ]
  },
  {
    "number": 73,
    "title": "When the input token exceeds 4096, an error will occur.",
    "created_at": "2024-09-02T06:23:20Z",
    "closed_at": "2024-09-03T11:34:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/73",
    "body": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/fastapi/routing.py\", line 210, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 651, in forward\r\n    causal_mask = self._update_causal_mask(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1624, in _update_causal_mask\r\n    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\r\n                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nRuntimeError: The size of tensor a (4096) must match the size of tensor b (8122) at non-singleton dimension 3",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/73/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-03T09:17:30Z",
        "body": "Hi, I havenâ€™t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?"
      },
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T09:30:43Z",
        "body": "> Hi, I havenâ€™t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?\r\n\r\nI compiled it using the most recent code, and my configs are: \r\n``` yaml\r\n- match:\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding\r\n  replace:\r\n    class: ktransformers.operators.RoPE.YarnRotaryEmbedding\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\.(?!.*self_attn\\\\.kv_b_proj).*$\"  # regular expression\r\n    class: torch.nn.Linear  # only match modules matching name and class simultaneously\r\n  replace:\r\n    class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n      generate_op: \"KLinearMarlin\"\r\n      prefill_op: \"KLinearTorch\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp$\"\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE\r\n  replace:\r\n    class: ktransformers.operators.experts.KDeepseekV2MoE     # mlp module with custom forward function\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp\\\\.experts$\"\r\n  replace:\r\n    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism\r\n    kwargs:\r\n      prefill_device: \"cuda\"\r\n      prefill_op: \"KExpertsTorch\"\r\n      generate_device: \"cpu\"\r\n      generate_op: \"KExpertsCPU\"\r\n      out_device: \"cuda\"\r\n  recursive: False # don't recursively inject submodules of this module\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.self_attn$\"\r\n  replace:\r\n    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model$\"\r\n  replace:\r\n    class: \"ktransformers.operators.models.KDeepseekV2Model\"\r\n    kwargs:\r\n      per_layer_prefill_intput_threshold: 0 # 0 is close layer wise prefill\r\n- match:\r\n    name: \"^model.embed_tokens\"\r\n  replace:\r\n    class: \"default\"\r\n    kwargs:\r\n      generate_device: \"cpu\"\r\n      prefill_device: \"cuda\"\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T10:05:26Z",
        "body": "I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release.\r\nIf you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens"
      },
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T11:34:15Z",
        "body": "> I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release. If you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens\r\n\r\nLGTM"
      }
    ]
  },
  {
    "number": 72,
    "title": "Support IQ4_XS dequantize",
    "created_at": "2024-09-02T04:07:58Z",
    "closed_at": "2024-09-06T02:06:10Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/72",
    "body": "From my testing, the IQ4_XS quant of deepseek coder v2 has noticeably better quality than the Q3_K quants, while still can fit into 128GB of DRAM, unlike the Q4_K quants.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/72/comments",
    "author": "sayap",
    "comments": [
      {
        "user": "ThomasBaruzier",
        "created_at": "2024-09-03T20:27:08Z",
        "body": "Edit: Works well on my RTX 3090!\r\n\r\n<details>\r\n  <summary>Previous message</summary>\r\n\r\nTrying this PR, got unlucky:\r\n\r\n```python\r\n$ python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path DeepSeek-V2-Chat-0628-IQ4-XS --cpu_infer 30\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/local_chat.py\", line 159, in <module>\r\n    fire.Fire(local_chat)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/local_chat.py\", line 106, in local_chat\r\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\r\n    load_weights(module, gguf_loader)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 85, in load_weights\r\n    module.load()\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/operators/base_operator.py\", line 60, in load\r\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 81, in load_weights\r\n    load_cur_state_dict(module, gguf_loader, prefix)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 71, in load_cur_state_dict\r\n    weights = gguf_loader.load_gguf_tensor(translated_key, device = device).to(dtype = target_dtype)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/custom_gguf.py\", line 283, in load_gguf_tensor\r\n    raise NotImplementedError(f\"ggml_type {ggml_type} not implemented\")\r\nNotImplementedError: ggml_type 23 not implemented\r\n```\r\n\r\n</details>\r\n"
      },
      {
        "user": "sayap",
        "created_at": "2024-09-04T04:21:25Z",
        "body": "Great :)\r\n\r\nBtw, I just realized that 128GB of DRAM is actually enough for the Q4_K_S quant, which has slightly better quality than IQ4_XS."
      },
      {
        "user": "ThomasBaruzier",
        "created_at": "2024-09-04T06:48:19Z",
        "body": "> Great :)\r\n> \r\n> Btw, I just realized that 128GB of DRAM is actually enough for the Q4_K_S quant, which has slightly better quality than IQ4_XS.\r\n\r\nI still don't understand why the Q4_K_M works very slowly on 128GB RAM + 24GB VRAM as its size is 133GB. There should be around 19GB of free space for context and OS after loading the model, right?"
      },
      {
        "user": "sayap",
        "created_at": "2024-09-05T02:35:45Z",
        "body": "With Q4_K_M, I can see the `kswapd0` process being quite active in `top` output, and there are a lot of disk read activity in the `vmstat 1`  output, e.g.\r\n```\r\n# during prefill\r\nprocs -----------memory---------- ---swap-- -----io---- -system-- -------cpu-------\r\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st gu\r\n 6  3      0 289244  11940 126919688    0    0 629864     0 67690 8384 41  5 49  6  0  0\r\n 9  0      0 327360  11940 126874760    0    0 506028     0 74559 7588 42  3 50  4  0  0\r\n 4  4      0 285072  11940 126927636    0    0 666304     0 80420 11166 40  4 50  6  0  0\r\n 5  4      0 256908  11940 126962388    0    0 1270196     0 111297 20141 29  5 49 17  0  0\r\n 8  0      0 393460  11948 126828420    0    0 1241244    20 119699 20492 29  7 47 17  0  0\r\n 8  1      0 324772  11948 126889052    0    0 757848     0 88407 12399 38  4 51  8  0  0\r\n 6  3      0 446696  11948 126755172    0    0 1162144     0 117587 16711 31  5 49 15  0  0\r\n...\r\n\r\n# during decode\r\nprocs -----------memory---------- ---swap-- -----io---- -system-- -------cpu-------\r\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st gu\r\n10  0      0 309680   1760 126875172    0    0 213236     0 50849 28478 42  4 43 11  0  0\r\n 8  0      0 356932   1756 126838984    0    0 300644     0 60901 21647 42  4 44 11  0  0\r\n 8  1      0 287424   1752 126911680    0    0 204052     0 30458 9555 48  2 44  5  0  0\r\n10  0      0 284936   1748 126915004    0    0 143544     0 27526 7944 49  2 45  4  0  0\r\n 9  0      0 415712   1744 126793764    0    0 431616     0 78055 12728 43  4 44 10  0  0\r\n 9  0      0 384060   1740 126790948    0    0 276432     0 47443 9688 46  3 45  7  0  0\r\n 2  7      0 314216   1736 126873164    0    0 292808     0 38419 9919 46  3 45  7  0  0\r\n...\r\n```\r\nI guess 128GB RAM is just not enough to hold the weights."
      }
    ]
  },
  {
    "number": 69,
    "title": "Missing pip packages flash_attn and wheel",
    "created_at": "2024-08-31T16:58:09Z",
    "closed_at": "2024-09-03T17:31:05Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/69",
    "body": "Might want to add those two packages to requirements file.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/69/comments",
    "author": "bitbottrap",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T03:17:24Z",
        "body": "Flash-attn is an optional package, and not all models require it. Additionally, if a precompiled package for flash-attn is not available, it needs to be compiled and installed, which can be very time-consuming. Therefore, flash-attn has not been added to the requirements.\r\n\r\nWe apologize for the omission of wheel package. We will include it in the next release. We use environments created with conda, which include wheel by default, so we did not encounter this issue."
      },
      {
        "user": "bitbottrap",
        "created_at": "2024-09-03T17:31:05Z",
        "body": "Sound reasoning. Might want to document the possibility of needing flash_attn. Great project!"
      }
    ]
  },
  {
    "number": 68,
    "title": "What is the maximum input token size supported for DeepSeek V2?",
    "created_at": "2024-08-31T02:55:50Z",
    "closed_at": "2024-09-11T06:36:50Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/68",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/68/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-03T08:55:49Z",
        "body": "For now it should be limited by the ability of the model it self and your VRAM size. "
      }
    ]
  },
  {
    "number": 54,
    "title": "Add a instruction for configuring CUDA_HOME and CUDA_PATH to the install section of README.md.",
    "created_at": "2024-08-27T08:51:46Z",
    "closed_at": "2024-08-28T07:05:12Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/54",
    "body": "I tried to compile kTransformers based on the source code, but the compilation failed.\r\n\r\n```\r\nninja: error: '/lib64/libcudart.so', needed by '/home/xxx/projects/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so', missing and no known rule to make it\r\n```\r\nThis error is caused by line 226 in ktransformers/ktransforms_ext/CMakeLists.txt, which links libcudart.so from CUDA_HOME by default. However, in my environment, I configured CUDA through PATH and LD_LIBRRY_PATH. \r\n\r\n```cmake\r\npybind11_add_module(${PROJECT_NAME} MODULE ${ALL_SOURCES})\r\ntarget_link_libraries(${PROJECT_NAME} PRIVATE llama)\r\nif(WIN32)\r\n    target_link_libraries(${PROJECT_NAME} PRIVATE \"$ENV{CUDA_PATH}/lib/x64/cudart.lib\")\r\nelseif(UNIX)\r\n    target_link_libraries(${PROJECT_NAME} PRIVATE \"$ENV{CUDA_HOME}/lib64/libcudart.so\") # this line cause ERROR\r\nendif()\r\n```\r\n\r\nAfter setting CUDA_HOME, I successfully compiled the project. Therefore, the configuration instructions for CUDA_HOME are preferred to be added to README.md.\r\n\r\nAnd, can I propose a PR to add these configuration instructions?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/54/comments",
    "author": "hyx1999",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-08-28T01:51:45Z",
        "body": "Thank you very much. We welcome PR to improve our project. about the README here, could you please set the reminder in the README to the \"Quick Start\" section under \"Install CUDA\"? \r\nAt the same time, We will also improve our CMakeLists file."
      },
      {
        "user": "hyx1999",
        "created_at": "2024-08-28T04:24:23Z",
        "body": "Thank you for your reply! I have submitted a PR request to add a reminder to set CUDA_HOME and CUDA_PATH. \r\n"
      },
      {
        "user": "GavinZhang0228",
        "created_at": "2025-02-18T19:24:13Z",
        "body": "ç¢°åˆ°äº†ä¸€äº›ç±»ä¼¼çš„é—®é¢˜ï¼Œåœ¨å®‰è£…ä½¿ç”¨install.sh çš„æ—¶å€™ å› ä¸ºCUDA_HOMEçš„å­˜åœ¨ å¯¼è‡´CUDA_PATHå’ŒPATHä¸ç”Ÿæ•ˆäº†ï¼Œå› ä¸ºæˆ‘çš„CUDA_HOMEæŒ‡å‘äº†ä¸€ä¸ªå·²ç»å¸è½½çš„è€ç‰ˆæœ¬CUDAï¼Œäºæ˜¯æŠ¥é”™No such file or directory ï¼ˆä½†æ˜¯å…¶å®æˆ‘å¥½åƒå·²ç»åˆ é™¤äº†è¿™ä¸ªç¯å¢ƒå˜é‡  ä½†æ˜¯ä¸çŸ¥é“ä¸ºä»€ä¹ˆè¿˜ç”Ÿæ•ˆï¼‰åœ¨é‡æ–°å†™äº†ä¸€ä¸ªCUDA_HOMEçš„å˜é‡è¦†ç›–è·¯å¾„ä¹‹åè§£å†³äº†"
      }
    ]
  },
  {
    "number": 53,
    "title": "Support for Mistral-Large-Instruct-2407-GGUF ï¼Ÿ",
    "created_at": "2024-08-23T07:29:06Z",
    "closed_at": "2024-08-29T09:16:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/53",
    "body": "Q2ã€Q3ã€Q4ã€Q8ã€BF16ã€FP16 ï¼Ÿ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/53/comments",
    "author": "LIUKAI0815",
    "comments": [
      {
        "user": "BITcyman",
        "created_at": "2024-08-23T10:28:07Z",
        "body": "Currently, ktransformers mainly focus on speeding up MoE models. So we support Mixtral-8x7b and Mixtral-8x22b in Release v0.1.2. But Mistral-Large-Instruct is a dense model, which we might not support in the near future."
      },
      {
        "user": "LIUKAI0815",
        "created_at": "2024-08-28T03:21:30Z",
        "body": "@BITcyman Thanks for replying and looking forward to the new release."
      }
    ]
  },
  {
    "number": 50,
    "title": "How to properly disable offloading MoE layers to CPU?",
    "created_at": "2024-08-21T17:18:35Z",
    "closed_at": "2024-08-26T03:12:23Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/50",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/50/comments",
    "author": "molamooo",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-08-22T02:30:28Z",
        "body": "Regarding your question on disabling the offloading of MoE layers to the CPU, Iâ€™ve provided a brief explanation in my earlier response (#46 ). Please take a look there for detailed instructions."
      },
      {
        "user": "molamooo",
        "created_at": "2024-08-22T04:27:26Z",
        "body": "I tried using custom rule file to put all layers on GPU, but KTransformer triggers oom when dequantizing parameters. I use a 24GB 4090 to run deepseek-v2-lite-chat with q8_0 quantization. It should only takes ~16GB memory. Other systems like llama.cpp can run without issue."
      },
      {
        "user": "ELigoP",
        "created_at": "2024-08-22T05:42:07Z",
        "body": "This is hardly possible.\r\nWhat is your model size? With q8 quantization it should be >200 GB. If you\r\n\r\n> put all layers on GPU\r\n\r\nno wonder it OOMs. Llama.cpp automatically offloads layers in to CPU suboptimal way. If you force it to offload everything to GPU it will also OOM.\r\n\r\nE.g. with my setup I get 7.5 tps with suboptimal llama.cpp CPU offload and 9 tps with ktransformers (even that it utilizies VRAM suboptimally)."
      },
      {
        "user": "molamooo",
        "created_at": "2024-08-22T05:43:37Z",
        "body": "> This is hardly possible. What is your model size? With q8 quantization it should be >200 GB.\r\n\r\nI mentioned that I use deepseek-v2-lite-chat, which is a 16B model. It shouldn't oom."
      },
      {
        "user": "Azure-Tang",
        "created_at": "2024-08-22T07:07:29Z",
        "body": "I see, if you are using KExpertsTorch as backend, it will dequant the weights to model's default dtype, for deepseek this is bf16. So this is why it oom. Maybe you can try marlin backend? But we didn't optimise this operator in generate phase."
      }
    ]
  },
  {
    "number": 47,
    "title": "Can I run llama3.1 70b with rtx4090+64g ddr5 ram?",
    "created_at": "2024-08-20T23:40:52Z",
    "closed_at": "2025-02-10T09:26:31Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/47",
    "body": "Can I run llama3.1 70b with rtx4090+64g ddr5 ram? \r\n\r\nAt what rate per second are tokens generated?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/47/comments",
    "author": "codeMonkey-shin",
    "comments": [
      {
        "user": "ELigoP",
        "created_at": "2024-08-21T08:00:48Z",
        "body": "This is not an issue, it is general question.\r\nAs far as I know, ktransformers speeds up only MoE models, and llama3.1 70b is dense model (no routing/expert layers).\r\nThe answer to your question depends on your model quantization level, motherboard, RAM used and CPU."
      },
      {
        "user": "Atream",
        "created_at": "2025-02-10T09:26:31Z",
        "body": "KTransformers does not offer as significant a performance improvement over llama.cpp on dense model as MOE models do, but it still performs well. You can try writing a YAML file yourself to place the most computationally intensive tasks on the GPU, while the remaining tasks can be completed on the CPU. As long as the DRAM can fully accommodate your GGUF file, KTransformers will be able to run."
      }
    ]
  },
  {
    "number": 45,
    "title": "Cannot run DeepSeek V2 Chat in server mode on 2 GPUs",
    "created_at": "2024-08-20T16:20:09Z",
    "closed_at": "2024-08-22T08:32:49Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/45",
    "body": "I have 2x RTX 3090. I've built `ktransformers` from source (needed to install some missing packages, but it worked in the end).\r\n\r\nWhen I run `local_chat` with\r\n\r\n```\r\npython -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ~/.cache/lm-studio/models/bartowski/DeepSeek-V2-Chat-0628-GGUF --optimize_rule_path ./build/lib.linux-x86_64-cpython-312/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml\r\n```\r\n\r\nin-console chat works, I get 9 tps, more than llama.cpp out-of-the-box 7.5 tps.\r\n\r\nI didn't find a way to pass multi-gpu optimization rule to `ktransformers` server, as `--optimize_config_path OPTIMIZE_CONFIG_PATH` option seems to be not about that. So I just replaced default file with `multi-gpu` version. The I run\r\n\r\n```\r\nktransformers --model_path deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ~/.cache/lm-studio/models/bartowski/DeepSeek-V2-Chat-0628-GGUF --port 1234\r\n```\r\n\r\nI get this error (in the end it says `Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!`).\r\n\r\n**Should I also change `ktransformers/configs/config.yaml` ?** How? Is the problem that only `cuda:0` is mentioned in config? But right when the server starts it seems to read optimize rule right - it split 0-29, 30-59 layers bettween GPUs:\r\n\r\n```\r\n...\r\nloading blk.29.attn_norm.weight to cuda:0\r\nloading blk.29.ffn_norm.weight to cuda:0\r\nloading blk.30.attn_q_a.weight to cuda:1\r\nloading blk.30.attn_q_a_norm.weight to cuda:1\r\n...\r\n```\r\n\r\nThe error:\r\n\r\n```\r\nINFO:     192.168.27.159:46148 - \"OPTIONS /v1/chat/completions HTTP/1.1\" 200 OK\r\n2024-08-20 19:03:00,415 WARNING /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[161]: change system to user\r\n2024-08-20 19:03:00,415 WARNING /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[167]: merge two adjacent user messages\r\n2024-08-20 19:03:00,461 DEBUG /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[185]: get input ids of shape torch.Size([1, 18])\r\n2024-08-20 19:03:00,461 DEBUG /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[240]: input_ids: torch.Size([1, 18])\r\n2024-08-20 19:03:00,462 DEBUG /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[262]: cache position: 0 to 18\r\nINFO:     192.168.27.159:46148 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 148, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 613, in forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1238, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n                                                          ^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/operators/attention.py\", line 163, in forward\r\n    return self.forward_chunck(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/operators/attention.py\", line 92, in forward_chunck\r\n    k_pe, compressed_kv = past_key_value.update(k_pe, compressed_kv, self.layer_idx, cache_kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/models/custom_cache.py\", line 105, in update\r\n    k_out[:, :, cache_position] = key_states\r\n    ~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/45/comments",
    "author": "ELigoP",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-08-21T06:31:09Z",
        "body": "Sorry for the inconvenience orz.\r\n1.  You can appoint optimise yaml rule by `--optimize_config_path` now. I have fixed by #48 . \r\n\r\n2. `Expected all tensors to be on the same device`\r\nOk this is a bug that server creating static cache on one device. Also fixed by #48 . \r\n\r\n"
      }
    ]
  },
  {
    "number": 32,
    "title": "ollama chat not realised",
    "created_at": "2024-08-12T17:14:51Z",
    "closed_at": "2025-02-11T09:25:39Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/32",
    "body": "Error when using api chat with vscode's continue plugin\r\n```\r\n  File \"E:\\open-webui\\backend\\python311\\Lib\\site-packages\\ktransformers\\server\\api\\ollama\\completions.py\", line 96, in chat\r\n    raise NotImplementedError\r\nNotImplementedError\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/32/comments",
    "author": "xldistance",
    "comments": [
      {
        "user": "xldistance",
        "created_at": "2024-08-13T01:25:18Z",
        "body": "When will ollama api chat and autocomplete be supported?"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-15T16:50:44Z",
        "body": "In the past few days, we have been focusing on enhancing support for native Windows, multiple GPUs......, the next version we will support long context and kv cache store.\r\nSo, in the following version, we will support the Ollama api chat."
      }
    ]
  },
  {
    "number": 22,
    "title": "error with ffn_down ",
    "created_at": "2024-08-04T17:38:23Z",
    "closed_at": "2024-08-05T13:18:29Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/22",
    "body": "I try to load ffn_weight and run it with cpuinfer_ext.linear. ffn_up and ffn_gate are good. But ffn_down result in nan. What is wrong\r\ncode is below \r\n```\r\n\r\nfrom ktransformers.util.custom_gguf import GGUFLoader\r\nimport cpuinfer_ext\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport psutil\r\nimport os\r\nimport ctypes\r\ngguf_path = \"/VM/share/models/Mixtral-8x7b-q4k_m/\"\r\ngguf_loader=GGUFLoader(gguf_path)\r\n# print(gguf_loader.tensor_info)\r\ntensor = gguf_loader.get_mmap_tensor(\"blk.0.ffn_down.0.weight\")\r\nprint(tensor)\r\na = torch.tensor(tensor,dtype= torch.uint8)\r\nprint(a.nbytes)\r\nprint(a.shape)\r\ntensor_fp32 = gguf_loader.load_gguf_tensor(\"blk.0.ffn_down.0.weight\").to(torch.bfloat16)\r\nprint(tensor_fp32.shape)\r\ngate_ptr = ctypes.addressof(\r\n            ctypes.cast(tensor.ctypes.data, ctypes.POINTER(ctypes.c_uint64)).contents\r\n        )\r\noutput_size =  4096\r\ninput_size = 14336\r\nstride = 16\r\nproj_type=12\r\nhidden_type=30\r\nCPUInfer = cpuinfer_ext.CPUInfer(32)\r\nconfig = cpuinfer_ext.linear.LinearConfig(input_size, output_size, stride, gate_ptr, proj_type, hidden_type)\r\nlinear = cpuinfer_ext.linear.Linear(config)\r\n\r\ninput = torch.randn((2, input_size), dtype=torch.bfloat16).contiguous()\r\noutput = torch.zeros((2, output_size), dtype=torch.bfloat16).contiguous()\r\nfor i in range(2):\r\n    CPUInfer.submit(linear.forward, input[i,:].data_ptr(), output[i,:].data_ptr())\r\nCPUInfer.sync()\r\nprint(output)\r\nout = F.linear(input,tensor_fp32)\r\nprint(out)\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/22/comments",
    "author": "Eutenacity",
    "comments": [
      {
        "user": "chenht2022",
        "created_at": "2024-08-05T05:13:49Z",
        "body": "I ran your script and got the following results, which seem to be fine.\r\n```\r\n(base) chenht@sapphire2:~/ktransformers-dev$ python test.py \r\n[151   3 227 ...   8 169 135]\r\n33030144\r\ntorch.Size([33030144])\r\ntorch.Size([4096, 14336])\r\ntensor([[ 0.2041, -3.1406, -0.9922,  ..., -1.1406,  0.3828,  0.4023],\r\n        [-1.7188,  0.3164,  0.5352,  ...,  1.5000,  1.5234,  1.0547]],\r\n       dtype=torch.bfloat16)\r\ntensor([[ 0.2070, -3.1562, -0.9883,  ..., -1.1484,  0.3828,  0.3984],\r\n        [-1.7109,  0.3262,  0.5469,  ...,  1.5000,  1.5156,  1.0547]],\r\n       dtype=torch.bfloat16)\r\n```\r\nMaybe you should check if your gguf file is intact and if the ffn_down weights are indeed q4_k quantized.\r\nIf you confirm that these two aspects are fine, please provide more information (e.g., use `lscpu` to show your CPU instruction set) to help us fix it."
      },
      {
        "user": "Eutenacity",
        "created_at": "2024-08-05T13:18:26Z",
        "body": "Many thanks for your quick response. I found the reason. The gguf I download has a different ggml_type down ffn compared to up ffn."
      }
    ]
  },
  {
    "number": 20,
    "title": "If I want to run a linear layer with q4_k_m on cpu using lamafile, how to do it with your implement",
    "created_at": "2024-08-03T12:50:18Z",
    "closed_at": "2024-08-04T06:13:14Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/20",
    "body": "I found the bench_linear.py. But the weight is fp32 not the quantized uint8. Can you give me a example ? thanks",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/20/comments",
    "author": "Eutenacity",
    "comments": [
      {
        "user": "chenht2022",
        "created_at": "2024-08-04T04:40:53Z",
        "body": "You are correct. For testing convenience, we randomly generated some FP32 weights in our bench program and let llamafile_sgemm treat them as quantized weights. The output obtained this way is meaningless; we use this bench program solely to test performance. In actual use, the weights are mmap-ed from gguf rather than randomly generated, ensuring that the computation results are meaningful. \r\n\r\nThe injection of the linear layer is coming soon, and you will then be able to run a linear layer on the CPU using llamafile by modifying a YAML configuration file."
      }
    ]
  },
  {
    "number": 15,
    "title": "GPU support without fp16. Multi gpu support",
    "created_at": "2024-07-29T12:14:28Z",
    "closed_at": "2024-07-29T13:46:27Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/15",
    "body": "Hello, I am very excited about this project. I wanted to ask if ktransformers supports or will support video cards that do not work with fp16? Is there multi gpu support like llama cpp?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/15/comments",
    "author": "AlexBefest",
    "comments": [
      {
        "user": "james0zan",
        "created_at": "2024-07-29T12:31:28Z",
        "body": "We are working on multi-gpu suppport. But the support for non-fp16 card may not come soon, because we do not have such devices for experiemnting and testing."
      }
    ]
  },
  {
    "number": 13,
    "title": "About 1M ctx models",
    "created_at": "2024-07-29T09:33:53Z",
    "closed_at": "2024-08-29T18:23:43Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/13",
    "body": "I saw elsewhere that you mentioned optimization for 1M ctx models (glm-4-9b-chat-1m/internlm2_5-7b-chat-1m) is on the roadmap and will be released in a future version. And I'm interested to know what kind of improvements you're anticipating achieving.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/13/comments",
    "author": "choyakawa",
    "comments": [
      {
        "user": "james0zan",
        "created_at": "2024-07-30T14:04:34Z",
        "body": "We intend to take advantage from the sparsity of attention operator. There are many great work study on this topic and we are evaluating and integrating some of them into the KTransformers framework."
      }
    ]
  },
  {
    "number": 11,
    "title": "Is Flash Attention 2 Necessary for Qwen2Moe?",
    "created_at": "2024-07-29T07:57:18Z",
    "closed_at": "2024-07-29T08:51:56Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/11",
    "body": "I tried to run Qwen2Moe but I only have access to T4 GPUs, which don't support Flash Attention 2.\r\nIs Flash Attention 2 a strict requirement for Qwen2Moe?\r\nAny guidance would be greatly appreciated!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/11/comments",
    "author": "cherrymorning",
    "comments": [
      {
        "user": "Atream",
        "created_at": "2024-07-29T08:51:56Z",
        "body": "Flash Attention 2 is required for Qwen2_moe, or it may overflow. This is Qwen2_moe's own character."
      }
    ]
  },
  {
    "number": 4,
    "title": "Native windows support",
    "created_at": "2024-07-27T10:50:41Z",
    "closed_at": "2024-08-16T07:27:45Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/4",
    "body": "First of all thanks for your AI community contribution, it's a huge leap forward to use MoE models for consumer grade users with limited VRAM.\r\nWould it be possible to add native Windows support to KTransformers? I'd love to see the project become accessible to windows users as well.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/4/comments",
    "author": "DotNetDevlll",
    "comments": [
      {
        "user": "azywait",
        "created_at": "2024-07-27T12:32:49Z",
        "body": "> it be possible to add native Windows support to KTransformers? I'd love to see the project become accessible to windows users as well.\r\n> \r\n> Thanks!\r\n\r\nThanks for your interest. Native Windows support is in our plans, but it may take some time.  ğŸ˜Š"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T03:14:13Z",
        "body": "Has anyone tried to run it under windows?"
      },
      {
        "user": "Atream",
        "created_at": "2024-08-12T03:18:17Z",
        "body": "You can try to install from source by running install.bat. Pre-built wheels will be released soon."
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T14:01:04Z",
        "body": ".\\install.bat \r\n\r\n```\r\nInstalling ktransformers\r\nProcessing c:\\users\\pc\\ktransformers\\ktransformers\\ktransformers\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  Ã— Preparing metadata (pyproject.toml) did not run successfully.\r\n  â”‚ exit code: 1\r\n  â•°â”€> [17 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\r\n          return hook(metadata_directory, config_settings)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\setuptools\\build_meta.py\", line 368, in prepare_metadata_for_build_wheel\r\n          self.run_setup()\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\setuptools\\build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 294, in <module>\r\n        File \"<string>\", line 132, in get_package_version\r\n        File \"<string>\", line 54, in get_cuda_bare_metal_version\r\n      TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\nÃ— Encountered error while generating package metadata.\r\nâ•°â”€> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\nInstallation completed successfully\r\n```"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-12T15:30:08Z",
        "body": "There might be a few possibilities here.\r\n\r\n1. CUDA is not installed on your machine.\r\n2. The CUDA environment variables are not active.\r\n3. The installed PyTorch is not for GPU, but for CPU.\r\n\r\nThere is a simple env check code\r\n```python\r\nimport torch\r\nimport subprocess\r\nfrom torch.utils.cpp_extension import CUDA_HOME\r\nprint(\"torch version is: \" + str(torch.__version__))\r\nprint(\"CUDA HOME is: \" + str(CUDA_HOME))\r\nraw_output = subprocess.check_output([str(CUDA_HOME) + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\r\nprint(\"nvcc version is : \" + raw_output)\r\n```\r\nThe output of my computer is:\r\n\r\n> torch version is: 2.4.0+**cu124**\r\n> CUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v12.5\r\n> nvcc version is : nvcc: NVIDIA (R) Cuda compiler driver\r\n> Copyright (c) 2005-2024 NVIDIA Corporation\r\n> Built on Wed_Apr_17_19:36:51_Pacific_Daylight_Time_2024\r\n> Cuda compilation tools, release 12.5, V12.5.40\r\n> Build cuda_12.5.r12.5/compiler.34177558_0\r\n\r\nCould you show me what the output is on your computer?"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T16:15:41Z",
        "body": "torch version is: 2.4.0+cpu\r\nCUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6\r\nnvcc version is : nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024\r\nCuda compilation tools, release 12.6, V12.6.20\r\nBuild cuda_12.6.r12.6/compiler.34431801_0\r\n\r\nthanks for your quick reply, after fixup some torch things ...\r\n\r\n```\r\npython -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\nERROR: The function received no value for the required argument: model_path\r\nUsage: local_chat.py MODEL_PATH <flags>\r\n  optional flags:        --optimize_rule_path | --gguf_path |\r\n                         --max_new_tokens | --cpu_infer\r\n\r\nFor detailed information on this command, run:\r\n  local_chat.py --help\r\n```"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-12T17:45:41Z",
        "body": "> torch version is: 2.4.0+cpu CUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6 nvcc version is : nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2024 NVIDIA Corporation Built on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024 Cuda compilation tools, release 12.6, V12.6.20 Build cuda_12.6.r12.6/compiler.34431801_0\r\n> \r\n> thanks for your quick reply, after fixup some torch things ...\r\n> \r\n> ```\r\n> python -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\n> ERROR: The function received no value for the required argument: model_path\r\n> Usage: local_chat.py MODEL_PATH <flags>\r\n>   optional flags:        --optimize_rule_path | --gguf_path |\r\n>                          --max_new_tokens | --cpu_infer\r\n> \r\n> For detailed information on this command, run:\r\n>   local_chat.py --help\r\n> ```\r\n\r\nPerhaps you can input --model_path instead of --model_name.\r\n```\r\npython -m ktransformers.local_chat --model_path Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\n```\r\n"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-13T12:54:23Z",
        "body": "Great, it works!\r\ni3900k+msi 4090 + ram 96GB\r\ngpu mem 6.3/24GB\r\nram 35GB maybe\r\n```\r\nChat: æ•°å­—9.11å’Œ9.9è°å¤§ï¼Ÿ \r\næ•°å­—9.11å’Œ9.9ä¸­ï¼Œ9.11æ¯”9.9å°ã€‚è¿™å¯ä»¥é€šè¿‡å°†ä¸¤ä¸ªæ•°å­—è½¬æ¢ä¸ºåˆ†æ•°æ¥æ›´æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œå…¶ä¸­9.11ä¸º911/1000å’Œ9.9ä¸º99/10ã€‚æ¯”è¾ƒä¸¤ä¸ªåˆ†æ•°ï¼Œ911/1000å°äº99/10ï¼Œå› æ­¤9.11å°äº9.9ã€‚\r\nprompt eval count:    31 token(s)\r\nprompt eval duration: 0.757000207901001s\r\nprompt eval rate:     40.95111160663528 tokens/s\r\neval count:           90 token(s)\r\neval duration:        6.651063442230225s\r\neval rate:            13.531670654132467 tokens/s\r\n```"
      }
    ]
  }
]