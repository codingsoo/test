[
  {
    "number": 509,
    "title": "在单张A100复现成功，但是gguf权重加载非常缓慢，有人遇到吗？",
    "created_at": "2025-02-19T12:16:09Z",
    "closed_at": "2025-02-20T04:12:37Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/509",
    "body": "attention一层加载大约要花10S",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/509/comments",
    "author": "jerryrual",
    "comments": [
      {
        "user": "3wweiweiwu",
        "created_at": "2025-02-19T13:48:47Z",
        "body": "我也遇到同样的问题，我的环境是在云上，2XA10，第一次读取要一个小时，之后只要不重启，就快很多，只要2分钟。但是我在用ollama的时候也有类似问题，所以不确定是不是运营商那的限制。"
      },
      {
        "user": "edisonchan",
        "created_at": "2025-02-20T01:30:52Z",
        "body": "检查一下是不是每次都重新下载模型。。"
      },
      {
        "user": "codedoves",
        "created_at": "2025-02-20T01:34:01Z",
        "body": "我也有同样的问题，r1Q3的模型加载完需要30min"
      },
      {
        "user": "3wweiweiwu",
        "created_at": "2025-02-20T02:00:06Z",
        "body": "> 检查一下是不是每次都重新下载模型。。\n\n应该不是重新下载模型，我的gguf都是本地的，而且是在是读gpu的时候特别慢。而且就算是下载模型，一个小时也不够吧。"
      },
      {
        "user": "johnray22",
        "created_at": "2025-02-20T02:14:30Z",
        "body": "模型要从硬盘加载到内存中，如果是SSD就快一些，一般2000M/S。机械盘就慢了，拉满也就200M"
      },
      {
        "user": "johnray22",
        "created_at": "2025-02-20T02:14:45Z",
        "body": "可以设置某个参数将模型常驻内存"
      },
      {
        "user": "jzw02",
        "created_at": "2025-02-20T02:19:29Z",
        "body": "我也遇到了这样的情况，load blk非常慢，并且chat的时候也很慢"
      },
      {
        "user": "codedoves",
        "created_at": "2025-02-20T02:47:45Z",
        "body": "**你用的是sdd还是机械硬盘？我也是单张A100，使用机械也是很慢，后来换成ssd会好很多"
      },
      {
        "user": "neichuanzhang",
        "created_at": "2025-02-20T03:23:56Z",
        "body": "用htop看一下加载模型的进程，如果进程是D状态，看下iostat -x 1看看是不是磁盘的iowait太高了。\n\n如果是一般是磁盘的性能问题"
      },
      {
        "user": "Wilbur0626",
        "created_at": "2025-02-20T03:49:58Z",
        "body": "我这里是磁盘的问题，需要把其他读写操作关掉\n"
      }
    ]
  },
  {
    "number": 503,
    "title": "V0.2.1 官方预填充速度80多咋测出来的？",
    "created_at": "2025-02-19T09:41:39Z",
    "closed_at": "2025-02-20T09:28:11Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/503",
    "body": "测试环境：\n* CPU：Intel(R) Xeon(R) Gold 6430, 29 cores per socket, 2 sockets, 2 numa nodes\n* GPU:  4090 24G VRAM\n* test after enough warm up\n* 944G DRAM\n* System: Ubuntu 22.04.3 LTS \n* Python 3.10.12\n* CUDA：Build cuda_12.3.r12.3 \n* NVIDIA Driver：12.2  -  NVIDIA GeForce RTX 4090 x 8 \n* PyTorch version: 2.6.0+cu124 \n* flash-attn: 2.7.1.post4\n\npython -m ktransformers.local_chat --model_path /workspace/Deepseek-models/DeepSeek-R1 --gguf_path /workspace/Deepseek-models/DeepSeek-R1-Q4_K_M --cpu_infer 65 --max_new_tokens 1000\n测试了不同的 cpu_infer （65、58、56、20），65效果最好\n\n测试结果：\n* Prefill token/s（最高）：13.35\n* Decode token/s（最高）：10.11\n* 内存占用:  903.6G/944G = 95.7%\n* 显存占用：14881MiB / 24564MiB = 14.54G/24G = 60.51%\n\n内存理论带宽是563 GB/s，实测出来的大约352GB/s\n\n官方速度：V0.2.1\n  * Prefill token/s：88.2\n  * Decode token/s：13.5\n\n 预填充速度差距较大，八十多咋测出来的？",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/503/comments",
    "author": "yansiyu550",
    "comments": [
      {
        "user": "dachengai",
        "created_at": "2025-02-20T07:28:44Z",
        "body": "测长 prompt   prefill 速度就上来了 "
      },
      {
        "user": "yansiyu550",
        "created_at": "2025-02-20T08:23:43Z",
        "body": "> 测长 prompt prefill 速度就上来了\n\n好的好的，我试试"
      },
      {
        "user": "seanzhang-zhichen",
        "created_at": "2025-02-20T09:45:20Z",
        "body": "不知道你们是怎么测出来速度这么快的，我发现有不少人，都和我一样，都没有 超过1token/s \n\n不知道问题出在哪里"
      },
      {
        "user": "yansiyu550",
        "created_at": "2025-02-20T09:51:40Z",
        "body": "我测出来Prefill 50.53  token/s，Decode 10.11  token/s，测试机器理论带宽 563 GB/s，实测带宽大概是 352 GB/s，官方提供的机器理论带宽 600 GB/s，我这个结果应该是符合预期的\n\n    * 内存占用：810.5G/944G = 85.9%\n    * 显存占用：15375MiB / 24564MiB = 15.01G/24G = 62.6%"
      }
    ]
  },
  {
    "number": 501,
    "title": "Necessary tips for Node.js related issues",
    "created_at": "2025-02-19T08:38:51Z",
    "closed_at": "2025-02-19T08:46:48Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/501",
    "body": "Necessary tips for Node.js related issues",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/501/comments",
    "author": "Zhoneym",
    "comments": [
      {
        "user": "Atream",
        "created_at": "2025-02-19T08:46:38Z",
        "body": "Thank you for your contribution!"
      }
    ]
  },
  {
    "number": 494,
    "title": "这能在4090跑起来r1 ? 有没有高手真的能跑的",
    "created_at": "2025-02-19T07:54:46Z",
    "closed_at": "2025-02-20T04:15:37Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/494",
    "body": "想问问事deepseek-r1 什么量级的， 确定可以跑吗， 感谢🙏， 如果可以想去搞个机器试试",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/494/comments",
    "author": "652994331",
    "comments": [
      {
        "user": "yansiyu550",
        "created_at": "2025-02-19T09:50:25Z",
        "body": "1T DRAM，14GB VRAM，单卡4090就能跑，是Q4量化版本的，不是满血版，CPU要支持AMX"
      },
      {
        "user": "paranoiagu",
        "created_at": "2025-02-19T23:58:51Z",
        "body": "一点要支持 AMX 吗？"
      },
      {
        "user": "VinayaLee",
        "created_at": "2025-02-20T01:18:21Z",
        "body": "> 一点要支持 AMX 吗？\n\nv0.3.0需要，之前的版本不需要"
      }
    ]
  },
  {
    "number": 481,
    "title": "Modify and add any incorrect or missing content in the `install.md`",
    "created_at": "2025-02-19T02:04:44Z",
    "closed_at": "2025-02-19T03:19:12Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/481",
    "body": "Modify and add any incorrect or missing content in the `install.md`",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/481/comments",
    "author": "Zhoneym",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-19T03:19:47Z",
        "body": "Thank you for your contribution! 🌹"
      }
    ]
  },
  {
    "number": 471,
    "title": "权重文件加载很慢是什么原因？",
    "created_at": "2025-02-18T12:03:04Z",
    "closed_at": "2025-02-18T12:45:29Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/471",
    "body": "CPU型号是Intel(R) Xeon(R) Platinum 8179M CPU @ 2.40GHz \n有一张3090显卡，显存24G\n内存DDR 4 ，2666 MT/s，8根64G=512G\n运行Q4满血版的gguf\n加载权重文件非常慢，几十分钟了还没加载完\nlog显示大概一分多钟才能出现新的blk数字：\n```\nloading blk.48.attn_q_a_norm.weight to cuda:0\nloading blk.48.attn_kv_a_norm.weight to cuda:0\nloading blk.48.attn_kv_b.weight to cuda:0\nloading blk.48.attn_norm.weight to cuda:0\nloading blk.48.ffn_norm.weight to cuda:0\nloading blk.49.attn_q_a_norm.weight to cuda:0\nloading blk.49.attn_kv_a_norm.weight to cuda:0\nloading blk.49.attn_kv_b.weight to cuda:0\nloading blk.49.attn_norm.weight to cuda:0\nloading blk.49.ffn_norm.weight to cuda:0\nloading blk.50.attn_q_a_norm.weight to cuda:0\nloading blk.50.attn_kv_a_norm.weight to cuda:0\nloading blk.50.attn_kv_b.weight to cuda:0\n```\n加载过程中进程显示内存占用70%\n```    \n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n2577178 being     20   0  419.5g 363.2g 358.8g R  2257  72.1 417:19.23 python\n```\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/471/comments",
    "author": "burugo",
    "comments": [
      {
        "user": "burugo",
        "created_at": "2025-02-18T12:45:29Z",
        "body": "大概知道原因了，文件放在机械硬盘上"
      },
      {
        "user": "jzw02",
        "created_at": "2025-02-19T08:56:10Z",
        "body": "我也是很慢，有解决办法吗"
      }
    ]
  },
  {
    "number": 457,
    "title": "调用OpenAI ChatCompletion API输入超过一个单词后就会出现Segmentation fault问题",
    "created_at": "2025-02-18T08:07:24Z",
    "closed_at": "2025-02-19T07:21:21Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/457",
    "body": "2025-02-19 00:16:02,186 DEBUG /home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/transformers.py[193]: get input ids of shape torch.Size([1, 102])\n<think>\n2025-02-19 00:16:02,186 DEBUG /home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py[125]: input_ids: torch.Size([1, 104])\n2025-02-19 00:16:02,187 DEBUG /home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py[151]: cache position: 0 to 104\nFatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation faultFatal Python error: \n\nFatal Python error: Segmentation faultFatal Python error: \n\nFatal Python error: Fatal Python error: Thread 0xFatal Python error: Segmentation faultFatal Python error: Segmentation fault\n\nFatal Python error: Fatal Python error: Fatal Python error: Segmentation faultFatal Python error: Fatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation fault00007f740b100200Segmentation faultFatal Python error: \n\nFatal Python error: Segmentation fault\n\nFatal Python error: Fatal Python error: Fatal Python error: Fatal Python error: Segmentation faultSegmentation faultSegmentation fault\n\nSegmentation faultSegmentation faultFatal Python error: Segmentation faultFatal Python error: Fatal Python error: Segmentation faultFatal Python error: \n\n (most recent call first):\n\n\n\n\nSegmentation faultSegmentation fault\n\nSegmentation faultSegmentation faultSegmentation fault\n\nSegmentation fault\n\n有什么解决办法吗？\n\n\n\n\n\n\nSegmentation fault\n\nSegmentation fault\n\nSegmentation faultSegmentation fault  File \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/cpuinfer.py\", line 740 in sync\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 222 in forward\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 579 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 849 in moe_kexperts\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/experts.py\", line 828 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/models/modeling_deepseek_v3.py\", line 1220 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/operators/models.py\", line 722 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/models/modeling_deepseek_v3.py\", line 1688 in forward\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562 in _call_impl\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553 in _wrapped_call_impl\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py\", line 160 in prefill\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36 in generator_context\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/transformers.py\", line 340 in inference\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/backend/interfaces/ktransformers.py\", line 181 in inference\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/api/openai/endpoints/chat.py\", line 40 in chat_completion\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 210 in run_endpoint_function\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 297 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 73 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 51 in wrapped_app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 76 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 288 in handle\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 735 in app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 715 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 51 in wrapped_app\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/applications.py\", line 113 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/applications.py\", line 1054 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70 in __call__\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406 in run_asgi\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/events.py\", line 84 in _run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/base_events.py\", line 1936 in _run_once\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/base_events.py\", line 608 in run_forever\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/base_events.py\", line 641 in run_until_complete\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/runners.py\", line 118 in run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/asyncio/runners.py\", line 190 in run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/server.py\", line 65 in run\n  File \"/home/xuxilie/anaconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/main.py\", line 577 in run\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/main.py\", line 91 in run_api\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/main.py\", line 118 in main\n  File \"/home/xuxilie/ktransformers-0.2/./ktransformers/server/main.py\", line 128 in <module>\nSegmentation fault (core dumped)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/457/comments",
    "author": "GodXuxilie",
    "comments": [
      {
        "user": "GodXuxilie",
        "created_at": "2025-02-19T07:21:21Z",
        "body": "已经解决，应该是没有配置好的问题。重新pip install wheel后，就可以正常使用吗。"
      },
      {
        "user": "jinec",
        "created_at": "2025-02-20T09:34:36Z",
        "body": "@GodXuxilie 重新安装哪个轮子呢？老哥"
      }
    ]
  },
  {
    "number": 448,
    "title": "请教一下，服务器上两张3090，251G内存，能否r1的量化满血版",
    "created_at": "2025-02-18T05:42:58Z",
    "closed_at": "2025-02-20T05:03:46Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/448",
    "body": "我有一台服务器，40核CPU，有251G内存，18T存储，现在想通过docker容器测试一下r1的满血版，看看这个资源能不能拉起这个服务。",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/448/comments",
    "author": "johnhillcn",
    "comments": [
      {
        "user": "xialang2012",
        "created_at": "2025-02-18T06:00:02Z",
        "body": "不能"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T06:22:05Z",
        "body": "可以试一下q2kxl格式的unsloth"
      }
    ]
  },
  {
    "number": 447,
    "title": "有支持ARM架构的计划么？",
    "created_at": "2025-02-18T04:40:55Z",
    "closed_at": "2025-02-18T10:48:41Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/447",
    "body": "有支持arm架构的计划么？大概什么时间呢？",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/447/comments",
    "author": "hexinlei",
    "comments": [
      {
        "user": "xialang2012",
        "created_at": "2025-02-18T06:00:30Z",
        "body": "没有，目前搞不定"
      },
      {
        "user": "hexinlei",
        "created_at": "2025-02-18T10:48:41Z",
        "body": "好的，感谢回复。"
      }
    ]
  },
  {
    "number": 443,
    "title": "不支持AMX的CPU也能运行吗",
    "created_at": "2025-02-18T03:38:29Z",
    "closed_at": "2025-02-18T06:25:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/443",
    "body": "有没有大佬使用不支持AMX的CPU成功运行了KT",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/443/comments",
    "author": "zhudongwork",
    "comments": [
      {
        "user": "ningpengtao-coder",
        "created_at": "2025-02-18T03:46:15Z",
        "body": "发布页有发布avx2和avx512包，直接下就行。12代酷睿后intel禁用了avx512只支持avx2了。"
      },
      {
        "user": "xialang2012",
        "created_at": "2025-02-18T06:01:12Z",
        "body": "AMX支持与否不影响，只是速度上不去"
      }
    ]
  },
  {
    "number": 432,
    "title": "为啥用两颗cpu只有用一颗cpu 十分之一的速度",
    "created_at": "2025-02-18T01:53:52Z",
    "closed_at": "2025-02-18T06:51:42Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/432",
    "body": "# env\nIntel(R) Xeon(R) Gold 6462C x2\n1024GB RAM(Hynix 64GB DDR5-4800 HMCG94MEBRA123N)\nNVIDIA L20 48GB x8\nUbuntu 22.04.5 LTS\nktransformers v0.2.1\n\n\n# single cpu\npython ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/ --cpu_infer 65 --max_new_tokens 1000\n## resource usage\n13986 MB VRAM\n390483 MB RAM\nCPU 6554%\nsingle gpu 10%\n## rate\nprompt eval count:    6 token(s)\nprompt eval duration: 1.7646489143371582s\nprompt eval rate:     3.4001097619203957 tokens/s\neval count:           408 token(s)\neval duration:        77.80391049385071s\neval rate:            5.24395235933863 tokens/s\n\n# rebuild with export USE_NUMA=1 to use two cpu\n\npython ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/ --cpu_infer 126 --max_new_tokens 1000\n## resource usage\n13986 MB VRAM\n904183 MB RAM\nCPU 10554%\nsingle gpu 0%\n## rate\nprompt eval count:    6 token(s)\nprompt eval duration: 12.765036821365356s\nprompt eval rate:     0.4700338968045559 tokens/s\neval count:           332 token(s)\neval duration:        640.3727271556854s\neval rate:            0.5184480630126604 tokens/s\n\n两颗cpu只有用一颗cpu 十分之一的速度",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/432/comments",
    "author": "yourchanges",
    "comments": [
      {
        "user": "chk4991",
        "created_at": "2025-02-18T06:18:14Z",
        "body": "用了一颗cpu速度很慢，基本1token要10秒，你解决了吗"
      },
      {
        "user": "yourchanges",
        "created_at": "2025-02-18T06:32:13Z",
        "body": "cpu infer =》65 从5token每秒到13token每秒了\n\n# rebuild with export USE_NUMA=1 to use two sockets\npython ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M/ --cpu_infer 65 --max_new_tokens 1000\n## resource usage\n13986 MB VRAM\n984183 MB RAM\nCPU 6554%\nsingle gpu 42%\n## rate\nprompt eval count:    10 token(s)\nprompt eval duration: 0.4073047637939453s\nprompt eval rate:     24.551640169519306 tokens/s\neval count:           1000 token(s)\neval duration:        72.73280572891235s\neval rate:            13.748953996456175 tokens/s"
      },
      {
        "user": "yydslc",
        "created_at": "2025-02-18T06:35:36Z",
        "body": "大佬，你的kt版本是v0.2.1还是v0.3.0?"
      },
      {
        "user": "yourchanges",
        "created_at": "2025-02-18T06:38:19Z",
        "body": "> 大佬，你的kt版本是v0.2.1还是v0.3.0?\n\n上面环境写了 编译的master 是0.2.1"
      }
    ]
  },
  {
    "number": 414,
    "title": "AttributeError: 'KDeepseekV2Attention' object has no attribute 'q_lora_rank'",
    "created_at": "2025-02-17T09:50:22Z",
    "closed_at": "2025-02-17T10:13:17Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/414",
    "body": "配置:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n模型:unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF\n执行的指令python3 -m ktransformers.local_chat ./DeepSeek-R1-Distill-Qwen-32B --gguf_path ./DeepSeek-R1-Distill-Qwen-32B-GGUF --optimize_rule_path ./DeepSeek-V3-Chat.yaml \n进入聊天界面发完消息报该错误",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/414/comments",
    "author": "AndrewBoom",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T10:13:12Z",
        "body": "我们没有测试过qwen32b的模型，请查看我们的安装指南里支持的模型“supported models”"
      }
    ]
  },
  {
    "number": 411,
    "title": "能出一个 纯CPU 加 大内存，无显卡的 可以运行的教程么。",
    "created_at": "2025-02-17T08:56:29Z",
    "closed_at": "2025-02-17T10:15:42Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/411",
    "body": "能出一个 纯CPU 加 大内存，无显卡的 可以运行的教程么。",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/411/comments",
    "author": "clywm520",
    "comments": [
      {
        "user": "clywm520",
        "created_at": "2025-02-17T09:09:12Z",
        "body": "或者出一教程，使用 16G内存能运行  deepseek-r1:70b，或  deepseek-r1:32b 的教程哈。只要能提升token速度就好了。最好是docker运行"
      },
      {
        "user": "kissint8",
        "created_at": "2025-02-17T09:39:20Z",
        "body": "纯CPU可在issue区找找看。\n\ndeepseek-r1 70b和32b实质上并不是deepseek-moe模型，是llama和qwen模型的结构，我之前的测试中llama70b存在运行问题，尚未得知是否已修复，qwen未测试。\n\n目前deepseek，推荐用671b的多种量化版本。\n\n此外，Ktransformers对于减少模型内存占用效果显著，但纯CPU加速方面如果有获得显著加速效果欢迎贴出。"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T10:14:39Z",
        "body": "纯cpu推理模型并不享受ktransformers带来的各种加速，考虑到框架的易用程度，如果想纯cpu推理请考虑ollama或者llamacpp～"
      }
    ]
  },
  {
    "number": 406,
    "title": "Fix cmake error caused by lack of environment variables in Windows environment",
    "created_at": "2025-02-17T07:59:03Z",
    "closed_at": "2025-02-18T12:10:00Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/406",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/406/comments",
    "author": "hoshinohikari",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T07:44:36Z",
        "body": "Hi, thanks for your contribution to KTransformers. Can you further explain what type of error this pr would solve?"
      },
      {
        "user": "hoshinohikari",
        "created_at": "2025-02-18T08:02:17Z",
        "body": "Of course. when compiling the project using the Developer Command for Visual Studio, the cmake generator used is ninja, but the environment variable is empty. If the cmake platform parameter is added, it will prompt that ninja does not support this parameter."
      }
    ]
  },
  {
    "number": 401,
    "title": "feat: refactor local_chat to openai server mode",
    "created_at": "2025-02-17T06:41:29Z",
    "closed_at": "2025-02-18T13:14:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/401",
    "body": "together with A9 in wechat-group",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/401/comments",
    "author": "zhaoyukoon",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T12:13:33Z",
        "body": "Local chat is a lightweight tool that we offer for experimentation, and we aim to keep it simple and easy to use. If you need access to the OpenAI API, we recommend using the server instead."
      },
      {
        "user": "zhaoyukoon",
        "created_at": "2025-02-18T13:04:49Z",
        "body": "i will commit to another `server.py`"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-18T13:13:39Z",
        "body": "> i will commit to another `server.py`\r\n\r\nThank you for your understanding! Looking forward to seeing your new PR."
      }
    ]
  },
  {
    "number": 378,
    "title": "Does it supports IQ1_S and Q2_K_XS?",
    "created_at": "2025-02-16T15:31:03Z",
    "closed_at": "2025-02-17T10:26:48Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/378",
    "body": "支持IQ1_S和Q2_K_XS这两种量化格式吗？能做到内存显存同时用吗？如果支持这些量化格式那会很有发展前景",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/378/comments",
    "author": "exumeOAO",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T10:26:18Z",
        "body": "We did not support iq_x fomat, except iq4; Q2_K_XS is supported; i didn't test Q2_K_XS but i saw someone say it is runnable"
      }
    ]
  },
  {
    "number": 362,
    "title": "Warning: orig_module is not set, but has in_features or out_features equals to 1, can't get in_features and out_features from GGUF",
    "created_at": "2025-02-16T08:14:24Z",
    "closed_at": "2025-02-16T10:26:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/362",
    "body": "使用ktransformers --model_path Qwen/Qwen2-57B-A14B-Instruct --gguf_path E:\\models\\Qwen\\qwen2-57b-a14b-instruct-q3_k_m  --port 12345 --web True命令启动模型报错如下：\nWarning: orig_module is not set, but has in_features or out_features equals to 1, can't get in_features and out_features from GGUF\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"E:\\anaconda3\\envs\\deepseek\\Scripts\\ktransformers.exe\\__main__.py\", line 7, in <module>\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\server\\main.py\", line 114, in main\n    create_interface(config=cfg, default_args=cfg)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\server\\utils\\create_interface.py\", line 27, in create_interface\n    GlobalInterface.interface = BackendInterface(default_args)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\server\\backend\\interfaces\\ktransformers.py\", line 47, in __init__\n    optimize_and_load_gguf(self.model, optimize_rule_path, gguf_path, config)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 128, in optimize_and_load_gguf\n    inject(module, optimize_config, model_config, gguf_loader)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 42, in inject\n    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 42, in inject\n    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 42, in inject\n    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n  [Previous line repeated 1 more time]\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\optimize\\optimize.py\", line 33, in inject\n    inject_module=module_cls(key = inject_module_meta[\"key\"], gguf_loader = gguf_loader, config = model_config, orig_module=child, **inject_module_meta[\"kwargs\"])\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\operators\\linear.py\", line 378, in __init__\n    KLinearBase.__init__(self, key, gguf_loader, config, orig_module, generate_device, **kwargs)\n  File \"E:\\anaconda3\\envs\\deepseek\\Lib\\site-packages\\ktransformers\\operators\\linear.py\", line 65, in __init__\n    self.out_features = self.gguf_loader.tensor_info[key + \".weight\"][\"shape\"][1]\n                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/362/comments",
    "author": "zerozhengsi",
    "comments": [
      {
        "user": "zerozhengsi",
        "created_at": "2025-02-16T08:21:11Z",
        "body": "我是windwos10系统，配置32G内存，16G显存4060 TI显卡，运行的模型是qwen2-57b-a14b-instruct-q3_k_m"
      },
      {
        "user": "zerozhengsi",
        "created_at": "2025-02-16T10:25:38Z",
        "body": "修改代码self.out_features = self.gguf_loader.tensor_info[key + \".weight\"][\"shape\"][1]为self.out_features = self.gguf_loader.tensor_info[key + \".weight\"][\"shape\"][-1]启动成功"
      },
      {
        "user": "Zha-Miku",
        "created_at": "2025-02-17T09:28:35Z",
        "body": "最后的效果咋样啊"
      }
    ]
  },
  {
    "number": 351,
    "title": "这个框架能支持多少并发？",
    "created_at": "2025-02-16T03:26:11Z",
    "closed_at": "2025-02-16T06:12:55Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/351",
    "body": "根据v0.3的配置，能支持多少并发推理呀？",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/351/comments",
    "author": "edoserbia",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-16T06:12:55Z",
        "body": "目前不支持并发，只能单batch"
      },
      {
        "user": "zengqingfu1442",
        "created_at": "2025-02-17T08:45:48Z",
        "body": "m"
      }
    ]
  },
  {
    "number": 344,
    "title": "Infinite Loop when CPU Infer is Low",
    "created_at": "2025-02-16T00:26:20Z",
    "closed_at": "2025-02-16T06:12:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/344",
    "body": "I encountered infinite loop issue today, in both v0.2 and v0.2.1.\n\n## Hardware Spec\n**NUMA Cores**: 4\n**CPU**: AMD EPYC 74F3 24-Core Processor\n**Socket**: 2\n**Cores per socket**: 18\n**Threads per Core**: 2\n\n## Software Env\n**KTransformers Version**: V0.2.0\n**Model**: DeepSeek-R1-Q4_K_M & DeepSeek-R1-UD-Q2_K_XL\n\n## Problem Statement\nDepends on the cpu_infer parameter, the DeepSeek Model will behave differently. When value is low, it falls into infinite loop. As we increase cpu infer count, the problem is resolved.\n\nFollowing table is measured against Q2 model, but similar behavior is observed in Q4_K_M\n\n| CPU Infer | Observation    | Token/s |\n| --------- | -------------- | ------- |\n| 18        | Infintely loop4/5 times | 6.857   |\n| 38        | OK             | 6.85    |\n| 50        | OK             | 6.72    |\n| 65        | OK             | 6.1     |\n\n\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/344/comments",
    "author": "3wweiweiwu",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-16T06:12:25Z",
        "body": "It seems due to triton mla kernel. #354 "
      }
    ]
  },
  {
    "number": 338,
    "title": "qwen2_5_vl 不支持",
    "created_at": "2025-02-15T16:52:55Z",
    "closed_at": "2025-02-16T06:21:27Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/338",
    "body": "- 系统信息：Ubuntu24.04\n- 显卡：3080\n- cuda: 12.4\n错误信息：\n```shell\nktransformers --model_path Qwen/Qwen2.5-VL-7B-Instruct --gguf_path /data/work/huggingface/Qwen/Qwen2.5-VL-7B-Instruct --port 10002 --web True\n2025-02-16 00:44:49,940 INFO /home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/main.py[29]: Creating SQL tables\n2025-02-16 00:44:49,942 INFO /home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/api/openai/assistants/assistants.py[75]: Creating default assistant\nTraceback (most recent call last):\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 989, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 691, in getitem\nraise KeyError(key)\nKeyError: 'qwen2_5_vl'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/bin/ktransformers\", line 8, in <module>\nsys.exit(main())\n^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/main.py\", line 114, in main\ncreate_interface(config=cfg, default_args=cfg)\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/utils/create_interface.py\", line 27, in create_interface\nGlobalInterface.interface = BackendInterface(default_args)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 29, in init\nconfig = AutoConfig.from_pretrained(args.model_dir, trust_remote_code=args.trust_remote_code)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/zhaofa/miniconda3/envs/ktransformers/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 991, in from_pretrained\nraise ValueError(\nValueError: The checkpoint you are trying to load has model type qwen2_5_vl but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date. ```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/338/comments",
    "author": "javaDer",
    "comments": [
      {
        "user": "KMSorSMS",
        "created_at": "2025-02-16T06:21:27Z",
        "body": "> Qwen2.5-VL-7B-Instruct\n\n目前暂不支持"
      }
    ]
  },
  {
    "number": 334,
    "title": "stable-diffusion-webui 能不能用這個??",
    "created_at": "2025-02-15T15:35:56Z",
    "closed_at": "2025-02-15T16:09:49Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/334",
    "body": "想問一下,如果可以用那麼sdxl 就能更快畫圖了!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/334/comments",
    "author": "upright2003",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-15T16:09:49Z",
        "body": "不能～"
      }
    ]
  },
  {
    "number": 324,
    "title": "Update DeepseekR1_V3_tutorial.md",
    "created_at": "2025-02-15T11:43:51Z",
    "closed_at": "2025-02-15T14:23:09Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/324",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/324/comments",
    "author": "BiFangKNT",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-15T14:26:32Z",
        "body": "感谢您的修正！💐"
      }
    ]
  },
  {
    "number": 321,
    "title": "NUMA模式不能工作",
    "created_at": "2025-02-15T10:06:40Z",
    "closed_at": "2025-02-15T16:26:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/321",
    "body": "系统配置\n  CPU: EPYC 7742 x2\n  DRAM: 64GB x 16\n  GPU: 3090 x 8\n  使用的模型: Ollama那边拉下来的8bit量化的gguf\n\n如果不使用NUMA模式, install.sh安装, 使用以下命令启动\npython -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V3 --cpu_infer 64 --max_new_tokens 1000 --gguf_path /home/adonis/temp/ds_v3\n\n可以正常工作, 用附加的test文件里的信息, 要求进行总结, 性能数据如下, 另外在这个时候cpu那边的内存占用只有19G\n\nprompt eval count:    966 token(s)\nprompt eval duration: 17.906471252441406s\nprompt eval rate:     53.9469774017197 tokens/s\neval count:           1000 token(s)\neval duration:        222.02748823165894s\neval rate:            4.5039468219206285 tokens/s\n\n如果使用NUMA模式, 也就是export NUMA=1; install.sh安装, 使用上述命令启动\n程序在输出一系列加载信息, 直到以下信息\n\nloading blk.46.ffn_norm.weight to cuda:0\nloading blk.47.attn_q_a_norm.weight to cuda:0\nloading blk.47.attn_kv_a_norm.weight to cuda:0\nloading blk.47.attn_kv_b.weight to cuda:0\n\n然后就killed了\n\n看了一下htop信息, 是1T内存都被占满了, 然后swap用光(swap就配置了8GB), 然后就killed了\n\nbtw, 非常感谢作者的工作, 非常非常棒的工作!!!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/321/comments",
    "author": "adonishong",
    "comments": [
      {
        "user": "NeilAlfred",
        "created_at": "2025-02-15T12:20:19Z",
        "body": "遇到了同样的问题"
      },
      {
        "user": "Radohead",
        "created_at": "2025-02-15T15:14:32Z",
        "body": "目前架构下用numa意味着有几路cpu，内存就复制几份，所以2路cpu跑q8 1t是不够的大概要1.5t"
      },
      {
        "user": "BiFangKNT",
        "created_at": "2025-02-15T16:06:17Z",
        "body": "认真看文档啊，你都知道numa了，难道不知道双路会把模型复制一份吗\n\n一份模型占713GB，两份就是 713*2=1426GB 啊"
      },
      {
        "user": "adonishong",
        "created_at": "2025-02-15T16:25:26Z",
        "body": "我错了, 确实看文档不够仔细"
      }
    ]
  },
  {
    "number": 293,
    "title": "是否支持 A6000 或者 A100 单卡？",
    "created_at": "2025-02-14T11:39:20Z",
    "closed_at": "2025-02-16T12:01:45Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/293",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/293/comments",
    "author": "Liangdi",
    "comments": [
      {
        "user": "vc5409ftu",
        "created_at": "2025-02-15T23:45:47Z",
        "body": "可以"
      },
      {
        "user": "Liangdi",
        "created_at": "2025-02-16T12:01:45Z",
        "body": "> 可以\n\n谢谢"
      }
    ]
  },
  {
    "number": 290,
    "title": "ensure that gguf_path argument is a directory.",
    "created_at": "2025-02-14T10:31:07Z",
    "closed_at": "2025-02-15T14:48:57Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/290",
    "body": "When pass gguf file to `--gguf_path`\r\n```\r\npython ktransformers/local_chat.py --model_path /workspace/models/Qwen2-57B-A14B-Instruct --gguf_path /workspace/models/Qwen2-57B-A14B-Instruct-GGUF/qwen2-57b-a14b-instruct-q4_k_m.gguf\r\n```\r\n`GGUFLoader.__init__` does not check whether gguf_path is a directory, and the error output is\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 179, in <module>\r\n    fire.Fire(local_chat)\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 110, in local_chat\r\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\r\n  File \"/workspace/ktransformers/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\r\n    load_weights(module, gguf_loader)\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 85, in load_weights\r\n    module.load()\r\n  File \"/workspace/ktransformers/ktransformers/operators/base_operator.py\", line 60, in load\r\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 81, in load_weights\r\n    load_cur_state_dict(module, gguf_loader, prefix)\r\n  File \"/workspace/ktransformers/ktransformers/util/utils.py\", line 76, in load_cur_state_dict\r\n    raise Exception(f\"can't find {translated_key} in GGUF file!\")\r\nException: can't find token_embd.weight in GGUF file!\r\n```\r\n\r\nAfter this patch, the output will be as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 179, in <module>\r\n    fire.Fire(local_chat)\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ktransformers/ktransformers/local_chat.py\", line 110, in local_chat\r\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\r\n  File \"/workspace/ktransformers/ktransformers/optimize/optimize.py\", line 126, in optimize_and_load_gguf\r\n    gguf_loader=GGUFLoader(gguf_path)\r\n                ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/ktransformers/ktransformers/util/custom_gguf.py\", line 170, in __init__\r\n    raise NotADirectoryError(f\"GGUF is not a dir: {gguf_path}\")\r\nNotADirectoryError: GGUF is not a dir: /workspace/models/Qwen2-57B-A14B-Instruct-GGUF/qwen2-57b-a14b-instruct-q4_k_m.gguf\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/290/comments",
    "author": "ZhangShuaiyi",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-14T19:17:38Z",
        "body": "Thanks for your contribution! Have you tested this PR to ensure everything works as expected?\r\n\r\nI have a suggestion: You could check the provided path and handle it more flexibly.\r\n\t•\tIf the input is a file, use its containing directory.\r\n\t•\tIf the input is already a directory, use it directly.\r\n\r\nThis approach would make it more user-friendly and adaptable. Let me know what you think."
      },
      {
        "user": "ZhangShuaiyi",
        "created_at": "2025-02-15T07:15:47Z",
        "body": "@Azure-Tang  Thanks, done. And raise an error if no gguf file is found in the directory."
      }
    ]
  },
  {
    "number": 277,
    "title": "Linux环境下安装ktransformers失败",
    "created_at": "2025-02-14T08:18:04Z",
    "closed_at": "2025-02-17T01:34:12Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/277",
    "body": "感谢你们开源了代码\n\n我在本地环境下部署ktransformers遇到了难题，部署的环境如下\nOS：Ubuntu22.04\nCUDA版本：12.4.1\ntorch版本：2.6.0\n\n尝试了两种安装方式，首先是pip安装，遇到的错误\n`WARNING: Generating metadata for package ktransformers produced metadata for project name unknown. Fix your #egg=ktransformers fragments.`\n后来尝试了源码安装的方式，能正常安装，但是无法通过`python -m ktransformers`使用，通过pip freeze查看能看到`UNKNOWN @ file:///home/ubuntu/ktransformers`\n\n请问这个问题应该如何解决呢？\npython的版本是：3.10.12\npip的版本是：25.0.1（已更新到最新版本）\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/277/comments",
    "author": "zbljz98",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-14T08:24:04Z",
        "body": "pip show ktransformers可以找到包么"
      },
      {
        "user": "SongXiaoMao",
        "created_at": "2025-02-15T02:08:49Z",
        "body": "看一下 nvcc -V是多少？"
      },
      {
        "user": "bigben446",
        "created_at": "2025-02-15T14:39:07Z",
        "body": "> pip show ktransformers可以找到包么\n\nlinux下能提供pipx命令安装吗，pip安装系统报错\npip安装报错：error: externally-managed-environment\n这个错误信息是由于 Python 的新策略（PEP 668）所导致的，该策略旨在防止用户在系统级 Python 环境中随意安装包，以避免破坏系统的 Python 安装或操作系统的稳定性。"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-15T14:52:16Z",
        "body": "> > pip show ktransformers可以找到包么\n> \n> linux下能提供pipx命令安装吗，pip安装系统报错 pip安装报错：error: externally-managed-environment 这个错误信息是由于 Python 的新策略（PEP 668）所导致的，该策略旨在防止用户在系统级 Python 环境中随意安装包，以避免破坏系统的 Python 安装或操作系统的稳定性。\n\n先装一个conda用来管理环境吧，我感觉是你直接用系统python的pip装了"
      },
      {
        "user": "youde2000",
        "created_at": "2025-02-16T09:11:52Z",
        "body": "你解决了么"
      },
      {
        "user": "zbljz98",
        "created_at": "2025-02-17T01:32:40Z",
        "body": "> pip show ktransformers可以找到包么\n\n问题解决了，之前用的系统的pip安装的，换了conda正常了"
      }
    ]
  },
  {
    "number": 258,
    "title": "AssertionError: assert self.gate_type == GGMLQuantizationType.BF16",
    "created_at": "2025-02-14T03:46:22Z",
    "closed_at": "2025-02-15T04:01:04Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/258",
    "body": "V0.3: \nktransformers --model_path /workspace/Deepseek-models/DeepSeek-R1 --gguf_path /workspace/Deepseek-models/DeepSeek-R1-Q4_K_M --port 10002\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/ktransformers\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/main.py\", line 114, in main\n    create_interface(config=cfg, default_args=cfg)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 27, in create_interface\n    GlobalInterface.interface = BackendInterface(default_args)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 47, in __init__\n    optimize_and_load_gguf(self.model, optimize_rule_path, gguf_path, config)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\n    load_weights(module, gguf_loader)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/experts.py\", line 520, in load\n    self.generate_experts.load(w)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/experts.py\", line 201, in load\n    assert self.gate_type == GGMLQuantizationType.BF16\nAssertionError",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/258/comments",
    "author": "yansiyu550",
    "comments": [
      {
        "user": "xuxiaoou",
        "created_at": "2025-02-14T06:09:00Z",
        "body": "+1\n v0.3 + 671b q4 same error\n v0.2 + 671b q4 ok"
      },
      {
        "user": "dtlzhuangz",
        "created_at": "2025-02-14T07:59:59Z",
        "body": "The same error."
      },
      {
        "user": "DearPlanet",
        "created_at": "2025-02-14T11:30:32Z",
        "body": "Same, failed at ktransformers/operators/experts.py when using prebuilt wheel ktransformers-0.3.0rc0+cu126torch26fancy-cp311-cp311-linux_x86_64.whl.\n```\nelif self.backend == \"AMXInt8\":\n            # Here\n            assert self.gate_type == GGMLQuantizationType.BF16\n            assert self.up_type == GGMLQuantizationType.BF16\n            assert self.down_type == GGMLQuantizationType.BF16\n            moe_config = AMX_MOEConfig(\n                n_routed_experts,\n                self.config.num_experts_per_tok,\n                self.config.hidden_size,\n                self.config.moe_intermediate_size,\n                25600,\n                gate_ptr,\n                up_ptr,\n                down_ptr,\n            )\n"
      },
      {
        "user": "hustwyk",
        "created_at": "2025-02-14T11:57:51Z",
        "body": "+1 The same error."
      },
      {
        "user": "Atream",
        "created_at": "2025-02-15T04:01:04Z",
        "body": "Version 0.3 is currently just a preview, and its features are not yet fully developed. It currently only supports on-the-fly quantization from BF16 in GGUF format. Version 0.3 is faster in prefill speed, but decoding can sometimes be slower than Version 0.2. It is recommended to use Version 0.2 for now."
      },
      {
        "user": "yansiyu550",
        "created_at": "2025-02-18T03:17:56Z",
        "body": "> Version 0.3 is currently just a preview, and its features are not yet fully developed. It currently only supports on-the-fly quantization from BF16 in GGUF format. Version 0.3 is faster in prefill speed, but decoding can sometimes be slower than Version 0.2. It is recommended to use Version 0.2 for now.\n\nHere is my test data. Why is V0.2 faster than V0.3 in both prefill and decode stages?\n# V0.3\nPerformance(T/s): prefill 1.9163943328367548, decode 1.6059428791832788. Time(s): tokenize 0.055808305740356445, prefill 5.218132734298706, decode 196.76914048194885\n# V0.2\n Performance(T/s): prefill 4.045294075735022, decode 3.956256483823671. Time(s): tokenize 0.01299285888671875, prefill 1.9776065349578857, decode 284.10695934295654"
      }
    ]
  },
  {
    "number": 247,
    "title": "[Doc]Fix dead link problem",
    "created_at": "2025-02-14T02:00:50Z",
    "closed_at": "2025-02-14T02:37:32Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/247",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/247/comments",
    "author": "liugddx",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-14T02:37:47Z",
        "body": "Thx for your fix~"
      }
    ]
  },
  {
    "number": 220,
    "title": "Add optimization config for Deepseek V3/R1 with 4 GPUs",
    "created_at": "2025-02-13T08:36:30Z",
    "closed_at": "2025-02-13T08:41:39Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/220",
    "body": "Add a new optimization config for Deepseek V3/R1 with 4 GPUs.\r\n\r\nWe tested it on our server with the following config:\r\n- CPU: 2 x AMD EPYC 7543 32-Core Processor\r\n- RAM: 16 × 64GB Hynix PC4-25600 3200MHz Dual Rank DDR4 RDIMM\r\n- GPU: 8 x Nvidia RTX 3080 10G\r\n- Motherboard: ASUS KMPG-D32 Series\r\n\r\nThe decoding speed is ~5 tokens/s.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/220/comments",
    "author": "MengshiZhang",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T08:52:48Z",
        "body": "Thank you so much for your valuable contribution! Your PR is greatly appreciated and will be shared to everyone. Keep up, and we look forward to more contributions from you!"
      },
      {
        "user": "chuangzhidan",
        "created_at": "2025-02-14T06:42:49Z",
        "body": "> Thank you so much for your valuable contribution! Your PR is greatly appreciated and will be shared to everyone. Keep up, and we look forward to more contributions from you!\r\n\r\nhow to reproduce your result with the right server,which type ,model and configuration did you use?\r\nthank u so much if you can help"
      },
      {
        "user": "yydslc",
        "created_at": "2025-02-17T14:23:52Z",
        "body": "大佬是否有开AVX加速？"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-17T16:00:42Z",
        "body": "> > Thank you so much for your valuable contribution! Your PR is greatly appreciated and will be shared to everyone. Keep up, and we look forward to more contributions from you!\r\n> \r\n> how to reproduce your result with the right server,which type ,model and configuration did you use? thank u so much if you can help\r\n\r\nPlz check our tutorial."
      }
    ]
  },
  {
    "number": 213,
    "title": "No module named 'ktransformers.models.modeling_deepseek_v3'",
    "created_at": "2025-02-13T07:27:25Z",
    "closed_at": "2025-02-13T08:18:42Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/213",
    "body": "\n<details><summary>cpu info</summary>\n<p>\n\n```shell\nprocessor       : 127\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 106\nmodel name      : Intel(R) Xeon(R) Platinum 8338C CPU @ 2.60GHz\nstepping        : 6\nmicrocode       : 0xd0003e7\ncpu MHz         : 800.000\ncache size      : 49152 KB\nphysical id     : 1\nsiblings        : 64\ncore id         : 31\ncpu cores       : 32\napicid          : 191\ninitial apicid  : 191\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 27\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nvmx flags       : vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs pml ept_mode_based_exec tsc_scaling\nbugs            : spectre_v1 spectre_v2 spec_store_bypass swapgs mmio_stale_data eibrs_pbrsb gds bhi\nbogomips        : 5200.00\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 46 bits physical, 57 bits virtual\n``` \n\n\n</p>\n</details> \n\n<details><summary>pip list</summary>\n<p>\n```sh\n(ktransformers) root@user-B7129F83AV8E4HR-N:/home/user/app/ktransformers/ktransformers/optimize/optimize_rules# pip list\nPackage                  Version\n------------------------ -----------\naccelerate               1.3.0\naiohappyeyeballs         2.4.6\naiohttp                  3.11.12\naiosignal                1.3.2\nannotated-types          0.7.0\nanyio                    4.8.0\nattrs                    25.1.0\nblessed                  1.20.0\nbuild                    1.2.2.post1\ncertifi                  2025.1.31\ncharset-normalizer       3.4.1\nclick                    8.1.8\ncolorlog                 6.9.0\ncpufeature               0.2.1\neinops                   0.8.1\nfastapi                  0.115.8\nfilelock                 3.17.0\nfire                     0.7.0\nflash_attn               2.7.4.post1\nfrozenlist               1.5.0\nfsspec                   2025.2.0\ngreenlet                 3.1.1\nh11                      0.14.0\nhttpcore                 1.0.7\nhttpx                    0.28.1\nhuggingface-hub          0.28.1\nidna                     3.10\nJinja2                   3.1.5\njsonpatch                1.33\njsonpointer              3.0.0\nktransformers            0.1.4\nlangchain                0.3.18\nlangchain-core           0.3.35\nlangchain-text-splitters 0.3.6\nlangsmith                0.3.8\nMarkupSafe               3.0.2\nmpmath                   1.3.0\nmultidict                6.1.0\nnetworkx                 3.4.2\nninja                    1.11.1.3\nnumpy                    1.26.4\nnvidia-cublas-cu12       12.4.5.8\nnvidia-cuda-cupti-cu12   12.4.127\nnvidia-cuda-nvrtc-cu12   12.4.127\nnvidia-cuda-runtime-cu12 12.4.127\nnvidia-cudnn-cu12        9.1.0.70\nnvidia-cufft-cu12        11.2.1.3\nnvidia-curand-cu12       10.3.5.147\nnvidia-cusolver-cu12     11.6.1.9\nnvidia-cusparse-cu12     12.3.1.170\nnvidia-cusparselt-cu12   0.6.2\nnvidia-nccl-cu12         2.21.5\nnvidia-nvjitlink-cu12    12.4.127\nnvidia-nvtx-cu12         12.4.127\norjson                   3.10.15\npackaging                24.2\npip                      25.0\npropcache                0.2.1\nprotobuf                 5.29.3\npsutil                   6.1.1\npydantic                 2.10.6\npydantic_core            2.27.2\npyproject_hooks          1.2.0\nPyYAML                   6.0.2\nregex                    2024.11.6\nrequests                 2.32.3\nrequests-toolbelt        1.0.0\nsafetensors              0.5.2\nsentencepiece            0.2.0\nsetuptools               75.8.0\nsix                      1.17.0\nsniffio                  1.3.1\nSQLAlchemy               2.0.38\nstarlette                0.45.3\nsympy                    1.13.1\ntenacity                 9.0.0\ntermcolor                2.5.0\ntokenizers               0.19.1\ntorch                    2.6.0\ntqdm                     4.67.1\ntransformers             4.43.2\ntriton                   3.2.0\ntyping_extensions        4.12.2\nurllib3                  2.3.0\nuvicorn                  0.34.0\nwcwidth                  0.2.13\nwheel                    0.45.1\nyarl                     1.18.3\nzstandard                0.23.0\n```\n\n</p>\n</details> \n\n<details><summary>run cli</summary>\n<p>\n\n```sh\npython -m ktransformers.local_chat \\\n    --model_path deepseek-ai/DeepSeek-R1  \\\n    --gguf_path ./gguf_file \\\n    --optimize_rule_path /home/user/app/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml\n\n```\n\n</p>\n</details> \n\n<details><summary>gpu info</summary>\n<p>\n\n```sh\nroot@user-B7129F83AV8E4HR-N:/home/user/app/models/deepseek-R1# nvidia-smi\nThu Feb 13 15:25:32 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40                     On  |   00000000:15:00.0 Off |                    0 |\n| N/A   29C    P8             54W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA L40                     On  |   00000000:18:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA L40                     On  |   00000000:19:00.0 Off |                    0 |\n| N/A   27C    P0             72W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA L40                     On  |   00000000:1C:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   4  NVIDIA L40                     On  |   00000000:1D:00.0 Off |                    0 |\n| N/A   29C    P0             69W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   5  NVIDIA L40                     On  |   00000000:96:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   6  NVIDIA L40                     On  |   00000000:99:00.0 Off |                    0 |\n| N/A   26C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   7  NVIDIA L40                     On  |   00000000:9A:00.0 Off |                    0 |\n| N/A   27C    P0             70W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   8  NVIDIA L40                     On  |   00000000:9D:00.0 Off |                    0 |\n| N/A   25C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   9  NVIDIA L40                     On  |   00000000:9E:00.0 Off |                    0 |\n| N/A   27C    P8             26W /  300W |       4MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n\n```\n\n</p>\n</details> \n\n<details><summary>Ram info</summary>\n<p>\n\n```sh\nroot@user-B7129F83AV8E4HR-N:/home/user/app/models/deepseek-R1# free -g\n               total        used        free      shared  buff/cache   available\nMem:             503         102         356           0          44         397\nSwap:              0           0           0\n```\n\n</p>\n</details> \n\n# Error show :\n\n```sh\n\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/local_chat.py\", line 159, in <module>\n    fire.Fire(local_chat)\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/local_chat.py\", line 106, in local_chat\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/optimize/optimize.py\", line 122, in optimize_and_load_gguf\n    gen_optimize_config(module, optimize_config, rule_list, default_device = default_device)\n  File \"/usr/local/anaconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/optimize/optimize.py\", line 68, in gen_optimize_config\n    module_cls=getattr(__import__(import_module_name, fromlist=[\"\"]), import_class_name)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'ktransformers.models.modeling_deepseek_v3'\n```\n\n\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/213/comments",
    "author": "Zha-Miku",
    "comments": [
      {
        "user": "Linzwcs",
        "created_at": "2025-02-13T07:39:01Z",
        "body": " ktransformers 0.1.4版本低了， 现在得从源码安装，pip源那边还没更新\n"
      }
    ]
  },
  {
    "number": 212,
    "title": "500G内存只用了18G",
    "created_at": "2025-02-13T07:23:23Z",
    "closed_at": "2025-02-14T08:53:00Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/212",
    "body": "500G内存\n志强CPU，单卡运行，速度还可以8 token每秒，使用了2.5量化模型。\n\n但只有18g内存用到，猜测需要开启内存载入，同样情况我在llama cpp上遇到过，解决了，但似乎对ktransformers无效。\n\n安装软件：ktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n\n命令：\npython -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n\n\n\n\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/212/comments",
    "author": "lymanzhao",
    "comments": [
      {
        "user": "murongweibo",
        "created_at": "2025-02-13T07:26:00Z",
        "body": "> 500G内存 志强CPU，单卡运行，速度还可以8 token每秒，使用了2.5量化模型。\n> \n> 但只有18g内存用到，猜测需要开启内存载入，同样情况我在llama cpp上遇到过，解决了，但似乎对ktransformers无效。\n> \n> 安装软件：ktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> \n> 命令： python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n\n你这个用什么gpu？我的报了错\n```\nRuntimeError: CUDA error: invalid device function\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```"
      },
      {
        "user": "shuchong",
        "created_at": "2025-02-13T07:26:56Z",
        "body": "> > 500G内存 志强CPU，单卡运行，速度还可以8 token每秒，使用了2.5量化模型。\n> > 但只有18g内存用到，猜测需要开启内存载入，同样情况我在llama cpp上遇到过，解决了，但似乎对ktransformers无效。\n> > 安装软件：ktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > 命令： python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> \n> 你这个用什么gpu？我的报了错\n> \n> ```\n> RuntimeError: CUDA error: invalid device function\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> ```\n\n这个报错是不是和卡有关系，我使用v100也有这个报错"
      },
      {
        "user": "lymanzhao",
        "created_at": "2025-02-13T07:52:47Z",
        "body": "> > 500G内存 志强CPU，单卡运行，速度还可以8 token每秒，使用了2.5量化模型。\n> > 但只有18g内存用到，猜测需要开启内存载入，同样情况我在llama cpp上遇到过，解决了，但似乎对ktransformers无效。\n> > 安装软件：ktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > 命令： python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> \n> 你这个用什么gpu？我的报了错\n> \n> ```\n> RuntimeError: CUDA error: invalid device function\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> ```\n\n我是4090，你这个看着是torch的问题，检查是不是版本问题呢"
      },
      {
        "user": "Xxianna",
        "created_at": "2025-02-13T08:02:38Z",
        "body": "同，加载deepseek r1 q4km后只有大概15g内存和15g显存占用\n但是加载过程产生了巨大的cache"
      },
      {
        "user": "futureltj",
        "created_at": "2025-02-13T08:06:43Z",
        "body": "同样的情况，推理速度很慢。有解吗？"
      },
      {
        "user": "shiontao",
        "created_at": "2025-02-13T08:33:30Z",
        "body": "> > > 500G内存 志强CPU，单卡运行，速度还可以8 token每秒，使用了2.5量化模型。\n> > > 但只有18g内存用到，猜测需要开启内存载入，同样情况我在llama cpp上遇到过，解决了，但似乎对ktransformers无效。\n> > > 安装软件：ktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > > 命令： python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> > \n> > \n> > 你这个用什么gpu？我的报了错\n> > ```\n> > RuntimeError: CUDA error: invalid device function\n> > CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> > For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> > Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> > ```\n> \n> 这个报错是不是和卡有关系，我使用v100也有这个报错\n\n一样的，应该是架构太老了的原因"
      },
      {
        "user": "murongweibo",
        "created_at": "2025-02-13T09:18:13Z",
        "body": "> > > 500G内存 志强CPU，单卡运行，速度还可以8 token每秒，使用了2.5量化模型。\n> > > 但只有18g内存用到，猜测需要开启内存载入，同样情况我在llama cpp上遇到过，解决了，但似乎对ktransformers无效。\n> > > 安装软件：ktransformers-0.2.0+cu124torch24avx2-cp311-cp311-linux_x86_64.whl\n> > > 命令： python -m ktransformers.local_chat --model_path \"/mnt/data/deepseek/DeepSeek-R1/\" --gguf_path \"/mnt/data/deepseek/DeepSeek-R1-GGUF/DeepSeek-R1-UD-Q2_K_XL/\" --force_think true --cpu_infer 60\n> > \n> > \n> > 你这个用什么gpu？我的报了错\n> > ```\n> > RuntimeError: CUDA error: invalid device function\n> > CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> > For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> > Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> > ```\n> \n> 这个报错是不是和卡有关系，我使用v100也有这个报错\n\n应该是卡的框架原因，我这里用的谷歌colab里面的分的T4，我看别人说V100也不行"
      },
      {
        "user": "ipfgao",
        "created_at": "2025-02-13T13:53:12Z",
        "body": "> 同，加载deepseek r1 q4km后只有大概15g内存和15g显存占用 但是加载过程产生了巨大的cache\n\n这样的话，不用32g内存也有希望运行deepseek r1 q4km了？"
      },
      {
        "user": "lymanzhao",
        "created_at": "2025-02-13T14:31:25Z",
        "body": "> > 同，加载deepseek r1 q4km后只有大概15g内存和15g显存占用 但是加载过程产生了巨大的cache\n> \n> 这样的话，不用32g内存也有希望运行deepseek r1 q4km了？\n\n你满足这点速度吗，这是硬盘缓存限制速度"
      },
      {
        "user": "lymanzhao",
        "created_at": "2025-02-14T08:52:56Z",
        "body": "top可以看到真实使用情况"
      }
    ]
  },
  {
    "number": 209,
    "title": "什么时候支持amd显卡啊， rocm生态这么差吗？",
    "created_at": "2025-02-13T06:39:11Z",
    "closed_at": "2025-02-13T06:44:57Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/209",
    "body": "什么时候支持amd显卡啊， rocm生态这么差吗？",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/209/comments",
    "author": "carsonfly",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T06:44:49Z",
        "body": "PR里有个大佬正在支持，您可以关注一下\n"
      }
    ]
  },
  {
    "number": 193,
    "title": "请问支持H系列显卡单卡运行DeepSeek V3/R1吗",
    "created_at": "2025-02-13T02:56:16Z",
    "closed_at": "2025-02-13T04:17:31Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/193",
    "body": "显卡系列：H20",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/193/comments",
    "author": "CarsonSo",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T04:05:06Z",
        "body": "这个主要取决于显卡的架构和显存，需要ampere及以上架构以及16GB以上显存。H系列是可以的"
      }
    ]
  },
  {
    "number": 189,
    "title": "fix typo in README.md",
    "created_at": "2025-02-13T02:16:27Z",
    "closed_at": "2025-02-13T02:24:01Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/189",
    "body": "replace misspelling 'ktransfermor' with 'ktransformers'",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/189/comments",
    "author": "Kattos",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T08:53:45Z",
        "body": "Thank you so much for your valuable contribution~"
      }
    ]
  },
  {
    "number": 180,
    "title": "doc: fix clerical error",
    "created_at": "2025-02-12T23:27:47Z",
    "closed_at": "2025-02-13T02:25:30Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/180",
    "body": "fix clerical error: Fed -> Feb",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/180/comments",
    "author": "lusipad",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-13T08:54:15Z",
        "body": "Thank you so much for your correction~"
      }
    ]
  },
  {
    "number": 169,
    "title": "请问支持的python最低版本是3.11及以上吗？",
    "created_at": "2025-02-12T09:47:40Z",
    "closed_at": "2025-02-13T02:27:52Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/169",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/169/comments",
    "author": "bachelor-dou",
    "comments": [
      {
        "user": "KMSorSMS",
        "created_at": "2025-02-12T12:20:14Z",
        "body": "> No description provided.\n\n我们都是在3.11版本以上跑的"
      },
      {
        "user": "Atream",
        "created_at": "2025-02-13T02:27:52Z",
        "body": "只要torch能正常运行就可以，python版本别太老旧就行，有人用python 3.10跑通过，更低的版本没有试过，但理论上没啥问题"
      }
    ]
  },
  {
    "number": 154,
    "title": "v0.2 run DeepSeek-R1-Q4_K_M failed.",
    "created_at": "2025-02-12T05:24:03Z",
    "closed_at": "2025-02-12T06:36:03Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/154",
    "body": "```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 278, in <module>\n    fire.Fire(local_chat)\n  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 215, in local_chat\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\n    load_weights(module, gguf_loader)\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/base_operator.py\", line 60, in load\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 83, in load_weights\n    load_weights(child, gguf_loader, prefix+name+\".\")\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 85, in load_weights\n    module.load()\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/gate.py\", line 110, in load\n    if w is None: w = self.load_weights(device=device)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/gate.py\", line 72, in load_weights\n    tensors = self.load_multi(key, targets, device=device)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/gate.py\", line 85, in load_multi\n    tensors[k] = self.gguf_loader.load_gguf_tensor(key + k, device=device)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/custom_gguf.py\", line 300, in load_gguf_tensor\n    values = values.view(shape[::-1])\n             ^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[256, 7168]' is invalid for input of size 0\n```\n```\npython3.11 -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-R1 --gguf_path /data/DeepSeek-R1-UD-Q2_K_XL  --cpu_infer 24 --max_new_tokens 1000\n```\nHi, UD-Q2_K_XL works pretty well on my server. But when I switch the model to Q4_K_M, the above RuntimeError was thrown.\nCould you please clarify for me?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/154/comments",
    "author": "ArYuZzz",
    "comments": [
      {
        "user": "Bob99uk",
        "created_at": "2025-02-12T06:37:56Z",
        "body": "Can i ask how you resolved this?"
      },
      {
        "user": "wangfan008",
        "created_at": "2025-02-17T08:07:25Z",
        "body": "+1"
      },
      {
        "user": "EBGU",
        "created_at": "2025-02-19T08:46:13Z",
        "body": "+10086"
      },
      {
        "user": "HuDi2018",
        "created_at": "2025-02-20T09:29:01Z",
        "body": "> Can i ask how you resolved this?\n\nCheck if the GGUF weights have been downloaded completely."
      }
    ]
  },
  {
    "number": 134,
    "title": "能否支持SM75的设备",
    "created_at": "2025-02-11T06:45:02Z",
    "closed_at": "2025-02-11T06:55:24Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/134",
    "body": "如果一定使用flash-attenttion的，是否可以适配flash-attenttion1.0版本",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/134/comments",
    "author": "bltcn",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-11T06:55:06Z",
        "body": "不支持sm75，我们用的marlin算子只支持ampere及以上架构"
      }
    ]
  },
  {
    "number": 133,
    "title": "装不了啊",
    "created_at": "2025-02-11T06:03:47Z",
    "closed_at": "2025-02-11T16:02:08Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/133",
    "body": "那个deepseekr1是怎么跑起来的。\n./install.sh 直接UNKNOW",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/133/comments",
    "author": "qixing-ai",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-11T06:52:24Z",
        "body": "严格follow readme了吗，贴一下报错？"
      },
      {
        "user": "Atream",
        "created_at": "2025-02-11T06:58:28Z",
        "body": "如果是要源码安装的话，试一下bash install.sh，如果已经安装成功要跑的话，看下面的python -m ktransformers.local_chat"
      }
    ]
  },
  {
    "number": 118,
    "title": "use HF cli to download ggufs instead of wget",
    "created_at": "2024-12-29T16:34:57Z",
    "closed_at": "2025-02-11T10:20:50Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/118",
    "body": "its easier to use HF cli to pull gulfs or any file/files directly.\r\n",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/118/comments",
    "author": "lzumot",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2025-02-11T10:20:44Z",
        "body": "Access to Hugging Face is limited in certain regions, so we have provided a more flexible solution to ensure consistent usage."
      }
    ]
  },
  {
    "number": 103,
    "title": "How to infer quantized models on CPU&GPU",
    "created_at": "2024-10-20T06:33:50Z",
    "closed_at": "2024-10-23T07:19:10Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/103",
    "body": "I found that ktransformers first performs a dequantize operation when loading the weights. Due to DRAM limitations, I want to directly infer the model with quantized weights on CPU&GPU. How can I implement this?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/103/comments",
    "author": "shuzhang-pku",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-10-23T07:18:37Z",
        "body": "The dequantised weight will be quantised again into marlin format to use marlin op (which is fast). So if you are using optimising rules that we provided, you are directly using quantized weights."
      }
    ]
  },
  {
    "number": 100,
    "title": "Does ktransformers support deepseek V2.5？",
    "created_at": "2024-10-12T11:34:10Z",
    "closed_at": "2024-10-15T06:37:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/100",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/100/comments",
    "author": "huliangbing",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-10-14T06:58:02Z",
        "body": "Yes～you can run v2.5 with deepseekv2‘s yaml."
      },
      {
        "user": "huliangbing",
        "created_at": "2024-10-14T17:39:28Z",
        "body": "Thank you very much!"
      }
    ]
  },
  {
    "number": 95,
    "title": "Suggestion to add DeepSeek v2.5 support",
    "created_at": "2024-09-24T08:40:58Z",
    "closed_at": "2024-09-25T08:49:30Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/95",
    "body": "Recently DeepSeek 2.5 MoE model was released. Please consider adding a support for it. (Q5_K_M or Q4_K_M)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/95/comments",
    "author": "arisau",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-24T10:31:11Z",
        "body": "It appears to be supported automatically, as some users have already tested it successfully."
      },
      {
        "user": "arisau",
        "created_at": "2024-09-24T21:04:31Z",
        "body": "Perfect. Thank you. \r\njust to confirm, only Q4_K_M works?"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-25T04:45:04Z",
        "body": "We have supported all Qx_km format~"
      },
      {
        "user": "arisau",
        "created_at": "2024-09-25T08:49:30Z",
        "body": "Understood. Thank you once again. "
      }
    ]
  },
  {
    "number": 90,
    "title": "Installation Problem",
    "created_at": "2024-09-15T04:32:18Z",
    "closed_at": "2024-09-24T06:33:07Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/90",
    "body": "My GPU device is L20, the two installation ways are not work for me, could you give me some suggestions?\r\n1. What I run 'bash install.sh'. The error is:\r\n      /root/ktransformers/ktransformers/ktransformers_ext/cpu_backend/cpuinfer.h:45:43: error: using invalid field ‘CPUInfer::enqueue(Func, Obj*, Args ...)::__lambda1::__args’\r\n      ninja: build stopped: subcommand failed.\r\n      Traceback (most recent call last):\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\r\n          return _build_backend().build_wheel(wheel_directory, config_settings,\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/build_meta.py\", line 415, in build_wheel\r\n          return self._build_with_temp_dir(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/build_meta.py\", line 397, in _build_with_temp_dir\r\n          self.run_setup()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 293, in <module>\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/__init__.py\", line 108, in setup\r\n          return distutils.core.setup(**attrs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 184, in setup\r\n          return run_commands(dist)\r\n                 ^^^^^^^^^^^^^^^^^^\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\r\n          dist.run_commands()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\r\n          self.run_command(cmd)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"<string>\", line 153, in run\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/wheel/_bdist_wheel.py\", line 378, in run\r\n          self.run_command(\"build\")\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\r\n          self.run_command(cmd_name)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/dist.py\", line 945, in run_command\r\n          super().run_command(command)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\r\n          cmd_obj.run()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 93, in run\r\n          _build_ext.run(self)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 866, in build_extensions\r\n          build_ext.build_extensions(self)\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 479, in build_extensions\r\n          self._build_extensions_serial()\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 505, in _build_extensions_serial\r\n          self.build_extension(ext)\r\n        File \"<string>\", line 288, in build_extension\r\n        File \"/root/.conda/envs/ktransformers/lib/python3.11/subprocess.py\", line 571, in run\r\n          raise CalledProcessError(retcode, process.args,\r\n      subprocess.CalledProcessError: Command '['cmake', '--build', '.']' returned non-zero exit status 1.\r\n      [end of output]\r\n  \r\n      note: This error originates from a subprocess, and is likely not a problem with pip.\r\n      ERROR: Failed building wheel for ktransformers\r\n      Failed to build ktransformers\r\n      ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\r\n\r\n2. When I run pip install ktransformers --no-build-isolation, there is no error, but when I run 'python -m ktransformers.local_chat --model_name deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ./DeepSeek-V2-Chat-0628-GGUFA', the error is:\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/root/ktransformers/ktransformers/local_chat.py\", line 25, in <module>\r\n    from ktransformers.optimize.optimize import optimize_and_load_gguf\r\n  File \"/root/ktransformers/ktransformers/optimize/optimize.py\", line 15, in <module>\r\n    from ktransformers.util.custom_gguf import GGUFLoader, translate_name_to_gguf\r\n  File \"/root/ktransformers/ktransformers/util/custom_gguf.py\", line 27, in <module>\r\n    import KTransformersOps\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.32' not found (required by /root/.conda/envs/ktransformers/lib/python3.11/site-packages/KTransformersOps.cpython-311-x86_64-linux-gnu.so)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/90/comments",
    "author": "Chain-Mao",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-18T00:44:44Z",
        "body": "This issue seems to be very similar to #37 , both involving the absence of libstdc++.so.6. Perhaps the methods mentioned there could useful?"
      }
    ]
  },
  {
    "number": 83,
    "title": "Use cond var to avoid busy loop",
    "created_at": "2024-09-11T09:12:11Z",
    "closed_at": "2024-10-09T10:57:17Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/83",
    "body": "Fix #80 ",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/83/comments",
    "author": "sayap",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-13T00:58:00Z",
        "body": "I will merge the code after completing the tests on the Windows platform."
      }
    ]
  },
  {
    "number": 82,
    "title": "Seg Fault on long replies",
    "created_at": "2024-09-10T14:44:09Z",
    "closed_at": "2024-09-11T13:43:47Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/82",
    "body": "I am trying to use this as a llama.cpp replacement as it is a lot faster.  I did modify the backend args file (`ktransformers/server/backend/args.py`) as max_new_tokens isn't an option for the sever:\r\n```\r\n-    max_new_tokens: int = Field(500, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n+    max_new_tokens: int = Field(2040, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n```\r\n\r\nto allow for longer responses as 500 tokens was causing a lot of my stuff to get cut off half way through.\r\nIt does generate some tokens (feels like about the 500 limit) and then hard crashes.  So maybe there is some other limit that I need to adjust?\r\n\r\nI am using DeekSeek-V2.5 in Q4_K_M, I did also try it with WizardLM 8x22B and the same thing happens.\r\n\r\nHardware: 1x 3090, Epyc 7402, 512 Gb Ram\r\n\r\n```\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../ktransformers.sh: line 12: 2152563 Segmentation fault      (core dumped) ktransformers --model_path /nvmes/models/DeepSeek-V2.5/ --gguf_path /nvmes/models/DeepSeek-V2.5-GGUF2/ --port 8081 --cpu_infer 32\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/82/comments",
    "author": "matthusby",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-11T06:38:22Z",
        "body": "Hi, maybe this one will help #73 "
      },
      {
        "user": "matthusby",
        "created_at": "2024-09-11T13:43:47Z",
        "body": "Thank you, yeah changing the `cache_lens` param is what was causing my problem.  I guess my chat was at just the right length where the longer max tokens would trigger that error.\r\n\r\nFor reference I doubled it and now when I try to run DeepSeek-V2.5 I get a out of memory crash, but on Wizard 8x22 its working great.\r\n\r\nIt looks like there are plans to address some of this soon, so I will close this issue."
      }
    ]
  },
  {
    "number": 73,
    "title": "When the input token exceeds 4096, an error will occur.",
    "created_at": "2024-09-02T06:23:20Z",
    "closed_at": "2024-09-03T11:34:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/73",
    "body": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/fastapi/routing.py\", line 210, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 651, in forward\r\n    causal_mask = self._update_causal_mask(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1624, in _update_causal_mask\r\n    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\r\n                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nRuntimeError: The size of tensor a (4096) must match the size of tensor b (8122) at non-singleton dimension 3",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/73/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-03T09:17:30Z",
        "body": "Hi, I haven’t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?"
      },
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T09:30:43Z",
        "body": "> Hi, I haven’t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?\r\n\r\nI compiled it using the most recent code, and my configs are: \r\n``` yaml\r\n- match:\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding\r\n  replace:\r\n    class: ktransformers.operators.RoPE.YarnRotaryEmbedding\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\.(?!.*self_attn\\\\.kv_b_proj).*$\"  # regular expression\r\n    class: torch.nn.Linear  # only match modules matching name and class simultaneously\r\n  replace:\r\n    class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n      generate_op: \"KLinearMarlin\"\r\n      prefill_op: \"KLinearTorch\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp$\"\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE\r\n  replace:\r\n    class: ktransformers.operators.experts.KDeepseekV2MoE     # mlp module with custom forward function\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp\\\\.experts$\"\r\n  replace:\r\n    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism\r\n    kwargs:\r\n      prefill_device: \"cuda\"\r\n      prefill_op: \"KExpertsTorch\"\r\n      generate_device: \"cpu\"\r\n      generate_op: \"KExpertsCPU\"\r\n      out_device: \"cuda\"\r\n  recursive: False # don't recursively inject submodules of this module\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.self_attn$\"\r\n  replace:\r\n    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model$\"\r\n  replace:\r\n    class: \"ktransformers.operators.models.KDeepseekV2Model\"\r\n    kwargs:\r\n      per_layer_prefill_intput_threshold: 0 # 0 is close layer wise prefill\r\n- match:\r\n    name: \"^model.embed_tokens\"\r\n  replace:\r\n    class: \"default\"\r\n    kwargs:\r\n      generate_device: \"cpu\"\r\n      prefill_device: \"cuda\"\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T10:05:26Z",
        "body": "I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release.\r\nIf you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens"
      },
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T11:34:15Z",
        "body": "> I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release. If you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens\r\n\r\nLGTM"
      }
    ]
  },
  {
    "number": 72,
    "title": "Support IQ4_XS dequantize",
    "created_at": "2024-09-02T04:07:58Z",
    "closed_at": "2024-09-06T02:06:10Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/pull/72",
    "body": "From my testing, the IQ4_XS quant of deepseek coder v2 has noticeably better quality than the Q3_K quants, while still can fit into 128GB of DRAM, unlike the Q4_K quants.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/72/comments",
    "author": "sayap",
    "comments": [
      {
        "user": "ThomasBaruzier",
        "created_at": "2024-09-03T20:27:08Z",
        "body": "Edit: Works well on my RTX 3090!\r\n\r\n<details>\r\n  <summary>Previous message</summary>\r\n\r\nTrying this PR, got unlucky:\r\n\r\n```python\r\n$ python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path DeepSeek-V2-Chat-0628-IQ4-XS --cpu_infer 30\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/local_chat.py\", line 159, in <module>\r\n    fire.Fire(local_chat)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/local_chat.py\", line 106, in local_chat\r\n    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/optimize/optimize.py\", line 129, in optimize_and_load_gguf\r\n    load_weights(module, gguf_loader)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 85, in load_weights\r\n    module.load()\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/operators/base_operator.py\", line 60, in load\r\n    utils.load_weights(child, self.gguf_loader, self.key+\".\")\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 83, in load_weights\r\n    load_weights(child, gguf_loader, prefix+name+\".\")\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 81, in load_weights\r\n    load_cur_state_dict(module, gguf_loader, prefix)\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/utils.py\", line 71, in load_cur_state_dict\r\n    weights = gguf_loader.load_gguf_tensor(translated_key, device = device).to(dtype = target_dtype)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/storage/ai/envs/ktransformers/lib/python3.12/site-packages/ktransformers/util/custom_gguf.py\", line 283, in load_gguf_tensor\r\n    raise NotImplementedError(f\"ggml_type {ggml_type} not implemented\")\r\nNotImplementedError: ggml_type 23 not implemented\r\n```\r\n\r\n</details>\r\n"
      },
      {
        "user": "sayap",
        "created_at": "2024-09-04T04:21:25Z",
        "body": "Great :)\r\n\r\nBtw, I just realized that 128GB of DRAM is actually enough for the Q4_K_S quant, which has slightly better quality than IQ4_XS."
      },
      {
        "user": "ThomasBaruzier",
        "created_at": "2024-09-04T06:48:19Z",
        "body": "> Great :)\r\n> \r\n> Btw, I just realized that 128GB of DRAM is actually enough for the Q4_K_S quant, which has slightly better quality than IQ4_XS.\r\n\r\nI still don't understand why the Q4_K_M works very slowly on 128GB RAM + 24GB VRAM as its size is 133GB. There should be around 19GB of free space for context and OS after loading the model, right?"
      },
      {
        "user": "sayap",
        "created_at": "2024-09-05T02:35:45Z",
        "body": "With Q4_K_M, I can see the `kswapd0` process being quite active in `top` output, and there are a lot of disk read activity in the `vmstat 1`  output, e.g.\r\n```\r\n# during prefill\r\nprocs -----------memory---------- ---swap-- -----io---- -system-- -------cpu-------\r\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st gu\r\n 6  3      0 289244  11940 126919688    0    0 629864     0 67690 8384 41  5 49  6  0  0\r\n 9  0      0 327360  11940 126874760    0    0 506028     0 74559 7588 42  3 50  4  0  0\r\n 4  4      0 285072  11940 126927636    0    0 666304     0 80420 11166 40  4 50  6  0  0\r\n 5  4      0 256908  11940 126962388    0    0 1270196     0 111297 20141 29  5 49 17  0  0\r\n 8  0      0 393460  11948 126828420    0    0 1241244    20 119699 20492 29  7 47 17  0  0\r\n 8  1      0 324772  11948 126889052    0    0 757848     0 88407 12399 38  4 51  8  0  0\r\n 6  3      0 446696  11948 126755172    0    0 1162144     0 117587 16711 31  5 49 15  0  0\r\n...\r\n\r\n# during decode\r\nprocs -----------memory---------- ---swap-- -----io---- -system-- -------cpu-------\r\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st gu\r\n10  0      0 309680   1760 126875172    0    0 213236     0 50849 28478 42  4 43 11  0  0\r\n 8  0      0 356932   1756 126838984    0    0 300644     0 60901 21647 42  4 44 11  0  0\r\n 8  1      0 287424   1752 126911680    0    0 204052     0 30458 9555 48  2 44  5  0  0\r\n10  0      0 284936   1748 126915004    0    0 143544     0 27526 7944 49  2 45  4  0  0\r\n 9  0      0 415712   1744 126793764    0    0 431616     0 78055 12728 43  4 44 10  0  0\r\n 9  0      0 384060   1740 126790948    0    0 276432     0 47443 9688 46  3 45  7  0  0\r\n 2  7      0 314216   1736 126873164    0    0 292808     0 38419 9919 46  3 45  7  0  0\r\n...\r\n```\r\nI guess 128GB RAM is just not enough to hold the weights."
      }
    ]
  },
  {
    "number": 69,
    "title": "Missing pip packages flash_attn and wheel",
    "created_at": "2024-08-31T16:58:09Z",
    "closed_at": "2024-09-03T17:31:05Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/69",
    "body": "Might want to add those two packages to requirements file.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/69/comments",
    "author": "bitbottrap",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T03:17:24Z",
        "body": "Flash-attn is an optional package, and not all models require it. Additionally, if a precompiled package for flash-attn is not available, it needs to be compiled and installed, which can be very time-consuming. Therefore, flash-attn has not been added to the requirements.\r\n\r\nWe apologize for the omission of wheel package. We will include it in the next release. We use environments created with conda, which include wheel by default, so we did not encounter this issue."
      },
      {
        "user": "bitbottrap",
        "created_at": "2024-09-03T17:31:05Z",
        "body": "Sound reasoning. Might want to document the possibility of needing flash_attn. Great project!"
      }
    ]
  },
  {
    "number": 68,
    "title": "What is the maximum input token size supported for DeepSeek V2?",
    "created_at": "2024-08-31T02:55:50Z",
    "closed_at": "2024-09-11T06:36:50Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/68",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/68/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-03T08:55:49Z",
        "body": "For now it should be limited by the ability of the model it self and your VRAM size. "
      }
    ]
  },
  {
    "number": 54,
    "title": "Add a instruction for configuring CUDA_HOME and CUDA_PATH to the install section of README.md.",
    "created_at": "2024-08-27T08:51:46Z",
    "closed_at": "2024-08-28T07:05:12Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/54",
    "body": "I tried to compile kTransformers based on the source code, but the compilation failed.\r\n\r\n```\r\nninja: error: '/lib64/libcudart.so', needed by '/home/xxx/projects/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so', missing and no known rule to make it\r\n```\r\nThis error is caused by line 226 in ktransformers/ktransforms_ext/CMakeLists.txt, which links libcudart.so from CUDA_HOME by default. However, in my environment, I configured CUDA through PATH and LD_LIBRRY_PATH. \r\n\r\n```cmake\r\npybind11_add_module(${PROJECT_NAME} MODULE ${ALL_SOURCES})\r\ntarget_link_libraries(${PROJECT_NAME} PRIVATE llama)\r\nif(WIN32)\r\n    target_link_libraries(${PROJECT_NAME} PRIVATE \"$ENV{CUDA_PATH}/lib/x64/cudart.lib\")\r\nelseif(UNIX)\r\n    target_link_libraries(${PROJECT_NAME} PRIVATE \"$ENV{CUDA_HOME}/lib64/libcudart.so\") # this line cause ERROR\r\nendif()\r\n```\r\n\r\nAfter setting CUDA_HOME, I successfully compiled the project. Therefore, the configuration instructions for CUDA_HOME are preferred to be added to README.md.\r\n\r\nAnd, can I propose a PR to add these configuration instructions?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/54/comments",
    "author": "hyx1999",
    "comments": [
      {
        "user": "UnicornChan",
        "created_at": "2024-08-28T01:51:45Z",
        "body": "Thank you very much. We welcome PR to improve our project. about the README here, could you please set the reminder in the README to the \"Quick Start\" section under \"Install CUDA\"? \r\nAt the same time, We will also improve our CMakeLists file."
      },
      {
        "user": "hyx1999",
        "created_at": "2024-08-28T04:24:23Z",
        "body": "Thank you for your reply! I have submitted a PR request to add a reminder to set CUDA_HOME and CUDA_PATH. \r\n"
      },
      {
        "user": "GavinZhang0228",
        "created_at": "2025-02-18T19:24:13Z",
        "body": "碰到了一些类似的问题，在安装使用install.sh 的时候 因为CUDA_HOME的存在 导致CUDA_PATH和PATH不生效了，因为我的CUDA_HOME指向了一个已经卸载的老版本CUDA，于是报错No such file or directory （但是其实我好像已经删除了这个环境变量  但是不知道为什么还生效）在重新写了一个CUDA_HOME的变量覆盖路径之后解决了"
      }
    ]
  },
  {
    "number": 53,
    "title": "Support for Mistral-Large-Instruct-2407-GGUF ？",
    "created_at": "2024-08-23T07:29:06Z",
    "closed_at": "2024-08-29T09:16:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/53",
    "body": "Q2、Q3、Q4、Q8、BF16、FP16 ？",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/53/comments",
    "author": "LIUKAI0815",
    "comments": [
      {
        "user": "BITcyman",
        "created_at": "2024-08-23T10:28:07Z",
        "body": "Currently, ktransformers mainly focus on speeding up MoE models. So we support Mixtral-8x7b and Mixtral-8x22b in Release v0.1.2. But Mistral-Large-Instruct is a dense model, which we might not support in the near future."
      },
      {
        "user": "LIUKAI0815",
        "created_at": "2024-08-28T03:21:30Z",
        "body": "@BITcyman Thanks for replying and looking forward to the new release."
      }
    ]
  },
  {
    "number": 50,
    "title": "How to properly disable offloading MoE layers to CPU?",
    "created_at": "2024-08-21T17:18:35Z",
    "closed_at": "2024-08-26T03:12:23Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/50",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/50/comments",
    "author": "molamooo",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-08-22T02:30:28Z",
        "body": "Regarding your question on disabling the offloading of MoE layers to the CPU, I’ve provided a brief explanation in my earlier response (#46 ). Please take a look there for detailed instructions."
      },
      {
        "user": "molamooo",
        "created_at": "2024-08-22T04:27:26Z",
        "body": "I tried using custom rule file to put all layers on GPU, but KTransformer triggers oom when dequantizing parameters. I use a 24GB 4090 to run deepseek-v2-lite-chat with q8_0 quantization. It should only takes ~16GB memory. Other systems like llama.cpp can run without issue."
      },
      {
        "user": "ELigoP",
        "created_at": "2024-08-22T05:42:07Z",
        "body": "This is hardly possible.\r\nWhat is your model size? With q8 quantization it should be >200 GB. If you\r\n\r\n> put all layers on GPU\r\n\r\nno wonder it OOMs. Llama.cpp automatically offloads layers in to CPU suboptimal way. If you force it to offload everything to GPU it will also OOM.\r\n\r\nE.g. with my setup I get 7.5 tps with suboptimal llama.cpp CPU offload and 9 tps with ktransformers (even that it utilizies VRAM suboptimally)."
      },
      {
        "user": "molamooo",
        "created_at": "2024-08-22T05:43:37Z",
        "body": "> This is hardly possible. What is your model size? With q8 quantization it should be >200 GB.\r\n\r\nI mentioned that I use deepseek-v2-lite-chat, which is a 16B model. It shouldn't oom."
      },
      {
        "user": "Azure-Tang",
        "created_at": "2024-08-22T07:07:29Z",
        "body": "I see, if you are using KExpertsTorch as backend, it will dequant the weights to model's default dtype, for deepseek this is bf16. So this is why it oom. Maybe you can try marlin backend? But we didn't optimise this operator in generate phase."
      }
    ]
  },
  {
    "number": 47,
    "title": "Can I run llama3.1 70b with rtx4090+64g ddr5 ram?",
    "created_at": "2024-08-20T23:40:52Z",
    "closed_at": "2025-02-10T09:26:31Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/47",
    "body": "Can I run llama3.1 70b with rtx4090+64g ddr5 ram? \r\n\r\nAt what rate per second are tokens generated?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/47/comments",
    "author": "codeMonkey-shin",
    "comments": [
      {
        "user": "ELigoP",
        "created_at": "2024-08-21T08:00:48Z",
        "body": "This is not an issue, it is general question.\r\nAs far as I know, ktransformers speeds up only MoE models, and llama3.1 70b is dense model (no routing/expert layers).\r\nThe answer to your question depends on your model quantization level, motherboard, RAM used and CPU."
      },
      {
        "user": "Atream",
        "created_at": "2025-02-10T09:26:31Z",
        "body": "KTransformers does not offer as significant a performance improvement over llama.cpp on dense model as MOE models do, but it still performs well. You can try writing a YAML file yourself to place the most computationally intensive tasks on the GPU, while the remaining tasks can be completed on the CPU. As long as the DRAM can fully accommodate your GGUF file, KTransformers will be able to run."
      }
    ]
  },
  {
    "number": 45,
    "title": "Cannot run DeepSeek V2 Chat in server mode on 2 GPUs",
    "created_at": "2024-08-20T16:20:09Z",
    "closed_at": "2024-08-22T08:32:49Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/45",
    "body": "I have 2x RTX 3090. I've built `ktransformers` from source (needed to install some missing packages, but it worked in the end).\r\n\r\nWhen I run `local_chat` with\r\n\r\n```\r\npython -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ~/.cache/lm-studio/models/bartowski/DeepSeek-V2-Chat-0628-GGUF --optimize_rule_path ./build/lib.linux-x86_64-cpython-312/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml\r\n```\r\n\r\nin-console chat works, I get 9 tps, more than llama.cpp out-of-the-box 7.5 tps.\r\n\r\nI didn't find a way to pass multi-gpu optimization rule to `ktransformers` server, as `--optimize_config_path OPTIMIZE_CONFIG_PATH` option seems to be not about that. So I just replaced default file with `multi-gpu` version. The I run\r\n\r\n```\r\nktransformers --model_path deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ~/.cache/lm-studio/models/bartowski/DeepSeek-V2-Chat-0628-GGUF --port 1234\r\n```\r\n\r\nI get this error (in the end it says `Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!`).\r\n\r\n**Should I also change `ktransformers/configs/config.yaml` ?** How? Is the problem that only `cuda:0` is mentioned in config? But right when the server starts it seems to read optimize rule right - it split 0-29, 30-59 layers bettween GPUs:\r\n\r\n```\r\n...\r\nloading blk.29.attn_norm.weight to cuda:0\r\nloading blk.29.ffn_norm.weight to cuda:0\r\nloading blk.30.attn_q_a.weight to cuda:1\r\nloading blk.30.attn_q_a_norm.weight to cuda:1\r\n...\r\n```\r\n\r\nThe error:\r\n\r\n```\r\nINFO:     192.168.27.159:46148 - \"OPTIONS /v1/chat/completions HTTP/1.1\" 200 OK\r\n2024-08-20 19:03:00,415 WARNING /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[161]: change system to user\r\n2024-08-20 19:03:00,415 WARNING /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[167]: merge two adjacent user messages\r\n2024-08-20 19:03:00,461 DEBUG /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[185]: get input ids of shape torch.Size([1, 18])\r\n2024-08-20 19:03:00,461 DEBUG /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[240]: input_ids: torch.Size([1, 18])\r\n2024-08-20 19:03:00,462 DEBUG /home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py[262]: cache position: 0 to 18\r\nINFO:     192.168.27.159:46148 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 148, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 613, in forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1238, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n                                                          ^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/operators/attention.py\", line 163, in forward\r\n    return self.forward_chunck(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/operators/attention.py\", line 92, in forward_chunck\r\n    k_pe, compressed_kv = past_key_value.update(k_pe, compressed_kv, self.layer_idx, cache_kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ai/miniconda3/envs/ktransformers/lib/python3.11/site-packages/ktransformers/models/custom_cache.py\", line 105, in update\r\n    k_out[:, :, cache_position] = key_states\r\n    ~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/45/comments",
    "author": "ELigoP",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-08-21T06:31:09Z",
        "body": "Sorry for the inconvenience orz.\r\n1.  You can appoint optimise yaml rule by `--optimize_config_path` now. I have fixed by #48 . \r\n\r\n2. `Expected all tensors to be on the same device`\r\nOk this is a bug that server creating static cache on one device. Also fixed by #48 . \r\n\r\n"
      }
    ]
  },
  {
    "number": 32,
    "title": "ollama chat not realised",
    "created_at": "2024-08-12T17:14:51Z",
    "closed_at": "2025-02-11T09:25:39Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/32",
    "body": "Error when using api chat with vscode's continue plugin\r\n```\r\n  File \"E:\\open-webui\\backend\\python311\\Lib\\site-packages\\ktransformers\\server\\api\\ollama\\completions.py\", line 96, in chat\r\n    raise NotImplementedError\r\nNotImplementedError\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/32/comments",
    "author": "xldistance",
    "comments": [
      {
        "user": "xldistance",
        "created_at": "2024-08-13T01:25:18Z",
        "body": "When will ollama api chat and autocomplete be supported?"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-15T16:50:44Z",
        "body": "In the past few days, we have been focusing on enhancing support for native Windows, multiple GPUs......, the next version we will support long context and kv cache store.\r\nSo, in the following version, we will support the Ollama api chat."
      }
    ]
  },
  {
    "number": 22,
    "title": "error with ffn_down ",
    "created_at": "2024-08-04T17:38:23Z",
    "closed_at": "2024-08-05T13:18:29Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/22",
    "body": "I try to load ffn_weight and run it with cpuinfer_ext.linear. ffn_up and ffn_gate are good. But ffn_down result in nan. What is wrong\r\ncode is below \r\n```\r\n\r\nfrom ktransformers.util.custom_gguf import GGUFLoader\r\nimport cpuinfer_ext\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport psutil\r\nimport os\r\nimport ctypes\r\ngguf_path = \"/VM/share/models/Mixtral-8x7b-q4k_m/\"\r\ngguf_loader=GGUFLoader(gguf_path)\r\n# print(gguf_loader.tensor_info)\r\ntensor = gguf_loader.get_mmap_tensor(\"blk.0.ffn_down.0.weight\")\r\nprint(tensor)\r\na = torch.tensor(tensor,dtype= torch.uint8)\r\nprint(a.nbytes)\r\nprint(a.shape)\r\ntensor_fp32 = gguf_loader.load_gguf_tensor(\"blk.0.ffn_down.0.weight\").to(torch.bfloat16)\r\nprint(tensor_fp32.shape)\r\ngate_ptr = ctypes.addressof(\r\n            ctypes.cast(tensor.ctypes.data, ctypes.POINTER(ctypes.c_uint64)).contents\r\n        )\r\noutput_size =  4096\r\ninput_size = 14336\r\nstride = 16\r\nproj_type=12\r\nhidden_type=30\r\nCPUInfer = cpuinfer_ext.CPUInfer(32)\r\nconfig = cpuinfer_ext.linear.LinearConfig(input_size, output_size, stride, gate_ptr, proj_type, hidden_type)\r\nlinear = cpuinfer_ext.linear.Linear(config)\r\n\r\ninput = torch.randn((2, input_size), dtype=torch.bfloat16).contiguous()\r\noutput = torch.zeros((2, output_size), dtype=torch.bfloat16).contiguous()\r\nfor i in range(2):\r\n    CPUInfer.submit(linear.forward, input[i,:].data_ptr(), output[i,:].data_ptr())\r\nCPUInfer.sync()\r\nprint(output)\r\nout = F.linear(input,tensor_fp32)\r\nprint(out)\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/22/comments",
    "author": "Eutenacity",
    "comments": [
      {
        "user": "chenht2022",
        "created_at": "2024-08-05T05:13:49Z",
        "body": "I ran your script and got the following results, which seem to be fine.\r\n```\r\n(base) chenht@sapphire2:~/ktransformers-dev$ python test.py \r\n[151   3 227 ...   8 169 135]\r\n33030144\r\ntorch.Size([33030144])\r\ntorch.Size([4096, 14336])\r\ntensor([[ 0.2041, -3.1406, -0.9922,  ..., -1.1406,  0.3828,  0.4023],\r\n        [-1.7188,  0.3164,  0.5352,  ...,  1.5000,  1.5234,  1.0547]],\r\n       dtype=torch.bfloat16)\r\ntensor([[ 0.2070, -3.1562, -0.9883,  ..., -1.1484,  0.3828,  0.3984],\r\n        [-1.7109,  0.3262,  0.5469,  ...,  1.5000,  1.5156,  1.0547]],\r\n       dtype=torch.bfloat16)\r\n```\r\nMaybe you should check if your gguf file is intact and if the ffn_down weights are indeed q4_k quantized.\r\nIf you confirm that these two aspects are fine, please provide more information (e.g., use `lscpu` to show your CPU instruction set) to help us fix it."
      },
      {
        "user": "Eutenacity",
        "created_at": "2024-08-05T13:18:26Z",
        "body": "Many thanks for your quick response. I found the reason. The gguf I download has a different ggml_type down ffn compared to up ffn."
      }
    ]
  },
  {
    "number": 20,
    "title": "If I want to run a linear layer with q4_k_m on cpu using lamafile, how to do it with your implement",
    "created_at": "2024-08-03T12:50:18Z",
    "closed_at": "2024-08-04T06:13:14Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/20",
    "body": "I found the bench_linear.py. But the weight is fp32 not the quantized uint8. Can you give me a example ? thanks",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/20/comments",
    "author": "Eutenacity",
    "comments": [
      {
        "user": "chenht2022",
        "created_at": "2024-08-04T04:40:53Z",
        "body": "You are correct. For testing convenience, we randomly generated some FP32 weights in our bench program and let llamafile_sgemm treat them as quantized weights. The output obtained this way is meaningless; we use this bench program solely to test performance. In actual use, the weights are mmap-ed from gguf rather than randomly generated, ensuring that the computation results are meaningful. \r\n\r\nThe injection of the linear layer is coming soon, and you will then be able to run a linear layer on the CPU using llamafile by modifying a YAML configuration file."
      }
    ]
  },
  {
    "number": 15,
    "title": "GPU support without fp16. Multi gpu support",
    "created_at": "2024-07-29T12:14:28Z",
    "closed_at": "2024-07-29T13:46:27Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/15",
    "body": "Hello, I am very excited about this project. I wanted to ask if ktransformers supports or will support video cards that do not work with fp16? Is there multi gpu support like llama cpp?",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/15/comments",
    "author": "AlexBefest",
    "comments": [
      {
        "user": "james0zan",
        "created_at": "2024-07-29T12:31:28Z",
        "body": "We are working on multi-gpu suppport. But the support for non-fp16 card may not come soon, because we do not have such devices for experiemnting and testing."
      }
    ]
  },
  {
    "number": 13,
    "title": "About 1M ctx models",
    "created_at": "2024-07-29T09:33:53Z",
    "closed_at": "2024-08-29T18:23:43Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/13",
    "body": "I saw elsewhere that you mentioned optimization for 1M ctx models (glm-4-9b-chat-1m/internlm2_5-7b-chat-1m) is on the roadmap and will be released in a future version. And I'm interested to know what kind of improvements you're anticipating achieving.",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/13/comments",
    "author": "choyakawa",
    "comments": [
      {
        "user": "james0zan",
        "created_at": "2024-07-30T14:04:34Z",
        "body": "We intend to take advantage from the sparsity of attention operator. There are many great work study on this topic and we are evaluating and integrating some of them into the KTransformers framework."
      }
    ]
  },
  {
    "number": 11,
    "title": "Is Flash Attention 2 Necessary for Qwen2Moe?",
    "created_at": "2024-07-29T07:57:18Z",
    "closed_at": "2024-07-29T08:51:56Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/11",
    "body": "I tried to run Qwen2Moe but I only have access to T4 GPUs, which don't support Flash Attention 2.\r\nIs Flash Attention 2 a strict requirement for Qwen2Moe?\r\nAny guidance would be greatly appreciated!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/11/comments",
    "author": "cherrymorning",
    "comments": [
      {
        "user": "Atream",
        "created_at": "2024-07-29T08:51:56Z",
        "body": "Flash Attention 2 is required for Qwen2_moe, or it may overflow. This is Qwen2_moe's own character."
      }
    ]
  },
  {
    "number": 4,
    "title": "Native windows support",
    "created_at": "2024-07-27T10:50:41Z",
    "closed_at": "2024-08-16T07:27:45Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/4",
    "body": "First of all thanks for your AI community contribution, it's a huge leap forward to use MoE models for consumer grade users with limited VRAM.\r\nWould it be possible to add native Windows support to KTransformers? I'd love to see the project become accessible to windows users as well.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/4/comments",
    "author": "DotNetDevlll",
    "comments": [
      {
        "user": "azywait",
        "created_at": "2024-07-27T12:32:49Z",
        "body": "> it be possible to add native Windows support to KTransformers? I'd love to see the project become accessible to windows users as well.\r\n> \r\n> Thanks!\r\n\r\nThanks for your interest. Native Windows support is in our plans, but it may take some time.  😊"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T03:14:13Z",
        "body": "Has anyone tried to run it under windows?"
      },
      {
        "user": "Atream",
        "created_at": "2024-08-12T03:18:17Z",
        "body": "You can try to install from source by running install.bat. Pre-built wheels will be released soon."
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T14:01:04Z",
        "body": ".\\install.bat \r\n\r\n```\r\nInstalling ktransformers\r\nProcessing c:\\users\\pc\\ktransformers\\ktransformers\\ktransformers\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × Preparing metadata (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [17 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\r\n          return hook(metadata_directory, config_settings)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\setuptools\\build_meta.py\", line 368, in prepare_metadata_for_build_wheel\r\n          self.run_setup()\r\n        File \"C:\\Users\\pc\\miniconda3\\Lib\\site-packages\\setuptools\\build_meta.py\", line 313, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 294, in <module>\r\n        File \"<string>\", line 132, in get_package_version\r\n        File \"<string>\", line 54, in get_cuda_bare_metal_version\r\n      TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\nInstallation completed successfully\r\n```"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-12T15:30:08Z",
        "body": "There might be a few possibilities here.\r\n\r\n1. CUDA is not installed on your machine.\r\n2. The CUDA environment variables are not active.\r\n3. The installed PyTorch is not for GPU, but for CPU.\r\n\r\nThere is a simple env check code\r\n```python\r\nimport torch\r\nimport subprocess\r\nfrom torch.utils.cpp_extension import CUDA_HOME\r\nprint(\"torch version is: \" + str(torch.__version__))\r\nprint(\"CUDA HOME is: \" + str(CUDA_HOME))\r\nraw_output = subprocess.check_output([str(CUDA_HOME) + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\r\nprint(\"nvcc version is : \" + raw_output)\r\n```\r\nThe output of my computer is:\r\n\r\n> torch version is: 2.4.0+**cu124**\r\n> CUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v12.5\r\n> nvcc version is : nvcc: NVIDIA (R) Cuda compiler driver\r\n> Copyright (c) 2005-2024 NVIDIA Corporation\r\n> Built on Wed_Apr_17_19:36:51_Pacific_Daylight_Time_2024\r\n> Cuda compilation tools, release 12.5, V12.5.40\r\n> Build cuda_12.5.r12.5/compiler.34177558_0\r\n\r\nCould you show me what the output is on your computer?"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-12T16:15:41Z",
        "body": "torch version is: 2.4.0+cpu\r\nCUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6\r\nnvcc version is : nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024\r\nCuda compilation tools, release 12.6, V12.6.20\r\nBuild cuda_12.6.r12.6/compiler.34431801_0\r\n\r\nthanks for your quick reply, after fixup some torch things ...\r\n\r\n```\r\npython -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\nERROR: The function received no value for the required argument: model_path\r\nUsage: local_chat.py MODEL_PATH <flags>\r\n  optional flags:        --optimize_rule_path | --gguf_path |\r\n                         --max_new_tokens | --cpu_infer\r\n\r\nFor detailed information on this command, run:\r\n  local_chat.py --help\r\n```"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-08-12T17:45:41Z",
        "body": "> torch version is: 2.4.0+cpu CUDA HOME is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6 nvcc version is : nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2024 NVIDIA Corporation Built on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024 Cuda compilation tools, release 12.6, V12.6.20 Build cuda_12.6.r12.6/compiler.34431801_0\r\n> \r\n> thanks for your quick reply, after fixup some torch things ...\r\n> \r\n> ```\r\n> python -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\n> ERROR: The function received no value for the required argument: model_path\r\n> Usage: local_chat.py MODEL_PATH <flags>\r\n>   optional flags:        --optimize_rule_path | --gguf_path |\r\n>                          --max_new_tokens | --cpu_infer\r\n> \r\n> For detailed information on this command, run:\r\n>   local_chat.py --help\r\n> ```\r\n\r\nPerhaps you can input --model_path instead of --model_name.\r\n```\r\npython -m ktransformers.local_chat --model_path Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF\r\n```\r\n"
      },
      {
        "user": "whisper-bye",
        "created_at": "2024-08-13T12:54:23Z",
        "body": "Great, it works!\r\ni3900k+msi 4090 + ram 96GB\r\ngpu mem 6.3/24GB\r\nram 35GB maybe\r\n```\r\nChat: 数字9.11和9.9谁大？ \r\n数字9.11和9.9中，9.11比9.9小。这可以通过将两个数字转换为分数来更清楚地看到，其中9.11为911/1000和9.9为99/10。比较两个分数，911/1000小于99/10，因此9.11小于9.9。\r\nprompt eval count:    31 token(s)\r\nprompt eval duration: 0.757000207901001s\r\nprompt eval rate:     40.95111160663528 tokens/s\r\neval count:           90 token(s)\r\neval duration:        6.651063442230225s\r\neval rate:            13.531670654132467 tokens/s\r\n```"
      }
    ]
  }
]