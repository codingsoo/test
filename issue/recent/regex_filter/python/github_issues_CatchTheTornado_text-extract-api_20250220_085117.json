[
  {
    "number": 105,
    "title": "Feature/98 universal text object",
    "created_at": "2025-01-20T01:10:55Z",
    "closed_at": "2025-01-20T22:22:10Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/pull/105",
    "body": "1. #98 \r\nRequired for #54 \r\n\r\nThis commit introduces a new ExtractResult class which replaces the use of a primitive type (string) for extract return values. This modification preserved vital metadata about the document which was earlier lost when just the resulting string was returned. This change also involved modifying the strategy interface as well as its execution to use the new ExtractResult class rather than primitive string.\r\n\r\n\r\n```\r\nIMPORTANT INFORMATION ABOUT THIS CLASS:\r\nThis is not the final version of the object, namespace, or intended use. \r\nFor this reason, I am not creating an interface, etc. Add code here as soon as possible \r\nalong with further integrations, and once we have gained sufficient experience, we will \r\nundertake a refactor.\r\nCurrently, the object's purpose is to replace the use of a primitive type, a string, for \r\nextract returns. The limitation of this approach became evident when returning only the \r\nresulting string caused us to lose valuable metadata about the document. Thanks to this \r\nclass, we retain Doc\r\n```\r\n\r\n2. Fix - storage filename was always default",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/105/comments",
    "author": "choinek",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2025-01-20T10:03:51Z",
        "body": "@choinek looks cool, merge as soon as you're ready"
      }
    ]
  },
  {
    "number": 93,
    "title": "LLama 3.2-vision without second stage OCR",
    "created_at": "2025-01-18T12:28:21Z",
    "closed_at": "2025-01-19T01:37:34Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/issues/93",
    "body": "Hi,\n\nThank you for providing this service. I see that LLaMA 3.2-vision is already quite capable and might not need a second stage OCR. To my knowledge, the Python wrapper currently does not allow for this.\n\nAm I doing something wrong?\n\n",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/93/comments",
    "author": "giuliofrey",
    "comments": [
      {
        "user": "choinek",
        "created_at": "2025-01-18T17:32:38Z",
        "body": "Hi!\n\nThe service is not performing the second stage of OCR.\n\nWe have two phases (or stages as you referred to):  \n\n1. **Parameter: \"strategy\" (initial phase)** – This phase converts the document into a unified format that can be processed by AI\n   - Currently, we only have OCR strategies for PDFs and images. However, we are working on Docling support, which will enable us to handle almost all types of text documents (e.g., word files) via our API – see #54\n\n2. **Parameter: \"model\" (final phase)** – This phase uses AI to structure the document into its final format\n\nIn the near future, we will create documentation to provide an architectural visualization of the entire process. :)\n\nBut please clarify what you mean by \"Python wrapper currently does not allow for this.\"  \nI'm not sure what you're referring to. Are you encountering any errors? If so, could you share more details?"
      }
    ]
  },
  {
    "number": 65,
    "title": "add api ",
    "created_at": "2025-01-10T14:47:50Z",
    "closed_at": "2025-01-11T09:04:19Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/issues/65",
    "body": "add api ",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/65/comments",
    "author": "Alaaibrahim2",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2025-01-10T17:46:08Z",
        "body": "What do you mean? :)"
      }
    ]
  },
  {
    "number": 52,
    "title": "Build error on python 3.10",
    "created_at": "2025-01-02T11:48:28Z",
    "closed_at": "2025-01-08T11:38:39Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/issues/52",
    "body": "```\r\n/var/tmp/pdf-extract-api on main > python3.10 -m venv .venv                                                                                                                                                            at 12:44:16\r\n/var/tmp/pdf-extract-api on main > source .venv/bin/activate                                                                                                                                                   took 3s at 12:44:42\r\n\r\n/var/tmp/pdf-extract-api on main > pip install -r app/requirements.txt                                                                                                                              py pdf-extract-api at 12:44:53\r\nCollecting fastapi\r\n  Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.8/94.8 KB 1.7 MB/s eta 0:00:00\r\nCollecting celery\r\n  Downloading celery-5.4.0-py3-none-any.whl (425 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 426.0/426.0 KB 5.5 MB/s eta 0:00:00\r\nCollecting redis\r\n  Downloading redis-5.2.1-py3-none-any.whl (261 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 261.5/261.5 KB 6.3 MB/s eta 0:00:00\r\nCollecting pytesseract\r\n  Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\r\nCollecting opencv-python-headless\r\n  Downloading opencv-python-headless-4.10.0.84.tar.gz (95.1 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.1/95.1 MB 20.3 MB/s eta 0:00:00\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nCollecting pdf2image\r\n  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\r\nCollecting ollama\r\n  Downloading ollama-0.4.5-py3-none-any.whl (13 kB)\r\nCollecting uvicorn[standard]\r\n  Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.3/62.3 KB 3.4 MB/s eta 0:00:00\r\nCollecting requests\r\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 KB 3.4 MB/s eta 0:00:00\r\nCollecting python-multipart\r\n  Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\r\nCollecting pdftext==0.3.7\r\n  Downloading pdftext-0.3.7-py3-none-any.whl (24 kB)\r\nCollecting argparse\r\n  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-api-python-client\r\n  Downloading google_api_python_client-2.156.0-py2.py3-none-any.whl (12.7 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.7/12.7 MB 50.7 MB/s eta 0:00:00\r\nCollecting google-auth-httplib2\r\n  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\r\nCollecting google-auth-oauthlib\r\n  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\r\nCollecting transformers==4.45.2\r\n  Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 54.8 MB/s eta 0:00:00\r\nCollecting surya-ocr==0.4.14\r\n  Downloading surya_ocr-0.4.14-py3-none-any.whl (94 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.5/94.5 KB 4.3 MB/s eta 0:00:00\r\nCollecting marker-pdf==0.2.6\r\n  Downloading marker_pdf-0.2.6-py3-none-any.whl (61 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.9/61.9 KB 3.8 MB/s eta 0:00:00\r\nCollecting boto3\r\n  Downloading boto3-1.35.90-py3-none-any.whl (139 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.2/139.2 KB 8.6 MB/s eta 0:00:00\r\nCollecting pydantic<3.0.0,>=2.7.1\r\n  Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 431.8/431.8 KB 19.9 MB/s eta 0:00:00\r\nCollecting pydantic-settings<3.0.0,>=2.2.1\r\n  Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\r\nCollecting pypdfium2<5.0.0,>=4.29.0\r\n  Downloading pypdfium2-4.30.1-py3-none-macosx_10_13_x86_64.whl (2.9 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 44.3 MB/s eta 0:00:00\r\nCollecting scikit-learn<2.0.0,>=1.4.2\r\n  Downloading scikit_learn-1.6.0-cp310-cp310-macosx_10_9_x86_64.whl (12.1 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 48.1 MB/s eta 0:00:00\r\nCollecting huggingface-hub<1.0,>=0.23.2\r\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 450.5/450.5 KB 21.3 MB/s eta 0:00:00\r\nCollecting regex!=2019.12.17\r\n  Downloading regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl (287 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 287.7/287.7 KB 14.0 MB/s eta 0:00:00\r\nCollecting pyyaml>=5.1\r\n  Downloading PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl (184 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.2/184.2 KB 9.5 MB/s eta 0:00:00\r\nCollecting packaging>=20.0\r\n  Using cached packaging-24.2-py3-none-any.whl (65 kB)\r\nCollecting filelock\r\n  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\r\nCollecting safetensors>=0.4.1\r\n  Downloading safetensors-0.4.5-cp310-cp310-macosx_10_12_x86_64.whl (392 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 392.3/392.3 KB 18.6 MB/s eta 0:00:00\r\nCollecting numpy>=1.17\r\n  Using cached numpy-2.2.1-cp310-cp310-macosx_10_9_x86_64.whl (21.2 MB)\r\nCollecting tqdm>=4.27\r\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 KB 4.2 MB/s eta 0:00:00\r\nCollecting tokenizers<0.21,>=0.20\r\n  Downloading tokenizers-0.20.3-cp310-cp310-macosx_10_12_x86_64.whl (2.7 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 18.3 MB/s eta 0:00:00\r\nCollecting opencv-python<5.0.0.0,>=4.9.0.80\r\n  Downloading opencv-python-4.10.0.84.tar.gz (95.1 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.1/95.1 MB 20.0 MB/s eta 0:00:00\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nCollecting tabulate<0.10.0,>=0.9.0\r\n  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nCollecting pillow<11.0.0,>=10.2.0\r\n  Downloading pillow-10.4.0-cp310-cp310-macosx_10_10_x86_64.whl (3.5 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 10.1 MB/s eta 0:00:00\r\nCollecting ftfy<7.0.0,>=6.1.3\r\n  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.8/44.8 KB 2.6 MB/s eta 0:00:00\r\nCollecting filetype<2.0.0,>=1.2.0\r\n  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\r\nCollecting python-dotenv<2.0.0,>=1.0.0\r\n  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\nINFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of pdftext to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Could not find a version that satisfies the requirement torch<3.0.0,>=2.3.0 (from surya-ocr) (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)\r\nERROR: No matching distribution found for torch<3.0.0,>=2.3.0\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/52/comments",
    "author": "antwal",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2025-01-07T18:19:33Z",
        "body": "Oh, it might be a problem, we've tested and developed the product on Python 3.12 and 3.13. If it's possible for you please do upgrade, otherwise it will require some investigation on the dependencies "
      }
    ]
  },
  {
    "number": 45,
    "title": "add missing poppler deps in dockerfile",
    "created_at": "2024-11-22T17:24:11Z",
    "closed_at": "2024-11-23T13:20:44Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/pull/45",
    "body": null,
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/45/comments",
    "author": "PasaOpasen",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2024-11-23T13:20:58Z",
        "body": "Thanks @PasaOpasen!"
      }
    ]
  },
  {
    "number": 34,
    "title": "Update README.md to remove that extra \"`\" in cloning .env codeblock",
    "created_at": "2024-11-14T00:28:38Z",
    "closed_at": "2024-11-14T11:19:19Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/pull/34",
    "body": null,
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/34/comments",
    "author": "hahouari",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2024-11-14T11:19:25Z",
        "body": "Thanks @hahouari \r\n"
      },
      {
        "user": "hahouari",
        "created_at": "2024-11-14T12:53:30Z",
        "body": "No problem, keep up the good work ✨."
      }
    ]
  },
  {
    "number": 28,
    "title": "OCR task failed.",
    "created_at": "2024-11-11T08:17:52Z",
    "closed_at": "2024-11-12T01:30:08Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/issues/28",
    "body": "hello, I used an Apple device to reproduce locally, but encountered the following situation:\r\n\r\n{'state': 'FAILURE', 'status': 'Server disconnected without sending a response.'}\r\nOCR task failed.\r\n\r\nI use :  python client/cli.py ocr --file examples/example-mri.pdf --ocr_cache --prompt_file=examples/example-mri-remove-pii.txt\r\nhow to solve this problem?",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/28/comments",
    "author": "PoleGeogry",
    "comments": [
      {
        "user": "PoleGeogry",
        "created_at": "2024-11-12T01:30:08Z",
        "body": "turn off VPN"
      },
      {
        "user": "pkarw",
        "created_at": "2024-11-12T13:25:43Z",
        "body": "Thanks @PoleGeogry! It might be usefull for other users as well, the problem with VPN was with OCR trying to get weights from internet or by accessing local server ports?"
      }
    ]
  },
  {
    "number": 20,
    "title": "Add MetaData LLM call",
    "created_at": "2024-11-05T08:07:58Z",
    "closed_at": "2025-01-08T10:27:45Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/pull/20",
    "body": "Related to #16\n\nAdd an optional LLM call for generating tags and summary of the file.\n\n* **app/main.py**\n  - Add a new endpoint `/llm_tags_summary` to generate tags and summary using the LLM.\n  - Update the `OllamaGenerateRequest` class to include a new field `generate_tags_summary`.\n  - Update the `generate_llama` function to handle the new `generate_tags_summary` field.\n\n* **app/tasks.py**\n  - Add a new function `generate_tags_summary` to generate tags and summary using the LLM.\n  - Update the `ocr_task` function to include an optional call to `generate_tags_summary` after extracting text.\n\n* **client/cli.py**\n  - Add a new command `llm_tags_summary` for generating tags and summary.\n  - Update the `main` function to handle the new `llm_tags_summary` command.\n\n* **.env.example**\n  - Add a new environment variable `LLM_TAGS_SUMMARY_API_URL`.\n\n",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/20/comments",
    "author": "chavan-arvind",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2024-11-05T19:34:17Z",
        "body": "Thanks this is cool!\n\nI requested minor changes. When applied and when #10 merged I'll also ask you to extend this feature for the metadata to be used within storage strategies to format file names (to use the tags or other fields within file names)"
      },
      {
        "user": "pkarw",
        "created_at": "2024-11-05T19:35:58Z",
        "body": "One more thing - please use the defined prompt (can be a env variable with some nice default) to have the prompt used for metadata configurable \n\nIt should return JSON object with metadata:\n\n```json\n{\n title: \"\",\n filename_title: \"\",\n tags: \"\",\n \"summary\": \"\"\n\n```"
      },
      {
        "user": "pkarw",
        "created_at": "2025-01-08T10:28:02Z",
        "body": "Closing due to long inactivity"
      }
    ]
  },
  {
    "number": 18,
    "title": "Add S3 storage strategy",
    "created_at": "2024-11-05T08:00:58Z",
    "closed_at": "2024-11-18T11:08:18Z",
    "labels": [],
    "url": "https://github.com/CatchTheTornado/text-extract-api/pull/18",
    "body": "Related to #15\n\nAdd S3 storage strategy for output files.\n\n* **New S3OCRStrategy Class**: Add `S3OCRStrategy` class in `app/ocr_strategies/s3.py` implementing the `OCRStrategy` interface using `boto3` to interact with AWS S3.\n* **Update OCR Strategies**: Import `S3OCRStrategy` in `app/tasks.py` and add `s3` strategy to `OCR_STRATEGIES` dictionary.\n* **Add AWS Configuration**: Add AWS S3 configuration variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_S3_BUCKET_NAME`, `AWS_REGION`) in `.env.example`.\n* **Add Dependency**: Add `boto3` dependency in `app/requirements.txt`.\n\n",
    "comments_url": "https://api.github.com/repos/CatchTheTornado/text-extract-api/issues/18/comments",
    "author": "chavan-arvind",
    "comments": [
      {
        "user": "pkarw",
        "created_at": "2024-11-05T18:38:34Z",
        "body": "Hey @chavan-arvind! Thanks for this PR isn't it a duplicate of #19?"
      },
      {
        "user": "pkarw",
        "created_at": "2024-11-05T18:40:14Z",
        "body": "I think you should review #10 as it's definitely not a right way to define the storage strategy as ocr strategy. Please check PR #10 and try to apply the s3 strategy accordingly based on it ok?\n\nI cannot accept this PR as it is right now due the architecture concerns "
      }
    ]
  }
]