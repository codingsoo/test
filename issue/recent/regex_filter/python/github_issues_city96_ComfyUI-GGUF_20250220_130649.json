[
  {
    "number": 198,
    "title": "Can this be used to run LLMs like Llama 3.2 with text input & output?",
    "created_at": "2025-01-08T17:20:58Z",
    "closed_at": "2025-01-08T18:41:54Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/198",
    "body": "Instead of having to setup Ollama and integrate it, can the text LLMs run directly in ComfyUI with this?",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/198/comments",
    "author": "TremendaCarucha",
    "comments": [
      {
        "user": "city96",
        "created_at": "2025-01-08T18:30:23Z",
        "body": "No this is just a wrapper for running image models and text encoders already supported by ComfyUI, except quantized and saved as a GGUF file.\r\n\r\nSo for actual LLMs you're probably still better off with ollama or the llama.cpp server example's openai-compatible endpoint. Unsure if there's any custom nodes that make use of either as I haven't messed much with LLM prompt enhancement/etc."
      },
      {
        "user": "TremendaCarucha",
        "created_at": "2025-01-08T18:41:54Z",
        "body": "Thank you for the swift response @city96!"
      }
    ]
  },
  {
    "number": 197,
    "title": "error patch line 27 #90",
    "created_at": "2025-01-08T14:57:04Z",
    "closed_at": "2025-01-08T15:45:44Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/197",
    "body": "can some one repost a hint \r\nthe image is gone \r\n#90\r\n",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/197/comments",
    "author": "kalle07",
    "comments": [
      {
        "user": "city96",
        "created_at": "2025-01-08T15:45:44Z",
        "body": "Huh, image is still there for me.\r\n\r\nAnyway, it's just right-clicking the part where it says \"Windows (CR LF)\" in the bottom right corner of Notepad++ and changing it to \"Unix (LF)\".\r\n\r\nI also updated the original post to include this same text instruction."
      }
    ]
  },
  {
    "number": 195,
    "title": "Support for PixArt models pleaseüôè",
    "created_at": "2025-01-04T15:29:15Z",
    "closed_at": "2025-01-08T15:39:37Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/195",
    "body": "He visto que ComfyUI ahora soporta de forma nativa estos modelos tanto Sigma como Alpha pero el modelo t5 que uso es un Q6 dada la poca capacidad de memoria que tengo, agradecer√≠a infinitamente que incluyeras la configuraci√≥n para este CLIP por favor. Saludos desde la isla de Cuba amigo",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/195/comments",
    "author": "Mike257art",
    "comments": [
      {
        "user": "city96",
        "created_at": "2025-01-04T20:54:49Z",
        "body": "Sorry, forgot to add it after the PixArt PR got merged in comfy. Should be working now."
      },
      {
        "user": "Mike257art",
        "created_at": "2025-01-05T03:52:24Z",
        "body": "Gracias por tu r√°pida ayuda amigo un gran abrazo desde la isla de Cuba!!"
      }
    ]
  },
  {
    "number": 169,
    "title": "UnetLoaderGGUF cannot reshape array of size 615776 into shape (12288,1728)   \"i spent 2 days and wdidn't resolved please any solution",
    "created_at": "2024-12-03T16:47:06Z",
    "closed_at": "2024-12-07T08:23:30Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/169",
    "body": "!!! Exception during processing !!! cannot reshape array of size 615776 into shape (12288,1728)\r\nTraceback (most recent call last):\r\n  File \"/workspace/ComfyUI/execution.py\", line 324, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/workspace/ComfyUI/execution.py\", line 199, in get_output_data\r\n    return_values = mapnode_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/workspace/ComfyUI/execution.py\", line 170, in mapnode_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/workspace/ComfyUI/execution.py\", line 159, in process_inputs\r\n    results.append(getattr(obj, func)(inputs))\r\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-GGUF/nodes.py\", line 258, in load_unet\r\n    sd = gguf_sd_loader(unet_path)\r\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-GGUF/nodes.py\", line 39, in gguf_sd_loader\r\n    reader = gguf.GGUFReader(path)\r\n  File \"/workspace/ComfyUI/venv/lib/python3.10/site-packages/gguf/gguf_reader.py\", line 130, in init**\r\n    self._build_tensors(offs, tensors_fields)\r\n  File \"/workspace/ComfyUI/venv/lib/python3.10/site-packages/gguf/gguf_reader.py\", line 314, in buildtensors\r\n    data = self._get(data_offs, item_type, item_count).reshape(np_dims),\r\nValueError: cannot reshape array of size 615776 into shape (12288,1728)\" the error with unet loader gguf",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/169/comments",
    "author": "habibteyeb",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-12-04T00:04:08Z",
        "body": "It could be a file integrity issue, since the error happens within the gguf package.\r\n\r\nDid you verify the integrity of the model file? (You can find the sha256 hash next to the file on huggingface.)\r\n\r\nIf yes, could you link to which file was used as well as some extra info about the env like pytorch/gguf version/etc."
      },
      {
        "user": "habibteyeb",
        "created_at": "2024-12-07T08:23:15Z",
        "body": "well thank you, my gguf model doesn't completed installing."
      }
    ]
  },
  {
    "number": 162,
    "title": "please add ltxv support to clip loader module",
    "created_at": "2024-11-22T17:21:03Z",
    "closed_at": "2024-11-22T23:12:49Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/162",
    "body": "with the addition of ltxv to comfy, the gguf clip loader needs to be able to output the new ltxv model layout i think",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/162/comments",
    "author": "cchance27",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-11-22T23:12:49Z",
        "body": "You would be correct. Added it to the clip loader node."
      }
    ]
  },
  {
    "number": 137,
    "title": "GGUF not working with aidock comfyui template",
    "created_at": "2024-10-23T16:42:23Z",
    "closed_at": "2024-10-24T01:13:03Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/137",
    "body": "I have also posted this on the ai dock github but havent gotten a response yet. I have tried many things to run a GGUF model but no matter what i do comfyui can not detect it. It can't detect the clip either. I've put the checkpoint and clip in the right place, downloaded the GGUF node, did what the github instructions said to do but it still cannot detect a GGUF. Regular flux dev and schnell work just fine, GGUF doesnt. This is on vast ai. Is there any chance you or anyone has a docker that works with GGUF?",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/137/comments",
    "author": "cubo94857",
    "comments": [
      {
        "user": "ibadmore",
        "created_at": "2024-11-04T03:06:20Z",
        "body": "did you find out what the issue was? GGUF models not showing up after i add them to unet folder"
      },
      {
        "user": "cubo94857",
        "created_at": "2024-11-05T16:59:01Z",
        "body": "@ibadmore yes lol in my case it was because the file was named in random numbers instead of the actual name which is whatevertheactualnameis.safetensors"
      }
    ]
  },
  {
    "number": 134,
    "title": "Can we perform parallel inference like llama.cpp does?",
    "created_at": "2024-10-21T09:47:23Z",
    "closed_at": "2024-10-26T20:59:27Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/134",
    "body": "Can we perform parallel inference like llama.cpp does?",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/134/comments",
    "author": "WOAI704",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-10-26T20:59:27Z",
        "body": "Do you mean multi-GPU inference across cards? In that case, no, this repo only handles quantization and the actual model uses the default ComfyUI inference code, which currently only uses a single GPU."
      }
    ]
  },
  {
    "number": 114,
    "title": "Previously good workflow now erroring \"AttributeError: module 'comfy.sd' has no attribute 'load_text_encoder_state_dicts'\"",
    "created_at": "2024-09-25T07:11:45Z",
    "closed_at": "2024-09-26T00:57:50Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/114",
    "body": "I upgraded/updated - and all to the good.\r\nI appreciate all the hard work it took to\r\nbring us this fabulous facility in ComfyUI\r\n\r\n~Thank You~",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/114/comments",
    "author": "Torcelllo",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-09-26T00:57:50Z",
        "body": "No worries, glad it works."
      }
    ]
  },
  {
    "number": 110,
    "title": "(IMPORT FAILED)",
    "created_at": "2024-09-20T08:27:14Z",
    "closed_at": "2024-09-21T08:59:27Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/110",
    "body": "(IMPORT FAILED): F:\\ComfyUI_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\r\nTRY FIX -have to do this every time",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/110/comments",
    "author": "eegneus",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-09-20T23:09:19Z",
        "body": "Could you provide some more information? What's the actual error for the import?"
      },
      {
        "user": "eegneus",
        "created_at": "2024-09-21T07:16:43Z",
        "body": " ## ComfyUI-Manager: installing dependencies done.\r\n** ComfyUI startup time: 2024-09-21 10:04:58.322050\r\n** Platform: Windows\r\n** Python version: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\r\n\r\nF:\\ComfyUI_portable\\python_embeded\\Lib\\site-packages\\kornia\\feature\\lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\r\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\r\nNote: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\r\nNumExpr defaulting to 8 threads.\r\nPyTorch version 2.4.0+cu121 available.\r\n\r\n File \"F:\\ComfyUI_portable\\ComfyUI\\nodes.py\", line 1994, in load_custom_node\r\n    module_spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"F:\\ComfyUI_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\__init__.py\", line 7, in <module>\r\n    from .nodes import NODE_CLASS_MAPPINGS\r\n  File \"F:\\ComfyUI_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 3, in <module>\r\n    import gguf\r\nModuleNotFoundError: No module named 'gguf'\r\nCannot import F:\\ComfyUI_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF module for custom nodes: No module named 'gguf'\r\n\r\n[notice] A new release of pip is available: 24.0 -> 24.2\r\n\r\nF:\\ComfyUI_portable\\python_embeded\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\r\n  check_for_updates()\r\nF:\\ComfyUI_portable\\python_embeded\\Lib\\site-packages\\vector_quantize_pytorch\\vector_quantize_pytorch.py:462: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n  @autocast(enabled = False)\r\nF:\\ComfyUI_portable\\python_embeded\\Lib\\site-packages\\vector_quantize_pytorch\\vector_quantize_pytorch.py:647: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n  @autocast(enabled = False)\r\nF:\\ComfyUI_portable\\python_embeded\\Lib\\site-packages\\vector_quantize_pytorch\\finite_scalar_quantization.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n  @autocast(enabled = False)\r\nF:\\ComfyUI_portable\\python_embeded\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nllama-cpp installed\r\nSuccessfully installed py-cord[voice]\r\n\r\n 0.0 seconds (IMPORT FAILED): F:\\ComfyUI_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF"
      },
      {
        "user": "eegneus",
        "created_at": "2024-09-21T08:59:23Z",
        "body": "decided, moved the folder 'gguf' from C:\\...python311\\site-packages\\ to F:\\ComfyUI_portable\\...site-packages\\"
      },
      {
        "user": "city96",
        "created_at": "2024-09-21T21:04:26Z",
        "body": "Did you use the command from the readme to install the package? It shouldn't end up in your system env if you did. (The `-s` switch is important.)\r\n\r\n```\r\n.\\python_embeded\\python.exe -s -m pip install -r .\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\requirements.txt\r\n```"
      },
      {
        "user": "eegneus",
        "created_at": "2024-09-22T05:14:43Z",
        "body": "used without -s :("
      },
      {
        "user": "changlinniao",
        "created_at": "2024-09-23T04:35:06Z",
        "body": "I also encountered the same problem, the loading failedÔºö\r\n0.0 seconds (IMPORT FAILED): D:\\AI\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF "
      }
    ]
  },
  {
    "number": 98,
    "title": "LORA issues",
    "created_at": "2024-09-05T20:30:29Z",
    "closed_at": "2024-09-06T16:13:38Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/98",
    "body": "Before the latest update, LORAs were apparently being applied again after every generation, so after 2-3 generations images started to get messed up.\r\n\r\nAfter the latest update, LORAs cause massive slowdown when generating (2-3 times slower). Then after 2-3 generations, I get this error:\r\n\r\n```\r\n!!! Exception during processing !!! Boolean value of Tensor with more than one value is ambiguous\r\nTraceback (most recent call last):\r\n  File \"C:\\stable-diffusion\\ComfyUI\\execution.py\", line 317, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\execution.py\", line 192, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy_extras\\nodes_custom_sampler.py\", line 612, in sample\r\n    samples = guider.sample(noise.generate_noise(latent), latent_image, sampler, sigmas, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=noise.seed)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\samplers.py\", line 706, in sample\r\n    self.inner_model, self.conds, self.loaded_models = comfy.sampler_helpers.prepare_sampling(self.model_patcher, noise.shape, self.conds)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\sampler_helpers.py\", line 66, in prepare_sampling\r\n    comfy.model_management.load_models_gpu([model] + models, memory_required=memory_required, minimum_memory_required=minimum_memory_required)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\model_management.py\", line 504, in load_models_gpu\r\n    use_more_memory(free_mem - minimum_memory_required, models_already_loaded, d)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\model_management.py\", line 362, in use_more_memory\r\n    extra_memory -= m.model_use_more_vram(extra_memory)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\model_management.py\", line 354, in model_use_more_vram\r\n    return self.model.partially_load(self.device, extra_memory)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\model_patcher.py\", line 524, in partially_load\r\n    self.load(device_to, lowvram_model_memory=current_used + extra_memory, full_load=full_load)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 187, in load\r\n    super().load(*args, force_patch_weights=True, **kwargs)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\comfy\\model_patcher.py\", line 393, in load\r\n    self.patch_weight_to_device(weight_key, device_to=device_to)\r\n  File \"C:\\stable-diffusion\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 151, in patch_weight_to_device\r\n    if patch not in out_weight.patches:\r\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\r\n```",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/98/comments",
    "author": "ManOrMonster",
    "comments": [
      {
        "user": "ManOrMonster",
        "created_at": "2024-09-05T20:48:17Z",
        "body": "I was using the Advanced UNet GGUF loader set to \"Target, Target, True\". When I set to default and off, it's no longer causing the error. There's still quite a bit of slowdown when generating with LORAs though."
      },
      {
        "user": "city96",
        "created_at": "2024-09-06T16:13:38Z",
        "body": "Should be fixed, I don't think there's a reason to allow applying the weight twice so instead of checking the new logic replaces them now. The slowdown is mostly normal, unless you mean compared to previous versions, in which case reopen and I'll look into it."
      }
    ]
  },
  {
    "number": 94,
    "title": "Extreme loss of speed when batching generations after recent updates",
    "created_at": "2024-09-03T14:41:01Z",
    "closed_at": "2024-09-06T16:15:53Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/94",
    "body": "I'll preface by saying that I updated the GGUF loader and ComfyUI at the same time, so I'm not 100% sure which is to blame.\r\n\r\nSince updating on September 3rd, generations have become extremely slow, but I have a suspicion as to why. FWIW, I always use a batch size of 3 as batching offers a reasonable speed boost. It's gone from <5 s/it to around 30 s/it.\r\n\r\nI use the Q8 version of the Flux model, but that doesn't fit completely into my 12GB of VRAM, and the FP16 version of the text encoder, which does fit into VRAM. \r\n\r\nMy interpretation of the previous behaviour is that after processing the prompt, the text encoders would be kicked out of VRAM and the Flux model loaded partially in. I'm not sure that the encoders are being kicked out any more.  I'm getting this in the console before generation begins. Also, GPU power usage is low during generations, suggesting to me that it's mainly accessing the model from system RAM:\r\n\r\nggml_sd_loader:\r\n GGMLQuantizationType.F16      476\r\n GGMLQuantizationType.Q8_0     304\r\ngot prompt\r\nmodel weight dtype torch.bfloat16, manual cast: None\r\nmodel_type FLUX\r\nclip missing: ['text_projection.weight']\r\nRequested to load FluxClipModel_\r\nLoading 1 new model\r\nloaded completely 0.0 9320.35888671875 True\r\nRequested to load Flux\r\nLoading 1 new model\r\nloaded partially 174.04657440185542 173.736328125 0\r\n\r\nThere are two way that I can mitigate this: I can either use the --disable-smart-memory command line switch, or I can use the Force/Set CLIP Device node to run it from CPU. In any case, this didn't used to be necessary.\r\n",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/94/comments",
    "author": "Silanda",
    "comments": [
      {
        "user": "Silanda",
        "created_at": "2024-09-03T15:00:32Z",
        "body": "Ah! I think it's related to the GGUF clip loader.\r\n\r\nEven though I was using the standard FP16 text encoder, I was using the GGUF dual clip loader to give me the option of using the quantized models if need be. \r\n\r\nIf I use the standard Comfy Dual Clip Loader, this problem does not seem to occur.\r\n\r\nEDIT: Scratch that, it still did, just not on the first generation for some reason. The temporary fix seems to work in any case."
      },
      {
        "user": "city96",
        "created_at": "2024-09-03T15:04:09Z",
        "body": "Yeah, the LoRA weight fix seems to mess with model management. I've reverted that part temporarily until a better solution is found. Updating should get you back to the normal behavior."
      }
    ]
  },
  {
    "number": 74,
    "title": "size mismatch when processing prompt",
    "created_at": "2024-08-26T14:35:48Z",
    "closed_at": "2024-08-26T14:39:02Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/74",
    "body": "Prompt executed in 3.75 seconds\r\ngot prompt\r\n**!!! Exception during processing !!! size mismatch, got input (154), mat (154x1280), vec (1)**\r\nTraceback (most recent call last):\r\n  File \"D:\\ComfyUI\\execution.py\", line 317, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\execution.py\", line 192, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"D:\\ComfyUI\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\nodes.py\", line 65, in encode\r\n    output = clip.encode_from_tokens(tokens, return_pooled=True, return_dict=True)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\sd.py\", line 126, in encode_from_tokens\r\n    o = self.cond_stage_model.encode_token_weights(tokens)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\sdxl_clip.py\", line 58, in encode_token_weights\r\n    g_out, g_pooled = self.clip_g.encode_token_weights(token_weight_pairs_g)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\sd1_clip.py\", line 41, in encode_token_weights\r\n    o = self.encode(to_encode)\r\n        ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\sd1_clip.py\", line 229, in encode\r\n    return self(tokens)\r\n           ^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\sd1_clip.py\", line 201, in forward\r\n    outputs = self.transformer(tokens, attention_mask_model, intermediate_output=self.layer_idx, final_layer_norm_intermediate=self.layer_norm_hidden_state, dtype=torch.float32)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\clip_model.py\", line 136, in forward\r\n    x = self.text_model(*args, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\clip_model.py\", line 112, in forward\r\n    x, i = self.encoder(x, mask=mask, intermediate_output=intermediate_output)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\clip_model.py\", line 69, in forward\r\n    x = l(x, mask, optimized_attention)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\clip_model.py\", line 50, in forward\r\n    x += self.self_attn(self.layer_norm1(x), mask, optimized_attention)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\comfy\\clip_model.py\", line 17, in forward\r\n    q = self.q_proj(x)\r\n        ^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\LTG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\ops.py\", line 147, in forward\r\n    x = torch.nn.functional.linear(x, weight, bias)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n**RuntimeError: size mismatch, got input (154), mat (154x1280), vec (1)**",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/74/comments",
    "author": "CHNtentes",
    "comments": [
      {
        "user": "CHNtentes",
        "created_at": "2024-08-26T14:37:58Z",
        "body": "be careful, this @sunshinezxq may shared a harmful file"
      },
      {
        "user": "CHNtentes",
        "created_at": "2024-08-26T14:39:02Z",
        "body": "I found out I did not change the type to flux..."
      },
      {
        "user": "city96",
        "created_at": "2024-08-26T14:56:14Z",
        "body": "Looks like the github spam bots are still at it. It's been a day, I have no idea how they haven't fixed it. I tried reporting them before but alas, nothing was done. They also target the last opened issue in a repository.\r\n\r\nGlad you got the issue figured out though."
      }
    ]
  },
  {
    "number": 56,
    "title": "Error occurred when executing DualCLIPLoaderGGUF:",
    "created_at": "2024-08-21T14:15:20Z",
    "closed_at": "2024-08-21T22:53:26Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/56",
    "body": "I'm getting this errors message wen loading the dual clip Gguf loader, im using the t5-v1_1-xxl-encoder-Q5_K_M.gguf\r\n\r\nError occurred when executing DualCLIPLoaderGGUF:\r\n\r\nmodule 'comfy.sd' has no attribute 'load_text_encoder_state_dicts'\r\n\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 316, in execute\r\noutput_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 191, in get_output_data\r\nreturn_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 168, in _map_node_over_list\r\nprocess_inputs(input_dict, i)\r\nFile \"K:\\ComfyUI\\ComfyUI\\execution.py\", line 157, in process_inputs\r\nresults.append(getattr(obj, func)(**inputs))\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 224, in load_clip\r\nreturn (self.load_patcher(clip_paths, clip_type, self.load_data(clip_paths)),)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"K:\\ComfyUI\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 183, in load_patcher\r\nclip = comfy.sd.load_text_encoder_state_dicts(\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/56/comments",
    "author": "kakachiex2",
    "comments": [
      {
        "user": "a-ru2016",
        "created_at": "2024-08-21T14:19:06Z",
        "body": "Please update comfyUI."
      },
      {
        "user": "kakachiex2",
        "created_at": "2024-08-21T21:55:30Z",
        "body": "It works thanks bro"
      }
    ]
  },
  {
    "number": 41,
    "title": "Error occurred when executing KSampler:  too many values to unpack (expected 2)",
    "created_at": "2024-08-18T07:31:15Z",
    "closed_at": "2024-08-18T07:45:21Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/41",
    "body": "This suddenly appeared:\r\n\r\n ```\r\nError occurred when executing KSampler (Efficient):\r\n\r\ntoo many values to unpack (expected 2)\r\n\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\execution.py\", line 316, in execute\r\noutput_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\execution.py\", line 191, in get_output_data\r\nreturn_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\execution.py\", line 168, in _map_node_over_list\r\nprocess_inputs(input_dict, i)\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\execution.py\", line 157, in process_inputs\r\nresults.append(getattr(obj, func)(**inputs))\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 732, in sample\r\nsamples, images, gifs, preview = process_latent_image(model, seed, steps, cfg, sampler_name, scheduler,\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\efficiency-nodes-comfyui\\efficiency_nodes.py\", line 550, in process_latent_image\r\nsamples = KSampler().sample(model, seed, steps, cfg, sampler_name, scheduler, positive, negative,\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\nodes.py\", line 1429, in sample\r\nreturn common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\nodes.py\", line 1396, in common_ksampler\r\nsamples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-Impact-Pack\\modules\\impact\\sample_error_enhancer.py\", line 9, in informative_sample\r\nreturn original_sample(*args, **kwargs) # This code helps interpret error messages that occur within exceptions but does not have any impact on other operations.\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-AnimateDiff-Evolved\\animatediff\\sampling.py\", line 434, in motion_sample\r\nreturn orig_comfy_sample(model, noise, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-Advanced-ControlNet\\adv_control\\sampling.py\", line 116, in acn_sample\r\nreturn orig_comfy_sample(model, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-Advanced-ControlNet\\adv_control\\utils.py\", line 116, in uncond_multiplier_check_cn_sample\r\nreturn orig_comfy_sample(model, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\sample.py\", line 43, in sample\r\nsamples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI_smZNodes\\smZNodes.py\", line 1447, in KSampler_sample\r\nreturn _KSampler_sample(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 829, in sample\r\nreturn sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI_smZNodes\\smZNodes.py\", line 1470, in sample\r\nreturn _sample(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 729, in sample\r\nreturn cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 716, in sample\r\noutput = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 695, in inner_sample\r\nsamples = sampler.sample(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 600, in sample\r\nsamples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\r\nreturn func(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\k_diffusion\\sampling.py\", line 144, in sample_euler\r\ndenoised = model(x, sigma_hat * s_in, **extra_args)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 299, in __call__\r\nout = self.inner_model(x, sigma, model_options=model_options, seed=seed)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI_smZNodes\\smZNodes.py\", line 993, in __call__\r\nreturn self.predict_noise(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI_smZNodes\\smZNodes.py\", line 1043, in predict_noise\r\nout = super().predict_noise(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 685, in predict_noise\r\nreturn sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 279, in sampling_function\r\nout = calc_cond_batch(model, conds, x, timestep, model_options)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\samplers.py\", line 228, in calc_cond_batch\r\noutput = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-Advanced-ControlNet\\adv_control\\utils.py\", line 68, in apply_model_uncond_cleanup_wrapper\r\nreturn orig_apply_model(self, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\model_base.py\", line 145, in apply_model\r\nmodel_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\ldm\\flux\\model.py\", line 159, in forward\r\nout = self.forward_orig(img, img_ids, context, txt_ids, timestep, y, guidance, control)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\ldm\\flux\\model.py\", line 118, in forward_orig\r\nimg, txt = block(img=img, txt=txt, vec=vec, pe=pe)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\ldm\\flux\\layers.py\", line 148, in forward\r\nimg_mod1, img_mod2 = self.img_mod(vec)\r\n^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\comfy\\ldm\\flux\\layers.py\", line 110, in forward\r\nout = self.lin(nn.functional.silu(vec))[:, None, :].chunk(self.multiplier, dim=-1)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\nreturn self._call_impl(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\nreturn forward_call(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\ops.py\", line 139, in forward\r\nweight, bias = self.get_weights(x.dtype)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\ops.py\", line 118, in get_weights\r\nweight = self.get_weight(self.weight, dtype)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\ops.py\", line 110, in get_weight\r\nweight = dequantize_tensor(tensor, dtype)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\dequant.py\", line 18, in dequantize_tensor\r\nout = dequantize(data, qtype, oshape, dtype=None)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\dequant.py\", line 39, in dequantize\r\nblocks = dequantize_blocks(blocks, block_size, type_size, dtype)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\ComfyUI_windows_portable_nightly_pytorch\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\dequant.py\", line 106, in dequantize_blocks_Q4_0\r\nd, qs = split_block_dims(blocks, 2, 2)\r\n^^^^^\r\n````\r\n\r\nworked fine yesterday so no clue.",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/41/comments",
    "author": "derspanier1",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-08-18T07:45:21Z",
        "body": "Sorry about that, though I tested all of them when I added K quants. Should be fixed now."
      }
    ]
  },
  {
    "number": 25,
    "title": "BUG: module 'comfy.sd' has no attribute 'load_diffusion_model_state_dict'",
    "created_at": "2024-08-16T17:21:26Z",
    "closed_at": "2024-08-16T17:42:20Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/25",
    "body": "When i try to run it i get this error.\r\n\r\n\r\n```\r\nError occurred when executing UnetLoaderGGUF:\r\n\r\nmodule 'comfy.sd' has no attribute 'load_diffusion_model_state_dict'\r\n\r\nFile \"E:\\comfy\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 152, in recursive_execute\r\noutput_data, output_ui = get_output_data(obj, input_data_all)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\comfy\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 82, in get_output_data\r\nreturn_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\comfy\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 75, in map_node_over_list\r\nresults.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"E:\\comfy\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 51, in load_unet\r\nmodel = comfy.sd.load_diffusion_model_state_dict(\r\n```",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/25/comments",
    "author": "cherryboio",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-08-16T17:42:20Z",
        "body": "Duplicate of #23 - update comfyui as instructed in the readme."
      }
    ]
  },
  {
    "number": 23,
    "title": "'load_diffusion_model_state_dict",
    "created_at": "2024-08-16T10:15:35Z",
    "closed_at": "2024-08-16T15:17:12Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/23",
    "body": "got prompt\r\n[rgthree] Using rgthree's optimized recursive execution.\r\n[rgthree]\r\n First run patching recursive_output_delete_if_changed and recursive_will_execute.\r\n[rgthree] Note: If execution seems broken due to forward ComfyUI changes, you can disable the optimization from rgthree settings in ComfyUI.\r\n[Impact Pack] Wildcards loading done.\r\n[Impact Pack] Wildcards loading done.\r\n[Impact Pack] Wildcards loading done.\r\n[Impact Pack] Wildcards loading done.\r\n[Impact Pack] Wildcards loading done.\r\nUsing xformers attention in VAE\r\nUsing xformers attention in VAE\r\nD:\\ComfyUI-aki-v1.3\\custom_nodes\\ComfyUI-GGUF\\ops.py:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\r\n  data = torch.from_numpy(tensor.data)\r\n\r\nggml_sd_loader:\r\n GGMLQuantizationType.F16      476\r\n GGMLQuantizationType.Q4_0     304\r\n\r\n!!! Exception during processing!!! module 'comfy.sd' has no attribute 'load_diffusion_model_state_dict'\r\nTraceback (most recent call last):\r\n  File \"D:\\ComfyUI-aki-v1.3\\execution.py\", line 152, in recursive_execute\r\n    output_data, output_ui = get_output_data(obj, input_data_all)\r\n  File \"D:\\ComfyUI-aki-v1.3\\execution.py\", line 82, in get_output_data\r\n    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\r\n  File \"D:\\ComfyUI-aki-v1.3\\execution.py\", line 75, in map_node_over_list\r\n    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\r\n  File \"D:\\ComfyUI-aki-v1.3\\custom_nodes\\ComfyUI-GGUF\\nodes.py\", line 51, in load_unet\r\n    model = comfy.sd.load_diffusion_model_state_dict(\r\nAttributeError: module 'comfy.sd' has no attribute 'load_diffusion_model_state_dict'\r\nÊèêÁ§∫ÔºöPython ËøêË°åÊó∂ÊäõÂá∫‰∫Ü‰∏Ä‰∏™ÂºÇÂ∏∏„ÄÇËØ∑Ê£ÄÊü•ÁñëÈöæËß£Á≠îÈ°µÈù¢„ÄÇ\r\n\r\nPrompt execu\r\n\r\nThanks sir, how can I solve this problem?",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/23/comments",
    "author": "masterliu2021",
    "comments": [
      {
        "user": "Fannovel16",
        "created_at": "2024-08-16T11:42:13Z",
        "body": "Update ComfyUI"
      },
      {
        "user": "city96",
        "created_at": "2024-08-16T15:17:12Z",
        "body": "As @Fannovel16 says above, you need to be on a recent enough version to support those changes - this is also mentioned in the readme:\r\n\r\n>Make sure your ComfyUI is on a recent-enough version to support custom ops when loading the UNET-only."
      }
    ]
  },
  {
    "number": 18,
    "title": "12GB VRAM getting OOM with Q8_0, but not in Forge.",
    "created_at": "2024-08-15T23:32:44Z",
    "closed_at": "2024-08-16T04:25:56Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/18",
    "body": "Able to use Forge with default settings to run the Q8_0 variant on a 3060 w 12GB VRAM and 32GB sys RAM.\r\n\r\nBut with this node in Comfy I always get OOM trying to load the models up for inference. Tried --lowvram mode too.",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/18/comments",
    "author": "caustiq",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-08-16T02:37:29Z",
        "body": "Just pushed a patch that should fix this. Please test it!"
      },
      {
        "user": "NHLOCAL",
        "created_at": "2024-08-16T04:00:24Z",
        "body": "I also had the same problem with lowvram, now the problem is solved with the update push!\r\n\r\nBut a comment is still received when loading the model:\r\n```\r\nC:\\Users\\Public\\StableSwarmUI\\StableSwarmUI-master\\dlbackend\\comfy\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\dequant.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n  data = torch.tensor(tensor.data)\r\nC:\\Users\\Public\\StableSwarmUI\\StableSwarmUI-master\\dlbackend\\comfy\\ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\r\n```"
      },
      {
        "user": "city96",
        "created_at": "2024-08-16T04:25:56Z",
        "body": "Those log messages can be safely ignored."
      },
      {
        "user": "caustiq",
        "created_at": "2024-08-16T05:42:54Z",
        "body": "Confirmed working here.\r\n\r\nNice one man!"
      },
      {
        "user": "caustiq",
        "created_at": "2024-08-16T07:49:32Z",
        "body": "edit: ran all night with no issues\r\n\r\n==\r\nI did get an OOM after about 20-30 gens of smooth sailing. \r\nI turned off Xorg and ran headless to free up a little more cause I was at 94-96% VRAM usage.\r\nBut it seems like Comfy will just use even more if it can, so still riding close to the edge.\r\nDunno if it's related to your changes so just thought I'd report that one OOM.\r\n\r\n"
      }
    ]
  },
  {
    "number": 15,
    "title": "Q2.0",
    "created_at": "2024-08-15T19:16:58Z",
    "closed_at": "2024-08-15T20:20:08Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/15",
    "body": "Please add Q2.0 quant!",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/15/comments",
    "author": "NarutoHokageSaskeUchihaSuperItachiMan",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-08-15T20:20:08Z",
        "body": "Q2_0 isn't a quant provided by the GGML library, the closest thing is Q2_K but K quants aren't supported and sub 3 bpw ones most likely wouldn't be coherent without imatrix anyway, same as with LLMs."
      }
    ]
  }
]