[
  {
    "number": 98,
    "title": "Plans for Cosmos 2?",
    "created_at": "2025-02-11T05:00:42Z",
    "closed_at": "2025-02-11T05:05:51Z",
    "labels": [],
    "url": "https://github.com/NVIDIA/Cosmos/issues/98",
    "body": "I've been following the development of Cosmos with great interest. While Cosmos 1 is still actively evolving and being refined, I noticed that the project's folder structure uses \"cosmos1\" naming convention, which made me curious about potential plans for future versions like Cosmos 2.\nSo, I have questions as follows. I'll appreciate if some of them can be answered:\n\n1. Are there any plans for a Cosmos 2 version in the future?\n2. Would it involve architectural modifications or different training recipe to the current T2W/V2W approaches? Or would it be related to future Cosmos helpful to Physical AI mentioned in the paper?\n\nThanks for your work and I'll stay tuned on Cosmos!",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/98/comments",
    "author": "2seungeun",
    "comments": [
      {
        "user": "mingyuliutw",
        "created_at": "2025-02-11T05:05:51Z",
        "body": "Yes, we are actively working on improving Cosmos and hope to lead to a better version. We are considering both, better training recipe to the current WFMs and other things we could do to help Physical AI builders."
      }
    ]
  },
  {
    "number": 93,
    "title": "KeyError: 'Trying to restore optimizer state but checkpoint contains only the model.",
    "created_at": "2025-02-08T03:22:20Z",
    "closed_at": "2025-02-14T17:39:51Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/93",
    "body": "When post train the AR model, I encounter the following error:\nKeyError: 'Trying to restore optimizer state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.' \n\nConfusingly, the same code runs successfully weeks ago. Do you have any suggestions?",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/93/comments",
    "author": "yuhuUSTC",
    "comments": [
      {
        "user": "ethanhe42",
        "created_at": "2025-02-11T19:16:02Z",
        "body": "can you try with latest main? our recent update should fix this. "
      },
      {
        "user": "yuhuUSTC",
        "created_at": "2025-02-12T09:05:26Z",
        "body": "> can you try with latest main? our recent update should fix this.\n\nI try the latest code, but meet the same problem. What is the cause of this problem? Can it be solved via modifying certain hyperparameters?"
      },
      {
        "user": "ethanhe42",
        "created_at": "2025-02-12T18:34:12Z",
        "body": "did you follow the latest instruction all over again? it uses a different container"
      },
      {
        "user": "pjannaty",
        "created_at": "2025-02-14T17:39:51Z",
        "body": "Recent update fixes."
      }
    ]
  },
  {
    "number": 84,
    "title": "Using Other Aspect Ratios in the Video2World Model",
    "created_at": "2025-01-26T13:58:33Z",
    "closed_at": "2025-01-27T18:49:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/84",
    "body": "Hi, and thank you for your amazing contribution to the community!\n\nI’m currently using the Cosmos-1.0-Diffusion-7B-Video2World model, where the default aspect ratio is 9:16 (704x1280, height x width). I’ve noticed that the ‘—height’ and ‘—width’ arguments only resize the output resolution (704x1280) and don’t affect the resolution during the video generation process itself.\n\nSince the Video2World model supports various aspect ratios, including 1:1 (960x960), 4:3 (960x704), 3:4 (704x960), 16:9 (1280x704), and 9:16 (704x1280), I’m wondering how to change the resolution for video generation to other aspect ratios.\n\nThanks a lot for your help! I really appreciate it!",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/84/comments",
    "author": "fuxiao0719",
    "comments": [
      {
        "user": "sophiahhuang",
        "created_at": "2025-01-27T18:49:19Z",
        "body": "Thank you for your interest in Cosmos. You can adjust `--height` and `--width` to get 1:1 (960x960), 4:3 (960x704), 3:4 (704x960), 16:9 (1280x704), and 9:16 (704x1280).\n"
      }
    ]
  },
  {
    "number": 83,
    "title": "Crash with cuda 12.8 and rtx5090",
    "created_at": "2025-01-26T08:50:42Z",
    "closed_at": "2025-01-28T00:48:01Z",
    "labels": [
      "bug",
      "enhancement"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/83",
    "body": "Transformer Engine must be updated",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/83/comments",
    "author": "johnnynunez",
    "comments": [
      {
        "user": "pjannaty",
        "created_at": "2025-01-28T00:48:01Z",
        "body": "Great work porting Cosmos to Jetsen. We are actively working on broadening Cosmos' arch repertoire. Due to the large number of possible cuda/arch parings, we cannot guarantee Cosmos will run for all specs.\n\nClosing, but please feel free to reopen and provide a crash log."
      }
    ]
  },
  {
    "number": 82,
    "title": "Generating longer videos with lower resolution using AR model",
    "created_at": "2025-01-24T14:55:36Z",
    "closed_at": "2025-02-14T17:57:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/82",
    "body": "Hello! QQ about my understanding of current architecture of the autoregressive model for video generation.\n\nCurrent limit of frames generated comes from \"max_seq_len\" - how many tokens we keep as context for generating new tokens. We shouldn't generate more tokens to start loosing tokens from beginning.\nBut doesn't it mean that if we set video resolution to 2x smaller (so 4x fewer tokens), we should be able to generate 4x longer videos? If I'm not mistaken it's just the memory constraint.\n\nIs there an error in that logic?\nThank you!",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/82/comments",
    "author": "zlenyk",
    "comments": [
      {
        "user": "pjannaty",
        "created_at": "2025-02-14T17:57:15Z",
        "body": "Cosmos currently does not officially support generating longer videos. We will consider the enhancement in future versions."
      }
    ]
  },
  {
    "number": 74,
    "title": "Generating Longer Videos in Diffusion Image2World Model",
    "created_at": "2025-01-21T09:02:39Z",
    "closed_at": "2025-02-14T18:03:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/74",
    "body": "I modified the `num_video_frames` parameter to 242 in the Diffusion Image2World model to create a longer video. However, only noisy blank frames are generated, and the output is not working as expected.\n\nI tested this using the A100 80G with 7B Offload, prompt upsampler, and guardrails models. For reference, when the `num_video_frames` is set to 121, the video generates correctly.",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/74/comments",
    "author": "kjrstory",
    "comments": [
      {
        "user": "pjannaty",
        "created_at": "2025-02-14T18:03:15Z",
        "body": "Nice experimentation! Cosmos currently does not officially support generating longer videos. We will consider the enhancement in future versions."
      }
    ]
  },
  {
    "number": 72,
    "title": "Distortion Issues in video2world Model During Image-to-Video Testing",
    "created_at": "2025-01-21T02:20:27Z",
    "closed_at": "2025-02-14T17:55:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/72",
    "body": "I attempted to use the video2world model to test the image-to-video generation feature. According to the project’s readme file, the model should support multiple resolutions and aspect ratios, including 1:1 (960x960), 4:3 (960x704), 3:4 (704x960), 16:9 (1280x704), and 9:16 (704x1280).\n\nHowever, during the actual testing process, I found that only at the aspect ratio of 9:16 (704x1280) did the generated video quality appear relatively normal. At other ratios, the videos exhibited significant distortions and artifacts. I followed the scripts provided in the readme file precisely.\n\nI would like to inquire if this situation is expected. If there were any errors in my operation or improper configuration, please advise on the correct usage. If not, is there a solution or planned fix for this issue?\n\nLooking forward to your response, thank you!",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/72/comments",
    "author": "lith0613",
    "comments": [
      {
        "user": "zlenyk",
        "created_at": "2025-01-21T15:40:37Z",
        "body": "Could you point out where in Readme did you see that? So far I noticed that the only resolution that is accepted by video2world model is 1024x640. Anything else needed changes to the code and produced bad outputs"
      },
      {
        "user": "mharrim-nv",
        "created_at": "2025-02-12T01:32:49Z",
        "body": "@lith0613 can you specify which model you were trying video2world? While Diffusion video2world supports different aspect ratios the Autoregressive videoworld only works with 1024x640 resolution videos. If the input image/video is not in this resolution, it will be resized and cropped."
      }
    ]
  },
  {
    "number": 69,
    "title": "RuntimeError: Missing key in checkpoint state_dict: module.decoder.layers.self_attention.q_layernorm._extra_state/shard_0_16.",
    "created_at": "2025-01-20T08:54:43Z",
    "closed_at": "2025-01-24T20:49:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/69",
    "body": "[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/cosmos1/models/autoregressive/nemo/post_training/general.py\", line 136, in <module>\n[rank0]:     main(args)\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/cosmos1/models/autoregressive/nemo/post_training/general.py\", line 64, in main\n[rank0]:     llm.api.train(\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/nemo/collections/llm/api.py\", line 106, in train\n[rank0]:     trainer.fit(model, data)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 543, in fit\n[rank0]:     call._call_and_handle_interrupt(\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank0]:     return function(*args, **kwargs)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 579, in _fit_impl\n[rank0]:     self._run(model, ckpt_path=ckpt_path)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 962, in _run\n[rank0]:     self.strategy.setup(self)\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 413, in setup\n[rank0]:     self.selective_restore()\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 785, in selective_restore\n[rank0]:     checkpoint = self.load_checkpoint(checkpoint_path=self.restore_config.path, selective_restore=True)\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 765, in load_checkpoint\n[rank0]:     checkpoint = self.checkpoint_io.load_checkpoint(\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/plugins/io/wrapper.py\", line 72, in load_checkpoint\n[rank0]:     return self.checkpoint_io.load_checkpoint(*args, **kwargs)\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/nemo/lightning/io/pl.py\", line 238, in load_checkpoint\n[rank0]:     checkpoint = dist_checkpointing.load(\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/Megatron-LM/megatron/core/dist_checkpointing/serialization.py\", line 147, in load\n[rank0]:     loaded_state_dict = sharded_strategy.load(sharded_state_dict, checkpoint_dir)\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/Megatron-LM/megatron/core/dist_checkpointing/strategies/fully_parallel.py\", line 204, in load\n[rank0]:     return self.base_strategy.load(sharded_state_dict, checkpoint_dir)\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py\", line 781, in load\n[rank0]:     checkpoint.load_state_dict(\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/typing_extensions.py\", line 2853, in wrapper\n[rank0]:     return arg(*args, **kwargs)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 39, in load_state_dict\n[rank0]:     return _load_state_dict(\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 220, in _load_state_dict\n[rank0]:     central_plan: LoadPlan = distW.reduce_scatter(\"plan\", local_step, global_step)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py\", line 192, in reduce_scatter\n[rank0]:     raise result\n[rank0]: torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0])\n[rank0]: Traceback (most recent call last): (RANK 0)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py\", line 165, in reduce_scatter\n[rank0]:     local_data = map_fun()\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py\", line 66, in wrapper\n[rank0]:     result = func(*args, **kwargs)\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 209, in local_step\n[rank0]:     local_plan = planner.create_local_plan()\n[rank0]:   File \"/ossfs/workspace/yuhu.yh/code/Cosmos/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py\", line 548, in create_local_plan\n[rank0]:     return super().create_local_plan()\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py\", line 197, in create_local_plan\n[rank0]:     return create_default_local_load_plan(\n[rank0]:   File \"/root/miniconda3/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py\", line 316, in create_default_local_load_plan\n[rank0]:     raise RuntimeError(f\"Missing key in checkpoint state_dict: {fqn}.\")\n[rank0]: RuntimeError: Missing key in checkpoint state_dict: module.decoder.layers.self_attention.q_layernorm._extra_state/shard_0_16.\n\n\nI encountered this problem when post_train the autoregressive models. Any suggestions ?",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/69/comments",
    "author": "yuhuUSTC",
    "comments": [
      {
        "user": "ArshKA",
        "created_at": "2025-01-20T19:58:46Z",
        "body": "Set `ckpt_load_strictness=False` in nl.MegatronStrategy\n```\nstrategy=nl.MegatronStrategy(\n    ...\n    ckpt_load_strictness=False,\n)\n```"
      }
    ]
  },
  {
    "number": 67,
    "title": "How to generate longer videos?",
    "created_at": "2025-01-19T17:27:09Z",
    "closed_at": "2025-02-06T16:40:48Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/67",
    "body": "Hi there, thank you for open-sourcing this wonderful work! I was playing with the Autoregressive-5B-Video2World model and have successfully got it to generate a 1 second video given an image as input. Just wondering if there's any h-param that I could set to generate longer videos. Thanks for the help!",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/67/comments",
    "author": "xuefei1",
    "comments": [
      {
        "user": "ymcki",
        "created_at": "2025-01-20T03:40:24Z",
        "body": "Use the last frame in the generated video to generate another video. Then concatenate the two."
      },
      {
        "user": "foxkw",
        "created_at": "2025-01-24T02:07:32Z",
        "body": "> Use the last frame in the generated video to generate another video. Then concatenate the two.\n\nThat's crazy"
      }
    ]
  },
  {
    "number": 65,
    "title": "AttributeError: 'NoneType' object has no attribute 'vocab_size' during Autoregressive Model Training in NeMo",
    "created_at": "2025-01-18T08:47:48Z",
    "closed_at": "2025-01-24T20:56:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/65",
    "body": "When training a autoregressive model using the provided script, I encountered an `AttributeError: 'NoneType' object has no attribute 'vocab_size'` error. This occurs within the `configure_model` method of the model class. The error seems to be related to the tokenizer being `None` when `configure_model` is called.\n\nNemo Version: This is happening on Nemo 2.1.0.\n\nHere is the specific traceback:\n\n```\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/data2/wy/WM/Cosmos/cosmos1/models/autoregressive/nemo/post_training/general.py\", line 129, in <module>\n[rank1]:     main(args)\n[rank1]:   File \"/data2/wy/WM/Cosmos/cosmos1/models/autoregressive/nemo/post_training/general.py\", line 57, in main\n[rank1]:     llm.api.train(\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/nemo/collections/llm/api.py\", line 107, in train\n[rank1]:     trainer.fit(model, data)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n[rank1]:     call._call_and_handle_interrupt(\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank1]:     return function(*args, **kwargs)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n[rank1]:     self._run(model, ckpt_path=ckpt_path)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 945, in _run\n[rank1]:     call._call_configure_model(self)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 119, in _call_configure_model\n[rank1]:     _call_lightning_module_hook(trainer, \"configure_model\")\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 167, in _call_lightning_module_hook\n[rank1]:     output = fn(*args, **kwargs)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/nemo/collections/llm/gpt/model/base.py\", line 346, in configure_model\n[rank1]:     self.module = self.config.configure_model(self.tokenizer)\n[rank1]:   File \"/data2/wy/WM/Cosmos/cosmos1/models/autoregressive/nemo/cosmos.py\", line 176, in configure_model\n[rank1]:     model = super().configure_model(tokenizer)\n[rank1]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/nemo/collections/llm/gpt/model/base.py\", line 199, in configure_model\n[rank1]:     f\"Use preset vocab_size: {vocab_size}, original vocab_size: {tokenizer.vocab_size}, dummy tokens:\"\n[rank1]: AttributeError: 'NoneType' object has no attribute 'vocab_size'\n```\n\n**Command Used:**\n\n```bash\ntorchrun --nproc-per-node $NUM_DEVICES cosmos1/models/autoregressive/nemo/post_training/general.py \\\n--data_path $OUTPUT_PREFIX \\\n--split_string 4,1,1 \\\n--log_dir ./logs \\\n--max_steps 20 --save_every_n_steps 10 \\\n--tensor_model_parallel_size $NUM_DEVICES \\\n--model_path nvidia/Cosmos-1.0-Autoregressive-4B/nemo\n```\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/65/comments",
    "author": "www-Ye",
    "comments": [
      {
        "user": "www-Ye",
        "created_at": "2025-01-18T10:23:11Z",
        "body": "I will change\n\n```python\nlogging.info(\nf\"Use preset vocab_size: {vocab_size}, original vocab_size: {tokenizer.vocab_size}, dummy tokens:\"\nf\" {vocab_size - tokenizer.vocab_size}.\"\n)\n```\n\nto\n\n```python\nif tokenizer is not None:\nlogging.info(\nf\"Use preset vocab_size: {vocab_size}, original vocab_size: {tokenizer.vocab_size}, dummy tokens:\"\nf\" {vocab_size - tokenizer.vocab_size}.\"\n)\n```\n\nAfterward, a new problem appears:\n\n```\n[rank0]: RuntimeError: Missing key in checkpoint state_dict: module.decoder.layers.self_attention.q_layernorm._extra_state/shard_0_16.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/data2/wy/WM/Cosmos/cosmos1/models/autoregressive/nemo/post_training/general.py\", line 130, in <module>\n[rank0]:     main(args)\n[rank0]:   File \"/data2/wy/WM/Cosmos/cosmos1/models/autoregressive/nemo/post_training/general.py\", line 58, in main\n[rank0]:     llm.api.train(\n[rank0]:   File \"/data2/wy/packages/NeMo-main/nemo/collections/llm/api.py\", line 106, in train\n[rank0]:     trainer.fit(model, data)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n[rank0]:     call._call_and_handle_interrupt(\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank0]:     return function(*args, **kwargs)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n[rank0]:     self._run(model, ckpt_path=ckpt_path)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 957, in _run\n[rank0]:     self.strategy.setup(self)\n[rank0]:   File \"/data2/wy/packages/NeMo-main/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 413, in setup\n[rank0]:     self.selective_restore()\n[rank0]:   File \"/data2/wy/packages/NeMo-main/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 785, in selective_restore\n[rank0]:     checkpoint = self.load_checkpoint(checkpoint_path=self.restore_config.path, selective_restore=True)\n[rank0]:   File \"/data2/wy/packages/NeMo-main/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 765, in load_checkpoint\n[rank0]:     checkpoint = self.checkpoint_io.load_checkpoint(\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/lightning/pytorch/plugins/io/wrapper.py\", line 72, in load_checkpoint\n[rank0]:     return self.checkpoint_io.load_checkpoint(*args, **kwargs)\n[rank0]:   File \"/data2/wy/packages/NeMo-main/nemo/lightning/io/pl.py\", line 238, in load_checkpoint\n[rank0]:     checkpoint = dist_checkpointing.load(\n[rank0]:   File \"/data2/wy/packages/Megatron-LM/megatron/core/dist_checkpointing/serialization.py\", line 147, in load\n[rank0]:     loaded_state_dict = sharded_strategy.load(sharded_state_dict, checkpoint_dir)\n[rank0]:   File \"/data2/wy/packages/Megatron-LM/megatron/core/dist_checkpointing/strategies/fully_parallel.py\", line 204, in load\n[rank0]:     return self.base_strategy.load(sharded_state_dict, checkpoint_dir)\n[rank0]:   File \"/data2/wy/packages/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py\", line 781, in load\n[rank0]:     checkpoint.load_state_dict(\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/typing_extensions.py\", line 2853, in wrapper\n[rank0]:     return arg(*args, **kwargs)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 41, in load_state_dict\n[rank0]:     return _load_state_dict(\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 222, in _load_state_dict\n[rank0]:     central_plan: LoadPlan = distW.reduce_scatter(\"plan\", local_step, global_step)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py\", line 191, in reduce_scatter\n[rank0]:     raise result\n[rank0]: torch.distributed.checkpoint.api.CheckpointException: CheckpointException ranks:dict_keys([0, 1])\n[rank0]: Traceback (most recent call last): (RANK 0)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/utils.py\", line 164, in reduce_scatter\n[rank0]:     local_data = map_fun()\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/logger.py\", line 83, in wrapper\n[rank0]:     result = func(*args, **kwargs)\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/state_dict_loader.py\", line 211, in local_step\n[rank0]:     local_plan = planner.create_local_plan()\n[rank0]:   File \"/data2/wy/packages/Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py\", line 548, in create_local_plan\n[rank0]:     return super().create_local_plan()\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py\", line 233, in create_local_plan\n[rank0]:     return create_default_local_load_plan(\n[rank0]:   File \"/data2/wy/envs/wm/lib/python3.10/site-packages/torch/distributed/checkpoint/default_planner.py\", line 354, in create_default_local_load_plan\n[rank0]:     raise RuntimeError(f\"Missing key in checkpoint state_dict: {fqn}.\")\n[rank0]: RuntimeError: Missing key in checkpoint state_dict: module.decoder.layers.self_attention.q_layernorm._extra_state/shard_0_16.\n```"
      },
      {
        "user": "ArshKA",
        "created_at": "2025-01-20T19:57:07Z",
        "body": "Set `ckpt_load_strictness=False` in nl.MegatronStrategy. Replace it with\n```\nstrategy=nl.MegatronStrategy(\n    ...\n    ckpt_load_strictness=False,\n)\n```"
      }
    ]
  },
  {
    "number": 62,
    "title": "Any plans to integrate in Diffusers",
    "created_at": "2025-01-17T13:07:08Z",
    "closed_at": "2025-01-28T00:32:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/62",
    "body": "As the title says.",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/62/comments",
    "author": "nitinmukesh",
    "comments": [
      {
        "user": "pjannaty",
        "created_at": "2025-01-28T00:32:25Z",
        "body": "We have plans to integrate more natively with HuggingFace."
      }
    ]
  },
  {
    "number": 61,
    "title": "autoregressive model",
    "created_at": "2025-01-17T07:08:34Z",
    "closed_at": "2025-01-24T21:06:05Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/61",
    "body": "How to set the number of seconds or frames in the output video",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/61/comments",
    "author": "MikeAiJF",
    "comments": [
      {
        "user": "sophiahhuang",
        "created_at": "2025-01-24T21:06:05Z",
        "body": "Thank you for your interest in Cosmos. We don't support it currently and will add this to planning. "
      }
    ]
  },
  {
    "number": 59,
    "title": "docker",
    "created_at": "2025-01-16T10:35:00Z",
    "closed_at": "2025-02-14T17:48:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/59",
    "body": "Why can't I connect to the image of Huggingface on Docker",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/59/comments",
    "author": "MikeAiJF",
    "comments": [
      {
        "user": "ethanhe42",
        "created_at": "2025-02-04T19:31:25Z",
        "body": "could you clarify your question? is it huggingface or docker problem?"
      }
    ]
  },
  {
    "number": 45,
    "title": "Image2Image",
    "created_at": "2025-01-13T07:12:51Z",
    "closed_at": "2025-01-24T20:53:29Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/45",
    "body": "Excellent work! Can I do Image2Image diffusion with a prompt (Just like ControlNet or FLUX)?",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/45/comments",
    "author": "ztianlin",
    "comments": [
      {
        "user": "ethanhe42",
        "created_at": "2025-01-13T17:58:05Z",
        "body": "you can try the diffusion Video2World model but use image instead of video"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-14T01:30:29Z",
        "body": "> you can try the diffusion Video2World model but use image instead of video\n\nBut that's Image2Video not Image2Image ;)\n"
      },
      {
        "user": "ztianlin",
        "created_at": "2025-01-14T02:00:53Z",
        "body": "> you can try the diffusion Video2World model but use image instead of video\n\nYeah it works well. But the output is still a video. I was wondering if it is possible to change the style of the image directly. It would be greater if there is a smaller model that can work!"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-14T02:36:18Z",
        "body": "> > you can try the diffusion Video2World model but use image instead of video\n> \n> Yeah it works well. But the output is still a video. I was wondering if it is possible to change the style of the image directly. It would be greater if there is a smaller model that can work!\n\nWhy would you want to do Image2Image with Cosmos? \n\nCosmos is supposedly a physics engine that tries to predict future frames based on physics. Not sure what its physics engine can do to change the style of an Image."
      },
      {
        "user": "ztianlin",
        "created_at": "2025-01-14T06:59:54Z",
        "body": "> > > you can try the diffusion Video2World model but use image instead of video\n> > \n> > \n> > Yeah it works well. But the output is still a video. I was wondering if it is possible to change the style of the image directly. It would be greater if there is a smaller model that can work!\n> \n> Why would you want to do Image2Image with Cosmos?\n> \n> Cosmos is supposedly a physics engine that tries to predict future frames based on physics. Not sure what its physics engine can do to change the style of an Image.\n\nI was hoping to change the environment (weather, lightness) in a given image based on WFM. And I assume that Cosmos can do better in physical details than other famous diffusion models."
      },
      {
        "user": "pjannaty",
        "created_at": "2025-01-24T20:53:29Z",
        "body": "We will support image2image in the future. Please stay tuned."
      }
    ]
  },
  {
    "number": 37,
    "title": "IDEA: COSMOS + DLSS4",
    "created_at": "2025-01-10T13:00:29Z",
    "closed_at": "2025-01-24T20:54:39Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/37",
    "body": "I read the system card in official page about cosmos. I think that it can be apply an idea of improve quality video.\n\nOther Properties Related to Output: The generated video will be a 5-second clip with a resolution of 1280x704 pixels at 24 frames per second (fps) it can be applied DLSS4 sdk",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/37/comments",
    "author": "johnnynunez",
    "comments": [
      {
        "user": "pjannaty",
        "created_at": "2025-01-24T20:54:39Z",
        "body": "Thank you for the suggestion. We have captured this feature request internally, however cannot guarantee we will implement at this point."
      }
    ]
  },
  {
    "number": 35,
    "title": "Can We Know World Parameters",
    "created_at": "2025-01-10T08:15:26Z",
    "closed_at": "2025-01-24T20:39:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/35",
    "body": "Since it is called a World Function Model, can we determine the exact physical parameters (position, point cloud) of the objects we generated in the video? In other words, can we not only visualize the new world but also digitize it?",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/35/comments",
    "author": "ztianlin",
    "comments": [
      {
        "user": "pjannaty",
        "created_at": "2025-01-24T20:39:36Z",
        "body": "Cosmos perceives the world visually. There is no semantic digitization."
      }
    ]
  },
  {
    "number": 34,
    "title": "Update INSTALL.md",
    "created_at": "2025-01-10T07:57:56Z",
    "closed_at": "2025-01-10T18:23:52Z",
    "labels": [],
    "url": "https://github.com/NVIDIA/Cosmos/pull/34",
    "body": null,
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/34/comments",
    "author": "thanhnamitit",
    "comments": [
      {
        "user": "ielh1",
        "created_at": "2025-01-10T18:23:52Z",
        "body": "It's preferred to use SSH cloning. Closing this PR for now."
      }
    ]
  },
  {
    "number": 32,
    "title": "Wonderful work!",
    "created_at": "2025-01-10T02:32:01Z",
    "closed_at": "2025-01-24T20:40:25Z",
    "labels": [
      "wontfix"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/32",
    "body": "Thank you very much for your work, which provides us with many new perspectives.",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/32/comments",
    "author": "Johanan528",
    "comments": [
      {
        "user": "ymcki",
        "created_at": "2025-01-10T07:43:07Z",
        "body": "Is this the only Physics model that can actually run locally?\n\nI heard about Genesis, Genie 2, WorldLabs. The latter two seems to be just API. Genesis' generative AI is still under development. Am I missing some other models?"
      },
      {
        "user": "SantiiRepair",
        "created_at": "2025-01-10T23:41:08Z",
        "body": "> Thank you very much for your work, which provides us with many new perspectives.\n\nSo is this an issue?"
      }
    ]
  },
  {
    "number": 26,
    "title": "How to improve videos generated from portrait images?",
    "created_at": "2025-01-09T13:46:27Z",
    "closed_at": "2025-01-19T00:51:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/26",
    "body": "I am getting decent but not perfect videos from 1280x704 images using the Cosmos-1.0-Diffusion-7B-Video2World.\n\nBut when I am using 704x1280 images and adding --height 1280 --width 704 to the command, I am getting serious distortions in the output video. Here is an example:\n\nPYTHONPATH=$(pwd) python cosmos1/models/diffusion/inference/video2world.py     --checkpoint_dir /workspace/checkpoints     --diffusion_transformer_dir Cosmos-1.0-Diffusion-7B-Video2World     --prompt \"This video captures a breathtaking nighttime landscape of a mountain range under a star-studded sky. The Milky Way galaxy is prominently visible, with shooting stars streaking across the heavens. In the foreground, a dirt path leads towards the mountains, where a few individuals are seen walking with flashlights. The scene is serene and majestic, highlighting the beauty of nature under the night sky.\"     --input_image_or_video_path mmexport1530074514786-704x1280.jpg     --num_input_frames 1     --seed 547312549     --video_save_name sky     --offload_tokenizer     --offload_diffusion_transformer     --offload_text_encoder_model     --disable_prompt_upsampler     --offload_guardrail_models --height 1280 --width 704\n\nIs there any way to fix that? Or Cosmos-1.0-Diffusion-7B-Video2World is not designed for portrait images?\n\nWill Cosmos-1.0-Autoregressive-5B-Video2World do better for portrait images?",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/26/comments",
    "author": "ymcki",
    "comments": [
      {
        "user": "ymcki",
        "created_at": "2025-01-10T12:22:53Z",
        "body": "I tried 704x960 also. Less distortion than 704x1280 but still not as usable as 1280x704."
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-10T13:21:18Z",
        "body": "960x960 has smaller distortion. I think it is in the usable range.\n\nSo how can I make better portrait videos? Maybe I should just make landscape ones and then chop off the sides?"
      },
      {
        "user": "lith0613",
        "created_at": "2025-01-13T05:00:33Z",
        "body": "> 960x960 has smaller distortion. I think it is in the usable range.\n> \n> So how can I make better portrait videos? Maybe I should just make landscape ones and then chop off the sides?\n\nhi,  do you solve this distortion ?"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-13T08:50:45Z",
        "body": "> > 960x960 has smaller distortion. I think it is in the usable range.\n> > So how can I make better portrait videos? Maybe I should just make landscape ones and then chop off the sides?\n> \n> hi, do you solve this distortion ?\n\nNope. Based on multiple tries. The video quality order is\n\n1280x704 > 960x960 >> 704x960 >>704x1280\n\nHaven't tried 960x704 yet."
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-18T12:51:05Z",
        "body": "Interestingly, using the ComfyUI workflow, I am able to generate a decent 704x1280 with small distortion. It still has the zoom in problem but at least the video is usable. I will give it more tries and see if this is real or just pure luck."
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-19T00:51:17Z",
        "body": "After multiple tries, I can confirm that ComfyUI works for portrait images. So I am going to close this issue."
      }
    ]
  },
  {
    "number": 20,
    "title": "Release gradio demo",
    "created_at": "2025-01-08T14:35:18Z",
    "closed_at": "2025-01-28T05:27:45Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/pull/20",
    "body": null,
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/20/comments",
    "author": "vaskers5",
    "comments": [
      {
        "user": "vaskers5",
        "created_at": "2025-01-08T14:35:43Z",
        "body": "@mingyuliutw I have created Gradio demo for yours repo, pls review it"
      },
      {
        "user": "FurkanGozukara",
        "created_at": "2025-01-08T14:41:02Z",
        "body": "Awesome ty i hope get merged asap"
      },
      {
        "user": "vaskers5",
        "created_at": "2025-01-09T10:48:26Z",
        "body": "@mingyuliutw I signed the license, added the header, and went through the pipeline. I also moved the demo, everything is according to Contributing.md and your edits"
      },
      {
        "user": "vaskers5",
        "created_at": "2025-01-10T09:52:58Z",
        "body": "@mingyuliutw Jently reminder about my contribution"
      },
      {
        "user": "vaskers5",
        "created_at": "2025-01-10T10:11:13Z",
        "body": "@grace-lam maybe you can help?"
      },
      {
        "user": "FurkanGozukara",
        "created_at": "2025-01-10T14:39:22Z",
        "body": "any other optimizations are possible other than just offloading?\r\n\r\nlike fp8 quant int4 vae slicing or tiling etc?"
      },
      {
        "user": "vaskers5",
        "created_at": "2025-01-10T14:57:45Z",
        "body": "Now I'am trying to add support for multi-gpu. It will make inference faster, but also it will require nemo container"
      },
      {
        "user": "win10ogod",
        "created_at": "2025-01-15T06:21:45Z",
        "body": "Are autoregressive models supported?"
      },
      {
        "user": "pjannaty",
        "created_at": "2025-01-28T05:27:45Z",
        "body": "Thank you for your interest and contribution. We really appreciate it. However, at this point we won't be supporting gradio in this repository to keep it easy to manage. "
      }
    ]
  },
  {
    "number": 10,
    "title": "\"Getting started\" should suggest to download only the 7B model",
    "created_at": "2025-01-07T22:46:22Z",
    "closed_at": "2025-01-24T20:29:03Z",
    "labels": [
      "documentation"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/10",
    "body": "The tutorial in Cosmos/cosmos1/models/diffusion/README.md is very easy to follow. However, it suggests using the following command:\n\n`PYTHONPATH=$(pwd) python cosmos1/scripts/download_diffusion.py --model_sizes 7B 14B --model_types Text2World Video2World`\n\nThis command will download both the 7B and 14B models, which are very large and slow to download. I suggest modifying the command to:\n\n`PYTHONPATH=$(pwd) python cosmos1/scripts/download_diffusion.py --model_sizes 7B --model_types Text2World Video2World`\n\nThis will download only the lightest model, making the getting-started process easier and faster for beginners.",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/10/comments",
    "author": "AlessioGalluccio",
    "comments": [
      {
        "user": "mharrim",
        "created_at": "2025-01-24T20:29:03Z",
        "body": "Thank you for your feedback and support! We appreciate your recommendation and will work on simplifying and improving the README in the next iteration to enhance usability."
      }
    ]
  },
  {
    "number": 1,
    "title": "Pixtral-12B doesn't have model.pt any more",
    "created_at": "2025-01-07T12:56:11Z",
    "closed_at": "2025-01-08T00:42:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/Cosmos/issues/1",
    "body": "It only has consolidated.safetensors, so I am getting this error:\nFileNotFoundError: [Errno 2] No such file or directory: '/workspace/Pixtral-12B/model.pt'\n\nSeems like mistral deleted the older pixtral models. What to do?",
    "comments_url": "https://api.github.com/repos/NVIDIA/Cosmos/issues/1/comments",
    "author": "ymcki",
    "comments": [
      {
        "user": "vird",
        "created_at": "2025-01-07T12:59:15Z",
        "body": "Works fine for me\nrun typical download model first\n`PYTHONPATH=$(pwd) python cosmos1/scripts/download_diffusion.py --model_sizes 7B 14B --model_types Text2World Video2World`\n\nthen just try generate some video with e.g.\n`cosmos1/models/diffusion/inference/text2world.py`\nIt will download missing Pixtral to checkpoints/Pixtral-12B/"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-07T13:19:50Z",
        "body": "Asked Qwen-2.5-Coder to write a script to convert safetensors to pt and successfully got a model.pt\n\n```\nimport torch\nfrom safetensors.torch import load_file\n\ndef convert_safetensors_to_pt(safetensors_path, pt_path):\n    \"\"\"\n    Converts a .safetensors file to a PyTorch .pt file.\n    \n    :param safetensors_path: Path to the .safetensors file.\n    :param pt_path: Path where the .pt file will be saved.\n    \"\"\"\n    # Load the model from the .safetensors file\n    tensors = load_file(safetensors_path)\n\n    # Save the model in PyTorch's .pt format\n    torch.save(tensors, pt_path)\n    print(f\"Model successfully saved to {pt_path}\")\n\nif __name__ == \"__main__\":\n    # Example usage\n    safetensors_path = \"consolidated.safetensors\"\n    pt_path = \"model.pt\"\n    convert_safetensors_to_pt(safetensors_path, pt_path)\n\n```\nBut then it crashes in create_vlm_prompt_upsampler\n```\nroot@018e4a0ef04f:/workspace# PYTHONPATH=$(pwd) python cosmos1/models/diffusion/inference/video2world.py     --checkpoint_dir /workspace     --diffusion_transformer_dir Cosmos-1.0-Diffusion-14B-Video2World     --prompt \"girl rotating\"     --input_image_or_video_path 00000-4214268904-1280x704.png     --num_input_frames 1     --video_save_name Cosmos-1.0-Diffusion-14B-Video2World_memory_efficient     --offload_tokenizer     --offload_diffusion_transformer     --offload_text_encoder_model     --offload_prompt_upsampler     --offload_guardrail_models --seed 547312549\n[01-07 13:10:51|INFO|cosmos1/utils/misc.py:106:set_random_seed] Using random seed 547312549.\n[01-07 13:10:51|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:606:generate] Run with prompt: girl rotating\n[01-07 13:10:51|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:607:generate] Run with image or video path: 00000-4214268904-1280x704.png\n[01-07 13:10:51|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:608:generate] Run with negative prompt: The video captures a series of frames showing ugly scenes, static with no motion, motion blur, over-saturation, shaky footage, low resolution, grainy texture, pixelated images, poorly lit areas, underexposed and overexposed scenes, poor color balance, washed out colors, choppy sequences, jerky movements, low frame rate, artifacting, color banding, unnatural transitions, outdated special effects, fake elements, unconvincing visuals, poorly edited content, jump cuts, visual noise, and flickering. Overall, the video is of poor quality.\n[01-07 13:10:51|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:609:generate] Run with prompt upsampler: True\n[01-07 13:10:51|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:621:generate] Run prompt upsampler on image or video, input prompt is not used\nTraceback (most recent call last):\n  File \"/workspace/cosmos1/models/diffusion/inference/video2world.py\", line 178, in <module>\n    demo(args)\n  File \"/workspace/cosmos1/models/diffusion/inference/video2world.py\", line 141, in demo\n    generated_output = pipeline.generate(\n  File \"/workspace/cosmos1/models/diffusion/inference/world_generation_pipeline.py\", line 622, in generate\n    prompt = self._run_prompt_upsampler_on_prompt_with_offload(image_or_video_path=image_or_video_path)\n  File \"/workspace/cosmos1/models/diffusion/inference/world_generation_pipeline.py\", line 189, in _run_prompt_upsampler_on_prompt_with_offload\n    self._load_prompt_upsampler_model()\n  File \"/workspace/cosmos1/models/diffusion/inference/world_generation_pipeline.py\", line 464, in _load_prompt_upsampler_model\n    self.prompt_upsampler = create_vlm_prompt_upsampler(\n  File \"/workspace/cosmos1/models/diffusion/prompt_upsampler/video2world_prompt_upsampler_inference.py\", line 56, in create_vlm_prompt_upsampler\n    return AutoRegressiveModel.build(\n  File \"/workspace/cosmos1/models/autoregressive/model.py\", line 255, in build\n    len(vit_checkpoint) > 0 and len(projector_checkpoint) > 0\nAssertionError: vit_checkpoint and projector_checkpoint cannot be empty. We do not support random initialization for vision_encoder and mm_projector.\n```\n\nWhat went wrong? Did I do the conversion wrong or Pixtral 12B 2409 is too new for Cosmos?"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-07T13:21:31Z",
        "body": "Ah... I find that Pixtral-12B-2409 is missing config.json. Can that be a problem? \n\nIt has params.json and tekken.json. Can I just rename them to config.json?"
      },
      {
        "user": "vird",
        "created_at": "2025-01-07T13:24:05Z",
        "body": "I didn't make any manual interventions with models\nThe only difference from official recipe - I didn't use docker, so it was more difficult"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-07T13:40:45Z",
        "body": "> Works fine for me run typical download model first `PYTHONPATH=$(pwd) python cosmos1/scripts/download_diffusion.py --model_sizes 7B 14B --model_types Text2World Video2World`\n> \n> then just try generate some video with e.g. `cosmos1/models/diffusion/inference/text2world.py` It will download missing Pixtral to checkpoints/Pixtral-12B/\n\nBut it doesn't automatically download Pixtral-12B for me\n```\nroot@018e4a0ef04f:/workspace# PYTHONPATH=$(pwd) python cosmos1/models/diffusion/inference/video2world.py     --checkpoint_dir /workspace     --diffusion_transformer_dir Cosmos-1.0-Diffusion-14B-Video2World     --prompt \"girl rotating\"     --input_image_or_video_path 00000-4214268904-1280x704.png     --num_input_frames 1     --video_save_name Cosmos-1.0-Diffusion-14B-Video2World_memory_efficient     --offload_tokenizer     --offload_diffusion_transformer     --offload_text_encoder_model     --offload_prompt_upsampler     --offload_guardrail_models --seed 547312549\n[01-07 13:38:16|INFO|cosmos1/utils/misc.py:106:set_random_seed] Using random seed 547312549.\n[01-07 13:38:16|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:606:generate] Run with prompt: girl rotating\n[01-07 13:38:16|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:607:generate] Run with image or video path: 00000-4214268904-1280x704.png\n[01-07 13:38:16|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:608:generate] Run with negative prompt: The video captures a series of frames showing ugly scenes, static with no motion, motion blur, over-saturation, shaky footage, low resolution, grainy texture, pixelated images, poorly lit areas, underexposed and overexposed scenes, poor color balance, washed out colors, choppy sequences, jerky movements, low frame rate, artifacting, color banding, unnatural transitions, outdated special effects, fake elements, unconvincing visuals, poorly edited content, jump cuts, visual noise, and flickering. Overall, the video is of poor quality.\n[01-07 13:38:16|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:609:generate] Run with prompt upsampler: True\n[01-07 13:38:16|INFO|cosmos1/models/diffusion/inference/world_generation_pipeline.py:621:generate] Run prompt upsampler on image or video, input prompt is not used\nTraceback (most recent call last):\n  File \"/workspace/cosmos1/models/diffusion/inference/video2world.py\", line 178, in <module>\n    demo(args)\n  File \"/workspace/cosmos1/models/diffusion/inference/video2world.py\", line 141, in demo\n    generated_output = pipeline.generate(\n  File \"/workspace/cosmos1/models/diffusion/inference/world_generation_pipeline.py\", line 622, in generate\n    prompt = self._run_prompt_upsampler_on_prompt_with_offload(image_or_video_path=image_or_video_path)\n  File \"/workspace/cosmos1/models/diffusion/inference/world_generation_pipeline.py\", line 189, in _run_prompt_upsampler_on_prompt_with_offload\n    self._load_prompt_upsampler_model()\n  File \"/workspace/cosmos1/models/diffusion/inference/world_generation_pipeline.py\", line 464, in _load_prompt_upsampler_model\n    self.prompt_upsampler = create_vlm_prompt_upsampler(\n  File \"/workspace/cosmos1/models/diffusion/prompt_upsampler/video2world_prompt_upsampler_inference.py\", line 56, in create_vlm_prompt_upsampler\n    return AutoRegressiveModel.build(\n  File \"/workspace/cosmos1/models/autoregressive/model.py\", line 225, in build\n    checkpoint = torch.load(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1234, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 600, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 581, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: '/workspace/Pixtral-12B/model.pt'\n\n```"
      },
      {
        "user": "ymcki",
        "created_at": "2025-01-08T00:42:16Z",
        "body": "Indeed, if it download Video2World model via download_diffusion.py, then it will do the conversion itself for Pixtral-12B\n\nPYTHONPATH=$(pwd) python cosmos1/scripts/download_diffusion.py --model_sizes 7B --model_types Video2World"
      },
      {
        "user": "aceliuchanghong",
        "created_at": "2025-01-09T01:12:51Z",
        "body": "1.downdown Pixtral files:\n\n2.`vi cosmos1/scripts/convert_pixtral_ckpt.py`\n注释掉下载的:\n```\n# snapshot_download(\n#    repo_id=repo_id,\n#    allow_patterns=[\"params.json\", \"consolidated.safetensors\"],\n#    local_dir=pixtral_ckpt_dir,\n#    local_dir_use_symlinks=False,\n#)\n```\n3.then python:\n```\nfrom cosmos1.scripts.convert_pixtral_ckpt import convert_pixtral_checkpoint\nconvert_pixtral_checkpoint(\n    checkpoint_dir=\"checkpoints\",\n    checkpoint_name=\"Pixtral-12B\",\n    vit_type=\"pixtral-12b-vit\",\n)\n```\n4.end "
      }
    ]
  }
]