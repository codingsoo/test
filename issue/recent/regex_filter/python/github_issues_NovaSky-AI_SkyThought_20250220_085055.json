[
  {
    "number": 68,
    "title": "Navigate installed package relative links",
    "created_at": "2025-02-07T22:57:29Z",
    "closed_at": "2025-02-10T18:56:48Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/68",
    "body": null,
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/68/comments",
    "author": "guspan-tanadi",
    "comments": [
      {
        "user": "SumanthRH",
        "created_at": "2025-02-10T18:56:26Z",
        "body": "Thanks @guspan-tanadi "
      }
    ]
  },
  {
    "number": 65,
    "title": "Add code for evaluating pass @ k to inference_and_check #64",
    "created_at": "2025-02-04T22:38:48Z",
    "closed_at": "2025-02-07T20:48:37Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/65",
    "body": "Fixes minor bug in `perform_check` and adds code for checking pass @ k metric for `n > 1` samples.\r\n\r\nFor example if we run the following with a saved file `DeepSeek-R1-Distill-Qwen-7B_aime_train_None_False_0_-1.json` with `n=128` examples per question\r\n```bash\r\npython inference_and_check.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task aime  --split train --max_tokens 32768   --inference --n 128  --temperatures 0.6 --tp 1 --check\r\n```\r\nWe will get the following output now:\r\n```bash\r\nTemperature: [0.6]\r\nLoaded 30 existing results.\r\nFound 3840 responses requiring reject sampling...\r\nProcessing Reject Sampling: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3840/3840 [00:02<00:00, 1432.24it/s]\r\nFinal reject-sampling accuracy: 2052/3840\r\nActual accuracy: 0.534375\r\nFinal pass @ k:\r\nk: 128, pass @ k: 90.0\r\nk: 64, pass @ k: 84.999\r\nk: 32, pass @ k: 82.379\r\nk: 16, pass @ k: 80.524\r\nk: 8, pass @ k: 78.576\r\nk: 4, pass @ k: 74.26\r\nk: 2, pass @ k: 65.496\r\nk: 1, pass @ k: 53.438\r\nTemperature 0.6 acc: 27/30 (0.9)\r\n```",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/65/comments",
    "author": "erictang000",
    "comments": [
      {
        "user": "SumanthRH",
        "created_at": "2025-02-07T20:48:38Z",
        "body": "Closing in favour of #67 "
      }
    ]
  },
  {
    "number": 56,
    "title": "Ablation suggestion",
    "created_at": "2025-01-31T01:04:09Z",
    "closed_at": "2025-02-01T23:00:42Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/56",
    "body": "It seems that the contribution of this work is suggesting a way to generate better training data for training models which don't have reasoning abilities.\n\nBut IMO, it would be better to provide the performance of the model trained with original dataset that have shorter sequences compared to the one generated by QwQ for justifying your data generation method.\n\nOr would you explain any motivation or reason for using generated data instead of samples from original datasets?\n\n\n\n",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/56/comments",
    "author": "kwonmha",
    "comments": [
      {
        "user": "tyler-griggs",
        "created_at": "2025-02-01T23:00:42Z",
        "body": "Hi @kwonmha, that's a great question. We will actually soon arxiv some work where we also provide experiment results based on training the model on the original short chain-of-thought data. Perhaps unsurprisingly, this leads to much lower accuracy across challenging math benchmarks than training on long chain-of-thought responses, as we expect harder problems to require \"more thinking\". Stay tuned for the arxiv soon!"
      },
      {
        "user": "kwonmha",
        "created_at": "2025-02-03T00:23:59Z",
        "body": "I got it. Thanks!"
      }
    ]
  },
  {
    "number": 53,
    "title": "RL on RoadMap?",
    "created_at": "2025-01-30T07:47:40Z",
    "closed_at": "2025-02-02T04:52:18Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/53",
    "body": "Will you be using the DeepSeek recipe for equivalent AIME2024 score. \nHow can I help?",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/53/comments",
    "author": "aurotripathy",
    "comments": [
      {
        "user": "tyler-griggs",
        "created_at": "2025-02-01T23:07:59Z",
        "body": "Thank you for reaching out! We definitely want to explore RL and self-improvement, as it is clearly an important component for improving reasoning capabilities. \n\nWe will continue to make public all of our approaches and artifacts in order to make progress with the wider community. As such, please also share any experiment results or observations/insights you develop in your own experimentation, as that will be very beneficial to us and the community!"
      },
      {
        "user": "aurotripathy",
        "created_at": "2025-02-02T04:50:25Z",
        "body": "Thanks, will follow your journey, Godspeed."
      }
    ]
  },
  {
    "number": 46,
    "title": "Clarification on \"O1 Preview Model\" and Training Approach",
    "created_at": "2025-01-24T15:55:38Z",
    "closed_at": "2025-01-24T19:38:41Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/46",
    "body": "What does \"your own O1 preview model\" refer to , or does it mean training the **Qwen2.5-32B-Instruct** model with the specified parameters under $450? Or can any open-source LLM to be used for this purpose in same cost?",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/46/comments",
    "author": "Pitambar206",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-24T19:38:42Z",
        "body": "Hi there, thank you for your issue.\n\nIt means that one can fine-tune from Qwen-2.5-32B-Instruct to match O1-Preview reasoning quality with $450 budget."
      }
    ]
  },
  {
    "number": 43,
    "title": "Sky-T1-32B-Flash, Reduce Overthinking",
    "created_at": "2025-01-23T17:07:52Z",
    "closed_at": "2025-01-23T18:06:15Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/43",
    "body": "This PR includes the code used to train our Sky-T1-32B-Flash model. \r\n\r\nThe key file is `response_rewrite.py`, which holds the logic for taking responses generated by `inference_and_check.py` and applying stages of filtering, rewriting, and formatting to produce preference pairs used in preference optimization (SimPO, DPO, etc.).\r\n\r\nThe implementation is specific to the set of filters/rewrites/reformats used to train Sky-T1-32B-Flash, but the file is organized such that it can be modified to alternative pipelines as desired.\r\n\r\nThe PR also includes the YAML file and configuration settings used to perform SimPO training on top of Sky-T1-32B-Preview to produce Sky-T1-32B-Flash. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/43/comments",
    "author": "tyler-griggs",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-23T17:53:00Z",
        "body": "Looks great to me! Flash Tyler! @tyler-griggs "
      }
    ]
  },
  {
    "number": 36,
    "title": "[Doc] Add Chat Demo & Discord Link",
    "created_at": "2025-01-20T06:36:09Z",
    "closed_at": "2025-01-20T06:37:16Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/36",
    "body": null,
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/36/comments",
    "author": "caoshiyi",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-20T06:37:11Z",
        "body": "Amazing @caoshiyi !"
      },
      {
        "user": "skprasadu",
        "created_at": "2025-02-11T15:50:32Z",
        "body": "I am interested to setup a demo instance in our lab, and infact, even open to providing a Huggingface inference endpoint for people to test it. Let me know if you are open for that. I am running this model in our lab, and it is cool."
      }
    ]
  },
  {
    "number": 35,
    "title": "Reproduce problem",
    "created_at": "2025-01-20T02:59:05Z",
    "closed_at": "2025-01-20T03:32:59Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/35",
    "body": "Hi guys, this work is so cool! I'm following your script and guideline to reproduce the training of SkyThougt-preview, but I meet the problem of OOM.\n\nThe machine and setting of running is  8*A100(80GB), context length = 16384. When I modify the context length as 8192, the problem of OOM not appear. \n\nI know this problem is relevant to the ability of GPU. But I think 8*A100(80GB) have the same ability to load same large model as 8*H100(80GB), H100 can load larger model than A100 but the difference should not very large, the most different of them is training speed.\n\nSo I want to know how to do (such as config deepspeed json file or config model parameter of other?) can make 8*A100(80GB) can traing qwen2.5-32b with context length 16384. Can you provide some config detail or trick of this problem?\nI'm looking forward for your responding. \nThank you!",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/35/comments",
    "author": "cyanrain7",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-20T06:56:13Z",
        "body": "Thanks! Did you fix the problem?"
      },
      {
        "user": "hiyouga",
        "created_at": "2025-01-24T06:12:34Z",
        "body": "Try `enable_liger_kernel: true`"
      }
    ]
  },
  {
    "number": 32,
    "title": "How many CPU RAM being used with DeepSpeed Zero-3 offloading?",
    "created_at": "2025-01-18T13:21:53Z",
    "closed_at": "2025-01-18T16:40:23Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/32",
    "body": null,
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/32/comments",
    "author": "mgcyung",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-18T16:38:35Z",
        "body": "We use a H100 box, which has 1.73T RAM."
      }
    ]
  },
  {
    "number": 27,
    "title": "Add demo",
    "created_at": "2025-01-18T07:32:49Z",
    "closed_at": "2025-01-20T07:16:08Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/27",
    "body": "Serve the model, add a website to support users to query the model.",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/27/comments",
    "author": "DachengLi1",
    "comments": [
      {
        "user": "caoshiyi",
        "created_at": "2025-01-20T07:16:08Z",
        "body": "Solved by #36 ."
      }
    ]
  },
  {
    "number": 20,
    "title": "fix AIME & MATH500 prompt format",
    "created_at": "2025-01-16T19:47:48Z",
    "closed_at": "2025-01-17T03:48:47Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/20",
    "body": "* Fix AIME and MATH500 prompt format\r\n* Fix README for NUMINA as requested \r\n* Update reported numbers for MATH500 \r\n\r\n",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/20/comments",
    "author": "lynnliu030",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-17T03:16:17Z",
        "body": "Looks amazing to me! Leave two comments, maybe we can change the hardcode model name in the codebase. @lynnliu030 "
      },
      {
        "user": "lynnliu030",
        "created_at": "2025-01-17T03:17:29Z",
        "body": "@DachengLi1 Haven't seen your comments yet, I think you need to finish reviews? "
      }
    ]
  },
  {
    "number": 19,
    "title": "add bon inference and check",
    "created_at": "2025-01-16T19:45:44Z",
    "closed_at": "2025-01-17T02:44:34Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/19",
    "body": "Refer to #13 , will add more advanced inference and checking methods later for curating high-quality traces.",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/19/comments",
    "author": "caoshiyi",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-16T20:11:18Z",
        "body": "Looks great! You are fantastic @caoshiyi!"
      }
    ]
  },
  {
    "number": 14,
    "title": "Improve math data generation pipeline with latest huggingface changes and dataset upload",
    "created_at": "2025-01-15T02:47:31Z",
    "closed_at": "2025-01-16T18:51:20Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/14",
    "body": "(1) Post release, the data is moved to huggingface. \r\n(2) The originally used lighteval/math is gone. We need to clean it ourselves,",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/14/comments",
    "author": "DachengLi1",
    "comments": [
      {
        "user": "caoshiyi",
        "created_at": "2025-01-16T18:51:20Z",
        "body": "Fixed by #18 ."
      }
    ]
  },
  {
    "number": 9,
    "title": "About the system prompt",
    "created_at": "2025-01-14T02:45:17Z",
    "closed_at": "2025-01-14T17:01:13Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/9",
    "body": "Thank you for your excellent work.\r\n\r\nI would like to know if you use the system prompt (i.e., \"Your role as an assistant involves ...\") in the dataset when evaluating the performance of the model. If so, have you tried using the same system prompt in \"Qwen-2.5-32B-Instruct\"? Since the system prompt is very explicit and detailed, can this directly improve performance (although it may still be worse than the fine-tuned model)?",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/9/comments",
    "author": "LetheSec",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-14T17:01:13Z",
        "body": "We haven't ablated on just the system prompt; I think the important thing should be instructing the model to output in a specific format, while the \"role as xxx, xxx\" shouldn't affect much."
      },
      {
        "user": "caoshiyi",
        "created_at": "2025-01-14T17:10:05Z",
        "body": "Thanks for your question! We have actually evaluated the Qwen-2.5-32B-Instruct model with the system prompt \"Your role as an assistant involves ...\" (the one used to evaluate our model) but it didn't improve the performance for Qwen-2.5-32B-Instruct."
      }
    ]
  },
  {
    "number": 8,
    "title": "docs(train/README): intended links",
    "created_at": "2025-01-14T01:18:06Z",
    "closed_at": "2025-01-26T00:20:20Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/8",
    "body": null,
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/8/comments",
    "author": "guspan-tanadi",
    "comments": [
      {
        "user": "tyler-griggs",
        "created_at": "2025-01-26T00:20:12Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 5,
    "title": "Including QwQ Information",
    "created_at": "2025-01-13T00:51:56Z",
    "closed_at": "2025-01-13T01:00:10Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/5",
    "body": "So you dropped a 17k json but you give no indication how this was generated aside from QwQ did it. Can you include your synthetic data generation details? This is a MAJOR piece of what the reasoning model is built upon so sort of weird to not include it. I mean, I can use this math data to train my own math model, but if I want to generate synthetic data for my own use case I have to start from scratch. Thanks for this repo but I think until you add that component it's sort of useless.",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/5/comments",
    "author": "patruff",
    "comments": [
      {
        "user": "caoshiyi",
        "created_at": "2025-01-13T00:59:59Z",
        "body": "Thanks for your interest! \r\n\r\nThe detailed steps on how the training data is generated can be found under `skythought/tools`."
      }
    ]
  },
  {
    "number": 4,
    "title": "Will the smaller models' results be released?",
    "created_at": "2025-01-12T13:30:10Z",
    "closed_at": "2025-01-12T18:35:37Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/issues/4",
    "body": "Many thanks for your guys' great work! \r\nI am particularly interested in the \"model size matters\" part. \r\nIs there any more experimental results for 7B or 14B models to be released? Have you guys done experiments on llama-3 series of models?",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/4/comments",
    "author": "REIGN12",
    "comments": [
      {
        "user": "DachengLi1",
        "created_at": "2025-01-12T18:25:43Z",
        "body": "Thank you!\r\n\r\nWe didn't release it in the blog, but would like to share here: In AIME 2024, both 7B and 14B model has no improvement (10.0, 16.7 respectively). We also experiment with truncating long CoTs (to 4k token threshold), as well as fine-tuning from the base version of the model (instead of the instruction tuned one), both do not lead to meaningful improvement. We didn't train on llama-3 series yet."
      },
      {
        "user": "caoshiyi",
        "created_at": "2025-01-12T18:32:36Z",
        "body": "Thanks for your interest!\r\n\r\nWhile our initial attempt on the smaller models (e.g., 7B and 14B) didn't show significant improvement, we are working on improving their performance. Please stay tuned!"
      }
    ]
  },
  {
    "number": 3,
    "title": "docs: update data/README.md",
    "created_at": "2025-01-12T08:24:40Z",
    "closed_at": "2025-01-12T09:00:45Z",
    "labels": [],
    "url": "https://github.com/NovaSky-AI/SkyThought/pull/3",
    "body": "avaialble -> available",
    "comments_url": "https://api.github.com/repos/NovaSky-AI/SkyThought/issues/3/comments",
    "author": "eltociear",
    "comments": [
      {
        "user": "caoshiyi",
        "created_at": "2025-01-12T09:00:38Z",
        "body": "Thanks for fixing the typo!"
      }
    ]
  }
]