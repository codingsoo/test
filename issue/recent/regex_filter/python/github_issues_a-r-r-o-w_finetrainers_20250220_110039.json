[
  {
    "number": 259,
    "title": "trainer.py errors out regarding Args",
    "created_at": "2025-02-09T20:12:57Z",
    "closed_at": "2025-02-18T23:59:31Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/259",
    "body": "### System Info / 系統信息\n\nRunning Linux Mint 24, \n\nfrom .args import Args, validate_args\nImportError: attempted relative import with no known parent package\n\n### Information / 问题信息\n\n- [ ] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nTry python3 trainer.py <toml>\n\n### Expected behavior / 期待表现\n\nI expected the trainer to run.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/259/comments",
    "author": "bettinaSim",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-02-18T23:59:31Z",
        "body": "`train.py` should be the entrypoint for running the model. `finetrainers/trainer.py` contains the core training implementation but it should not be run directly"
      }
    ]
  },
  {
    "number": 253,
    "title": "[chore] relax requirements a bit.",
    "created_at": "2025-01-30T07:22:23Z",
    "closed_at": "2025-02-03T04:08:38Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/253",
    "body": "Until and unless a user needs it, I think it is okay to not install `torchao` and `bitsandbytes` by default. ",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/253/comments",
    "author": "sayakpaul",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-30T19:39:31Z",
        "body": "Will keep this open for a bit if you don't mind. In #245, I'm also exploring using torchao fp8 directly since that would be true fp8 training instead of what we have at the moment, so maybe not too problematic to have it as a direct dependancy. Our layerwise implementation is good for most GPUs but, in the end, it is unstable. TorchAO fp8 would be true fp8 and more stable, but currently limited to Ada and Hopper only I think"
      }
    ]
  },
  {
    "number": 249,
    "title": "Removing the FP8 specific bits",
    "created_at": "2025-01-28T06:01:35Z",
    "closed_at": "2025-01-28T06:23:37Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/249",
    "body": "We have shipped layerwise upcasting in `diffusers`. Should we still keep the upcasting related bits in `finetrainers`? (I understand we need to keep peft related patches).\n\n@a-r-r-o-w ",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/249/comments",
    "author": "sayakpaul",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-28T06:23:37Z",
        "body": "For now, will keep as-is without changes to existing code to avoid merge conflicts with #245. Already taken care of it locally, but need to get the more important bits of parallism working and then we can focus on the latest memory optimization aspects (group offloading being another candidate with diffusers layerwise casting). Will be wrapped up this week"
      }
    ]
  },
  {
    "number": 238,
    "title": "Condition",
    "created_at": "2025-01-22T21:04:32Z",
    "closed_at": "2025-01-27T08:10:05Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/238",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/238/comments",
    "author": "ArEnSc",
    "comments": [
      {
        "user": "neph1",
        "created_at": "2025-01-26T06:26:39Z",
        "body": "Is this basically what is needed to train a control net for ltx? If so, are times comparable to lora training?"
      },
      {
        "user": "ArEnSc",
        "created_at": "2025-01-27T08:06:44Z",
        "body": "> Is this basically what is needed to train a control net for ltx? If so, are times comparable to lora training?\r\n\r\nThis is just me understanding what is going on with LTX not gonna lie I give up it doesn't work."
      }
    ]
  },
  {
    "number": 237,
    "title": "fix(tests/hunyuanvideo-lora): typo in id token",
    "created_at": "2025-01-22T11:16:25Z",
    "closed_at": "2025-01-22T11:54:25Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/237",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/237/comments",
    "author": "badayvedat",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2025-01-22T11:54:28Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 232,
    "title": "README.md issue",
    "created_at": "2025-01-20T08:17:17Z",
    "closed_at": "2025-01-22T07:34:29Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/232",
    "body": "Some events in the news are incorrectly dated\nAdd I2V in README, users may think that only T2V training is in this repo",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/232/comments",
    "author": "jiashenggu",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-22T07:34:29Z",
        "body": "The new Trainer API does not support I2V yet (and the README is only tracking things related to Trainer). This will be supported soon"
      }
    ]
  },
  {
    "number": 230,
    "title": "USE_PEFT_BACKEND is true and calls scale_lora_layers in full finetune?",
    "created_at": "2025-01-18T05:40:51Z",
    "closed_at": "2025-01-19T07:04:37Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/230",
    "body": "### System Info / 系統信息\n\nShouldn't this be False during a full-fine-tune? This is specifically relevant to the LTX model:\n\nif USE_PEFT_BACKEND:\n    # weight the LoRA layers by setting `lora_scale` for each PEFT layer\n    scale_lora_layers(self, lora_scale)\n\n### Information / 问题信息\n\n- [ ] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nCurrent build\n\n### Expected behavior / 期待表现\n\nNot to be set during fine tuning",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/230/comments",
    "author": "ArEnSc",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-18T05:46:29Z",
        "body": "Since there are no lora layers during full-finetuning, this is a null-op and doesn't really impact the training. I personally think mixing the lora part with the forward pass is bad design but shouldn't really be problematic. LMK if you've found otherwise "
      }
    ]
  },
  {
    "number": 229,
    "title": "NPU Support",
    "created_at": "2025-01-16T14:28:23Z",
    "closed_at": "2025-01-23T16:45:24Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/229",
    "body": "Can NPU train successful?\ncause i notice there's deepseed.yaml",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/229/comments",
    "author": "BlackTea-c",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-22T07:32:42Z",
        "body": "We currently have not experimented with devices under than cuda. This will hopefully be improved in the future to support other devices better. Any PRs to make it work on other devices are welcome 🤗"
      }
    ]
  },
  {
    "number": 221,
    "title": "Replace prompt embedding methods with `encode_prompt()`",
    "created_at": "2025-01-15T09:46:14Z",
    "closed_at": "2025-01-15T11:24:07Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/221",
    "body": "Currently, we make use of adapted / copy-pasted utilities from `Diffusers` to embed captions. But since `encode_prompt()` method of a pipeline is flexible, we could use that directly. It will help us reduce LoC quite a bit and have a nice separation of concerns (in this case we delegate `encode_prompt()` to `diffusers`). \r\n\r\n@a-r-r-o-w would you be open to supporting this? If so, I can make a quick PoC PR to get feedback and ship.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/221/comments",
    "author": "sayakpaul",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-15T10:17:14Z",
        "body": "Thanks for the suggestion. The design we have currently is carefully thought of and part of broader design goal and so I don't want to rely on using diffusers `encode_prompt` at the moment, or in future, because:\r\n- they contain additional logic that is not relevant to training\r\n- we cannot really control or modify the encode prompt logic for custom text preprocessing pipeline\r\n- feel free to disagree but it is easier to read the code of entire training related utilities in a single file like we have currently imo\n- it really is more of a `transformers` thing\r\n\r\nAs an example of what I envision custom text preprocessing pipeline to be, think about how a ComfyUI workflow works. You can have various nodes that handle preprocessing before the underlying things you're working with is queued for actual processing. Each part is modular and performs exactly one thing, and these different parts can be combined in various ways to perform preprocessing as you like, be modified or opted out of on-the-fly, etc. This is to say that there are caption filtering/dropout/modification/etc. methods proposed in various papers and I would like to implement them in a modular manner, with users being able to opt-in to them when needed (or choosing different strategy for different dataset being trained on - style/concept/character lora training all require different kinds of caption preprocessing for good training results). The `encode_prompt` method, while being great for just going from text to embeddings, will divert the code flow out of finetrainers - this is something I want to limit as much as possible so that more things are under our control. I don't want to tie the design too closely with diffusers for two reasons:\r\n- In its current implementation, it might be hard to see but the broader goal for me is to be able to provide an API where anyone can hook in a custom model and utilities (as some specification we design) and train it with all the goodies we offer: loras, full finetuning/training, distillation, distributedness with DDP/FSDP/PP/TP/CP blackboxed, etc.\r\n- The lesser we tie in upstream dependencies, the lesser prone we are to breaking changes. Also, having all code to read at one place is good for educational purposes and I find it really helpful. For this reason and as an example, any custom triton kernels I introduce for memory/speed efficiency eventually, will either be implemented/benchmarked by ourselves or be copied verbatim from upstream library (for example, sage attention). I do not see this as a burden of maintainence because we have more control over modifying things to fit our design and work with all models we want to support OOTB"
      },
      {
        "user": "sayakpaul",
        "created_at": "2025-01-15T11:24:07Z",
        "body": "All good, thanks a lot for explaining!"
      }
    ]
  },
  {
    "number": 183,
    "title": "Fine Tuning?",
    "created_at": "2025-01-05T08:01:47Z",
    "closed_at": "2025-01-13T18:53:28Z",
    "labels": [
      "in-the-works"
    ],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/183",
    "body": "### System Info / 系統信息\n\n@sayakpaul \r\nIs unlocking finetuning as simple as enabling gradients on the transformer then not adding in the loRA module?\r\nThanks if that's the case I will make a PR.\r\nLet me know if other things need to be done.\n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [X] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nFine Tuning\n\n### Expected behavior / 期待表现\n\nShould be able to Fine Tune",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/183/comments",
    "author": "ArEnSc",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2025-01-05T12:08:56Z",
        "body": "> Is unlocking finetuning as simple as enabling gradients on the transformer then not adding in the loRA module?\r\n\r\nIf you're referring to full-finetuning then yes. It's on our mind and will be added soon. Cc: @a-r-r-o-w. \r\n\r\nShould be relatively straightforward to support. "
      },
      {
        "user": "ArEnSc",
        "created_at": "2025-01-11T17:40:06Z",
        "body": "Would it be easy to verify if I memorize a video sample using the parameters suggested above? I don't have a toy dataset with which to do this. I figured I would find a 5-second video 1 prompt and memorize that video after captioning it within distribution"
      }
    ]
  },
  {
    "number": 182,
    "title": "CogvideoX-1.5 inference all black",
    "created_at": "2025-01-05T08:00:30Z",
    "closed_at": "2025-02-19T00:09:29Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/182",
    "body": "### System Info / 系統信息\n\n1. The diffusers is 0.33.0.dev0\r\n2. The pytorch is 2.5.1\r\n3. The CUDA is 12.4\n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [X] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\n1. The diffusers is 0.33.0.dev0\r\n2. The pytorch is 2.5.1\r\n3. The CUDA is 12.4\r\n\r\nI find that when bfloat16 inference, everything is OK, but it occurs NAN when fp16. Have you find similar problem?\n\n### Expected behavior / 期待表现\n\nI hope that someone can give me solutions to fix this bug.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/182/comments",
    "author": "cococozibojia",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-05T10:00:30Z",
        "body": "Many of the models were trained primarily in bf16, which makes them susceptible to going out of range in fp16. So, unless the underlying model was a full fp16 finetune of original bf16 models, or a fp32 trained model, it will either lead to NaNs or make the quality much worse"
      }
    ]
  },
  {
    "number": 181,
    "title": "Added parameter description",
    "created_at": "2025-01-04T11:20:42Z",
    "closed_at": "2025-01-21T20:52:40Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/181",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/181/comments",
    "author": "ootsuka-repos",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2025-01-05T13:53:44Z",
        "body": "Thanks for this PR!\r\n\r\nSince we provide `help` for almost all the important arguments, I am wondering if using `train.py -h` would help the users know all about them? \r\n\r\nPerhaps we could just add a note about that? @a-r-r-o-w WDYT?"
      }
    ]
  },
  {
    "number": 179,
    "title": "Validation videos generated by inference are all black",
    "created_at": "2025-01-04T04:40:15Z",
    "closed_at": "2025-01-11T01:05:40Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/179",
    "body": "Hello! Thank you for your answer! My problem now is that the validation videos generated by inference are all black. I would like to ask if there is a corresponding solution?",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/179/comments",
    "author": "julia-0105",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-04T05:08:11Z",
        "body": "@julia-0105 Hi!\r\n\r\nIf you're on pytorch versions lower than 2.5.1, you might also be getting a NaN loss alongside black videos. Could you check that and ensure if you're working with pytorch version 2.5.1 or above? \r\n\r\nCould you also share the output from running `diffusers-cli env` and which model you're trying to train, along with the training script launcher? Thanks!"
      }
    ]
  },
  {
    "number": 167,
    "title": "Load Cogvideo SFT weight",
    "created_at": "2024-12-31T07:54:02Z",
    "closed_at": "2025-01-01T03:41:59Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/167",
    "body": "### Feature request  / 功能建议\n\nHi, Great work! I want to ask how can I load the SFT weight of Cogvideo to do inference.\r\nLora weights can be loaded this way:\r\n```\r\n+ pipe.load_lora_weights(\"my-awesome-name/my-awesome-lora\", adapter_name=\"cogvideox-lora\")\r\n+ pipe.set_adapters([\"cogvideox-lora\"], [1.0])\r\n```\r\nHow can SFT weights be loaded?\n\n### Motivation / 动机\n\nLoad Cogvideo SFT weight\n\n### Your contribution / 您的贡献\n\n.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/167/comments",
    "author": "JonathanLi19",
    "comments": [
      {
        "user": "guidongnan",
        "created_at": "2025-01-07T06:01:15Z",
        "body": "Hi, any solution about that? @JonathanLi19 "
      },
      {
        "user": "JonathanLi19",
        "created_at": "2025-01-07T06:08:37Z",
        "body": "You can first load each component's weight, and then pass them to the pipeline. An example is provided below.\r\n```\r\n    tokenizer    = T5Tokenizer.from_pretrained(model_card, subfolder=\"tokenizer\")\r\n    text_encoder = T5EncoderModel.from_pretrained(model_card, subfolder=\"text_encoder\").cuda()\r\n    vae          = AutoencoderKLCogVideoX.from_pretrained(model_card, subfolder=\"vae\").cuda()\r\n    transformer  = CogVideoXTransformer3DModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\r\n    scheduler    = CogVideoXDPMScheduler.from_pretrained(model_card, subfolder=\"scheduler\")\r\n    pipe         = CogVideoXImageToVideoPipeline(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, transformer=transformer, scheduler=scheduler).to(torch.bfloat16)\r\n```\r\n    \r\n\r\n> Hi, any solution about that? @JonathanLi19\r\n\r\n"
      }
    ]
  },
  {
    "number": 161,
    "title": "Index out of range",
    "created_at": "2024-12-29T03:52:15Z",
    "closed_at": "2025-01-23T11:07:00Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/161",
    "body": "### System Info / 系統信息\n\ncuda118, diffusers==0.32.0.dev0\n\n### Information / 问题信息\n\n- [ ] The official example scripts / 官方的示例脚本\n- [X] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nWhen the Dataloader dispatches data, it exceeds the index range of the CSV file, even though the len(Dataloader) matches the dataset’s length. Could there be a bug in the custom BucketSampler implementation?\n\n### Expected behavior / 期待表现\n\nCheck the code of BucketSampler.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/161/comments",
    "author": "suimuc",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-12-30T02:43:02Z",
        "body": "Cc: @a-r-r-o-w \r\n\r\n@suimuc could you also share your training command? "
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-23T11:07:00Z",
        "body": "Closing due to inactivity"
      }
    ]
  },
  {
    "number": 156,
    "title": "Resume and reprod of reproducibility for Hunayuan ",
    "created_at": "2024-12-27T04:03:05Z",
    "closed_at": "2024-12-27T19:59:54Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/156",
    "body": "I notice that hunyuan finetuning script seems not support resume_from_checkpoint mehod and it is signed as \"TODO\". Do you plan to add this. Besides, I wonder how to use your inference API to generate the same results in finetuning validation by the checkpoint of finetuning? Thank you!",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/156/comments",
    "author": "Aristo23333",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-27T19:59:54Z",
        "body": "Support for this has been added in #139. Could you give it a try? Marking the issue as resolved, but if it does not work as expected, please do re-open and ping `@sayakpaul`"
      }
    ]
  },
  {
    "number": 153,
    "title": "How to generate result of validation and resolution. ",
    "created_at": "2024-12-26T15:21:22Z",
    "closed_at": "2025-01-10T23:38:39Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/153",
    "body": "Hi author:\r\nI am using your hunyuan finetuning bash to finetune lora on my own dataset with original resolution of 1080p. But I find your model can only run on video with both height and weight can be divided by 32. Can the model also be trained  on video with 360p or 720p and why?",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/153/comments",
    "author": "Aristo23333",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-28T13:46:22Z",
        "body": "It is possible to train with any resolution of your choice. You can specify different resolution by using the `--resolution_buckets` flag. The arguments must be of the format `{num_frames}x{height}x{width}`.\r\n\r\nFor example: `--resolution_buckets 49x512x768 81x512x512 161x768x1024`. The resizing will automatically try and force your video into these resolution buckets based on whatever would fit best"
      }
    ]
  },
  {
    "number": 148,
    "title": "Grad Norm tracking in DeepSpeed",
    "created_at": "2024-12-25T14:04:59Z",
    "closed_at": "2024-12-25T14:38:09Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/148",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/148/comments",
    "author": "a-r-r-o-w",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-12-25T14:39:22Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 147,
    "title": "does support train lora in 4090  with HunYuan model?",
    "created_at": "2024-12-25T12:00:43Z",
    "closed_at": "2025-01-14T19:10:41Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/147",
    "body": "### System Info / 系統信息\r\n\r\ndiffusers: from source\r\n\r\n### Information / 问题信息\r\n\r\n- [X] The official example scripts / 官方的示例脚本\r\n- [ ] My own modified scripts / 我自己修改的脚本和任务\r\n\r\n### Reproduction / 复现过程\r\n\r\ni have added --precompute_conditions and --gradient_checkpointing with training lora by hunyuan model on 4090, but it still oom\r\ndoes support train lora on 4090  with HunYuan model?\r\n\r\n### Expected behavior / 期待表现\r\n\r\n问题得到解决",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/147/comments",
    "author": "syyxsxx",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-25T12:58:03Z",
        "body": "Unfortunately, it still requires a 48 GB GPU unless one trains with fp8. However, with the new Trainer implementation we are building, fp8 support has not yet been added. We will try and prioritize this as soon as possible!"
      },
      {
        "user": "syyxsxx",
        "created_at": "2024-12-28T03:40:34Z",
        "body": "Thank you for your reply. I'm looking forward to the FP8 training feature."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2025-01-14T19:10:41Z",
        "body": "Hi. We just merged #184, which should allow training under 24 GB upto certain resolution/frame limits. LMK if you face any problems :hugs:"
      }
    ]
  },
  {
    "number": 143,
    "title": "nan loss",
    "created_at": "2024-12-24T08:21:25Z",
    "closed_at": "2024-12-26T01:43:07Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/143",
    "body": "### System Info / 系統信息\r\n\r\nCUDA:  12.4  \r\npython: 3.10\r\ndiffusers: 0.33.0.dev0\r\n\r\n### Information / 问题信息\r\n\r\n- [ ] The official example scripts / 官方的示例脚本\r\n- [X] My own modified scripts / 我自己修改的脚本和任务\r\n\r\n### Reproduction / 复现过程\r\n\r\n1. I randomly selected a few videos to fine-tune hunyuanvideo\r\n2. But I ended up with nan loss at the first step.\r\n3. the log is \r\n`Training steps:   2%|██                                                                                                        | 1/50 [00:56<46:19, 56.73s/it, loss=nan, lr=2e-7]`\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior / 期待表现\r\n\r\nGet the correct loss value.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/143/comments",
    "author": "tanshuai0219",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-25T22:24:39Z",
        "body": "@tanshuai0219 I believe we interacted in the Diffusers issue too. Upgrading pytorch to 2.5.1 has been confirmed to fix the nan training loss problem by atleast 3 other people now. Could you give it a try?"
      },
      {
        "user": "tanshuai0219",
        "created_at": "2024-12-26T01:43:07Z",
        "body": "> @tanshuai0219 I believe we interacted in the Diffusers issue too. Upgrading pytorch to 2.5.1 has been confirmed to fix the nan training loss problem by atleast 3 other people now. Could you give it a try?\r\n\r\nYes,it works for me~ The loss value is normal now. Thanks!"
      },
      {
        "user": "BlackTea-c",
        "created_at": "2025-01-17T03:42:20Z",
        "body": "same problem.butmy torch version is 2.5.1"
      }
    ]
  },
  {
    "number": 141,
    "title": "Videos are being Orange and encoded wrong?",
    "created_at": "2024-12-24T07:31:10Z",
    "closed_at": "2025-01-02T22:14:42Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/141",
    "body": "### System Info / 系統信息\n\nFor the LTX LoRA I get orange ish videos that are encoded like this.\r\n\r\n```\r\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from '/Users/bt/Downloads/validation_399_be63d7482f2c5f9194f0.mp4':\r\n  Metadata:\r\n    major_brand     : isom\r\n    minor_version   : 512\r\n    compatible_brands: isomiso2mp41\r\n  Duration: 00:00:03.27, start: 0.000000, bitrate: 3212 kb/s\r\n  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 768x512 [SAR 1:1 DAR 3:2], 3209 kb/s, 15 fps, 15 tbr, 15360 tbn (default)\r\n      Metadata:\r\n        handler_name    : VideoHandler\r\n        vendor_id       : [0][0][0][0]\r\n```\r\n\r\nand rather than 24 fps they are 15 fps :S which is confusing! thanks lmk if this is a problem I am going to be looking downstream as well. \n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\norange videos during validation\r\nthat don't play and show only the first image\n\n### Expected behavior / 期待表现\n\nvideos are normal color\r\nthey play and have 24 fps?",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/141/comments",
    "author": "ArEnSc",
    "comments": [
      {
        "user": "neph1",
        "created_at": "2024-12-24T12:21:03Z",
        "body": "How do you inference? I've inferenced both with diffusers and comfyui (although the latter encoded them as webp) and both played fine (and not orange).\r\nOr is it just a problem with the validation videos (I haven't done any)"
      },
      {
        "user": "ArEnSc",
        "created_at": "2024-12-28T02:22:30Z",
        "body": "> How do you inference? I've inferenced both with diffusers and comfyui (although the latter encoded them as webp) and both played fine (and not orange). Or is it just a problem with the validation videos (I haven't done any)\r\n\r\nhow did you infer? Yeah this is during validation pass during lora training"
      },
      {
        "user": "sayakpaul",
        "created_at": "2025-01-02T14:20:07Z",
        "body": "Without any visual it's a little tricky to understand the issue. Also, your training command and training data could play a part here. In our tests, we didn't experience this yet."
      },
      {
        "user": "ArEnSc",
        "created_at": "2025-01-02T22:14:42Z",
        "body": "@sayakpaul @neph1 I resolved it, it had to do with image io and hugging face I just updated the package and it worked again thanks for commenting on this"
      },
      {
        "user": "sayakpaul",
        "created_at": "2025-01-03T02:00:20Z",
        "body": "Glad!\r\n\r\nDo you think we should update the README about this possible issue? If so, would you maybe interested in opening a PR?"
      }
    ]
  },
  {
    "number": 140,
    "title": "Bucketing Explained?",
    "created_at": "2024-12-24T07:22:40Z",
    "closed_at": "2024-12-28T01:53:43Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/140",
    "body": "### System Info / 系統信息\n\nHey, what is Bucketing's purpose and how does it work with the fine trainer's library and what does it do?\r\n\r\nMy thought from my experience in training is, it allows generalizing the number of frames and resolutions by exposing the model to different samples. Is this correct?\r\nIt supports different resolutions + durations through the number of frames.\r\n\r\nMy question surrounding this was, then what is the correct settings for LTXV ?\r\nit seems like they say they support\r\n\r\n24 FPS videos at 768x512\r\nSo the bucket sampler says more or less by default needs 2 second videos at that resolution, with this configuration \r\n 49x512x768?\r\n \r\nDoes this mean it will only use videos within that frame range and or truncate anything out of bounds of the number of frames?\r\nI assume it if you add more parameters like so\r\n\r\n17x512x768 49x512x768 61x512x768 129x512x768\r\n\r\nIt will find the closest number of frames for a sample then truncate the remainder or padd the remainder of frames correct?\r\n\r\nThanks\r\n@a-r-r-o-w \r\n\n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nThis is just a question\n\n### Expected behavior / 期待表现\n\nAnswer to the question.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/140/comments",
    "author": "ArEnSc",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-25T22:36:12Z",
        "body": "The purpose of bucketing is to enable batch_size > 1 training and allowing training only with some fixed resolutions and number of frames.\r\n\r\nFor example, setting it to `49x512x768 81x1024x1024` will round all your videos down to the nearest resolution/frame bucket. If a video has 60 frames, some of its in-between frames will be discarded to select a total of 49 frames uniformly, and resizing to `512x768` resolution will be performed. If a video has 80 frames, it will still be put in the 49 frame bucket. So, the behaviour is to find the closest smaller bucket size in which the video does not fit. Multiple videos put into same bucket can be combined together for a single forward pass, if batch size is greater than 1. There is no padding involved\r\n\r\nIf no smaller bucket size is compatible for a video (let's say, it has lesser number of frames than the smallest specified frame bucket size), it can either be discarded or used individually  based on some dataset CLI argumnets"
      },
      {
        "user": "ArEnSc",
        "created_at": "2024-12-28T01:53:41Z",
        "body": "@a-r-r-o-w thanks for the explainer! much appreciated!"
      }
    ]
  },
  {
    "number": 132,
    "title": "Error when using batch_size.",
    "created_at": "2024-12-22T10:35:20Z",
    "closed_at": "2025-01-01T12:18:57Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/132",
    "body": "### System Info / 系統信息\n\nThank you for providing a way to finetune LTX-video. Training seems to work well, but when I try changing batching (in this case to 4), I encounter this exception. I'm using the suggested example script in the repo. Running on diffusers from github, updated two days ago. \r\n\r\nThis is the exception:\r\n\r\n```\r\n12/21/2024 12:38:27 - ERROR - finetrainers - Traceback (most recent call last):\r\n  File \"/some_path/train.py\", line 34, in main\r\n    trainer.train()\r\n  File \"/some_path/trainer.py\", line 424, in train\r\n    noisy_latents = (1.0 - sigmas) * latent_conditions[\"latents\"] + sigmas * noise\r\nRuntimeError: The size of tensor a (4) must match the size of tensor b (128) at non-singleton dimension 2\r\n```\r\n\r\nAlso hijacking my own issue: \r\nI tried adding `--resume_from_checkpoint` since that worked with the old cogvideox-factory trainer. But it doesn't seem to do anything now. Is that correct?\r\n\r\nThanks!\n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nChange   `--batch_size 1` to something other than 1\n\n### Expected behavior / 期待表现\n\nTraining would continue with batching enabled",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/132/comments",
    "author": "neph1",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-22T22:52:43Z",
        "body": "Oh interesting, thanks for reporting! I'll look into this shortly.\r\n\r\n`--resume_from_checkpoint` support will be added soon too. I think #130 has it working, so it will be in `main` soon hopefully"
      },
      {
        "user": "neph1",
        "created_at": "2025-01-01T12:18:57Z",
        "body": "This is no longer an issue."
      }
    ]
  },
  {
    "number": 125,
    "title": "conv3d error",
    "created_at": "2024-12-17T19:02:10Z",
    "closed_at": "2025-01-10T23:40:17Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/125",
    "body": "### System Info / 系統信息\r\n\r\ndiffusers==0.32.0.dev0\r\n\r\n\r\n### Information / 问题信息\r\n\r\n- [ ] The official example scripts / 官方的示例脚本\r\n- [X] My own modified scripts / 我自己修改的脚本和任务\r\n\r\n### Reproduction / 复现过程\r\n\r\nI train the Lora for I2V, and when tried simple inference using the following code. I see the following error. '\r\n`lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 603, in _conv_forward\r\n    return F.conv3d(\r\n           ^^^^^^^^^\r\nRuntimeError: Calculated padded input size per channel: (1 x 2402 x 2402). Kernel size: (3 x 3 x 3). Kernel size can't be greater than actual input size`\r\n\r\nFollowing is the code where I am extracting the first frame from my test video and reading the prompt.\r\n\r\n```\r\nimport argparse\r\nimport os\r\nimport torch\r\nimport cv2\r\nfrom diffusers import CogVideoXImageToVideoPipeline\r\nfrom diffusers.utils import export_to_video, load_image\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nfrom PIL import Image\r\n\r\ndef extract_first_frame(video_path):\r\n    \"\"\"Extract the first frame from a video file, convert to PIL.Image, and resize to 480x720.\"\"\"\r\n    cap = cv2.VideoCapture(video_path)\r\n    ret, frame = cap.read()\r\n    cap.release()\r\n    if not ret:\r\n        raise ValueError(f\"Failed to read the first frame from {video_path}\")\r\n    \r\n    # Convert frame (BGR to RGB)\r\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n    print(\"Original Frame Shape:\", frame_rgb.shape)  # Shape: (H, W, C)\r\n\r\n    # Convert to PIL.Image\r\n    image = Image.fromarray(frame_rgb)\r\n\r\n    # Resize to 480x720\r\n    image_resized = image.resize((720, 480))  # Resize to (width=720, height=480)\r\n    print(\"Resized Image Size (PIL):\", image_resized.size)  # Shape: (W, H)\r\n\r\n    # Confirm channels by converting back to NumPy\r\n    resized_np = np.array(image_resized)\r\n    print(\"Resized Image Shape (NumPy):\", resized_np.shape)  # Should confirm 3 channels (H, W, C)\r\n    \r\n    return image_resized\r\n\r\n\r\n\r\ndef generate_video(model_path, image, prompt, lora_path, lora_name, output_file, fps):\r\n    \"\"\"Generate video using CogVideoX pipeline.\"\"\"\r\n    # Load the pipeline and set LoRA weights\r\n    pipe = CogVideoXImageToVideoPipeline.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\")\r\n    pipe.load_lora_weights(lora_path, weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=lora_name)\r\n    pipe.set_adapters([lora_name], [1.0])\r\n    #del pipe.transformer.patch_embed.pos_embedding\r\n    pipe.transformer.patch_embed.use_learned_positional_embeddings = False\r\n    pipe.transformer.config.use_learned_positional_embeddings = False\r\n\r\n    # Process input image and prompt\r\n    video = pipe(image, prompt=prompt, use_dynamic_cfg=True).frames[0]\r\n    export_to_video(video, output_file, fps=fps)\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\"Generate videos using CogVideoX and LoRA weights\")\r\n    parser.add_argument(\"--input_dir\", type=str, required=False, default=\"./diffusers_data_v1/\", help=\"Path to the directory containing videos and prompt files\")\r\n    parser.add_argument(\"--model_path\", type=str, default=\"THUDM/CogVideoX1.5-5B-I2V\", help=\"Base Model path or HF ID\")\r\n    parser.add_argument(\"--lora_path\", type=str, default=\"./\", required=False, help=\"Path to the LoRA weights\")\r\n    parser.add_argument(\"--lora_name\", type=str, default=\"lora_adapter\", help=\"Name of the LoRA adapter\")\r\n    parser.add_argument(\"--output_dir\", type=str, default=\"./output_videos\", help=\"Directory to save output videos\")\r\n    parser.add_argument(\"--fps\", type=int, default=8, help=\"Frames per second for the output video\")\r\n    args = parser.parse_args()\r\n\r\n    # Paths to input files\r\n    video_list_path = os.path.join(args.input_dir, \"videos.txt\")  # File with video filenames\r\n    prompt_path = os.path.join(args.input_dir, \"prompt.txt\")  # File with prompts\r\n\r\n    # Read video filenames and prompts\r\n    with open(video_list_path, \"r\") as f:\r\n        video_filenames = [line.strip() for line in f if line.strip()]\r\n    with open(prompt_path, \"r\") as f:\r\n        prompts = [line.strip() for line in f if line.strip()]\r\n\r\n    # Ensure the output directory exists\r\n    os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    # Check if lengths match\r\n    if len(video_filenames) != len(prompts):\r\n        raise ValueError(\"Mismatch between number of videos in videos.txt and prompts in prompt.txt.\")\r\n\r\n    # Process each video\r\n    for idx, (video_filename, prompt) in enumerate(zip(video_filenames, prompts)):\r\n        # Construct the full path to the video file\r\n        video_path = os.path.join(args.input_dir, video_filename)  # Use \"videos\" subdirectory\r\n        output_file = os.path.join(args.output_dir, f\"output_{idx}.mp4\")\r\n\r\n        print(f\"Processing video: {video_path} with prompt: '{prompt}'\")\r\n\r\n\r\n        #Extract first frame\r\n        first_frame = extract_first_frame(video_path)\r\n\r\n        # Generate video\r\n        generate_video(\r\n            args.model_path,\r\n            first_frame,\r\n            prompt,\r\n            args.lora_path,\r\n            args.lora_name,\r\n            output_file,\r\n            args.fps\r\n        )\r\n        \r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\nDetailed Error.\r\n\r\n`  File \"/mnt/round-cake/home/umar/cogvideox-factory/./tests/inferency.py\", line 92, in main\r\n    generate_video(\r\n  File \"/mnt/round-cake/home/umar/cogvideox-factory/./tests/inferency.py\", line 50, in generate_video\r\n    video = pipe(image, prompt=prompt, use_dynamic_cfg=True).frames[0]\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/pipelines/cogvideo/pipeline_cogvideox_image2video.py\", line 776, in __call__\r\n    latents, image_latents = self.prepare_latents(\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/pipelines/cogvideo/pipeline_cogvideox_image2video.py\", line 381, in prepare_latents\r\n    image_latents = [retrieve_latents(self.vae.encode(img.unsqueeze(0)), generator) for img in image]\r\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\r\n    return method(self, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 1232, in encode\r\n    h = self._encode(x)\r\n        ^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 1204, in _encode\r\n    x_intermediate, conv_cache = self.encoder(x_intermediate, conv_cache=conv_cache)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 807, in forward\r\n    hidden_states, new_conv_cache[conv_cache_key] = down_block(\r\n                                                    ^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 439, in forward\r\n    hidden_states, new_conv_cache[conv_cache_key] = resnet(\r\n                                                    ^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 304, in forward\r\n    hidden_states, new_conv_cache[\"conv1\"] = self.conv1(hidden_states, conv_cache=conv_cache.get(\"conv1\"))\r\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 144, in forward\r\n    output = self.conv(inputs)\r\n             ^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py\", line 62, in forward\r\n    output_chunks.append(super().forward(input_chunk))\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 608, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/round-cake/home/umar/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 603, in _conv_forward\r\n    return F.conv3d(\r\n           ^^^^^^^^^\r\nRuntimeError: Calculated padded input size per channel: (1 x 2402 x 2402). Kernel size: (3 x 3 x 3). Kernel size can't be greater than actual input size`\r\n\r\n### Expected behavior / 期待表现\r\n\r\nI am running simple inference, but it seems that the vae encoder raises errors.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/125/comments",
    "author": "umarkhalidAI",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-18T23:01:04Z",
        "body": "Could you try passing `height` and `width` as parameters to the pipeline call and see if the error goes away? Currently, it infers that the height and width should be 2400 based on the RoPE configuration from the transformer, but this is incorrect. We've mentioned this in the examples and docs"
      }
    ]
  },
  {
    "number": 121,
    "title": "about  the transformer config in CogVideoX1.5-5B-I2V",
    "created_at": "2024-12-12T06:37:25Z",
    "closed_at": "2025-01-02T14:20:45Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/121",
    "body": "hi, I'm trying to run the cogvideox_image_to_video_sft.py using the disney dataset, but I find that the sample_width and sample_height in transformer config is 300 and 300, which is different from my preprocessed latents shape (w=90,h=60), should i change the sample_width, sample_height  and sample_frames in the transformer config?",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/121/comments",
    "author": "lin076",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-18T23:00:08Z",
        "body": "300 for sample height and width is correct for CogVideoX 1.5. Instead of using a frame/height/width adaptive RoPE, they slice a section of the rope embeddings. You can refer to the implementation of `get_3d_rotary_pos_embed` in Diffusers for an example on how they slice.\r\n\r\nOn your end, I don't think you have to modify in the config. "
      },
      {
        "user": "sayakpaul",
        "created_at": "2025-01-02T14:20:45Z",
        "body": "Closing as I think Aryan has already answered it. Feel free to reopen. "
      }
    ]
  },
  {
    "number": 120,
    "title": "Differences between CogVideoXDPMScheduler and CogVideoXDDIMScheduler for cogvideo-X-1.5-I2V",
    "created_at": "2024-12-11T09:30:23Z",
    "closed_at": "2024-12-19T14:44:06Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/120",
    "body": "It seems that in training script, CogVideoXDPMScheduler is used for both training and validation. \r\n\r\n```scheduler = CogVideoXDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")```\r\n\r\nHowever, when using \r\n```pipe = CogVideoXImageToVideoPipeline.from_pretrained(\"CogVideoX1.5-5B-I2V\", torch_dtype=torch.bfloat16)```\r\nCogVideoXDDIMScheduler is used as can be seen from the diffusers ckpt file ```CogVideoX1.5-5B/scheduler/scheduler_config.json```\r\n```\r\n{\r\n  \"_class_name\": \"CogVideoXDDIMScheduler\",\r\n  \"_diffusers_version\": \"0.32.0.dev0\",\r\n  \"beta_end\": 0.012,\r\n  \"beta_schedule\": \"scaled_linear\",\r\n  \"beta_start\": 0.00085,\r\n  \"clip_sample\": false,\r\n  \"clip_sample_range\": 1.0,\r\n  \"num_train_timesteps\": 1000,\r\n  \"prediction_type\": \"v_prediction\",\r\n  \"rescale_betas_zero_snr\": true,\r\n  \"sample_max_value\": 1.0,\r\n  \"set_alpha_to_one\": true,\r\n  \"snr_shift_scale\": 1.0,\r\n  \"steps_offset\": 0,\r\n  \"timestep_spacing\": \"trailing\",\r\n  \"trained_betas\": null\r\n}\r\n```\r\n\r\nCould you tell me what's the difference between these two schedulers? Thanks a lot.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/120/comments",
    "author": "hjrPhoebus",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-12-19T14:40:05Z",
        "body": "Ccing @zRzRzRzRzRzRzR from the Cog team for more details. But will close this as it's not directly related to `finetrainers`. "
      }
    ]
  },
  {
    "number": 111,
    "title": "Why Batch size 4 training is much slower than Batch size 1 training under deepspeed yaml",
    "created_at": "2024-12-04T02:00:58Z",
    "closed_at": "2025-01-02T14:27:37Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/111",
    "body": "### System Info / 系統信息\n\nThis part is correct, everything works.\n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nI am using 8 A800 GPUS to train the cogvideo 5b model sft, and I realize the training speed is slower when I set batch size to 4 compared with when I set batch size equal to 1. Below is an example of the speed difference. I am using the deepspeed setup to help training.\r\n\r\nGPU train 4 batch 8 gpu:\r\nSteps:   0%|▏                                                                                                                                                          | 18/20000 [23:57<403:54:45, 72.77s/it, loss=0.0777, lr=2.25e-6]\r\ntrain 1 batch 8 gpu:\r\nSteps:   0%|▏                                                                                                                                                           | 17/20000 [07:43<119:15:51, 21.49s/it, loss=0.118, lr=2.13e-6]\r\n\r\n\n\n### Expected behavior / 期待表现\n\nShouldn't it be the other way around, the training should be faster with batch size 4? Maybe it is because of the cpu offload option in the deepspeed?",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/111/comments",
    "author": "Bensong0506",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-04T05:23:27Z",
        "body": "When you increase batch_size, you are doing more computation per step, so this is expected behaviour. In fact, your training is indeed happening slightly faster, because at `batch_size=1`, I see each step taking about `21` seconds, and at `batch_size=4` it's taking a little less than 4x that.\r\n\r\nHowever, note that when you set `batch_size=1`, you are training for `8 GPU x 1 batch_size x 20000 steps` for an effective `160000` sample data points. But for `batch_size=4`, you are training for `8 GPU x 4 batch_size x 20000 steps` for an effective `640000` sample data points. In order to do the comparison fairly, you should reduce the number of training steps to 5000, so that the effective number of samples seen by the model is the same (this will however reduce the number of gradient updates that happen, but that is what is expected when increasing batch_size and keeping effective number of samples seen by the model the same).\r\n\r\nThen, the time taken by `batch_size=1` run would be roughly `21 * 20000 = 420000 seconds`, and the time taken by `batch_size=4` run would be roughly `72 * 5000 = 360000 seconds`, which is indeed lower."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-04T05:27:49Z",
        "body": "On another note, yes DeepSpeed training is a bit slower but it lets you squeeze in higher batch size, due to the gradient offloading and optimizer state existing on the CPU. I would recommend using torch.compile with dynamic=True (if you're doing multiresolution, otherwise dynamic=False would be better) to speed up training quite a bit"
      },
      {
        "user": "lijain",
        "created_at": "2024-12-04T07:20:50Z",
        "body": "> to speed up training quite a bit\r\nDo you want to ask why bs=1 itertime=21.49, bs=4 itertime=72.77, why the itertime of a single card is so large, and then why the multiple cards are multiplied\r\n"
      },
      {
        "user": "Bensong0506",
        "created_at": "2024-12-04T08:39:50Z",
        "body": "Thanks a lot for your kind reply. This really helps a lot! "
      }
    ]
  },
  {
    "number": 101,
    "title": "does sft training  need  consider   vae.config.invert_scale_latents       1.5 I2V",
    "created_at": "2024-11-28T06:31:26Z",
    "closed_at": "2025-01-02T14:30:54Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/101",
    "body": "### System Info / 系統信息\r\n\r\ndiffusers                 0.32.0.dev0  \r\n\r\n### Information / 问题信息\r\n\r\n- [ ] The official example scripts / 官方的示例脚本\r\n- [X] My own modified scripts / 我自己修改的脚本和任务\r\n\r\n### Reproduction / 复现过程\r\n\r\n(1)When testing, the determination of whether to multiply or divide by the vae_scaling_factor_image is based on the vae.config.invert_scale_latents parameter. \r\n        if not self.vae.config.invert_scale_latents:\r\n            image_latents = self.vae_scaling_factor_image * image_latents\r\n        else:\r\n            # This is awkward but required because the CogVideoX team forgot to multiply the\r\n            # scaling factor during training :)\r\n            image_latents = 1 / self.vae_scaling_factor_image * image_latents\r\n\r\nwhy is it that when invert_scaling_latents is set to true, it divides by the VAE scaling factor instead of not multiplying? The latent representations of the images differ by a square when this parameter is true versus false.?\r\n\r\n(2) In my case，the SFT training process did not handle this parameter, resulting in darker test outputs. \r\nThe impact of the first frame image is becoming increasingly insignificant.\r\n\r\n### Expected behavior / 期待表现\r\n\r\nNormal brightness\r\n\r\n",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/101/comments",
    "author": "zy0406",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-11-29T12:29:18Z",
        "body": "Cc: @zRzRzRzRzRzRzR @a-r-r-o-w "
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-01T18:35:35Z",
        "body": "As the comment explains, the scaling factor was not multiplied during training, and I believe it was divided instead. This is what causes the squared difference, but I will let @zRzRzRzRzRzRzR, from the CogVideoX team, comment further. I don't think the scripts in this repo take this into account yet. I will run some finetuning tests and report my results"
      },
      {
        "user": "zRzRzRzRzRzRzR",
        "created_at": "2024-12-05T07:18:49Z",
        "body": "Thank you for your help @a-r-r-o-w.\r\nWhen CogVideoX 1.5 was trained, the image condition was not multiplied, so during inference, the image condition cannot be multiplied, but before the final decode, it still needs to be divided by the scale factor."
      },
      {
        "user": "zRzRzRzRzRzRzR",
        "created_at": "2024-12-05T07:19:58Z",
        "body": "Therefore, during training, an image condition is needed, and during reasoning, when using the decoder, it is necessary to divide by the scale factor"
      },
      {
        "user": "zy0406",
        "created_at": "2024-12-05T07:32:27Z",
        "body": "> Therefore, during training, an image condition is needed, and during reasoning, when using the decoder, it is necessary to divide by the scale factor\r\n\r\nThank you for your reply. I have indeed tried training using the method described below, and the generated videos are relatively normal.\r\n\r\n\r\n```\r\n                if not vae.config.invert_scale_latents:\r\n                    image_latents = image_latent_dist.sample() * VAE_SCALING_FACTOR\r\n                else:\r\n                     image_latents =  image_latent_dist.sample()\r\n                image_latents = image_latents.permute(0, 2, 1, 3, 4)  # [B, F, C, H, W]\r\n                image_latents = image_latents.to(memory_format=torch.contiguous_format, dtype=weight_dtype) \r\n                video_latents = latent_dist.sample() * VAE_SCALING_FACTOR\r\n```"
      },
      {
        "user": "sayakpaul",
        "created_at": "2025-01-02T14:30:54Z",
        "body": "Closing the issue then. Feel free to reopen."
      }
    ]
  },
  {
    "number": 84,
    "title": "sft with multigpu",
    "created_at": "2024-11-12T07:24:40Z",
    "closed_at": "2024-11-27T09:02:29Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/84",
    "body": "sft with multigpu and gradient accumulation, gradient_norm_before_clip undefined error",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/84/comments",
    "author": "zhipuch",
    "comments": [
      {
        "user": "glide-the",
        "created_at": "2024-11-27T09:01:51Z",
        "body": "It seems that there is a bug in the accelerator.is_main_process task and accelerator.distributed_type. Is_main_process has scheduling problems during the training initiation and training stages.\r\nLine 540 also has such a problem.\r\ntracker_name = args.tracker_name or \"cogvideox-sft\""
      }
    ]
  },
  {
    "number": 80,
    "title": "does support multi card training？",
    "created_at": "2024-11-07T11:21:50Z",
    "closed_at": "2024-11-19T09:04:05Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/80",
    "body": "### System Info / 系統信息\n\ngpu: 4090\n\n### Information / 问题信息\n\n- [ ] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\ni use two 4090 training lora, with set the accelerate config，but time has become longer\r\nhow  using multi card to training？\r\n\r\nthanks\n\n### Expected behavior / 期待表现\n\n问题得到解决",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/80/comments",
    "author": "syyxsxx",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-11-11T18:29:24Z",
        "body": "Yes, multi-GPU training is supported. You need to set the correct accelerate config in order to do this.\r\n\r\nWhen you say time has become longer, can you pinpoint what exactly is slower?"
      },
      {
        "user": "syyxsxx",
        "created_at": "2024-11-13T09:47:01Z",
        "body": "@a-r-r-o-w thanks for replay, i will try later"
      }
    ]
  },
  {
    "number": 72,
    "title": "way to reduce memory cost",
    "created_at": "2024-10-28T06:45:35Z",
    "closed_at": "2024-11-18T11:38:20Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/72",
    "body": "Hi, thanks for the project. I wonder why the current scripts can save a lot of gpu memory costs compared to cogvideo repo. Another question is that I found inference cost more gpu(npu in my environment) memory than training, and I encountered oom in npu environment（with 64GB npu memory）, could you please give me some hints for reasons of this phenomenon (inference cost more memory than training). \r\nThank you!",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/72/comments",
    "author": "WangFeng18",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-11-14T09:38:09Z",
        "body": "> Hi, thanks for the project. I wonder why the current scripts can save a lot of gpu memory costs compared to cogvideo repo.\r\n\r\nThe memory savings primarily come from precomputation of latents and text embeddings. \r\n\r\n> Another question is that I found inference cost more gpu(npu in my environment) memory than training, and I encountered oom in npu environment（with 64GB npu memory）, could you please give me some hints for reasons of this phenomenon (inference cost more memory than training).\r\n\r\nWe don't have access to an NPU device so, we're unable to offer help regarding that at the moment. "
      }
    ]
  },
  {
    "number": 71,
    "title": "For whom has problem when inferencing",
    "created_at": "2024-10-28T04:22:15Z",
    "closed_at": "2024-10-28T07:21:26Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/71",
    "body": "### System Info / 系統信息\n\nDiffusers = 0.31\n\n### Information / 问题信息\n\n- [X] The official example scripts / 官方的示例脚本\n- [ ] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nfrom diffusers import export_to_video\r\n\r\nshould be \r\nfrom diffusers.utils import export_to_video\r\n\n\n### Expected behavior / 期待表现\n\nError info: cannot import name 'export_to_video' from 'diffusers'",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/71/comments",
    "author": "HWT-WalterHu",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-28T05:19:44Z",
        "body": "Thanks. Which file are you referring to that contains this error?"
      },
      {
        "user": "HWT-WalterHu",
        "created_at": "2024-10-28T07:17:40Z",
        "body": "> Thanks. Which file are you referring to that contains this error?\r\n\r\nThe codes are from the inference example in main-page of the project(REAMDE.md),  section of quickstart."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-28T07:20:17Z",
        "body": "Oh I see, thanks - will fix asap!"
      }
    ]
  },
  {
    "number": 69,
    "title": "TARGET_FPS variable in prepare_dataset.sh",
    "created_at": "2024-10-24T23:38:20Z",
    "closed_at": "2024-10-28T12:09:34Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/69",
    "body": "I am confused about the purpose of the TARGET_FPS variable in prepare_dataset.sh. I have videos consisting of 128 frames. What is the effect of TARGET_FPS = 8 in this case? ",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/69/comments",
    "author": "MustafaUtkuAydogdu",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-27T10:57:52Z",
        "body": "TARGET_FPS does nothing but save output pre-processed videos at a specific FPS. It does not have anything to do with training. So, if you choose frame buckets as say `48` and `88`, your preprocessed videos will be saved with lengths 6 seconds and 11 seconds. Even if it is not a perfect multiple, it will not affect the number of frames you are using for training so you can safely ignore it"
      }
    ]
  },
  {
    "number": 65,
    "title": "Time for a single iteration of the 5B I2V model under the full-parameter SFT training setup",
    "created_at": "2024-10-22T06:42:24Z",
    "closed_at": "2025-01-10T23:39:55Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/65",
    "body": " I am training on an H100, and a single iteration takes about 18 seconds, but in the SAT of Cogvideo-main, it only takes 8 seconds. Why is this?",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/65/comments",
    "author": "CacacaLalala",
    "comments": [
      {
        "user": "adamas-fsw",
        "created_at": "2024-10-22T08:59:39Z",
        "body": "HI! I wander ask how should I execute the code for SAT SFT, and how should the data be stored?"
      },
      {
        "user": "CacacaLalala",
        "created_at": "2024-10-22T09:05:12Z",
        "body": "Just use finetune_single_gpu.sh or finetune_multi_gpus.sh. And the base yaml is cogvideox_i2v_5b.yaml\r\nHere is the example:\r\n`echo \"RUN on `hostname`, CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\r\n\r\nenvirons=\"WORLD_SIZE=1 RANK=0 LOCAL_RANK=0 LOCAL_WORLD_SIZE=1\"\r\n\r\nrun_cmd=\"$environs python train_video.py --base configs/cogvideox_5b_i2v.yaml configs/sft.yaml --seed $RANDOM\"\r\n\r\necho ${run_cmd}\r\neval ${run_cmd}\r\n\r\necho \"DONE on `hostname`\"`\r\nData can be loaded through csv file or others."
      },
      {
        "user": "adamas-fsw",
        "created_at": "2024-10-22T09:10:47Z",
        "body": "Thanks for your answering, i'm confused of data structure, I wander ask how do you put the image data into dataset and how code can read it"
      },
      {
        "user": "CacacaLalala",
        "created_at": "2024-10-22T09:12:19Z",
        "body": "I didn't use image data, only video data."
      },
      {
        "user": "adamas-fsw",
        "created_at": "2024-10-22T09:13:29Z",
        "body": "fine，thanks a lot！"
      },
      {
        "user": "chenbinghui1",
        "created_at": "2024-11-27T07:12:10Z",
        "body": "@CacacaLalala  I also met the same issue, have you solved this problem. I am on A100, using deepspeed for sft, the data_worker_num is set to 4 and the training time for each iter is nearly 13s."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-11-27T07:49:30Z",
        "body": "You can remove gradnorm calculation to get a little speedup as well. I am yet to find time for profiling the scripts for where the slow synchronizations are occuring. Any insights or PRs would be appreciated!"
      },
      {
        "user": "lijain",
        "created_at": "2024-11-28T03:04:05Z",
        "body": "> @CacacaLalala I also met the same issue, have you solved this problem. I am on A100, using deepspeed for sft, the data_worker_num is set to 4 and the training time for each iter is nearly 13s.\r\nI have the same card with the same problem. Could you please tell me that you have solved it? Or what good method does the landlord have\r\n"
      },
      {
        "user": "sayakpaul",
        "created_at": "2024-11-29T12:24:56Z",
        "body": "Maybe @zRzRzRzRzRzRzR could help us figure out anything obvious that we're doing wrong that is contributing to the slow-down?"
      },
      {
        "user": "lijain",
        "created_at": "2024-12-03T06:22:58Z",
        "body": "At present, I trained on 16 cards of A100 80g, batchsize=4, picture size 384x640, and found that a row backward(loss) would take about 12s, Moreover, the processing time of vae.encode(videos).latent_dist will also take about 8s (the pre-processing of images can be tensor). At present, I do not have a good solution, may I ask you to solve?\r\nLater, with the enlargement of the picture, the occupying time of the top two pieces will also increase greatly.\r\n"
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-03T06:25:34Z",
        "body": "Yes, I plan to work on optimizing the scripts as soon as I find some time, but at the moment the vae encode behaviour is slow whether you start from an image or a precomputed latent (marginally a bit faster). I am unsure what causes this because for precomputed tensor, we just need to continuously stream to GPU and there is no VAE encode step involved. If you have any insights, or would like to help improve this, PRs are very welcome!"
      },
      {
        "user": "lijain",
        "created_at": "2024-12-04T01:32:29Z",
        "body": "> backward(loss)\r\n\r\nI really want to give opinions, as mentioned above, vae is really no way to save time. What I can't figure out is that the time of the line accelerator.backward(loss) is 12s longer with the same setting. I have located this problem before, and there is no better optimization method at present. Or do you have any ideas"
      },
      {
        "user": "buoyancy99",
        "created_at": "2024-12-21T09:42:43Z",
        "body": "> At present, I trained on 16 cards of A100 80g, batchsize=4, picture size 384x640, and found that a row backward(loss) would take about 12s, Moreover, the processing time of vae.encode(videos).latent_dist will also take about 8s (the pre-processing of images can be tensor). At present, I do not have a good solution, may I ask you to solve? Later, with the enlargement of the picture, the occupying time of the top two pieces will also increase greatly.\r\n\r\nIs the batch size total batch size or per GPU batch size?"
      },
      {
        "user": "lijain",
        "created_at": "2024-12-25T01:15:48Z",
        "body": "The batch size on each gpu is 4, for a total of 16 Gpus"
      },
      {
        "user": "buoyancy99",
        "created_at": "2024-12-25T03:54:14Z",
        "body": "> The batch size on each gpu is 4, for a total of 16 Gpus\r\n\r\nAre you using deepspeed? I found CPU offload to slow down training significantly but it does saves memory to make batch size 4 possible"
      },
      {
        "user": "lijain",
        "created_at": "2024-12-27T01:12:09Z",
        "body": "Have you tried what iter is without the cpu? Some time ago, I found that the acceleration may not be used in the calculation of attn, and I will change it later when there is time"
      }
    ]
  },
  {
    "number": 64,
    "title": "Is portrait resolution supported?",
    "created_at": "2024-10-22T06:35:03Z",
    "closed_at": "2024-11-11T18:26:43Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/64",
    "body": "### System Info / 系統信息\n\nIt'd be great to train with portrait(竖屏) videos\n\n### Information / 问题信息\n\n- [ ] The official example scripts / 官方的示例脚本\n- [X] My own modified scripts / 我自己修改的脚本和任务\n\n### Reproduction / 复现过程\n\nN/A\n\n### Expected behavior / 期待表现\n\nAble to train with portrait videos",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/64/comments",
    "author": "jinqiupeter",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-10-23T04:56:46Z",
        "body": "Provide more details in English so that all the maintainers of this repository could take part in the discussions. "
      },
      {
        "user": "jinqiupeter",
        "created_at": "2024-10-26T16:17:55Z",
        "body": "I was asking if cogvideo-factory can train portrait resolutions such as (1280 height, 720 width)."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-28T12:01:32Z",
        "body": "Yes, after #31, you should be able to train at any resolution for I2V models. T2V multiresolution training is already supported - you need to specify `--height_buckets`, `--width_buckets` and `--frame_buckets`. You can set these to your custom portrait resolutions. Will try and make it more convenient soon"
      },
      {
        "user": "jinqiupeter",
        "created_at": "2024-10-29T03:16:26Z",
        "body": "Thank you! Looking forward to #31 to be merged."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-11-11T18:26:43Z",
        "body": "Thanks for patiently waiting! It has been merged now. Feel free to try it out and let me know if you encounter any problems"
      }
    ]
  },
  {
    "number": 62,
    "title": "(typo):I've made some corrections to your Documentation",
    "created_at": "2024-10-21T15:14:13Z",
    "closed_at": "2024-10-21T21:11:58Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/62",
    "body": "I have made some corrections to your repository to help clarify certain aspects. These improvements aim to enhance the overall understanding and usability of the project. I hope they contribute positively to your work!",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/62/comments",
    "author": "PrathameshSPawar",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-21T21:11:58Z",
        "body": "thanks, but these are rather unnecessary. would prefer same casing across table headings, and the other changes seem not so significant in relation to what is trying to be conveyed"
      },
      {
        "user": "PrathameshSPawar",
        "created_at": "2024-10-22T15:35:16Z",
        "body": "ok no issue"
      }
    ]
  },
  {
    "number": 60,
    "title": "prepare_dataset.sh throws RuntimeError: use_libuv was requested but PyTorch was build without libuv support (Windows)",
    "created_at": "2024-10-20T21:18:33Z",
    "closed_at": "2024-11-29T12:19:13Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/60",
    "body": "When running `prepare_dataset.sh` in Windows, PyTorch installed through pip will throw an error about not being built with libuv support. I tried installing PyTorch from source with libuv support but never succeeded. The build completes, but the installation always fails in the final phase. I wrote a post about this on the PyTorch subreddit.\r\n\r\nI managed to bypass the issue by removing the `torchrun` line in `$CMD_WITH_PRE_ENCODING`. This will work fine with PyTorch installed directly from pip.\r\n\r\nSince I only have one GPU, I don't need the `$NUM_GPUS` parameter. However, I'm unsure how to solve this for a multi-GPU setup without the help of `torchrun` set in Windows.\r\n\r\n@a-r-r-o-w Let me know if you want me to try a different approach or if you have any other ideas for a solution.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/60/comments",
    "author": "Nojahhh",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-10-23T04:57:37Z",
        "body": "Thanks!\r\n\r\n> Since I only have one GPU, I don't need the $NUM_GPUS parameter. However, I'm unsure how to solve this for a multi-GPU setup without the help of torchrun set in Windows.\r\n\r\nEven in that case `torchrun` should work. "
      },
      {
        "user": "sayakpaul",
        "created_at": "2024-11-29T12:19:13Z",
        "body": "closing due to inactivity. "
      }
    ]
  },
  {
    "number": 56,
    "title": "Can I change frame_buckets parameter In case of LORA training?",
    "created_at": "2024-10-20T14:17:19Z",
    "closed_at": "2024-10-20T20:20:10Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/56",
    "body": "Can I change frame_buckets parameter in case of LORA training? (if I want to train on 16 frames videos)\r\nOr it should be 49 as the base model? (in my case THUDM/CogVideoX-5b-I2V)\r\n",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/56/comments",
    "author": "FDoKE",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-20T20:20:07Z",
        "body": "Yes, you should be able to change the number of frames. However, keep in mind that the VAE implementation is only compatible with `4 * k` or `4 * k + 1` number of frames, and the first frame will be taken as condition for I2V."
      }
    ]
  },
  {
    "number": 55,
    "title": "Support passing custom height and width for validation resolutions",
    "created_at": "2024-10-19T22:49:27Z",
    "closed_at": "2024-12-18T23:04:10Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/55",
    "body": "When doing multi-resolution/frame training, validation only occurs for 49 x 720 x 480 videos. Since we support multiple prompts (`--validation_prompts`) and images as validation inputs (`--validation_images`) by making use of the `--validation_prompt_separator`, I think it also makes sense to support multiple resolutions via a `--validation_resolutions` parameter.\r\n\r\n@zRzRzRzRzRzRzR @sayakpaul ",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/55/comments",
    "author": "a-r-r-o-w",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-10-20T04:22:31Z",
        "body": "Yeah not a bad idea but the logic could get messy if I am envisioning it correctly. Better could be add a small note saying something like \"since this trained using multiple resolutions, be sure to test it on multiple resolutions to confirm effectiveness.\" "
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-18T23:04:08Z",
        "body": "Supported now in the trainer API with `{prompt}@@@49x480x720`. The part after `@@@` is the num_frames, height and width."
      }
    ]
  },
  {
    "number": 48,
    "title": "Windows support for T2V scripts",
    "created_at": "2024-10-18T23:07:39Z",
    "closed_at": "2024-10-20T21:10:05Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/48",
    "body": "@Nojahhh Would you mind giving this a review? You've already fixed the dataset side of things with lambda pickling error, and this PR replaces the inline collate_fn with a class instance as per your PR #32 in the remaining scripts ",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/48/comments",
    "author": "a-r-r-o-w",
    "comments": [
      {
        "user": "Nojahhh",
        "created_at": "2024-10-19T16:43:49Z",
        "body": "@a-r-r-o-w I think it looks good. Will try it out and report back if I encounter any errors."
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-20T21:09:50Z",
        "body": "Thanks for all your help @Nojahhh!"
      }
    ]
  },
  {
    "number": 47,
    "title": "New prepare dataset = instant oom even on one video",
    "created_at": "2024-10-18T16:38:31Z",
    "closed_at": "2024-10-20T20:34:32Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/47",
    "body": "I have run prepare_dataset.py in the past but since the rework I can't get past encoding videos, it takes up my 24gb 4090 and my 64gb of RAM",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/47/comments",
    "author": "Cubey42",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-18T19:40:25Z",
        "body": "Could you share the exact command you are running with? Will try to repro on 24gb 4090 as well. It's important to make sure `--enable_tiling`, and `--enable_slicing` (if batch_size is > 1) are used. The intermediate states in VAE can take close to 20 GB by themselves apart from model weights if tiling is not enabled."
      },
      {
        "user": "Cubey42",
        "created_at": "2024-10-20T19:13:23Z",
        "body": "I did not prepare with tiling or slicing, I just used my old command, I'll try again today"
      },
      {
        "user": "Cubey42",
        "created_at": "2024-10-20T20:34:31Z",
        "body": "It was my lack of --use_tiling, thank you."
      }
    ]
  },
  {
    "number": 40,
    "title": "How to load the fine-tuned I2V model's LoRA module",
    "created_at": "2024-10-16T17:25:21Z",
    "closed_at": "2024-10-16T18:07:54Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/40",
    "body": "I have successfully fine-tuned an I2V model (locally, without pushing to HF) and would like to load it for inference. I use the following code suggested in the readme\r\n\r\n```\r\nmodel_name = \"THUDM/CogVideoX-5b-I2V\" \r\npipe = CogVideoXImageToVideoPipeline.from_pretrained(\r\n    model_name, torch_dtype=torch.bfloat16\r\n).to(\"cuda\")\r\n\r\npipe.load_lora_weights(\"MyLocalLoRAPath\", adapter_name=[\"cogvideox-lora\"])\r\npipe.set_adapters([\"cogvideox-lora\"], [1.0])\r\n```\r\n\r\nHowever I encounter the error \r\n\r\n```\r\nFile ~/anaconda3/envs/cogvideox-i2v/lib/python3.11/site-packages/diffusers/loaders/lora_pipeline.py:2451, in CogVideoXLoraLoaderMixin.load_lora_into_transformer(cls, state_dict, transformer, adapter_name, _pipeline):\r\n\r\nif adapter_name in getattr(transformer, \"peft_config\", {}):\r\naise ValueError(\r\n   f\"Adapter name {adapter_name} already in use in the transformer - please select a new adapter name.\"    )\r\n\r\nTypeError: unhashable type: 'list'\r\n```\r\n\r\nNote: in the trained LoRA folders, there is only a `pytorch_lora_weights.safetensors`",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/40/comments",
    "author": "Yuancheng-Xu",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-16T17:48:29Z",
        "body": "In pipe.load_lora_weights, please pass just the string for adapter_name and not a list. Just saw that the README has a mistake. Will fix asap"
      },
      {
        "user": "Yuancheng-Xu",
        "created_at": "2024-10-16T18:07:54Z",
        "body": "Yep it works Thank you!"
      },
      {
        "user": "euminds",
        "created_at": "2024-12-03T03:01:22Z",
        "body": "> I have successfully fine-tuned an I2V model (locally, without pushing to HF) and would like to load it for inference. I use the following code suggested in the readme\r\n> \r\n> ```\r\n> model_name = \"THUDM/CogVideoX-5b-I2V\" \r\n> pipe = CogVideoXImageToVideoPipeline.from_pretrained(\r\n>     model_name, torch_dtype=torch.bfloat16\r\n> ).to(\"cuda\")\r\n> \r\n> pipe.load_lora_weights(\"MyLocalLoRAPath\", adapter_name=[\"cogvideox-lora\"])\r\n> pipe.set_adapters([\"cogvideox-lora\"], [1.0])\r\n> ```\r\n> \r\n> However I encounter the error\r\n> \r\n> ```\r\n> File ~/anaconda3/envs/cogvideox-i2v/lib/python3.11/site-packages/diffusers/loaders/lora_pipeline.py:2451, in CogVideoXLoraLoaderMixin.load_lora_into_transformer(cls, state_dict, transformer, adapter_name, _pipeline):\r\n> \r\n> if adapter_name in getattr(transformer, \"peft_config\", {}):\r\n> aise ValueError(\r\n>    f\"Adapter name {adapter_name} already in use in the transformer - please select a new adapter name.\"    )\r\n> \r\n> TypeError: unhashable type: 'list'\r\n> ```\r\n> \r\n> Note: in the trained LoRA folders, there is only a `pytorch_lora_weights.safetensors`\r\n\r\n\r\nIs your device a 4090 24GB or H100 or A100?\r\n"
      }
    ]
  },
  {
    "number": 32,
    "title": "Update for windows compability",
    "created_at": "2024-10-15T06:38:14Z",
    "closed_at": "2024-10-15T13:40:50Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/32",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/32/comments",
    "author": "Nojahhh",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-15T08:58:33Z",
        "body": "Thanks for the awesome work and helping this run on Windows! Many more folks should be able to run it now :)\n\nI think it's okay to have the image.permute part. Could you revert the last commit?"
      },
      {
        "user": "Nojahhh",
        "created_at": "2024-10-15T09:30:20Z",
        "body": "You're welcome! I'm just happy to help and appreciate the work you do for the community.\r\n\r\nI reverted the changes and think this PR is ready for merge. Let me know if there is anything else I can assist with in future updates!"
      }
    ]
  },
  {
    "number": 15,
    "title": "add \"max_sequence_length\": model_config.max_text_seq_length,",
    "created_at": "2024-10-10T09:35:09Z",
    "closed_at": "2024-10-10T11:38:40Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/15",
    "body": "prepare_dataset.sh Include max_sequence_length, bug train verify log not it ",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/15/comments",
    "author": "glide-the",
    "comments": [
      {
        "user": "sayakpaul",
        "created_at": "2024-10-10T11:16:53Z",
        "body": "Works for me. \r\n\r\nIf we are serializing the text embeddings beforehand, I think this argument shouldn't affect the training script, no?"
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-10T11:38:34Z",
        "body": "This is only used during validation, so prepared embeddings should already have the same max sequence length. Basically, when someone is preparing the precomputed embeddings, they'd have to make sure what the model is training with, and what was specified as an argument to the prepare_dataset.py script are the same (and even if not, training will remaining unaffected)."
      }
    ]
  },
  {
    "number": 10,
    "title": "DeepSpeed and DDP Configs",
    "created_at": "2024-10-08T11:51:11Z",
    "closed_at": "2024-10-08T21:24:19Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/pull/10",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/10/comments",
    "author": "a-r-r-o-w",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-08T13:08:24Z",
        "body": "DeepSpeed errors out with: (cc @sayakpaul)\r\n\r\n<details>\r\n\r\n```\r\n[rank1]: Traceback (most recent call last):                                                                                                                                                                                                                             \r\n[rank1]:   File \"/raid/aryan/cogvideox-distillation/training/cogvideox_text_to_video_lora.py\", line 911, in <module>                                                                                                                                                    \r\n[rank1]:     main(args)                                                                                                                                                                                                                                                 \r\n[rank1]:   File \"/raid/aryan/cogvideox-distillation/training/cogvideox_text_to_video_lora.py\", line 684, in main                                                                                                                                                        \r\n[rank1]:     model_output = transformer(                                                                                                                                                                                                                                \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl                                                                                                                                    \r\n[rank1]:     return self._call_impl(*args, **kwargs)                                                                                                                                                                                                                    \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl                                                                                                                                            \r\n[rank1]:     return forward_call(*args, **kwargs)                                                                                                                                                                                                                       \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 18, in wrapped_fn                                                                                                                                                 \r\n[rank1]:     ret_val = func(*args, **kwargs)                                                                                                                                                                                                                            \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1899, in forward                                                                                                                                              \r\n[rank1]:     loss = self.module(*inputs, **kwargs)                                                                                                                                                                                                                      \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl                                                                                                                                    \r\n[rank1]:     return self._call_impl(*args, **kwargs)                                                                                                                                                                                                                    \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl                                                                                                                                            \r\n[rank1]:     return forward_call(*args, **kwargs)                                                                                                                                                                                                                       \r\n[rank1]:   File \"/home/aryan/work/diffusers/src/diffusers/models/transformers/cogvideox_transformer_3d.py\", line 443, in forward                                                                                                                                        \r\n[rank1]:     emb = self.time_embedding(t_emb, timestep_cond)                                                                                                                                                                                                            \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl                                                                                                                                    \r\n[rank1]:     return self._call_impl(*args, **kwargs)                                                                                                                                                                                                                    \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl                                                                                                                                            \r\n[rank1]:     return forward_call(*args, **kwargs)                                                                                                                                                                                                                       \r\n[rank1]:   File \"/home/aryan/work/diffusers/src/diffusers/models/embeddings.py\", line 805, in forward                                                                                                                                                                   \r\n[rank1]:     sample = self.linear_1(sample)                                                                                                                                                                                                                             \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl                                                                                                                                    \r\n[rank1]:     return self._call_impl(*args, **kwargs)                                                                                                                                                                                                                    \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl                                                                                                                                            \r\n[rank1]:     return forward_call(*args, **kwargs)                                                                                                                                                                                                                       \r\n[rank1]:   File \"/raid/aryan/nightly-venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 125, in forward                                                                                                                                                \r\n[rank1]:     return F.linear(input, self.weight, self.bias)                                                                                                                                                                                                             \r\n[rank1]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and BFloat16\r\n```\r\n\r\n</details>\r\n\r\nDDP + uncompiled: works\r\n\r\nDDP + compiled: does not work. I don't think this setting has ever worked for me, or that they are compatible with each other (seems like so from some quick googling)"
      },
      {
        "user": "sayakpaul",
        "created_at": "2024-10-08T14:05:19Z",
        "body": "> DeepSpeed errors out with:\r\n\r\nDetails unfold to something blank. \r\n\r\n"
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-08T14:09:21Z",
        "body": "Oh sorry, really weird! Updated"
      },
      {
        "user": "sayakpaul",
        "created_at": "2024-10-08T14:12:47Z",
        "body": "Thanks. How can I reproduce the error? "
      },
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-08T14:47:22Z",
        "body": "> Thanks. How can I reproduce the error?\r\n\r\nI think you just have to change the config in the train_text_to_video_lora.sh file to use the DeepSpeed one.\r\n\r\nThis is what I'm using for example (from the root folder of the repo):\r\n\r\n```\r\nexport TORCH_LOGS=\"+dynamo,recompiles,graph_breaks\"\r\nexport TORCHDYNAMO_VERBOSE=1\r\nexport WANDB_MODE=\"offline\"\r\nexport NCCL_P2P_DISABLE=1\r\nexport TORCH_NCCL_ENABLE_MONITORING=0\r\n\r\nGPU_IDS=\"2,3\"\r\n\r\nDATA_ROOT=\"training/dump\"\r\n\r\nCAPTION_COLUMN=\"prompts.txt\"\r\nVIDEO_COLUMN=\"videos.txt\"\r\n\r\ncmd=\"accelerate launch --config_file accelerate_configs/deepspeed.yaml --gpu_ids $GPU_IDS training/cogvideox_text_to_video_lora.py \\\r\n  --pretrained_model_name_or_path THUDM/CogVideoX-5b \\\r\n  --data_root $DATA_ROOT \\\r\n  --caption_column $CAPTION_COLUMN \\\r\n  --video_column $VIDEO_COLUMN \\\r\n  --id_token BW_STYLE \\\r\n  --height_buckets 480 \\\r\n  --width_buckets 720 \\\r\n  --frame_buckets 49 \\\r\n  --load_tensors \\\r\n  --validation_prompt \\\"BW_STYLE A black and white animated scene unfolds with an anthropomorphic goat surrounded by musical notes and symbols, suggesting a playful environment. Mickey Mouse appears, leaning forward in curiosity as the goat remains still. The goat then engages with Mickey, who bends down to converse or react. The dynamics shift as Mickey grabs the goat, potentially in surprise or playfulness, amidst a minimalistic background. The scene captures the evolving relationship between the two characters in a whimsical, animated setting, emphasizing their interactions and emotions\\\" \\\r\n  --validation_prompt_separator ::: \\\r\n  --num_validation_videos 1 \\\r\n  --validation_epochs 1 \\\r\n  --seed 42 \\\r\n  --rank 64 \\\r\n  --lora_alpha 64 \\\r\n  --mixed_precision bf16 \\\r\n  --output_dir /raid/aryan/cogvideox-lora \\\r\n  --max_num_frames 49 \\\r\n  --train_batch_size 1 \\\r\n  --max_train_steps 3000 \\\r\n  --checkpointing_steps 1000 \\\r\n  --gradient_accumulation_steps 1 \\\r\n  --gradient_checkpointing \\\r\n  --learning_rate 0.0001 \\\r\n  --lr_scheduler constant \\\r\n  --lr_warmup_steps 200 \\\r\n  --lr_num_cycles 1 \\\r\n  --enable_slicing \\\r\n  --enable_tiling \\\r\n  --optimizer adamw \\\r\n  --beta1 0.9 \\\r\n  --beta2 0.95 \\\r\n  --beta3 0.99 \\\r\n  --weight_decay 0.001 \\\r\n  --max_grad_norm 1.0 \\\r\n  --allow_tf32 \\\r\n  --report_to wandb \\\r\n  --nccl_timeout 1800\"\r\n\r\necho \"Running command: $cmd\"\r\neval $cmd\r\necho -ne \"-------------------- Finished executing script --------------------\\n\\n\"\r\n```\r\n\r\n"
      },
      {
        "user": "sayakpaul",
        "created_at": "2024-10-08T16:21:04Z",
        "body": "@a-r-r-o-w DeepSpeed seems to be working.\r\n\r\n<details>\r\n<summary>Patch:</summary>\r\n\r\n```diff\r\ndiff --git a/training/cogvideox_text_to_video_lora.py b/training/cogvideox_text_to_video_lora.py\r\nindex fa6b6e0..c2a29d6 100644\r\n--- a/training/cogvideox_text_to_video_lora.py\r\n+++ b/training/cogvideox_text_to_video_lora.py\r\n@@ -315,7 +315,7 @@ def main(args):\r\n             \"bf16\" in accelerator.state.deepspeed_plugin.deepspeed_config\r\n             and accelerator.state.deepspeed_plugin.deepspeed_config[\"bf16\"][\"enabled\"]\r\n         ):\r\n-            weight_dtype = torch.float16\r\n+            weight_dtype = torch.bfloat16\r\n     else:\r\n         if accelerator.mixed_precision == \"fp16\":\r\n             weight_dtype = torch.float16\r\n@@ -631,7 +631,7 @@ def main(args):\r\n \r\n                 videos = latent_dist.sample() * VAE_SCALING_FACTOR\r\n                 videos = videos.permute(0, 2, 1, 3, 4)  # [B, F, C, H, W]\r\n-                videos = videos.to(memory_format=torch.contiguous_format).float()\r\n+                videos = videos.to(memory_format=torch.contiguous_format).to(weight_dtype)\r\n                 model_input = videos\r\n \r\n                 # Encode prompts\r\n@@ -646,7 +646,7 @@ def main(args):\r\n                         requires_grad=False,\r\n                     )\r\n                 else:\r\n-                    prompt_embeds = prompts\r\n+                    prompt_embeds = prompts.to(weight_dtype)\r\n \r\n                 # Sample noise that will be added to the latents\r\n                 noise = torch.randn_like(model_input)\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>accelerate config</summary>\r\n\r\n```yaml\r\ncompute_environment: LOCAL_MACHINE\r\ndebug: false\r\ndeepspeed_config:\r\n  gradient_accumulation_steps: 1\r\n  gradient_clipping: 1.0\r\n  offload_optimizer_device: cpu\r\n  offload_param_device: cpu\r\n  zero3_init_flag: false\r\n  zero_stage: 2\r\ndistributed_type: DEEPSPEED\r\ndowncast_bf16: 'no'\r\nenable_cpu_affinity: false\r\nmachine_rank: 0\r\nmain_training_function: main\r\nmixed_precision: bf16\r\nnum_machines: 1\r\nnum_processes: 1\r\nrdzv_backend: static\r\nsame_network: true\r\ntpu_env: []\r\ntpu_use_cluster: false\r\ntpu_use_sudo: false\r\nuse_cpu: false\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n</summary>Training command</summary>\r\n\r\n```bash\r\n# export TORCH_LOGS=\"+dynamo,recompiles,graph_breaks\"\r\n# export TORCHDYNAMO_VERBOSE=1\r\nexport WANDB_MODE=\"offline\"\r\nexport NCCL_P2P_DISABLE=1\r\nexport TORCH_NCCL_ENABLE_MONITORING=0\r\n\r\nGPU_IDS=\"2\"\r\n\r\nDATA_ROOT=\"video-dataset-disney\"\r\n\r\nCAPTION_COLUMN=\"prompt.txt\"\r\nVIDEO_COLUMN=\"videos.txt\"\r\n\r\ncmd=\"accelerate launch --config_file accelerate_configs/deepspeed.yaml --gpu_ids $GPU_IDS training/cogvideox_text_to_video_lora.py \\\r\n  --pretrained_model_name_or_path THUDM/CogVideoX-5b \\\r\n  --data_root $DATA_ROOT \\\r\n  --caption_column $CAPTION_COLUMN \\\r\n  --video_column $VIDEO_COLUMN \\\r\n  --id_token BW_STYLE \\\r\n  --height_buckets 480 \\\r\n  --width_buckets 720 \\\r\n  --frame_buckets 49 \\\r\n  --load_tensors \\\r\n  --validation_prompt \\\"BW_STYLE A black and white animated scene unfolds with an anthropomorphic goat surrounded by musical notes and symbols, suggesting a playful environment. Mickey Mouse appears, leaning forward in curiosity as the goat remains still. The goat then engages with Mickey, who bends down to converse or react. The dynamics shift as Mickey grabs the goat, potentially in surprise or playfulness, amidst a minimalistic background. The scene captures the evolving relationship between the two characters in a whimsical, animated setting, emphasizing their interactions and emotions\\\" \\\r\n  --validation_prompt_separator ::: \\\r\n  --num_validation_videos 1 \\\r\n  --validation_epochs 1 \\\r\n  --seed 42 \\\r\n  --rank 64 \\\r\n  --lora_alpha 64 \\\r\n  --mixed_precision bf16 \\\r\n  --output_dir lora \\\r\n  --max_num_frames 49 \\\r\n  --train_batch_size 1 \\\r\n  --max_train_steps 3000 \\\r\n  --checkpointing_steps 1000 \\\r\n  --gradient_accumulation_steps 1 \\\r\n  --gradient_checkpointing \\\r\n  --learning_rate 0.0001 \\\r\n  --lr_scheduler constant \\\r\n  --lr_warmup_steps 200 \\\r\n  --lr_num_cycles 1 \\\r\n  --enable_slicing \\\r\n  --enable_tiling \\\r\n  --optimizer adamw \\\r\n  --beta1 0.9 \\\r\n  --beta2 0.95 \\\r\n  --beta3 0.99 \\\r\n  --weight_decay 0.001 \\\r\n  --max_grad_norm 1.0 \\\r\n  --allow_tf32 \\\r\n  --report_to wandb \\\r\n  --nccl_timeout 1800\"\r\n\r\necho \"Running command: $cmd\"\r\neval $cmd\r\necho -ne \"-------------------- Finished executing script --------------------\\n\\n\"\r\n```\r\n\r\n</details>\r\n\r\n"
      }
    ]
  },
  {
    "number": 2,
    "title": "Make DeepSpeed compatible",
    "created_at": "2024-09-27T13:06:31Z",
    "closed_at": "2024-10-10T13:06:42Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/2",
    "body": null,
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/2/comments",
    "author": "sayakpaul",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-10-10T13:06:42Z",
        "body": "I think we should be compatible now, thanks for all the help Sayak! :heart:"
      }
    ]
  }
]