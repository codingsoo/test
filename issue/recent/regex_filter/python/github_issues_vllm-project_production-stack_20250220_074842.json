[
  {
    "number": 139,
    "title": "[Doc] fixed chart-name related misalignments in tutorial docs",
    "created_at": "2025-02-17T10:49:07Z",
    "closed_at": "2025-02-17T15:16:08Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/139",
    "body": "Aligned tutorial docs with chart:\r\n- Changed instances of `production-stack` to `vllm-stack` (chart-name)\r\n- Changed instances of `llmstack` to `vllm` (release-name)",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/139/comments",
    "author": "vMaroon",
    "comments": [
      {
        "user": "ApostaC",
        "created_at": "2025-02-17T15:16:30Z",
        "body": "Thanks for your contribution üéâ!"
      }
    ]
  },
  {
    "number": 138,
    "title": "[Doc] minor tutorial doc fix",
    "created_at": "2025-02-17T09:00:23Z",
    "closed_at": "2025-02-17T09:39:20Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/138",
    "body": "Fixed chart name `vllm/production-stack` -> `vllm/vllm-stack` in tutorial 05.\r\n",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/138/comments",
    "author": "vMaroon",
    "comments": [
      {
        "user": "Shaoting-Feng",
        "created_at": "2025-02-17T09:12:36Z",
        "body": "Thanks for contributing to the production stack!\r\nCan you sign off the commit by doing git commit --amend --sign-off in your local branch and update the remote repo again?"
      },
      {
        "user": "vMaroon",
        "created_at": "2025-02-17T09:13:39Z",
        "body": "@Shaoting-Feng done, my bad."
      },
      {
        "user": "vMaroon",
        "created_at": "2025-02-17T09:34:20Z",
        "body": "@gaocegege FYI this issue is shared in other tutorials, it may make sense to rename the chart to `production-stack` instead but I guess that is a maintainer-level decision."
      },
      {
        "user": "Shaoting-Feng",
        "created_at": "2025-02-17T09:39:09Z",
        "body": "@vMaroon Thanks for pointing it out. We are going through the tutorials to fix the mismatched names."
      }
    ]
  },
  {
    "number": 136,
    "title": "[Discussion] Unspecified versions in requirements.txt",
    "created_at": "2025-02-17T05:36:30Z",
    "closed_at": "2025-02-18T04:43:24Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/136",
    "body": "As of now the requirements.txt for the router has no specified versions, which could cause potential issues like inconsistent environments and dependency conflicts. I would propose either at least specifying the versions of packages, or using a more production suitable env tool such as `poetry` (vllm should be using this instead of requirements.txt as well). \nHowever if the python version of the router is going to be deprecated quickly then this might not be a pressing issue.\nOn a side note, using the official fastapi images for build in the dockerfile will save a lot of headache.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/136/comments",
    "author": "BrianPark314",
    "comments": [
      {
        "user": "YuhanLiu11",
        "created_at": "2025-02-17T19:33:11Z",
        "body": "Good point. We could specify the versions of packages for now. We can open a PR to quickly fix this. \n\ncc @Shaoting-Feng  "
      }
    ]
  },
  {
    "number": 127,
    "title": "Fix the problem of 1k prompts.",
    "created_at": "2025-02-14T14:51:26Z",
    "closed_at": "2025-02-18T03:40:42Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/127",
    "body": "httpx client has a default max_connections(100) which will block the requests if the concurrency is over 100.\r\n\r\nThis commit enhances the router's concurrency capabilities. Key changes:\r\n1. Set `httpx.AsyncClient` max connections to unlimited to unblock the requests.\r\n2. In `process_request`, overwrite host header for proxies routing by host.\r\n3. Call `set_ulimit` in `main` to prevent uvicorn from dropping requests due to high FD count.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/127/comments",
    "author": "ggaaooppeenngg",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-02-15T05:05:47Z",
        "body": "\r\nThanks for your contribution! :tada: :+1:\r\n\r\nPlease fix the pre commit linting issues.\r\n\r\n"
      },
      {
        "user": "ggaaooppeenngg",
        "created_at": "2025-02-15T05:58:10Z",
        "body": "@gaocegege Done!"
      }
    ]
  },
  {
    "number": 103,
    "title": "fix: set Deployment strategy as Recreate instead of RollingUpdates",
    "created_at": "2025-02-10T05:53:34Z",
    "closed_at": "2025-02-10T06:32:43Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/103",
    "body": "RollingUpdates can't be used if replica count is set to 1. It would wait indefinitely to re-deploy the pods. Need to use Recreate or make it configurable in the chart. \r\n",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/103/comments",
    "author": "amasolov",
    "comments": [
      {
        "user": "BrianPark314",
        "created_at": "2025-02-10T06:30:42Z",
        "body": "> RollingUpdates can't be used if replica count is set to 1. It would wait indefinitely to re-deploy the pods. Need to use Recreate or make it configurable in the chart.\r\n\r\nRolling update should work even if the replica count is set to 1. The indefinite wait is likely due to lack of additional gpu nodes."
      },
      {
        "user": "amasolov",
        "created_at": "2025-02-10T06:32:43Z",
        "body": "@BrianPark314 there are 2 more available nodes but I see where you are coming from. I'll close this one and investigate further. "
      }
    ]
  },
  {
    "number": 76,
    "title": "[Add] fix for router files api and example to post query to the api",
    "created_at": "2025-02-07T03:24:08Z",
    "closed_at": "2025-02-07T04:33:49Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/76",
    "body": "This PR fixes a problem that the router will break when the \"purpose\" does not exist in the query to `/files` endpoint.\r\n\r\nIt also contains a minimal example to query the `/files` endpoint.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/76/comments",
    "author": "ApostaC",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T04:34:08Z",
        "body": "Thanks for the fix!"
      }
    ]
  },
  {
    "number": 74,
    "title": "[CI/CD] Automatically build the router docker image via github action",
    "created_at": "2025-02-07T02:50:56Z",
    "closed_at": "2025-02-09T23:56:23Z",
    "labels": [
      "feature request",
      "good first issue",
      "help wanted"
    ],
    "url": "https://github.com/vllm-project/production-stack/issues/74",
    "body": "Right now, we don't have an automatic pipeline to build the docker image for the router.\n\nIdeally, every time there is merged a PR about the router, there should be a GitHub action to automatically build the docker image and release it.\n\nAlso, when PR is created and ready to merge (not merged yet), there should be a way to trigger the end-to-end test pipeline to test the router. ",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/74/comments",
    "author": "ApostaC",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T05:45:09Z",
        "body": "> Also, when PR is created and ready to merge (not merged yet), there should be a way to trigger the end-to-end test pipeline to test the router.\n\nDo we have the end-to-end test for router now?"
      },
      {
        "user": "ApostaC",
        "created_at": "2025-02-07T17:40:39Z",
        "body": "> Do we have the end-to-end test for router now?\n\nWe have a functionality test for the whole helm chart, but not specifically for router."
      },
      {
        "user": "Sozhan308",
        "created_at": "2025-02-09T10:02:04Z",
        "body": "Hi @ApostaC,\n\nI‚Äôd really like to help out with this task. Let me know if I can take it on!"
      }
    ]
  },
  {
    "number": 52,
    "title": "feat: OpenAI batch API part 1",
    "created_at": "2025-02-01T00:26:36Z",
    "closed_at": "2025-02-07T02:54:06Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/52",
    "body": "This is the part 1 of #47 \r\n\r\nfile.Storage has been introduced to abstract file operations, to support both OpenAI files and the uploads API. The user-id in the interface is defaulted to uid-default, which is kept for future extensibility.\r\n\r\nI believe OpenAI uses a database, such as PostgreSQL, to store file metadata. For now, we are using simple file operations to handle this.\r\n\r\nI‚Äôve raised this PR to discuss whether this approach aligns with our requirements. Then will implement the batch API in part 2.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/52/comments",
    "author": "gaocegege",
    "comments": [
      {
        "user": "ApostaC",
        "created_at": "2025-02-03T15:04:52Z",
        "body": "Thanks @gaocegege . I will take a look at this PR soon!"
      },
      {
        "user": "ApostaC",
        "created_at": "2025-02-06T17:50:25Z",
        "body": "@gaocegege Hey, do you want this PR to be merged? Or you want to edit it a bit more?"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T02:04:24Z",
        "body": "I will address the comments first"
      },
      {
        "user": "ApostaC",
        "created_at": "2025-02-07T02:05:42Z",
        "body": "Thanks! Take your time please üëç "
      },
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T02:26:09Z",
        "body": "Please take a look, thanks for your time :smile:"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T02:48:46Z",
        "body": "Comments are addressed, please take a look.\r\nThanks for your time :smile:"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T02:54:55Z",
        "body": "Thanks. I will work on the part 2: the batch API."
      }
    ]
  },
  {
    "number": 51,
    "title": "Polish the 'Setting Up a Kubernetes Environment with GPUs' tutorial",
    "created_at": "2025-01-31T20:04:54Z",
    "closed_at": "2025-01-31T23:21:54Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/51",
    "body": "This PR includes some troubleshooting tips to fight against the common seen \"too many open files\" issue, when installing workloads in minikube.\r\n\r\nHopefully it can (1) make the tutorial more robust, (2) save people some time during the setup.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/51/comments",
    "author": "waltforme",
    "comments": [
      {
        "user": "ApostaC",
        "created_at": "2025-01-31T22:01:38Z",
        "body": "Thanks for contributing to the production stack project. Can you run pre-commit to make sure it passes the format checker?\r\n```\r\npre-commit run --all-files\r\n```\r\n(Details about `pre-commit` can be found in the \"Contributing\" section of the main README)"
      },
      {
        "user": "waltforme",
        "created_at": "2025-01-31T23:22:04Z",
        "body": "Thanks @ApostaC for the pointer! I should have checked out the README more carefully.\r\n\r\nBTW nice tooling!"
      }
    ]
  },
  {
    "number": 35,
    "title": "Add `pre-commit` based linting and formatting",
    "created_at": "2025-01-29T12:41:34Z",
    "closed_at": "2025-01-29T17:44:42Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/35",
    "body": "The hooks added are as follows:\r\n- `actionlint` - GitHub Actions\r\n- `check-{json/toml/yaml}` - files with these extensions\r\n- `end-of-file-fixer` and `trailing-whitespace`- all files\r\n- `requirements-txt-fixer` - Python requirements files\r\n- `hadolint` - Dockerfiles (currently disabled)\r\n- `helmlint` - Helm charts (currently disabled)\r\n- `black` and `isort` - Python\r\n- `shellcheck` - shell (currently disabled)\r\n- `markdownlint` - markdown\r\n- `codespell` - spelling",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/35/comments",
    "author": "hmellor",
    "comments": [
      {
        "user": "hmellor",
        "created_at": "2025-01-29T12:59:16Z",
        "body": "> Is it possible to install pre-commit locally, like vLLM? We could document it if it is possible.\r\n\r\nYes, of course. I added `requirements-lint.txt` to make this a bit clearer to users. I'll add it to the contributing section of the README too"
      },
      {
        "user": "hmellor",
        "created_at": "2025-01-29T13:00:13Z",
        "body": "@ApostaC since the repo has been unformatted until now, there are lots of changes that the hooks want to make. Would you prefer if I do this in this PR, or in a follow up? (meaning that this PR would merge with pre-commit failing)"
      },
      {
        "user": "KuntaiDu",
        "created_at": "2025-01-29T14:25:49Z",
        "body": "I would prefer fixing the pre-commit in this PR to make sure the CI is always green, @ApostaC what's your thought?"
      }
    ]
  },
  {
    "number": 29,
    "title": "Documentation fixes",
    "created_at": "2025-01-28T18:54:27Z",
    "closed_at": "2025-01-29T17:29:50Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/29",
    "body": "Updated tutorial to latest file name for values minimal example (`values-minimal-example.yaml `-> `values-01-minimal-example.yaml`)",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/29/comments",
    "author": "dmatch01",
    "comments": [
      {
        "user": "KuntaiDu",
        "created_at": "2025-01-29T14:19:29Z",
        "body": "The change LGTM but could you please fix the DCO issue so that we can merge it? @dmatch01 "
      },
      {
        "user": "dmatch01",
        "created_at": "2025-01-29T17:27:17Z",
        "body": "Thanks @KuntaiDu for the quick review.  DCO should now be resolved."
      }
    ]
  },
  {
    "number": 28,
    "title": "feat: Add Ingress configuration",
    "created_at": "2025-01-28T18:08:59Z",
    "closed_at": "2025-02-04T18:06:17Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/28",
    "body": "I will need to expose models to external users through an Ingress. I plan to open a PR for this, but wanted to document this in an issue for tracking.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/28/comments",
    "author": "0xThresh",
    "comments": [
      {
        "user": "KuntaiDu",
        "created_at": "2025-01-29T17:37:55Z",
        "body": "Thanks for contributing üéâ "
      }
    ]
  },
  {
    "number": 21,
    "title": "fix(router): Support engine stats and request internal",
    "created_at": "2025-01-25T04:45:52Z",
    "closed_at": "2025-01-27T00:21:00Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/pull/21",
    "body": null,
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/21/comments",
    "author": "gaocegege",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-01-25T04:46:18Z",
        "body": "/assign @ApostaC "
      },
      {
        "user": "ApostaC",
        "created_at": "2025-01-26T20:55:00Z",
        "body": "@gaocegege Feel free to merge after fixing the conflict. Thanks!"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-01-27T00:13:26Z",
        "body": "@ApostaC I rebased the upstream main, PTAL."
      }
    ]
  },
  {
    "number": 17,
    "title": "chore: Make router a python package",
    "created_at": "2025-01-24T04:22:33Z",
    "closed_at": "2025-01-26T20:54:06Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/17",
    "body": "Making the router a Python package offers several benefits:\n\n- The package can be installed with pip, simplifying dependency management and deployment.\n- Tests can be run more easily, and the package can be integrated into CI/CD pipelines for automated testing.\n- The package can be versioned, allowing for controlled updates and backward compatibility.\n\nBesides, we should include some basic end-to-end (e2e) tests as part of the continuous integration (CI) pipeline.",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/17/comments",
    "author": "gaocegege",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-01-24T04:35:03Z",
        "body": "Hey, I saw the roadmap mentions a \"more performant router using non-Python languages.\" If we plan to tackle that soon, maybe we could explore rewriting the router in another language for a performance boost and ignore this. Just a thought."
      },
      {
        "user": "ApostaC",
        "created_at": "2025-01-24T05:23:13Z",
        "body": "Hey @gaocegege , making it a python package sounds great to me!\n\nWe are right now working on end-to-end tests and CI pipeline. And it should be available soon. Thanks for pointing that out!"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-01-24T08:35:48Z",
        "body": "Thanks for the prompt response.\n\nShould we turn it into a Python package now, or wait until the CI setup is ready? It may impact the CI implementation."
      },
      {
        "user": "ApostaC",
        "created_at": "2025-01-24T16:33:00Z",
        "body": "It would be great if we could turn it into a python package now. (otherwise we need to adjust the CI again IIUC?)"
      }
    ]
  },
  {
    "number": 16,
    "title": "Does routing logic works depends on the just QPS of the endpoint?",
    "created_at": "2025-01-24T01:54:51Z",
    "closed_at": "2025-02-07T03:17:31Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/16",
    "body": null,
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/16/comments",
    "author": "pandyamarut",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-01-24T04:31:53Z",
        "body": "Based on the code, when the round-robin policy is enabled, the QPS information won't be used. However, if the session-based policy is enabled, requests without a session ID will be routed to the instance with the lowest QPS."
      },
      {
        "user": "ApostaC",
        "created_at": "2025-01-24T05:28:15Z",
        "body": "Yeah, currently the session-id policy is based on the QPS. We will incorporate more routing logic in the future. \n\nIf you want to have your customized routing logic, feel free to extend the `RoutingInterface` class defined in `routing_logic.py`. The main routing function takes in the available endpoints, engine statistics, query-level statistics, and the request itself as the input, and returns the target URL as the output. So should be highly customizable"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-02-07T03:17:31Z",
        "body": "I am closing the issue. But feel free to leave comments if there is any problem."
      }
    ]
  },
  {
    "number": 15,
    "title": "With the session-based routing, what happens when a pod goes away?",
    "created_at": "2025-01-24T00:43:50Z",
    "closed_at": "2025-01-24T02:41:13Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/15",
    "body": "Sorry if I missed something but from a quick look through the code, it looks like all subsequent requests belonging to sessions assigned to that pod will fail?",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/15/comments",
    "author": "njhill",
    "comments": [
      {
        "user": "gaocegege",
        "created_at": "2025-01-24T01:56:09Z",
        "body": "It is related to #8 "
      },
      {
        "user": "njhill",
        "created_at": "2025-01-24T02:41:13Z",
        "body": "Ah thanks @gaocegege, I should have read the other issues first!"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-01-24T02:47:04Z",
        "body": "Hi, I‚Äôm working on this and will share a PR soon. I‚Äôll reach out when it‚Äôs ready."
      }
    ]
  },
  {
    "number": 14,
    "title": "The tutorials all suggest storing your tokens as plaintext",
    "created_at": "2025-01-23T22:00:43Z",
    "closed_at": "2025-01-27T00:26:21Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/14",
    "body": "Just a heads up that all of your tutorials suggest setting your HF token in plaintext as an environment variable in the deployments like\n```\n    env:\n      - name: HF_TOKEN\n        value: <YOUR_HF_TOKEN>\n```\n\nThis _might_ be fine to do on a local minikube cluster on your machine, but it's super unsafe to do on a remote cluster.\n\nI'd suggest setting up a secret in the helm chart to hold the token, so that it can be safely referenced in the deployment instead like:\n```\n    env\n       - name: HF_TOKEN\n         valueFrom:\n           secretKeyRef:\n             name: hf-token-secret\n             key: token\n```\n",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/14/comments",
    "author": "joerunde",
    "comments": [
      {
        "user": "ApostaC",
        "created_at": "2025-01-24T05:29:02Z",
        "body": "Thanks for pointing it out! This is on our current TODO list and will be updated soon."
      },
      {
        "user": "ApostaC",
        "created_at": "2025-01-26T22:23:40Z",
        "body": "Hey @joerunde , I just created PR #22 to fix this. Feel free to take a look and leave your comments there."
      }
    ]
  },
  {
    "number": 8,
    "title": "feat: Support Dynamic Addition/Removal of Instances in Session-Based Routing",
    "created_at": "2025-01-23T07:41:04Z",
    "closed_at": "2025-01-24T05:32:37Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/8",
    "body": "The current setup uses a basic hashing approach for routing, which doesn‚Äôt handle node changes very well. \n\n\n```py\n            hash_digest = hashlib.sha256(session_id.encode()).hexdigest()\n            index = int(hash_digest, 16) % len(endpoints)\n            url = endpoints[index].url\n            self.key_to_server_id[session_id] = URL\n```\n\nConsistent hashing is a better fit because it minimizes data shuffling when instances are added or removed, keeping things running smoothly. Maybe we could give it a try?\n\n\n",
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/8/comments",
    "author": "gaocegege",
    "comments": [
      {
        "user": "ApostaC",
        "created_at": "2025-01-23T15:39:56Z",
        "body": "Sounds great! This makes a lot of sense. We are more than happy to see the contributions!"
      },
      {
        "user": "gaocegege",
        "created_at": "2025-01-24T01:54:40Z",
        "body": "I am glad to help with it. #dibs\n"
      }
    ]
  },
  {
    "number": 5,
    "title": "Architecture Stack Image is Broken",
    "created_at": "2025-01-22T22:17:39Z",
    "closed_at": "2025-01-23T15:58:30Z",
    "labels": [],
    "url": "https://github.com/vllm-project/production-stack/issues/5",
    "body": null,
    "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/5/comments",
    "author": "robertgshaw2-redhat",
    "comments": [
      {
        "user": "joerunde",
        "created_at": "2025-01-22T23:34:12Z",
        "body": "The grafana image is broken as well"
      },
      {
        "user": "ApostaC",
        "created_at": "2025-01-23T15:33:25Z",
        "body": "Thanks for pointing out the problems! Just fixed the broken links. "
      }
    ]
  }
]