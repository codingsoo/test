[
  {
    "number": 13,
    "title": "运行时报错栈溢出",
    "created_at": "2024-12-15T11:24:31Z",
    "closed_at": "2024-12-16T09:58:51Z",
    "labels": [],
    "url": "https://github.com/zhihu/ZhiLight/issues/13",
    "body": "zhilight) root@7e3f0f7cd7c4f4a9:~/ZhiLight# python -m zhilight.server.openai.entrypoints.api_server --model-path /workspace/heavenly-goat-v8-model\r\nINFO 12-15 11:06:57 api_server.py:152] ZhiLight OpenAI-Compatible Server version 0.4.8.\r\nINFO 12-15 11:06:57 api_server.py:160] args: Namespace(host='0.0.0.0', port=8080, api_key='*', served_model_name=None, response_role='assistant', uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], zhilight_version=None, environ=[], pip=[], model_path='/workspace/heavenly-goat-v8-model', max_model_len=8192, disable_flash_attention=False, enable_cpm_chat=False, disable_tensor_parallel=False, enable_prefix_caching=False, disable_log_stats=False, quantization=None, dyn_max_batch_size=8, dyn_max_beam_size=4, ignore_eos=False, disable_log_requests=False, max_log_len=None)\r\nINFO 12-15 11:06:57 llm_engine.py:20] engine config => EngineConfig(model_path='/workspace/heavenly-goat-v8-model', model_file='/workspace/heavenly-goat-v8-model/model-00001-of-00006.safetensors', vocab_file='/workspace/heavenly-goat-v8-model/vocabs.txt', is_cpm_directory_struct=False, use_safetensors=True, model_config={'_name_or_path': 'DeverDever/heavenly-goat-v8', 'architectures': ['MistralModel'], 'attention_dropout': 0.0, 'bos_token_id': 1, 'eos_token_id': 32000, 'head_dim': 128, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 14336, 'max_position_embeddings': 32768, 'model_type': 'mistral', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 8, 'rms_norm_eps': 1e-05, 'rope_theta': 10000.0, 'sliding_window': 4096, 'tie_word_embeddings': False, 'torch_dtype': 'float32', 'transformers_version': '4.47.0', 'use_cache': False, 'vocab_size': 32002, 'num_layers': 32, 'dim_model': 4096, 'num_heads': 32, 'num_kv_heads': 8, 'max_token': 32768, 'dim_ff': 14336, 'eps': 1e-05, 'activate_fn': 'silu', 'bfloat16': False, 'new_vocab': False}, dyn_batch_config=DynamicBatchConfig(max_batch=8, max_beam_size=4, task_queue_size=8, max_total_token=8192, seed=0, bos_id=0, eos_id=2, nccl=-1, rag_buffer=True), quant_config={'type': <QuantType.NoQuant: 0>}, memory_limit=0, enable_tensor_parallel=True, is_chatml=False, max_model_len=8192)\r\n[DEV]Config: HIGH_PRECISION=1; DUAL_STREAM=None; CPM_FUSE_QKV=1; CPM_FUSE_FF_IN=1; REDUCE_TP_INT8_THRES=None; W4_INT8_ALGO=None; W4_FP8_ALGO=None\r\ndist_config: parallel=True\r\n********* world_size=1, nccl_version=21505 *********\r\n*** stack smashing detected ***: terminated\r\nAborted (core dumped)",
    "comments_url": "https://api.github.com/repos/zhihu/ZhiLight/issues/13/comments",
    "author": "machuanhu",
    "comments": [
      {
        "user": "spetrel",
        "created_at": "2024-12-16T03:02:50Z",
        "body": "what GPU do you use ?"
      },
      {
        "user": "machuanhu",
        "created_at": "2024-12-16T03:13:05Z",
        "body": "> what GPU do you use ?\r\n\r\n4090"
      },
      {
        "user": "spetrel",
        "created_at": "2024-12-16T09:49:44Z",
        "body": "'torch_dtype': 'float32' is not supported. Only float16, bfloat16 or GPTQ/AWQ models."
      }
    ]
  },
  {
    "number": 8,
    "title": "[QUESTION] About all-reduce overlap",
    "created_at": "2024-12-12T03:44:00Z",
    "closed_at": "2024-12-12T06:17:19Z",
    "labels": [],
    "url": "https://github.com/zhihu/ZhiLight/issues/8",
    "body": "Thx for ur wonderful work! I am very interested in your method of `Encode and all-reduce overlap, we named \"dual streams\"`. Could you provide a general explanation of this technical approach?",
    "comments_url": "https://api.github.com/repos/zhihu/ZhiLight/issues/8/comments",
    "author": "ZhongYingMatrix",
    "comments": [
      {
        "user": "unix1986",
        "created_at": "2024-12-12T04:20:45Z",
        "body": "@ZhongYingMatrix Thank you for your attention. Dual-Streams employs two CUDA streams, one designated for All-Reduce operations and the other for Prefill computations. The Prefill process is segmented into multiple chunks based on tokens. The approach involves computing one chunk initially, followed by an All-Reduce step, and then proceeding to compute the next chunk in parallel, iterating through this cycle until completion. This feature is contributed by my colleague @spetrel ."
      },
      {
        "user": "ZhongYingMatrix",
        "created_at": "2024-12-16T07:58:03Z",
        "body": "> The Prefill process is segmented into multiple chunks based on tokens. The approach involves computing one chunk initially, followed by an All-Reduce step, and then proceeding to compute the next chunk in parallel, iterating through this cycle until completion.\r\n\r\n@unix1986 @spetrel Hi, sorry to bother. I have a new question regarding the attention computation. Since the attention involves all of the tokens, how is this handled across different chunks?\r\n\r\n"
      },
      {
        "user": "spetrel",
        "created_at": "2024-12-16T09:56:35Z",
        "body": "We do \"chunks\" only for prompt encoding computation, the KV cache is \"continuous\" as one piece. "
      },
      {
        "user": "ZhongYingMatrix",
        "created_at": "2024-12-17T02:29:38Z",
        "body": "@spetrel Is it chunked before or after QKV projection? I suppose that any query chunk should refer to all of the key/value chunks."
      },
      {
        "user": "spetrel",
        "created_at": "2024-12-17T02:53:42Z",
        "body": "before, at the input of block layer. Yes, all query chunks refer to the whole KV cache. cuda events is used to synchronize between chunks attentions to make sure previous chunk attention get done before next chunk attention."
      }
    ]
  },
  {
    "number": 6,
    "title": "源码安装后，运行报错",
    "created_at": "2024-12-10T12:17:48Z",
    "closed_at": "2024-12-16T10:03:10Z",
    "labels": [],
    "url": "https://github.com/zhihu/ZhiLight/issues/6",
    "body": "(zhihu_infer) root@localhost:~# pip3 list | grep zhilight\r\nzhilight                          0.4.8\r\n\r\n执行服务部署时，报错：\r\npython -m zhilight.server.openai.entrypoints.api_server --model-path /workspace/sdb/models/QWen2.5/Qwen2.5-7B-Instruct-GPTQ-Int4\r\nTraceback (most recent call last):\r\n  File \"/miniconda/envs/zhihu_infer/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/miniconda/envs/zhihu_infer/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/miniconda/envs/zhihu_infer/lib/python3.10/site-packages/zhilight-0.4.8-py3.10-linux-x86_64.egg/zhilight/__init__.py\", line 2, in <module>\r\n    from .llama import LLaMA\r\n  File \"/miniconda/envs/zhihu_infer/lib/python3.10/site-packages/zhilight-0.4.8-py3.10-linux-x86_64.egg/zhilight/llama.py\", line 11, in <module>\r\n    from . import C\r\nImportError: /miniconda/envs/zhihu_infer/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /miniconda/envs/zhihu_infer/lib/python3.10/site-packages/zhilight-0.4.8-py3.10-linux-x86_64.egg/zhilight/C.cpython-310-x86_64-linux-gnu.so)\r\n(zhihu_infer) root@localhost:~#",
    "comments_url": "https://api.github.com/repos/zhihu/ZhiLight/issues/6/comments",
    "author": "ChuanhongLi",
    "comments": [
      {
        "user": "unix1986",
        "created_at": "2024-12-10T12:24:19Z",
        "body": "像是哪里gcc版本不对应，但是信息量太少，很难给予有效帮助，你用我们的Docker镜像试试呢"
      },
      {
        "user": "ZonePG",
        "created_at": "2024-12-11T18:46:40Z",
        "body": "conda install -c conda-forge libstdcxx-ng"
      },
      {
        "user": "spetrel",
        "created_at": "2024-12-12T07:10:47Z",
        "body": "Can you try uninstall libstdcxx-ng ?"
      }
    ]
  }
]