[
  {
    "number": 117,
    "title": "Getting Errors When following Readme Instruction on ARM",
    "created_at": "2024-11-15T07:56:44Z",
    "closed_at": "2024-11-21T07:54:00Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/117",
    "body": "In file included from /root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-quants.h:4,\n                 from /root/BitNet/src/ggml-bitnet-lut.cpp:9:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-common.h:154:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\n  154 |         struct {\n      |                ^\n/root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-common.h:175:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\n  175 |         struct {\n      |                ^\n/root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-common.h:196:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\n  196 |         struct {\n      |                ^\nIn file included from /root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-quants.h:4,\n                 from /root/BitNet/src/ggml-bitnet-lut.cpp:9:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-common.h:261:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\n  261 |         struct {\n      |                ^\n/root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-common.h:288:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\n  288 |         struct {\n      |                ^\n/root/BitNet/3rdparty/llama.cpp/ggml/src/./ggml-common.h:305:16: warning: ISO C++ prohibits anonymous structs [-Wpedantic]\n  305 |         struct {\n      |                ^\nIn file included from /root/BitNet/src/ggml-bitnet-lut.cpp:10:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:24:6: warning: no previous declaration for ‘void per_tensor_quant(int, void*, void*)’ [-Wmissing-declarations]\n   24 | void per_tensor_quant(int k, void* lut_scales_, void* b_) {{\n      |      ^~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:53:6: warning: no previous declaration for ‘void partial_max_reset(void*)’ [-Wmissing-declarations]\n   53 | void partial_max_reset(void* lut_scales_) {{\n      |      ^~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h: In function ‘void tbl_impl_14336_4096(int32_t*, int8_t*, uint8_t*)’:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:190:50: note: use ‘-flax-vector-conversions’ to permit conversions between vectors with differing element types or numbers of subparts\n  190 |     const int8x16_t vec_zero = vdupq_n_s16(0x0000);\n      |                                                  ^\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:190:43: error: cannot convert ‘int16x8_t’ to ‘const int8x16_t’ in initialization\n  190 |     const int8x16_t vec_zero = vdupq_n_s16(0x0000);\n      |                                ~~~~~~~~~~~^~~~~~~~\n      |                                           |\n      |                                           int16x8_t\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:202:44: error: cannot convert ‘const int8x16_t’ to ‘int16x8_t’\n  202 |             vec_c[i] = vandq_s16(vec_c[i], vec_zero);\n      |                                            ^~~~~~~~\n      |                                            |\n      |                                            const int8x16_t\nIn file included from /root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/ggml-bitnet.h:7,\n                 from /root/BitNet/src/ggml-bitnet-lut.cpp:8:\n/usr/lib/gcc/aarch64-linux-gnu/10.3.1/include/arm_neon.h:1591:37: note:   initializing argument 2 of ‘int16x8_t vandq_s16(int16x8_t, int16x8_t)’\n 1591 | vandq_s16 (int16x8_t __a, int16x8_t __b)\n      |                           ~~~~~~~~~~^~~\nIn file included from /root/BitNet/src/ggml-bitnet-lut.cpp:10:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:217:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  217 |             vec_c[0] += vec_v_left_0.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:217:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:218:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  218 |             vec_c[0] += vec_v_right_0.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:218:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:219:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  219 |             vec_c[1] += vec_v_left_0.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:219:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:220:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  220 |             vec_c[1] += vec_v_right_0.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:220:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:231:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  231 |             vec_c[0] += vec_v_left_1.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:231:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:232:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  232 |             vec_c[0] += vec_v_right_1.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:232:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:233:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  233 |             vec_c[1] += vec_v_left_1.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:233:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:234:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  234 |             vec_c[1] += vec_v_right_1.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:234:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:245:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  245 |             vec_c[2] += vec_v_left_2.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:245:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:246:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  246 |             vec_c[2] += vec_v_right_2.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:246:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:247:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  247 |             vec_c[3] += vec_v_left_2.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:247:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:248:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  248 |             vec_c[3] += vec_v_right_2.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:248:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:259:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  259 |             vec_c[2] += vec_v_left_3.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:259:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:260:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  260 |             vec_c[2] += vec_v_right_3.val[0];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:260:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:261:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  261 |             vec_c[3] += vec_v_left_3.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:261:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:262:22: error: invalid operands to binary + (have ‘int16x8_t’ and ‘int8x16_t’)\n  262 |             vec_c[3] += vec_v_right_3.val[1];\n      |             ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:262:22: note:   in evaluation of ‘operator+=(int16x8_t, int8x16_t)’\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h: At global scope:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:287:9: warning: no previous declaration for ‘int32_t qgemm_lut_14336_4096(void*, void*, void*, void*, void*)’ [-Wmissing-declarations]\n  287 | int32_t qgemm_lut_14336_4096(void* A, void* LUT, void* Scales, void* LUT_Scales, void* C) {\n      |         ^~~~~~~~~~~~~~~~~~~~\nIn file included from /root/BitNet/src/ggml-bitnet-lut.cpp:10:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:299:2: warning: extra ‘;’ [-Wpedantic]\n  299 | };\n      |  ^\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h: In function ‘void tbl_impl_4096_14336(int32_t*, int8_t*, uint8_t*)’:\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:308:43: error: cannot convert ‘int16x8_t’ to ‘const int8x16_t’ in initialization\n  308 |     const int8x16_t vec_zero = vdupq_n_s16(0x0000);\n      |                                ~~~~~~~~~~~^~~~~~~~\n      |                                           |\n      |                                           int16x8_t\n/root/BitNet/3rdparty/llama.cpp/ggml/src/../../../../include/bitnet-lut-kernels.h:320:44: error: cannot convert ‘const int8x16_t’ to ‘int16x8_t’\n  320 |             vec_c[i] = vandq_s16(vec_c[i], vec_zero);\n",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/117/comments",
    "author": "zwx109473",
    "comments": [
      {
        "user": "zwx109473",
        "created_at": "2024-11-15T07:58:31Z",
        "body": "In the log files I am getting this when running python setup_env.py -md /root/Llama3-8B-1.58-100B-tokens -q i2_s\non linux server with arm CPU."
      },
      {
        "user": "noppej",
        "created_at": "2024-11-19T12:43:34Z",
        "body": "@zwx109473 I had similar issues (Debian container image running on Mac M1 - aarch64) until I discovered (help from my issue) that the `clang` install script did not set CC and CXX environment variables, and therefore the process was using gcc rather than clang. Fixing that allowed me to build successfully."
      }
    ]
  },
  {
    "number": 111,
    "title": "Can you support reordering and vector embedding models？",
    "created_at": "2024-11-12T11:57:11Z",
    "closed_at": "2024-11-26T07:53:40Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/111",
    "body": "Can you support reordering and vector embedding models？",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/111/comments",
    "author": "tanyo520",
    "comments": [
      {
        "user": "potassiummmm",
        "created_at": "2024-11-26T07:53:40Z",
        "body": "This is not part of the repository, which provides inference services for large language models."
      }
    ]
  },
  {
    "number": 109,
    "title": "Support for Loading GGUF Quantized Model in Python Pipeline",
    "created_at": "2024-11-11T10:41:26Z",
    "closed_at": "2024-11-26T07:49:44Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/109",
    "body": "Hello, and thanks for this excellent project!\n\nI am currently using the Llama3-8B-1.58-100B-tokens quantized model (ggml-model-i2_s.gguf) from the BitNet repository. The model performs well during inferencing, but I am having difficulty loading the GGUF file format directly in my Python chatbot pipeline to interact with CCV files.\n\nI have tried using llama.cpp and the transformers library, but both approaches resulted in compatibility issues due to the GGUF file format.\n\n**What I’ve Tried:**\nTested inferencing with BitNet, which worked as expected.\nAttempted to load the GGUF file using llama.cpp and transformers, but encountered incompatibilities.\nSearched through the documentation and available issues but couldn’t find a solution or official guidance for loading GGUF models in custom Python pipelines.\n\n**Request:** Could you please provide guidance on how to load the GGUF model directly in Python? Alternatively, is there any internal BitNet function or script that supports GGUF model loading for integration into Python-based chatbot pipelines?\n\nThank you for your help, and I appreciate any advice or guidance you can provide!",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/109/comments",
    "author": "NiloufarAb",
    "comments": [
      {
        "user": "potassiummmm",
        "created_at": "2024-11-26T07:49:44Z",
        "body": "This is not part of the repository. For integration into the python pipeline, we recommend either loading the original model directly or implementing your own set of i2_s quantization methods in python."
      }
    ]
  },
  {
    "number": 108,
    "title": "Cannot quantize to f16 with i2_s",
    "created_at": "2024-11-10T15:53:26Z",
    "closed_at": "2024-11-26T07:49:55Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/108",
    "body": "I am quite a newbie to LLM's, but was running into the following issue:\n\nCommand: `python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s --quant-embd`\nOutput: `ERROR:root:Error occurred while running command: Command '['/home/pis7/miniconda3/envs/bitnet-cpp/bin/python', 'utils/convert-hf-to-gguf-bitnet.py', 'models/Llama3-8B-1.58-100B-tokens', '--outtype', 'f32']' died with <Signals.SIGKILL: 9>., check details in logs/convert_to_f32_gguf.log`\n\nSpecified log output does not show anything wrong (no errors).",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/108/comments",
    "author": "pis7",
    "comments": [
      {
        "user": "Tangweirui2021",
        "created_at": "2024-11-16T00:57:28Z",
        "body": "Just check if the program runs out of memory, I have exactly the same problem with you."
      }
    ]
  },
  {
    "number": 98,
    "title": "Question. why cmake just use one core to compile, its so slow ?",
    "created_at": "2024-10-31T04:08:49Z",
    "closed_at": "2024-11-21T07:55:23Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/98",
    "body": "I ran this command, found its very slow, just use one core to compile the project.\n\npython setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s\n\nthen i changed the code in  setup_env.py as below, but it can't work,  what should I do? \n\nrun_command([\"cmake\", \"--build\", \"build\", \"--config\", \"Release\",\"--parallel\",\"200\"], log_step=\"compile\")",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/98/comments",
    "author": "YouUpDay",
    "comments": [
      {
        "user": "potassiummmm",
        "created_at": "2024-10-31T09:11:13Z",
        "body": "Of course you can increase the speed by adding this command, but I'm curious why it doesn't work, can you show me the contents of the `logs/compile.log` file?"
      }
    ]
  },
  {
    "number": 91,
    "title": "File utils/kernel_tuning.py not used.",
    "created_at": "2024-10-28T16:56:13Z",
    "closed_at": "2024-11-03T00:51:11Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/91",
    "body": "Could you help us understan why this file is empty and what its relevence is to the project?",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/91/comments",
    "author": "Gabrielfernandes7",
    "comments": [
      {
        "user": "sd983527",
        "created_at": "2024-10-31T07:53:34Z",
        "body": "it is for a future update, that support further tuning of the parameters in block size. we will put it out once it is ready. thanks."
      },
      {
        "user": "Gabrielfernandes7",
        "created_at": "2024-10-31T14:02:21Z",
        "body": "> it is for a future update, that support further tuning of the parameters in block size. we will put it out once it is ready. thanks.\n\nThanks, man! 👏🏽"
      }
    ]
  },
  {
    "number": 90,
    "title": "Question - Am I missing something?",
    "created_at": "2024-10-26T15:36:24Z",
    "closed_at": "2024-11-06T16:35:34Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/90",
    "body": "I ran your first example, and it knew Mary was in the garden.\n\nThen I tried to ask it a simple question, \"Yes or No, do you know how to write C# code?\"\n\nHere is the answer after I cranked up the token to get more of a response)\n\nAnswer:\nYes or No, do you know how to write C# code? If you do, then you can write C# code for us. If you don’t, then you can learn to write C# code for us. We will pay you by the hour for your C# code.The first step in the process of creating a new product is to identify the need for the product. This is done by conducting a market research study. The market research study is conducted by a marketing research firm. The marketing research firm is a company that specializes in conducting market research.\n\nDid I ask the question wrong, or is this the wrong type of question?\n\nThis project has nearly 10K stars, so I must be the dumb one, but every result I get back is garbage.\n\nMaybe I need to learn more about \"What is this thing supposed to be able to do?\"\n\n\n\n\n\n\n\n",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/90/comments",
    "author": "DataJuggler",
    "comments": [
      {
        "user": "kth8",
        "created_at": "2024-10-27T03:51:48Z",
        "body": "The models available right now are research models, not instruct models. They won't answer your question but instead act more like an autocomplete."
      },
      {
        "user": "DataJuggler",
        "created_at": "2024-10-27T09:54:21Z",
        "body": "Thanks for the clarification. So with future updates it should have more funcationality?"
      },
      {
        "user": "kth8",
        "created_at": "2024-10-27T10:15:16Z",
        "body": "Yes, there should be better models in the future as BitNet develops. Try formatting your prompt in a way it can autocomplete such as `the sky is blue because` instead of `why is the sky blue?`"
      },
      {
        "user": "edindirangu",
        "created_at": "2024-10-31T13:25:30Z",
        "body": "I tried autocomplete in the following way... Again, and again it still returns wrong answers. \n\nQuestion: python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/Llama3-8B-1.58-100B-tokens-TQ1_0-F16.gguf -p \"Kenya is a country in?\\nAnswer:\" -n 6 -temp 0\n\nAnswer: India is a country in Asia\n\nAlso, if I change the initial Mary question even a slightly, it returns pretty senseless answers. It's strange considering the hype on YouTube about BitNet and 1-Bit LLMs or are people pretraining their own custom models perhaps? Am still hopeful."
      },
      {
        "user": "grctest",
        "created_at": "2024-11-01T16:33:53Z",
        "body": "This is expected, it's not an issue with the BitNet inference framework but rather the limitations of the model you're quantitizing."
      },
      {
        "user": "edindirangu",
        "created_at": "2024-11-02T07:13:42Z",
        "body": "> This is expected, it's not an issue with the BitNet inference framework but rather the limitations of the model you're quantitizing.\n\nThanks. Kindly help me understand your answer more fully."
      },
      {
        "user": "grctest",
        "created_at": "2024-11-02T14:17:26Z",
        "body": "> Thanks. Kindly help me understand your answer more fully.\n\nSure, your issue has nothing to do with BitNet."
      }
    ]
  },
  {
    "number": 79,
    "title": "Getting Errors When following Readme Instruction on ARM server with ubuntu24.04 #74",
    "created_at": "2024-10-23T20:19:35Z",
    "closed_at": "2024-11-06T16:30:10Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/pull/79",
    "body": "Seems the developers forgot to commit these necessary libraries. The changes look safe for me, but I'm unable to verify it for x86 arch and Windows.\r\n\r\nIt's tested and works for MacBook Pro M3 (arm64), with TL1.\r\n\r\nPlease merge carefully.",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/79/comments",
    "author": "MrEcco",
    "comments": [
      {
        "user": "rebroad",
        "created_at": "2024-10-25T23:17:42Z",
        "body": "thank you. I can confirm this fixes the compile issue on Raspbian bookworm"
      }
    ]
  },
  {
    "number": 76,
    "title": "BitNet model outputs repetitive gibberish ('Breis') regardless of input during text generation",
    "created_at": "2024-10-23T02:44:05Z",
    "closed_at": "2024-11-06T16:40:35Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/76",
    "body": "Problem: The model generates repetitive, nonsensical outputs like \"Breis\" regardless of the input provided. This happens even with different generation settings (e.g., temperature, top_k, top_p).\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer from Hugging Face\nmodel = AutoModelForCausalLM.from_pretrained(\"1bitLLM/bitnet_b1_58-3B\")\nLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n\nfrom transformers import AutoConfig\n\n# Load model configuration\nconfig = AutoConfig.from_pretrained(\"1bitLLM/bitnet_b1_58-3B\")\nprint(config)\nLlamaConfig {\n  \"_name_or_path\": \"1bitLLM/bitnet_b1_58-3B\",\n  \"architectures\": [\n    \"BitnetForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 100,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3200,\n  \"initializer_range\": 0.02,\n  \"input_bits\": 8,\n  \"intermediate_size\": 8640,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 26,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 32000,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.45.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32002,\n  \"weight_bits\": 1\n}\n\nfrom tokenization_bitnet import BitnetTokenizer\n\n# Load the custom tokenizer\ntokenizer = BitnetTokenizer(\n    vocab_file=\"bitnet_tokenizer/tokenizer.model\",\n    special_tokens_map_file=\"bitnet_tokenizer/special_tokens_map.json\",\n    tokenizer_config_file=\"bitnet_tokenizer/tokenizer_config.json\",\n    unk_token='<unk>',\n    bos_token='<s>',\n    eos_token='</s>',\n)\n\n# Example input for inference\ninput_text = \"?\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Perform inference\noutputs = model.generate(**inputs, max_length=20, temperature=0.7, top_k=50, top_p=0.9)\nprint(tokenizer.decode(outputs[0]))\n\nThe output: \n<s>? suppis Breis Breis Breis Breis Breis Breis Breis Breis\n\n",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/76/comments",
    "author": "EladWarshawsky",
    "comments": [
      {
        "user": "chinguetti",
        "created_at": "2024-10-23T08:08:45Z",
        "body": "I have the same issue."
      },
      {
        "user": "DatCaptainHorse",
        "created_at": "2024-10-23T13:05:20Z",
        "body": "The listed models are \"proof-of-concept\" at most, they are not meant for real use, someone would need to train a new model from beginning with BitNet in mind for coherent output."
      },
      {
        "user": "EladWarshawsky",
        "created_at": "2024-10-23T17:44:57Z",
        "body": "> The listed models are \"proof-of-concept\" at most, they are not meant for real use, someone would need to train a new model from beginning with BitNet in mind for coherent output.\n\nYeah I guess 100B tokens isn't enough to even have autocomplete. That sucks. I will see about training it potentially."
      },
      {
        "user": "grctest",
        "created_at": "2024-10-25T23:05:53Z",
        "body": "As others have said, this isn't a problem with BitNet."
      }
    ]
  },
  {
    "number": 75,
    "title": "error : cannot use 'throw' with exceptions disabled",
    "created_at": "2024-10-23T01:36:22Z",
    "closed_at": "2024-11-06T16:45:19Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/75",
    "body": "I am on Windows 10. When I run  \npython setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s\nI get\nERROR:root:Error occurred while running command: Command '['cmake', '--build', 'build', '--config', 'Release']' returned non-zero exit status 1., check details in logs\\compile.log\n\nIn compile.log --\n\nBuilding Custom Rule C:/Users/ut/Desktop/Danet/BitNet/3rdparty/llama.cpp/src/CMakeLists.txt\nC:\\Users\\ut\\Desktop\\Danet\\BitNet\\3rdparty\\llama.cpp\\src\\llama.cpp(122,9): error : cannot use 'throw' with exceptions disabled [C:\\Users\\ut\\Desktop\\Danet\\BitNet\\build\\3rdparty\\llama.cpp\\src\\llama.vcxproj]\n",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/75/comments",
    "author": "alake1",
    "comments": [
      {
        "user": "GTJoey",
        "created_at": "2024-10-24T12:16:53Z",
        "body": "upgrade clang >= 18.0 .\ndefault clang version 17.0.3 in vs2022 will get this problem."
      },
      {
        "user": "Chandanpoddar26",
        "created_at": "2024-11-09T11:05:34Z",
        "body": "Still same issue after updating clang to 19.0 version"
      }
    ]
  },
  {
    "number": 72,
    "title": "Fix Readme",
    "created_at": "2024-10-22T16:12:24Z",
    "closed_at": "2024-10-25T05:55:19Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/72",
    "body": "There is an unneeded space at the start of the installation script for LLVM listed in the requirements area.",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/72/comments",
    "author": "RealMrCactus",
    "comments": [
      {
        "user": "shumingma",
        "created_at": "2024-10-25T05:55:19Z",
        "body": "Thanks for reporting the issue! The commit 4943bfd286e2aaff566c6e5a13feb491de380cc3 fixed it."
      }
    ]
  },
  {
    "number": 54,
    "title": "Fix memory leak in quantize_i2_s",
    "created_at": "2024-10-21T09:27:57Z",
    "closed_at": "2024-10-24T08:52:09Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/pull/54",
    "body": null,
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/54/comments",
    "author": "deiteris",
    "comments": [
      {
        "user": "Eddie-Wang1120",
        "created_at": "2024-10-24T08:51:30Z",
        "body": "Good Catch!"
      }
    ]
  },
  {
    "number": 15,
    "title": "Question - Speed comparison to llama.cpp",
    "created_at": "2024-10-18T16:02:13Z",
    "closed_at": "2024-10-20T05:58:23Z",
    "labels": [],
    "url": "https://github.com/microsoft/BitNet/issues/15",
    "body": "Were you running the same model in llama.cpp and bitnet.cpp when making the speed comparisons?",
    "comments_url": "https://api.github.com/repos/microsoft/BitNet/issues/15/comments",
    "author": "jurke-solwr",
    "comments": [
      {
        "user": "sd983527",
        "created_at": "2024-10-20T02:29:07Z",
        "body": "The same model weights value and the output result is the same, however the weights packing methods are different so that different model files are used."
      },
      {
        "user": "jurke-solwr",
        "created_at": "2024-10-20T05:58:24Z",
        "body": "Thanks for clarifying. Your results are very impressive."
      }
    ]
  }
]