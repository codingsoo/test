[
  {
    "number": 612,
    "title": "GH-611: Update docs version in release process",
    "created_at": "2025-02-13T01:59:11Z",
    "closed_at": "2025-02-13T04:33:50Z",
    "labels": [],
    "url": "https://github.com/apache/arrow-java/pull/612",
    "body": "Fixes GH-611.",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/612/comments",
    "author": "kou",
    "comments": [
      {
        "user": "lidavidm",
        "created_at": "2025-02-13T02:22:02Z",
        "body": "In theory...we could change conf.py to read it out of the pom.xml..."
      },
      {
        "user": "kou",
        "created_at": "2025-02-13T03:37:03Z",
        "body": "Oh, it's a good idea! Let's use the approach."
      },
      {
        "user": "kou",
        "created_at": "2025-02-13T04:33:50Z",
        "body": "I close this in favor of #613."
      }
    ]
  },
  {
    "number": 501,
    "title": "MINOR: Bump version to 19.0.0",
    "created_at": "2025-01-10T04:22:09Z",
    "closed_at": "2025-01-10T05:18:47Z",
    "labels": [],
    "url": "https://github.com/apache/arrow-java/pull/501",
    "body": null,
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/501/comments",
    "author": "kou",
    "comments": [
      {
        "user": "kou",
        "created_at": "2025-01-10T04:24:07Z",
        "body": "We will not release a new version immediately but can we merge this to main for easy to GH-496 test?"
      },
      {
        "user": "kou",
        "created_at": "2025-01-10T05:18:47Z",
        "body": "Ah, OK. I'll use 18.2.0 instead of this.\r\nIf we use 18.2.0, we don't need to revert this, right?"
      },
      {
        "user": "kou",
        "created_at": "2025-01-10T05:20:48Z",
        "body": "GH-502"
      },
      {
        "user": "lidavidm",
        "created_at": "2025-01-10T05:32:59Z",
        "body": "Oh, I would've also been OK with 19.0.0 -> 19.1.0 or something. Either way I'll approve the other PR. "
      }
    ]
  },
  {
    "number": 486,
    "title": "GH-469: Unify `ValueVector.getObject` and `VariableWidthFieldVector.get` behavior about null value",
    "created_at": "2025-01-06T15:24:47Z",
    "closed_at": "2025-01-14T05:41:44Z",
    "labels": [],
    "url": "https://github.com/apache/arrow-java/pull/486",
    "body": "Fixes #469.",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/486/comments",
    "author": "aqni",
    "comments": [
      {
        "user": "lidavidm",
        "created_at": "2025-01-13T08:15:16Z",
        "body": "Ah, my UI wasn't showing the approve button..."
      },
      {
        "user": "aqni",
        "created_at": "2025-01-13T09:37:33Z",
        "body": "Some jobs have failed, and the logs show that it seems to be a download timeout. Can you help me re-run the failed jobs?"
      },
      {
        "user": "lidavidm",
        "created_at": "2025-01-13T11:49:24Z",
        "body": "Looks good now. Any other review comments?"
      }
    ]
  },
  {
    "number": 469,
    "title": "`arrow.enable_null_check_for_get` causes change in the behavior of `VarBinaryVector#getObject`",
    "created_at": "2024-12-27T10:52:28Z",
    "closed_at": "2025-01-14T05:41:44Z",
    "labels": [
      "Type: bug",
      "Type: usage"
    ],
    "url": "https://github.com/apache/arrow-java/issues/469",
    "body": "### Describe the bug, including details regarding any error messages, version, and platform.\n\n- version: 18.1.0\r\n- platform: windows, jdk17\r\n\r\n\r\n```java\r\n  @Test\r\n  void testVarbinary(){\r\n    try(RootAllocator allocator = new RootAllocator(Long.MAX_VALUE);\r\n        BigIntVector bigIntVector = new BigIntVector(\"test\", allocator);\r\n        VarBinaryVector varBinVector = new VarBinaryVector(\"test\", allocator)){\r\n      bigIntVector.setNull(0);\r\n      bigIntVector.setValueCount(1);\r\n      varBinVector.setNull(0);\r\n      varBinVector.setValueCount(1);\r\n      Assertions.assertNull(bigIntVector.getObject(0));\r\n      Assertions.assertNull(varBinVector.getObject(0));\r\n    }\r\n  }\r\n```\r\n\r\nWhen I set `arrow.enable_null_check_for_get` to true, this test passes. However, when I set `arrow.enable_null_check_for_get` to false, it failed.\r\n\r\n```\r\norg.opentest4j.AssertionFailedError: \r\nExpected :null\r\nActual   :[B@73e9cf30\r\n\tat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n\tat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n\tat org.junit.jupiter.api.AssertNull.failNotNull(AssertNull.java:50)\r\n\tat org.junit.jupiter.api.AssertNull.assertNull(AssertNull.java:35)\r\n\tat org.junit.jupiter.api.AssertNull.assertNull(AssertNull.java:30)\r\n\tat org.junit.jupiter.api.Assertions.assertNull(Assertions.java:279)\r\n\tat demo.MainTest.testVarbinary(MainTest.java:27)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n```\r\n\r\nWhy does BigIntVector's getObject always return null, while VarBinary Vector's getObject has a different behavior?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/469/comments",
    "author": "aqni",
    "comments": [
      {
        "user": "lidavidm",
        "created_at": "2025-01-03T01:27:14Z",
        "body": "Because you've disabled null checking entirely, so VarBinaryVector takes it as an opportunity to skip the null check and return whatever string data happens to be in that slot (even though the slot is meant to be null, it can still have data allocated, though it is probably an empty string). BigIntVector doesn't have that optimization."
      },
      {
        "user": "lidavidm",
        "created_at": "2025-01-03T01:27:33Z",
        "body": "Arguably, in this case, BigIntVector should also be returning non-null."
      },
      {
        "user": "aqni",
        "created_at": "2025-01-06T08:51:35Z",
        "body": "Is there any plan in the future to unify the behavior of `getObject` when `arrow.enable_null_check_for_get` is set to false?"
      },
      {
        "user": "lidavidm",
        "created_at": "2025-01-06T10:00:29Z",
        "body": "Contributions would be welcome. I don't think anyone is actively working on it."
      }
    ]
  },
  {
    "number": 466,
    "title": "Modules aren't properly declared",
    "created_at": "2024-12-26T03:59:25Z",
    "closed_at": "2024-12-30T02:29:04Z",
    "labels": [
      "Type: bug"
    ],
    "url": "https://github.com/apache/arrow-java/issues/466",
    "body": "### Describe the bug, including details regarding any error messages, version, and platform.\n\n- org.apache.arrow.flight.grpc isn't exported\r\n- JDBC driver uses 'flight.sql.jdbc.core' which doesn't match other packages like 'org.apache.arrow.flight.core'",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/466/comments",
    "author": "lidavidm",
    "comments": [
      {
        "user": "lidavidm",
        "created_at": "2024-12-26T05:55:35Z",
        "body": "So it seems we can't fix the automatic module name for the JDBC driver since Avatica is packaged in a way that it can't become an automatic module :/"
      },
      {
        "user": "wgtmac",
        "created_at": "2024-12-30T02:26:20Z",
        "body": "> So it seems we can't fix the automatic module name for the JDBC driver since Avatica is packaged in a way that it can't become an automatic module :/\r\n\r\nIs it because Avatica is shaded with package name transformed? (It might be a dumb question since I'm still new to JPMS)."
      },
      {
        "user": "lidavidm",
        "created_at": "2024-12-30T02:27:53Z",
        "body": "I didn't save the error but IIRC it had to do with service providers"
      }
    ]
  },
  {
    "number": 465,
    "title": "Deep copy a VectorSchemaRoot?",
    "created_at": "2024-12-24T12:29:02Z",
    "closed_at": "2024-12-26T05:46:13Z",
    "labels": [
      "Type: enhancement"
    ],
    "url": "https://github.com/apache/arrow-java/issues/465",
    "body": "### Describe the enhancement requested\r\n\r\nI'm writing a convertor method to convert a base64 encoded byte array into Arrow batches and returns it to the user.\r\n\r\n```java\r\npublic List<VectorSchemaRoot> readArrowBatches(String rows, BufferAllocator allocator) {\r\n    final List<VectorSchemaRoot> batches = new ArrayList<>();\r\n    final byte[] data = Base64.getDecoder().decode(rows);\r\n    final ByteArrayInputStream stream = new ByteArrayInputStream(data);\r\n    try (final ArrowStreamReader reader = new ArrowStreamReader(stream, allocator)) {\r\n        while (reader.loadNextBatch()) {\r\n            batches.add(new Table(reader.getVectorSchemaRoot()).toVectorSchemaRoot());\r\n        }\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n    return batches;\r\n}\r\n```\r\n\r\nSince `ArrowStreamReader` replace the batch referred by `getVectorSchemaRoot` in each iteration, I have to do a deepcopy of VectorSchemaRoot every time.\r\n\r\nCurrently, I use Table's method as a workaround, but wonder if `VectorSchemaRoot` deserves a `copy` method, or I implement such a typically use case in a wrong way.",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/465/comments",
    "author": "tisonkun",
    "comments": [
      {
        "user": "lidavidm",
        "created_at": "2024-12-26T03:59:54Z",
        "body": "You should use VectorLoader/VectorUnloader to \"move\" the contents of the reader's root into your own"
      },
      {
        "user": "tisonkun",
        "created_at": "2024-12-26T05:38:54Z",
        "body": "That seems exactly what the inner of `Table` does. Do we have some util or a `copy` method for that. Or I just wrap by myself .. It seems quite a common usage and I don't want to hook outside of arrow-java.\r\n\r\n```java\r\nwhile (reader.loadNextBatch()) {\r\n    final VectorSchemaRoot source = reader.getVectorSchemaRoot();\r\n    final VectorUnloader unloader = new VectorUnloader(source);\r\n    final VectorSchemaRoot copy = VectorSchemaRoot.create(source.getSchema(), allocator);\r\n    final VectorLoader loader = new VectorLoader(copy);\r\n    loader.load(unloader.getRecordBatch());\r\n    batches.add(copy);\r\n}\r\n```"
      },
      {
        "user": "lidavidm",
        "created_at": "2024-12-26T05:43:07Z",
        "body": "That is the intended usage. What is the problem?\r\n\r\n(Note that you can also just keep an array of the batches from the unloader, and load/stream them through a root as necessary.)"
      },
      {
        "user": "tisonkun",
        "created_at": "2024-12-26T05:46:09Z",
        "body": "OK thanks. Yes it seems a list of ArrowRecordBatch owns the buffer and doesn't need to tune with the lifecycle of allocator."
      },
      {
        "user": "tisonkun",
        "created_at": "2024-12-26T05:56:55Z",
        "body": "Emmm .. No. The ArrowRecordBatch's buffer is still bound to the allocator, and it doesn't have the schema info where we need to store elsewhere."
      },
      {
        "user": "lidavidm",
        "created_at": "2024-12-26T06:00:19Z",
        "body": "Yes, there isn't really any way of untying things from an allocator (this is intentional). There are APIs to transfer memory between allocators (or you can just keep a single allocator across different contexts)."
      },
      {
        "user": "tisonkun",
        "created_at": "2024-12-26T06:02:21Z",
        "body": "@lidavidm Thanks for your information! Is there some docs/cookbook for copy VectorSchemaRoot? It seems challenging to ensure the lifetime of both data and allocator are aligned and I suppose some demo code would help a lot."
      },
      {
        "user": "tisonkun",
        "created_at": "2024-12-26T06:03:25Z",
        "body": "For example, when I wrote:\r\n\r\n```java\r\n        while (reader.loadNextBatch()) {\r\n            final VectorSchemaRoot source = reader.getVectorSchemaRoot();\r\n            final VectorSchemaRoot copy = VectorSchemaRoot.create(source.getSchema(), allocator);\r\n            new VectorLoader(copy).load(new VectorUnloader(source).getRecordBatch());\r\n            batches.add(copy);\r\n        }\r\n```\r\n\r\nIt seems the intermediate ArrowRecordBatch should be closed but it's very easy to get it wrong and receive a runtime exception .."
      },
      {
        "user": "lidavidm",
        "created_at": "2024-12-26T06:16:48Z",
        "body": "Unfortunately not. You should do something like\r\n\r\n```java\r\ntry (var batch = unloader.getRecordBatch()) {\r\n  loader.load(batch);\r\n}\r\n```"
      }
    ]
  },
  {
    "number": 444,
    "title": "GH-22: Add CODEOWNERS",
    "created_at": "2024-12-03T15:58:53Z",
    "closed_at": "2024-12-05T16:35:21Z",
    "labels": [],
    "url": "https://github.com/apache/arrow-java/pull/444",
    "body": "Fixes #22.",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/444/comments",
    "author": "wgtmac",
    "comments": [
      {
        "user": "wgtmac",
        "created_at": "2024-12-03T16:00:17Z",
        "body": "I just blindly add default codeowners to all files, though I think there are obviously better reviewers for some files. @lidavidm "
      },
      {
        "user": "lidavidm",
        "created_at": "2024-12-04T00:42:58Z",
        "body": "@laurentgo I haven't seen anything from you, is this OK?"
      },
      {
        "user": "wgtmac",
        "created_at": "2024-12-05T16:33:56Z",
        "body": "Let me merge this first in order to get PR notifications :)"
      },
      {
        "user": "laurentgo",
        "created_at": "2024-12-05T16:42:32Z",
        "body": "Sorry, was out for a couple of days, but yes, it's fine! thanks"
      }
    ]
  },
  {
    "number": 436,
    "title": "Disable Maven RAT check in favor of pre-commit one",
    "created_at": "2024-12-02T06:31:30Z",
    "closed_at": "2024-12-12T00:38:19Z",
    "labels": [
      "Type: enhancement"
    ],
    "url": "https://github.com/apache/arrow-java/issues/436",
    "body": "### Describe the enhancement requested\n\nWe have two separate RAT checks now, one run during the build and one done via pre-commit. IMO, we should only use the pre-commit one. The build-time RAT check doesn't share config and it is annoying to have the build interrupted during development.",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/436/comments",
    "author": "lidavidm",
    "comments": [
      {
        "user": "kou",
        "created_at": "2024-12-02T07:09:04Z",
        "body": "Oh, sorry.\r\nI missed this."
      },
      {
        "user": "lidavidm",
        "created_at": "2024-12-02T08:02:13Z",
        "body": "Nah, I forgot about this check too until I ran into it today :grimacing: \r\n\r\nFor somewhat similar reasons I'm not sure if we'd want to keep Spotless (in Maven) or run google-java-format via pre-commit"
      },
      {
        "user": "kou",
        "created_at": "2024-12-02T08:30:38Z",
        "body": "Ah, Spotless isn't a formatter. (I thought that it's a Java formatter.)\r\nWe may want to unify all formatters/linters/... to pre-commit."
      }
    ]
  },
  {
    "number": 49,
    "title": "[Java] ArrowFileReader.getRecordBlocks() seems incompatibility with JDK 8",
    "created_at": "2024-10-05T07:24:58Z",
    "closed_at": "2024-11-26T19:34:48Z",
    "labels": [
      "Type: bug"
    ],
    "url": "https://github.com/apache/arrow-java/issues/49",
    "body": "## Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nI run Arrow's demo codes under JDK8 and Arrow 17.0.0, but it seems not work.\r\nIPC reader example code as below:\r\n`\r\n\r\ntry(\r\n                BufferAllocator allocator = new RootAllocator(Long.MAX_VALUE);\r\n                FileInputStream fileInputStream = new FileInputStream(new File(\"random_access_file.arrow\"));\r\n                ArrowFileReader reader = new ArrowFileReader(fileInputStream.getChannel(), allocator);\r\n        ){\r\n            System.out.println(\"Record batches in file: \" + reader.getRecordBlocks().size());\r\n            for (ArrowBlock arrowBlock : reader.getRecordBlocks()) {\r\n                reader.loadRecordBatch(arrowBlock);\r\n                VectorSchemaRoot root = reader.getVectorSchemaRoot();\r\n                System.out.println(\"VectorSchemaRoot read: \\n\" + root.contentToTSVString());\r\n            }\r\n        } catch (IOException e) {\r\n            e.printStackTrace();\r\n        }\r\n\r\n`\r\n\r\n\r\ndemo\r\nmaven:\r\n\r\n> <dependency>\r\n>             <groupId>org.apache.arrow</groupId>\r\n>             <artifactId>arrow-vector</artifactId>\r\n>             <version>17.0.0</version>\r\n>         </dependency>\r\n>         <dependency>\r\n>             <groupId>org.apache.arrow</groupId>\r\n>             <artifactId>arrow-memory-netty</artifactId>\r\n>             <version>17.0.0</version>\r\n>         </dependency>\r\n\r\nthe error info is:\r\n`\r\nException in thread \"main\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;\r\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.readSchema(ArrowFileReader.java:94)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:179)\r\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.initialize(ArrowFileReader.java:121)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\r\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.getRecordBlocks(ArrowFileReader.java:180)\r\n\tat com.hd.arrowdemo.Main.ipcReader(Main.java:93)\r\n\tat com.hd.arrowdemo.Main.main(Main.java:44)\r\n`\r\n\r\n### Component(s)\r\n\r\nJava",
    "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/49/comments",
    "author": "ovasty",
    "comments": [
      {
        "user": "vibhatha",
        "created_at": "2024-10-05T08:59:54Z",
        "body": "Just curious, what happens if you run with JDK 11? "
      },
      {
        "user": "saurabhchauhan006",
        "created_at": "2024-11-25T15:22:01Z",
        "body": "So the problem is that arrow 17.0.0 version is compiled using java 9 and above.\r\nIn ArrowFileReader.readSchema() it is calling ByteBuffer.flip() and because in java 8 there is no implementation for ByteBuffer.flip() it fails.\r\nTo fix this I have forked this class and type casted ByteBuffer class with Buffer.\r\n`((Buffer)buffer).flip();`\r\n\r\nSo basically in java 9 and above ByteBuffer.flip() just calls Buffer.flip(). So by this way it solves the problem with Java 8.\r\nI am not pushing a fix for this as the current version of Arrow requires java 11 and above, so there is no point of fixing this now."
      }
    ]
  }
]