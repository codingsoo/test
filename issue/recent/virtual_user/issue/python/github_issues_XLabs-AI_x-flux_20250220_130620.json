[
  {
    "number": 51,
    "title": "How to generate several images at one time by main.py(lora)",
    "created_at": "2024-08-16T13:07:49Z",
    "closed_at": "2024-08-21T07:50:36Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/51",
    "body": "I don't see some parameters represents the number of the image\r\nso how can i modify the `forward` function to generate such as 4 images at one time? ",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/51/comments",
    "author": "arrowonstr",
    "comments": [
      {
        "user": "Anghellia",
        "created_at": "2024-08-20T14:10:22Z",
        "body": "Hi, we added `--num_images_per_prompt` argument, please check"
      }
    ]
  },
  {
    "number": 46,
    "title": "How to train Flux LoRa with a local model dir?",
    "created_at": "2024-08-15T02:34:17Z",
    "closed_at": "2024-08-26T08:50:16Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/46",
    "body": "I download the model to my local dir (following the file structure of the flux repo), so i wanna use the local dir to train flux instead of downloading model by the procedure.\r\nI think it's easier to manage. \r\nHow can I do it? I tried to modify the code but failed\r\nI will be grateful if you can solve the problem",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/46/comments",
    "author": "arrowonstr",
    "comments": [
      {
        "user": "chy-chiu",
        "created_at": "2024-08-15T05:47:43Z",
        "body": "You can do that by specifying environment variables FLUX_DEV, FLUX_DEV_FP8, and AE"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-15T06:13:32Z",
        "body": "Thanks!\r\nI got it \"ckpt_path=os.getenv(\"FLUX_DEV\")\"\r\nI think local dir FLUX_DEV should contain \"flux1-dev.safetensors\"\"model_index.json\"\"ae.safetensors\" the three files\r\n\r\nand these:\r\n```\r\ndef load_t5(device: str | torch.device = \"cuda\", max_length: int = 512) -> HFEmbedder:\r\n    # max length 64, 128, 256 and 512 should work (if your sequence is short enough)\r\n    return HFEmbedder(\"xlabs-ai/xflux_text_encoders\", max_length=max_length, torch_dtype=torch.bfloat16).to(device)\r\n\r\ndef load_clip(device: str | torch.device = \"cuda\") -> HFEmbedder:\r\n    return HFEmbedder(\"openai/clip-vit-large-patch14\", max_length=77, torch_dtype=torch.bfloat16).to(device)\r\n```\r\n```\r\nclass HFEmbedder(nn.Module):\r\n    def __init__(self, version: str, max_length: int, **hf_kwargs):\r\n        super().__init__()\r\n        self.is_clip = version.startswith(\"openai\")\r\n        self.max_length = max_length\r\n        self.output_key = \"pooler_output\" if self.is_clip else \"last_hidden_state\"\r\n\r\n        if self.is_clip:\r\n            self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: CLIPTextModel = CLIPTextModel.from_pretrained(version, **hf_kwargs)\r\n        else:\r\n            self.tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: T5EncoderModel = T5EncoderModel.from_pretrained(version, **hf_kwargs)\r\n```\r\ndownload the tow repo and change the version to local dir, I will try it, hope it works!"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-16T05:07:23Z",
        "body": "It works and make sure to follow these:\r\n\r\n```\r\nexport FLUX_DEV=\"/root/autodl-tmp/project/flux-d/model/FLUX.1-dev/flux1-dev.safetensors\"\r\nexport AE=\"/root/autodl-tmp/project/flux-d/model/FLUX.1-dev/ae.safetensors\"\r\n```\r\nmodify the code\r\n```\r\ndef load_t5(device: str | torch.device = \"cuda\", max_length: int = 512) -> HFEmbedder:\r\n    # max length 64, 128, 256 and 512 should work (if your sequence is short enough)\r\n    return HFEmbedder(\"local_dir/xflux_text_encoders\", max_length=max_length, torch_dtype=torch.bfloat16).to(device)\r\n\r\ndef load_clip(device: str | torch.device = \"cuda\") -> HFEmbedder:\r\n    return HFEmbedder(\"local_dir/clip-vit-large-patch14\", max_length=77, torch_dtype=torch.bfloat16).to(device)\r\n```\r\n```\r\nclass HFEmbedder(nn.Module):\r\n    def __init__(self, version: str, max_length: int, **hf_kwargs):\r\n        super().__init__()\r\n        # from local dir\r\n        #self.is_clip = version.startswith(\"openai\")\r\n        if version == \"/root/autodl-tmp/project/flux-d/model/xflux_text_encoders\":\r\n            self.is_clip= False\r\n        else:\r\n            self.is_clip= True\r\n        self.max_length = max_length\r\n        self.output_key = \"pooler_output\" if self.is_clip else \"last_hidden_state\"\r\n\r\n        if self.is_clip:\r\n            self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: CLIPTextModel = CLIPTextModel.from_pretrained(version, **hf_kwargs)\r\n        else:\r\n            self.tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: T5EncoderModel = T5EncoderModel.from_pretrained(version, **hf_kwargs)\r\n\r\n        self.hf_module = self.hf_module.eval().requires_grad_(False)\r\n```\r\n\r\ncarefully modify the code to specify which version represent a clip model\r\n```\r\n if version == \"/root/autodl-tmp/project/flux-d/model/xflux_text_encoders\":\r\n            self.is_clip= False\r\n        else:\r\n            self.is_clip= True\r\n```"
      }
    ]
  }
]