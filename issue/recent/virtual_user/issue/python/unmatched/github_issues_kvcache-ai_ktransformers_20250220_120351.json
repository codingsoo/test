[
  {
    "number": 73,
    "title": "When the input token exceeds 4096, an error will occur.",
    "created_at": "2024-09-02T06:23:20Z",
    "closed_at": "2024-09-03T11:34:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/73",
    "body": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/fastapi/routing.py\", line 210, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 651, in forward\r\n    causal_mask = self._update_causal_mask(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1624, in _update_causal_mask\r\n    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\r\n                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nRuntimeError: The size of tensor a (4096) must match the size of tensor b (8122) at non-singleton dimension 3",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/73/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T09:30:43Z",
        "body": "> Hi, I haven\u2019t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?\r\n\r\nI compiled it using the most recent code, and my configs are: \r\n``` yaml\r\n- match:\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding\r\n  replace:\r\n    class: ktransformers.operators.RoPE.YarnRotaryEmbedding\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\.(?!.*self_attn\\\\.kv_b_proj).*$\"  # regular expression\r\n    class: torch.nn.Linear  # only match modules matching name and class simultaneously\r\n  replace:\r\n    class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n      generate_op: \"KLinearMarlin\"\r\n      prefill_op: \"KLinearTorch\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp$\"\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE\r\n  replace:\r\n    class: ktransformers.operators.experts.KDeepseekV2MoE     # mlp module with custom forward function\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp\\\\.experts$\"\r\n  replace:\r\n    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism\r\n    kwargs:\r\n      prefill_device: \"cuda\"\r\n      prefill_op: \"KExpertsTorch\"\r\n      generate_device: \"cpu\"\r\n      generate_op: \"KExpertsCPU\"\r\n      out_device: \"cuda\"\r\n  recursive: False # don't recursively inject submodules of this module\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.self_attn$\"\r\n  replace:\r\n    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model$\"\r\n  replace:\r\n    class: \"ktransformers.operators.models.KDeepseekV2Model\"\r\n    kwargs:\r\n      per_layer_prefill_intput_threshold: 0 # 0 is close layer wise prefill\r\n- match:\r\n    name: \"^model.embed_tokens\"\r\n  replace:\r\n    class: \"default\"\r\n    kwargs:\r\n      generate_device: \"cpu\"\r\n      prefill_device: \"cuda\"\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T10:05:26Z",
        "body": "I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release.\r\nIf you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens"
      }
    ]
  }
]