[
  {
    "number": 630,
    "title": "Checkpoint saving differences",
    "created_at": "2024-12-15T09:30:49Z",
    "closed_at": "2024-12-17T02:16:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/630",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\nWhen having `grad_accumulation_steps` set to a different value than `1`, the checkpoint saving is a bit unexpected:\r\n\r\nIn `trainer.py` for setting `save_per_updates`:\r\n\r\n```python\r\nif global_step % (self.save_per_updates * self.grad_accumulation_steps) == 0:\r\n    self.save_checkpoint(global_step)\r\n```\r\n\r\nfor setting `last_per_steps`:\r\n\r\n```python\r\nif global_step % self.last_per_steps == 0:\r\n    self.save_checkpoint(global_step, last=True)\r\n```\r\n\r\nConsequently, for my global batch-size of `19200*8` the value of `save_per_updates` needs to be divided by `8` to be comparable to the setting of `last_per_steps` and the overall variable `global_step` shown via `tqdm`.\r\n\r\nIs this intended ?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/630/comments",
    "author": "lumpidu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T10:11:43Z",
        "body": "yes, intended\r\nstep and update are different stuffs"
      }
    ]
  },
  {
    "number": 546,
    "title": "Add functionality to the Transcription section",
    "created_at": "2024-11-28T04:53:46Z",
    "closed_at": "2024-12-13T07:49:04Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/546",
    "body": "### Checks\n\n- [X] This template is only for feature request.\n- [X] I have thoroughly reviewed the project documentation but couldn't find any relevant information that meets my needs.\n- [X] I have searched for existing issues, including closed ones, and found not discussion yet.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### 1. Is this request related to a challenge you're experiencing? Tell us your story.\n\nI can't upload a large volume of audio files in the \"Transcription data\" section to create a dataset through the audio file upload window, everything freezes. My file size is 20 GB\n\n### 2. What is your suggested solution?\n\nCan you add a window in \"Gradio\" in the \"Transcription data\" section where I can specify the path to the folder where the audio files are located, this will eliminate the freeze and make it possible to upload a large volume of files to create a dataset.\r\n\r\nThank you\n\n### 3. Additional context or comments\n\n_No response_\n\n### 4. Can you help us with this feature?\n\n- [ ] I am interested in contributing to this feature.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/546/comments",
    "author": "nikish3",
    "comments": [
      {
        "user": "danielw97",
        "created_at": "2024-11-28T05:21:53Z",
        "body": "I don't have the interface in front of me at the moment, but there is an audio from path checkbox in the gradio training interface if memory serves.\r\nYou can then place your files into the dataset subdirectory of your dataset.\r\ni.e. if your dataset is called voice1, the directory would be voice1/dataset\r\nHope this helps a bit."
      }
    ]
  },
  {
    "number": 359,
    "title": "small update gradio finetune",
    "created_at": "2024-11-01T09:23:40Z",
    "closed_at": "2024-11-01T10:22:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/359",
    "body": "@SWivid just small fix stuff \r\nso now when audio stereo always get duraction mono and resample\r\nafter train take case stereo to mono and resample\r\nand just fix error speling the bf16",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/359/comments",
    "author": "lpscr",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T10:22:32Z",
        "body": "@lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification.\r\nJust remove the num_channel in eq is fine."
      }
    ]
  },
  {
    "number": 197,
    "title": "Minimal GPU Memory Requirements for Running the Model??",
    "created_at": "2024-10-21T07:10:33Z",
    "closed_at": "2024-10-21T11:24:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/197",
    "body": "I would like to know:\r\n\r\nWhat is the minimal GPU memory requirement for running this model smoothly?\r\nIs there a recommended batch size or other configurations (like precision or memory optimizations) I should use to reduce memory usage?\r\ni managed to change batch_size of trainer.py script, utils_infer.py still encountering cuda oom!! can you tell me the minimal requirements of GPU mmeory??",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/197/comments",
    "author": "sachin-seisei",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:05:56Z",
        "body": "the minimal GPU memory is like 2G (just loading F5/E2 TTS model, and not leverage ASR model to do transcription)\r\nbtw you were to do training or inference.\r\n"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:08:13Z",
        "body": "@SWivid so inference only i am doing as of now!! thats why i want to know in inference also why its showing cuda oom!!\r\nPS: i am not leveraging ASR even i am providing ref text also and not an empty string!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:15:49Z",
        "body": "which script are you using? gradio_app.py or inference-cli.py"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:16:16Z",
        "body": "inference-cli.py \r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:48:26Z",
        "body": "how many gpu mem do you have.\r\ni just tweak the `inference-cli.py` script, make sure it is not loading unneeded asr pipeline.\r\nit takes 1622M gpu mem for me running it"
      }
    ]
  },
  {
    "number": 192,
    "title": "RuntimeError: Input type (c10::Half) and bias type (float) should be the same",
    "created_at": "2024-10-21T01:57:10Z",
    "closed_at": "2024-10-21T17:55:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/192",
    "body": "I just started getting this error with the latest build:\r\n```\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/ttspod/speech/f5.py\", line 214, in infer_batch\r\n    generated_wave = self.vocos.decode(generated_mel_spec.cpu())\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n        ^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 308, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/192/comments",
    "author": "ajkessel",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T02:32:21Z",
        "body": "`generated = generated.to(torch.float32)` should be added as we done along with .half() for model and input\r\n```\r\n# Final result\r\ngenerated = generated.to(torch.float32)\r\ngenerated = generated[:, ref_audio_len:, :]\r\ngenerated_mel_spec = generated.permute(0, 2, 1)\r\ngenerated_wave = vocos.decode(generated_mel_spec.cpu())\r\n```"
      }
    ]
  },
  {
    "number": 190,
    "title": "Broken on MPS",
    "created_at": "2024-10-20T19:29:57Z",
    "closed_at": "2024-10-20T20:06:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/190",
    "body": "Not sure what happened, but looks like the app is broken on Macs at the moment.\r\n\r\nJust did a fresh install and the app itself runs, but the resulting audio is empty.\r\n\r\nAlso I am not sure if the logs are useful but pasting just in case:\r\n\r\n```\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n  warnings.warn(\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n\r\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\ngen_text 0 Reference text will be automatically transcribed with Whisper if not provided. For best results, keep your reference clips short\r\nBuilding prefix dict from the default dictionary ...\r\nLoading model from cache /Users/x/pinokio/cache/TMPDIR/jieba.cache\r\nLoading model cost 0.426 seconds.\r\nPrefix dict has been built successfully.\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:574: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\r\n  warnings.warn(warning.format(data.dtype))\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:577: RuntimeWarning: invalid value encountered in cast\r\n  data = data.astype(np.int16)\r\n```\r\n\r\nI am not completely sure but I don't remember seeing this many warning messages previously.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/190/comments",
    "author": "cocktailpeanut",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:35:37Z",
        "body": "a default fp16 inference setting was added.\r\nsee if the last commit works d3badb95cf1b97a61472d65d4787a35cddf9c908"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-20T20:05:25Z",
        "body": "> Could you share what the switch to fp16 means from end user's point of view (performance, etc.)? Appreciate it!\r\n\r\nA bit faster than using fp32, ~half graphics card usage (the %), and more environmentally friendly maybe lol\r\nCompared to a more aggressive int8 quantization, it can be seen as no performance (quality) penalty."
      }
    ]
  }
]