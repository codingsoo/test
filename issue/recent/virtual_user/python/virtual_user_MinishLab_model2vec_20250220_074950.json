[
  {
    "id": "https://github.com/MinishLab/model2vec/issues/160",
    "source": {
      "issue_number": 160
    },
    "initial_question": {
      "title": "distill",
      "body": "Hi! Thanks for the great repo. I'm trying to distill a model2vec model from a relatively small sentence transformer, but it's taking several hours on GPU. Do you have any advice on optimizing this?\n\n```\nfrom model2vec.distill import distill\nm2v_model = distill(\"MY MODEL\", pca_dims=256)\nm2v_model.save_pretrained(\"m2v_model\")\n```\n"
    },
    "satisfaction_conditions": [
      "Model distillation process successfully initiates and completes",
      "Model loading succeeds without code execution errors",
      "Works with or without GPU availability"
    ],
    "created_at": "2025-01-23T14:47:13Z"
  },
  {
    "id": "https://github.com/MinishLab/model2vec/issues/137",
    "source": {
      "issue_number": 137
    },
    "initial_question": {
      "title": "Using Model2Vec for Token Classification",
      "body": "Hi Model2Vec Team,\r\n\r\nThis is truely great work, thanks so much for the contribution! Awesome project!\r\n\r\nOne dumb question: Is it possible to apply model2vec to a token classification model, e.g., BERT model connected to a NER tagger?\r\n\r\nI've been playing with Model2Vec, and seems that for now it is designed to only output embedding for a whole sequence (e.g., [CLS]). It seems that Model2Vec currently does not support generating embeddings for every token in a sequence. Is this understanding correct?\r\n\r\nIf so, how would you recommend that I augment Model2Vec to support generating token-level embeddings, and potentially drop it into a NER tagger?\r\n\r\nThank you!"
    },
    "satisfaction_conditions": [
      "The solution must provide token-level embeddings for input sequences",
      "The solution must work with existing Model2Vec infrastructure",
      "The solution must support batch processing of multiple input sequences"
    ],
    "created_at": "2024-11-29T01:57:11Z"
  },
  {
    "id": "https://github.com/MinishLab/model2vec/issues/110",
    "source": {
      "issue_number": 110
    },
    "initial_question": {
      "title": "HFValidationError using custom model",
      "body": "Hi,\r\n\r\nI am trying to use Model2Vec with a custom model we have.\r\nThe `distill` seems to run fine, until it tries to call `validate_repo_id` with the custom path I have (`./model`).\r\n\r\n1. I have a custom model that I can load successfully:\r\n\r\n```python\r\nfrom transformers import AutoModelForSequenceClassification\r\n\r\nmodel_save_path = \"./model\"\r\nmodel = AutoModelForSequenceClassification.from_pretrained(model_save_path)\r\n```\r\n\r\n2. I `distill` the custom model:\r\n\r\n```python\r\nfrom model2vec.distill import distill\r\n\r\nmodel_save_path = \"./model\"\r\nm2v_model = distill(model_name=model_save_path, pca_dims=256)\r\n```\r\n\r\n3. The following error is raised:\r\n\r\n```\r\nHFValidationError\r\n\r\n    This cell raised an exception: HFValidationError('Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './model'.')\r\n```\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Model path reference must be valid for HuggingFace validation",
      "Custom model must be successfully loaded for distillation",
      "Distillation process completes without validation errors"
    ],
    "created_at": "2024-10-23T11:59:24Z"
  },
  {
    "id": "https://github.com/MinishLab/model2vec/issues/108",
    "source": {
      "issue_number": 108
    },
    "initial_question": {
      "title": "Number of tokens (151646) does not match number of vectors (151643)",
      "body": "Hello.\r\n\r\nI tried testing `model2vec` in the following environment and encountered the error below. Is there a way to resolve this?\r\n\r\n### Environment\r\n\r\n```python\r\nmodel2vec==0.3.0\r\ntokenizers==0.19.1\r\n```\r\n\r\n### Code\r\n\r\n```python\r\nfrom model2vec.distill.distillation import distill\r\n\r\n# Choose a Sentence Transformer model\r\nmodel_id = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\r\n\r\n# Distill an output model with the chosen dimensions\r\nmodel = distill(model_name=model_id, device=\"cpu\", pca_dims=256)\r\n```\r\n\r\n### Error\r\n\r\n```python\r\nValue Error: Number of tokens (151646) does not match number of vectors (151643)\r\n```"
    },
    "satisfaction_conditions": [
      "Accurate vocabulary size representation",
      "Reliable vocabulary size detection"
    ],
    "created_at": "2024-10-23T08:25:59Z"
  },
  {
    "id": "https://github.com/MinishLab/model2vec/issues/79",
    "source": {
      "issue_number": 79
    },
    "initial_question": {
      "title": "AttributeError: 'NoneType' object has no attribute 'get'",
      "body": "If I try to use the distilled model with Sentence Transformers, I am getting the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[4], line 7\r\n      5 model_distilled = distill(model_name=model_name, pca_dims=256)\r\n      6 model_distilled.save_pretrained(\"./multilingual-e5-small-distilled\")\r\n----> 7 st_model_distilled = SentenceTransformer(\"multilingual-e5-small-distilled\")\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:306, in SentenceTransformer.__init__(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\r\n    294         modules, self.module_kwargs = self._load_sbert_model(\r\n    295             model_name_or_path,\r\n    296             token=token,\r\n   (...)\r\n    303             config_kwargs=config_kwargs,\r\n    304         )\r\n    305     else:\r\n--> 306         modules = self._load_auto_model(\r\n    307             model_name_or_path,\r\n    308             token=token,\r\n    309             cache_folder=cache_folder,\r\n    310             revision=revision,\r\n    311             trust_remote_code=trust_remote_code,\r\n    312             local_files_only=local_files_only,\r\n    313             model_kwargs=model_kwargs,\r\n    314             tokenizer_kwargs=tokenizer_kwargs,\r\n    315             config_kwargs=config_kwargs,\r\n    316         )\r\n    318 if modules is not None and not isinstance(modules, OrderedDict):\r\n    319     modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1454, in SentenceTransformer._load_auto_model(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\r\n   1451 tokenizer_kwargs = shared_kwargs if tokenizer_kwargs is None else {**shared_kwargs, **tokenizer_kwargs}\r\n   1452 config_kwargs = shared_kwargs if config_kwargs is None else {**shared_kwargs, **config_kwargs}\r\n-> 1454 transformer_model = Transformer(\r\n   1455     model_name_or_path,\r\n   1456     cache_dir=cache_folder,\r\n   1457     model_args=model_kwargs,\r\n   1458     tokenizer_args=tokenizer_kwargs,\r\n   1459     config_args=config_kwargs,\r\n   1460 )\r\n   1461 pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \"mean\")\r\n   1462 self.model_card_data.set_base_model(model_name_or_path, revision=revision)\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:56, in Transformer.__init__(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\r\n     53     config_args = {}\r\n     55 config = AutoConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir)\r\n---> 56 self._load_model(model_name_or_path, config, cache_dir, **model_args)\r\n     58 if max_seq_length is not None and \"model_max_length\" not in tokenizer_args:\r\n     59     tokenizer_args[\"model_max_length\"] = max_seq_length\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:87, in Transformer._load_model(self, model_name_or_path, config, cache_dir, **model_args)\r\n     85     self._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\r\n     86 else:\r\n---> 87     self.auto_model = AutoModel.from_pretrained(\r\n     88         model_name_or_path, config=config, cache_dir=cache_dir, **model_args\r\n     89     )\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    562 elif type(config) in cls._model_mapping.keys():\r\n    563     model_class = _get_model_class(config, cls._model_mapping)\r\n--> 564     return model_class.from_pretrained(\r\n    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\r\n    566     )\r\n    567 raise ValueError(\r\n    568     f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\r\n    569     f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\r\n    570 )\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3792, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\r\n   3789 with safe_open(resolved_archive_file, framework=\"pt\") as f:\r\n   3790     metadata = f.metadata()\r\n-> 3792 if metadata.get(\"format\") == \"pt\":\r\n   3793     pass\r\n   3794 elif metadata.get(\"format\") == \"tf\":\r\n\r\nAttributeError: 'NoneType' object has no attribute 'get'\r\n```\r\n\r\nUse the following code snippet to reproduce the error:\r\n\r\n```python\r\nfrom model2vec.distill import distill\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\nmodel_name = \"intfloat/multilingual-e5-small\"\r\nmodel_distilled = distill(model_name=model_name, pca_dims=256)\r\nmodel_distilled.save_pretrained(\"./multilingual-e5-small-distilled\")\r\nst_model_distilled = SentenceTransformer(\"multilingual-e5-small-distilled\")\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Model initialization must be compatible with SentenceTransformer architecture",
      "Proper model distillation integration with SentenceTransformer framework",
      "Model architecture maintains required embedding functionality",
      "Correct component initialization sequence"
    ],
    "created_at": "2024-10-12T19:01:40Z"
  }
]