[
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/122",
    "source": {
      "issue_number": 122
    },
    "initial_question": {
      "title": "cuda error: an illegal memory access was encountered",
      "body": "I updated the CogVideoXWrapper about 10 hours ago. after this update it keep showing this error when the workflow comes to Cogvideo Decode. I tried i2v and the Pose workflow, they both showed the same error. the error is only shown on the cmd window, and the Cogvideo Decode node is stuck since then.  the  full log is as follows:\r\ngot prompt\r\nTemporal tiling disabled\r\nException in thread Thread-22 (prompt_worker):\r\nTraceback (most recent call last):\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 317, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 192, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\custom_nodes\\ComfyUI-CogVideoXWrapper\\nodes.py\", line 926, in decode\r\n    frames = vae.decode(latents).sample\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\utils\\accelerate_utils.py\", line 46, in wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 1184, in decode\r\n    decoded = self._decode(z).sample\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 1142, in _decode\r\n    return self.tiled_decode(z, return_dict=return_dict)\r\n\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 1332, in tiled_decode\r\n    tile = self.decoder(tile)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 877, in forward\r\n    hidden_states = up_block(hidden_states, temb, sample)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 602, in forward\r\n    hidden_states = resnet(hidden_states, temb, zq)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 303, in forward\r\n    hidden_states = self.conv2(hidden_states)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 144, in forward\r\n    output = self.conv(inputs)\r\n\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl_cogvideox.py\", line 64, in forward\r\n    return super().forward(input)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 610, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 605, in _conv_forward\r\n    return F.conv3d(\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\threading.py\", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"<enhanced_experience vendors.sentry_sdk.integrations.threading>\", line 99, in run\r\n  File \"<enhanced_experience vendors.sentry_sdk.integrations.threading>\", line 94, in _run_old_run_func\r\n  File \"<enhanced_experience vendors.sentry_sdk.utils>\", line 1649, in reraise\r\n  File \"<enhanced_experience vendors.sentry_sdk.integrations.threading>\", line 92, in _run_old_run_func\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\threading.py\", line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\main.py\", line 125, in prompt_worker\r\n    e.execute(item[2], prompt_id, item[3], item[4])\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\custom_nodes\\rgthree-comfy\\__init__.py\", line 211, in rgthree_execute\r\n    return self.rgthree_old_execute(*args, **kwargs)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 494, in execute\r\n    result, error, ex = execute(self.server, dynamic_prompt, self.caches, node_id, extra_data, executed, prompt_id, execution_list, pending_subgraph_results)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 384, in execute\r\n    input_data_formatted[name] = [format_value(x) for x in inputs]\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 384, in <listcomp>\r\n    input_data_formatted[name] = [format_value(x) for x in inputs]\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\execution.py\", line 236, in format_value\r\n    return str(x)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\_tensor.py\", line 461, in __repr__\r\n    return torch._tensor_str._str(self, tensor_contents=tensor_contents)\r\n  File \"I:\\stable_diffusion\\ComfyUI-aki-v1.2\\ComfyUI-aki-v1.2\\python\\lib\\site-packages\\torch\\_tensor_str.py\", line 677, in _str\r\n------------------------\r\nFault Traceback: \r\nNot Available"
    },
    "satisfaction_conditions": [
      "VAE decoding operation completes without CUDA memory errors",
      "CogVideo Decode node successfully processes frames",
      "Compatible VAE tiling configuration",
      "Maintains backward workflow compatibility"
    ],
    "created_at": "2024-10-02T14:55:13Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/119",
    "source": {
      "issue_number": 119
    },
    "initial_question": {
      "title": "CogVideoX-Fun-V1.1",
      "body": "is the CogVideoX-Fun-V1.1 not compatible with I2V like the first one? it completely distorts and warps images, no matter the noise strength \"wich i have no idea how it works\" or cfg, i had a poor cat turn into a strange matter akin to a demon from the depths of hell so many times that i had to delete the poor thing, i downloaded the model directly from HF, could that be the reason tho? i might re-download it from the loader tomorrow maybe."
    },
    "satisfaction_conditions": [
      "Model generates stable, non-distorted video output",
      "Noise strength parameter is properly calibrated"
    ],
    "created_at": "2024-10-01T21:00:40Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/100",
    "source": {
      "issue_number": 100
    },
    "initial_question": {
      "title": "Base model and the GGUF model differences?",
      "body": "What's the difference between a 5bFun model and the 5bfun-GGUF model? i get images more vivid and darker with GGUF versions, is that the only thing it's for? i am so confused about these model versions, is there a description for each somewhere?"
    },
    "satisfaction_conditions": [
      "Explains the technical relationship between base and GGUF versions of the same model",
      "Describes observable differences in model performance and output",
      "Clarifies resource usage differences between versions",
      "Addresses the purpose/benefits of using GGUF format"
    ],
    "created_at": "2024-09-27T22:23:00Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/97",
    "source": {
      "issue_number": 97
    },
    "initial_question": {
      "title": "1Torch was not compiled with flash attention",
      "body": "I know this most likely has nothing to do with Cog, but I'm getting the following:\r\n```ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention.```\r\nIt still runs okay, I'm just wondering if this is compromising my quality or speed or anything.\r\n\r\nThanks in advance for any help."
    },
    "satisfaction_conditions": [
      "System continues to function despite the warning message",
      "Performance impact is clarified",
      "Platform compatibility status is explained",
      "Impact on core functionality is addressed"
    ],
    "created_at": "2024-09-25T06:30:07Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/95",
    "source": {
      "issue_number": 95
    },
    "initial_question": {
      "title": "OneDiff Support",
      "body": "Hello! \r\nI was running all sample workflows on my system (Linux, PyTorch 2.4, onediff, RTX 4090). \r\nThey all work, but onediff is not being used. \r\n\r\nWhat do I need to trigger it? \r\n\r\nThank you! "
    },
    "satisfaction_conditions": [
      "Model format must be compatible with OneDiff"
    ],
    "created_at": "2024-09-24T17:03:29Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/85",
    "source": {
      "issue_number": 85
    },
    "initial_question": {
      "title": "Question about decoder",
      "body": "Apologies if this is a stupid question, I have pretty much zero knowledge about how video models work compared to image models.\r\n\r\nIs the way the sampler and decoder currently work somehow similar to generating images in batches with SA or Flux? If so, would it be theoretically possible to unbatch the latents or specify which frame(s) to send to the decoder by index?\r\n\r\nOr am I totally off the mark and the decoder already handles this already, and doesn't decode all frames at once.\r\n\r\nThe reason I got curious about this is because with Fun-xB models (I assume probably caused by the models themselves, not the wrapper) I noticed that it will output extra/duplicated frames at the beginning of image sequence (ie: it outputs 16 frames when selecting 13 frames as length), so I was wondering if it was possible to remove those before sending to the decoder, thus somehow saving on a bit of resources along the way when decoding.\r\n\r\nI tried a few different nodes to separate latents from batch/select by index but obviously it doesn't work ;)"
    },
    "satisfaction_conditions": [
      "Video frame count discrepancies must be explained",
      "Temporal compression ratio must be specified",
      "Minimum decoding batch size must be defined",
      "Artifact-free frame reconstruction must be achieved",
      "Memory optimization method must be provided"
    ],
    "created_at": "2024-09-21T21:39:28Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/32",
    "source": {
      "issue_number": 32
    },
    "initial_question": {
      "title": "`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but got: `prompt_embeds` torch.Size([1, 452, 4096]) != `negative_prompt_embeds` torch.Size([1, 226, 4096]).",
      "body": "Error occurred when executing CogVideoSampler:\r\n\r\n`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but got: `prompt_embeds` torch.Size([1, 452, 4096]) != `negative_prompt_embeds` torch.Size([1, 226, 4096]).\r\n\r\nFile \"/data/ComfyUI/execution.py\", line 317, in execute\r\noutput_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\nFile \"/data/ComfyUI/execution.py\", line 192, in get_output_data\r\nreturn_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\nFile \"/data/ComfyUI/execution.py\", line 169, in _map_node_over_list\r\nprocess_inputs(input_dict, i)\r\nFile \"/data/ComfyUI/execution.py\", line 158, in process_inputs\r\nresults.append(getattr(obj, func)(**inputs))\r\nFile \"/data/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/nodes.py\", line 289, in process\r\nlatents = pipeline[\"pipe\"](\r\nFile \"/etc/anaconda3/envs/comfyui/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nreturn func(*args, **kwargs)\r\nFile \"/data/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/pipeline_cogvideox.py\", line 391, in __call__\r\nself.check_inputs(\r\nFile \"/data/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/pipeline_cogvideox.py\", line 225, in check_inputs"
    },
    "satisfaction_conditions": [
      "System must provide clear feedback about token/dimension mismatches"
    ],
    "created_at": "2024-08-30T09:52:27Z"
  },
  {
    "id": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/9",
    "source": {
      "issue_number": 9
    },
    "initial_question": {
      "title": "Results from this are different to official CogVideoX demo and I can't figure out how to match it.",
      "body": "Using the default settings provided with the example workflow, with 16 frames at 8 fps, it only renders a two second video (as expected). But by default, CogVideoX does 6 second clips. However, upping the amount of frames to 48 does not have the intended effect and instead generates three disjointed clips, like three 2 second clips stuck together. \r\n\r\nHow do I get a single 6 second video like it's supposed to do?"
    },
    "satisfaction_conditions": [
      "Video output must be a single continuous clip without disjointed segments",
      "Frame count and temporal parameters must be properly synchronized"
    ],
    "created_at": "2024-08-10T07:37:18Z"
  }
]