[
  {
    "number": 88,
    "title": "Missing wheels on pypi",
    "created_at": "2025-02-17T10:04:41Z",
    "closed_at": "2025-02-18T05:22:09Z",
    "labels": [],
    "url": "https://github.com/tile-ai/tilelang/issues/88",
    "body": "Hi,\n\nI noticed you recently added pre-built wheels for Python versions other than 3.10, but these are missing on pypi - I can install the wheels in the github release just fine, but having them on pypi would be nice.",
    "comments_url": "https://api.github.com/repos/tile-ai/tilelang/issues/88/comments",
    "author": "falkaer",
    "comments": [
      {
        "user": "LeiWang1999",
        "created_at": "2025-02-17T10:51:39Z",
        "body": "@falkaer Thanks for your report, sorry for the trouble."
      },
      {
        "user": "LeiWang1999",
        "created_at": "2025-02-18T05:21:58Z",
        "body": "@falkaer uploaded :)"
      },
      {
        "user": "falkaer",
        "created_at": "2025-02-18T07:32:48Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 40,
    "title": "Layout infer error when intermediate buffers are only assigned value",
    "created_at": "2025-01-24T11:01:52Z",
    "closed_at": "2025-01-25T17:19:12Z",
    "labels": [],
    "url": "https://github.com/tile-ai/tilelang/issues/40",
    "body": "When assign value for `acc_o`, `logsum` or `scores_max` (Any buffer created)\n                \nwill get error:\n>  File \"/root/TileLang/src/transform/layout_inference.cc\", line 274\nInternalError: Check failed: layout_map.count(buffer) != 0 (0 vs. 0) : The layout for fragment scores_max can not be inferred correctly.\n\nThe problem occurs when doing:\n```\nmod = tl.transform.LayoutInference()(mod)\n```\n\nHowever, if we uncomment the code below the value assignment, where the buffer will be used, it can infer correctly.\n\nCode:\n```\n# type: ignore\n\nimport torch\nimport torch.nn.functional as F\nimport tilelang\nfrom tilelang import Profiler\nfrom tilelang.autotuner import *\nimport tilelang.language as T\nimport itertools\nimport argparse\nfrom functools import partial\n\ndef flashdecoding(batch, heads, seqlen_q, seqlen_kv, dim, is_casual, num_split, tune=False):\n    scale = (1.0 / dim) ** 0.5 * 1.44269504  # log2(e)\n    shape_q    = [batch, seqlen_q, heads, dim]\n    shape_kv   = [batch, seqlen_kv, heads, dim]\n    part_shape = [batch, seqlen_q, heads, num_split, dim]\n    dtype      = \"float16\"\n    accum_dtype = \"float\"\n\n    def kernel_func(block_M, block_N):\n        \n        @T.macro\n        def MMA0(\n            K: T.Buffer(shape_kv, dtype),\n            Q_shared: T.Buffer([block_M, dim], dtype),\n            K_shared: T.Buffer([block_N, dim], dtype),\n            acc_s: T.Buffer([block_M, block_N], accum_dtype),\n            k: T.int32,\n            bx: T.int32,\n            by: T.int32,\n            bz: T.int32,\n        ):\n            T.copy(K[bz, k * block_N:(k + 1) * block_N, by, :], K_shared)\n            if is_casual:\n                for i, j in T.Parallel(block_M, block_N):\n                    acc_s[i, j] = T.if_then_else(bx * block_M + i >= k * block_N + j, 0,\n                                                 -T.infinity(acc_s.dtype))\n            else:\n                T.clear(acc_s)\n            T.gemm(Q_shared, K_shared, acc_s, transpose_B=True, policy=T.GemmWarpPolicy.FullRow)\n\n        @T.macro\n        def MMA1(\n                V: T.Buffer(shape_kv, dtype),\n                V_shared: T.Buffer([block_M, dim], dtype),\n                acc_s_cast: T.Buffer([block_M, block_N], dtype),\n                acc_o: T.Buffer([block_M, dim], accum_dtype),\n                k: T.int32,\n                by: T.int32,\n                bz: T.int32,\n        ):\n            T.copy(V[bz, k * block_N:(k + 1) * block_N, by, :], V_shared)\n            T.gemm(acc_s_cast, V_shared, acc_o, policy=T.GemmWarpPolicy.FullRow)\n\n        @T.macro\n        def Softmax(\n                acc_s: T.Buffer([block_M, block_N], accum_dtype),\n                acc_s_cast: T.Buffer([block_M, block_N], dtype),\n                scores_max: T.Buffer([block_M], accum_dtype),\n                scores_max_prev: T.Buffer([block_M], accum_dtype),\n                scores_scale: T.Buffer([block_M], accum_dtype),\n                scores_sum: T.Buffer([block_M], accum_dtype),\n                logsum: T.Buffer([block_M], accum_dtype),\n        ):\n            T.copy(scores_max, scores_max_prev)\n            T.fill(scores_max, -T.infinity(accum_dtype))\n            T.reduce_max(acc_s, scores_max, dim=1, clear=False)\n\n            for i in T.Parallel(block_M):\n                scores_scale[i] = T.exp2(scores_max_prev[i] * scale - scores_max[i] * scale)\n            for i, j in T.Parallel(block_M, block_N):\n                acc_s[i, j] = T.exp2(acc_s[i, j] * scale - scores_max[i] * scale)\n                \n            T.reduce_sum(acc_s, scores_sum, dim=1)\n            for i in T.Parallel(block_M):\n                logsum[i] = logsum[i] * scores_scale[i] + scores_sum[i]\n            T.copy(acc_s, acc_s_cast)\n\n        @T.macro\n        def Rescale(\n                acc_o: T.Buffer([block_M, dim], accum_dtype),\n                scores_scale: T.Buffer([block_M], accum_dtype),\n        ):\n            for i, j in T.Parallel(block_M, dim):\n                acc_o[i, j] *= scores_scale[i]\n\n        @T.macro\n        def flash_attn_split(\n            Q: T.Buffer(shape_q, dtype),\n            K: T.Buffer(shape_kv, dtype),\n            V: T.Buffer(shape_kv, dtype),\n            Output: T.Buffer(shape_q, dtype),\n        ):\n            with T.Kernel(T.ceildiv(seqlen_q, block_M), heads * batch, num_split, threads=128 * 2) as (bx, by, bz):\n                Q_shared = T.alloc_shared([block_M, dim], dtype)\n                K_shared = T.alloc_shared([block_N, dim], dtype)\n                V_shared = T.alloc_shared([block_N, dim], dtype)\n                O_shared = T.alloc_shared([block_M, dim], dtype)\n                acc_s = T.alloc_fragment([block_M, block_N], accum_dtype)\n                acc_s_cast = T.alloc_fragment([block_M, block_N], dtype)\n                acc_o = T.alloc_fragment([block_M, dim], accum_dtype)\n                scores_max = T.alloc_fragment([block_M], accum_dtype)\n                scores_max_prev = T.alloc_fragment([block_M], accum_dtype)\n                scores_scale = T.alloc_fragment([block_M], accum_dtype)\n                scores_sum = T.alloc_fragment([block_M], accum_dtype)\n                logsum = T.alloc_fragment([block_M], accum_dtype)\n                \n                mid = bx\n                hid = by % heads\n                bid = by // heads\n                sid = bz\n\n                T.annotate_layout({Q_shared: tl.layout.make_swizzled_layout(Q_shared)})\n                T.copy(Q[bid, 0, hid, :], Q_shared[0, :])\n                T.fill(acc_o, 0)\n                T.fill(logsum, 0)\n                T.fill(scores_max, -T.infinity(accum_dtype))\n\n                # loop_range = (\n                #     T.min(T.ceildiv(seqlen_kv, block_N), T.ceildiv((mid + 1) * block_M, block_N)) \n                #     if is_casual else T.ceildiv((seqlen_kv // num_split), block_N)\n                # )\n\n                # for k in T.Pipelined(loop_range, num_stages=2):\n                #     MMA0(K, Q_shared, K_shared, acc_s, k, bx, by, bz)\n                #     Softmax(acc_s, acc_s_cast, scores_max, scores_max_prev, scores_scale,\n                #             scores_sum, logsum)\n                #     Rescale(acc_o, scores_scale)\n                #     MMA1(V, V_shared, acc_s_cast, acc_o, k, by, bz)\n                # for i, j in T.Parallel(block_M, dim):\n                #     acc_o[i, j] /= logsum[i]\n                # T.copy(acc_o, O_shared)\n                # T.copy(O_shared, Output[bz, bx * block_M:(bx + 1) * block_M, by, :])\n\n        @T.prim_func\n        def main(\n                Q: T.Buffer(shape_q, dtype),\n                K: T.Buffer(shape_kv, dtype),\n                V: T.Buffer(shape_kv, dtype),\n                Output: T.Buffer(shape_q, dtype),\n        ):\n            flash_attn_split(Q, K, V, Output)\n\n        return main\n\n    def kernel(block_M, block_N):\n        return kernel_func(block_M, block_N)\n\n    return kernel\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--batch', type=int, default=1, help='batch size')\n    parser.add_argument('--heads', type=int, default=32, help='heads')\n    parser.add_argument('--seqlen_kv', type=int, default=4096, help='sequence length')\n    parser.add_argument('--dim', type=int, default=128, help='dim')\n    parser.add_argument('--is_casual', action='store_true', help='causal')\n    parser.add_argument('--tune', action='store_true', help='tune configs')\n    args = parser.parse_args()\n\n    batch, heads, seqlen_kv, dim, is_casual = args.batch, args.heads, args.seqlen_kv, args.dim, args.is_casual\n    seqlen_q   = 1\n    num_splits = 4\n\n    program = flashdecoding(\n                batch, heads, seqlen_q, seqlen_kv, dim, is_casual, num_splits, tune=args.tune)(\n                block_M=128, block_N=128)\n    jit_kernel = tilelang.JITKernel(program, out_idx=[3], target=\"cuda\")\n\n    q = torch.randn(batch, seqlen_q, heads, dim, dtype=torch.float16, device='cuda')\n    k = torch.randn(batch, seqlen_kv, heads, dim, dtype=torch.float16, device='cuda')\n    v = torch.randn(batch, seqlen_kv, heads, dim, dtype=torch.float16, device='cuda')\n\n    out_flash = jit_kernel(q, k, v)\n\n```\n\n",
    "comments_url": "https://api.github.com/repos/tile-ai/tilelang/issues/40/comments",
    "author": "DD-DuDa",
    "comments": [
      {
        "user": "LeiWang1999",
        "created_at": "2025-01-25T12:11:27Z",
        "body": "cc @chengyupku "
      },
      {
        "user": "chengyupku",
        "created_at": "2025-01-25T15:17:45Z",
        "body": "Hi Dayou, during the lowering process of tilelang, it is necessary to know how each fragment is assigned to all threads (thread binding). Therefore, if a fragment is only declared but not used, it is impossible to determine the binding. In such cases, you can manually add the thread binding using the `T.annotate_layout` primitive. \nFor example: \n```\nT.annotate_layout({scores_max: T.Fragment(scores_max.shape, forward_thread_fn=lambda i: i)})\n```"
      },
      {
        "user": "DD-DuDa",
        "created_at": "2025-01-25T15:19:37Z",
        "body": "I see! Thanks!"
      }
    ]
  }
]