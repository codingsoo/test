[
  {
    "number": 77,
    "title": "OOM on 48G gpu",
    "created_at": "2025-01-16T00:16:03Z",
    "closed_at": "2025-01-17T01:27:48Z",
    "labels": [],
    "url": "https://github.com/Yuanshi9815/OminiControl/issues/77",
    "body": "I'm currently trying to train it using the a6000 ada gpus, I set the strategy to FSDP but it still reports OOM\r\nDo we have to use 80gb memory for training?",
    "comments_url": "https://api.github.com/repos/Yuanshi9815/OminiControl/issues/77/comments",
    "author": "PeterYYZhang",
    "comments": [
      {
        "user": "Yuanshi9815",
        "created_at": "2025-01-16T07:05:55Z",
        "body": "Hi @PeterYYZhang,\r\n\r\nJust FYI - you should be able to run the 512x512 config on 48GB GPUs by enabling `gradient_checkpointing: true ` in your config.\r\n\r\n:D"
      },
      {
        "user": "PeterYYZhang",
        "created_at": "2025-01-17T01:27:43Z",
        "body": "Thanks @Yuanshi9815! Now it works!!"
      },
      {
        "user": "rishabh063",
        "created_at": "2025-01-28T15:01:15Z",
        "body": "hey i am getting oom on 48gb vram even when gradient checkpoint is true"
      }
    ]
  },
  {
    "number": 53,
    "title": "LoRA scale=0.0 for non-Conditional tokens",
    "created_at": "2024-12-21T11:08:16Z",
    "closed_at": "2024-12-27T12:20:03Z",
    "labels": [],
    "url": "https://github.com/Yuanshi9815/OminiControl/issues/53",
    "body": "Hello @Yuanshi9815 !\r\nGreat work!\r\n\r\nI have a question regarding setting lora weight for non-conditional tokens to 0.0\r\nSee #3 \r\nCan you please mention where you do it in the code? In which module?\r\n\r\nKind regards,\r\nIgor ",
    "comments_url": "https://api.github.com/repos/Yuanshi9815/OminiControl/issues/53/comments",
    "author": "reallyigor",
    "comments": [
      {
        "user": "Yuanshi9815",
        "created_at": "2024-12-23T05:54:49Z",
        "body": "Hiii @reallyigor ,\r\n\r\nThanks for your attention. \r\n\r\nYou may find it in the file `src/lora_controller.py`. \ud83d\ude4c"
      },
      {
        "user": "reallyigor",
        "created_at": "2024-12-23T10:57:48Z",
        "body": "Thank you for such a swift answer!\r\ncan you please tell me the exact row?\r\n\r\ne.g. `set_lora_scale` function is not used in the code at all"
      },
      {
        "user": "Yuanshi9815",
        "created_at": "2024-12-25T04:06:39Z",
        "body": "Hi @reallyigor ,\r\n\r\nMerry Christmas! \ud83c\udf84\r\n\r\nThe LoRA weight control is implemented through the `enable_lora` context manager. For example, in `src/transformer.py ` line 98:\r\n  ```python\r\n  with enable_lora((self.x_embedder,), model_config.get(\"latent_lora\", False)):\r\n      hidden_states = self.x_embedder(hidden_states)\r\n  ```\r\nWhen`latent_lora` is `False`, the LoRA weights are set to 0 for the `x_embedder` module."
      },
      {
        "user": "reallyigor",
        "created_at": "2024-12-25T08:18:28Z",
        "body": "Thank you very much, @Yuanshi9815 !\r\n\ud83c\udf84 Merry Christmas! \ud83c\udf84\r\nHow this `model_config` controls conditional/non-conditional tokens? it enables/disables lora for the whole model, doesnt it? "
      },
      {
        "user": "Yuanshi9815",
        "created_at": "2024-12-27T04:45:38Z",
        "body": "Hi @reallyigor ,\r\n\r\nWe dynamically enable/disable LoRA during model execution. The `model_config` doesn't determine conditional/non-conditional tokens - it only controls whether to use LoRA on non-conditional tokens, which should match the training configuration. \r\n\r\nIn our released models, LoRA is used on conditional tokens but not on non-conditional tokens, hence the default False setting.\r\n\r\nXD"
      },
      {
        "user": "reallyigor",
        "created_at": "2024-12-27T12:20:03Z",
        "body": "Thank you! "
      }
    ]
  },
  {
    "number": 4,
    "title": "What's the actual name?",
    "created_at": "2024-11-25T09:13:58Z",
    "closed_at": "2024-11-25T09:37:12Z",
    "labels": [],
    "url": "https://github.com/Yuanshi9815/OminiControl/issues/4",
    "body": "Paper says \"Omini\" but the conclusion says \"omni\", twice. The Readme of this repo says \"Omini\", except the \"Features\" part refers to it as \"Omni\".\r\nWas the project called OmniControl originally and then switched to \"OminiControl\"? Does Omini has a meaning (I can't find any)?",
    "comments_url": "https://api.github.com/repos/Yuanshi9815/OminiControl/issues/4/comments",
    "author": "Qu3tzal",
    "comments": [
      {
        "user": "Yuanshi9815",
        "created_at": "2024-11-25T09:22:03Z",
        "body": "Thank you for pointing that out. The name is indeed \"OminiControl,\" not \"OmniControl.\" We will correct it in the paper. \r\n\r\n\"Omini\" combines \"omni\" and \"mini\" to reflect the strengths of our framework. It\u2019s a unified and versatile image-guided generation framework that achieves great results while requiring only minimal additional parameters and network modifications."
      },
      {
        "user": "Qu3tzal",
        "created_at": "2024-11-25T09:37:12Z",
        "body": "Understood, thank you!"
      }
    ]
  }
]