[
  {
    "number": 93,
    "title": "Semantic Similarity does not work - got an unexpected keyword argument 'similarity_threshold'",
    "created_at": "2024-12-15T11:53:26Z",
    "closed_at": "2024-12-23T17:35:17Z",
    "labels": [
      "bug",
      "documentation",
      "in progress"
    ],
    "url": "https://github.com/chonkie-ai/chonkie/issues/93",
    "body": "Semantic Similarity does not work\r\n**Error**\r\n```\r\nchunker = SemanticChunker(\r\nTypeError: __init__() got an unexpected keyword argument 'similarity_threshold'\r\n```\r\n\r\n**To Reproduce**\r\n```\r\nfrom chonkie import SemanticChunker\r\nfrom chonkie import SentenceTransformerEmbeddings\r\n\r\nembeddings = SentenceTransformerEmbeddings(\"all-MiniLM-L6-v2\") #all-mpnet-base-v2\"\r\n\r\nprint(embeddings)\r\n\r\na = embeddings.embed(\"Hi\")\r\nprint(a) # prints a embedding\r\n\r\nchunker = SemanticChunker(\r\n    embedding_model='all-MiniLM-L6-v2',\r\n    similarity_threshold=0.5,\r\n    chunk_size=512\r\n)\r\ntext = \"\"\"The neural network processes input data through layers.\r\nTraining data is essential for model performance.\r\nGPUs accelerate neural network computations significantly.\r\nQuality training data improves model accuracy.\r\nTPUs provide specialized hardware for deep learning.\r\nData preprocessing is a crucial step in training.\"\"\"\r\n\r\nchunks = chunker.chunk(text)\r\n\r\nfor chunk in chunks:\r\n    print(f\"Chunk text: {chunk.text}\")\r\n    print(f\"Token count: {chunk.token_count}\")\r\n    print(f\"Number of sentences: {len(chunk.sentences)}\")\r\n```\r\nIt prints the embedding, just errors when initializing the semantic chunker class.\r\n**Output**\r\n```\r\nSentenceTransformerEmbeddings(model=all-MiniLM-L6-v2)\r\n[-9.04762074e-02  4.04395871e-02  2.39057038e-02  5.89479916e-02\r\n -2.28823498e-02 -4.72201295e-02  4.50476483e-02  1.57863833e-02\r\n -4.81994823e-02 -3.77941355e-02 -1.90776791e-02  2.13088039e-02\r\n -4.68304241e-03 -4.33081537e-02  5.99147603e-02  5.91033921e-02\r\n -2.80367117e-02 -5.92184067e-02 -1.24403164e-01 -3.55999470e-02\r\n -6.08053710e-03  3.24290805e-02 -3.78007963e-02  2.47109272e-02\r\n -4.27244082e-02 -4.24538366e-02  4.59356494e-02  9.86255109e-02\r\n -4.99980375e-02 -3.52358408e-02  7.08397478e-02  3.31632011e-02\r\n  2.65883375e-02  1.73212480e-04  3.88164306e-03  3.04672271e-02\r\n -7.82026500e-02 -1.20379530e-01  1.80415660e-02  2.28290427e-02\r\n -1.77501398e-03 -2.34498996e-02  3.05816252e-03  2.43557133e-02\r\n  4.41539772e-02 -4.01097350e-02  2.01923326e-02  1.08881649e-02\r\n  2.87315268e-02  1.23677272e-02 -9.13190767e-02 -6.81244433e-02\r\n  6.19151257e-03 -1.25605203e-02  9.28248987e-02  2.79071033e-02\r\n -3.12197749e-02 -2.52352040e-02  7.84362331e-02 -7.33027086e-02\r\n -6.69823512e-02  1.39002353e-02 -1.42814413e-01  8.77210591e-03\r\n  2.07010098e-02  9.07208887e-05 -5.91357350e-02 -6.52026758e-02\r\n -3.80246937e-02 -6.19724393e-02 -2.50714133e-03 -4.24507726e-03\r\n -4.13620546e-02 -4.95713837e-02  2.24600956e-02 -3.56280915e-02\r\n  4.03861962e-02  4.88409922e-02  5.20196520e-02  3.16421390e-02\r\n  3.02730724e-02 -3.80800143e-02 -1.65185332e-02 -6.83415355e-03\r\n -8.96493811e-03 -3.80861685e-02  2.37352401e-02 -8.56116693e-03\r\n -5.12796491e-02  1.02583468e-02 -1.06715269e-01  5.38776256e-02\r\n  3.03738285e-02 -3.54465619e-02 -7.69298300e-02 -6.11885712e-02\r\n  7.78368711e-02  1.08476519e-03 -1.23568341e-01  2.79173732e-01\r\n  4.79933247e-02  5.21415398e-02  4.21877131e-02  1.02176912e-01\r\n -2.11793073e-02  5.31466976e-02 -5.25944047e-02  7.74884447e-02\r\n -5.98228956e-03  2.23340485e-02  2.56445501e-02 -1.77126937e-03\r\n -2.79253628e-02 -1.91525444e-02  5.49053364e-02  7.07536936e-02\r\n -3.37207830e-03  7.49428431e-03  2.57694367e-02 -7.10282251e-02\r\n -2.47852914e-02 -5.38527109e-02  3.37568410e-02 -4.50824425e-02\r\n -1.98343415e-02 -2.40076575e-02  3.07824998e-03 -4.39805003e-33\r\n  7.13740960e-02 -2.46291980e-02  4.34765853e-02  8.48252997e-02\r\n -4.63567004e-02 -3.41022797e-02 -2.29421165e-02 -4.99801710e-02\r\n  2.06044386e-03  5.18267974e-03  8.05836357e-03  5.91129763e-03\r\n -4.03364375e-02 -2.34233146e-03 -2.57336590e-02  3.13819796e-02\r\n  4.91707362e-02  5.95755540e-02  3.62815298e-02  3.59637477e-02\r\n -7.95034245e-02 -3.18895243e-02  2.03541014e-02  5.40887266e-02\r\n  3.40220742e-02 -2.66799964e-02  5.13579790e-03 -1.37623310e-01\r\n  5.35681173e-02  6.24607988e-02  4.66554910e-02  8.25161114e-03\r\n -1.12271623e-03  1.58642866e-02 -1.48686823e-02 -1.06545649e-02\r\n  1.39251528e-02 -5.11116050e-02 -5.11570461e-02  1.81104448e-02\r\n -2.60318164e-02  4.10491973e-02  6.18396588e-02 -4.09641080e-02\r\n  2.03002878e-02  4.35215682e-02  1.52085321e-02  2.21398696e-02\r\n  5.85440174e-03  3.38419490e-02 -5.83182760e-02  4.04238282e-03\r\n -1.38970748e-01  4.21360228e-03 -5.61999716e-03 -4.35496978e-02\r\n -5.90716023e-03 -7.45447353e-02  8.35962221e-02  4.02504168e-02\r\n  3.14086713e-02  7.68125802e-02 -2.74488702e-02  1.83432046e-02\r\n -1.53332144e-01 -3.48912850e-02  4.94039617e-02 -3.46934460e-02\r\n  1.14418186e-01 -2.06293724e-02 -4.37950604e-02 -1.24856578e-02\r\n  7.63989612e-03  4.81630154e-02 -3.52325886e-02  3.67844477e-02\r\n  6.14347272e-02  1.70743000e-02  1.93933006e-02 -1.52372774e-02\r\n -4.23128437e-03  4.59325276e-02 -8.32644291e-03  1.58503801e-02\r\n  5.95141463e-02 -9.18054860e-03 -1.49881067e-02 -6.76251948e-02\r\n -7.32812434e-02 -3.96396481e-02 -7.59298950e-02  2.27835216e-02\r\n  8.00924152e-02 -2.19015963e-02  1.66906882e-02  3.85790707e-33\r\n  9.64368805e-02  6.16989620e-02 -5.79451546e-02 -1.41535383e-02\r\n -1.89403780e-02 -1.41951339e-02 -2.27934029e-03  8.97949338e-02\r\n -8.37799758e-02 -2.26818696e-02  6.83509111e-02 -3.02902162e-02\r\n  6.81150332e-02  1.75223202e-02  4.47526723e-02  2.56442390e-02\r\n  9.22513977e-02  4.75600474e-02 -7.28711560e-02  4.11393214e-03\r\n -3.27286161e-02 -3.42148729e-02 -9.29743350e-02 -6.22671954e-02\r\n -7.87806790e-03  5.04693948e-03  1.51909823e-02  6.85868636e-02\r\n -5.93004487e-02 -2.59447042e-02  7.01026097e-02 -9.64850094e-03\r\n  1.14026219e-02  5.20743877e-02  1.79106928e-03  1.04113437e-01\r\n  1.21684037e-02 -7.34238848e-02  3.34012397e-02 -9.78091061e-02\r\n -4.58965227e-02  2.78538559e-02 -1.98339615e-02  9.23914686e-02\r\n -1.23828566e-02 -3.98118272e-02  5.08529507e-03  4.14596274e-02\r\n -8.42346847e-02  1.00746350e-02 -8.32033977e-02 -2.83304099e-02\r\n  4.24229316e-02  4.08155238e-03 -4.56181802e-02  5.45455925e-02\r\n  1.30894929e-02  5.93257509e-02  3.16439569e-02  1.33298235e-02\r\n  1.34292301e-02  5.42032495e-02  8.32033996e-03  8.84347260e-02\r\n  2.47976631e-02  3.48344892e-02 -1.57302599e-02 -9.23916139e-03\r\n -3.04034073e-02 -5.37000932e-02  4.26000878e-02 -9.55903251e-03\r\n  3.53703499e-02  2.62070466e-02 -1.59974173e-02 -2.27997378e-02\r\n  1.13233551e-02  3.48057449e-02 -1.15139170e-04  6.14551120e-02\r\n  2.60663908e-02 -3.49001703e-03 -2.68242657e-02  4.36767377e-02\r\n  1.96862146e-02  1.42173460e-02  4.13494073e-02  3.12109925e-02\r\n -2.94173625e-03 -5.59458956e-02  7.75543228e-03  7.96629488e-02\r\n  7.01818839e-02 -5.30647561e-02  8.65287241e-03 -1.44800048e-08\r\n  2.99871266e-02 -3.74180116e-02  6.35029897e-02  9.18339044e-02\r\n  5.34940287e-02  5.86865991e-02 -2.92609241e-02 -1.45442439e-02\r\n -3.18563916e-02  4.17883061e-02  5.59052750e-02  3.48980539e-02\r\n -3.88737209e-02 -5.25139980e-02  5.35428450e-02 -2.30162870e-02\r\n -2.96264552e-02  4.33764495e-02 -5.71561642e-02 -1.37616053e-01\r\n  3.65539044e-02  2.52274647e-02  2.29083188e-03 -3.21153738e-02\r\n  7.02529401e-03 -6.36366978e-02 -3.57009172e-02 -1.72419497e-03\r\n  1.16311871e-02 -6.13933019e-02 -1.34474589e-02  1.83892518e-01\r\n -7.69876363e-03 -1.57939177e-02  2.97547020e-02 -2.22598761e-02\r\n -9.80211049e-03 -1.19002592e-02  6.23303056e-02 -2.51287296e-02\r\n -4.21328209e-02 -5.61932549e-02 -4.12699766e-02 -3.24218832e-02\r\n -1.57817528e-02  4.64272611e-02  1.15918054e-03 -5.41495644e-02\r\n  3.64914089e-02 -6.65330216e-02 -4.34342101e-02 -2.28455123e-02\r\n  5.47489263e-02  7.31127188e-02  3.00164483e-02  6.17795885e-02\r\n  1.61923058e-02  1.22879585e-02 -5.36394771e-03 -8.56415648e-03\r\n  1.50863796e-01  6.81504086e-02  1.78656653e-02  1.12762554e-02]\r\nTraceback (most recent call last):\r\n  File \"D:\\.......\\semantic.py\", line 27, in <module>\r\n    chunker = SemanticChunker(\r\nTypeError: __init__() got an unexpected keyword argument 'similarity_threshold'\r\n```\r\n**Additional context**\r\nsentence transformer version - 2.3.0\r\nchonkie version - 0.2.2\r\npython version - 3.9.13\r\n\r\nNot sure if I am doing something silly, but I have been at it for a while changing versions of packages etc but I just can't seem to work.\r\n",
    "comments_url": "https://api.github.com/repos/chonkie-ai/chonkie/issues/93/comments",
    "author": "armsp",
    "comments": [
      {
        "user": "bhavnicksm",
        "created_at": "2024-12-15T11:56:12Z",
        "body": "sorryyyyyy~! The documentation is lagging behind a lot! \n\ncould you try changing `similarity_threshold` to `threshold`? That should work...\n\nThanks \ud83d\ude03"
      },
      {
        "user": "armsp",
        "created_at": "2024-12-15T11:59:47Z",
        "body": "I had tried that as well. But I get the error at the line `chunks = chunker.chunk(text)` -\r\n```\r\n  1.61923058e-02  1.22879585e-02 -5.36394771e-03 -8.56415648e-03\r\n  1.50863796e-01  6.81504086e-02  1.78656653e-02  1.12762554e-02]\r\nTraceback (most recent call last):\r\n  File \"D:\\......semantic.py\", line 39, in <module>\r\n    chunks = chunker.chunk(text)\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\chunker\\semantic.py\", line 516, in chunk\r\n    sentence_groups = self._group_sentences(sentences)\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\chunker\\semantic.py\", line 424, in _group_sentences\r\n    return self._group_sentences_window(sentences)\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\chunker\\semantic.py\", line 411, in _group_sentences_window\r\n    similarities = self._compute_pairwise_similarities(sentences)\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\chunker\\semantic.py\", line 256, in _compute_pairwise_similarities\r\n    return [\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\chunker\\semantic.py\", line 257, in <listcomp>\r\n    self._get_semantic_similarity(\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\chunker\\semantic.py\", line 243, in _get_semantic_similarity\r\n    similarity = self.embedding_model.similarity(embedding1, embedding2)\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\chonkie\\embeddings\\sentence_transformer.py\", line 77, in similarity\r\n    return self.model.similarity(u, v).item()\r\n  File \"C:\\Users\\joe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1931, in __getattr__\r\n    raise AttributeError(\r\nAttributeError: 'SentenceTransformer' object has no attribute 'similarity'\r\n```\r\nIt seems like its got something to do with other packages...most probably Sentence Transformer"
      },
      {
        "user": "armsp",
        "created_at": "2024-12-15T14:53:05Z",
        "body": "Sentence Transformers 3.0.0 works (i think thats the least version of sentence transformers that will work with chonkie). And I think Numpy should be less than 2.0.0\r\n\r\nFeel free to close this issue!"
      },
      {
        "user": "shreyashnigam",
        "created_at": "2024-12-23T17:35:17Z",
        "body": "Fixed the docs with the latest accurate information. Thanks for flagging this! "
      }
    ]
  },
  {
    "number": 83,
    "title": "[BUG] Semantic Chunks are to Big",
    "created_at": "2024-12-09T09:23:39Z",
    "closed_at": "2024-12-16T16:45:52Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/chonkie-ai/chonkie/issues/83",
    "body": "**Describe the bug**\r\nEven though chunk size is set 512 semantic chunking is returning one big chunk for my text with 3263 tokens\r\n\r\n\r\n**To Reproduce**\r\n```python\r\nfrom chonkie import SemanticChunker\r\n\r\n# Basic initialization with default parameters\r\nchunker = SemanticChunker(\r\n    embedding_model=\"minishlab/potion-base-8M\",  # Default model\r\n    threshold=0.6,                   # Similarity threshold (0-1)\r\n    chunk_size=512,                             # Maximum tokens per chunk\r\n    # initial_sentences=1                         # Initial sentences per chunk\r\n)\r\n\r\ntext = re.sub(r'[\\r\\n]+', ' ', text)\r\ntext = re.sub(r'[^A-Za-z0-9 ]+', '', text)\r\nchunks = chunker.chunk(text)\r\n\r\nfor chunk in chunks:\r\n    print(f\"Chunk text: {chunk.text}\")\r\n    print(f\"Token count: {chunk.token_count}\")\r\n    print(f\"Number of sentences: {len(chunk.sentences)}\")\r\n\r\n-- Output\r\n\r\nToken count: 3263\r\nNumber of sentences: 1\r\n```\r\n\r\n**Expected behavior**\r\nI expect to have least 7 chunks for the input text clustered by semantic similarity \r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/chonkie-ai/chonkie/issues/83/comments",
    "author": "aribornstein",
    "comments": [
      {
        "user": "bhavnicksm",
        "created_at": "2024-12-09T13:20:52Z",
        "body": "Hey @aribornstein!\r\n\r\nThanks for opening the issue! \ud83d\ude0a\r\n\r\nI'll try to replicate the issue today. If you could pass me a colab notebook which replicates it, I would be able to create a fix for it sooner. If not, even a sample text piece to reproduce the issue would be helpful!\r\n\r\nWill update on this asap! Thanks! \u263a\ufe0f"
      },
      {
        "user": "bhavnicksm",
        "created_at": "2024-12-12T17:17:51Z",
        "body": "Hey @aribornstein!\r\n\r\nThis example is very clever! \ud83d\ude02\r\n\r\nI just noticed you removed all sentence markers, which means it treats the entire passage as a single sentence, and since we have `min_sentences` set to 1 by default, it would by-pass the `chunk_size` argument since `min_sentences` takes priority here.\r\n\r\nIf you have text without sentence delimiters like this, this would fail every SentenceChunker or SemanticChunker out there, I would suggest using `TokenChunker` or `WordChunker` as an alternative. \r\n\r\nWould love to hear if you have an alternate solution for this or feedback :)\r\n\r\nThanks! \ud83d\ude0a"
      },
      {
        "user": "bhavnicksm",
        "created_at": "2024-12-16T16:45:52Z",
        "body": "Closing issue because it's expected behavior; please reopen the issue if there is any feedback or mods required.\r\n\r\nThanks! \ud83d\ude0a"
      },
      {
        "user": "aribornstein",
        "created_at": "2024-12-22T12:11:38Z",
        "body": "This makes sense. I didn't realize that chonky relies on delimiters to segment text.  I would consider seeing if there is a way to use a light weight model to segment coherent text if the used doesn't provide sentences. Let me think about this as well."
      }
    ]
  },
  {
    "number": 33,
    "title": "[BUG] TypeError: dataclass() got an unexpected keyword argument 'slots'",
    "created_at": "2024-11-18T05:12:55Z",
    "closed_at": "2024-11-18T07:16:17Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/chonkie-ai/chonkie/issues/33",
    "body": "**Describe the bug**\r\nWhen I do a `from chonkie import TokenChunker` I get the error\r\n\r\n**To Reproduce**\r\nRun `from chonkie import TokenChunker` after a fresh install\r\n\r\n**Expected behavior**\r\nThe TokenChunker package should be imported without errors\r\n\r\n**Screenshots**\r\n```\r\nDownloading chonkie-0.2.0-py3-none-any.whl (23 kB)\r\nDownloading autotiktokenizer-0.2.1-py3-none-any.whl (8.9 kB)\r\nInstalling collected packages: autotiktokenizer, chonkie\r\nSuccessfully installed autotiktokenizer-0.2.1 chonkie-0.2.0\r\n(.llm_vnev) mis@mispl-lap-163:~ $ python3\r\nPython 3.8.10 (default, Sep 11 2024, 16:02:53) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from chonkie import TokenChunker\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mis/Documents/docxtract/llm/.llm_vnev/lib/python3.8/site-packages/chonkie/__init__.py\", line 1, in <module>\r\n    from .chunker import (BaseChunker, Chunk, SDPMChunker, SemanticChunk,\r\n  File \"/home/mis/Documents/docxtract/llm/.llm_vnev/lib/python3.8/site-packages/chonkie/chunker/__init__.py\", line 1, in <module>\r\n    from .base import BaseChunker, Chunk\r\n  File \"/home/mis/Documents/docxtract/llm/.llm_vnev/lib/python3.8/site-packages/chonkie/chunker/base.py\", line 9, in <module>\r\n    @dataclass(slots=True)\r\nTypeError: dataclass() got an unexpected keyword argument 'slots'\r\n```\r\n\r\n**Additional context**\r\nDid a fresh install using `pip install chonkie` and tried to run the tool based on the given guide.\r\n\r\nAnything that I did is wrong?\r\n",
    "comments_url": "https://api.github.com/repos/chonkie-ai/chonkie/issues/33/comments",
    "author": "AgentT30",
    "comments": [
      {
        "user": "bhavnicksm",
        "created_at": "2024-11-18T06:44:39Z",
        "body": "Hey @AgentT30!\r\n\r\nThanks for raising an issue \ud83d\ude0a\r\n\r\nThe issue is with Python slots, which were introduced first in python3.10 as the parameter that I am using inside the `dataclass`. Before that, it was `__slots__` for versions 3.8 and 3.9, which needs to be added in the codebase.\r\n\r\nIt seems like it got missed as an error. As a workaround for now, could you try it with Python 3.10? \r\n\r\nI'll add a backward compatible version of slots into Chonkie in a patch release as soon as possible.\r\n\r\nThanks! "
      },
      {
        "user": "AgentT30",
        "created_at": "2024-11-18T06:53:26Z",
        "body": "@bhavnicksm Works with python 3.10 and above.\r\n\r\nThanks!"
      }
    ]
  }
]