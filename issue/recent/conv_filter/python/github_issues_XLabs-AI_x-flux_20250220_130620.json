[
  {
    "number": 75,
    "title": "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
    "created_at": "2024-08-23T05:19:32Z",
    "closed_at": "2024-08-26T00:15:25Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/75",
    "body": "  File \"/root/data/x-flux/train_flux_lora_deepspeed.py\", line 301, in <module>\r\n    main()\r\n  File \"/root/data/x-flux/train_flux_lora_deepspeed.py\", line 149, in main\r\n    dit, optimizer, _, lr_scheduler = accelerator.prepare(\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1292, in prepare\r\n    result = tuple(\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1293, in <genexpr>\r\n    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1169, in _prepare_one\r\n    return self.prepare_model(obj, device_placement=device_placement)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1412, in prepare_model\r\n    model = model.to(self.device)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1174, in to\r\n    return self._apply(convert)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 780, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 805, in _apply\r\n    param_applied = fn(param)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1167, in convert\r\n    raise NotImplementedError(\r\nNotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\nTraceback (most recent call last):\r\n  File \"/opt/miniconda/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\r\n    args.func(args)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1082, in launch_command\r\n    simple_launcher(args)\r\n  File \"/opt/miniconda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 688, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/opt/miniconda/bin/python', 'train_flux_lora_deepspeed.py', '--config', 'train_configs/test_lora.yaml']' returned non-zero exit status 1.",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/75/comments",
    "author": "starinskycc",
    "comments": [
      {
        "user": "whiterm",
        "created_at": "2024-08-25T20:52:46Z",
        "body": "It looks like you are trying to use the quantized model, which is not supported. You need to use the full version, not the quantized one."
      },
      {
        "user": "starinskycc",
        "created_at": "2024-08-26T00:15:22Z",
        "body": "Tks"
      }
    ]
  },
  {
    "number": 51,
    "title": "How to generate several images at one time by main.py(lora)",
    "created_at": "2024-08-16T13:07:49Z",
    "closed_at": "2024-08-21T07:50:36Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/51",
    "body": "I don't see some parameters represents the number of the image\r\nso how can i modify the `forward` function to generate such as 4 images at one time? ",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/51/comments",
    "author": "arrowonstr",
    "comments": [
      {
        "user": "Anghellia",
        "created_at": "2024-08-20T14:10:22Z",
        "body": "Hi, we added `--num_images_per_prompt` argument, please check"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-21T07:50:35Z",
        "body": "@Anghellia Thanks!"
      }
    ]
  },
  {
    "number": 46,
    "title": "How to train Flux LoRa with a local model dir?",
    "created_at": "2024-08-15T02:34:17Z",
    "closed_at": "2024-08-26T08:50:16Z",
    "labels": [],
    "url": "https://github.com/XLabs-AI/x-flux/issues/46",
    "body": "I download the model to my local dir (following the file structure of the flux repo), so i wanna use the local dir to train flux instead of downloading model by the procedure.\r\nI think it's easier to manage. \r\nHow can I do it? I tried to modify the code but failed\r\nI will be grateful if you can solve the problem",
    "comments_url": "https://api.github.com/repos/XLabs-AI/x-flux/issues/46/comments",
    "author": "arrowonstr",
    "comments": [
      {
        "user": "chy-chiu",
        "created_at": "2024-08-15T05:47:43Z",
        "body": "You can do that by specifying environment variables FLUX_DEV, FLUX_DEV_FP8, and AE"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-15T06:13:32Z",
        "body": "Thanks!\r\nI got it \"ckpt_path=os.getenv(\"FLUX_DEV\")\"\r\nI think local dir FLUX_DEV should contain \"flux1-dev.safetensors\"\"model_index.json\"\"ae.safetensors\" the three files\r\n\r\nand these:\r\n```\r\ndef load_t5(device: str | torch.device = \"cuda\", max_length: int = 512) -> HFEmbedder:\r\n    # max length 64, 128, 256 and 512 should work (if your sequence is short enough)\r\n    return HFEmbedder(\"xlabs-ai/xflux_text_encoders\", max_length=max_length, torch_dtype=torch.bfloat16).to(device)\r\n\r\ndef load_clip(device: str | torch.device = \"cuda\") -> HFEmbedder:\r\n    return HFEmbedder(\"openai/clip-vit-large-patch14\", max_length=77, torch_dtype=torch.bfloat16).to(device)\r\n```\r\n```\r\nclass HFEmbedder(nn.Module):\r\n    def __init__(self, version: str, max_length: int, **hf_kwargs):\r\n        super().__init__()\r\n        self.is_clip = version.startswith(\"openai\")\r\n        self.max_length = max_length\r\n        self.output_key = \"pooler_output\" if self.is_clip else \"last_hidden_state\"\r\n\r\n        if self.is_clip:\r\n            self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: CLIPTextModel = CLIPTextModel.from_pretrained(version, **hf_kwargs)\r\n        else:\r\n            self.tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: T5EncoderModel = T5EncoderModel.from_pretrained(version, **hf_kwargs)\r\n```\r\ndownload the tow repo and change the version to local dir, I will try it, hope it works!"
      },
      {
        "user": "arrowonstr",
        "created_at": "2024-08-16T05:07:23Z",
        "body": "It works and make sure to follow these:\r\n\r\n```\r\nexport FLUX_DEV=\"/root/autodl-tmp/project/flux-d/model/FLUX.1-dev/flux1-dev.safetensors\"\r\nexport AE=\"/root/autodl-tmp/project/flux-d/model/FLUX.1-dev/ae.safetensors\"\r\n```\r\nmodify the code\r\n```\r\ndef load_t5(device: str | torch.device = \"cuda\", max_length: int = 512) -> HFEmbedder:\r\n    # max length 64, 128, 256 and 512 should work (if your sequence is short enough)\r\n    return HFEmbedder(\"local_dir/xflux_text_encoders\", max_length=max_length, torch_dtype=torch.bfloat16).to(device)\r\n\r\ndef load_clip(device: str | torch.device = \"cuda\") -> HFEmbedder:\r\n    return HFEmbedder(\"local_dir/clip-vit-large-patch14\", max_length=77, torch_dtype=torch.bfloat16).to(device)\r\n```\r\n```\r\nclass HFEmbedder(nn.Module):\r\n    def __init__(self, version: str, max_length: int, **hf_kwargs):\r\n        super().__init__()\r\n        # from local dir\r\n        #self.is_clip = version.startswith(\"openai\")\r\n        if version == \"/root/autodl-tmp/project/flux-d/model/xflux_text_encoders\":\r\n            self.is_clip= False\r\n        else:\r\n            self.is_clip= True\r\n        self.max_length = max_length\r\n        self.output_key = \"pooler_output\" if self.is_clip else \"last_hidden_state\"\r\n\r\n        if self.is_clip:\r\n            self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: CLIPTextModel = CLIPTextModel.from_pretrained(version, **hf_kwargs)\r\n        else:\r\n            self.tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(version, max_length=max_length)\r\n            self.hf_module: T5EncoderModel = T5EncoderModel.from_pretrained(version, **hf_kwargs)\r\n\r\n        self.hf_module = self.hf_module.eval().requires_grad_(False)\r\n```\r\n\r\ncarefully modify the code to specify which version represent a clip model\r\n```\r\n if version == \"/root/autodl-tmp/project/flux-d/model/xflux_text_encoders\":\r\n            self.is_clip= False\r\n        else:\r\n            self.is_clip= True\r\n```"
      }
    ]
  }
]