[
  {
    "number": 160,
    "title": "distill",
    "created_at": "2025-01-23T14:47:13Z",
    "closed_at": "2025-01-23T20:39:51Z",
    "labels": [],
    "url": "https://github.com/MinishLab/model2vec/issues/160",
    "body": "Hi! Thanks for the great repo. I'm trying to distill a model2vec model from a relatively small sentence transformer, but it's taking several hours on GPU. Do you have any advice on optimizing this?\n\n```\nfrom model2vec.distill import distill\nm2v_model = distill(\"MY MODEL\", pca_dims=256)\nm2v_model.save_pretrained(\"m2v_model\")\n```\n",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/160/comments",
    "author": "Mahhos",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2025-01-23T16:41:29Z",
        "body": "Hello @Mahhos!\n\nThis shouldn't be happening. Distillation usually takes seconds on a GPU. Can you tell us more about the model?\n\nSt\u00e9phan"
      },
      {
        "user": "Mahhos",
        "created_at": "2025-01-23T17:19:57Z",
        "body": "it is a fine-tuned variant of BERT-based sentence transformer with 400m parameters trained for sentence-level tasks such as similarity, or classification."
      },
      {
        "user": "stephantul",
        "created_at": "2025-01-23T17:28:24Z",
        "body": "Ok, interesting! Does it show a progress bar at all? Could you rerun with your logging level set to `INFO`? \n\ne.g.,\n```python\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nIf it doesn't show a progress bar, you actually never start distillation, so that means it is stuck somewhere else.\n\nIf that doesn't work: Can you confirm that you can load the model and the tokenizer? How many tokens does the tokenizer have?\n\nHope one of these steps solves the issue!\nSt\u00e9phan"
      },
      {
        "user": "Mahhos",
        "created_at": "2025-01-23T17:46:53Z",
        "body": "Thanks for your response. It doesn't show a progress bar. A message is showing (even without logging):\n\n> The repository for _path_to_model_ contains custom code which must be executed to correctly load the model. You can inspect the repository c...\n\nYes, I can load the model and tokenizer, the tokenizer has 30522 tokens.\n\nI can load model using:\n`model = SentenceTransformer(path_to_model, trust_remote_code=True)\n`\n\nis there any required dependencies/version? "
      },
      {
        "user": "stephantul",
        "created_at": "2025-01-23T17:58:46Z",
        "body": "Hey @Mahhos!\n\nWe load with `trust_remote_code=False`, which is the default. You should be able to just press `y` to load the remote code. You can also load the model and tokenizer first, and then distill using `distill_from_model`.\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nfrom model2vec.distill import distill_from_model\n\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = Autotokenizer.from_pretrained(model_name)\ndistill_from_model(model, tokenizer)\n```\n\nThis should take care of the issue."
      },
      {
        "user": "Mahhos",
        "created_at": "2025-01-23T18:08:12Z",
        "body": "Thank you, Stephan! I could be able to run this using `trust_remote_code=True`. But, to save the model, `m2v_model.save_pretrained()` gives me below error. \n\n`ModuleNotFoundError: No module named 'huggingface_hub.errors'\n`"
      },
      {
        "user": "stephantul",
        "created_at": "2025-01-23T18:12:33Z",
        "body": "Hey @Mahhos ,\n\nThat can be resolved by upgrading `huggingface_hub`. I'll also fix that in the next release so that it works with lower versions of `huggingface_hub`."
      },
      {
        "user": "Mahhos",
        "created_at": "2025-01-23T18:13:31Z",
        "body": "Awesome! one last question, do we necessarily need gpu to run this?"
      },
      {
        "user": "stephantul",
        "created_at": "2025-01-23T18:15:26Z",
        "body": "Hey! No, not at all. It makes it a little bit slower, but there is no need to use a gpu. We automatically select a gpu when possible though."
      },
      {
        "user": "stephantul",
        "created_at": "2025-01-23T20:39:51Z",
        "body": "Hello! I'm closing this issue because it seems to be resolved, let me know if you run into further issues!"
      }
    ]
  },
  {
    "number": 137,
    "title": "Using Model2Vec for Token Classification",
    "created_at": "2024-11-29T01:57:11Z",
    "closed_at": "2024-11-29T18:36:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/137",
    "body": "Hi Model2Vec Team,\r\n\r\nThis is truely great work, thanks so much for the contribution! Awesome project!\r\n\r\nOne dumb question: Is it possible to apply model2vec to a token classification model, e.g., BERT model connected to a NER tagger?\r\n\r\nI've been playing with Model2Vec, and seems that for now it is designed to only output embedding for a whole sequence (e.g., [CLS]). It seems that Model2Vec currently does not support generating embeddings for every token in a sequence. Is this understanding correct?\r\n\r\nIf so, how would you recommend that I augment Model2Vec to support generating token-level embeddings, and potentially drop it into a NER tagger?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/137/comments",
    "author": "kelayamatoz",
    "comments": [
      {
        "user": "Pringled",
        "created_at": "2024-11-29T07:12:16Z",
        "body": "Hi @kelayamatoz,\r\n\r\nThanks for the kind words! You can actually use Model2Vec for that purpose out of the box, the following should work:\r\n\r\n```python\r\nfrom model2vec import StaticModel\r\n\r\n# Load a model from the HuggingFace hub (in this case the potion-base-8M model)\r\nmodel = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\r\n\r\n# Make sequences of token embeddings\r\ntoken_embeddings = model.encode_as_sequence([\"It's dangerous to go alone!\", \"It's a secret to everybody.\"])\r\n```\r\n\r\nThis will give you the individual token embedding for every input token. Let me know if this works for you or if you have any other questions! "
      },
      {
        "user": "kelayamatoz",
        "created_at": "2024-11-29T18:36:14Z",
        "body": "This works perfectly -- thank you for the pointer! My apologies, I just realized that the tutorial does include token embedding examples, and I somehow overlooked them. Sorry about that!\r\n"
      }
    ]
  },
  {
    "number": 110,
    "title": "HFValidationError using custom model",
    "created_at": "2024-10-23T11:59:24Z",
    "closed_at": "2024-10-23T12:29:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/110",
    "body": "Hi,\r\n\r\nI am trying to use Model2Vec with a custom model we have.\r\nThe `distill` seems to run fine, until it tries to call `validate_repo_id` with the custom path I have (`./model`).\r\n\r\n1. I have a custom model that I can load successfully:\r\n\r\n```python\r\nfrom transformers import AutoModelForSequenceClassification\r\n\r\nmodel_save_path = \"./model\"\r\nmodel = AutoModelForSequenceClassification.from_pretrained(model_save_path)\r\n```\r\n\r\n2. I `distill` the custom model:\r\n\r\n```python\r\nfrom model2vec.distill import distill\r\n\r\nmodel_save_path = \"./model\"\r\nm2v_model = distill(model_name=model_save_path, pca_dims=256)\r\n```\r\n\r\n3. The following error is raised:\r\n\r\n```\r\nHFValidationError\r\n\r\n    This cell raised an exception: HFValidationError('Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './model'.')\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/110/comments",
    "author": "tomsquest",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2024-10-23T12:04:42Z",
        "body": "Hi @tomsquest ,\r\n\r\nJust using `model` as a path should do the trick. Could you let us know whether that works? If that doesn't work, you can always load the model and tokenizer yourself, and use `distill_from_model`\r\n\r\n```python\r\nfrom model2vec.distill import distill_from_model\r\n\r\n# Some way to get your model and tokenizer.\r\nmodel, tokenizer = load_model_and_tokenizer(...)\r\n\r\nm2v_model = distill_from_model(model, tokenizer, pca_dims=256)\r\n```\r\n\r\nSt\u00e9phan"
      },
      {
        "user": "tomsquest",
        "created_at": "2024-10-23T12:27:31Z",
        "body": "Hi @stephantul ,\r\n\r\nYes, it's working using just `model` (instead of `./model`). That was my workaround. \ud83d\udc4d"
      },
      {
        "user": "stephantul",
        "created_at": "2024-10-23T12:29:39Z",
        "body": "Alright! Happy to be of service, I'm closing this for now. Let me know if you have other issues, and please reopen or make another issue if that's the case. Thanks for using model2vec! \ud83d\ude4f "
      }
    ]
  },
  {
    "number": 108,
    "title": "Number of tokens (151646) does not match number of vectors (151643)",
    "created_at": "2024-10-23T08:25:59Z",
    "closed_at": "2024-10-24T00:22:14Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/108",
    "body": "Hello.\r\n\r\nI tried testing `model2vec` in the following environment and encountered the error below. Is there a way to resolve this?\r\n\r\n### Environment\r\n\r\n```python\r\nmodel2vec==0.3.0\r\ntokenizers==0.19.1\r\n```\r\n\r\n### Code\r\n\r\n```python\r\nfrom model2vec.distill.distillation import distill\r\n\r\n# Choose a Sentence Transformer model\r\nmodel_id = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\r\n\r\n# Distill an output model with the chosen dimensions\r\nmodel = distill(model_name=model_id, device=\"cpu\", pca_dims=256)\r\n```\r\n\r\n### Error\r\n\r\n```python\r\nValue Error: Number of tokens (151646) does not match number of vectors (151643)\r\n```",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/108/comments",
    "author": "su-park",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2024-10-23T08:30:33Z",
        "body": "Hey @su-park ,\r\n\r\nThanks for reporting! This shouldn't be happening, I'll take a look as soon as possible. \r\n\r\nSt\u00e9phan"
      },
      {
        "user": "stephantul",
        "created_at": "2024-10-23T08:39:09Z",
        "body": "Hey, I figured it out. Somehow, the number of tokens of the backend tokenizer and the number of tokens in the HF tokenizer doesn't match. \r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-Qwen2-7B-instruct\")\r\ntokenizer.vocab_size\r\n# 151643\r\ntokenizer.backend_tokenizer.get_vocab_size()\r\n# 151646\r\nlen(tokenizer.get_vocab())\r\n# 151646\r\n```\r\n\r\nI checked, and there are definitely 151646 tokens in the vocabulary, not 15643. So I think the `vocab_size` being off by three is actually a bug in Transformers, not a bug in `model2vec`. In the mean-time, we could rely on the length of the vocabulary instead. I'll open a PR to change this."
      },
      {
        "user": "stephantul",
        "created_at": "2024-10-23T08:45:24Z",
        "body": "PR is here #109 . \r\n\r\nI'll see if we can push this together with #107 and release a bugfix release soon."
      },
      {
        "user": "stephantul",
        "created_at": "2024-10-23T11:32:37Z",
        "body": "I accidentally closed this because I merged the PR. Sorry about that. If you want, you can pull main and retry. I think it should work."
      },
      {
        "user": "su-park",
        "created_at": "2024-10-24T00:22:14Z",
        "body": "Thank you for your swift support.\r\n\r\nIt worked!"
      }
    ]
  },
  {
    "number": 79,
    "title": "AttributeError: 'NoneType' object has no attribute 'get'",
    "created_at": "2024-10-12T19:01:40Z",
    "closed_at": "2024-10-12T19:37:09Z",
    "labels": [],
    "url": "https://github.com/MinishLab/model2vec/issues/79",
    "body": "If I try to use the distilled model with Sentence Transformers, I am getting the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[4], line 7\r\n      5 model_distilled = distill(model_name=model_name, pca_dims=256)\r\n      6 model_distilled.save_pretrained(\"./multilingual-e5-small-distilled\")\r\n----> 7 st_model_distilled = SentenceTransformer(\"multilingual-e5-small-distilled\")\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:306, in SentenceTransformer.__init__(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\r\n    294         modules, self.module_kwargs = self._load_sbert_model(\r\n    295             model_name_or_path,\r\n    296             token=token,\r\n   (...)\r\n    303             config_kwargs=config_kwargs,\r\n    304         )\r\n    305     else:\r\n--> 306         modules = self._load_auto_model(\r\n    307             model_name_or_path,\r\n    308             token=token,\r\n    309             cache_folder=cache_folder,\r\n    310             revision=revision,\r\n    311             trust_remote_code=trust_remote_code,\r\n    312             local_files_only=local_files_only,\r\n    313             model_kwargs=model_kwargs,\r\n    314             tokenizer_kwargs=tokenizer_kwargs,\r\n    315             config_kwargs=config_kwargs,\r\n    316         )\r\n    318 if modules is not None and not isinstance(modules, OrderedDict):\r\n    319     modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1454, in SentenceTransformer._load_auto_model(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\r\n   1451 tokenizer_kwargs = shared_kwargs if tokenizer_kwargs is None else {**shared_kwargs, **tokenizer_kwargs}\r\n   1452 config_kwargs = shared_kwargs if config_kwargs is None else {**shared_kwargs, **config_kwargs}\r\n-> 1454 transformer_model = Transformer(\r\n   1455     model_name_or_path,\r\n   1456     cache_dir=cache_folder,\r\n   1457     model_args=model_kwargs,\r\n   1458     tokenizer_args=tokenizer_kwargs,\r\n   1459     config_args=config_kwargs,\r\n   1460 )\r\n   1461 pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \"mean\")\r\n   1462 self.model_card_data.set_base_model(model_name_or_path, revision=revision)\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:56, in Transformer.__init__(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\r\n     53     config_args = {}\r\n     55 config = AutoConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir)\r\n---> 56 self._load_model(model_name_or_path, config, cache_dir, **model_args)\r\n     58 if max_seq_length is not None and \"model_max_length\" not in tokenizer_args:\r\n     59     tokenizer_args[\"model_max_length\"] = max_seq_length\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:87, in Transformer._load_model(self, model_name_or_path, config, cache_dir, **model_args)\r\n     85     self._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\r\n     86 else:\r\n---> 87     self.auto_model = AutoModel.from_pretrained(\r\n     88         model_name_or_path, config=config, cache_dir=cache_dir, **model_args\r\n     89     )\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    562 elif type(config) in cls._model_mapping.keys():\r\n    563     model_class = _get_model_class(config, cls._model_mapping)\r\n--> 564     return model_class.from_pretrained(\r\n    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\r\n    566     )\r\n    567 raise ValueError(\r\n    568     f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\r\n    569     f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\r\n    570 )\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3792, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\r\n   3789 with safe_open(resolved_archive_file, framework=\"pt\") as f:\r\n   3790     metadata = f.metadata()\r\n-> 3792 if metadata.get(\"format\") == \"pt\":\r\n   3793     pass\r\n   3794 elif metadata.get(\"format\") == \"tf\":\r\n\r\nAttributeError: 'NoneType' object has no attribute 'get'\r\n```\r\n\r\nUse the following code snippet to reproduce the error:\r\n\r\n```python\r\nfrom model2vec.distill import distill\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\nmodel_name = \"intfloat/multilingual-e5-small\"\r\nmodel_distilled = distill(model_name=model_name, pca_dims=256)\r\nmodel_distilled.save_pretrained(\"./multilingual-e5-small-distilled\")\r\nst_model_distilled = SentenceTransformer(\"multilingual-e5-small-distilled\")\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/79/comments",
    "author": "aoezdTchibo",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2024-10-12T19:29:18Z",
        "body": "Hey, you are unfortunately not able to use `model2vec` like this. Please use the following snippet:\r\n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom sentence_transformers.models import StaticEmbedding\r\n\r\nmodel_name = \"intfloat/multilingual-e5-small\"\r\nstatic_embedding = StaticEmbedding.from_distillation(model_name, device=\"cuda\")\r\nmodel = SentenceTransformer(modules=[static_embedding])\r\n```\r\n\r\nShould you have any further questions, please don't hesitate to reach out."
      },
      {
        "user": "aoezdTchibo",
        "created_at": "2024-10-12T19:37:09Z",
        "body": "Works with the provided snippet, thank you!"
      }
    ]
  }
]