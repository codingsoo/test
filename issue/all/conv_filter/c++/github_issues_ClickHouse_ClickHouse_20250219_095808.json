[
  {
    "number": 76040,
    "title": "How to configure to stop insert data into local table in specific machine with cluster table scenario?",
    "created_at": "2025-02-13T09:03:51Z",
    "closed_at": "2025-02-14T02:32:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/76040",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nIn such situation, a clickhouse cluster with 4 machines: ck0, ck1, ck2, ck3,\nand local table test_tb in all 4 machines, and corresponding cluster table test_tb_cluster in ck0.\nSelect test_tb_cluster table data in ck0 will query every test_tb table data from all 4 machines.\nWhat I want is store table test_tb data in just ck1, ck2, ck3 these 3 machines, not store in ck0(reduct insert stress and select stress in this machine, just use for insert&select task distribution), \nMust create table test_tb in ck0, right? Otherwise operate test_tb_cluster will raise error: \nThere is no table `dbxxx`.`test_tb ` on server: ck0:9000\n\nSo how can I config table test_tb in ck0 to make it just as a empty local table? \nInsert data into test_tb_cluster will never choice test_tb in ck0 to store.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/76040/comments",
    "author": "flyly0755",
    "comments": [
      {
        "user": "cangyin",
        "created_at": "2025-02-14T01:52:38Z",
        "body": "I come up with 3 ways, hope one of them can be helpful:\n\n1. Configure the ingestion behavior on client side (for example only let the client know about the addresses of ck1 ~ ck3).\n\n2. Another cluster and another Distributed table\n\n    Create another logical cluster in `<remote_servers>` without ck0, say `cluster_no_ck0`\n\n    Create another Distributed table on cluster `cluster_no_ck0`\n\n\n3. With `insert_distributed_one_random_shard`\n \n    Recreate the distributed table with NO sharding key.\n\n    Configure the weight of shard 0 to zero in `<remote_servers>`.\n\n    And insert with settings `insert_distributed_one_random_shard=1`\n\n"
      },
      {
        "user": "flyly0755",
        "created_at": "2025-02-14T02:32:00Z",
        "body": "I have tried the second ways, which is perfect for solving this problem, thx!"
      }
    ]
  },
  {
    "number": 74005,
    "title": "A lot of folders with the prefix clone_ in the detached directory under the ClickHouse local table folder.",
    "created_at": "2024-12-31T06:17:37Z",
    "closed_at": "2025-01-21T07:34:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/74005",
    "body": "### Company or project name\n\ncompany\uff1acoohom\uff1b\r\nUse clickhouse to store monitoring logs\uff1b\n\n### Question\n\nThanks for your work\uff01\r\n\r\nThis is table structure:\r\n````sql\r\nCREATE TABLE monitor.qunhe_log\r\n(\r\n    `timestamp` DateTime64(3, 'Asia/Shanghai'),\r\n    `hostGroup` String,\r\n    `ip` String,\r\n    `podname` String,\r\n    `level` String,\r\n    `cls` String,\r\n    `behavior` String,\r\n    `message` String,\r\n    `id` String DEFAULT '',\r\n    INDEX message_index message TYPE tokenbf_v1(30720, 3, 0) GRANULARITY 1\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/qunhe_log_20240112_v1', '{replica}')\r\nPARTITION BY toYYYYMMDD(timestamp)\r\nORDER BY (level, hostGroup, podname, ip, timestamp)\r\nSETTINGS storage_policy = 'hot_and_cold', index_granularity = 8192 \r\n\r\n1 row in set. Elapsed: 0.002 sec. \r\n````\r\n\r\nBut has a lot of folders with the prefix clone_ in the detached directory under the ClickHouse local table folder. There is no operation about ```ALTER TABLE ... FREEZE```\r\n````shell\r\n[root@10 detached]# pwd\r\n/data-ssd/clickhouse-data/store/815/815edd8b-ced9-4459-be8b-ec82e7b95cb0/detached\r\n[root@10 detached]# ls -l\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5406_5412_1\r\n......\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5416_5421_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5422_5422_0\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5423_5452_2\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5471_5478_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5483_5491_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5638_5643_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5644_5649_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5801_5801_0\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5878_5882_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5893_5898_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5969_5976_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_6070_6070_0\r\n[root@10 detached]# ls -l|grep \"clone_\"|wc -l\r\n1184\r\n[root@10 detached]# \r\n````\r\nThese folders will take up a lot of disk space. I don't know under what circumstances such folders will be created. Is there any readme text to explain it? Thanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/74005/comments",
    "author": "ChaoHsupin",
    "comments": [
      {
        "user": "wzb5212",
        "created_at": "2024-12-31T08:43:04Z",
        "body": "@save-my-heart \u5f1b"
      },
      {
        "user": "den-crane",
        "created_at": "2024-12-31T14:00:07Z",
        "body": "It's completely normal that Clickhouse creates clone_ folders.\r\nIt's complete normal to delete them `rm -rf clone_*`\r\n\r\nParts are renamed to \u2018cloned\u2019 if ClickHouse have had some parts on local disk while repairing lost replica so already existed parts being renamed and put in detached directory. Controlled by setting `merge_tree/detach_old_local_parts_when_cloning_replica`.\r\n\r\n```\r\nselect * from system.merge_tree_settings where name = 'detach_old_local_parts_when_cloning_replica';\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u252c\u2500changed\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\r\n\u2502 detach_old_local_parts_when_cloning_replica \u2502 1     \u2502       0 \u2502 Do not remove old local parts when repairing lost replica. \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\r\n```"
      },
      {
        "user": "ChaoHsupin",
        "created_at": "2025-01-21T07:34:18Z",
        "body": "@den-crane thanks"
      }
    ]
  },
  {
    "number": 73873,
    "title": "Upgrade to 24.12.1.1614",
    "created_at": "2024-12-27T08:28:57Z",
    "closed_at": "2024-12-27T15:44:35Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/73873",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nI want to upgrade ClickHouse (version: 24.5.4.49) to 24.12.1.1614, but I noticed that the structure of configuration items like config.xml has changed. Originally, it was:\r\n```\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n   Configuration content\r\n</yandex>\r\n```\r\nNow, it is:\r\n```\r\n<clickhouse>\r\n   Configuration content\r\n</clickhouse>\r\n```\r\n1.Do I need to modify all configuration files to the new structure now? This seems like a very risky operation.\r\n2. Additionally, I need to replace Zookeeper   with Keeper. Is it sufficient to only edit metrika.xml? The Keeper cluster has already been deployed.\r\nContent of metrika.xml:\r\n```\r\n<yandex>\r\n    <clickhouse_remote_servers>\r\n        <cluster_1s_1r>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>server01</host>\r\n                    <port>9000</port>\r\n                    <user>default</user>\r\n                    <password>_CLUster0369</password>\r\n                </replica>\r\n            </shard>\r\n          </cluster_1s_1r>\r\n    </clickhouse_remote_servers>\r\n\r\n    <zookeeper-servers>\r\n        <node index=\"1\">\r\n            <host>172.16.13.11</host>\r\n            <port>2181</port>\r\n        </node>\r\n\t\t<node index=\"2\">\r\n            <host>172.16.13.12</host>\r\n            <port>2181</port>\r\n        </node>\r\n\t\t<node index=\"3\">\r\n            <host>172.16.13.13</host>\r\n            <port>2181</port>\r\n        </node>\r\n    </zookeeper-servers>\r\n\r\n    <macros>\r\n        <layer>01</layer>\r\n        <shard>01</shard>\r\n        <replica>cluster01-01-1</replica>\r\n    </macros>\r\n    <networks>\r\n        <ip>::/0</ip>\r\n    </networks>\r\n\r\n    <clickhouse_compression>\r\n        <case>\r\n            <min_part_size>10000000000</min_part_size>\r\n            <min_part_size_ratio>0.01</min_part_size_ratio>\r\n            <method>lz4</method>\r\n        </case>\r\n    </clickhouse_compression>\r\n</yandex>\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/73873/comments",
    "author": "EminemJK",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-12-27T14:38:27Z",
        "body": ">1.Do I need to modify all configuration files to the new structure now? This seems like a very risky operation.\r\n\r\nNo, both `<yandex>` and `<clickhouse>` are supported. You can edit part of the files and replace to clickhouse gradually.\r\n\r\n>metrika.xml\r\n\r\nPlease don't use metrika.xml. It's obsolete.\r\n\r\nUse: `/etc/clickhouse-server/config.d/my_config.xml`\r\n\r\n```\r\ncat /etc/clickhouse-server/config.d/my_config.xml\r\n\r\n<clickhouse>\r\n    <remote_servers>\r\n        <cluster_1s_1r>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>server01</host>\r\n                    <port>9000</port>\r\n                    <user>default</user>\r\n                    <password>_CLUster0369</password>\r\n                </replica>\r\n            </shard>\r\n          </cluster_1s_1r>\r\n    </remote_servers>\r\n\r\n    <zookeeper>\r\n        <node>\r\n            <host>172.16.13.11</host>\r\n            <port>2181</port>\r\n        </node>\r\n\t\t<node>\r\n            <host>172.16.13.12</host>\r\n            <port>2181</port>\r\n        </node>\r\n\t\t<node>\r\n            <host>172.16.13.13</host>\r\n            <port>2181</port>\r\n        </node>\r\n    </zookeeper>\r\n\r\n    <macros>\r\n        <layer>01</layer>\r\n        <shard>01</shard>\r\n        <replica>cluster01-01-1</replica>\r\n    </macros>\r\n\r\n    <compression>\r\n        <case>\r\n            <min_part_size>10000000000</min_part_size>\r\n            <min_part_size_ratio>0.01</min_part_size_ratio>\r\n            <method>lz4</method>\r\n        </case>\r\n    </compression>\r\n</clickhouse>\r\n```\r\n\r\n"
      },
      {
        "user": "EminemJK",
        "created_at": "2024-12-30T08:44:05Z",
        "body": "ok\uff0cthx"
      }
    ]
  },
  {
    "number": 72695,
    "title": "Does attach part support rollback if it fails?",
    "created_at": "2024-12-02T12:07:39Z",
    "closed_at": "2024-12-02T12:40:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/72695",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nIf ALTER TABLE %s ATTACH PART '%s' fails, is detach rollback supported?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/72695/comments",
    "author": "caicancai",
    "comments": [
      {
        "user": "rschu1ze",
        "created_at": "2024-12-02T12:10:04Z",
        "body": "Maybe I don't understand your question right but if the ATTACH operation fails, the part is not attached. Why should DETACH be rolled back?"
      },
      {
        "user": "caicancai",
        "created_at": "2024-12-02T12:40:19Z",
        "body": "I think I understand, thanks"
      }
    ]
  },
  {
    "number": 72530,
    "title": "Can i use a nested path in JSONExtractArrayRaw?",
    "created_at": "2024-11-27T05:09:54Z",
    "closed_at": "2024-11-27T14:46:44Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/72530",
    "body": "### Company or project name\r\n\r\n_No response_\r\n\r\n### Question\r\n\r\nClickHouse 24.10.3.21 running on x64 Debian\r\n\r\nI'm trying to extract, ideally as a ClickHouse array, values of a nested array from JSON. I can do it in quite a convoluted way:\r\n\r\n```\r\nSELECT JSONExtractArrayRaw(JSONExtract(JSONExtract('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'String'), 'details', 'String'), 'hobbies')\r\n\r\nQuery id: 2628ff82-8ad1-4669-98df-183700e03779\r\n\r\n   \u250c\u2500JSONExtractArrayRaw(JSONExtract(JSONExtract('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'String'), 'details', 'String'), 'hobbies')\u2500\u2510\r\n1. \u2502 ['{\"key\":1}','{\"key\":2}','{\"key\":3}']                                                                                                                                  \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThat's verbose. \r\n\r\nIs it possible to pass nested path to JSONExtractArrayRaw - something like _user.details.hobbies_?\r\n\r\nI know that the new JSON data type / functions are on their way, but I'd like to stick to stable functionality.\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/72530/comments",
    "author": "pakud",
    "comments": [
      {
        "user": "tiagoskaneta",
        "created_at": "2024-11-27T08:37:33Z",
        "body": "`JSONExtract*` all support multiple keys:\r\n\r\n```\r\nSELECT JSONExtractArrayRaw('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'details', 'hobbies')\r\n```\r\n\r\nYou can also use `JSON_VALUE` if you prefer:\r\n\r\n```\r\nSELECT JSON_VALUE('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', '$.user.details.hobbies') settings function_json_value_return_type_allow_complex=true\r\n```"
      },
      {
        "user": "pakud",
        "created_at": "2024-11-27T10:23:31Z",
        "body": "that's it - thanks a lot @tiagoskaneta !"
      }
    ]
  },
  {
    "number": 71394,
    "title": "How to force ClickHouse to return column names prefixed with table names when INNER JOIN?",
    "created_at": "2024-11-01T21:45:03Z",
    "closed_at": "2024-11-20T14:16:24Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/71394",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nFor example:\r\n\r\nCREATE TABLE ta\r\n(\r\n    `a` UInt32,\r\n    `id` UInt32\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\n\r\nCREATE TABLE tb\r\n(\r\n    `b` UInt32,\r\n    `id` UInt32\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\n\r\ninsert into ta(*) VALUES (0,1)\r\ninsert into tb(*) VALUES (1,1)\r\n\r\nWITH\r\n    a AS\r\n    (\r\n        SELECT\r\n            a,\r\n            id\r\n        FROM ta\r\n    ),\r\n    b AS\r\n    (\r\n        SELECT\r\n            b,\r\n            id\r\n        FROM tb\r\n    )\r\nSELECT *\r\nFROM a\r\nINNER JOIN b ON a.id = b.id\r\n\r\nThis returns\r\n\u250c\u2500a\u2500\u252c\u2500id\u2500\u252c\u2500b\u2500\u252c\u2500b.id\u2500\u2510\r\n\u2502 0     \u2502  1    \u2502 1     \u2502    1 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nHow to force the returned qualified column names, i.e. a.a, a.id, b.b and b.id?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/71394/comments",
    "author": "wlzywang",
    "comments": [
      {
        "user": "amabilee",
        "created_at": "2024-11-05T01:14:57Z",
        "body": "In `ClickHouse`, column names are not automatically prefixed with their respective table names when performing a `join`. \r\n\r\nHowever, you can achieve this by explicitly specifying the column names in your `SELECT` statement. Here's an example that forces the returned qualified column names as a.a, a.id, b.b, and b.id:\r\n\r\n```\r\nCREATE TABLE ta\r\n(\r\n    a UInt32,\r\n    id UInt32\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nCREATE TABLE tb\r\n(\r\n    b UInt32,\r\n    id UInt32\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nINSERT INTO ta VALUES (0, 1);\r\nINSERT INTO tb VALUES (1, 1);\r\n\r\nWITH\r\n    a AS\r\n    (\r\n        SELECT\r\n            a,\r\n            id\r\n        FROM ta\r\n    ),\r\n    b AS\r\n    (\r\n        SELECT\r\n            b,\r\n            id\r\n        FROM tb\r\n    )\r\nSELECT\r\n    a.a AS `a.a`,\r\n    a.id AS `a.id`,\r\n    b.b AS `b.b`,\r\n    b.id AS `b.id`\r\nFROM a\r\nINNER JOIN b ON a.id = b.id;\r\n```\r\nThis will return:\r\n```\r\n\u250c\u2500a.a\u2500\u252c\u2500a.id\u2500\u252c\u2500b.b\u2500\u252c\u2500b.id\u2500\u2510\r\n\u2502  0  \u2502   1  \u2502  1  \u2502   1  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      },
      {
        "user": "wlzywang",
        "created_at": "2024-11-05T02:09:08Z",
        "body": "Yeah, actually the alias is what I am doing now but it is kinda tedious if I have many columns to select and several tables to join together. Thank you for confirming, Amabilee."
      }
    ]
  },
  {
    "number": 71340,
    "title": "clickhouse-keeper path gets deleted after ReplicatedMergeTree table recreation",
    "created_at": "2024-11-01T08:28:27Z",
    "closed_at": "2024-11-01T09:50:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/71340",
    "body": "### Company or project name\r\n\r\n_No response_\r\n\r\n### Question\r\nClickhouse Version: 24.9.1\r\nEnvironment:\r\n- ClickHouse cluster with 7 shards, 2 replicas each\r\n- One machine in shard 6 failed and was replaced\r\n\r\nSteps to reproduce:\r\n1. Replaced failed machine in shard 6\r\n2. Reconfigured the node and rejoined it to the cluster  (replica = 02)\r\n3. Recreated the table using ZooKeeper path: ``` (' /clickhouse/tables/7e5645d5-6728-4c27-ba8b-b96ba2dcb9bd/06','{replica}')```\r\n4. Table creation succeeded, Exec query ``` SELECT\r\n    database,\r\n    `table`,\r\n    is_leader,\r\n    is_readonly,\r\n    total_replicas,\r\n    active_replicas,\r\n    zookeeper_path,\r\n    queue_size,\r\n    inserts_in_queue,\r\n    merges_in_queue,\r\n    log_max_index,\r\n    log_pointer\r\nFROM system.replicas\r\nWHERE database = 'pro2_signoz_traces'  AND `table` = ''signoz_index_v2'') ```\r\n> total_replicas=2 active_replicas=2\r\n5. After a few minutes, the keeper path (/clickhouse/tables/7e5645d5-6728-4c27-ba8b-b96ba2dcb9bd/06/replicas/02) gets automatically deleted\r\n\r\n6. The table still exists in ClickHouse  \r\n> total_replicas=0 active_replicas=0\r\n\r\nQuestions:\r\n1. What could cause the ZooKeeper path to be automatically deleted while the table remains?\r\n2. How to properly recreate a replicated table after node failure?\r\nAny debug logs or configuration details that would be helpful in diagnosing this issue?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/71340/comments",
    "author": "youfu-fun",
    "comments": [
      {
        "user": "panzhilin007",
        "created_at": "2024-11-01T09:32:56Z",
        "body": "By default, ClickHouse will remove the zookeeeper path 480 seconds after you drop the table.\r\n\r\nTry to use `drop table tableName sync` instead.\r\n\r\n"
      },
      {
        "user": "youfu-fun",
        "created_at": "2024-11-01T09:41:18Z",
        "body": "> By default, ClickHouse will remove the zookeeeper path 480 seconds after you drop the table.\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cClickHouse \u4f1a\u5728\u4f60\u653e\u8868\u540e 480 \u79d2\u5220\u9664 zookeeeper \u8def\u5f84\u3002\r\n> \r\n> Try to use `drop table tableName sync` instead.\u5c1d\u8bd5\u6539\u7528 `drop table tableName sync`\u3002\r\n\r\nProblem solved, thanks \uff5e"
      }
    ]
  },
  {
    "number": 70599,
    "title": "ClickHouse distributed JOIN vs common JOIN",
    "created_at": "2024-10-12T19:24:28Z",
    "closed_at": "2024-10-14T10:19:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/70599",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\n1. Set up\r\n\r\nClickHouse cluster with 2 shards, 1 replica on each shard\r\nLocal table testjoin on each replica\r\n```\r\nCREATE TABLE testjoin\r\n(\r\n    `user` String,\r\n    `type` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/testjoin', '{replica}')\r\nPARTITION BY type\r\nORDER BY type\r\n```\r\nDistributed table testall\r\n```\r\nCREATE TABLE testall\r\n(\r\n    `user` String,\r\n    `type` String\r\n)\r\nENGINE = Distributed('cluster', 'default', 'testjoin', rand())\r\n```\r\nShard 1 Replica 1 has the following data\r\nSELECT *\r\nFROM testjoin\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u2510\r\n\u2502 user1     \u2502 type1     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nShard 2 Replica 1 has the following data\r\nSELECT *\r\nFROM testjoin\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u2510\r\n\u2502 user1     \u2502 type2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2. Query with INNER JOIN and GLOBAL INNER JOIN yields the same result. distributed_product_mode = 'allow' or  'local' or 'deny' makes no difference either. Is this expected behavior?\r\n\r\nWITH\r\n    t1 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type1'\r\n    ),\r\n    t2 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type2'\r\n    )\r\nSELECT *\r\nFROM t1\r\nINNER JOIN t2 ON t1.user = t2.user\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nWITH\r\n    t1 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type1'\r\n    ),\r\n    t2 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type2'\r\n    )\r\nSELECT *\r\nFROM t1\r\nGLOBAL INNER JOIN t2 ON t1.user = t2.user\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/70599/comments",
    "author": "wlzywang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2024-10-14T10:19:36Z",
        "body": "When JOINing subqueries, including those from CTEs (rather than regular tables), JOIN is fully performed on the initiating node, and does not depend on the mode (local or global)."
      },
      {
        "user": "wlzywang",
        "created_at": "2024-10-14T17:01:27Z",
        "body": "Hi Alexey-Milovidov\r\n\r\nEven if I do not use WITH, the result is the same\r\n\r\n```\r\nSELECT *\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM testall\r\n    WHERE type = 'type1'\r\n) AS t1\r\nINNER JOIN\r\n(\r\n    SELECT *\r\n    FROM testall\r\n    WHERE type = 'type2'\r\n) AS t2 ON t1.user = t2.user\r\n```\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nCould you please explain a bit more?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2024-10-14T18:28:14Z",
        "body": "When JOINing subqueries, JOIN is fully performed on the initiating node, and does not depend on the mode (local or global)."
      },
      {
        "user": "wlzywang",
        "created_at": "2024-10-14T19:19:50Z",
        "body": "I see. Now I feel safe in my usage. Thank you for confirming, Alexey-Milovidov."
      }
    ]
  },
  {
    "number": 70548,
    "title": "To upgrade Clickhouse from 22.x to 24.x",
    "created_at": "2024-10-10T05:11:39Z",
    "closed_at": "2024-10-10T21:06:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/70548",
    "body": "### Company or project name\n\nTimePlay \n\n### Question\n\nI set up a test Clickhouse server running version 24.8.4.13 and noticed the data layout in the S3 bucket is different from that of  Clickhouse server running version 22.8.6.71.\r\n\r\nEssentially, with Clickhouse version 22.8.6.71, the S3 bucket contains object keys without  a prefix, for example,  \"aaapkcyerlxwvoeuyqfeqasxbsxpkdtq\", while with version 24.8.4.13, the object keys have a prefix, for example, \"abq/prlqajkzrohcpycwzbabdnnoehhls\"\r\n\r\nI wonder if this discrepancy in object key naming would cause any issues if I am to upgrade a  Clickhouse cluster from version 22.8.6.71 to 24.8.4.13.\r\n\r\nThank you.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/70548/comments",
    "author": "tlegit",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-10-10T11:14:00Z",
        "body": "No worries. It's a performance improvement to overcome some AWS S3 limitations. \r\nIt's backward and forward compatible.\r\n\r\n\r\nThe link to s3 object is stored in a metadata file (..../data/db/table/some.bin). Clickhouse-server reads this metadata file and finds that the object name is `aaapkcyerlxwvoeuyqfeqasxbsxpkdtq` or `abq/prlqajkzrohcpycwzbabdnnoehhls` or `hello.world`, then it reads the object. \r\nDuring writing Clickhouse-server generates some random name for an s3 object and saves this name to the metadata files."
      },
      {
        "user": "tlegit",
        "created_at": "2024-10-10T21:03:48Z",
        "body": "Thank you, Den."
      }
    ]
  },
  {
    "number": 69232,
    "title": "s3 table function fetch the whole parquet instead of metadata for count when it's a small file",
    "created_at": "2024-09-03T19:37:10Z",
    "closed_at": "2024-09-04T15:53:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/69232",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\ni observed an issue with select count(*) from s3() reading parquet files that are small (900-1000KB)\r\nwhen i check how much byte it read\r\nit shows ReadBufferFromS3Bytes = sum of size of all files\r\nbut if i merge 2 files together and size is about 1.8MB\r\n```\r\nINSERT INTO FUNCTION s3(gcs_login, filename = 'clickhouse/test_merged.parquet') SELECT *\r\nFROM s3(gcs_login, filename = 'clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=..../time=..../{0,1}.parquet', format = 'Parquet')\r\n```\r\nand try again, it only fech 64KB which i think it's the metadata\r\n\r\n\r\n```\r\nSELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('85c68382-d77f-4f23-929b-27752199a873', '209b6fd3-7dd8-432b-9c91-d7adbb9efd6d')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC;\r\n\r\nQuery id: 6f912e2d-ded3-4a29-ba2f-004bff20ff47\r\n\r\n   \u250c\u2500query\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500bytes\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 select count(*) from s3(gcs_login,filename='clickhouse/test_merged.parquet', format='Parquet');                                                                                        \u2502 64.00 KiB \u2502\r\n2. \u2502 select count(*) from s3(gcs_login,filename='clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=..../time=..../*', format='Parquet'); \u2502 66.17 MiB \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2 rows in set. Elapsed: 0.005 sec.\r\n```\r\n\r\nis there any threshold that decide to fetch the whole file or just the metadata? can i force clickhouse to only read the metadata? we need to run qa validation script on few TB data to monitor the count and most of them are small files (<1.5MB) so it only make sense if we can just fetch the metadata.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/69232/comments",
    "author": "jiayeZhu",
    "comments": [
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-03T21:56:53Z",
        "body": "additional info:\r\nI'm running clickhouse 24.8.3.59\r\nand i also tested `s3Cluster` table function. seems `s3Cluster` works fine and only read 64KB instead of the whole file"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-04T13:31:34Z",
        "body": "can you also  use `SETTINGS send_logs_level='trace'` for both queries to see what is going on exactly?"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:07:40Z",
        "body": "it's a little bit hard to reproduce same result looks like once query on one s3 file it somehow no longer download the whole file again for the count.  but after few try i found this pattern\r\nfor small file:\r\n```\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.609867 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Debug> executeQuery: (from 127.0.0.1:44268) select count(*) from s3(gcs_login,filename='clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=weekly/time=202431/{0,1}.parquet') SETTINGS send_logs_level = 'trace'; (stage: Complete)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.612962 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> S3Client: Provider type: GCS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.612983 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> S3Client: API mode of the S3 client: AWS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.657866 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Planner: Query to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.658639 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.693984 [ 1422 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.693999 [ 1455 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n**[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694043 [ 1422 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 0 to 1 rows (from 0.00 B) in 0.000983196 sec. (0.000 rows/sec., 0.00 B/sec.)**\r\n**[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694052 [ 1455 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 0 to 1 rows (from 0.00 B) in 0.000983714 sec. (0.000 rows/sec., 0.00 B/sec.)**\r\n**[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694262 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> StorageObjectStorageSource: Downloading object of size 1105725 with initial prefetch**\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694273 [ 1467 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694294 [ 1467 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694502 [ 1467 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 99999 to 1 rows (from 0.00 B) in 0.00143994 sec. (69446643.610 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.814955 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.814990 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.815278 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 100000 to 1 rows (from 0.00 B) in 0.122190712 sec. (818392.809 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.815292 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Merging aggregated data\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.815319 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> HashTablesStatistics: Statistics updated for key=4678785569880944277: new sum_of_sizes=4, median_size=1\r\n```\r\nfor large file:\r\n```\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:56:59.912725 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Debug> executeQuery: (from 127.0.0.1:44268) select count(*) from s3(gcs_login, filename='clickhouse/test2.parquet') SETTINGS send_logs_level = 'trace'; (stage: Complete)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:56:59.915309 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> S3Client: Provider type: GCS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:56:59.915333 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> S3Client: API mode of the S3 client: AWS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.055491 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Planner: Query to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.056022 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111326 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111358 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111492 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> AggregatingTransform: Aggregated. 199999 to 1 rows (from 0.00 B) in 0.055219536 sec. (3621888.456 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111508 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Aggregator: Merging aggregated data\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111531 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> HashTablesStatistics: Statistics updated for key=4218342127720216905: new sum_of_sizes=1, median_size=1\r\n```\r\nso looks like for small file the problem is it somehow did a first round aggregationg without any data (those  Aggregated. 0 to 1 rows) then try to download the whole file and aggregate"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-04T15:14:41Z",
        "body": "I see the smaller file is prefetched which makes sense based on the code and the size itself.\r\nWhat I don't understand is why 1.8MiB is determined to be a large file. The cutoff is `2 * max_download_buffer_size`.\r\n`max_download_buffer_size` by default is 10MiB.\r\n\r\nCan you try setting `max_download_buffer_size` to a really small value (maybe 1) just to confirm the theory?\r\nIn any case, the prefetch of 1MiB is done to optimize throughput. For larger files, parallel download should be used so no need to prefetch anything. \r\n"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:41:14Z",
        "body": "thanks @antonio2368 \r\ni just created a new file with same content and with settings max_download_buffer_size=1\r\nhere is the trace\r\n```\r\nSELECT count(*)\r\nFROM s3(gcs_login, filename = 'clickhouse/test3.parquet')\r\nSETTINGS max_download_buffer_size = 1, send_logs_level = 'trace'\r\n\r\nQuery id: 4cb45d52-b28c-4dbc-be31-d849ef5f0623\r\n\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.025083 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> executeQuery: (from 127.0.0.1:39620) select count(*) from s3(gcs_login, filename='clickhouse/test3.parquet') SETTINGS max_download_buffer_size=1,send_logs_level = 'trace'; (stage: Complete)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.028246 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> S3Client: Provider type: GCS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.028266 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> S3Client: API mode of the S3 client: AWS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.188557 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Planner: Query to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.189271 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.220829 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.220908 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.221074 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> AggregatingTransform: Aggregated. 199999 to 1 rows (from 0.00 B) in 0.031441883 sec. (6360910.382 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.221088 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Aggregator: Merging aggregated data\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.221114 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> HashTablesStatistics: Statistics updated for key=11572164019212161807: new sum_of_sizes=1, median_size=1\r\n   \u250c\u2500count()\u2500\u2510\r\n1. \u2502  199999 \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.222198 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> executeQuery: Read 199999 rows, 40.00 B in 0.197258 sec., 1013895.5074065438 rows/sec., 202.78 B/sec.\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.222431 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> MemoryTracker: Peak memory usage (for query): 225.56 KiB.\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.222447 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> TCPHandler: Processed in 0.197843186 sec.\r\n\r\n1 row in set. Elapsed: 0.197 sec. Processed 200.00 thousand rows, 40.00 B (1.01 million rows/s., 202.57 B/s.)\r\nPeak memory usage: 225.56 KiB.\r\n\r\nchi-ch-analytical-ch-analytical-2-0-0.chi-ch-analytical-ch-analytical-2-0.clickhouse.svc.cluster.local :) SELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('4cb45d52-b28c-4dbc-be31-d849ef5f0623')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC;\r\n\r\nSELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('4cb45d52-b28c-4dbc-be31-d849ef5f0623')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC\r\n\r\nQuery id: ab87e7a3-b0bf-43a9-8e4a-a1af83d33d05\r\n\r\n   \u250c\u2500query\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500bytes\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 select count(*) from s3(gcs_login, filename='clickhouse/test3.parquet') SETTINGS max_download_buffer_size=1,send_logs_level = 'trace'; \u2502 64.00 KiB \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.008 sec.\r\n\r\nchi-ch-analytical-ch-analytical-2-0-0.chi-ch-analytical-ch-analytical-2-0.clickhouse.svc.cluster.local :) select _size from s3(gcs_login, filename='clickhouse/test3.parquet') limit 1;\r\n\r\nSELECT _size\r\nFROM s3(gcs_login, filename = 'clickhouse/test3.parquet')\r\nLIMIT 1\r\n\r\nQuery id: f85cf378-8dd1-44c8-a0e9-4da394382aa0\r\n\r\n   \u250c\u2500\u2500\u2500_size\u2500\u2510\r\n1. \u2502 2044228 \u2502 -- 2.04 million\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:44:16Z",
        "body": "also tried a new file with not same content but similar size (200k rows) still same result only downloaded 64KB"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-04T15:47:13Z",
        "body": "okay, so now the expected amount is downloaded?\r\n`max_download_buffer_size` should not be left to `1` as it is used by the parallel reader for larger file sizes. If you set it to such a low value, performance will suffer.\r\nI don't see a different way to forcefully disable it for smaller files only, but maybe we can add a new setting."
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:53:33Z",
        "body": "so seems like max_download_buffer_size=1 solve my issue with small files for now, we just don't want to download the whole file even if it has degraded performance. the inter region data transfer fee in gcs will be crazy without this settings.\r\nwe have plan to merge small files in the future so we no longer need to set max_download_buffer_size=1 and have better performance.\r\n\r\ni'm closing this one. thanks"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-05T07:05:54Z",
        "body": "Please set `max_download_threads` to 1 so you explicitly disable parallel reading for such files and don't end up generating a lot of requests."
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-05T07:27:18Z",
        "body": "@jiayeZhu I found a better solution for those small files, try setting `remote_filesystem_read_prefetch` to `false`\r\nSeems like there is no need to add a new setting"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-05T13:49:57Z",
        "body": "thanks @antonio2368. i tried `settings remote_filesystem_read_prefetch=0` it works. will use that when needed. thanks!"
      }
    ]
  },
  {
    "number": 68596,
    "title": "Docker Image mount point for data not properly documented?",
    "created_at": "2024-08-20T09:42:44Z",
    "closed_at": "2024-08-20T10:24:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/68596",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nAccording to the docker image documentation, I have to mount\r\n\r\n`/var/lib/clickhouse/ - main folder where ClickHouse stores the data`\r\n\r\nfor persistence. This doesnt work. After a restart, the data is gone.\r\n\r\nInspecting the container, I can find the `/data` folder with the expected data, but contents are symlinked to `/store`.\r\n\r\nSo from my understanding, `/data`, `/store` and `/var/lib/clickhouse` have to be mounted somehow?\r\n\r\nIs that correct? I would like to get some background about this, as when I start clickhouse server locally, everything is being stored in the current folder.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/68596/comments",
    "author": "christo-ph",
    "comments": [
      {
        "user": "antaljanosbenjamin",
        "created_at": "2024-08-20T10:05:50Z",
        "body": "Both `data` and `store` directories should be inside `/var/lib/clickhouse`:\r\n```\r\nls -la ch_data/\r\ntotal 64\r\ndrwxr-xr-x 14 systemd-resolve systemd-journal 4096 Aug 20 10:03 .\r\ndrwxrwxr-x 22 ubuntu          ubuntu          4096 Aug 20 10:00 ..\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 access\r\ndrwxr-x---  4 systemd-resolve systemd-journal 4096 Aug 20 10:00 data\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 dictionaries_lib\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 flags\r\ndrwxr-xr-x  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 format_schemas\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 metadata\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 metadata_dropped\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 preprocessed_configs\r\n-rw-r-----  1 systemd-resolve systemd-journal   55 Aug 20 10:03 status\r\ndrwxr-x--- 11 systemd-resolve systemd-journal 4096 Aug 20 10:02 store\r\ndrwxr-xr-x  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 tmp\r\ndrwxr-xr-x  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 user_files\r\ndrwxr-x---  2 systemd-resolve systemd-journal 4096 Aug 20 10:00 user_scripts\r\n-rw-r-----  1 systemd-resolve systemd-journal   36 Aug 20 10:00 uuid\r\n```\r\n\r\nI tried to reproduce your issue by running command from the docker image readme and it works for me.\r\n\r\nWhich commands did you use?"
      },
      {
        "user": "christo-ph",
        "created_at": "2024-08-20T10:20:46Z",
        "body": "Hi thanks for your answer.\r\n\r\nI think the default config.xml was causing this storage behavior. It had a lot of sample configuration inside, as well as a splitter storage example which was active.\r\n\r\nWhen I mount my config.xml properly:\r\n\r\n```\r\nvolumes:\r\n      - ./config/config.xml:/etc/clickhouse-server/cofig.xml\r\n      - ./ch_data:/var/lib/clickhouse/\r\n      - ./ch_logs:/var/log/clickhouse-server/\r\n```\r\n\r\nit works as you have shown in your example!"
      }
    ]
  },
  {
    "number": 67976,
    "title": "When the clickhouse distributed table query, why only read very little data from the node\uff0cHow did this do\uff1f",
    "created_at": "2024-08-07T11:39:03Z",
    "closed_at": "2024-08-09T02:00:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/67976",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nAfter testing, I found that when the distributed table query data of Clickhouse is not simply, it is not simply to gather all data from other nodes to nodes that initiate requests.\r\n\r\nIn fact, there are very few data read from other nodes\uff0cBut I did not find related articles to explain its principle or algorithm.\r\n\r\nDo you have related articles or blogs?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/67976/comments",
    "author": "ChaoHsupin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-08-07T12:04:20Z",
        "body": "Explanation from chatgpt, it's close to the reality):\r\n\r\n-----\r\n\r\nIn ClickHouse, distributed tables are used to handle data that is spread across multiple servers (shards). The system allows for efficient querying and processing by distributing the workload across these shards and then combining the results. Here's a breakdown of how this works, especially concerning aggregate functions:\r\n\r\n### 1. **Distributed Table Structure**\r\n   - A **distributed table** is a virtual table that does not store data itself. Instead, it references physical tables (often replicated tables) on multiple shards.\r\n   - When a query is executed against a distributed table, ClickHouse automatically distributes the query to the underlying shards, which contain the actual data.\r\n\r\n### 2. **Aggregate Function States**\r\n   - Aggregate functions in ClickHouse can operate in multiple stages: **state calculation** and **finalization**.\r\n   - **State Calculation:** When an aggregate function is applied, it calculates an intermediate state (e.g., sum, average, count) rather than the final result. This intermediate state can be merged with other states later.\r\n   - **Finalization:** The final result is obtained by finalizing the merged states (e.g., computing the average from the sum and count).\r\n\r\n### 3. **Pre-calculation on Shards**\r\n   - When a query involves an aggregation, each shard independently calculates the **aggregate function states** for its portion of the data.\r\n   - This distributed pre-calculation reduces the amount of data that needs to be transferred across the network, as only the intermediate states (instead of raw data) need to be sent to the initiator node.\r\n\r\n### 4. **Finalization at the Initiator**\r\n   - The initiator node is the server that initially receives the query.\r\n   - After receiving the pre-calculated aggregate states from all shards, the initiator node **merges** these states.\r\n   - The initiator then **finalizes** the aggregate function by computing the final result from the merged states (e.g., summing counts and computing the final average).\r\n\r\n### 5. **Example Workflow**\r\n   1. A query requesting an aggregate, like `SELECT AVG(value) FROM distributed_table`, is issued.\r\n   2. The query is distributed to all shards.\r\n   3. Each shard calculates the **sum** and **count** for `value` and returns these states to the initiator.\r\n   4. The initiator merges the sums and counts from all shards.\r\n   5. The initiator finalizes the `AVG` by dividing the merged sum by the merged count.\r\n\r\n### 6. **Benefits**\r\n   - **Efficiency:** By performing most of the heavy lifting on the shards and only sending essential data back to the initiator, ClickHouse minimizes network overhead and speeds up query processing.\r\n   - **Scalability:** This approach allows ClickHouse to scale horizontally, effectively handling large datasets spread across multiple nodes.\r\n\r\nThis distributed approach is particularly powerful for large-scale data processing, making ClickHouse a strong choice for analytics on massive datasets."
      },
      {
        "user": "ChaoHsupin",
        "created_at": "2024-08-08T02:17:33Z",
        "body": "> Explanation from chatgpt, it's close to the reality):\r\n> \r\n> In ClickHouse, distributed tables are used to handle data that is spread across multiple servers (shards). The system allows for efficient querying and processing by distributing the workload across these shards and then combining the results. Here's a breakdown of how this works, especially concerning aggregate functions:\r\n> \r\n> ### 1. **Distributed Table Structure**\r\n> * A **distributed table** is a virtual table that does not store data itself. Instead, it references physical tables (often replicated tables) on multiple shards.\r\n> * When a query is executed against a distributed table, ClickHouse automatically distributes the query to the underlying shards, which contain the actual data.\r\n> \r\n> ### 2. **Aggregate Function States**\r\n> * Aggregate functions in ClickHouse can operate in multiple stages: **state calculation** and **finalization**.\r\n> * **State Calculation:** When an aggregate function is applied, it calculates an intermediate state (e.g., sum, average, count) rather than the final result. This intermediate state can be merged with other states later.\r\n> * **Finalization:** The final result is obtained by finalizing the merged states (e.g., computing the average from the sum and count).\r\n> \r\n> ### 3. **Pre-calculation on Shards**\r\n> * When a query involves an aggregation, each shard independently calculates the **aggregate function states** for its portion of the data.\r\n> * This distributed pre-calculation reduces the amount of data that needs to be transferred across the network, as only the intermediate states (instead of raw data) need to be sent to the initiator node.\r\n> \r\n> ### 4. **Finalization at the Initiator**\r\n> * The initiator node is the server that initially receives the query.\r\n> * After receiving the pre-calculated aggregate states from all shards, the initiator node **merges** these states.\r\n> * The initiator then **finalizes** the aggregate function by computing the final result from the merged states (e.g., summing counts and computing the final average).\r\n> \r\n> ### 5. **Example Workflow**\r\n> 1. A query requesting an aggregate, like `SELECT AVG(value) FROM distributed_table`, is issued.\r\n> 2. The query is distributed to all shards.\r\n> 3. Each shard calculates the **sum** and **count** for `value` and returns these states to the initiator.\r\n> 4. The initiator merges the sums and counts from all shards.\r\n> 5. The initiator finalizes the `AVG` by dividing the merged sum by the merged count.\r\n> \r\n> ### 6. **Benefits**\r\n> * **Efficiency:** By performing most of the heavy lifting on the shards and only sending essential data back to the initiator, ClickHouse minimizes network overhead and speeds up query processing.\r\n> * **Scalability:** This approach allows ClickHouse to scale horizontally, effectively handling large datasets spread across multiple nodes.\r\n> \r\n> This distributed approach is particularly powerful for large-scale data processing, making ClickHouse a strong choice for analytics on massive datasets.\r\n\r\nThanks for you explain and examples, helped me very much\uff01\r\n\r\nI have a question, I hope to get your help. For example, the SQL\uff1a\r\n```\r\nSELECT * FROM monitor.qunhe_log_all ORDER BY timestamp DESC LIMIT 50\r\n```\r\nThe distributed query below is to query 50 in each piece, and then merge on query node, and then sort to take the top 50?\r\nOr, when the nodes scan data, they will communicate with each other's current data status, and then only send a small amount of effective data until the query node.\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2024-08-08T13:47:52Z",
        "body": ">The distributed query below is to query 50 in each piece, and then merge on query node, and then sort to take the top 50?\r\n\r\nLike this. Each node reads data, finds 50 rows, sends them back to the query-initator, query-initator sort them again and leave only 50 rows.\r\n\r\n>Or, when the nodes scan data, they will communicate with each other's current data status, and then only send a small amount of effective data until the query node.\r\n\r\nNo, shards don't communicate. They are not aware about each other. They behave like a single (standalone) Clickhouse server, they execute usual query, there is no difference if they receive a query from Distributed table or from go/java/clickhouse-client client. \r\nDistributed table (query-initator) communicates with shards as a simple client, using the same TCP protocol and Native format the same way as go-library (go-lang client)."
      },
      {
        "user": "ChaoHsupin",
        "created_at": "2024-08-09T02:00:41Z",
        "body": "@den-crane Thanks for your help!"
      }
    ]
  },
  {
    "number": 67878,
    "title": "Compression and uncompression of data flowing over a network",
    "created_at": "2024-08-06T04:43:10Z",
    "closed_at": "2024-08-06T13:29:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/67878",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nI want to copy data between clickhouses using the REMOTE function.\r\nWill the data flow over the network in compressed or uncompressed form?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/67878/comments",
    "author": "v1tam1nb2",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-08-06T12:14:52Z",
        "body": "By default, compression is enabled and lz4 is used.\r\n\r\n--network_compression_method arg                                                                   Allows you to select the method of data compression when writing.\r\n--network_zstd_compression_level arg                                                               Allows you to select the level of ZSTD compression.\r\n\r\n```\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 0.772 sec. Processed 10.07 million rows, 80.58 MB (13.04 million rows/s., 104.36 MB/s.)\r\n\r\n\r\nset network_compression_method='none';\r\n\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 1.430 sec. Processed 10.01 million rows, 80.06 MB (7.00 million rows/s., 55.98 MB/s.)\r\n\r\n\r\nset network_compression_method='zstd', network_zstd_compression_level=22;\r\n\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 0.673 sec. Processed 10.01 million rows, 80.06 MB (14.86 million rows/s., 118.87 MB/s.)\r\n\r\n\r\nset network_compression_method='lz4';\r\n\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 0.763 sec. Processed 10.07 million rows, 80.58 MB (13.20 million rows/s., 105.61 MB/s.)\r\n```"
      },
      {
        "user": "v1tam1nb2",
        "created_at": "2024-08-06T13:29:52Z",
        "body": "Thank you for your detailed response.\r\nThis question is closed."
      }
    ]
  },
  {
    "number": 66925,
    "title": "mysql engine on_duplicate_clause usage",
    "created_at": "2024-07-23T10:29:10Z",
    "closed_at": "2024-07-24T11:34:08Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/66925",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nHi, I would like to ask why there are errors when using the MySQL engine to update data. I hope to receive your help\r\n\r\n```\r\nselect version();\r\n23.7.4.5\r\n```\r\n```\r\nCREATE NAMED COLLECTION mysql_test on cluster ck_cluster AS\r\n        host = 'xxx',\r\n        port = xxxx,\r\n        database = 'xxx',\r\n        user = 'xxx',\r\n        password = 'xxx' ,\r\nreplace_query = 0,\r\non_duplicate_clause = 1;\r\n```\r\n\r\n```\r\nCREATE TABLE test.test_01\r\n(\r\n    id UInt64 ,\r\n  package_name   Nullable(String) ,\r\n  app_name   Nullable(String) ,\r\n  source_app_name   Nullable(String) ,\r\n  update_time   Nullable(DateTime) ,\r\n  modify_time  Nullable(DateTime) \r\n) ENGINE = MySQL(mysql_test ,  table='cz_game_package_mapping')';\r\n```\r\n\r\n```\r\ninsert into test.test_01(id, package_name, app_name, source_app_name, update_time, modify_time)\r\nvalues( 775285052,'test_00',null, 'test_01', null,null) \r\nON DUPLICATE KEY UPDATE source_app_name = source_app_name;\r\n```\r\nerror message:\r\n`Code: 27. DB::ParsingException: Cannot parse input: expected '(' before: 'ON DUPLICATE KEY UPDATE source_app_name = source_app_name;':  at row 1: While executing ValuesBlockInputFormat: data for INSERT was parsed from query. (CANNOT_PARSE_INPUT_ASSERTION_FAILED)`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/66925/comments",
    "author": "12frame",
    "comments": [
      {
        "user": "pmusa",
        "created_at": "2024-07-23T20:49:03Z",
        "body": "Hey @12frame, it seems there is a bug when creating a table using named collections. When I try to create the table with `on_duplicate_clause=1`, clickhouse raises an error.\r\n\r\n```\r\nCREATE TABLE test_mysql3 (\r\n    id Int32,\r\n    name String\r\n) ENGINE = MySQL('localhost:3306', 'test', 'test', 'root', '', 0, 1);\r\n\r\nReceived exception:\r\nCode: 36. DB::Exception: Argument 'on_duplicate_clause' must be a literal with type String, got UInt64. (BAD_ARGUMENTS)\r\n ```\r\n\r\nBut indeed, if I do the same thing with named collections, no errors are raised.\r\n \r\nThe docs are not clear IMHO, but you need to define it as `on_duplicate_clause=UPDATE source_app_name = source_app_name`. And then, not pass anything when doing the insert: `insert into ... values ...;`\r\n\r\nBTW, your duplicate syntax didn't work as expected for me. You might need to look into MySQL correct syntax to update with the new value. It worked with a fixed string though: `on_duplicate_clause = 'UPDATE name=\\'duplicate\\''`."
      },
      {
        "user": "12frame",
        "created_at": "2024-07-24T02:02:29Z",
        "body": "Thank you very much. You can now work normally\r\nThe description in the document is indeed very vague, and there are no relevant cases to describe it. I hope you can supplement and explain it to him. Thank you"
      }
    ]
  },
  {
    "number": 65392,
    "title": "Restrict a clickhouse user with limited permission to execute queries on system database",
    "created_at": "2024-06-18T13:23:40Z",
    "closed_at": "2024-06-18T15:41:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/65392",
    "body": "### Company or project name\n\nSelf\n\n### Question\n\nQuestion : How to restrict a clickhouse user with limited permission to execute queries on tables under system database\r\n\r\nSteps:\r\n1) Create a clickhouse db : \r\n    CREATE DATABASE IF NOT EXISTS <db> ON CLUSTER <cluster>\r\n2) Create a clickhouse db user with limited permission :  \r\n    CREATE USER IF NOT EXISTS <username> ON CLUSTER <cluster> IDENTIFIED BY '$password'\r\n    GRANT SELECT ON <database>.* TO <username>\r\n\r\nWith above configuration, the user is still table to query data from tables under system database e.g. select version() , select * from system.settings\r\n    ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/65392/comments",
    "author": "nnaik25",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-06-18T15:13:59Z",
        "body": "it's impossible to restrict an access to `select version()` , `system.settings` because they are needed for clients libraries, and they don't expose sensitive information."
      },
      {
        "user": "nnaik25",
        "created_at": "2024-06-18T15:41:47Z",
        "body": "Thanks for your prompt response"
      }
    ]
  },
  {
    "number": 65199,
    "title": "Temporary stopping automatic dictionary updates",
    "created_at": "2024-06-13T10:23:46Z",
    "closed_at": "2024-06-17T09:50:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/65199",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nWhen a dictionary is based on a clickhouse table with a lifetime, what should be done when updating the source table (truncate table source_table; insert into source_table...) to prevent clickhouse from updating the dictionary with partial or no data?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/65199/comments",
    "author": "bjne",
    "comments": [
      {
        "user": "Algunenano",
        "created_at": "2024-06-13T10:39:21Z",
        "body": "As far as I know, there is no way to stop a dictionary reload.\r\n\r\nMaybe you could avoid having partial data in the source table. For example, instead of truncating + insert, you could create a temporal table with the same schema, insert into that table, then EXCHANGE it with the old one and drop the temporal table (that now has the old data). Would that help in your situation?"
      },
      {
        "user": "UnamedRus",
        "created_at": "2024-06-13T10:58:44Z",
        "body": "> what should be done\r\n\r\nAwful hack, but you can add WHERE throwIf((SELECT * FROM status FINAL) = 'Updating' ) in dictionary source definition\r\n\r\nSo, you will have additional status table, which you can use as lock for your dictionary updates. (to explicitly trigger exception which prevent reload of dictionary)\r\n\r\nBut, it's not better than doing EXCHANGE TABLES and staging tables "
      },
      {
        "user": "bjne",
        "created_at": "2024-06-13T11:10:39Z",
        "body": "Thanks to the both of you. I think I will go for the table exchange, but I like your style, @UnamedRus :laughing: "
      }
    ]
  },
  {
    "number": 63100,
    "title": "Not executing fetch of part xxx because 8 fetches already executing, max 8",
    "created_at": "2024-04-29T04:17:38Z",
    "closed_at": "2024-04-30T14:44:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/63100",
    "body": "My ch version\r\n\r\nClickHouse client version 23.8.8.20 (official build).\r\n\r\nI have a lot of queues \uff0cI want to set background_fetches_pool_size = 32 but not work/\r\n\r\n```python\r\nSELECT\r\n    database,\r\n    table,\r\n    type,\r\n    max(last_exception),\r\n    max(postpone_reason),\r\n    min(create_time),\r\n    max(last_attempt_time),\r\n    max(last_postpone_time),\r\n    max(num_postponed) AS max_postponed,\r\n    max(num_tries) AS max_tries,\r\n    min(num_tries) AS min_tries,\r\n    countIf(last_exception != '') AS count_err,\r\n    countIf(num_postponed > 0) AS count_postponed,\r\n    countIf(is_currently_executing) AS count_executing,\r\n    count() AS count_all\r\nFROM system.replication_queue\r\nGROUP BY\r\n    database,\r\n    table,\r\n    type\r\nORDER BY count_all DESC\r\n\r\nQuery id: 345b6e7c-e993-4227-bc60-939ac2ee23a7\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u252c\u2500max(last_exception)\u2500\u252c\u2500max(postpone_reason)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500min(create_time)\u2500\u252c\u2500max(last_attempt_time)\u2500\u252c\u2500max(last_postpone_time)\u2500\u252c\u2500max_postponed\u2500\u252c\u2500max_tries\u2500\u252c\u2500min_tries\u2500\u252c\u2500count_err\u2500\u252c\u2500count_postponed\u2500\u252c\u2500count_executing\u2500\u252c\u2500count_all\u2500\u2510\r\n\u2502 xxx    \u2502 xxx \u2502 GET_PART \u2502                     \u2502 Not executing fetch of part ff8d5acf92437a06b529a9152e275fbc_4379_4379_0 because 8 fetches already executing, max 8. \u2502 2024-04-27 22:51:46 \u2502    2024-04-29 12:15:05 \u2502     2024-04-29 12:15:08 \u2502          2221 \u2502         1 \u2502         0 \u2502         0 \u2502          673114 \u2502               1 \u2502    673114 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT\r\n    type,\r\n    count(*)\r\nFROM system.replication_queue\r\nGROUP BY type\r\n\r\nQuery id: 0b339b1e-323d-4069-b2a8-8fc8222c65b3\r\n\r\n\u250c\u2500type\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 GET_PART \u2502  672841 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE name IN ('background_fetches_pool_size', 'background_schedule_pool_size', 'background_pool_size')\r\n\r\nQuery id: 5136cca9-d3e9-4682-9125-3a9c6628a240\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2510\r\n\u2502 background_pool_size          \u2502 16    \u2502\r\n\u2502 **\r\n\r\n> **background_fetches_pool_size**\r\n\r\n**  \u2502 16    \u2502\r\n\u2502 background_schedule_pool_size \u2502 128   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n``` \r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/63100/comments",
    "author": "kernel288",
    "comments": [
      {
        "user": "cangyin",
        "created_at": "2024-04-29T06:35:46Z",
        "body": "> ```\r\n> SELECT\r\n>     name,\r\n>     value\r\n> FROM system.settings\r\n> WHERE name IN ('background_fetches_pool_size', 'background_schedule_pool_size', 'background_pool_size')\r\n> ```\r\n\r\nThe pool size settings are server settings. Values in `system.settings` with same names are deprecated.\r\n\r\nCheck `system.server_settings` or `/var/lib/clickhouse/preprocessed_configs/config.xml`"
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T06:45:21Z",
        "body": "> system.server_settings\r\n\r\nThank U for reply\r\n\r\n```python\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE name LIKE '%background%'\r\n\r\nQuery id: 38dc8001-dd92-4adc-aba5-0026dd7115b5\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 background_pool_size                          \u2502 16          \u2502\r\n\u2502 background_merges_mutations_concurrency_ratio \u2502 2           \u2502\r\n\u2502 background_merges_mutations_scheduling_policy \u2502 round_robin \u2502\r\n\u2502 background_move_pool_size                     \u2502 8           \u2502\r\n\u2502 background_fetches_pool_size                  \u2502 8           \u2502\r\n\u2502 background_common_pool_size                   \u2502 8           \u2502\r\n\u2502 background_buffer_flush_schedule_pool_size    \u2502 16          \u2502\r\n\u2502 background_schedule_pool_size                 \u2502 128         \u2502\r\n\u2502 background_message_broker_schedule_pool_size  \u2502 16          \u2502\r\n\u2502 background_distributed_schedule_pool_size     \u2502 16          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n10 rows in set. Elapsed: 0.004 sec. \r\n\r\ncat /etc/clickhouse-server/users.xml \r\n\r\n<?xml version=\"1.0\"?>\r\n<clickhouse>\r\n    <!-- See also the files in users.d directory where the settings can be overridden. -->\r\n\r\n    <!-- Profiles of settings. -->\r\n    <profiles>\r\n        <!-- Default settings. -->\r\n        <default>\r\n            <background_fetches_pool_size>16</background_fetches_pool_size>\r\n\r\n``` \r\n\r\n**It's still not work** \r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "cangyin",
        "created_at": "2024-04-29T06:49:43Z",
        "body": "```xml\r\n<clickhouse>\r\n    ...\r\n    <background_fetches_pool_size>16</background_fetches_pool_size>\r\n    ...\r\n</clickhouse>\r\n```\r\n\r\nIt's a server setting, should be placed under `<clickhouse>`, not inside profile settings (or user settings).\r\n"
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T07:06:56Z",
        "body": "> ```\r\n>     <background_fetches_pool_size>16</background_fetches_pool_size>\r\n> ```\r\n\r\nThank U  for reply\r\n\r\n```python\r\n[root@SHPL007176031 ~]# head -n 15  /etc/clickhouse-server/users.xml \r\n<?xml version=\"1.0\"?>\r\n<clickhouse>\r\n    <!-- See also the files in users.d directory where the settings can be overridden. -->\r\n    <background_fetches_pool_size>16</background_fetches_pool_size>\r\n    <background_pool_size>96</background_pool_size>\r\n    <!-- Profiles of settings. -->\r\n    <profiles>\r\n        <!-- Default settings. -->\r\n        <default>\r\n            <!-- Maximum memory usage for processing single query, in bytes. -->\r\n            <max_memory_usage>100000000000</max_memory_usage>\r\n            <max_partitions_per_insert_block>5000000</max_partitions_per_insert_block>\r\n            <max_insert_block_size>100000000</max_insert_block_size>\r\n            <min_insert_block_size_rows>100000000</min_insert_block_size_rows>\r\n            <min_insert_block_size_bytes>500000000</min_insert_block_size_bytes>\r\n\r\n\r\nClickHouse client version 23.8.8.20 (official build).\r\nConnecting to database ztmdb at 127.0.0.1:9000 as user default.\r\nConnected to ClickHouse server version 23.8.8 revision 54465.\r\n\r\nWarnings:\r\n * Table system.session_log is enabled. It's unreliable and may contain garbage. Do not use it for any kind of security monitoring.\r\n\r\nSHPL007176031 :) select name,value from system.server_settings where name like '%background%' ;\r\n\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE name LIKE '%background%'\r\n\r\nQuery id: afedd27a-f4ce-4e21-8e5d-f8c560b42581\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 background_pool_size                          \u2502 16          \u2502\r\n\u2502 background_merges_mutations_concurrency_ratio \u2502 2           \u2502\r\n\u2502 background_merges_mutations_scheduling_policy \u2502 round_robin \u2502\r\n\u2502 background_move_pool_size                     \u2502 8           \u2502\r\n\u2502 background_fetches_pool_size                  \u2502 8           \u2502\r\n\u2502 background_common_pool_size                   \u2502 8           \u2502\r\n\u2502 background_buffer_flush_schedule_pool_size    \u2502 16          \u2502\r\n\u2502 background_schedule_pool_size                 \u2502 128         \u2502\r\n\u2502 background_message_broker_schedule_pool_size  \u2502 16          \u2502\r\n\u2502 background_distributed_schedule_pool_size     \u2502 16          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n``` \r\n\r\n**I add the server settings . But still not work** "
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T07:38:54Z",
        "body": "> ```\r\n> <clickhouse>\r\n>     ...\r\n>     <background_fetches_pool_size>16</background_fetches_pool_size>\r\n>     ...\r\n> </clickhouse>\r\n> ```\r\n> \r\n> It's a server setting, should be placed under `<clickhouse>`, not inside profile settings (or user settings).\r\n\r\nYes u are right .\r\n\r\nI use the old config.xml  . I upgrade ck version . i use the old config .\r\n\r\n```python\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE name LIKE '%background%'\r\n\r\nQuery id: ad8ced44-4f07-4327-adc0-7f65f4aab3d9\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 background_pool_size                          \u2502 128         \u2502\r\n\u2502 background_merges_mutations_concurrency_ratio \u2502 2           \u2502\r\n\u2502 background_merges_mutations_scheduling_policy \u2502 round_robin \u2502\r\n\u2502 background_move_pool_size                     \u2502 16          \u2502\r\n\u2502 background_fetches_pool_size                  \u2502 16          \u2502\r\n\u2502 background_common_pool_size                   \u2502 16          \u2502\r\n\u2502 background_buffer_flush_schedule_pool_size    \u2502 32          \u2502\r\n\u2502 background_schedule_pool_size                 \u2502 256         \u2502\r\n\u2502 background_message_broker_schedule_pool_size  \u2502 32          \u2502\r\n\u2502 background_distributed_schedule_pool_size     \u2502 32          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n``` "
      },
      {
        "user": "den-crane",
        "created_at": "2024-04-29T12:06:30Z",
        "body": "> /etc/clickhouse-server/users.xml \r\n\r\npool settings are in config.xml now"
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T12:23:14Z",
        "body": "> > /etc/clickhouse-server/users.xml\r\n> \r\n> pool settings are in config.xml now\r\n\r\nThis is fix it . thank u "
      }
    ]
  },
  {
    "number": 59780,
    "title": "[Question]: Working with File with Native format, how to create an empty file?",
    "created_at": "2024-02-08T20:37:35Z",
    "closed_at": "2024-02-08T21:02:08Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/59780",
    "body": "In CSVWithNames it's simple, just need to add headers and put the file in the `/data/default/[table]/data.CSVWithNames` after that I could select it without any problem:\r\n\r\n```bash\r\n# 1. Set up the file_engine_table table:\r\nCREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(CSVWithNames);\r\n\r\n# 2. Manually create /var/lib/clickhouse/data/default/file_engine_table/data.CSVWithNames containing:\r\n$ cat data.CSVWithNames \r\nname,value\r\n\r\n# 3. Query the data:\r\nSELECT *\r\nFROM file_engine_table\r\n\r\nQuery id: 5a865226-6674-4190-a0a6-2d606f823f17\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.008 sec.\r\n```\r\n\r\nBut with `Native` I don't know if it's ok to just touch the file ( 0 bytes ).",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/59780/comments",
    "author": "mirusky",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2024-02-08T21:01:13Z",
        "body": "Yes, an empty `Native` file is a valid representation of an empty dataset of any structure.\r\n\r\nExample:\r\n```\r\n$ touch test.native\r\n$ clickhouse-local --query \"SELECT * FROM file('test.native', Native, 'x String')\"\r\n$ clickhouse-local --query \"SELECT * FROM file('test.native', Native, 'x String, y Array(UInt64)')\"\r\n```\r\n\r\nHowever an empty file does not contain the structure, so it has to be specified manually:\r\n```\r\n$ clickhouse-local --query \"SELECT * FROM file('test.native')\"\r\nCode: 636. DB::Exception: Cannot extract table structure from Native format file, file is empty. You can specify the structure manually: (in file/uri /home/milovidov/work/clickhouse-presentations/test.native). (CANNOT_EXTRACT_TABLE_STRUCTURE)\r\n```\r\n\r\nYou can also create a `Native` file with a structure, but with zero size of every column - and it will represent an empty dataset of a particular structure:\r\n\r\n```\r\n$ xxd test4.native \r\n00000000: 0200 0178 0653 7472 696e 6701 7905 5549  ...x.String.y.UI\r\n00000010: 6e74 38                                  nt8\r\n```\r\n\r\n```\r\n02 - two columns\r\n00 - zero rows\r\n01 - the length of the name of the first column 'x'\r\n78 - the name of the first column 'x'\r\n06 - the length of the name of the type of first column 'x'\r\nString - the type of first column 'x'\r\nno data - the serialized String column with 0 rows\r\n01 - the length of the name of the second column 'y'\r\n79 - the name of the second column 'y'\r\n05 - the length of the name of the type of second column 'y'\r\nUInt8 - the type of second column 'y'\r\nno data - the serialized UInt8 column with 0 rows\r\n```\r\n\r\n```\r\n$ clickhouse-local --query \"DESCRIBE file('test4.native')\"\r\nx       String\r\ny       UInt8\r\n$ clickhouse-local --query \"SELECT * FROM file('test4.native')\"\r\n```\r\n"
      },
      {
        "user": "mirusky",
        "created_at": "2024-02-09T15:03:41Z",
        "body": "Thank you for clarifying, just to be sure I understood it. \r\n\r\n```sh\r\n# data.Native\r\n0300 0161 0653 7472 696e 6701 6205 5549 \r\n6e74 3805 6305 496e 74\r\n```\r\n\r\nIs equivalent to a 3 column, with 0 rows, with a String b UInt8 c Int right? Or I messed up the file?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2024-02-09T22:38:55Z",
        "body": "It is messed up.\r\nI've formatted your file:\r\n\r\n```\r\n03\r\n00 \r\n01\r\n   61 \r\n06 \r\n   53 74 72 69 6e 67\r\n01 \r\n   62\r\n05 \r\n   55 49 6e 74 38\r\n05 \r\n   63 05 49 6e 74\r\n```\r\n\r\nBut it should be:   \r\n```\r\n03\r\n00 \r\n01\r\n   61 \r\n06 \r\n   53 74 72 69 6e 67\r\n01 \r\n   62\r\n05 \r\n   55 49 6e 74 38\r\n01 \r\n   63\r\n03 \r\n   49 6e 74\r\n```\r\n"
      }
    ]
  },
  {
    "number": 59251,
    "title": "Port is not opened on keepers only using composable protocols",
    "created_at": "2024-01-26T08:54:26Z",
    "closed_at": "2024-01-26T10:31:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/59251",
    "body": "\r\n**Description**\r\nWhen using the composable protocols configuration for serving prometheus metrics with tls, clickhouse server with only keeper configuration does not open specified port.\r\n\r\n**How to reproduce**\r\n* Clickhouse version 23.3.19.32\r\n\r\nTo reproduce the issue two nodes should be deployed, one as \"data\" node and the other one as a keeper.\r\nExample config:\r\n```xml\r\n<remote_servers>\r\n<example>\r\n  <shard>\r\n    <internal_replication>true</internal_replication>\r\n    <replica>\r\n      <host>data-node.example.com</host>\r\n      <port>9000</port>\r\n      <secure>true</secure>\r\n    </replica>\r\n  </shard>\r\n</example>\r\n</remote_servers>\r\n\r\n<keeper_server>\r\n  <server_id>1</server_id>\r\n  <tcp_port_secure>2181</tcp_port_secure>\r\n  <raft_configuration>\r\n      <secure>true</secure>\r\n\r\n      <server>\r\n          <id>1</id>\r\n          <hostname>keeper-node.example.com</hostname>\r\n          <port>2182</port>\r\n      </server>\r\n  </raft_configuration>\r\n</keeper_server>\r\n\r\n\r\n```\r\n\r\nIn both configs I do specify prometheus metrics endpoint using composable protocols following explanation given in this issue #49003:\r\n```xml\r\n<prometheus>\r\n  <endpoint>/metrics</endpoint>\r\n  <metrics>true</metrics>\r\n  <events>true</events>\r\n  <asynchronous_metrics>true</asynchronous_metrics>\r\n</prometheus>\r\n\r\n<protocols>\r\n  <prometheus_protocol>\r\n      <type>prometheus</type>\r\n      <description>prometheus protocol</description>\r\n  </prometheus_protocol>\r\n\r\n  <prometheus_secure>\r\n      <type>tls</type>\r\n      <impl>prometheus_protocol</impl>\r\n      <port>9999</port>\r\n      <description>prometheus over tls</description>\r\n  </prometheus_secure>\r\n</protocols>\r\n\r\n```\r\n\r\n\r\n**Expected behavior**\r\nPort 9999 is opened on both \"data\" node and keeper node and is under TLS.\r\n\r\n**Actual behavior**\r\nPort 9999 is only opened on the \"data\" and not the keeper node.\r\n\r\n**Logs**\r\nThere were no messages in log with a mention of this prometheus configuration, checked it on trace level.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/59251/comments",
    "author": "ilyatrefilov",
    "comments": [
      {
        "user": "antonio2368",
        "created_at": "2024-01-26T09:51:44Z",
        "body": "Keeper currently doesn't support composable protocols so the \"old\" way should be used of specifying `<port>` under `<prometheus>`."
      },
      {
        "user": "ilyatrefilov",
        "created_at": "2024-01-26T10:13:15Z",
        "body": "@antonio2368 Thank you for the clarification on this point.\r\nIs there any chance of composable protocols support on keepers in near future or it's better to look for workaround in long term?\r\nSeems like there are no out-of-box solutions left to collect metrics in secure way."
      },
      {
        "user": "antonio2368",
        "created_at": "2024-01-26T10:31:15Z",
        "body": "@ilyatrefilov I can't promise you anything specific but I think it should be done in the near future along with some small changes to Keeper prometheus metrics (send only Keeper specific data).\r\n\r\nI'll close this issue for now and ping you when there is update for it."
      }
    ]
  },
  {
    "number": 58932,
    "title": "Does the Kafka engine automatically decompress messages when LZ4 compression is used?",
    "created_at": "2024-01-18T02:03:41Z",
    "closed_at": "2024-01-18T08:42:01Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/58932",
    "body": "Does the Kafka engine automatically decompress messages when LZ4 compression is used? I am currently using Azure Event Hubs and hoping to reduce data processing fees.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/58932/comments",
    "author": "Bamboo-devops",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2024-01-18T08:09:48Z",
        "body": "It you mean the compression on the level of the topic - yes, it should work out of the box."
      },
      {
        "user": "Bamboo-devops",
        "created_at": "2024-01-19T03:40:03Z",
        "body": "Do you mean the kafka engine is already getting messages with compression enabled? "
      },
      {
        "user": "Algunenano",
        "created_at": "2024-01-19T12:35:04Z",
        "body": "The kafka engine receives messages. If the messages are compressed (by whoever generates them) then it decompress them."
      },
      {
        "user": "Bamboo-devops",
        "created_at": "2024-01-24T02:17:13Z",
        "body": "thank you"
      }
    ]
  },
  {
    "number": 58030,
    "title": "distributed table can't query in cluster dicovery mode",
    "created_at": "2023-12-19T12:27:07Z",
    "closed_at": "2023-12-19T13:29:52Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/58030",
    "body": "(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\nI trying to deploy clickhouse in cluster dicovery mode, but it seems the data can't be query out.\r\n\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use -- 23.4\r\n\r\nI deploy test cluster by docker-compose, manifest is like below. In summary:\r\n - Cluster contains three node: chnode1-3, which are organized in cluster-discovery mode\r\n - create local table and distributed table on cluster\r\n - after insert a single row into local table in chnode-1, expect to query that out in chnode2 and chnode3 by distributed table. But got nothing.\r\n - What's more, I can get data when I using manual-config to deploy cluster.\r\n\r\n## Test env\r\n### common part\r\n#### docker-compose.yml\r\n```yaml\r\nversion: '3.3'\r\nservices:\r\n  zookeeper:\r\n    image: zookeeper:3.4.9\r\n    container_name: ch-zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 500M\r\n    environment:\r\n      - TZ=Asia/Shanghai\r\n      - ZOO_MY_ID=1\r\n  chnode1:\r\n    image: clickhouse/clickhouse-server:23.4\r\n    container_name: chnode1\r\n    depends_on:\r\n      - zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 1G\r\n    volumes:\r\n      - type: 'bind'\r\n        source: './config/remote-servers.xml'\r\n        target: '/etc/clickhouse-server/config.d/remote-servers.xml'\r\n      - type: 'bind'\r\n        source: './config/use-keeper.xml'\r\n        target: '/etc/clickhouse-server/config.d/use-keeper.xml'\r\n  chnode2:\r\n    image: clickhouse/clickhouse-server:23.4\r\n    container_name: chnode2\r\n    depends_on:\r\n      - zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 1G\r\n    volumes:\r\n      - type: 'bind'\r\n        source: './config/remote-servers.xml'\r\n        target: '/etc/clickhouse-server/config.d/remote-servers.xml'\r\n      - type: 'bind'\r\n        source: './config/use-keeper.xml'\r\n        target: '/etc/clickhouse-server/config.d/use-keeper.xml'\r\n  chnode3:\r\n    image: clickhouse/clickhouse-server:23.4\r\n    container_name: chnode3\r\n    depends_on:\r\n      - zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 1G\r\n    volumes:\r\n      - type: 'bind'\r\n        source: './config/remote-servers.xml'\r\n        target: '/etc/clickhouse-server/config.d/remote-servers.xml'\r\n      - type: 'bind'\r\n        source: './config/use-keeper.xml'\r\n        target: '/etc/clickhouse-server/config.d/use-keeper.xml'\r\n  ```\r\n#### ./config/use-keeper.xml\r\n```xml\r\n<clickhouse>\r\n    <zookeeper>\r\n        <node index=\"1\">\r\n            <host>ch-zookeeper</host>\r\n            <port>2181</port>\r\n        </node>\r\n    </zookeeper>\r\n</clickhouse>\r\n```\r\n\r\n### deploy with cluster discovery\r\n#### ./config/remote-servers.xml\r\n```xml\r\n<clickhouse>\r\n    <allow_experimental_cluster_discovery>1</allow_experimental_cluster_discovery>\r\n    <remote_servers replace=\"true\">\r\n        <cluster_NS_1R>\r\n            <discovery>\r\n                <internal_replication>false</internal_replication>\r\n                <path>/clickhouse/discovery/cluster_NS_1R</path>\r\n                <shard>1</shard>\r\n            </discovery>\r\n        </cluster_NS_1R>\r\n    </remote_servers>\r\n</clickhouse>\r\n```\r\n\r\n### deploy with manual cluster setting\r\n#### ./config/remote-servers.xml\r\n```xml\r\n<clickhouse>\r\n    <remote_servers replace=\"true\">\r\n        <cluster_NS_1R>\r\n            <secret>mysecretphrase</secret>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>chnode1</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>chnode2</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>chnode3</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n        </cluster_NS_1R>\r\n    </remote_servers>\r\n</clickhouse>\r\n```\r\n\r\n## Test to query from distributed table after insert into local table\r\n```\r\n-- on chnode1\r\nCREATE DATABASE db1 ON CLUSTER cluster_NS_1R;\r\n\r\nCREATE TABLE db1.table1 ON CLUSTER cluster_NS_1R\r\n(\r\n    `id` UInt64,\r\n    `column1` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nCREATE TABLE db1.table1_dist ON CLUSTER cluster_NS_1R\r\n(\r\n    `id` UInt64,\r\n    `column1` String\r\n)\r\nENGINE = Distributed('cluster_NS_1R', 'db1', 'table1', rand());\r\n\r\nINSERT INTO db1.table1 (id, column1) VALUES (3, 'abc');\r\n\r\nselect * from db1.table1_dist; -- return one row, is ok\r\n\r\n\r\n-- on chnode2 or chnode3\r\nselect * from db1.table1_dist; -- return none in cluster-discover mode, but ok in manual-config\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/58030/comments",
    "author": "gogodjzhu",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-12-19T13:29:52Z",
        "body": "23.4 is out of support\r\n\r\nClickhouse Operator is not supported here. Ask in the Operator github repo."
      },
      {
        "user": "UnamedRus",
        "created_at": "2023-12-19T13:32:31Z",
        "body": "> Clickhouse Operator \r\n\r\nWhere is operator definition? "
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-19T13:45:34Z",
        "body": "> > Clickhouse Operator\r\n> \r\n> Where is operator definition?\r\n\r\nmy bad. I misread it. "
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-19T13:46:00Z",
        "body": "I'll try to reproduce it with the docker"
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-19T15:05:33Z",
        "body": "`deploy with manual cluster setting` -- creates a cluster with 3 shards.\r\n`deploy with cluster discovery` -- creates a cluster with 1 shard and 3 replicas without using replicated tables."
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-19T15:09:25Z",
        "body": "And it's funny but if you will insert into distributed table, data will be replicated by a distributed table  `<internal_replication>false</internal_replication>`\r\n\r\n```\r\nINSERT INTO db1.table1_dist (id, column1) VALUES (3, 'abc');\r\n```"
      },
      {
        "user": "gogodjzhu",
        "created_at": "2023-12-22T08:56:24Z",
        "body": "Thank you, @den-crane and @UnamedRus, for your valuable assistance.\r\n\r\nI previously found the shard property confusing. From my current understanding, the shard is a unique identifier for a shard. If multiple nodes start with the same shard number (e.g., '1' in my example above), they act as replicas for each other.\r\n\r\nTherefore, I have updated the configuration as follows, incorporating an environment variable named IP_NUM, which is an integer computed based on the host IP. This setup is designed to organize a cluster with a single shard (without replicas) on each node. The absence of replicas is intentional as they are unnecessary. This configuration also appears to be convenient for scaling up in a Kubernetes environment. If there are any deficiencies or suggestions, please inform me.\r\n\r\n```\r\n<clickhouse>\r\n    <allow_experimental_cluster_discovery>1</allow_experimental_cluster_discovery>\r\n    <remote_servers replace=\"true\">\r\n        <single>\r\n            <discovery>\r\n                <path>/clickhouse/discovery/logs</path>\r\n                <shard from_env=\"IP_NUM\"/>\r\n            </discovery>\r\n        </single>\r\n    </remote_servers>\r\n</clickhouse>\r\n```\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 57382,
    "title": "Distributed queries is giving inconsistency output between replicas and shards",
    "created_at": "2023-11-30T10:02:08Z",
    "closed_at": "2023-12-05T12:23:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/57382",
    "body": "I have 3 shards and 2 replicas of clickhouse cluster and also zookeeper with 3 znodes for replication in my eks cluster. In that, I created a database called \"app\" with the replicated engine in all shards and replicas (i.e., in all 6 pods). After I created a mergeTree table called \"alerts_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"alerts\" for inserting and retrieving the data evenly in all pods.\r\n\r\nClickhouse config:\r\n`\r\n\r\n```\r\napiVersion: \"clickhouse.altinity.com/v1\"\r\nkind: \"ClickHouseInstallation\"\r\nmetadata:\r\n  name: \"clickhouse\"\r\nspec:\r\n  defaults:\r\n    replicasUseFQDN: \"no\"\r\n    distributedDDL:\r\n      profile: admin_profile\r\n    templates:\r\n      podTemplate: clickhouse-pod\r\n      dataVolumeClaimTemplate: clickhouse-storage\r\n      serviceTemplate: clickhouse-svc\r\n  configuration:\r\n    users:\r\n      default/networks/host_regexp: clickhouse.svc.cluster.local$\r\n      default/networks/ip:\r\n        - 127.0.0.1\r\n        - 0.0.0.0\r\n      admin/networks/host_regexp: clickhouse.svc.cluster.local$\r\n      admin/networks/ip:\r\n        - 127.0.0.1\r\n        - 0.0.0.0\r\n      admin/k8s_secret_password: clickhouse/clickhouse-operator-altinity-clickhouse-operator/password\r\n      admin/access_management: 1\r\n      admin/named_collection_control: 1\r\n      admin/show_named_collections: 1\r\n      admin/show_named_collections_secrets: 1\r\n      admin/profile: admin_profile\r\n      admin/quote: admin_quote\r\n    profiles:\r\n      admin_profile/max_memory_usage: 600000000000\r\n      admin_profile/readonly: 0\r\n      admin_profile/max_insert_threads: 32\r\n    quotas:\r\n      admin_quota/interval/duration: 3600\r\n      admin_quota/interval/queries: 0\r\n      admin_quota/interval/errors: 0\r\n      admin_quota/interval/result_rows: 0\r\n      admin_quota/interval/read_rows: 0\r\n      admin_quota/interval/execution_time: 0\r\n    settings:\r\n      disable_internal_dns_cache: 1\r\n    files:\r\n      conf.d/chop-generated-storage.xml: |\r\n        <yandex>\r\n            <storage_configuration>\r\n                <disks>\r\n                    <default>\r\n                        <metadata_path>/var/lib/clickhouse/</metadata_path>\r\n                    </default>\r\n                    <fast_ssd>\r\n                        <path>/mnt/fast_ssd/clickhouse/</path>\r\n                    </fast_ssd>\r\n                    <s3>\r\n                        <type>s3</type>\r\n                        <endpoint>S3 URL</endpoint>\r\n                        <use_environment_credentials>true</use_environment_credentials>\r\n                        <metadata_path>/var/lib/clickhouse/disks/s3/</metadata_path>\r\n                    </s3>\r\n                </disks>\r\n                <policies>\r\n                    <moving_from_hot_to_cold>\r\n                        <volumes>\r\n                            <default>\r\n                                <disk>default</disk>\r\n                            </default>\r\n                            <hot>\r\n                                <disk>fast_ssd</disk>\r\n                            </hot>\r\n                            <cold>\r\n                                <disk>s3</disk>\r\n                            </cold>\r\n                        </volumes>\r\n                    </moving_from_hot_to_cold>\r\n            \t</policies>\r\n            </storage_configuration>\r\n        </yandex>\r\n    zookeeper:\r\n      nodes:\r\n        - host: zookeeper-0.zookeepers.zoo-keeper\r\n          port: 2181\r\n        - host: zookeeper-1.zookeepers.zoo-keeper\r\n          port: 2181\r\n        - host: zookeeper-2.zookeepers.zoo-keeper\r\n          port: 2181\r\n    clusters:\r\n      - name: prod-cluster\r\n        templates:\r\n          podTemplate: clickhouse-pod\r\n          dataVolumeClaimTemplate: clickhouse-storage\r\n        layout:\r\n          shardsCount: 3\r\n          replicasCount: 2\r\n          shards:\r\n            - name: shard0\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n            - name: shard1\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n            - name: shard2\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n  templates:\r\n    serviceTemplates:\r\n      - name: clickhouse-svc\r\n        spec:\r\n          ports:\r\n            - name: http\r\n              port: 8123\r\n            - name: tcp\r\n              port: 9000\r\n          externalTrafficPolicy: Cluster\r\n          type: LoadBalancer\r\n    podTemplates:\r\n      - name: clickhouse-pod\r\n        spec:\r\n          containers:\r\n            - name: clickhouse-pod\r\n              image: clickhouse/clickhouse-server:23.7\r\n              resources:\r\n                limits:\r\n                  cpu: \"3\"\r\n                  memory: 8Gi\r\n                requests:\r\n                  cpu: \"2\"\r\n                  memory: 6Gi\r\n              volumeMounts:\r\n                - name: clickhouse-storage\r\n                  mountPath: /var/lib/clickhouse\r\n    volumeClaimTemplates:\r\n      - name: clickhouse-storage\r\n        spec:\r\n          storageClassName: default\r\n          accessModes:\r\n            - ReadWriteOnce\r\n          resources:\r\n            requests:\r\n              storage: 100Gi\r\n```\r\n\r\n`\r\n              \r\n I have inserted million of data in \"alerts\" table in one month. But when I try select number of count() in alerts table where source='high', it giving inconsistency output in replicas and shards\r\n \r\n In 3 pods, chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0 it giving below output\r\n \r\n```\r\n SELECT count()\r\nFROM alerts\r\nWHERE source = 'high'\r\n\r\nQuery id: 81a6ed42-5583-4b9d-957b-9a69bccbac40\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    1389 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.232 sec. Processed 56.50 million rows, 1.02 GB (243.85 million rows/s., 4.39 GB/s.)\r\n```\r\n\r\nIn 3 pods, chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0, it giving below output\r\n\r\n```\r\nSELECT count()\r\nFROM alerts\r\nWHERE source = 'high'\r\n\r\nQuery id: 4c07a903-7ad6-4c92-bbc2-1ac86dcf8691\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    1489 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.216 sec. Processed 56.96 million rows, 1.03 GB (263.13 million rows/s., 4.74 GB/s.)\r\n```\r\n\r\n\r\nWhy there is inconsistency in output between shards and replicas? Can anyone from clickhouse help here.. what might be cause of this issue?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/57382/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-11-30T17:05:50Z",
        "body": ">mergeTree table called \"alerts_storage\"\r\n\r\nMergeTree or ReplicatedMergeTree?\r\nProvide `create table alerts_storage` `create table alerts`"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-01T05:03:39Z",
        "body": "@den-crane MergeTree Table. Identified the cause. I checked my alerts data by date wise and found out that this inconsistency in output occurs at 05/11/2023. Before 05/11/2023, the distributed data is giving correct output in both shard and replica. At 05/11/2023, I have restarted zookeeper pod to reduce its memory and cpu limit. That were problem occurs. After restarting the zookeeper, this inconsistency is starts to happen. Is there any way solve this one?"
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-03T09:43:48Z",
        "body": "Please reset your \"replicasCount: 2\" to \"replicasCount: 1\" if you use MergeTree table engine and query them from a distributed table. Multiple replicas only suitable for Replicated* tables querying from distributed tables consistently."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-03T12:53:05Z",
        "body": "@lampjian If I reset the replicasCount to 1, what will happen to the data in 2nd replicas? Will those data will be moved to shards or deleted? "
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-03T16:46:50Z",
        "body": "The data in 2nd replica still stores somewhere you can not query anymore. It seems to be deleted. You shall transfer the data to 1st replica before you remove the other replica."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-04T04:56:34Z",
        "body": "@lampjian  Okay. I have one more doubt. When during resetting the replicaCount to 1 for 2nd replicas, the shards(chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) and it's pvc storage won't be affected, right? They will remains as it is, right? Or will the pvc storage of above shards will be recreated?"
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-04T14:08:13Z",
        "body": "Probably unchanged, and your local and remote S3 storage configurations are all alike. But you shall merge the data to one replica and backup data first of all, your pod instances may run on different machines after restarting the cluster. Try to test all of the operations as much as possible in a similar smaller environment."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-04T18:03:34Z",
        "body": "@lampjian Tested in dev environment from the sctrach. Found out that this is not zookeeper problem. From my understanding, it's how the clickhouse actually works with my current settings(shardsCount - 3, replicasCount - 2). \r\n        \r\nIn dev enironment, I created a database called \"app\" with the replicated engine in all shards and replicas. After I created a mergeTree table called \"num_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"num\" for inserting and retrieving the data evenly.\r\n        \r\nWhen I inserted 1 to 1000 numbers into the pod chi-clickhouse-prod-cluster-shard0-0-0, the data are distributed evenly inserted across the three shards (chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) only and in its 2nd replicas which is came as independent pod, the data are not inserted.\r\n\r\n\r\n```\r\nchi-clickhouse-prod-cluster-shard0-0-0.chi-clickhouse-prod-cluster-shard0-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 997bd316-c646-49d3-8454-6fea1f7d6e5d\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     351 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard1-0-0.chi-clickhouse-prod-cluster-shard1-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: c00fc7d2-359d-462e-9251-4b953eb0698a\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     315 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard2-0-0.chi-clickhouse-prod-cluster-shard2-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 2fbdbe2a-3728-4d7b-a95c-34f2d3c3c8d7\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     334 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n```\r\n \r\n\r\nWhen I inserted 1 to 10 numbers into pod chi-clickhouse-prod-cluster-shard0-1-0(which is a 2nd replicas of shard0-0), it data is evenly inserted across the 2nd replicas of shards(chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0).\r\n        \r\n ```\r\nchi-clickhouse-prod-cluster-shard0-1-0.chi-clickhouse-prod-cluster-shard0-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 07eb979c-6348-4b6b-8870-c6faa60630e2\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       4 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard1-1-0.chi-clickhouse-prod-cluster-shard1-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 2022a66e-6188-4443-b36c-be685658250c\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard2-1-0.chi-clickhouse-prod-cluster-shard2-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 00f53697-ae60-4e14-8c21-7f1809db002c\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\n```\r\n\r\n\r\nSo In conclusion, When we have clickhouse with 3 shards and 2 replicas count and it's use MergeTree table engine and query them from a distributed table on a replicated database, the clickhouse will consider the shards(chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) as one cluster and its 2nd replicas which came as independent pod(chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0) as a another cluster. \r\n\r\n\r\nIs my conclusion is right? If yes, Could you please tell me what this behaviour of clickhouse is called?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-04T18:24:56Z",
        "body": "@Ragavendra-Vigneshwaran-R \r\n\r\n>database called \"app\" with the replicated engine \r\n\r\nreplicated database does not replicate data. It synchronizes the structures of the tables.\r\nYou need to use replicated **tables**.\r\n"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-05T12:23:15Z",
        "body": "Okay. I have reseted my replicasCount to 1 after migrating the data from 2nd replica to 1st replicas. Thanks @lampjian @den-crane for you clarification!!! It's really helped for doing the changes in my producation environment without any data loses. Thanks for your help."
      }
    ]
  },
  {
    "number": 56454,
    "title": "distributed engine inserts exceed memory, even if there is no limit set",
    "created_at": "2023-11-08T10:02:42Z",
    "closed_at": "2023-11-09T08:26:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/56454",
    "body": "Creating new ticket as #50744 is closed and issue is not resolved.\r\n\r\nBackground inserts into distributed tables started throwing exception:\r\n DB::Exception: Memory limit (for query) exceeded: would use 9.31 GiB (attempt to allocate chunk of 4360448 bytes), maximum: 9.31 GiB\r\n\r\nEven if i run SYSTEM FLUSH DISTRIBUTED ON CLUSTER cluster default.table, i get the same error.\r\n\r\nInserts on local node work ok. It also works ok with insert_distributed_sync=1. But as i would prefer to use async, i would like to go back to background inserts.\r\n\r\nMemory limits are the same on all nodes:\r\n```\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default\u2500\u252c\u2500value\u2500\u2510\r\n\u2502 max_memory_usage                 \u2502 0       \u2502 0     \u2502\r\n\u2502 max_memory_usage_for_user        \u2502 0       \u2502 0     \u2502\r\n\u2502 max_memory_usage_for_all_queries \u2502 0       \u2502 0     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nsystem.distribution_queue has 2 entries(1 for each node it is trying to insert to). \r\ndata_compressed_bytes: 9692170902\r\n\r\nEach shard has a queue of around 13k files, ~10G in size. Even if i leave just 1 file in the queue, it still throws memory exceeded.\r\nIf i remove the first file, i get file not found exception.\r\n\r\nHow do i tell clickhouse to not use 10G memory limit? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/56454/comments",
    "author": "Nikoslav",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-11-08T11:07:40Z",
        "body": "It's because `.bin` file stores settings which were applied during the initial insertion and the distributed table applies them during propagation of data into *MergeTree.\r\n\r\nHere is an example\r\n\r\n```\r\ncreate table T ( A Int64 ) Engine MergeTree partition by A order by A;\r\n\r\ncreate table TD as T Engine Distributed (default, currentDatabase(), T);\r\n\r\nset prefer_localhost_replica = 0;\r\nset max_partitions_per_insert_block = 1;\r\n\r\ninsert into TD select * from numbers(100);\r\n\r\nselect substr(last_exception,1, 150) from system.distribution_queue format Vertical;\r\nCode: 252. DB::Exception: Received from localhost:9000. DB::Exception: Too many partitions for single INSERT block (more than 1).\r\n```\r\n\r\nNow TD is unable to re-insert `1.bin` and it's impossible to change `1.bin` to redefine `max_partitions_per_insert_block`. \r\n\r\nyou can:\r\n\r\n* recreate table TD (drop/create and lost all not inserted data, all .bin files)\r\n* detach table, move bin files to user_files and try to read them\r\n\r\n```\r\n:) detach table TD;\r\n\r\n# cd /var/lib/clickhouse/data/default/TD/shard1_replica1/\r\n\r\n# mv *.bin /var/lib/clickhouse/user_files/\r\n\r\n:) attach table TD;\r\n\r\n-- data is accessible using `Distributed` format\r\n:) select * from file('*.bin', Distributed) limit 3\r\n\u250c\u2500A\u2500\u2510\r\n\u2502 0 \u2502\r\n\u2502 1 \u2502\r\n\u2502 2 \u2502\r\n\u2514\u2500\u2500\u2500\u2518\r\n\r\n:) select count() from file('*.bin', Distributed);\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n:) insert into T select * from file('*.bin', Distributed);\r\n\r\n:) select count() from T;\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n# rm /var/lib/clickhouse/user_files/*.bin\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-11-08T11:12:37Z",
        "body": "Also you can do this\r\n\r\n```\r\ncreate table TDNew as TD Engine Distributed (default, currentDatabase(), T);\r\nexchange tables TDNew and TD;\r\n```\r\nthen not inserted .bin files will be in TDNew (/var/lib/clickhouse/data/default/TDNew/shard1_replica1/)"
      },
      {
        "user": "Nikoslav",
        "created_at": "2023-11-09T08:26:52Z",
        "body": "Thanks a lot! Detach, move files, attach and insert worked perfectly."
      }
    ]
  },
  {
    "number": 56330,
    "title": "Delete query timeout on table that is being optimized atm",
    "created_at": "2023-11-04T10:18:46Z",
    "closed_at": "2023-11-04T21:35:07Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/56330",
    "body": "Hi there.\r\nI have java service that is making frequent inserts in the specific table based on ReplacingMergeTree engine. Target table has around 470 millions rows.\r\nDue to the business specific requirements i also have to use delete query following each insert query.\r\nOn top of that i'm using scheduled optimize on daily basis at 5.20 am.\r\n\r\nThe problem i encountered is following:\r\n- due to the table size optimize process takes some significant time meanwhile delete queries execution time also increases drastically. Some times it causes my service to shut down with connection exception: **Connection is not available, request timed out after 30000ms**. There are no exceptions in clickhouse logs when it happens.\r\n\r\nQuestions are:\r\n- Are there any table locks during optimize process that are slowing down delete queries? May be it is a stupid question, but i see no difference in performance of insert queries during optimize process.\r\n- Are there any workarounds to solve that issue except increasing max timeout?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/56330/comments",
    "author": "Mad-Melon",
    "comments": [
      {
        "user": "lampjian",
        "created_at": "2023-11-04T16:19:18Z",
        "body": "Try to use a final query with deleted flag, and stop the scheduled optimize and delete tasks."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2023-11-04T21:35:07Z",
        "body": "DELETE will wait for the merges already assigned by OPTIMIZE.\r\n\r\nAbout your problem - the OPTIMIZE query is rarely needed, and it is usually a mistake to do it."
      },
      {
        "user": "Mad-Melon",
        "created_at": "2023-11-05T11:39:09Z",
        "body": "> DELETE will wait for the merges already assigned by OPTIMIZE.\r\n> \r\n> About your problem - the OPTIMIZE query is rarely needed, and it is usually a mistake to do it.\r\n\r\nThanks for such fast response. I'll get rid of scheduled optimize and rely on Clickhouse background merges then."
      }
    ]
  },
  {
    "number": 56017,
    "title": "Aggregated table contains more records then MergeTree table",
    "created_at": "2023-10-25T13:03:44Z",
    "closed_at": "2023-10-26T07:26:45Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/56017",
    "body": "When inserting data into distributed table, from which later materialized view inserts data into aggregated table, in end table there are increased amount of records comparing to data in initial table. When data is inserted into local MergeTree table the amount of records matches the one in aggregated table.\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nThe amount of records is bigger in aggregated table than it is in a base table if you insert data using distributed table. Present in latest  lts version 23.8.2.7. On an outdated version 22.8.14.53 such issue is not present.\r\n\r\n**How to reproduce**\r\nUse version 23.8. Run following scripts to create data structure\r\n\r\n```\r\nCREATE DATABASE IF NOT EXISTS cdp_metrics_distr_no_rep ON CLUSTER 'cluster_cross_r1';\r\n\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.delays_metrics_local ON CLUSTER 'cluster_cross_r1' (\r\n                                                                                                    MetricName String,\r\n                                                                                                    Tags String,\r\n                                                                                                    DateTimeStartPeriod DateTime,\r\n                                                                                                    DelayMs UInt64,\r\n                                                                                                    SlaMs UInt64\r\n)\r\n    ENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/{cluster_name}/tables/{shard}/cdp_metrics_distr_no_rep.delays_metrics_local_1', '{replica}')\r\n        PARTITION BY toYYYYMMDD(DateTimeStartPeriod)\r\n        ORDER BY (\r\n                  MetricName,\r\n                  Tags,\r\n                  DateTimeStartPeriod\r\n            )\r\n        TTL DateTimeStartPeriod + INTERVAL 7 DAY\r\n        SETTINGS ttl_only_drop_parts = 1;\r\n\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.delays_metrics ON CLUSTER 'cluster_cross_r1' AS cdp_metrics_distr_no_rep.delays_metrics_local\r\nENGINE = Distributed(cluster_cross_r1, cdp_metrics_distr_no_rep, delays_metrics_local, cityHash64(MetricName, Tags, toString(DateTimeStartPeriod)));\r\n\r\n\r\n--aggregated table for overall delays metrics\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.overall_delays_metrics_local ON CLUSTER 'cluster_cross_r1' (\r\n        MetricName String,\r\n        Tags String,\r\n        DateTimeStartPeriod DateTime,\r\n        Count AggregateFunction(count, UInt64),\r\n        Quantiles AggregateFunction(quantiles(0.5,0.75,0.95,0.99), UInt64)\r\n)\r\n    ENGINE = ReplicatedAggregatingMergeTree(ENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/{cluster_name}/tables/{shard}/cdp_metrics_distr_no_rep.overall_delays_metrics_1', '{replica}')\r\n        PARTITION BY toYYYYMM(DateTimeStartPeriod)\r\n        ORDER BY (\r\n                  MetricName,\r\n                  Tags,\r\n                  DateTimeStartPeriod\r\n            )\r\n        TTL DateTimeStartPeriod + INTERVAL 60 DAY\r\n        SETTINGS ttl_only_drop_parts = 1;\r\n\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.overall_delays_metrics ON CLUSTER 'cluster_cross_r1' AS cdp_metrics_distr_no_rep.overall_delays_metrics_local\r\nENGINE = Distributed(cluster_cross_r1, cdp_metrics_distr_no_rep, overall_delays_metrics_local);\r\n\r\n--materialized view for overall delays metrics\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS cdp_metrics_distr_no_rep.overall_delays_metrics_local_mv ON CLUSTER 'cluster_cross_r1'\r\n        TO cdp_metrics_distr_no_rep.overall_delays_metrics_local\r\nAS SELECT\r\n       MetricName,\r\n       Tags,\r\n       DateTimeStartPeriod,\r\n       countState(DelayMs) AS Count,\r\n       quantilesState(0.5,0.75,0.95,0.99)(DelayMs) AS Quantiles\r\n   FROM cdp_metrics_distr_no_rep.delays_metrics_local\r\n   GROUP BY MetricName, Tags, DateTimeStartPeriod;\r\n```\r\n\r\n\r\n\r\nInsert data\r\n\r\n```\r\nINSERT INTO cdp_metrics_distr_no_rep.delays_metrics VALUES\r\n                                                 ('event_processing_delay', '{\"target\":\"flow_trigger\"}', '2023-10-28 00:00:00', 1272, 20000),\r\n                                                 ('event_processing_delay','{\"target\":\"flow_trigger\"}','2023-10-28 00:00:00',1431,20000),\r\n                                                 ('event_processing_delay', '{\"target\":\"flow_trigger\"}', '2023-10-28 00:00:00', 1222, 20000),\r\n                                                 ('event_processing_delay','{\"target\":\"flow_trigger\"}','2023-10-28 00:00:00',1435,20000),\r\n                                                 ('event_processing_delay', '{\"target\":\"flow_trigger\"}', '2023-10-28 00:00:00', 1242, 20000),\r\n                                                 ('event_processing_delay','{\"target\":\"flow_trigger\"}','2023-10-28 00:00:00',1461,20000)\r\n\r\n```\r\n\r\nCompare data in aggregated table and base table\r\n```\r\nSELECT MetricName, Tags, DateTimeStartPeriod, countMerge(Count), quantilesMerge(0.5,0.75,0.95,0.99)(Quantiles)\r\nfrom cdp_metrics_distr_no_rep.overall_delays_metrics\r\nwhere DateTimeStartPeriod >= '2023-10-28 00:00:00'\r\nGROUP BY MetricName,Tags,DateTimeStartPeriod;\r\n\r\nSELECT  Tags, count()\r\nfrom cdp_metrics_distr_no_rep.delays_metrics\r\nwhere DateTimeStartPeriod >= '2023-10-28 00:00:00'\r\nGROUP BY Tags\r\n```\r\n\r\n**Expected behavior**\r\n\r\nAmount of records in aggregated table and base table match.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/56017/comments",
    "author": "nikola2188",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-10-25T13:46:43Z",
        "body": "What is `cluster_cross_r1` ? What is the configuration of your cluster? provide `<remote_servers>`"
      },
      {
        "user": "den-crane",
        "created_at": "2023-10-25T13:48:36Z",
        "body": "please fix the issues like\r\n\r\n```\r\n    ENGINE = ReplicatedAggregatingMergeTree(ENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/{cluster_name}/tables/{shard}/cdp_metrics_distr_no_rep.overall_delays_metrics_1', '{replica}')\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-10-25T13:59:00Z",
        "body": "Check `<internal_replication>true</internal_replication>`, it must be true in `remote_servers`"
      },
      {
        "user": "nikola2188",
        "created_at": "2023-10-26T06:37:30Z",
        "body": "Yeah, cluster_cross_r1  is our cluster. You are right, for some reason we were missing internal_replication setting for that cluster. I will update it and test again."
      },
      {
        "user": "nikola2188",
        "created_at": "2023-10-26T07:26:45Z",
        "body": "Resloved, the root cause was indeed lack of internal replication setting on cluster."
      }
    ]
  },
  {
    "number": 55286,
    "title": "Internal_replication is set to true or false for replicated database engine.",
    "created_at": "2023-10-07T08:18:43Z",
    "closed_at": "2023-10-07T08:57:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/55286",
    "body": "Let's assume I have 3 shards and 2 replicas of clickhouse cluster and also zookeeper with 3 znodes for replication in my eks cluster. In that, I created a database called \"app\" with the replicated engine in all shards and replicas (i.e., in all 6 pods). After I created a mergeTree table called \"num_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"num\" for inserting and retrieving the data evenly in shards. If I insert some data into the \"num\" table ( which is a distributed table), the data will distributed in all shards and data will be replicated into the replica respectively. In this case, do I need to set the internal_replication as true or false? Which one will be best case for above scenario?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/55286/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "genzgd",
        "created_at": "2023-10-07T08:38:52Z",
        "body": "When using a Replicated*MergeTree or the Replicated database engine, internal replication should always be set to `true` in your cluster definition."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-10-07T08:57:43Z",
        "body": "Thanks @genzgd for your answer"
      }
    ]
  },
  {
    "number": 55231,
    "title": "Clickhouse user configuration",
    "created_at": "2023-10-04T11:03:25Z",
    "closed_at": "2023-10-05T01:55:26Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/55231",
    "body": "I have installed Clickhouse with 3 shards and 2 replicas in eks cluster. The user setting are in /etc/clickhouse-server/users.d/ folder in the clickhouse pod. In that folder, I have added the admin users setting. \r\n\r\nProblem description : \r\n\r\nWhen connected to the clickhouse in local( inside the clickhouse pod) with admin user and credentials, it throws the below exception \r\n\r\nException : \r\ncommand used --> clickhouse-client -u admin --password '****'\r\nClickHouse client version 23.7.5.30 (official build).\r\nConnecting to localhost:9000 as user admin.\r\nCode: 516. DB::Exception: Received from localhost:9000. DB::Exception: admin: Authentication failed: password is incorrect, or there is no user with such name.. (AUTHENTICATION_FAILED)\r\n\r\nBut when I connect to the clickhouse through the service URL as host with admin user credentials, it getting connected to clickhouse.\r\n\r\ncommand used --> clickhouse-client -h clickhouse-clickhouse -u admin --password '****'\r\nClickHouse client version 23.7.5.30 (official build).\r\nConnecting to clickhouse-clickhouse:9000 as user admin.\r\nConnected to ClickHouse server version 23.7.5 revision 54465.\r\nchi-clickhouse-prod-cluster-shard0-0-0.chi-clickhouse-prod-cluster-shard0-0.clickhouse.svc.cluster.local :)\r\n\r\n\r\nDoes this mean the admin user setting have to be present in under /etc/clickhouse-server/users.d/ folder and also be present in /etc/clickhouse-server/users.xml file in clickhouse?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/55231/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-10-04T12:31:46Z",
        "body": "it means that you misconfigured `<network>` section and disallowed to connect from a localhost.\r\n\r\nplease share your configs / hide passwords with ****\r\n\r\n"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-10-05T01:55:26Z",
        "body": "Got the issue. I didn't add localhost in the admin user network/ip section. Thanks @den-crane for your answer."
      }
    ]
  },
  {
    "number": 54875,
    "title": "Cannot convert string 1999-1-1 00:00:00 to type DateTime64(3)",
    "created_at": "2023-09-21T10:17:44Z",
    "closed_at": "2023-09-21T12:11:27Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/54875",
    "body": "Hello, I changed `date_time_input_format` to best_effort.\r\n\r\nSchema\r\n\r\n```\r\nCREATE TABLE default.datetime64_test\r\n(\r\n    `occur_time` DateTime64(3),\r\n)\r\nENGINE = MergeTree\r\nORDER BY occur_time\r\nSETTINGS index_granularity = 8192;\r\n```\r\n\r\nInserting data:\r\n\r\n```\r\ninsert into default.datetime64_test(occur_time) values ('1999-1-1 00:00:00');\r\n```\r\n\r\nThe failing query :\r\n\r\n```\r\nselect * from default.datetime64_testwhere occur_time='1999-1-1 00:00:00';\r\n````\r\n\r\nCode: 53. DB::Exception: Received from localhost:29010. DB::Exception: Cannot convert string 1999-1-1 00:00:00 to type DateTime64(3): while executing 'FUNCTION equals(occur_time : 0, '1999-1-1 00:00:00' : 1) -> equals(occur_time, '1999-1-1 00:00:00') UInt8 : 2'. (TYPE_MISMATCH)\r\n\r\nExpected result - no error (1 row match)\r\n\r\n\r\nclickhouse version 23.3.14.1\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/54875/comments",
    "author": "chenxin57085122",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-09-21T12:11:27Z",
        "body": "it's expected behaviour.\r\n\r\nYou should use either `occur_time='1999-01-01 00:00:00'` \r\nor `occur_time=parseDateTimeBestEffort('1999-1-1 00:00:00')`"
      },
      {
        "user": "chenxin57085122",
        "created_at": "2023-09-22T05:50:53Z",
        "body": "thanks!\r\n\r\nBut why can this data YYYY-M-D HH:mm:ss be written directly, but also need to be converted when it is a query condition\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 54127,
    "title": "Does ReplacingMergeTree have the capability to identify partitions that have changed?",
    "created_at": "2023-08-31T13:34:43Z",
    "closed_at": "2023-08-31T14:16:06Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/54127",
    "body": "I am currently using version 21.8 of `ReplacingMergeTree` frequently. I have noticed that after I insert data, it seems that all partitions undergo automatic merging, even if some partitions do not require merging.\r\nI would like to know if the current version of `ReplacingMergeTree` has the capability to identify `partitions` that have changed, so that only the necessary `partitions` can be automatically merged. If the latest version does not have this functionality, perhaps I can contribute by adding this optimization.\r\nexample:\r\n```\r\nCREATE TABLE test_table\r\n(\r\n    `key` UInt64,\r\n    `someCol` String\r\n)\r\nENGINE = ReplacingMergeTree\r\nPARTITION BY modulo(key, 40) \r\nPRIMARY KEY key \r\nORDER BY key;\r\n```\r\nThen insert a large amount of data into it, covering all partitions from 0 to 39.\r\nThen `optimize table test_table final;`\r\nAfterwards, insert two datas.\r\n```\r\ninsert into test_table(key, someCol) values(1, 'test1');\r\ninsert into test_table(key, someCol) values(5, 'test5');\r\n```\r\nWould `ReplacingMergeTree` only automatically merge `partition(1)` and `partition(5)` at this point? Or will all `partitions (0-39)` be automatically merged after a certain period of time?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/54127/comments",
    "author": "Chen768959",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-08-31T14:16:07Z",
        "body": "  --optimize_skip_merged_partitions arg                                                Skip partitions with one part with level > 0 in optimize final\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2023-08-31T14:16:28Z",
        "body": "`optimize table test_table final settings optimize_skip_merged_partitions=1;`"
      },
      {
        "user": "Chen768959",
        "created_at": "2023-09-01T02:40:09Z",
        "body": "> --optimize_skip_merged_partitions arg Skip partitions with one part with level > 0 in optimize final\r\n\r\nThank you for your answer"
      }
    ]
  },
  {
    "number": 53397,
    "title": "INSERTing into a table named 'table' fails",
    "created_at": "2023-08-14T08:39:56Z",
    "closed_at": "2023-08-14T11:23:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/53397",
    "body": "```sql\r\n-- this works\r\nCREATE TABLE tab (val1 UInt32, val2 UInt32) ENGINE=Memory;\r\nINSERT INTO tab VALUES (42, 24);\r\n\r\n-- this doesn't:\r\nCREATE TABLE table (val1 UInt32, val2 UInt32) ENGINE=Memory;\r\nINSERT INTO table VALUES (42, 24); -- Code: 62. DB::Exception: Syntax error: failed at position 27 ('42'): 42, 24);. Expected one of: list of elements, insert element, COLUMNS matcher, COLUMNS, qualified asterisk, compound identifier, identifier, asterisk. (SYNTAX_ERROR)\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/53397/comments",
    "author": "rschu1ze",
    "comments": [
      {
        "user": "evillique",
        "created_at": "2023-08-14T10:22:37Z",
        "body": "The docs should probably be adjusted because this works:\r\n``` SQL\r\nINSERT INTO TABLE table VALUES (42, 24);\r\n```"
      },
      {
        "user": "rschu1ze",
        "created_at": "2023-08-14T10:57:51Z",
        "body": "Didn't know there is a long form of `INSERT INTO` \ud83d\ude04  I agree, a doc update will be fine."
      },
      {
        "user": "lampjian",
        "created_at": "2023-08-16T14:09:59Z",
        "body": "This also works.\r\n```SQL\r\nINSERT INTO `table` VALUES (42, 24);\r\n```"
      }
    ]
  },
  {
    "number": 52620,
    "title": "(branch 23.5) `Looking for mutations for part` appears in client with `send_logs_level=debug`",
    "created_at": "2023-07-26T12:50:22Z",
    "closed_at": "2023-07-26T12:56:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/52620",
    "body": "```sql\r\nALTER TABLE tableA MATERIALIZE INDEX ginidx;\r\n\r\nSET send_logs_level=debug;\r\n\r\nINSERT INTO tableB SELECT * FROM  tableA LIMIT 1000000\r\n\r\nQuery id: 865a8654-159d-40a3-ada1-90febde40416\r\n\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.162059 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> executeQuery: Query span trace_id for opentelemetry log: 00000000-0000-0000-0000-000000000000\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.163246 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> executeQuery: (from 127.0.0.1:57642, user: operator) INSERT INTO tableB SELECT * FROM  tableA LIMIT 1000000; (stage: Complete)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167003 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 00538dfb52fdf41c1a36ff5f8a9b3f86_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167077 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0298f752bf557334fc80759843abe567_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167104 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0413d695373e7a54f73ae0886afed664_0_0_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167272 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0413d695373e7a54f73ae0886afed664_1_1_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167298 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 04ccd2b5bf76dec05640587b037abdcf_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167367 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0816cf91eb7efa18de40dcdb1c8ed388_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167410 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 093315f287eafd6f14001e3a1f1ae873_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167481 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 098efa9cec0b236ab538700d75525ef8_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167514 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 09d7a7a3a5c5c0cb9867f4c2d26b760b_0_0_0_5 (part data version 5, part metadata version 2)\r\n...(repeated logs)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/52620/comments",
    "author": "cangyin",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T12:56:44Z",
        "body": "As expected"
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T12:59:13Z",
        "body": "@tavplubix  please explain a bit more"
      },
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T13:00:27Z",
        "body": "@cangyin, please explain what exactly was unexpected for you"
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T13:06:39Z",
        "body": "I'm just surprised at first glance. This becomes reasonable for me now."
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T13:10:52Z",
        "body": "But the same batch of  log `Looking for mutations for part ...` keeps showing if I repeat my query\r\n```sql\r\nSELECT * FROM  tableA \r\n```\r\n\r\nIs this also expected?\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T13:44:50Z",
        "body": "Yes, because some unfinished mutations have to be applied when reading (for example, if data type of some column was altered)"
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T13:54:17Z",
        "body": "Nop, there is no unfinished merge/mutations\r\n```sql\r\nSELECT create_time,database,table,command,mutation_id,parts_to_do,parts_to_do_names,is_done,latest_fail_reason FROM system.mutations WHERE is_done=0\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.011 sec.\r\n\r\nSELECT * FROM  tableA\r\n\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.864773 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part e9b788f63533f3ad7f314f7a2d25d601_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.864891 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part eb5ee5089fab350478a03192fd3d224c_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.864959 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part ec919c9b451468ea6fab9ae43edc9f82_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865020 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part ee342e7e28f7515269f147012d5125b1_0_0_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865111 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part ee342e7e28f7515269f147012d5125b1_1_1_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865215 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part eeb786a6f9edf84e9db8b6ee168f750d_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865311 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part efd25dbd5fad5eaf16ba3e577063d29a_0_0_0_6 (part data version 6, part metadata version 2)\r\n...\r\n```\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T14:02:52Z",
        "body": "But SELECT queries need to check if there are some mutations to apply. Even if there are no unfinished mutations, a SELECT query doesn't know that without checking."
      }
    ]
  },
  {
    "number": 52516,
    "title": "Possible to connect to secure Keeper with clickhouse-keeper-client?",
    "created_at": "2023-07-24T09:53:55Z",
    "closed_at": "2023-08-03T16:34:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/52516",
    "body": "My Keeper is secured and runs on port 9281. Version 23.6.2.18-stable.\r\n```bash\r\nlsof -nPi | grep LISTEN | grep clickhouse\r\nclickhous 3253968 clickhouse   37u  IPv6 14940597      0t0  TCP *:9444 (LISTEN)\r\nclickhous 3253968 clickhouse   38u  IPv4 14940601      0t0  TCP 10.100.116.91:9281 (LISTEN)\r\n```\r\nConnecting to unsecured Keeper worked fine (before I enabled SSL). \r\n```bash\r\n[root@bdtpsqlfdw01 ~]# clickhouse keeper-client -h bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9181 --query=\"ls /\"\r\nkeeper\r\n```\r\nI tried to connect with clickhouse-keeper-client to secured cluster but it doesn't work.\r\n```bash\r\n[root@bdtpsqlfdw01 clickhouse-keeper]# clickhouse keeper-client -h bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9281 --query=\"ls /\"\r\nCoordination::Exception: Connection loss, path: All connection tries failed while connecting to ZooKeeper. nodes: 10.100.116.91:9281\r\nCode: 999. Coordination::Exception: Unexpected handshake length received: 352518400 (Marshalling error): while receiving handshake from ZooKeeper. (KEEPER_EXCEPTION) (version 23.6.2.18 (official build)), 10.100.116.91:9281\r\nCode: 999. Coordination::Exception: Unexpected handshake length received: 352518400 (Marshalling error): while receiving handshake from ZooKeeper. (KEEPER_EXCEPTION) (version 23.6.2.18 (official build)), 10.100.116.91:9281\r\nCode: 999. Coordination::Exception: Unexpected handshake length received: 352518400 (Marshalling error): while receiving handshake from ZooKeeper. (KEEPER_EXCEPTION) (version 23.6.2.18 (official build)), 10.100.116.91:9281\r\n```\r\nThis is from /var/log/clickhouse-keeper/clickhouse-keeper.log\r\n```bash\r\n2023.07.24 11:49:59.320064 [ 3253971 ] {} <Error> ServerErrorHandler: Code: 210. DB::NetException: SSL Exception: error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER, while reading from socket (10.100.116.91:42634). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000000fd2eb5 in /usr/bin/clickhouse-keeper\r\n1. ? @ 0x0000000001093967 in /usr/bin/clickhouse-keeper\r\n2. DB::ReadBufferFromPocoSocket::nextImpl() @ 0x0000000001093583 in /usr/bin/clickhouse-keeper\r\n3. DB::KeeperTCPHandler::runImpl() @ 0x0000000000c35245 in /usr/bin/clickhouse-keeper\r\n4. Poco::Net::TCPServerConnection::start() @ 0x0000000001335f34 in /usr/bin/clickhouse-keeper\r\n5. Poco::Net::TCPServerDispatcher::run() @ 0x00000000013371b1 in /usr/bin/clickhouse-keeper\r\n6. Poco::PooledThread::run() @ 0x000000000146d807 in /usr/bin/clickhouse-keeper\r\n7. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000000146b1dc in /usr/bin/clickhouse-keeper\r\n (version 23.6.2.18 (official build))\r\n2023.07.24 11:49:59.322919 [ 3253972 ] {} <Error> ServerErrorHandler: Code: 210. DB::NetException: SSL Exception: error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER, while reading from socket (10.100.116.91:42614). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000000fd2eb5 in /usr/bin/clickhouse-keeper\r\n1. ? @ 0x0000000001093967 in /usr/bin/clickhouse-keeper\r\n2. DB::ReadBufferFromPocoSocket::nextImpl() @ 0x0000000001093583 in /usr/bin/clickhouse-keeper\r\n3. DB::KeeperTCPHandler::runImpl() @ 0x0000000000c35245 in /usr/bin/clickhouse-keeper\r\n4. Poco::Net::TCPServerConnection::start() @ 0x0000000001335f34 in /usr/bin/clickhouse-keeper\r\n5. Poco::Net::TCPServerDispatcher::run() @ 0x00000000013371b1 in /usr/bin/clickhouse-keeper\r\n6. Poco::PooledThread::run() @ 0x000000000146d807 in /usr/bin/clickhouse-keeper\r\n7. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000000146b1dc in /usr/bin/clickhouse-keeper\r\n (version 23.6.2.18 (official build))\r\n2023.07.24 11:49:59.323113 [ 3253970 ] {} <Error> ServerErrorHandler: Code: 210. DB::NetException: SSL Exception: error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER, while reading from socket (10.100.116.91:42628). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000000fd2eb5 in /usr/bin/clickhouse-keeper\r\n1. ? @ 0x0000000001093967 in /usr/bin/clickhouse-keeper\r\n2. DB::ReadBufferFromPocoSocket::nextImpl() @ 0x0000000001093583 in /usr/bin/clickhouse-keeper\r\n3. DB::KeeperTCPHandler::runImpl() @ 0x0000000000c35245 in /usr/bin/clickhouse-keeper\r\n4. Poco::Net::TCPServerConnection::start() @ 0x0000000001335f34 in /usr/bin/clickhouse-keeper\r\n5. Poco::Net::TCPServerDispatcher::run() @ 0x00000000013371b1 in /usr/bin/clickhouse-keeper\r\n6. Poco::PooledThread::run() @ 0x000000000146d807 in /usr/bin/clickhouse-keeper\r\n7. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000000146b1dc in /usr/bin/clickhouse-keeper\r\n (version 23.6.2.18 (official build))\r\n```\r\nConnecting through zkCli works:\r\n```bash\r\n[root@bdtpsqlfdw01 clickhouse-keeper]# /usr/share/zookeeper/bin/zkCli.sh -client-configuration /etc/clickhouse-keeper/zkclient.cfg -server bdtpsqlfdw01.bdtest.sit.pbz.hr:9281 ls / | egrep -v 'INFO|ERROR|^$|WATCHER|WatchedEvent'          Connecting to bdtpsqlfdw01.bdtest.sit.pbz.hr:9281\r\n2023-07-24 11:36:49,987 [myid:bdtpsqlfdw01.bdtest.sit.pbz.hr:9281] - WARN  [nioEventLoopGroup-2-1:ClientCnxnSocket@150] - Connected to an old server; r-o mode will be unavailable\r\n[clickhouse, keeper]\r\n```\r\nIs there any configuration that I shoulh setup and that is missing here?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/52516/comments",
    "author": "msestak",
    "comments": [
      {
        "user": "pufit",
        "created_at": "2023-07-29T02:59:23Z",
        "body": "@msestak I believe you just need to add `secure://` before your host.\r\ne.g. `clickhouse keeper-client -h secure://bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9181 --query=\"ls /\"`"
      },
      {
        "user": "msestak",
        "created_at": "2023-08-04T08:19:00Z",
        "body": "Yes.\r\n\r\n```bash\r\n[root@bdtclick02 ~]# clickhouse keeper-client -h secure://bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9281 --query=\"ls /\"\r\nclickhouse keeper\r\n```"
      }
    ]
  },
  {
    "number": 52434,
    "title": "How to efficiently materialize data into a table when a field is set with default_expression?",
    "created_at": "2023-07-21T15:25:54Z",
    "closed_at": "2023-07-24T17:37:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/52434",
    "body": "Our application scenario requires fast updates on a specific column, with data volume ranging from over 30 billion to even hundreds of billions. To achieve fast updates, our current approach involves synchronizing the required data column and associated keys to ClickHouse using Join Engine to create a table.\r\n\r\nWe then add a new field to the target table and set its default_expression as follows:\r\n\r\nALTER TABLE db.tab_1 ADD COLUMN col_new type DEFAULT joinGet(join_storage_table, `value_column`, join_keys);\r\n\r\nAfter that, we change the field names using the following command:\r\n\r\nALTER TABLE db.tab_1\r\n    RENAME COLUMN col TO col_old,\r\n    RENAME COLUMN col_new TO col,\r\n    DROP COLUMN col_old;\r\n\r\nThis way, the logical modification is completed almost instantly, and we can use the updated table. However, we are aware that the current value in col is obtained through joinGet from the join_storage_table. If the join_storage_table is deleted, col becomes unusable. Hence, we would like to inquire whether there is a method to efficiently materialize the data of this column into the table, without depending on join_storage_table.\r\n\r\nthank\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/52434/comments",
    "author": "BarryAllen1993",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-07-22T16:14:06Z",
        "body": "options:\r\n\r\n* `alter table  db.tab_1 update col = col where 1`\r\n* `alter table  db.tab_1 materialize column col`\r\n\r\n"
      },
      {
        "user": "BarryAllen1993",
        "created_at": "2023-07-27T04:53:20Z",
        "body": "Thank"
      }
    ]
  },
  {
    "number": 52233,
    "title": "Insertion stuck on ReplicatedMergeTree when keeper connection lost.",
    "created_at": "2023-07-18T08:50:44Z",
    "closed_at": "2023-07-18T10:03:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/52233",
    "body": "I setup a cluster with 2 nodes. each node run a clickhouse server as well as an embedded keeper.\r\nIf i shut down one node. The keeper node will fail because quorum is 2 and only 1 keeper alive.\r\nIn this case, ReplicatedMergeTree tables should be readonly on the remaining node.\r\n\r\nBut when I do insertion on the table. it never returns and stuck. Is this expected behavior?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/52233/comments",
    "author": "yexia0",
    "comments": [
      {
        "user": "yexia0",
        "created_at": "2023-07-18T09:18:46Z",
        "body": "Actually it returns read-only after more than 2 minutes. From the log in `ZooKeeperRetriesControl` it tries to connect to zookeeper and retried 20 times. Is there a way to reduce this number? I want it to fail fast and handle the read-only error on our own code."
      },
      {
        "user": "nickitat",
        "created_at": "2023-07-18T10:03:55Z",
        "body": "setting `insert_keeper_max_retries`"
      },
      {
        "user": "nickitat",
        "created_at": "2023-07-18T10:06:05Z",
        "body": "fyi having only 2 keeper nodes is not recommended "
      },
      {
        "user": "yexia0",
        "created_at": "2023-07-18T10:12:23Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 51875,
    "title": "about summingmergetree Keep the earliest records",
    "created_at": "2023-07-06T04:00:16Z",
    "closed_at": "2023-07-06T04:07:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51875",
    "body": "create table test (\r\nid int64,\r\ncreated_time DateTime,\r\ncnt Int64\r\n)ENGINE=ReplicatedSummingMergeTree() order by id \r\n\r\ninsert into test (1,'2023-07-05 01:00:00',1)\r\ninsert into test (1,'2023-07-05 02:00:00',2)\r\ninsert into test (1,'2023-07-05 03:00:00',3)\r\nAfter data merging\r\nselect * from test:\r\nI hope it's the following result\r\n1    '2023-07-05 01:00:00'   6\r\nrather than\r\n1    '2023-07-05 03:00:00'   6\r\n\r\nWhat do I need to do\uff1f\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51875/comments",
    "author": "yangshike",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-07-06T13:21:59Z",
        "body": "```sql\r\nCREATE TABLE test\r\n(\r\n    `id` Int64,\r\n    `created_time` SimpleAggregateFunction(min, DateTime),\r\n    `cnt` Int64\r\n)\r\nENGINE = SummingMergeTree\r\nORDER BY id;\r\n\r\ninsert into test values(1,'2023-07-05 01:00:00',1);\r\ninsert into test values(1,'2023-07-05 02:00:00',2);\r\ninsert into test values(1,'2023-07-05 03:00:00',3);\r\n\r\noptimize table test final;\r\n\r\nselect * from test;\r\n\u250c\u2500id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500created_time\u2500\u252c\u2500cnt\u2500\u2510\r\n\u2502  1 \u2502 2023-07-05 01:00:00 \u2502   6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "yangshike",
        "created_at": "2023-07-07T07:56:18Z",
        "body": "thank you\uff01\uff01\r\n\r\n\r\nThe default seems to be to keep the first inserted line"
      }
    ]
  },
  {
    "number": 51474,
    "title": "Cannot allocate RAFT instance due to \"Address family not supported by protocol\" (IPv6 disabled)",
    "created_at": "2023-06-27T11:09:14Z",
    "closed_at": "2023-07-10T09:44:41Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51474",
    "body": "\r\nIt's look like that resolving of issue #33381 hasn't fixed the problem with ClickHouse-Keeper on machines with disabled IPv6:\r\n\r\nIn /var/log/clickhouse-server/clickhouse-server.log we have:\r\n\r\n2023.06.27 13:47:36.661896 [ 113898 ] {} <Error> RaftInstance: got exception: open: Address family not supported by protocol [system:97] on port 9234\r\n2023.06.27 13:47:36.661947 [ 113898 ] {} <Debug> KeeperDispatcher: Server initialized, waiting for quorum\r\n2023.06.27 13:50:36.662692 [ 113898 ] {} <Error> void DB::KeeperDispatcher::initialize(const Poco::Util::AbstractConfiguration &, bool, bool): Code: 568. DB::Exception: Failed to wait RAFT initialization. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa3ec73a in /usr/bin/clickhouse\r\n1. DB::KeeperServer::waitInit() @ 0x16062e35 in /usr/bin/clickhouse\r\n2. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool) @ 0x1604df4a in /usr/bin/clickhouse\r\n3. DB::Context::initializeKeeperDispatcher(bool) const @ 0x14875d4d in /usr/bin/clickhouse\r\n4. DB::Server::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa48d880 in /usr/bin/clickhouse\r\n5. Poco::Util::Application::run() @ 0x189a11a6 in /usr/bin/clickhouse\r\n6. DB::Server::run() @ 0xa47e8fe in /usr/bin/clickhouse\r\n7. mainEntryClickHouseServer(int, char**) @ 0xa47be97 in /usr/bin/clickhouse\r\n8. main @ 0xa3da86b in /usr/bin/clickhouse\r\n9. __libc_start_main @ 0x3ad85 in /usr/lib64/libc-2.28.so\r\n10. _start @ 0xa19a46e in /usr/bin/clickhouse\r\n (version 22.8.6.71 (official build))\r\n\r\n...and no listening ports from ClickHouse...\r\nss -tunlp\r\nNetid                 State                  Recv-Q                 Send-Q                                 Local Address:Port                                  Peer Address:Port                Process                \r\nudp                   UNCONN                 0                      0                                            0.0.0.0:111                                        0.0.0.0:*                                          \r\nudp                   UNCONN                 0                      0                                          127.0.0.1:323                                        0.0.0.0:*                                          \r\nudp                   UNCONN                 0                      0                                            0.0.0.0:45755                                      0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:10050                                      0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:111                                        0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:22                                         0.0.0.0:*  \r\n\r\nThere are no uncommented IPv6 wildcards in config.xml, config.d configs and only one default user. To config.xml we added only:\r\n<listen_host>0.0.0.0</listen_host>\r\n<interserver_listen_host>0.0.0.0</interserver_listen_host>\r\nEverything else is default after ClickHouse installed from rpm.\r\n\r\nClickHouse release: 22.8.6.71-lts\r\nOS: RHEL 8\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51474/comments",
    "author": "SavelyevPavel",
    "comments": [
      {
        "user": "antonio2368",
        "created_at": "2023-06-28T12:27:06Z",
        "body": "Can you try setting `keeper_server.enable_ipv6` to `false` in your XML?\r\n`interserver_listen_host` support in Keeper is in 22.9+."
      },
      {
        "user": "SavelyevPavel",
        "created_at": "2023-07-10T09:02:47Z",
        "body": "Yes, `keeper_server.enable_ipv6` works! Thank you, @antonio2368! "
      }
    ]
  },
  {
    "number": 51401,
    "title": "DateTime Default value on missing/empty key/value pair.",
    "created_at": "2023-06-26T06:08:50Z",
    "closed_at": "2023-06-26T13:33:00Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51401",
    "body": "\r\n**Describe what's wrong**\r\n\r\nA DateTime column with Default expr as now() does not populate value from now if the value being inserted is empty or if the column is missing in the data source.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes. 23.2.2.20.\r\n\r\n**How to reproduce**\r\n\r\n* 23.2.2.20\r\n* clickhouse-client\r\n* Non-default settings: NA\r\n*\r\n\r\n```\r\nCREATE TABLE default.data_set\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Memory\r\n```\r\n\r\n\r\n```\r\nCREATE TABLE default.data_set_kafka\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Kafka\r\nSETTINGS kafka_broker_list = 'kafka-host:9092', kafka_topic_list = 'topic-name', kafka_group_name = 'kafka-group-name', kafka_format = 'JSONEachRow', kafka_skip_broken_messages = 10\r\n\r\n\r\nor you can create a regular table just for testing.\r\n\r\nCREATE TABLE default.data_set_kafka\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Memory\r\n```\r\n\r\n\r\n```\r\nCREATE MATERIALIZED VIEW default.data_set_mv TO default.data_set\r\n(\r\n    `createdAt` DateTime\r\n) AS\r\nSELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\nFROM default.data_set_kafka\r\n\r\n\r\nor even if you use just select without multiif, it does not work\r\n\r\n\r\nCREATE MATERIALIZED VIEW default.data_set_mv TO default.data_set\r\n(\r\n    `createdAt` DateTime\r\n) AS\r\nSELECT createdAt FROM default.data_set_kafka\r\n```\r\n\r\n\r\nFor regular kafka table (memory engine)\r\n```\r\nINSERT INTO default.data_set_kafka values ('2023-06-07')\r\nINSERT INTO default.data_set_kafka values (NULL)\r\nINSERT INTO default.data_set_kafka values ()\r\n```\r\n\r\nFor insertion using kcat command.\r\n```\r\n{}\r\n{\"createdAt\":\"\"}\r\n{\"createdAt\":\"2023-06-29\"}\r\n```\r\n\r\n\r\n* select * from default.data_set\r\n\r\n**Expected behavior**\r\n\r\nWe expect values to have the result of now() for missing column or empty value for the createdAt field. Explicitly specifying NULL works, however other options do not work.\r\n\r\n**Error message and/or stacktrace**\r\nNo error messages.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51401/comments",
    "author": "ilugid",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2023-06-26T11:06:08Z",
        "body": "> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\n\r\nBy default fields are non nullable\r\n\r\n> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\n\r\nIn MV you should match not positions of columns, but their names\r\n\r\nIE \r\n\r\n> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt) as createdAt"
      },
      {
        "user": "ilugid",
        "created_at": "2023-06-26T18:36:40Z",
        "body": "That worked. Thanks!"
      }
    ]
  },
  {
    "number": 50591,
    "title": "Why SYSTEM RESTORE REPLICA ... is not supported for Materialized views with Replica engines?",
    "created_at": "2023-06-05T15:01:50Z",
    "closed_at": "2023-06-06T11:20:33Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/50591",
    "body": "I am using the `SYSTEM RESTORE REPLICA` functionality to restore a MV with `ReplicatedAggregatingMergeTree`. In the docs it is just stated that the restore will only work for replicated tables, but nothing about not supporting it from MV. \r\nThis is needed in case the source table of MV has retention (TTL), then recreating the MV will not work\r\nA workaround would be to create another table localle and send all the data there\r\nI can also provide the data schema if this is unexpected behavior respectively a bug\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/50591/comments",
    "author": "yasha-dev1",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2023-06-06T10:15:07Z",
        "body": "You can do that for .inner table of the MV. \r\n\r\nAlso best practice is to use explicit storage table for the MV (i.e. syntax `CREATE MATERIALIZED VIEW ... TO target_table AS SELECT ...`)"
      },
      {
        "user": "yasha-dev1",
        "created_at": "2023-06-06T11:20:33Z",
        "body": "thnx"
      }
    ]
  },
  {
    "number": 49854,
    "title": "Support more filtering conditions for read in order optimization",
    "created_at": "2023-05-13T10:23:33Z",
    "closed_at": "2023-05-13T12:48:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/49854",
    "body": "I need to pull huge chunks of data from table with (category, time) primary key, for set of category values and stream result set in time order.\r\nThe amount of data can be really huge so I'd like to avoid full sort on top of query by any cost.\r\nWhen I filter with category = value, the RAM profile is low -- afaik thanks to MergeTreeInOrder read optimization.\r\nHowever, if I use other filtering function, like category in (...), the full sort seems to be applied and RAM usage sky rocket.\r\n\r\nCREATE TABLE order_test_1 (`ts` Int64,`lc` Int32) ENGINE = MergeTree PRIMARY KEY (lc) ORDER BY (lc, ts)\r\nas (select (number + 1000000000) as ts, (number % 16) as lc from numbers(1000000000));\r\n\r\n```\r\nselect version();\r\n23.5.1.1169\r\n\r\n-- this query looks good, takes ~35M memory \r\nexplain pipeline\r\nselect * from order_test_1 where lc = 0 order by ts;\r\n\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 20 \u2192 1\r\n    (Expression)\r\n    ExpressionTransform \u00d7 20\r\n      (ReadFromMergeTree)\r\n      MergeTreeInOrder \u00d7 20 0 \u2192 1\r\n\r\n-- but this takes ~1.5G memory to run full sort\r\nexplain pipeline\r\nselect * from order_test_1 where lc in (0,1) order by ts; -- format Null;\r\n\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 16 \u2192 1\r\n    MergeSortingTransform \u00d7 16\r\n      LimitsCheckingTransform \u00d7 16\r\n        PartialSortingTransform \u00d7 16\r\n          (Expression)\r\n          ExpressionTransform \u00d7 16\r\n            (ReadFromMergeTree)\r\n            MergeTreeThread \u00d7 16 0 \u2192 1\r\n```\r\n\r\nInterestingly, I can force ClickHouse into required behavior to some extent, by using VIEW with UNION ALL pre-filtered subqueries\r\n```\r\n-- memory ~75M\r\nexplain pipeline\r\nselect * from (\r\n\tselect * from order_test_1 where lc = 0 order by ts\r\n\tunion all\r\n\tselect * from order_test_1 where lc = 1 order by ts\r\n\tunion all\r\n\tselect * from order_test_1 where lc = 2 order by ts\r\n\tunion all\r\n\tselect * from order_test_1 where lc = 3 order by ts\r\n) where lc in (0, 1) order by ts;\r\n\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 46 \u2192 1\r\n    (Union)\r\n      (Expression)\r\n      ExpressionTransform \u00d7 20\r\n        (Filter)\r\n        FilterTransform \u00d7 20\r\n          (ReadFromMergeTree)\r\n          MergeTreeInOrder \u00d7 20 0 \u2192 1\r\n      (Expression)\r\n      ExpressionTransform \u00d7 20\r\n        (Filter)\r\n        FilterTransform \u00d7 20\r\n          (ReadFromMergeTree)\r\n          MergeTreeInOrder \u00d7 20 0 \u2192 1\r\n      (Expression)\r\n      ExpressionTransform \u00d7 5\r\n        (Filter)\r\n        FilterTransform \u00d7 5\r\n          (ReadFromMergeTree)\r\n          MergeTreeInOrder \u00d7 5 0 \u2192 1\r\n      (Expression)\r\n      ExpressionTransform\r\n        (Filter)\r\n        FilterTransform\r\n          (ReadFromMergeTree)\r\n```\r\n\r\nUnfortunately, I can't use the same in real-life scenario due to much higher cardinality of category values.\r\n\r\nFor some more context -- we're evaluating ClickHouse as a direct replacement for own in-house solution. One of existing application processes time-series data replay (simulation) streamed in time order. 1 minute of data is ~50G, and we need to process hours of data in one run. Ideally, we'd like to make one query and receive continuous stream of data which will not OOM the server.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/49854/comments",
    "author": "bepec",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-05-13T12:48:51Z",
        "body": "The problem here is that the table is ordered by `(lc, ts)` and you query needs `by ts`.\r\nBecause the order does not match, Clickhouse sorts the data, and does not use index for read_in_order.\r\n\r\n```sql\r\nCREATE TABLE order_test_1 (ts Int64,lc Int32) ENGINE = MergeTree PRIMARY KEY (lc) ORDER BY (lc, ts)\r\nas (select (number + 1000000000) as ts, (number % 16) as lc from numbers(1000000000));\r\n\r\nselect * from order_test_1 where lc = 0 order by ts limit 1000 format Null;\r\nPeak memory usage (for query): 14.24 MiB.\r\n0 rows in set. Elapsed: 0.010 sec. Processed 842.25 thousand rows, 10.11 MB (86.98 million rows/s., 1.04 GB/s.)\r\n```\r\n\r\ndo you really need all rows?\r\nlimit 1000 \r\n```\r\nselect * from order_test_1 where lc in (0,1) order by ts limit 1000 format Null;\r\nMemoryTracker: Peak memory usage (for query): 28.25 MiB.\r\n0 rows in set. Elapsed: 0.514 sec. Processed 125.02 million rows, 1.50 GB (243.15 million rows/s., 2.92 GB/s.)\r\n```\r\n\r\nall rows\r\n```\r\nselect * from order_test_1 where lc in (0,1) order by ts format Null;\r\nMemoryTracker: Peak memory usage (for query): 1.41 GiB.\r\n0 rows in set. Elapsed: 6.002 sec. Processed 125.02 million rows, 1.50 GB (20.83 million rows/s., 249.97 MB/s.)\r\n```\r\n\r\norder by lc,ts\r\n```\r\nselect * from order_test_1 where lc in (0,1) order by lc,ts format Null;\r\nMemoryTracker: Peak memory usage (for query): 24.47 MiB.\r\n0 rows in set. Elapsed: 1.829 sec. Processed 125.02 million rows, 1.50 GB (68.36 million rows/s., 820.25 MB/s.)\r\n```\r\n\r\norder by lc,ts, order by ts\r\n```\r\nselect * from (\r\n   select * from order_test_1 where lc in (0,1) order by lc,ts \r\n) order by ts\r\nformat Null;\r\nMemoryTracker: Peak memory usage (for query): 1.41 GiB.\r\n0 rows in set. Elapsed: 5.918 sec. Processed 125.02 million rows, 1.50 GB (21.12 million rows/s., 253.50 MB/s.)\r\n```\r\n\r\n## The trick\r\n\r\n```\r\ndrop table order_test_1;\r\n\r\nCREATE TABLE order_test_1 (ts Int64,lc Int32) ENGINE = MergeTree \r\nPRIMARY KEY (toStartOfDay(toDateTime(ts)), lc) ORDER BY (toStartOfDay(toDateTime(ts)), lc, ts)\r\nas (select (number + 1000000000) as ts, (number % 16) as lc from numbers(1000000000));\r\n\r\nselect * from order_test_1 where lc in (0,1) order by ts format Null;\r\nMemoryTracker: Peak memory usage (for query): 1.41 GiB.\r\n0 rows in set. Elapsed: 2.694 sec. Processed 220.29 million rows, 2.64 GB (81.77 million rows/s., 980.69 MB/s.)\r\n\r\nselect * from order_test_1 where lc in (0,1) order by toStartOfDay(toDateTime(ts)), ts format Null;\r\nMemoryTracker: Peak memory usage (for query): 42.42 MiB.\r\n0 rows in set. Elapsed: 6.779 sec. Processed 220.29 million rows, 2.64 GB (32.50 million rows/s., 389.96 MB/s.)\r\n```\r\n\r\nlimit 1000\r\n```\r\nset max_block_size=1000, max_threads=1;\r\nselect * from order_test_1 where lc in (0,1) order by ts limit 1000 format Null;\r\nMemoryTracker: Peak memory usage (for query): 1.01 GiB.\r\n0 rows in set. Elapsed: 13.471 sec. Processed 220.29 million rows, 1.94 GB (16.35 million rows/s., 143.84 MB/s.)\r\n\r\n\r\nselect * from order_test_1 where lc in (0,1) order by toStartOfDay(toDateTime(ts)), ts format Null;\r\nMemoryTracker: Peak memory usage (for query): 8.09 MiB.\r\n0 rows in set. Elapsed: 27.058 sec. Processed 220.29 million rows, 1.94 GB (8.14 million rows/s., 71.61 MB/s.)\r\n```\r\n\r\n## With normal DateTime, with normal date range 2001-09-09 01:46:40 - 2004-11-09 11:33:20\r\n\r\ndate range is important to select function toStartOfMonth / toStartOfDay,\r\n\r\n```\r\ndrop table order_test_1;\r\nCREATE TABLE order_test_1 (ts DateTime,lc Int32) ENGINE = MergeTree \r\nPRIMARY KEY (toStartOfMonth(ts), lc) ORDER BY (toStartOfMonth(ts), lc, ts)\r\nas (select (number/10 + 1000000000) as ts, (number % 16) as lc from numbers(1000000000));\r\n\r\nset max_block_size=1000, max_threads=1;\r\n\r\nselect * from order_test_1 where lc in (0,1) order by toStartOfMonth(ts), ts limit 1000 format Null;\r\nMemoryTracker: Peak memory usage (for query): 28.13 MiB.\r\n0 rows in set. Elapsed: 0.230 sec. Processed 2.38 million rows, 19.02 MB (10.38 million rows/s., 82.86 MB/s.)\r\n```\r\n----\r\nPeak memory usage (for query): 28.13 MiB.\r\nElapsed: 0.230 sec. Processed 2.38 million rows\r\n\r\n\r\n\r\n"
      },
      {
        "user": "bepec",
        "created_at": "2023-05-15T10:48:06Z",
        "body": "@den-crane thanks for comment.\r\n\r\n> The problem here is that the table is ordered by (lc, ts) and you query needs by ts.\r\n> Because the order does not match, Clickhouse sorts the data, and does not use index for read_in_order.\r\n\r\nThe intuition behind PRIMARY KEY(lc, ts) is that it may enable stream selected lc pre-ordered by ts in parallel without full sort -- the same way it happens when I represent a table with sorted UNION ALL sub-queries.\r\n\r\nAnyway, thank you for a trick suggestion with coarse time ordering. I tried sorting with toStartOfSecond before, but didn't realize it has to be passed explicitly to SORT BY clause to be effectively used by planner. It looks good on my toy table, will check how it plays out with the real data.\r\nWould be nice if planner recognized monotonic key function automatically in this case."
      },
      {
        "user": "bepec",
        "created_at": "2023-05-15T12:11:24Z",
        "body": "> do you really need all rows?\r\n\r\nActually, yes, in this particular use case we need to playback stream of time-ordered events for simulation."
      }
    ]
  },
  {
    "number": 49379,
    "title": "How to return an error from External UDFs?",
    "created_at": "2023-05-01T21:02:02Z",
    "closed_at": "2024-05-10T16:14:03Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/49379",
    "body": "How do you return an error from an external UDF?   An external UDF is a daemon-like process that constantly running, reading from STDIN, and writing response to STDOUT.   One way is to let the process die when an error happens but it's not ideal because starting up the process is costly, and also I cannot return a meaningful error message.\r\n\r\nSuppose there is an external UDF `f(key)`.  A key is one of `a` or `b`.  When something else is passed, I would like to raise \"invalid key\" error, or at least raise a generic error without interrupting the process.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/49379/comments",
    "author": "knoguchi",
    "comments": [
      {
        "user": "davenger",
        "created_at": "2023-05-04T14:21:08Z",
        "body": "There is no special way to return error, but you can modify your UDF to return a tuple with 2 elements: f(key) and error_message.\r\n\r\nSet the return type as \"Tuple(UInt64,String)\"\r\n```\r\n<functions>                                     \r\n    <function>                                  \r\n        <type>executable</type>                 \r\n        <name>test_function_python</name>       \r\n        <return_type>Tuple(UInt64,String)</return_type>       \r\n        <argument><type>String</type></argument>\r\n        <argument><type>String</type></argument>\r\n        <format>TabSeparated</format>           \r\n        <command>test_function.py</command>     \r\n        <execute_direct>1</execute_direct>      \r\n    </function>                                 \r\n</functions>\r\n```\r\n\r\nIn the UDF write the return value as \"(result, message)\"\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport sys\r\n\r\nif __name__ == '__main__':\r\n    i = 0\r\n    for line in sys.stdin:\r\n        arg1, arg2 = line.rstrip().split('\\t')\r\n        message = f'arguments are: arg1={arg1} arg2={arg2}'\r\n        print(f'({i},\\'{message}\\')', end='\\n')\r\n        sys.stdout.flush()\r\n        i += 1\r\n\r\n```\r\n\r\nThen you can access the result value and the message as elements of the tuple:\r\n```\r\nSELECT\r\n    test_function_python(number, number + 1) AS res,\r\n    res.1 AS result,\r\n    res.2 AS message\r\nFROM numbers(5)\r\n\r\nQuery id: fbe4d0ee-a614-4a1c-9b5d-0cdfb0ca4279\r\n\r\n\u250c\u2500res\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500result\u2500\u252c\u2500message\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (0,'arguments are: arg1=0 arg2=1') \u2502      0 \u2502 arguments are: arg1=0 arg2=1 \u2502\r\n\u2502 (1,'arguments are: arg1=1 arg2=2') \u2502      1 \u2502 arguments are: arg1=1 arg2=2 \u2502\r\n\u2502 (2,'arguments are: arg1=2 arg2=3') \u2502      2 \u2502 arguments are: arg1=2 arg2=3 \u2502\r\n\u2502 (3,'arguments are: arg1=3 arg2=4') \u2502      3 \u2502 arguments are: arg1=3 arg2=4 \u2502\r\n\u2502 (4,'arguments are: arg1=4 arg2=5') \u2502      4 \u2502 arguments are: arg1=4 arg2=5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "knoguchi",
        "created_at": "2023-05-04T19:34:32Z",
        "body": "Thanks for the idea.  That's one way to achieve my goal.\r\n\r\nHere is my random thought.  If we can introduce header+body just like HTTP, the UDF can return error easily.   There is a config `send_chunk_header` in the XML that adds a header in the request.  Similarly it could add something like receive_status_header.\r\n"
      },
      {
        "user": "davenger",
        "created_at": "2023-05-09T11:48:38Z",
        "body": "Actually returning error from UDF might not be the best approach because typically a query processes not one row but a set of rows. If the query calls UDF for those rows and one of the rows makes the UDF return an error, then the whole query will fail as there is now way to return error for one row and valid results for other rows. So the approach with returning a tuple of result and status (or error message) columns from UDF addresses this scenario."
      }
    ]
  },
  {
    "number": 48836,
    "title": "How clickhouse respect alter delete and insert order ",
    "created_at": "2023-04-17T07:01:08Z",
    "closed_at": "2023-04-20T11:52:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/48836",
    "body": "I have some ETL task that collects data and insert them into clickhouse (22.3.* version). Typically data are inserted into distributed + replicated table (4x4) in bathes. If there is some problem with one or more batches we need to reprocess some of them. Data in reprocessed batch may be same as data inserted before or differ, entirely or partially. Reprocess may occur when we spot problem after few minutes or few days there is no rule here (Mention this to address possibile problem with deduplication mechanism). On reprocessing we do query like: \r\n\r\n```\r\nALTER TABLE table ON CLUSTER cluster DELETE \r\nWHERE event_date > {batch_start_date} AND event_date <= {batch_end_date}\r\n```\r\n\r\nThis will create mutation and my question is whenever execution time of this mutation influence insert that occurs right after ALTER TABLE query?  If data inserted after ALTER QUERY called and before actual execution of mutation may disappear if they meet WHERE clause conditions? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/48836/comments",
    "author": "pulina",
    "comments": [
      {
        "user": "Algunenano",
        "created_at": "2023-04-19T08:45:35Z",
        "body": "For MergeTree tables any part has a `data_version` number which related to when the part was created and what mutations are pending to be applied.\r\n\r\nLet's say that currently your parts are in `data_version=5` and you send a mutation (delete, replace, whatever). Any new part that is inserted will have `data_version=6`, that is they don't need to apply anything, while old parts will be \"mutated\" in the background from version 5 to version 6 by applying the query.\r\nSo data inserted before the mutation was sent receives the value of 5 and will be mutated. Data inserted after the mutation was sent will receive the value of 6 and won't be mutation."
      },
      {
        "user": "pulina",
        "created_at": "2023-04-20T11:52:54Z",
        "body": "Ok and after that there probably will be some part merging and we will end up with correct amount of data in single partition.  Than you. "
      },
      {
        "user": "Algunenano",
        "created_at": "2023-04-20T11:54:47Z",
        "body": "> Ok and after that there probably will be some part merging and we will end up with correct amount of data in single partition.\r\n\r\nYes. And merges only happen across parts with the same `data_version`, so before merging old data with new data the old one needs to be mutated."
      }
    ]
  },
  {
    "number": 48149,
    "title": "partition replace",
    "created_at": "2023-03-29T09:00:09Z",
    "closed_at": "2023-03-29T11:54:10Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/48149",
    "body": "Hello!\r\nHow can the be executed following query?\r\nalter table target_tbl replace partition (\r\n    select top 1 partition from `system`.parts where table in ('tbl_cache') /*or any question returns partition name */\r\n) from tbl_cache\r\n;\r\nThis is required to complete the next case.\r\nI want to implement the following behavior of system tables: target_tbl and tbl_cache. Tables has identical structure and partitioned by \"actuality\" (Data). Everyday a new portion of data is placed in target_tbl. One partition is formed and named like 'YYYY-MM-DD'. Fore example '2023-03-29'. Further i want add single partition of tbl_cache into target_tbl as new if the partition is not there or replace existing one. \r\nI think best way for this is command **alter table ... replace ...** . And i want do this command independent from partition name. It is possible?\r\nMaybe there is another way to implement a similar scenario without explicitly specifying the date?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/48149/comments",
    "author": "elau7e",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-03-29T11:54:10Z",
        "body": "No. Such alters are not supported.\r\nYou can generate an SQL script:\r\n\r\n```sql\r\n \r\nselect 'alter table target_tbl replace partition '||partition||' from tbl_cache;' \r\nfrom ( \r\nselect top 1 partition from system.parts where table in ('tbl_cache') /*or any question returns partition name */\r\n)\r\n;\r\n```"
      },
      {
        "user": "elau7e",
        "created_at": "2023-03-30T09:31:08Z",
        "body": "Thank you very much for your help!\r\n\r\nThere is in your answer script generated, but not executed. \r\nIt would be great if the ClickHouse could execute it (generated script) immediately like Oracle/PostgreSQL **EXECUTE** _'generated sql'_   command.\r\n\r\nI hope there will be such a feature."
      }
    ]
  },
  {
    "number": 48107,
    "title": "generate xml access control configuration from SQL ",
    "created_at": "2023-03-28T12:15:06Z",
    "closed_at": "2023-03-28T12:58:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/48107",
    "body": "I created a user with specific role configuration using the SQL-driven Access Control and Account Management, \r\nis it possible to generate the xml user/role configuration from the previous queries to copy it to a production server ? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/48107/comments",
    "author": "tahayk",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-03-28T12:24:49Z",
        "body": "No. XML and SQL user management are two independent not compatible mechanisms. "
      },
      {
        "user": "tahayk",
        "created_at": "2023-03-28T12:58:49Z",
        "body": "@den-crane thank you for you quick reply."
      }
    ]
  },
  {
    "number": 47949,
    "title": "Default compression behavior",
    "created_at": "2023-03-23T16:54:21Z",
    "closed_at": "2023-03-23T17:14:55Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/47949",
    "body": "I have a CH DB version 22.03 with some data running with compression method zstd.\r\nI have changed in the file config.xml to compression method lz4\r\nAfter restart CH, I have noted that only new data inserted in a table has parts with file default_compression_codec.txt CODEC(LZ4) and the old data previous to the change has parts with: CODEC(ZSTD(1))\r\nMy question is:\r\nA change in the default compression method only works for new data inserted or I need to run OPTIMIZE TABLE ... FINAL in all the tables to apply the changes?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/47949/comments",
    "author": "arodmond",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-03-23T17:14:55Z",
        "body": ">A change in the default compression method only works for new data inserted \r\n\r\nyes. Only newly inserted and merged data.\r\n\r\n> I need to run OPTIMIZE TABLE ... FINAL in all the tables to apply the changes?\r\n\r\nYes, you need."
      },
      {
        "user": "arodmond",
        "created_at": "2023-03-23T17:22:17Z",
        "body": "Thank you."
      }
    ]
  },
  {
    "number": 47521,
    "title": "OOM on group by query",
    "created_at": "2023-03-13T06:45:57Z",
    "closed_at": "2023-03-14T21:03:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/47521",
    "body": "> You have to provide the following information whenever possible.\r\n\r\nselect v, count(*) ... group by 1 produces OOM\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nI created table from external tsv as MergeTree:\r\n\r\nset max_memory_usage = 8000000000;\r\nset max_memory_usage_for_user = 8000000000;\r\nset max_bytes_before_external_group_by = 10000000;\r\nset max_bytes_before_external_sort = 10000000;\r\n\r\ncreate table if not exists t ENGINE = MergeTree() order by v1 as\r\nselect *\r\nfrom file('/usr/proj/tsv-*', TabSeparated, 'v1 text, v2 text, v3 text, seq bigint');\r\n\r\nTable has 3B rows.\r\n\r\nI run following query:\r\n\r\nselect v1, count(*) from t group by 1 limit 5;\r\n\r\nI receive following error:\r\n\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 7.45 GiB (attempt to allocate chunk of 4225696 bytes), maximum: 7.45 GiB.: While executing SourceFromNativeStream. (MEMORY_LIMIT_EXCEEDED)\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/47521/comments",
    "author": "rikuiki",
    "comments": [
      {
        "user": "rikuiki",
        "created_at": "2023-03-14T06:16:32Z",
        "body": "Alexey, maybe you have a chance to comment why do you think it is not a potential bug?.."
      },
      {
        "user": "nickitat",
        "created_at": "2023-03-14T21:03:40Z",
        "body": "memory limit currently strictly limits only pre aggregation phase (separate aggregation on execution threads). at the time of merging we need to bring smth around `total_aggr_state_size * aggregation_memory_efficient_merge_threads / NUM_BUCKETS (=256)` into memory. and you got error at merging phase as you see.\r\nas a workaround you could try to lower `aggregation_memory_efficient_merge_threads` if increasing memory limit is not an option.\r\n#40588 is aimed to address this issue \r\nP.S. it is not a good idea to set `max_bytes_before_external_group_by` too low. the lower this setting the more intermediate files will be generated. apart from consuming time it also affect memory usage in a bad way: for reading these files we will allocate 1MB buffer for each of them. you could easily get hundreds or even thousands of such small files with really small `max_bytes_before_external_group_by`. good default value is half of the memory limit."
      },
      {
        "user": "rikuiki",
        "created_at": "2023-03-15T01:36:23Z",
        "body": "Thank you for explanation!\r\n\r\n> it is not a good idea to set max_bytes_before_external_group_by too low. \r\n\r\nI did this just for testing purposes, to check if ClickHouse can handle aggregation when data doesn't fit into memory, and reduced it even more when seen OOM."
      }
    ]
  },
  {
    "number": 47369,
    "title": "The splitByChar function cannot use invisible characters",
    "created_at": "2023-03-08T14:51:42Z",
    "closed_at": "2023-03-08T21:44:35Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/47369",
    "body": "My business data is split per row with '\\036'. I want to split columns using splitByChar, but an error is reported.\r\n\r\nThe usage is as follows:\r\n\r\nselect splitByChar('\\036',message) as signSec from kafka.kafka_padx_otc_v2_log limit 10;\r\n\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception:  Illegal separator for function splitByChar. Must be exactly one byte..",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/47369/comments",
    "author": "ZXBmmt",
    "comments": [
      {
        "user": "cangyin",
        "created_at": "2023-03-08T15:42:10Z",
        "body": "As per #37765, literals like '\\036' are not parsed as oct numbers. you have to use hexidecimal form.\r\n\r\n```sql\r\nselect splitByChar('\\x41', 'XXAYY')\r\n\r\n\u250c\u2500splitByChar('A', 'XXAYY')\u2500\u2510\r\n\u2502 ['XX','YY']               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-03-08T16:37:42Z",
        "body": "You mean?\r\n\r\nDecimal | Octal | Hex | Binary | Value | Description\r\n-- | -- | -- | -- | -- | --\r\n030 | 036 | 1E | 0001 1110 | RS | request to send/record separator\r\n\r\n```sql\r\nselect  splitByChar('\\x1E', 'A\\x1EAA');\r\n\u250c\u2500splitByChar('', 'AAA')\u2500\u2510\r\n\u2502 ['A','AA']             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```sql\r\nselect  splitByChar(char(30), 'A\\x1EAA');\r\n\u250c\u2500splitByChar(char(30), 'AAA')\u2500\u2510\r\n\u2502 ['A','AA']                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n"
      },
      {
        "user": "ZXBmmt",
        "created_at": "2023-03-09T01:14:27Z",
        "body": "yes, thank you very much !"
      }
    ]
  },
  {
    "number": 46943,
    "title": "Naming convention about 'abstract' class",
    "created_at": "2023-02-27T09:35:59Z",
    "closed_at": "2023-02-27T13:26:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46943",
    "body": "I am learning from CH code and have a question about naming convention for abstract class. I see some classes called 'I-***', for example IColumn, IDataType, have defined both pure virtual functions and normal virtual functions. In Java/C# context, these classes are 'abstract' classes. I am wondering if the classes with 'I-***' names are representing both abstract classes and interfaces in CH source code. \r\n\r\nBy the way, is there no specific distinction between the abstract class and the interface in CH source code? Just want to follow the convention.\r\n\r\nThank you. ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46943/comments",
    "author": "Alex-Cheng",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2023-02-27T13:26:17Z",
        "body": "I would say that C++ does not have such a concept as an \"interface\". Java and C# have restrictions on multiple inheritance (to avoid the \"diamond problem\"), that's why interfaces are required (to allow classes implementing multiple interfaces). In C++ it's totally fine to have \"interfaces\" (well, maybe it would be better to call them \"the most base abstract classes\") with non-pure virtual methods, non-virtual methods, and even with fields. So the 'I-' prefix in a class name indicates that it's \"the most base abstract class\": it may or may not be a pure interface, it does not really matter. "
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-27T14:10:06Z",
        "body": "Thank you very much. So in CH repository, the I-* named classes are abstract base classes or interfaces (if there are all pure virtual functions) and we don't care about distingushing them. "
      }
    ]
  },
  {
    "number": 46757,
    "title": "Join in materialized view is 60x slower than in select statement.",
    "created_at": "2023-02-23T02:52:11Z",
    "closed_at": "2023-02-23T04:27:49Z",
    "labels": [
      "question",
      "comp-joins",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46757",
    "body": "**Describe the situation**\r\n\r\nSeems materialized view with join not working with **FillRightFirst** plan, please loot at the script blew.\r\n\r\nThank you for your time. \r\n\r\nSelect statement statistics: \r\n`0 rows in set. Elapsed: 0.021 sec. Processed 262.02 thousand rows, 2.10 MB (12.69 million rows/s., 101.50 MB/s.)`\r\n\r\nMaterialized view statistics(a insert statement to source table):\r\n\r\n`1 row in set. Elapsed: 1.422 sec.`\r\n\r\n**How to reproduce**\r\n1. Create null table t_null;\r\n2. Create mergetree table t_numbers with 100000000 rows;\r\n3. Execute Select Statement 1 as blow code;\r\n4. Create materialized view with the same query;\r\n5. Insert only 1 record into t_null to trigger materialized view;\r\n\r\n\r\n* Which ClickHouse server version to use\r\nClickHouse client version 22.8.13.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 22.8.13 revision 54460.\r\n\r\n* Non-default settings, if any\r\nNone\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\n`CREATE TABLE t_numbers\r\n(\r\n    `A` UInt64,\r\n    `B` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY A;`\r\n\r\n `CREATE TABLE t_null\r\n(\r\n    `A` UInt64,\r\n    `B` String\r\n)\r\nENGINE = Null;`\r\n\r\n`CREATE MATERIALIZED VIEW mv_null\r\nENGINE = MergeTree\r\nORDER BY A AS\r\nSELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A;`\r\n\r\n* Sample data for all these tables\r\n\r\n`INSERT INTO t_numbers SELECT\r\n    number,\r\n    toString(cityHash64(number))\r\nFROM numbers(100000000)`\r\n\r\n* Queries to run that lead to slow performance\r\n\r\nQuery 1, which runs very fast!\r\n\r\n`SELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A;\r\n`\r\nQuery 2, which runs very slow!\r\n\r\n`INSERT INTO t_null VALUES(1, 'insert');`\r\n \r\n\r\n**Expected performance**\r\nStatement in materialized view runs as fast as select statement, instead of 67x slow.\r\n\r\n**Additional context**\r\n\r\nIf we check the plan, clickhouse choose FillRightFirst for both inner and outer join, may be materialized view is not using the same plan?\r\n\r\nEXPLAIN\r\nSELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A\r\n\r\nQuery id: 0e4c95b7-204e-4922-8d01-414f6ad86758\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                                      \u2502\r\n\u2502   Join (JOIN **FillRightFirst**)                                                                     \u2502\r\n\u2502     Expression (Before JOIN)                                                                     \u2502\r\n\u2502       ReadFromStorage (Null)                                                                     \u2502\r\n\u2502     Expression ((Joined actions + (Rename joined columns + (Projection + Before ORDER BY))))     \u2502\r\n\u2502       Join (JOIN **FillRightFirst**)                                                                 \u2502\r\n\u2502         Expression (Before JOIN)                                                                 \u2502\r\n\u2502           ReadFromMergeTree (rtd.t_numbers)                                                      \u2502\r\n\u2502         Expression ((Joined actions + (Rename joined columns + (Projection + Before ORDER BY)))) \u2502\r\n\u2502           ReadFromStorage (Null)                                                                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46757/comments",
    "author": "qiang5714",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-02-23T04:22:34Z",
        "body": "You compare incorrectly.\r\n\r\nt_null has ENGINE = Null, so it's empty in the SELECT and not empty in the MATVIEW!!!\r\n\r\n```sql\r\nCREATE TABLE t_Memory ( A UInt64,B String ) ENGINE = Memory;\r\nINSERT INTO t_Memory VALUES(1, 'insert');\r\n\r\nSELECT null_outer.A, concat(null_outer.B, subquery.B) \r\n  FROM t_Memory AS null_outer LEFT JOIN \r\n ( SELECT num.A, 'inner' AS B FROM t_numbers AS num \r\n   INNER JOIN t_Memory AS null_inner ON num.A = null_inner.A ) AS subquery ON null_outer.A = subquery.A; \r\n\r\n1 row in set. Elapsed: 0.406 sec. Processed 100.00 million rows, 800.00 MB (246.21 million rows/s., 1.97 GB/s.)\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-02-23T04:27:14Z",
        "body": "You can use this \r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW mv_null ENGINE = MergeTree ORDER BY A \r\nAS SELECT null_outer.A, concat(null_outer.B, subquery.B) \r\n   FROM t_null AS null_outer \r\n   LEFT JOIN ( \r\n               SELECT A, 'inner' AS B FROM t_numbers where A in (select A from t_null)    \r\n             ) AS subquery ON null_outer.A = subquery.A;\r\n\r\nINSERT INTO t_null VALUES(1, 'insert');\r\n\r\n1 row in set. Elapsed: 0.021 sec.\r\n\r\nINSERT INTO t_null VALUES(1, 'insert');\r\n\r\n1 row in set. Elapsed: 0.014 sec.\r\n```"
      },
      {
        "user": "qiang5714",
        "created_at": "2023-02-23T06:37:49Z",
        "body": "@den-crane \r\nThank you for you time. I understand that t_null is not null in materialized view, I did this on purpose because I want to find a way to trigger a materialized view from another table except the left most one in the Join.\r\n\r\nSuppose we have two tables, t_left and t_right, a materialized view will not trigger by t_right if we use \r\n\r\n`t_left join t_right on t_left.id = t_right.id`\r\n\r\nbut, if my program can collect both t_left.id and t_right.id,  and we can insert all id into a t_null to do this, as we change the view to \r\n\r\n`t_null left join (t_left join t_right on t_left.id = t_right.id) as t_inner on t_null.id = t_inner.id `\r\n\r\nand do this:\r\n\r\n`insert into t_null values(t_left.id)(t_right.id);`\r\n\r\nthen all data changed in t_left and t_right will trigger the materialized view\u3002 And, If t_left and t_right is very large, we should use t_null as a filter, put it in the right side, that should make t_left join t_right very fast\uff0c that's what I want.  just like what your advice did:\r\n\r\n`t_null left join (t_left join t_right on t_left.id = t_right.id and **t_right.id in (select trigger.id from t_null trigger)**) as t_inner on t_null.id = t_inner.id `\r\n\r\nMy question is, Why my query is slow than yours, or **why IN operator is faster than join**?\r\n\r\nThank you."
      },
      {
        "user": "den-crane",
        "created_at": "2023-02-23T13:36:24Z",
        "body": "Clickhouse inner join has an optimization to join with empty tables:\r\n\r\n```sql\r\nCREATE TABLE left ( A UInt64,B String ) ENGINE = MergeTree ORDER BY A as \r\nSELECT number, toString(cityHash64(number)) FROM numbers(100000000);\r\n\r\nCREATE TABLE right ( A UInt64, B String ) ENGINE = MergeTree ORDER BY A;\r\n\r\nselect count() from left inner join right using A;\r\n\r\n1 row in set. Elapsed: 0.010 sec. Processed 261.64 thousand rows, 2.09 MB (27.24 million rows/s., 217.93 MB/s.)\r\n\r\n\r\ninsert  into right SELECT number, toString(cityHash64(number)) FROM numbers(1);\r\n\r\nselect count() from left inner join right using A;\r\n1 row in set. Elapsed: 0.367 sec. Processed 100.00 million rows, 800.00 MB (272.51 million rows/s., 2.18 GB/s.)\r\n```\r\nwhat's why SELECT with join with engine Null is fast.\r\n\r\n---------------------\r\nClickhouse `JOIN` does not use indexes, Clickhouse `IN` uses indexes.\r\n\r\n```sql\r\nselect count() from left inner join right using A;\r\n1 row in set. Elapsed: 0.367 sec. Processed 100.00 million rows, 800.00 MB (272.56 million rows/s., 2.18 GB/s.)\r\n\r\nselect count() from left inner join right using B;\r\n1 row in set. Elapsed: 1.030 sec. Processed 100.00 million rows, 2.84 GB (97.12 million rows/s., 2.76 GB/s.)\r\n\r\nselect count() from left where A in (select A from right);\r\n1 row in set. Elapsed: 0.010 sec. Processed 8.19 thousand rows, 65.54 KB (856.87 thousand rows/s., 6.85 MB/s.)\r\n\r\nselect count() from left where B in (select B from right);\r\n1 row in set. Elapsed: 0.932 sec. Processed 100.00 million rows, 2.84 GB (107.29 million rows/s., 3.05 GB/s.)\r\n```\r\n`where A in (` is fast because it uses primary index of the `left`\r\n"
      },
      {
        "user": "qiang5714",
        "created_at": "2023-02-24T02:23:46Z",
        "body": "Got it, no more questions. Thank you very much!"
      }
    ]
  },
  {
    "number": 46637,
    "title": "Distributed table with distributed_group_by_no_merge settings",
    "created_at": "2023-02-21T09:05:21Z",
    "closed_at": "2023-02-22T02:31:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46637",
    "body": "im using clickhouse version 22.9.7.34 with 2 shards\r\n\r\neach shard contains replicated and distributed table(to query across shard). below the details:\r\nshard 1\r\n```\r\nCREATE TABLE default.repli          (id String default '', `rank` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/000-000/repli', 'clickhouse00')  order by (id)     SETTINGS index_granularity = 8192;\r\nCREATE TABLE default.distri    (id String default '', `rank` String)\r\nENGINE = Distributed('local-clickhouse', 'default', 'repli', rand());\r\ninsert into default.repli(id, rank) values ('1','a');\r\n```\r\n\r\nshard 2\r\n```\r\nCREATE TABLE default.repli          (id String default '', `rank` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/001-000/repli', 'clickhouse1')  order by (id)     SETTINGS index_granularity = 8192;\r\nCREATE TABLE default.distri    (id String default '', `rank` String)\r\nENGINE = Distributed('local-clickhouse', 'default', 'repli', rand());\r\ninsert into default.repli(id, rank) values ('2','a');\r\n```\r\n\r\n\r\nshard 1\r\n```\r\nselect any(id),rank from default.distri group by rank settings distributed_group_by_no_merge = 1;// return 2 rows\r\nselect any(id),rank from default.distri group by rank limit 1 settings distributed_group_by_no_merge = 1;// still return 2 rows\r\n```\r\n\r\nis distributed_group_by_no_merge executed after limit and offset?. Please help to solve this issue, thanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46637/comments",
    "author": "Kev1ntan",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-02-21T13:42:09Z",
        "body": "try distributed_group_by_no_merge = 2\r\n```\r\n --distributed_group_by_no_merge arg                                                 \r\n      If 1, Do not merge aggregation states from different servers for distributed queries (shards will process query up to the Complete stage, initiator just proxies the data from the shards). \r\n      If 2 the initiator will apply ORDER BY and LIMIT stages (it is not in case when shard process query up to the Complete stage)```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-02-21T13:43:23Z",
        "body": "Also see, `set optimize_distributed_group_by_sharding_key=1` instead of distributed_group_by_no_merge\r\n\r\n```\r\n--optimize_distributed_group_by_sharding_key arg    \r\n   Optimize GROUP BY sharding_key queries (by avoiding costly aggregation on the initiator server).\r\n```"
      },
      {
        "user": "Kev1ntan",
        "created_at": "2023-02-22T02:31:23Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 46470,
    "title": "Is there any way to view keeper data without system.zookeeper table?",
    "created_at": "2023-02-16T09:47:03Z",
    "closed_at": "2023-02-16T10:59:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46470",
    "body": "I deployed a keeper cluster as a separate service. Is there a client or api that allows me to operate the keeper service directly ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46470/comments",
    "author": "hhalei",
    "comments": [
      {
        "user": "maybedino",
        "created_at": "2023-02-16T10:40:53Z",
        "body": "You should be able to use any zookeeper client for clickhouse-keeper."
      },
      {
        "user": "hhalei",
        "created_at": "2023-02-16T11:38:53Z",
        "body": "This way is OK\uff0cthanks\uff01"
      }
    ]
  },
  {
    "number": 46049,
    "title": "mutation update throw NOT_FOUND_COLUMN_IN_BLOCK exception",
    "created_at": "2023-02-05T08:16:31Z",
    "closed_at": "2023-02-07T03:27:17Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46049",
    "body": "ClickHouse server version 22.9.3 revision 54460\r\n\r\n**Describe what's wrong**\r\n\r\nWhen I try to update the table by `ALTER TABLE events UPDATE costs_usd = cost_usd / 100 WHERE costs_usd = 0 AND cost_usd > 0;`\r\n\r\nI see **Code: 10. DB::Exception: Not found column clickid in block.** in system.mutations table\r\n\r\nthe `show create table events` result:\r\n\r\n```\r\nCREATE TABLE demo.events\r\n(\r\n    `event_time` DateTime('UTC'),\r\n    `queue_msg_id` String DEFAULT '',\r\n    `click_id` FixedString(32),\r\n    `clickid` String DEFAULT '',\r\n    `host` LowCardinality(String),\r\n    `post_status` LowCardinality(String),\r\n    `cost_usd` Int32,\r\n    `costs_usd` Decimal(12, 6) DEFAULT 0,\r\n    `costs_rub` Decimal(12, 6) DEFAULT 0,\r\n    `costs_cny` Decimal(12, 6) DEFAULT 0,\r\n    `costs_gbp` Decimal(12, 6) DEFAULT 0,\r\n    `costs_eur` Decimal(12, 6) DEFAULT 0,\r\n    `cost_rub` Int32,\r\n    `cost_cny` Int32,\r\n    `cost_eur` Int32,\r\n    `cost_gbp` Int32,\r\n    `rev_usd` Int32,\r\n    `revs_usd` Decimal(10, 4) DEFAULT 0,\r\n    `revs_rub` Decimal(10, 4) DEFAULT 0,\r\n    `revs_cny` Decimal(10, 4) DEFAULT 0,\r\n    `revs_gbp` Decimal(10, 4) DEFAULT 0,\r\n    `revs_eur` Decimal(10, 4) DEFAULT 0,\r\n    `rev_rub` Int32,\r\n    `rev_cny` Int32,\r\n    `rev_eur` Int32,\r\n    `rev_gbp` Int32,\r\n    `country` LowCardinality(String),\r\n    `region` LowCardinality(String),\r\n    `city` LowCardinality(String),\r\n    `postal_code` LowCardinality(String),\r\n    `isp` LowCardinality(String),\r\n    `asn` LowCardinality(String) DEFAULT '',\r\n    INDEX clickid clickid TYPE minmax GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(event_time)\r\nORDER BY (owner_id, camp_id, event_time)\r\nSETTINGS min_rows_for_wide_part = 1000000, index_granularity = 8192 \r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46049/comments",
    "author": "jasonbigl",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2023-02-06T14:39:45Z",
        "body": "> version 22.9.3\r\n\r\nFirst of all, this version is obsolete, and you should upgrade. \r\n\r\n> ALTER TABLE events UPDATE costs_usd = cost_usd / 100 WHERE costs_usd = 0 AND cost_usd > 0;\r\n> Not found column clickid in block\r\n\r\nThis ALTER does not use `clickid` column. Probably some previous mutation has failed with this error, so all subsequent mutations simply rethrow the error.\r\n\r\nPlease share `select * from system.mutations where is_done=0` "
      },
      {
        "user": "jasonbigl",
        "created_at": "2023-02-07T03:26:50Z",
        "body": "> > version 22.9.3\r\n> \r\n> First of all, this version is obsolete, and you should upgrade.\r\n> \r\n> > ALTER TABLE events UPDATE costs_usd = cost_usd / 100 WHERE costs_usd = 0 AND cost_usd > 0;\r\n> > Not found column clickid in block\r\n> \r\n> This ALTER does not use `clickid` column. Probably some previous mutation has failed with this error, so all subsequent mutations simply rethrow the error.\r\n> \r\n> Please share `select * from system.mutations where is_done=0`\r\n\r\nHi, thank you for point the reason that it's because other mutation fails, I have tried a few times and alter clickid to a new column. now all works good, and `select * from system.mutations where is_done=0` return **0 rows**, `select count() from system.mutations` return 43.\r\n\r\nThank you for your time :)"
      }
    ]
  },
  {
    "number": 45951,
    "title": "Why size field of StringRef is 64bit (8 bytes)",
    "created_at": "2023-02-02T08:17:10Z",
    "closed_at": "2023-02-02T17:42:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45951",
    "body": "StringRef is usually used for representing a string and contains a pointer and size. A pointer has to be 64 bit in my x64 machine, however the size is not necessarily 64bit in my opinion, because usually string's length is less then 65535 and two bytes is enough. \r\n\r\nFor each string, 6 bytes are wasted. For big amount of strings, the wasted memory is considerable.\r\n\r\nWhy we choose 64bit (size_t) for string's size? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45951/comments",
    "author": "Alex-Cheng",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-02T17:41:25Z",
        "body": "4 bytes are sometimes not enough (there are memory ranges larger than 4 GiB)."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-02T17:46:21Z",
        "body": "Alignment often makes this saving useless. For example, if you have two StringRefs, one adjacent to another, the second one must be aligned by 8 bytes."
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-03T11:44:07Z",
        "body": "in my cases, all strings are less than 1000, and we could avoid alignment by 8 bytes via designing a specific container class (i.e. another implementation of vector<string>. If we did it then we could save a lot of memory, e.g. for 1billion of strings it would save 8GiB memory."
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-06T03:00:36Z",
        "body": "@alexey-milovidov please consider about the idea. I cannot re-open the ticket."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-07T22:12:11Z",
        "body": "It is possible to have strings larger than 4 GB in ClickHouse. Therefore, we should not use just 32 bits for string size."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-07T22:24:19Z",
        "body": "Here is an example with 5 GB string:\r\n\r\n```\r\nmilovidov-desktop :) SELECT length(*) FROM file('/home/milovidov/Downloads/output.tsv', RawBLOB)\r\n\r\nSELECT length(*)\r\nFROM file('/home/milovidov/Downloads/output.tsv', RawBLOB)\r\n\r\nQuery id: 89bbcc01-06b1-4461-9574-2dd8acfd3826\r\n\r\n\u250c\u2500length(raw_blob)\u2500\u2510\r\n\u2502       5491800000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 4.174 sec. \r\n\r\nmilovidov-desktop :)\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-07T22:24:53Z",
        "body": "Limiting something to 32 bit is a signature of old software, I don't want to have these limitations in ClickHouse."
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-09T08:58:24Z",
        "body": "Got it! And StringRef has two fields: ptr, size. As ptr is 8-bytes, then the size of StringRef still 8-bytes even if the size is changed to 32bit."
      }
    ]
  },
  {
    "number": 45945,
    "title": "light weight `ALTER DROP INDEX`",
    "created_at": "2023-02-02T04:41:25Z",
    "closed_at": "2023-02-02T14:13:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45945",
    "body": "Can I just do `ALTER DROP INDEX`, followed by `KILL MUTATION`, to make it a lightweight operation, that only deletes the index metadata and leave the data parts as is ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45945/comments",
    "author": "cangyin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-02-02T13:48:02Z",
        "body": "yes you can , but it will leave garbage files in parts. Why do you need it?"
      },
      {
        "user": "cangyin",
        "created_at": "2023-02-02T14:13:19Z",
        "body": "Because we are going to upgrade index name from `gin` to `inverted`, and table contains to many data parts. As for garbage files,  we can leave them to TTL cleaner script :)."
      },
      {
        "user": "cangyin",
        "created_at": "2023-02-02T14:13:29Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 45616,
    "title": "What endpoint to check with HAProxy",
    "created_at": "2023-01-25T16:26:27Z",
    "closed_at": "2023-01-25T16:37:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45616",
    "body": "Hello, we're using HAProxy to route http reqs to our cluster.\r\n\r\nWhat is the best endpoint to check the replica is alive? We've tried `/replicas_status` or `/ping`.\r\n\r\nIs there any recommended one? E.g. `/replicas_status` often gives errs when ZK decides to put a table to RO mode.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45616/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-25T16:31:51Z",
        "body": "`/ping` is fine.\r\n\r\nWhy do you want to use `/replicas_status` ? Distributed tables query not stale replicas automatically (even if the local replica is stale)."
      },
      {
        "user": "simPod",
        "created_at": "2023-01-25T16:37:19Z",
        "body": "Yes, we've been using `/replicas_status` and worked kinda well with 21.1 but we've upgraded to 22.12 and it caused issues so we've switched to `/ping`.\r\n\r\nThanks for confirmation."
      }
    ]
  },
  {
    "number": 45477,
    "title": "Question: PARTITION BY efficiency in MergeTree table.",
    "created_at": "2023-01-20T16:22:59Z",
    "closed_at": "2023-01-22T14:06:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45477",
    "body": "Hello!\r\nSorry for disturbing, but I want to clarify can I use PARTITION BY in my case.\r\n**The case:**\r\n-  We have table with metrics for multitenant setup. We have columns like tenant_id and event_time. \r\n-  By requirements this table should be optimized for per-tenant use. \r\n-  Almost always tenant will query data for the LAST MONTH or for the LAST day. So we choose ORDER BY (tenant_id, event_time) key. And this works great.\r\n-  Analytics want to use this database also for aggregated statistics (query will have range for event_time and no tenant_id specified). So we decided to add PARTITION BY toYYYYMM(event_time). \r\n- Table will contain data only for one year (will be controlled by database (TTL feature)). So we will get only 12 unique partition keys.\r\n- Table engine will be ReplicatedMergeTree\r\n\r\n**Our results**\r\nTable with PARTITION BY gives us 25% less rows scanned for **analytical** queries on synthetic data. Tenant's queries are good in both cases.\r\n**Question**\r\nIn many places it is pointed that PARTITION BY is not recommended to use for MergeTree-family engines. So I'm afraid that we can run into issues on production because of this decision.\r\nCan we use PARTITION BY in this case or should we refuse to do this despite of better performance in our tests?\r\n\r\nThank you!\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45477/comments",
    "author": "ilya-girman",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-20T17:34:55Z",
        "body": ">In many places it is pointed that PARTITION BY is not recommended to use for MergeTree-family engines.\r\n\r\nWhat places?????? You have misunderstood something. \r\n\r\nPARTITION BY is only for MergeTree-family engines.\r\n\r\nIn your case I would recommend exactly `PARTITION BY toYYYYMM(event_time)`.\r\n\r\n"
      },
      {
        "user": "ilya-girman",
        "created_at": "2023-01-22T14:06:41Z",
        "body": "Thank you a lot!\r\nI wanted to tell that usually it is not recommended to assign PARTITION BY manually."
      }
    ]
  },
  {
    "number": 45232,
    "title": "CANNOT_PARSE_TEXT errors exceeded 600,000 times",
    "created_at": "2023-01-12T17:48:59Z",
    "closed_at": "2023-01-12T18:48:58Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45232",
    "body": "ClickHouse Version: 22.10.1.1248\r\n\r\nThe following errors occur in /var/log/clickhouse-server/clickhouse-server.err.log almost every second.\r\n```\r\n<Error> TCPHandler: Code: 6. DB::Exception: Cannot parse string '2022-11-30 019:48:33.237' as DateTime64(6): syntax error at position 19 (parsed just '2022-11-30 019:48:3'): while executing 'FUNCTION toDateTime64(time : 0, 6 :: 1) -> toDateTime64(time, 6) DateTime64(6) : 2'. (CANNOT_PARSE_TEXT), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./build_docker/../src/Common/Exception.cpp:69: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb29f568 in /usr/bin/clickhouse\r\n1. DB::throwExceptionForIncompletelyParsedValue(DB::ReadBuffer&, DB::IDataType const&) @ 0x6ed06fc in /usr/bin/clickhouse\r\n2. bool DB::callOnIndexAndDataType<DB::DataTypeDateTime64, DB::FunctionConvert<DB::DataTypeDateTime64, DB::NameToDateTime64, DB::ToDateTimeMonotonicity>::executeInternal(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const::'lambda'(auto const&, auto const&)&, DB::ConvertDefaultBehaviorTag>(DB::TypeIndex, auto&&, DB::ConvertDefaultBehaviorTag&&) @ 0x73cec64 in /usr/bin/clickhouse\r\n3. DB::FunctionConvert<DB::DataTypeDateTime64, DB::NameToDateTime64, DB::ToDateTimeMonotonicity>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x73ca5bc in /usr/bin/clickhouse\r\n4. ./build_docker/../src/Functions/IFunction.cpp:0: DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7b158 in /usr/bin/clickhouse\r\n5. ./build_docker/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:115: DB::IExecutableFunction::executeWithoutSparseColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7ba94 in /usr/bin/clickhouse\r\n6. ./build_docker/../contrib/libcxx/include/vector:399: DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7cf64 in /usr/bin/clickhouse\r\n7. ./build_docker/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:115: DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0xf7d7378 in /usr/bin/clickhouse\r\n8. ./build_docker/../contrib/libcxx/include/vector:505: DB::ExpressionActions::execute(DB::Block&, bool) const @ 0xf7d81d0 in /usr/bin/clickhouse\r\n9. ./build_docker/../contrib/libcxx/include/vector:1416: DB::MergeTreePartition::executePartitionByExpression(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Block&, std::__1::shared_ptr<DB::Context const>) @ 0x106413f0 in /usr/bin/clickhouse\r\n10. ./build_docker/../contrib/libcxx/include/list:916: DB::MergeTreeDataWriter::splitBlockIntoParts(DB::Block const&, unsigned long, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::Context const>) @ 0x106960e0 in /usr/bin/clickhouse\r\n11. ./build_docker/../contrib/libcxx/include/vector:1408: DB::MergeTreeSink::consume(DB::Chunk) @ 0x107b13e4 in /usr/bin/clickhouse\r\n12. ./build_docker/../contrib/libcxx/include/__memory/shared_ptr.h:702: DB::SinkToStorage::onConsume(DB::Chunk) @ 0x10b84270 in /usr/bin/clickhouse\r\n13. ./build_docker/../contrib/libcxx/include/__memory/shared_ptr.h:702: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::ExceptionKeepingTransform::work()::$_1, void ()> >(std::__1::__function::__policy_storage const*) @ 0x10af2474 in /usr/bin/clickhouse\r\n14. ./build_docker/../src/Processors/Transforms/ExceptionKeepingTransform.cpp:122: DB::runStep(std::__1::function<void ()>, DB::ThreadStatus*, std::__1::atomic<unsigned long>*) @ 0x10af2198 in /usr/bin/clickhouse\r\n15. ./build_docker/../contrib/libcxx/include/__functional/function.h:813: DB::ExceptionKeepingTransform::work() @ 0x10af1abc in /usr/bin/clickhouse\r\n16. ./build_docker/../src/Processors/Executors/ExecutionThreadContext.cpp:52: DB::ExecutionThreadContext::executeTask() @ 0x109471a0 in /usr/bin/clickhouse\r\n17. ./build_docker/../src/Processors/Executors/PipelineExecutor.cpp:228: DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x1093c1ac in /usr/bin/clickhouse\r\n18. ./build_docker/../src/Processors/Executors/PipelineExecutor.cpp:127: DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0x1093b654 in /usr/bin/clickhouse\r\n19. ./build_docker/../src/Server/TCPHandler.cpp:713: DB::TCPHandler::processInsertQuery() @ 0x108eba3c i\r\n```\r\n\r\nI didn't call the toDateTime64 function, the only thing that may have affected is this table:\r\n```\r\n-- simplify\r\nCREATE TABLE test.test_tb(\r\n    `time` String,\r\n    a String,\r\n    b String,\r\n    c String\r\n) \r\nENGINE = ReplacingMergeTree()\r\nPARTITION BY toDate(toDateTime64(time, 6))\r\nORDER BY (a, b, c);\r\n```\r\nBut I have also truncate the table data\r\n\r\nWhy does this error keep happening? Is there a good way to locate it?\r\n\r\nThanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45232/comments",
    "author": "Onehr7",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-12T18:44:23Z",
        "body": "> didn't call the toDateTime64 function, the only thing that may have affected is this table:\r\n\r\n`PARTITION BY toDate(          toDateTime64(           time, 6))`\r\n\r\n\r\n```\r\nselect toDateTime64('2022-11-30 019:48:33.237', 6);\r\n\r\nDB::Exception: Cannot parse string '2022-11-30 019:48:33.237' as DateTime64(6):\r\n```\r\n\r\n\r\n```sql\r\nselect parseDateTime64BestEffortOrZero('2022-11-30 019:48:33.237', 6);\r\n\u250c\u2500parseDateTime64BestEffortOrZero('2022-11-30 019:48:33.237', 6)\u2500\u2510\r\n\u2502                                     1970-01-01 00:00:00.000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect toDateTime64OrZero('2022-11-30 019:48:33.237', 6);\r\n\u250c\u2500toDateTime64OrZero('2022-11-30 019:48:33.237', 6)\u2500\u2510\r\n\u2502                        1970-01-01 00:00:00.000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-01-12T18:48:31Z",
        "body": "I suggest to use `parseDateTime64BestEffortOrZero`\r\n\r\n```sql\r\nPARTITION BY toDate(parseDateTime64BestEffortOrZero(time, 6))\r\n```"
      },
      {
        "user": "Onehr7",
        "created_at": "2023-01-13T00:27:45Z",
        "body": "thanks, it works"
      }
    ]
  },
  {
    "number": 44878,
    "title": "String values in Parquet files",
    "created_at": "2023-01-03T18:58:10Z",
    "closed_at": "2023-01-03T20:40:59Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/44878",
    "body": "Somewhat related to #43970, I've noticed ClickHouse doesn't mark string fields as Strings when exporting to Parquet. BigQuery will treat the fields as binary (unless overridden) as will QGIS when opening PQ files generated by ClickHouse.\r\n\r\n```bash\r\n$ cat California.jsonl \\\r\n    | clickhouse local \\\r\n          --input-format JSONEachRow \\\r\n          -q \"SELECT *\r\n              FROM table\r\n              FORMAT Parquet\" \\\r\n    > cali.snappy.pq\r\n```\r\n\r\n```python\r\nIn [1]: import pyarrow.parquet as pq\r\n\r\nIn [2]: pf = pq.ParquetFile('cali.snappy.pq')\r\n\r\nIn [3]: pf.schema\r\nOut[3]: \r\n<pyarrow._parquet.ParquetSchema object at 0x105ccc940>\r\nrequired group field_id=-1 schema {\r\n  optional int64 field_id=-1 release;\r\n  optional binary field_id=-1 capture_dates_range;\r\n  optional binary field_id=-1 geom;\r\n}\r\n```\r\n\r\nMost other tools, including the Rust release of Arrow will mark these fileds as strings.\r\n\r\n```bash\r\n$ json2parquet \\\r\n      -c snappy \\\r\n      California.jsonl \\\r\n      California.snappy.pq\r\n```\r\n\r\n```python\r\nIn [1]: import pyarrow.parquet as pq\r\n\r\nIn [2]: pf = pq.ParquetFile('California.snappy.pq')\r\n\r\nIn [3]: pf.schema\r\nOut[3]: \r\n<pyarrow._parquet.ParquetSchema object at 0x109a11380>\r\nrequired group field_id=-1 arrow_schema {\r\n  optional binary field_id=-1 capture_dates_range (String);\r\n  optional binary field_id=-1 geom (String);\r\n  optional int64 field_id=-1 release;\r\n}\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/44878/comments",
    "author": "marklit",
    "comments": [
      {
        "user": "Avogar",
        "created_at": "2023-01-03T19:29:39Z",
        "body": "It's because ClickHouse `String` type can contain arbitrary binary data, so it can be invalid UTF8. To output ClickHouse `String` type as Parquet `string` you can use special setting `output_format_parquet_string_as_string`"
      },
      {
        "user": "marklit",
        "created_at": "2023-01-03T20:44:15Z",
        "body": "Understood, thank you."
      }
    ]
  },
  {
    "number": 44621,
    "title": "Is 'ON CLUSTER' clause necessary for 'ALTER TABLE DELETE|UPDATE|ADD|DROP' for ReplicatedMergeTree?",
    "created_at": "2022-12-27T03:26:23Z",
    "closed_at": "2022-12-27T17:10:53Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/44621",
    "body": "Saying I have a ReplicatedMergeTree `t1` created by the folllowing query:\r\n```SQL\r\ncreate table db1.events on cluster 'ch-pro-cluster' (ID Int64, x01 String, x02 String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/dd', '{replica}') order by ID partition by ID % 5  settings min_rows_for_wide_part=10, min_bytes_for_wide_part=1;\r\n```\r\n\r\nAnd the table's data has been initialized by query \r\n\r\n`insert into db1.events(ID, x01, x02) select number, leftPad(toString(number + 01), 8, '0'), toString(number + 02) from numbers(5000000)`.\r\n\r\nThen I want to delete, update, add or drop table. I found omitting 'ON CLUSTER' the synchronization can be also done by mechanism of ReplicatedMergeTree.\r\n\r\nFor example, `alter table db1.events delete where ID % 10 = 3 settings mutations_sync=2;` can spread the changes over cluster. Of course, query `alter table db1.events on cluster 'ch-pro-cluster' delete where ID % 10 = 3 settings mutations_sync=2;` also work but the difference is that the later query will add a zk node in `/clickhouse/task_queue/ddl ` and the first query won't. \r\n\r\nWhat is the best practice? Shall I omit 'ON CLUSTER' for doing 'ALTER TABLE ...' on ReplicatedMergeTree tables?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/44621/comments",
    "author": "Alex-Cheng",
    "comments": [
      {
        "user": "save-my-heart",
        "created_at": "2022-12-27T04:50:28Z",
        "body": "Only `ON CLUSTER` clause can spread the changes over cluster.\r\nWithout `ON CLUSTER`, ALTER queries only works on single node, or all replica nodes in a shard."
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2022-12-27T05:22:16Z",
        "body": "My cluster is one shard, multiple replicas. So I think without 'ON CLUSTER', ALTER queries work well."
      },
      {
        "user": "den-crane",
        "created_at": "2022-12-27T17:10:53Z",
        "body": ">My cluster is one shard, multiple replicas\r\n\r\nReplicatedMergeTree performs `ALTER TABLE DELETE|UPDATE|ADD column|DROP column` at all replicas by Replication mechanism, no need  'ON CLUSTER'.\r\n\r\nto perform `create table / drop table` you need to use ON CLUSTER"
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2022-12-28T02:18:53Z",
        "body": "@den-crane  I highly appreciate your help. More question: if I have a multiple shards cluster and table engine is ReplicatedMergeTree, shall I use ON CLUSTER in ALTER TABLE? "
      },
      {
        "user": "den-crane",
        "created_at": "2022-12-29T15:00:56Z",
        "body": ">More question: if I have a multiple shards cluster and table engine is ReplicatedMergeTree, shall I use ON CLUSTER in ALTER TABLE?\r\n\r\nYes, you shall."
      }
    ]
  },
  {
    "number": 44474,
    "title": "ClickHouse Reports a lot of CLIENT_HAS_CONNECTED_TO_WRONG_PORT error",
    "created_at": "2022-12-21T03:36:15Z",
    "closed_at": "2022-12-21T05:57:09Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/44474",
    "body": "When I upgrade ClickHouse from 22.4 to 22.11, it began to report errors.\r\n\r\n```\r\nWITH arrayMap(x -> demangle(addressToSymbol(x)), last_error_trace) AS `all`\r\nSELECT\r\n    name,\r\n    arrayStringConcat(`all`, '\\n') AS res\r\nFROM system.errors\r\nWHERE name = 'CLIENT_HAS_CONNECTED_TO_WRONG_PORT'\r\nSETTINGS allow_introspection_functions = 1\r\n\r\nQuery id: 5b03cb4a-7d0e-48d4-b7d0-299294674f08\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname: CLIENT_HAS_CONNECTED_TO_WRONG_PORT\r\nres:  DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, int, bool)\r\nDB::TCPHandler::receiveHello()\r\nDB::TCPHandler::runImpl()\r\nDB::TCPHandler::run()\r\nPoco::Net::TCPServerConnection::start()\r\nPoco::Net::TCPServerDispatcher::run()\r\nPoco::PooledThread::run()\r\nPoco::ThreadImpl::runnableEntry(void*)\r\n\r\n__clone\r\n\r\n1 row in set. Elapsed: 0.003 sec.\r\n```\r\n\r\nAnd it's a lot.\r\n```\r\nRow 6:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname:               CLIENT_HAS_CONNECTED_TO_WRONG_PORT\r\ncode:               217\r\nvalue:              30361\r\nlast_error_time:    2022-12-21 03:31:03\r\nlast_error_message: Client has connected to wrong port\r\nlast_error_trace:   [228653722,339140737,339111492,339193977,387649044,387655611,389289415,389279741,140518016472585,140518015574323]\r\nremote:             0\r\n```\r\n\r\nI use ClickHouse C++ SDK with TCP port 9000, the timeout is 70s.\r\n\r\nThe result of `netstat` is:\r\n```\r\n[root]# netstat -tnlp | grep click\r\ntcp        0      0 0.0.0.0:9004            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9005            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9009            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9363            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:8123            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9000            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\n```\r\n\r\nThe version is 22.11.2.30.\r\n\r\nI have not encountered this problem at 22.4, and the `system.errors` table did not query this error.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/44474/comments",
    "author": "LGDHuaOPER",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2022-12-21T04:46:48Z",
        "body": "Some health checks maybe? "
      },
      {
        "user": "LGDHuaOPER",
        "created_at": "2022-12-21T05:08:40Z",
        "body": "> Some health checks maybe?\r\n\r\nYes, it has health-check. But will it cause this?"
      },
      {
        "user": "filimonov",
        "created_at": "2022-12-21T05:57:09Z",
        "body": "IF you will send some non-http data to http port, or if you will use http data to a non-http port, such exception is expected."
      },
      {
        "user": "LGDHuaOPER",
        "created_at": "2022-12-21T15:56:55Z",
        "body": "You're right.I did a health check using `curl localhost:9000`."
      }
    ]
  },
  {
    "number": 44043,
    "title": "Error compiling llvm of Clickhouse ",
    "created_at": "2022-12-08T13:25:43Z",
    "closed_at": "2022-12-08T23:52:54Z",
    "labels": [
      "question",
      "build"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/44043",
    "body": "try to compiling clickhouse of the latest\r\nCmake version\r\n3.22.1\r\nNinja version\r\n1.10.1\r\n\r\nFAILED: contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o \r\n/usr/bin/c++ -DSTD_EXCEPTION_HAS_STACK_TRACE=1 -D_GNU_SOURCE -D_LIBCPP_ENABLE_THREAD_SAFETY_ANNOTATIONS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -I/mnt/chuhedb/ClickHouse/build/contrib/llvm-project/llvm/utils/TableGen -I/mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen -I/mnt/chuhedb/ClickHouse/build/contrib/llvm-project/llvm/include -I/mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/include -I/mnt/chuhedb/ClickHouse/base/glibc-compatibility/memcpy -isystem /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include -isystem /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxxabi/include -isystem /mnt/chuhedb/ClickHouse/contrib/libunwind/include -fdiagnostics-color=always -fsized-deallocation -fcoroutines  -pipe -mssse3 -msse4.1 -msse4.2 -mpclmul -mpopcnt -fasynchronous-unwind-tables -ffile-prefix-map=/mnt/chuhedb/ClickHouse=. -falign-functions=32 -Wa,-mbranches-within-32B-boundaries -w -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -pedantic -Wno-long-long -Wimplicit-fallthrough -Wno-maybe-uninitialized -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wnon-virtual-dtor -Wdelete-non-virtual-dtor -Wsuggest-override -Wmisleading-indentation -fdiagnostics-color -ffunction-sections -fdata-sections -O2 -g -DNDEBUG -O3 -g -gdwarf-4  -fno-pie   -D OS_LINUX -g0 -Werror -nostdinc++ -std=c++14 -MD -MT contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o -MF contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o.d -o contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o -c /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp\r\nIn file included from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__functional/invoke.h:17,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/type_traits:421,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/limits:107,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/math.h:309,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/cmath:309,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/include/llvm/Support/MathExtras.h:19,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/include/llvm/ADT/APInt.h:19,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.h:17,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp:14:\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__type_traits/decay.h: In instantiation of 'struct std::__1::__decay<const llvm::APInt&, true>':\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__type_traits/decay.h:56:89:   required from 'struct std::__1::decay<const llvm::APInt&&>'\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__utility/pair.h:132:16:   required by substitution of 'template<class _Tuple, typename std::__1::enable_if<typename std::__1::conditional<(std::__1::__tuple_like_with_size<_Tuple, 2, typename std::__1::remove_cv<typename std::__1::remove_reference<_Tp>::type>::type>::value && (! std::__1::is_same<typename std::__1::decay<_Tp>::type, std::__1::pair<const llvm::Record*, llvm::OpcodeInfo> >::value)), std::__1::pair<const llvm::Record*, llvm::OpcodeInfo>::_CheckTupleLikeConstructor, std::__1::__check_tuple_constructor_fail>::type::__enable_implicit<_Tuple>(), void>::type* <anonymous> > constexpr std::__1::pair<const llvm::Record*, llvm::OpcodeInfo>::pair(_Tuple&&) [with _Tuple = const llvm::APInt&&; typename std::__1::enable_if<typename std::__1::conditional<(std::__1::__tuple_like_with_size<_Tuple, 2, typename std::__1::remove_cv<typename std::__1::remove_reference<_Tp>::type>::type>::value && (! std::__1::is_same<typename std::__1::decay<_Tp>::type, std::__1::pair<const llvm::Record*, llvm::OpcodeInfo> >::value)), std::__1::pair<const llvm::Record*, llvm::OpcodeInfo>::_CheckTupleLikeConstructor, std::__1::__check_tuple_constructor_fail>::type::__enable_implicit<_Tuple>(), void>::type* <anonymous> = <missing>]'\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp:372:69:   required from here\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__type_traits/decay.h:47:30: error: forming pointer to reference type 'std::__1::remove_extent<const llvm::APInt&>::type' {aka 'const llvm::APInt&'}\r\n   47 |                      >::type type;\r\n      |                              ^~~~\r\n[35/6130] Building CXX object contrib/llvm-project/llvm/lib/MC/CMakeFiles/LLVMMC.dir/MCContext.cpp.o",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/44043/comments",
    "author": "Grfire",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-12-08T23:52:54Z",
        "body": "```\r\n/usr/bin/c++\r\n```\r\n\r\nOnly `clang` can be used to compile ClickHouse.\r\n`gcc` cannot be used."
      },
      {
        "user": "Grfire",
        "created_at": "2022-12-27T11:26:04Z",
        "body": "sorry to reply this issues too late. this reason , i used version of clang is outdated"
      }
    ]
  },
  {
    "number": 42858,
    "title": "ParsingException when importing Point data from CSV",
    "created_at": "2022-11-01T11:03:47Z",
    "closed_at": "2022-11-02T08:48:53Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42858",
    "body": "When importing CSV files with geo Point data, I always get this error:\r\n\r\n> `Code: 27. DB::ParsingException: Cannot parse input: expected '\"' before: '(10, 10)\"'`\r\n\r\nTo reproduce the problem under simple conditions, I created a table with a Point column:\r\n`CREATE TABLE default.geo_point (p Point) ENGINE = Memory()`\r\n\r\nI also created a geo_point.csv file with a single point in it:\r\n```\r\n\"(10, 10)\"\r\n```\r\n\r\nI import the file with the clickhouse client (version 22.9.3.18) like this:\r\n`cat geo_point.csv | clickhouse-client -q 'INSERT INTO default.geo_point FORMAT CSV'`\r\n\r\nI get the above error, even when removing the double quotes. Not sure if this is a bug, or if I need to pass a certain flag to the clickhouse-client (I didn't find a flag that works). \r\n\r\nOther datatypes are imported just fine. Also, when I change the data type from Point to String, the import works fine.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42858/comments",
    "author": "aimfeld",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-11-02T08:48:53Z",
        "body": "it should be `10,10` in CSV.\r\n\r\n```\r\nINSERT INTO geo_point VALUES((10, 10));\r\n\r\nselect * from default.geo_point format CSV;\r\n10,10\r\n\r\ninsert  into default.geo_point format CSV 20,20\r\n              ;\r\n\r\nselect * from default.geo_point;\r\n\u250c\u2500p\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (10,10) \u2502\r\n\u2502 (20,20) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "aimfeld",
        "created_at": "2022-11-08T15:36:06Z",
        "body": "Thanks, I tried TabSeparated format instead of CSV, and it worked. I can now import my postgres table into clickhouse, using psql and clickhouse-client:\r\n\r\n`psql -d my_db -U postgres -h localhost -c \"\\COPY (SELECT * FROM my_table) TO stdout WITH (FORMAT TEXT)\" | clickhouse-client  -q 'INSERT INTO my_db.my_table FORMAT TabSeparated'`\r\n"
      }
    ]
  },
  {
    "number": 42557,
    "title": "Join generated data",
    "created_at": "2022-10-21T08:58:34Z",
    "closed_at": "2022-10-21T11:49:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42557",
    "body": "Hello clickhouse community,\r\n\r\nI'm stuck on a statement that I can't make.\r\n\r\nI want to get a table with two columns containing generated data, let me explain:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)(price)\r\n        from pricing\r\n        limit 1)\r\n    ) as quant_min\r\n;\r\n```\r\n\r\nIt gives me:\r\n\r\n```txt\r\nquant_min\r\n0.0000056\r\n0.00850023\r\n0.013097947\r\n0.020124\r\n0.032167118\r\n0.0437904\r\n0.051556416\r\n0.0644\r\n0.0896712\r\n0.1346728\r\n```\r\n\r\nNow this one:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)(price)\r\n        from pricing\r\n        limit 1)\r\n    ) as quant_max\r\n;\r\n\r\ngive me:\r\n\r\n```txt\r\nquant_max\r\n0.00850023\r\n0.0131056\r\n0.02019656\r\n0.032187376\r\n0.0437904\r\n0.0515056\r\n0.06435264\r\n0.0894664\r\n0.1344904\r\n34.22802\r\n```\r\nI want a request where the result is:\r\n\r\n```txt\r\nquant_min       |     quant_max\r\n0.0000056       |     0.00850023\r\n0.00850023      |     0.0131056\r\n0.013097947     |     0.02019656\r\n0.020124        |     0.032187376\r\n0.032167118     |     0.0437904\r\n0.0437904       |     0.0515056\r\n0.051556416     |     0.06435264\r\n0.0644          |     0.0894664\r\n0.0896712       |     0.1344904\r\n0.1346728       |     34.22802\r\n```\r\n\r\nI had try this request:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)(price)\r\n        from zone_hit\r\n        where\r\n        event_date > today() - 3\r\n        and rtb_bid_price > 0\r\n        and hit_type_id = 'rtb'\r\n        limit 1)\r\n    ) as quant_min,\r\n    arrayJoin(\r\n        (select quantilesExact(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)(price)\r\n         from zone_hit\r\n         where event_date > today() - 3\r\n           and rtb_bid_price > 0\r\n           and hit_type_id = 'rtb'\r\n         limit 1)\r\n    ) as quant_max\r\n;\r\n```\r\n\r\nBut it give me 100 results because every quant_min is join to every quant_max.\r\n\r\nIf a life saver is in the neighberhood ?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42557/comments",
    "author": "MockingMagician",
    "comments": [
      {
        "user": "nickitat",
        "created_at": "2022-10-21T10:43:07Z",
        "body": "you can use `arrayZip` to merge arrays:\r\n\r\n``` sql\r\nSELECT\r\n    A.1 AS quant_min,\r\n    A.2 AS quant_max\r\nFROM\r\n(\r\n    SELECT arrayJoin(arrayZip((\r\n            SELECT quantilesExact(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)(number)\r\n            FROM numbers_mt(100)\r\n        ), (\r\n            SELECT quantilesExact(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)(number)\r\n            FROM numbers_mt(100)\r\n        ))) AS A\r\n)\r\n```"
      },
      {
        "user": "MockingMagician",
        "created_at": "2022-10-21T11:48:45Z",
        "body": "Hello @nickitat \r\n\r\nIt works fucking great! That's just what I was looking for.\r\n\r\nHuge thanks to you!"
      },
      {
        "user": "MockingMagician",
        "created_at": "2022-10-21T11:49:36Z",
        "body": "I close it!"
      }
    ]
  },
  {
    "number": 42375,
    "title": "Is there a way to wait while a replicated table will have consistent state between masters?",
    "created_at": "2022-10-17T06:41:00Z",
    "closed_at": "2023-09-16T00:06:48Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42375",
    "body": "In my project I often rename table or alter it's partitions. When the next time process connects to other master, it might try to change table where partitions wasn't changed yet. And this leads to data loss and mysterious behaviour.\r\n\r\n If there a way to wait while all manipulations with a replicated table are done on all masters before I'll apply further changes?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42375/comments",
    "author": "svetlyak40wt",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-10-17T07:14:15Z",
        "body": "`alter table ..... settings mutations_sync=2,replication_alter_partitions_sync=2`\r\n\r\nmutations_sync - Wait for synchronous execution of ALTER TABLE UPDATE/DELETE queries (mutations). 0 - execute asynchronously. 1 - wait current server. 2 - wait all replicas if they exist.\r\n\r\n\r\nreplication_alter_partitions_sync - Wait for actions to manipulate the partitions. 0 - do not wait, 1 - wait for execution only of itself, 2 - wait for everyone."
      },
      {
        "user": "svetlyak40wt",
        "created_at": "2022-10-17T09:17:35Z",
        "body": "Cool! Thank you, Den!\r\n\r\nIs there something like this for RENAME TABLE and EXCHANGE TABLES?"
      },
      {
        "user": "azat",
        "created_at": "2023-09-15T16:22:10Z",
        "body": "For `RENAME`/`EXCHANGE` you can use `ON CLUSTER`"
      }
    ]
  },
  {
    "number": 42318,
    "title": "Example of how to use `remove` override in yaml format?",
    "created_at": "2022-10-14T14:58:20Z",
    "closed_at": "2022-10-15T08:02:59Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42318",
    "body": "I'd like to leave the main config file untouched and simply remove the entries for `postgresql_port` and `mysql_port` in a separate config file. I'm using yaml rather than XML. I'm not seeing an example of this in the docs. Thanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42318/comments",
    "author": "tasdflkjweio",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-10-14T16:35:15Z",
        "body": "```\r\n# cat /etc/clickhouse-server/users.d/xxxxx.yaml\r\nusers:\r\n  default:\r\n    \"@remove\": remove\r\n\r\n# cat /etc/clickhouse-server/config.d/postgresql_port.yaml\r\npostgresql_port:\r\n    \"@remove\": remove\r\nmysql_port:\r\n    \"@remove\": remove\r\n```"
      },
      {
        "user": "tasdflkjweio",
        "created_at": "2022-10-14T17:23:05Z",
        "body": "many thanks!"
      }
    ]
  },
  {
    "number": 41914,
    "title": "Calculate avg from distributed tables returns wrong value",
    "created_at": "2022-09-28T18:14:40Z",
    "closed_at": "2022-09-29T16:57:05Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41914",
    "body": "We having an issue where for a specific dataset (not happening for other customers), the result of calling avg(measurement) always returns 18446744073709552000 which looks like an overflown value and measurement column is of type int64.\r\nThis only happens when running the query agains a virtual distributed table, if we query the actual replicated table it returns the correct value.\r\nWe found a workaround, if we cast measurement value to int64, the avg value returns correct. This is very odd since the column is already an int64 value. The workaround looks something like:\r\n`SELECT avg(toInt64(measurement)) FROM distributed_table`\r\nNot sure why when recasting to int64 or querying the replicated table works and not when querying the distributed table, so we were wondering if this is a known issue or a potential bug.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41914/comments",
    "author": "alazo8807",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-28T21:40:16Z",
        "body": "Please provide `show create table distributed_table` and `show create table local_table` \r\nCheck that columns in both tables have the same types."
      },
      {
        "user": "alazo8807",
        "created_at": "2022-09-29T15:50:03Z",
        "body": "Thanks @den-crane. The distributed table is a virtual one, so the type of the column is the same. \r\nThis is a simplified version of our table definition. In our client application we query the distributed virtual table.\r\n\r\n\r\n```\r\nCREATE TABLE IF NOT EXISTS data.replicated_table_v1\r\nON CLUSTER '{cluster}'\r\n(\r\n    date Date DEFAULT toDate(timestamp, 'UTC') Codec(ZSTD),\r\n    timestamp UInt64 Codec(DoubleDelta, LZ4),\r\n    measure_int Int64 Codec(Gorilla, LZ4),\r\n    ...\r\n)\r\nENGINE = ReplicatedMergeTree(\r\n    '/clickhouse/tables/{shard}/data/replicated_table_v1,\r\n    '{replica}'\r\n)\r\nPARTITION BY toStartOfMonth(date)\r\nORDER BY (\r\n    timestamp\r\n)\r\nTTL date + INTERVAL ${TTL_INTERVAL} DELETE\r\n```\r\n\r\nThen we have a merge virtual table that pulls from all the replicated\r\n```\r\nCREATE TABLE IF NOT EXISTS entity.merged_table\r\nON CLUSTER '{cluster}'\r\nAS data.replicated_table_v1\r\nENGINE = Merge(\r\n    data,\r\n    '^replicated_table_v1'\r\n)\r\n```\r\n\r\nAnd finally another virtual distributed table that pulls from the merged_table\r\n```\r\nCREATE TABLE IF NOT EXISTS entity.distributed_table\r\nAS data.replicated_table_v1\r\nENGINE = Distributed(\r\n    '{cluster}',\r\n    data,\r\n    merged_table\r\n)\r\n\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-29T16:57:05Z",
        "body": ">This is a simplified version of our table definition.\r\n\r\nNo requested info. Closing."
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-29T17:06:13Z",
        "body": "here is an example\r\n\r\n```sql\r\ncreate table local( A Int32, B Int32) Engine=MergeTree order by A;\r\ncreate table distr as local Engine=Distributed('test_shard_localhost', currentDatabase(), local, rand());\r\n\r\nalter table local modify column B Int64;\r\n\r\ndesc local\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\r\n\u2502 A    \u2502 Int32\r\n\u2502 B    \u2502 Int64  -----<<<<<<<\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\ndesc distr;\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\r\n\u2502 A    \u2502 Int32\r\n\u2502 B    \u2502 Int32  -----<<<<<<< NOT INT64 !!!!!!\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\n\r\n\r\ninsert into local values ( 1, toUInt64(-1)/2-100);\r\n\r\nselect * from local;\r\n\u250c\u2500A\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500B\u2500\u2510\r\n\u2502 1 \u2502 9223372036854775807 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n select * from distr;\r\n\u250c\u2500A\u2500\u252c\u2500\u2500B\u2500\u2510\r\n\u2502 1 \u2502 -1 \u2502 -----<<<<<<< NOT 9223372036854775807 !!!!!\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\r\n\r\nalter table distr modify column B Int64; --------<<<<<<<<< FIX !!!!\r\n\r\nselect * from distr;\r\n\u250c\u2500A\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500B\u2500\u2510\r\n\u2502 1 \u2502 9223372036854775807 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "alazo8807",
        "created_at": "2022-09-29T18:17:25Z",
        "body": "In our case I just checked and can confirm by running desc that both the distributed virtual table and the replicated table have the column defined as int64. \r\nAnything else I could look at?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-29T20:44:17Z",
        "body": "Please share the result of \r\n\r\n```sql\r\nSELECT\r\n    type,\r\n    count()\r\nFROM clusterAllReplicas('{cluster}', system.columns)\r\nWHERE name = 'measurement'\r\nGROUP BY type\r\n```"
      },
      {
        "user": "alazo8807",
        "created_at": "2022-09-30T12:10:26Z",
        "body": "It looks like measurement is defined as uint64 in one of our materialized views. Thanks for all the help"
      }
    ]
  },
  {
    "number": 41758,
    "title": "ClickHouse version 21.12.4.1 use BACKUP table async",
    "created_at": "2022-09-26T03:17:53Z",
    "closed_at": "2022-09-26T14:25:27Z",
    "labels": [
      "question",
      "obsolete-version",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41758",
    "body": "ClickHouse version 21.12.4.1 \r\n\r\nBACKUP database default  TO Disk('backups', 'backup-20220926/') ASYNC\r\n\r\nCode: 62. DB::Exception: Syntax error: failed at position 115 ('ASYNC'): ASYNC. Expected one of: FILTER, OVER, SETTINGS, end of query. (SYNTAX_ERROR) (version 21.12.4.1 (official build))\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41758/comments",
    "author": "hbzhu",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-26T14:25:26Z",
        "body": "`BACKUP database` command is available in the later releases. You need to upgrade to 22.9."
      },
      {
        "user": "hbzhu",
        "created_at": "2022-09-27T02:19:03Z",
        "body": "Thanks\uff0casynchronous backup tables need to upgrade to 22.9 to\uff1f"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-27T02:56:42Z",
        "body": "> Thanks\uff0casynchronous backup tables need to upgrade to 22.9 to\uff1f\r\n\r\nYes."
      },
      {
        "user": "hbzhu",
        "created_at": "2022-09-27T06:54:58Z",
        "body": "Thank you very much,I upgrated to 22.9.2.7,BACKUP commond  very nice!!!"
      }
    ]
  },
  {
    "number": 41714,
    "title": "When I bulk insert the replica table, all the data in the table becomes empty",
    "created_at": "2022-09-23T07:51:23Z",
    "closed_at": "2022-09-23T14:56:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41714",
    "body": "**Describe what's wrong**\r\n\r\n> When I bulk insert the replica table, all the data in the table becomes empty\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\nclickhouse-version: **v22.6.3.35-stable**\r\nzookeeper-version: **3.4.9**\r\n\r\n* Firstly, I create a replica table\r\n```sql\r\nCREATE TABLE IF NOT EXISTS test_db.test1 (\r\n    `token_id` String,\r\n    `event_id` String,\r\n    `login_id` String,\r\n    `distinct_id` String,\r\n    `ctime` DateTime64(3),\r\n    `utime` DateTime64(3),\r\n    `cdate` String,\r\n    `udate` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/test_db/test1', '{replica}')\r\nORDER BY ( distinct_id, login_id, utime, token_id)\r\nPARTITION BY (toYYYYMMDD(utime))\r\nTTL toDateTime(udate) + toIntervalMonth(4)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n* Then I batched 3 times, each batch of 40,000 pieces of data, a total of 120,000 pieces of data were inserted,\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 8b2071f7-7a50-49e2-a5fc-6dc18e46da07\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502  120000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.009 sec.\r\n```\r\n\r\n* Then I inserted 40,000 pieces of data in batches, each batch of 10,000 pieces of data, a total of 4 batches. During the insert process, I keep querying and find that the data has been decreasing\r\n\r\n> **The data inserted above, each batch has more than 100 partitions of data;**\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 174d0072-f9c9-44ea-b8c5-bff5d2527dc4\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   22993 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.014 sec. \r\n\r\n```\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 978eb77c-b517-4296-b84e-f6955efbec03\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    8038 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.007 sec\r\n\r\n```\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: dd5287da-fb75-42f3-b455-225bbfa05803\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.012 sec\r\n\r\n```\r\n* In the end I only get 10000 pieces of data, but I want to get 160000. I don't know why the data is lost, I checked the logs, the local files are all cleaned up\r\n  \r\n```\r\n2022.09.23 15:36:55.302288 [ 16832 ] {} <Trace> test_db.test1 (ReplicatedMergeTreeCleanupThread): Cleared 196 old blocks from ZooKeeper\r\n2022.09.23 15:36:55.302539 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220725_1_6_1\r\n2022.09.23 15:36:55.302574 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220725_1_6_1\r\n2022.09.23 15:36:55.313579 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220726_1_6_1\r\n2022.09.23 15:36:55.313632 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220726_1_6_1\r\n2022.09.23 15:36:55.315411 [ 16720 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000891 - log-0000000891\r\n2022.09.23 15:36:55.319817 [ 16720 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2022.09.23 15:36:55.321111 [ 16663 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Executing DROP_RANGE 20220725_1_6_1\r\n2022.09.23 15:36:55.321264 [ 16663 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Removed 0 entries from queue. Waiting for 0 entries that are currently executing.\r\n2022.09.23 15:36:55.321386 [ 16663 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing parts.\r\n2022.09.23 15:36:55.325987 [ 16663 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 7 parts inside 20220725_1_6_1.\r\n2022.09.23 15:36:55.332289 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220727_1_6_1\r\n2022.09.23 15:36:55.332332 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220727_1_6_1\r\n2022.09.23 15:36:55.334647 [ 16734 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000892 - log-0000000892\r\n2022.09.23 15:36:55.336885 [ 16734 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2022.09.23 15:36:55.337758 [ 16691 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Executing DROP_RANGE 20220726_1_6_1\r\n2022.09.23 15:36:55.337782 [ 16691 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Removed 0 entries from queue. Waiting for 0 entries that are currently executing.\r\n2022.09.23 15:36:55.337837 [ 16691 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing parts.\r\n2022.09.23 15:36:55.342937 [ 16691 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 7 parts inside 20220726_1_6_1.\r\n2022.09.23 15:36:55.348930 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220728_1_6_1\r\n2022.09.23 15:36:55.348958 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220728_1_6_1\r\n...\r\n...\r\n2022.09.23 15:36:57.503607 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Found 560 old parts to remove.\r\n2022.09.23 15:36:57.503822 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing 560 old parts from ZooKeeper\r\n2022.09.23 15:36:57.511469 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_1_1_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512061 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_1_6_1 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512068 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_2_2_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512073 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_3_3_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512078 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_4_4_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512082 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_5_5_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512085 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_6_6_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512091 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_1_1_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512096 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_1_6_1 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512100 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_2_2_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512104 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_3_3_0 in ZooKeeper, it was only in filesystem\r\n...\r\n...\r\n2022.09.23 15:36:57.535842 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 560 old parts from ZooKeeper. Removing them from filesystem.\r\n2022.09.23 15:36:57.536166 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_1_6_1\r\n2022.09.23 15:36:57.536167 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_1_1_0\r\n2022.09.23 15:36:57.536836 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_2_2_0\r\n2022.09.23 15:36:57.537405 [ 16780 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000971 - log-0000000971\r\n2022.09.23 15:36:57.537988 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_3_3_0\r\n2022.09.23 15:36:57.538036 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_4_4_0\r\n2022.09.23 15:36:57.538724 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_5_5_0\r\n2022.09.23 15:36:57.538762 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_6_6_0\r\n2022.09.23 15:36:57.539189 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_1_1_0\r\n2022.09.23 15:36:57.539742 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_1_6_1\r\n2022.09.23 15:36:57.540188 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_2_2_0\r\n2022.09.23 15:36:57.540191 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_3_3_0\r\n...\r\n2022.09.23 15:36:57.702064 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_3_3_0\r\n2022.09.23 15:36:57.702507 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_4_4_0\r\n2022.09.23 15:36:57.702957 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_5_5_0\r\n2022.09.23 15:36:57.702959 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_6_6_0\r\n2022.09.23 15:36:57.704515 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 560 old parts\r\n2022.09.23 15:36:57.721899 [ 16832 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeCleanupThread): Removed 81 old log entries: log-0000000881 - log-0000000961\r\n2022.09.23 15:36:57.724084 [ 16832 ] {} <Trace> test_db.test1 (ReplicatedMergeTreeCleanupThread): Execution took 221 ms.\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41714/comments",
    "author": "JohnZp",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-09-23T10:55:48Z",
        "body": "> TTL toDateTime(udate) + toIntervalMonth(4)\r\n\r\nI suppose that's the answer "
      },
      {
        "user": "JohnZp",
        "created_at": "2022-09-23T13:44:08Z",
        "body": "> > TTL toDateTime(udate) + toIntervalMonth(4)\r\n> \r\n> I suppose that's the answer\r\n\r\nI can guarantee that the expiration time is not reached"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T14:12:14Z",
        "body": "@JohnZp check server time `select now()`,\r\nremove TTL and check data insertion without TTL."
      },
      {
        "user": "tavplubix",
        "created_at": "2022-09-23T14:12:58Z",
        "body": "> I can guarantee that the expiration time is not reached\r\n\r\nPlease share a reproducible example then, so we will check if there's a bug in TTL"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T14:15:44Z",
        "body": ">`udate` String\r\n>TTL toDateTime(udate)\r\n\r\nAlso probably your String contains something which is evaluated to `1970-01-01` with toDateTime.\r\n\r\n```\r\nselect toDateTime('haba-01-01 01:21:33');\r\n\u250c\u2500toDateTime('haba-01-01 01:21:33')\u2500\u2510\r\n\u2502               1970-01-01 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "JohnZp",
        "created_at": "2022-09-23T14:56:42Z",
        "body": "Oh! you are right! It is indeed caused by TTL. I used toDatetime('yyyyMMdd'), and then all were converted to 1970-xx-xx\r\n```\r\nSELECT toDateTime('20220701')\r\n\r\nQuery id: ae080147-81c5-474b-8cfa-50219519cd8b\r\n\r\n\u250c\u2500toDateTime('20220701')\u2500\u2510\r\n\u2502    1970-08-23 08:51:41 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n```\r\n\r\n**Thank you very very very  much!!**  @den-crane @tavplubix \r\n\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T21:35:03Z",
        "body": "```sql\r\nSELECT parseDateTimeBestEffort('20220701')\r\n\r\n\r\n\u250c\u2500parseDateTimeBestEffort('20220701')\u2500\u2510\r\n\u2502                 2022-07-01 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      }
    ]
  },
  {
    "number": 41101,
    "title": "The size of processed data",
    "created_at": "2022-09-08T10:14:14Z",
    "closed_at": "2022-09-08T14:27:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41101",
    "body": "```sql\r\nlocalhouse :) SELECT * FROM hits WHERE URL LIKE '%google%' ORDER BY EventTime LIMIT 10;\r\n...\r\n10 rows in set. Elapsed: 6.029 sec. Processed 100.00 million rows, 25.45 GB (16.58 million rows/s., 4.22 GB/s.) \r\n\r\nlocalhost :) SELECT * FROM hits ORDER BY EventTime LIMIT 10 Format Null\r\n...\r\n0 rows in set. Elapsed: 9.844 sec. Processed 100.00 million rows, 82.77 GB (10.16 million rows/s., 8.41 GB/s.)\r\n```\r\n\r\nThe size of processed data of the first SQL is 25.45 GB which is much less than the second SQL. That looks so cool. Can the primary index used in this case? Or any other magic technology here like deferred materialize? Any docs about this?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41101/comments",
    "author": "Lloyd-Pottiger",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-08T12:08:16Z",
        "body": "There is `PREWHERE` and `optimize_move_to_prewhere`.\r\n\r\nBecause of optimize_move_to_prewhere =1, CH moves `URL LIKE '%google%'` to PREWHERE.\r\n\r\nBecause of `PREWHERE URL LIKE '%google%'` CH reads first only URL column, filters out granules, and after that reads only that granules to solve remaining parts of a query `SELECT *`."
      },
      {
        "user": "Lloyd-Pottiger",
        "created_at": "2022-09-08T14:27:43Z",
        "body": "> There is `PREWHERE` and `optimize_move_to_prewhere`.\r\n> \r\n> Because of optimize_move_to_prewhere =1, CH moves `URL LIKE '%google%'` to PREWHERE.\r\n> \r\n> Because of `PREWHERE URL LIKE '%google%'` CH reads first only URL column, filters out granules, and after that reads only that granules to solve remaining parts of a query `SELECT *`.\r\n\r\nI get it. Thanks for your clear reply and your excellent work, it is really a wonderful idea."
      }
    ]
  },
  {
    "number": 41073,
    "title": "CREATE   EMPTY AS SELECT  ERROR on 22.8",
    "created_at": "2022-09-07T08:33:26Z",
    "closed_at": "2022-09-07T09:55:06Z",
    "labels": [
      "question",
      "need-docs"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41073",
    "body": "hello !\r\nwhen i run \r\n create table xx1 empty as select * from xx;\r\n\r\nCREATE TABLE xx1 EMPTY AS\r\nSELECT *\r\nFROM xx\r\n\r\nQuery id: fd018c3a-79db-4fbd-8e34-c893307ce9b6\r\n\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\nReceived exception from server (version 22.8.4):\r\nCode: 119. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED)\r\n\r\nI tried  the table engine of  MergeTree() and Replicated_MergeTree().\r\nall of these engines  i got this error!\r\n\r\nwhat can i do?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41073/comments",
    "author": "yangshike",
    "comments": [
      {
        "user": "yangshike",
        "created_at": "2022-09-07T08:42:02Z",
        "body": "/var/log/clickhouse/clickhouse-server.log\uff1a\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa38beba in /usr/bin/clickhouse\r\n1. DB::InterpreterCreateQuery::setDefaultTableEngine(DB::ASTStorage&, std::__1::shared_ptr<DB::Context const>) @ 0x14c3dc2b in /usr/bin/clickhouse\r\n2. DB::InterpreterCreateQuery::setEngine(DB::ASTCreateQuery&) const @ 0x14c3bcce in /usr/bin/clickhouse\r\n3. DB::InterpreterCreateQuery::getTablePropertiesAndNormalizeCreateQuery(DB::ASTCreateQuery&) const @ 0x14c38c67 in /usr/bin/clickhouse\r\n4. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x14c401a9 in /usr/bin/clickhouse\r\n5. DB::InterpreterCreateQuery::execute() @ 0x14c4a38d in /usr/bin/clickhouse\r\n6. ? @ 0x14fecab4 in /usr/bin/clickhouse\r\n7. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x14fe9f0e in /usr/bin/clickhouse\r\n8. DB::TCPHandler::runImpl() @ 0x15cb97ad in /usr/bin/clickhouse\r\n9. DB::TCPHandler::run() @ 0x15ccdd59 in /usr/bin/clickhouse\r\n10. Poco::Net::TCPServerConnection::start() @ 0x18a617b3 in /usr/bin/clickhouse\r\n11. Poco::Net::TCPServerDispatcher::run() @ 0x18a62c2d in /usr/bin/clickhouse\r\n12. Poco::PooledThread::run() @ 0x18c2d9c9 in /usr/bin/clickhouse\r\n13. Poco::ThreadImpl::runnableEntry(void*) @ 0x18c2b242 in /usr/bin/clickhouse\r\n14. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n15. __clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n"
      },
      {
        "user": "melvynator",
        "created_at": "2022-09-07T08:55:02Z",
        "body": "Hello\r\n\r\nIn your example you don't define the engine and you don't specify the primary key. The examples below should be working:\r\n\r\n```\r\nCREATE TABLE xx1 \r\nENGINE=MergeTree() \r\nORDER BY number \r\nEMPTY \r\nAS\r\n\tSELECT *\r\n\tFROM numbers(100)\r\n```\r\n\t\r\n\r\n`CREATE TABLE xx2 Engine=MergeTree() ORDER by number EMPTY AS SELECT * FROM xx1\r\n`"
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T09:19:38Z",
        "body": "thanks, This is indeed the problem\uff01\r\n\r\n\r\nHowever, I think that when creating a new table based on the original table, most users want the structure, sorting, primary key and partition of the original table to be the same.\r\n\r\nCan this be the case: if the engine and other parameters are not specified, it is like this: create table xx1 empty as select * from XX; By default, all attributes are the same as the original table. If you need to modify parameters such as the engine, the creation syntax can be as follows:\r\n\r\ncreate table xx1 engine=xx empty as select * from xx;\r\n\r\ncreate table xx1 primary key xxx empty as select * from xx;\r\n\r\ncreate table xx1 order by xxx empty as select * from xx;\r\n\r\nFor unspecified parameters, the value of the original table is taken"
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T09:23:03Z",
        "body": "Like mysql, create table xx1 like XX; No engine is specified. InnoDB is created by default"
      },
      {
        "user": "melvynator",
        "created_at": "2022-09-07T09:26:05Z",
        "body": "It's a feature we added recently, there is room for improvement. I will make sure these feedbacks are added as feature request. "
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T09:27:40Z",
        "body": "Especially after upgrading the database engine atomic, replicatedmergetree adopts the default_ replica_ path and default_ replica_ After the name parameter is set, there should be no problem with this creation syntax.\r\n\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2022-09-07T09:55:06Z",
        "body": "> However, I think that when creating a new table based on the original table, most users want the structure, sorting, primary key and partition of the original table to be the same.\r\n\r\nBut in your example a new table is created based on the SELECT query, not on other table. SELECT query has result structure, but does not have engine. And we cannot simply take the storage definition from the table specified in FROM section, because SELECT query may contain UNION or JOIN or subquery and therefore may read from multiple different tables with different storage definitions. That's why you have to specify storage definition explicitly when using `CREATE AS SELECT`. But you can use `CREATE TABLE AS <table>`:\r\n```\r\nCREATE TABLE xx1 AS xx;\r\n```\r\nIt will take storage definition from the original table. \r\nSee also `default_table_engine` setting (but it will not completely solve your problem, because *MergeTree requires ORDER BY key which cannot be chosen automatically)."
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T10:26:14Z",
        "body": "oh thank you\uff0cCREATE TABLE AS <table>: Solved my problem\u3002 But I have to enter on cluster XXX every time\r\n\r\nWhen creating a database, atomic and replicated can be specified at the same time\r\n\r\n"
      }
    ]
  },
  {
    "number": 41051,
    "title": "\u3010Merge Engine related\u3011How to use Merge Engine for distributed query ?",
    "created_at": "2022-09-06T14:14:54Z",
    "closed_at": "2022-09-15T07:08:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41051",
    "body": "Build distributed engine  on Merge Engine and Merge Engine on ReplicatedMergeTree Engine,\r\nOr build Merge Engine on distributed engine and  distributed engine on merge tree,\r\nwhich is better ?\r\n\r\nThank you guys.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41051/comments",
    "author": "mo-avatar",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-06T15:13:04Z",
        "body": "You can do any.\r\n\r\nSuggested / supposed: `Distributed -> Merge -> ReplicatedMergeTree`\r\n\r\nSome users use: `Merge -> Distributed -> ReplicatedMergeTree` but for some very rare use-cases."
      },
      {
        "user": "rkozlo",
        "created_at": "2022-09-07T12:42:29Z",
        "body": "We are using second setting. For some reason first one was noticeable slower"
      },
      {
        "user": "mo-avatar",
        "created_at": "2022-09-15T07:08:27Z",
        "body": "Thanks for your guys answering."
      }
    ]
  },
  {
    "number": 40479,
    "title": "system.replicas error",
    "created_at": "2022-08-22T07:49:38Z",
    "closed_at": "2022-08-22T14:22:40Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40479",
    "body": "execute  (SELECT * FROM `system`.replicas LIMIT 0, 200  \uff09 error\uff1a\r\nSQL Error [1000]: ClickHouse exception, code: 1000, host: 10.99.84.60, port: 31197; Poco::Exception. Code: 1000, e.code() = 2002, e.displayText() = mysqlxx::ConnectionFailed: Can't connect to MySQL server on '10.99.85.241' (115) ((nullptr):0) (version 20.3.8.53 (official build))",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40479/comments",
    "author": "zhwuhuang",
    "comments": [
      {
        "user": "rkozlo",
        "created_at": "2022-08-22T08:38:43Z",
        "body": "Version 20.3 is a version of clickhouse? Start with update because this version years behind its support."
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-22T14:22:40Z",
        "body": "Upgrade your CH to 22.8."
      },
      {
        "user": "zhwuhuang",
        "created_at": "2022-08-23T02:29:41Z",
        "body": "OK, solved. Thank you"
      }
    ]
  },
  {
    "number": 40427,
    "title": "Don't know how to debug MySQLHandler: DB::Exception: Cannot read all data. ",
    "created_at": "2022-08-19T22:42:09Z",
    "closed_at": "2022-08-20T17:38:32Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40427",
    "body": "Hi,\r\n\r\nI have ClickHouse 22.1.3.7 installed and it is working fine and my inserts are going as expected. However in log I have multiple lines like this:\r\n\r\n`<Error> MySQLHandler: DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 3.`\r\n\r\nI have tried to set logging level to test, but I am unable to get any useful information to understand what is going on.\r\n\r\nCan you possibly hint me to the right direction?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40427/comments",
    "author": "edo888",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-08-20T17:38:32Z",
        "body": "@edo888 Someone is connecting to the MySQL endpoint and does not send any data.\r\nIt might be a monitoring script.\r\n\r\nThis error does not necessarily require attention."
      },
      {
        "user": "edo888",
        "created_at": "2022-08-20T18:23:59Z",
        "body": "Yes, that was exactly that. I have connection reuse mechanism in place and connections can stay open for up to 30 seconds.\r\n\r\nIs it better to do that or close and reopen connection every time a query needs to be pushed?\r\n\r\nThanks!"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-08-21T17:16:29Z",
        "body": "For clickhouse-server it does not make any difference. For clients, reusing connections can be better (for latency)."
      }
    ]
  },
  {
    "number": 40384,
    "title": "Configured background pool size does not match system.settings",
    "created_at": "2022-08-19T07:31:43Z",
    "closed_at": "2022-08-19T08:01:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40384",
    "body": "**Describe what's wrong**\r\n\r\nValue of background_fetches_pool_size configured in config.xml (as per #36425):\r\n\r\n```xml\r\n<background_fetches_pool_size>64</background_fetches_pool_size>\r\n```\r\n\r\ndoes not show system.settings which has the default value instead:\r\n\r\n```sql\r\nSELECT name, value FROM system.settings WHERE name LIKE 'background_fetches_pool_size'\r\n\u250c\u2500value\u2500\u2510\r\n\u2502 8     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nwhile the correct value shows in the log:\r\n\r\n```\r\nInitialized background executor for fetches with num_threads=64, num_tasks=64\r\n```\r\n\r\nand BackgroundFetchesPoolTask sometimes exceeds the default so it looks like it's actually using the configured value\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes (suppose 22.8 LTS will be added to version_date.tsv)\r\n\r\n**How to reproduce**\r\n\r\nClickHouse server version 22.8.1.2097",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40384/comments",
    "author": "larry-cdn77",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-08-19T08:01:30Z",
        "body": "This setting is obsolete:\r\n```\r\nClickHouse client version 22.8.1.1.\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 22.8.1 revision 54460.\r\n\r\ndell9510 :) select * from system.settings where name='background_fetches_pool_size'\r\n\r\nSELECT *\r\nFROM system.settings\r\nWHERE name = 'background_fetches_pool_size'\r\n\r\nQuery id: c4256263-ee40-4cf4-ad2d-9352fea6b5e7\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u252c\u2500changed\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500min\u2500\u2500\u252c\u2500max\u2500\u2500\u252c\u2500readonly\u2500\u252c\u2500type\u2500\u2500\u2500\u2510\r\n\u2502 background_fetches_pool_size \u2502 8     \u2502       0 \u2502 Obsolete setting, does nothing. \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502        0 \u2502 UInt64 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.036 sec. \r\n```\r\n\r\n It was replaced with configuration parameter with the same name."
      },
      {
        "user": "larry-cdn77",
        "created_at": "2022-08-22T09:18:05Z",
        "body": "Thank you, indeed the confusion I had was in thinking that this configuration parameter (config.xml) can be viewed via system.settings"
      }
    ]
  },
  {
    "number": 40327,
    "title": "Extract specific symbols in the string",
    "created_at": "2022-08-18T08:06:20Z",
    "closed_at": "2022-08-22T18:53:25Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40327",
    "body": "Hi!\r\n\r\nI would be grateful if you help me to find specific symbols in the string. \r\nI have a table **'comment'** with the column **'text'**\r\nIn that column i have some text, for example : \r\n\"_Address: Chicago, 15 and Number of people: **2**_\" \r\n\r\nHow can I extract \"**2**\" out of this text? I mean I need only \"**2**\" in this string\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40327/comments",
    "author": "sabinajvdva",
    "comments": [
      {
        "user": "DerekChia",
        "created_at": "2022-08-18T14:52:42Z",
        "body": "Hello! Does your text always conform to the structure of `Address: XX, YY and Number of people: ZZ`? And do you only want to extract the last integer?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-18T22:00:31Z",
        "body": "extractAll ?\r\n\r\n```sql\r\nSELECT\r\n    'Address: Chicago, 15 and Number of people: 2' AS text,\r\n    extractAll(text, '2') AS r\r\n\r\nQuery id: f728b623-2743-4056-87b3-efe7e041cd41\r\n\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 2 \u2502 ['2'] \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-18T22:02:13Z",
        "body": "substring ?\r\n\r\n```sql\r\nselect 'Address: Chicago, 15 and Number of people: 2' text, substring(text, -1) r;\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 2 \u2502 2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-18T22:05:23Z",
        "body": "```sql\r\nSELECT\r\n    'Address: Chicago, 15 and Number of people: 2' AS text,\r\n    extract(text, '(\\\\d)$') AS r\r\n\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 2 \u2502 2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```sql\r\nSELECT\r\n    'Address: Chicago, 15 and Number of people: 22' AS text,\r\n    extractAll(text, '(\\\\d+)') AS r\r\n\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 22 \u2502 ['15','22'] \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "sabinajvdva",
        "created_at": "2022-08-19T05:40:19Z",
        "body": "> Hello! Does your text always conform to the structure of `Address: XX, YY and Number of people: ZZ`? And do you only want to extract the last integer?\r\n\r\nNo text is always different, but it has 'Number of people: 2', so I need only the integer (2) out of this text. \r\n\r\n> ```sql\r\n> ```sql\r\n> SELECT\r\n>     'Address: Chicago, 15 and Number of people: 2' AS text,\r\n>     extract(text, '(\\\\d)$') AS r\r\n> \r\n> \u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2510\r\n> \u2502 Address: Chicago, 15 and Number of people: 2 \u2502 2 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n> ```\r\n> \r\n> \r\n>     \r\n>       \r\n>     \r\n> \r\n>       \r\n>     \r\n> \r\n>     \r\n>   \r\n> ```sql\r\n> SELECT\r\n>     'Address: Chicago, 15 and Number of people: 22' AS text,\r\n>     extractAll(text, '(\\\\d+)') AS r\r\n> \r\n> \u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n> \u2502 Address: Chicago, 15 and Number of people: 22 \u2502 ['15','22'] \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> ```\r\n> ```\r\n\r\nThank you! I will try that. \r\n\r\nAnd I also found this way and it worked: \r\n```\r\ntoInt32OrZero(cast(\r\n\t\t\t\tif \r\n\t\t\t\t(\r\n\t\t\t\ttrim(\r\n\t\t\t\t\treplace(\r\n\t\t\t\t\t\tmid\r\n\t\t\t\t\t\t(text,\r\n\t\t\t\t\t\tPOSITION (text,'Number of people:')+LENGTH('Number of people:'),\r\n\t\t\t\t\t\tPOSITION(mid(bc.text,POSITION(text,'Number of people:')+LENGTH('Number of people:')),'.')\r\n\t\t\t\t\t\t)\r\n\t\t\t\t\t,'.',' ')\r\n\t\t\t\t)='',\r\n\t\t\t\ttrim(mid(text,POSITION(text,'Number of people:')+LENGTH('Number of people:'))),\r\n\t\t\t\ttrim(\r\n\t\t\t\t\treplace(\r\n\t\t\t\t\t\tmid\r\n\t\t\t\t\t\t(text,\r\n\t\t\t\t\t\tPOSITION(text,'Number of people:')+LENGTH('Number of people:'),\r\n\t\t\t\t\t\tPOSITION(mid(text,POSITION(text,'Number of people:')+LENGTH('Number of people:')),'.')\r\n\t\t\t\t\t\t)\r\n\t\t\t\t\t,'.',' ')\r\n\t\t\t\t)\r\n\t\t\t\t) as text ) \r\n```\r\n"
      }
    ]
  },
  {
    "number": 39920,
    "title": "How to using a select SQL with case insensitive\uff1f",
    "created_at": "2022-08-05T09:42:41Z",
    "closed_at": "2022-08-05T12:00:26Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39920",
    "body": "hi, team,\r\n\r\nI'm sorry to have your attention, but i just want to know how to make query condition case insensitive like MySQL query. just only can using lower function convert it  like this?\r\n\r\n`select * from log_table_test where lower(file) = lower('File_name')` \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39920/comments",
    "author": "martin-chips",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-08-05T12:00:26Z",
        "body": "yes, `lower(file) = lower('File_name')` is the right way with ClickHouse."
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-05T12:01:15Z",
        "body": "Also you may use `ilike`\r\n\r\n```\r\nselect * from log_table_test where file ilike 'File_name'\r\n```"
      },
      {
        "user": "martin-chips",
        "created_at": "2022-08-08T01:30:39Z",
        "body": "Thank you for your prompt reply."
      }
    ]
  },
  {
    "number": 39616,
    "title": "How to quickly load file data into local tables\uff1f",
    "created_at": "2022-07-26T12:35:50Z",
    "closed_at": "2022-08-01T06:56:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39616",
    "body": "hello everyone\r\n\r\nI wanted to load some data to CH for using  it.\r\n\r\nAfter spending a long time with CH, found that write Distributed table is faster then local table\r\n\r\nFirst file: csv.data  **60GB+**\r\nSample data:\r\n\r\n```\r\n1,SEX,1,2022-06-12 00:00:00,1\r\n2,SEX,1,2022-06-12 00:00:00,1\r\n3,SEX,1,2022-06-12 00:00:00,1\r\n```\r\n\r\nSecond file: This is the local table  load.\r\n\r\n```\r\nCREATE TABLE tag.tag_test_base_info\r\n(\r\n     `offset` UInt64,\r\n    `tag_code` String,\r\n    `tag_value` String,\r\n     `ts` DateTime,\r\n    `sign` Int8\r\n)\r\nENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/{shard}/tag_test_base_info', '{replica}', sign)\r\nPARTITION BY ts\r\nORDER BY (ts, tag_code, offset)\r\nSETTINGS storage_policy = 'default', use_minimalistic_part_header_in_zookeeper = 1, index_granularity = 8192\r\n```\r\n\r\nTesting CSV load:\r\n```\r\ncat csv.dat | clickhouse-client -h 127.0.0.1 -d default -m -u default --password 123456 --format_csv_delimiter= ',' --query=\"INSERT INTO tag.tag_test_base_info FORMAT CSV\";\r\n\r\n```\r\n\r\nSo it took **39:12.29** seconds.\r\n\r\nThree file:This is the Distributed table load.\r\n\r\n```\r\nCREATE TABLE tag.tag_test_base_info\r\n(\r\n     `offset` UInt64,\r\n    `tag_code` String,\r\n    `tag_value` String,\r\n     `ts` DateTime,\r\n    `sign` Int8\r\n)\r\nENGINE = Distributed('tagclickhouse', 'tag', 'tag_test_base_info', rand())\r\n```\r\n\r\nTesting CSV load:\r\n```\r\ncat csv.dat | clickhouse-client -h 127.0.0.1 -d default -m -u default --password 123456 --format_csv_delimiter= ',' --query=\"INSERT INTO tag.tag_test_base_info_dist FORMAT CSV\";\r\n\r\n```\r\n\r\nSo it took **16:50.01** seconds.\r\n\r\n\r\nThe  Server is  6  virtual machine  **16C+64g+500G** \r\n\r\nThat means using Distributed table  is = **2X** faster (39:12.29 seconds / 16:50.01 seconds)\r\nOr local table  is **2X** slower! \r\n\r\n\r\nMy question\r\n\r\n1. Why is it faster to load Distributed tables than local tables\r\n2. How to quickly load data into local tables\uff1f(i 'am using ``` --input_format_parallel_parsing=0 --compression=0``` ,args There is no change in speed) \r\n\r\n\r\n\r\nthank you\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39616/comments",
    "author": "longfeizheng",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-07-26T12:40:16Z",
        "body": "technically the insert is faster but data is not inserted because insert into Distributed is async by default \r\n\r\ncheck with `--insert_distributed_sync arg                                                      If setting is enabled, insert query into distributed waits until data will be sent to all nodes in cluster.`\r\n\r\n```\r\ncat csv.dat | clickhouse-client --insert_distributed_sync=1 -h 127.0.0.1 -d default -m -u default --password 123456 --format_csv_delimiter= ',' --query=\"INSERT INTO tag.tag_test_base_info_dist FORMAT CSV\";\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-07-26T12:41:42Z",
        "body": "you problem is here `PARTITION BY ts` it's too granular\r\n\r\nexpected `PARTITION BY toYYYYMM(ts)`  \r\n\r\nAlso try `ORDER BY (tag_code, offset, ts)`"
      },
      {
        "user": "longfeizheng",
        "created_at": "2022-07-26T13:11:43Z",
        "body": "> technically the insert is faster but data is not inserted because insert into Distributed is async by default\r\n> \r\n> check with `--insert_distributed_sync arg If setting is enabled, insert query into distributed waits until data will be sent to all nodes in cluster.`\r\n> \r\n> ```\r\n> cat csv.dat | clickhouse-client --insert_distributed_sync=1 -h 127.0.0.1 -d default -m -u default --password 123456 --format_csv_delimiter= ',' --query=\"INSERT INTO tag.tag_test_base_info_dist FORMAT CSV\";\r\n> ```\r\n\r\nNo wonder distributed tables are written so fast\r\n"
      },
      {
        "user": "longfeizheng",
        "created_at": "2022-07-26T13:17:16Z",
        "body": "> you problem is here `PARTITION BY ts` it's too granular\r\n> \r\n> expected `PARTITION BY toYYYYMM(ts)`\r\n> \r\n> Also try `ORDER BY (tag_code, offset, ts)`\r\n\r\nAlthough the partition is ```PARTITION BY ts```, the data is ```2022-06-12 00:00:00``` format \r\n\r\nI am tring to change ```ORDER BY (tag_code, offset, ts)``` testing \r\n"
      },
      {
        "user": "longfeizheng",
        "created_at": "2022-08-01T06:54:51Z",
        "body": "hi everyone:\r\n\r\nI used the shard file (Each file is about 30g in size) to load local tables from multiple clients and finished it.\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 39549,
    "title": "TCPHandlerFactory creating TCPhandler is slow. Takes 3 sec within intranet",
    "created_at": "2022-07-25T09:17:05Z",
    "closed_at": "2022-07-25T11:10:33Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39549",
    "body": "checked log file, /var/log/clickhouse-server/clickhouse-server.log, I got logs below,\r\n\r\n2022.07.25 17:02:23.535994 [ 12473 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::ffff:172.22.254.xx]:57308\r\n2022.07.25 17:02:26.554058 [ 12473 ] {} <Debug> TCPHandler: Connected ClickHouse client version 1.1.0, revision: 54380, database: xxxxx, user: xxxx.\r\n\r\nIt takes 3 sec to create a TCP connection in my opion,\r\n\r\nAlready done below,\r\n1. Ping is OK, stable and fast. Intranet.\r\n2. It returns the correct results, just slow.\r\n3. ClickHouse Server version: 20.8.3 revision 54438\r\n4. Stop the firewall on both machines.\r\n5. No error logs in /var/log/clickhouse-server/clickhouse-server.err.log\r\n\r\nAny suggestion will be grateful. \r\n\r\nWarm Regards,\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39549/comments",
    "author": "daggerin3",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-07-25T11:10:33Z",
        "body": "> ClickHouse Server version: 20.8.3\r\n\r\nUpgrade to supported version"
      },
      {
        "user": "tavplubix",
        "created_at": "2022-07-25T11:12:02Z",
        "body": "> ClickHouse client version 1.1.0\r\n\r\nAnd upgrade client as well (versions 1.x are almost 4 years old)"
      },
      {
        "user": "den-crane",
        "created_at": "2022-07-25T12:51:59Z",
        "body": "Looks like a DNS timeout / issue."
      },
      {
        "user": "daggerin3",
        "created_at": "2022-07-26T01:58:02Z",
        "body": "> Looks like a DNS timeout / issue.\r\n\r\nThanks a lot. Problem solved. 1st DNS server is unreachable."
      }
    ]
  },
  {
    "number": 39548,
    "title": "Updating a dictionary by `update_field`, even if there is no data to update",
    "created_at": "2022-07-25T08:54:14Z",
    "closed_at": "2022-08-01T05:15:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39548",
    "body": "```\r\nselect version();\r\n\u250c\u2500version()\u2500\u2500\u2500\u2510\r\n\u2502 22.7.1.2484 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nCREATE TABLE test_dictionary_time\r\n(\r\n    `ID` UInt64,\r\n    `Value` String,\r\n    ModifiedTime DateTime default now()\r\n)\r\nENGINE = MergeTree\r\nORDER BY ID;\r\n\r\nINSERT INTO test_dictionary_time (ID, Value) SELECT     number,     concat('value_', toString(number)) FROM numbers(100000);\r\n\r\ncreate dictionary test_dictionary_updated_field\r\n(\r\n  ID UInt64,\r\n  Value String\r\n)\r\nPRIMARY KEY ID\r\nSOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' PASSWORD '' DB 'default' TABLE 'test_dictionary_time' update_field 'ModifiedTime'))\r\nLIFETIME(30)\r\nLAYOUT(Complex_Key_Hashed ());\r\n\r\nselect * from test_dictionary_updated_field;\r\n```\r\nDictionary loaded\r\n```\r\nselect name, status, type, element_count, load_factor, lifetime_min, lifetime_max, loading_start_time, last_successful_update_time, loading_duration, last_exception, comment from system.dictionaries where name = 'test_dictionary_updated_field';\r\n```\r\nData is not added to the table\r\nAfter 30 seconds of seconds in the logs the message\r\n```\r\n2022.07.25 08:04:10.682885 [ 17232 ] {} <Debug> executeQuery: (internal) SELECT `ID`, `Value` FROM `default`.`test_dictionary_time` WHERE ModifiedTime >= '2022-07-25 08:04:04'; (stage: Complete)\r\n```\r\nRepeating in the database\r\n```\r\nSELECT\r\n    ID,\r\n    Value\r\nFROM default.test_dictionary_time\r\nWHERE ModifiedTime >= '2022-07-25 08:04:04'\r\n\r\nQuery id: 01279644-53c6-4df7-a87f-330a8ff93de0\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.008 sec. Processed 1.00 million rows, 4.00 MB (124.75 million rows/s., 499.02 MB/s.)\r\n\r\n```\r\nWatching dictionary update time\r\n```\r\nselect name, status, type, element_count, load_factor, lifetime_min, lifetime_max, loading_start_time, last_successful_update_time, loading_duration, last_exception, comment from system.dictionaries where name = 'test_dictionary_updated_field';\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500status\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500element_count\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500load_factor\u2500\u252c\u2500lifetime_min\u2500\u252c\u2500lifetime_max\u2500\u252c\u2500\u2500loading_start_time\u2500\u252c\u2500last_successful_update_time\u2500\u252c\u2500loading_duration\u2500\u252c\u2500last_exception\u2500\u252c\u2500comment\u2500\u2510\r\n\u2502 test_dictionary_updated_field \u2502 LOADED \u2502 ComplexKeyHashed \u2502       1000000 \u2502 0.476837158203125 \u2502            0 \u2502           30 \u2502 2022-07-25 08:05:55 \u2502         2022-07-25 08:05:56 \u2502            0.426 \u2502                \u2502         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nThe dictionary is still reloaded even if there is no data to update.\r\nThe dictionary is loaded into memory every 30 seconds, even if no data is added to the table.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39548/comments",
    "author": "rchadin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-07-25T15:32:15Z",
        "body": "I don't understand. It works as it supposed to work.\r\n\r\nDictionary reloads `LIFETIME(30)`, checks the source, and changes `loading_start_time` `last_successful_update_time` because it was reloaded. "
      },
      {
        "user": "rchadin",
        "created_at": "2022-07-26T04:57:22Z",
        "body": "Dictionary uses ```update_field 'ModifiedTime'```\r\nAs far as I understand from the documentation, in this case it should check for new entries, and reload the dictionary only when they appear, as indicated by the log message that I cited above."
      },
      {
        "user": "den-crane",
        "created_at": "2022-07-26T16:57:28Z",
        "body": "Apparently we put different meanings in the \"dictionary reload\".\r\n\r\nDictionary reload does not mean that it loaded all data from the source. \r\n\r\nThough a dictionary with update_field rebuild its internal structure with each iteration.\r\n\r\n`last_successful_update_time` -- reflects the time of a last successful iteration, it does not matter how many rows were updated.\r\n"
      },
      {
        "user": "rchadin",
        "created_at": "2022-07-28T05:15:19Z",
        "body": "Maybe. Thanks for the clarification. I understood this field in such a way that if there is no data to update, then it will not reload the structure. If the dictionary is large, then resources will be spent on rebuilding it, although this does not make sense. I tested it on big data. And the documentation stated that when using this modifier, only new data is loaded, but in general it looks like the entire dictionary is being reloaded.\r\nThank you."
      }
    ]
  },
  {
    "number": 39407,
    "title": "Clickhouse analogue function",
    "created_at": "2022-07-20T08:28:46Z",
    "closed_at": "2022-07-20T13:40:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39407",
    "body": "What will be the analogue of function DAYOFWEEK( date_format(b.createdon_date, '%Y-01-06'))? \r\n\r\nAnd also I need help to know how to start the week not from monday or sunday but from Thursday on clickhouse? \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39407/comments",
    "author": "bilelik",
    "comments": [
      {
        "user": "evillique",
        "created_at": "2022-07-20T13:02:13Z",
        "body": "1. Considering Monday is 1, and Sunday is 7:\r\n`DAYOFWEEK(toDateTime(formatDateTime(date, '%Y-01-06')))`\r\n\r\n2. If we only need to change the day of the week:\r\n`(DAYOFWEEK(date) + 3) % 7 + 1`\r\n```\r\nWITH today() + number AS date\r\nSELECT\r\n    date,\r\n    DAYOFWEEK(date) AS old,\r\n    ((DAYOFWEEK(date) + 3) % 7) + 1 AS new\r\nFROM numbers(7)\r\n\r\nQuery id: 840ca0cb-9176-4ea0-a4f6-e319a9c58a8b\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500old\u2500\u252c\u2500new\u2500\u2510\r\n\u2502 2022-07-20 \u2502   3 \u2502   7 \u2502\r\n\u2502 2022-07-21 \u2502   4 \u2502   1 \u2502\r\n\u2502 2022-07-22 \u2502   5 \u2502   2 \u2502\r\n\u2502 2022-07-23 \u2502   6 \u2502   3 \u2502\r\n\u2502 2022-07-24 \u2502   7 \u2502   4 \u2502\r\n\u2502 2022-07-25 \u2502   1 \u2502   5 \u2502\r\n\u2502 2022-07-26 \u2502   2 \u2502   6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "bilelik",
        "created_at": "2022-07-20T13:27:05Z",
        "body": "Thank you! That was useful \r\n"
      }
    ]
  },
  {
    "number": 39068,
    "title": "Is it possible to change sharding key of a Distributed table",
    "created_at": "2022-07-10T09:00:13Z",
    "closed_at": "2022-07-11T00:45:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39068",
    "body": "Currently, I have a history(almost 2yrs) distributed table with rand sharding key.  However,  recently I come across lots of business scenarios where I have to do distribute join on business key, such as userid(**which is evenly distributed by nature**). So I want to change the sharing key to userid.\r\n1. it is possible at the moment\r\n2. if not, is there any possible way i can make the minimal change to get it done\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39068/comments",
    "author": "jacoffee",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-07-10T14:23:05Z",
        "body": "You can drop Distributed table and create it back with another sharding key.\r\nExisting (history) data stays unchanged (sharded by rand). If you need to change it, you need to re-insert it."
      },
      {
        "user": "jacoffee",
        "created_at": "2022-07-11T00:45:20Z",
        "body": "Thanks, I got it. I will try with your suggestion."
      }
    ]
  },
  {
    "number": 38591,
    "title": "can clickhouse do the whole subquery in remote server when using remote function?",
    "created_at": "2022-06-29T16:04:07Z",
    "closed_at": "2022-06-30T01:46:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38591",
    "body": "1. db.table_1 created in server 10.1.4.160, table ddl as below\r\n\r\n```\r\nCREATE TABLE db.table_1\r\n(\r\n\r\n    `column1` String,\r\n\r\n    `column2` String,\r\n\r\n    `value1` Int32,\r\n\r\n    `value2` Int32\r\n)\r\nENGINE = MergeTree\r\nORDER BY (column1,\r\n column2)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\n2. subquery using remote function in another server (exp: 10.1.4.159)\r\n\r\n```\r\nselect column1, sum(value1) as value1_sum from (\r\n  select * from remote('10.1.4.160:9000', `db.table_1`) where value1 > 0\r\n) group by column1 order by value1_sum\r\n```\r\n\r\n3. select from 10.1.4.160's system.query_log, \r\n\r\n```\r\nselect query from system.query_log where type='QueryFinish' and has(databases, 'db') order by event_time desc\r\n```\r\n\r\n\r\n4. I find the query executed in remote is as below\r\n\r\n```\r\nSELECT `column1`, `value1` FROM `db`.`table_1` WHERE `value1` > 0\r\n```\r\n\r\n5 finally, my question is that, can I make the whole subquery being executed in remote server, and how can I do that?\r\n\r\nthe whole subquery I expected is as below\r\n\r\n```\r\nSELECT `column1`, sum(`value1`) AS `value1_sum` FROM `db`.`table_1` WHERE `value1` > 0 GROUP BY `column1` ORDER BY `value1_sum` ASC\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38591/comments",
    "author": "zhnpeng",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2022-06-30T00:11:21Z",
        "body": "Try\r\n\r\n```\r\nSelect * from remote('...', view(select whatever should be passed in...))\r\n```"
      },
      {
        "user": "zhnpeng",
        "created_at": "2022-06-30T01:46:30Z",
        "body": "@filimonov It works, thank you!"
      }
    ]
  },
  {
    "number": 38375,
    "title": "different results returned when using date filter and string(equals to the date)",
    "created_at": "2022-06-24T08:57:44Z",
    "closed_at": "2022-06-25T01:00:47Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38375",
    "body": "version:  21.8.14.5\r\n\r\ndistributed table\r\n```sql\r\nlocalhost :) show create table xdr_fullaudit_monitor;\r\n\r\nSHOW CREATE TABLE xdr_fullaudit_monitor\r\n\r\nQuery id: cc0f4d1e-8156-4a48-871b-7d9bdef6c6f9\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE falcon0325.xdr_fullaudit_monitor\r\n(\r\n    `logid` String,\r\n    `srcip4` Int64,\r\n    `srcip6` String,\r\n    `srcport` Int32,\r\n    `destip4` Int64,\r\n    `destip6` String,\r\n    `destport` Int32,\r\n    `isipv4` UInt8,\r\n    `gathertime` Int64,\r\n    `taskid` Int32,\r\n    `probeid` String,\r\n    `ispid` Int32,\r\n    `roomid` Int32,\r\n    `protocolid` Int32,\r\n    `reftaskid` Int32,\r\n    `interfacetype` Int32,\r\n    `rattype` Int32,\r\n    `proceduretype` Int32,\r\n    `begintime` Int64,\r\n    `endtime` Int64,\r\n    `procedurestatus` Int32,\r\n    `callerphonenumber` String,\r\n    `phonenumber` String,\r\n    `imsi` String,\r\n    `imei` String,\r\n    `nesrcip4` Int64,\r\n    `nesrcip6` String,\r\n    `nedestip4` Int64,\r\n    `nedestip6` String,\r\n    `apn` String,\r\n    `lactac` Int32,\r\n    `cieci` Int32,\r\n    `smssenderphone` String,\r\n    `smsrecieverphone` String,\r\n    `smsdirection` Int32,\r\n    `smscode` Int32,\r\n    `smstime` Int64,\r\n    `smslength` Int32,\r\n    `smscontent` String,\r\n    `pdate` Date,\r\n    `srcip` String,\r\n    `destip` String,\r\n    `nesrcip` String,\r\n    `nedestip` String,\r\n    `mcc` Int32,\r\n    `mnc` Int32\r\n)\r\nENGINE = Distributed('falcon0325', 'falcon0325', 'xdr_fullaudit_monitor_local', rand()) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec.\r\n```\r\n\r\nlocal table\r\n```sql\r\nlocalhost :) show create table xdr_fullaudit_monitor_local;\r\n\r\nSHOW CREATE TABLE xdr_fullaudit_monitor_local\r\n\r\nQuery id: 90d5c288-67f8-4758-b033-685d0d2187ad\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE falcon0325.xdr_fullaudit_monitor_local\r\n(\r\n    `logid` String,\r\n    `srcip4` Int64,\r\n    `srcip6` String,\r\n    `srcport` Int32,\r\n    `destip4` Int64,\r\n    `destip6` String,\r\n    `destport` Int32,\r\n    `isipv4` UInt8,\r\n    `gathertime` Int64,\r\n    `taskid` Int32,\r\n    `probeid` String,\r\n    `ispid` Int32,\r\n    `roomid` Int32,\r\n    `protocolid` Int32,\r\n    `reftaskid` Int32,\r\n    `interfacetype` Int32,\r\n    `rattype` Int32,\r\n    `proceduretype` Int32,\r\n    `begintime` Int64,\r\n    `endtime` Int64,\r\n    `procedurestatus` Int32,\r\n    `callerphonenumber` String,\r\n    `phonenumber` String,\r\n    `imsi` String,\r\n    `imei` String,\r\n    `nesrcip4` Int64,\r\n    `nesrcip6` String,\r\n    `nedestip4` Int64,\r\n    `nedestip6` String,\r\n    `apn` String,\r\n    `lactac` Int32,\r\n    `cieci` Int32,\r\n    `smssenderphone` String,\r\n    `smsrecieverphone` String,\r\n    `smsdirection` Int32,\r\n    `smscode` Int32,\r\n    `smstime` Int64,\r\n    `smslength` Int32,\r\n    `smscontent` String,\r\n    `pdate` Date,\r\n    `srcip` String,\r\n    `destip` String,\r\n    `nesrcip` String,\r\n    `nedestip` String,\r\n    `mcc` Int32,\r\n    `mnc` Int32\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/xdr_fullaudit_monitor_local', '{replica}')\r\nPARTITION BY pdate\r\nORDER BY (gathertime, srcip4)\r\nSETTINGS index_granularity = 8192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec.\r\n```\r\n\r\nquery result: different results returned\r\n```sql\r\nlocalhost :) select imsi, count(), toDateTime(today())\r\n:-] from xdr_fullaudit_monitor\r\n:-] where phonenumber='8613910000009'\r\n:-] and toDate(gathertime) < '2022-06-24'\r\n:-] group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < '2022-06-24')\r\nGROUP BY imsi\r\n\r\nQuery id: 40159474-b3dd-4559-bb8a-4b4e10447ac6\r\n\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.049 sec. Processed 1.01 million rows, 54.71 MB (20.68 million rows/s., 1.12 GB/s.)\r\n\r\nlocalhost :) select imsi, count(), toDateTime(today())\r\n:-] from xdr_fullaudit_monitor\r\n:-] where phonenumber='8613910000009'\r\n:-] and toDate(gathertime) < today()\r\n:-] group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < today())\r\nGROUP BY imsi\r\n\r\nQuery id: 179157c5-37db-4c50-b6f7-031262ad814f\r\n\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.029 sec. Processed 521.39 thousand rows, 28.14 MB (18.23 million rows/s., 984.12 MB/s.)\r\n\r\nlocalhost :)\r\n```\r\n\r\nexpectation: consistent value returned\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38375/comments",
    "author": "iriszhang1121",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2022-06-24T09:05:54Z",
        "body": "Do \r\n\r\n```\r\nSET send_logs_level='trace' \r\n```\r\n\r\nafter that repeat your queries, and share the output (text please, no  screenshots)."
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T09:24:46Z",
        "body": "```sql\r\nlocalhost :) SET send_logs_level='trace' ;\r\n\r\nSET send_logs_level = 'trace'\r\n\r\nQuery id: 7ad97f44-0f67-412f-aef1-48347feb023b\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nlocalhost :) select imsi, count(), toDateTime(today())\u3000from xdr_fullaudit_monitor\u3000where phonenumber='8613910000009'\u3000and toDate(gathertime) < '2022-06-24'\u3000group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < '2022-06-24')\r\nGROUP BY imsi\r\n\r\nQuery id: f42408de-1d16-40c6-aa4f-4c21020a1f94\r\n\r\n[localhost.localdomain] 2022.06.24 17:23:27.669378 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> executeQuery: (from 192.168.70.74:42489, user: ck) select imsi, count(), toDateTime(today()) from xdr_fullaudit_monitor where phonenumber='8613910000009' and toDate(gathertime) < '2022-06-24' group by imsi;\r\n[localhost.localdomain] 2022.06.24 17:23:27.793402 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:27.795122 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:27.796139 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < '2022-06-24'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 17:23:27.796855 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 17:23:27.797026 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 17:23:27.797183 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 17:23:27.797697 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 19166]), unknown, and, (toDate(column 0) in (-inf, 19166]), and\r\n[localhost.localdomain] 2022.06.24 17:23:27.798080 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 17:23:27.798349 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798342 [ 11063 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798376 [ 22098 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_186_38 with 4 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798491 [ 11078 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_12 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798491 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798695 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798712 [ 11063 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798741 [ 11078 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798785 [ 22098 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798791 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798963 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 10/10 parts by partition key, 8 parts by primary key, 70/72 marks by primary key, 70 marks to read from 8 ranges\r\n[localhost.localdomain] 2022.06.24 17:23:27.799331 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2022.06.24 17:23:27.812845 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.812890 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.812937 [ 11047 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.812978 [ 11047 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.813546 [ 11053 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.813573 [ 11053 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.815083 [ 11044 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.815115 [ 11044 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.816556 [ 11047 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 886 to 1 rows (from 20.77 KiB) in 0.009364386 sec. (94613.785 rows/sec., 2.17 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:27.817786 [ 11044 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.010620213 sec. (2448.162 rows/sec., 57.38 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.371083 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> executeQuery: (from 192.168.70.77:44342, user: ck, initial_query_id: f42408de-1d16-40c6-aa4f-4c21020a1f94) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < '2022-06-24') GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 17:23:27.819509 [ 11053 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 97 to 1 rows (from 2.27 KiB) in 0.012371923 sec. (7840.333 rows/sec., 183.76 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:27.826737 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.019616167 sec. (2395.983 rows/sec., 56.16 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:27.826802 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 05:55:36.372256 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < '2022-06-24'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 05:55:36.372702 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 05:55:36.372899 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 05:55:36.373453 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 19166]), unknown, and, (toDate(column 0) in (-inf, 19166]), and\r\n[localhost.localdomain] 2021.04.14 05:55:36.463249 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 05:55:36.484728 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 3 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484752 [ 19829 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 22 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484872 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 4 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484903 [ 19829 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 7 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484982 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484997 [ 19829 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.485070 [ 19702 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 46 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.485060 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.485318 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 6 parts by primary key, 63/65 marks by primary key, 63 marks to read from 6 ranges\r\n[localhost.localdomain] 2021.04.14 05:55:36.485688 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Reading approx. 491993 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 05:55:36.491037 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 05:55:36.491109 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 05:55:36.493944 [ 17595 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 05:55:36.493998 [ 17595 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 05:55:36.500811 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 05:55:36.500881 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 05:55:36.502973 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 705 to 1 rows (from 16.52 KiB) in 0.016656931 sec. (42324.724 rows/sec., 991.99 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.503033 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.016718803 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.514505 [ 17595 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 153 to 1 rows (from 3.59 KiB) in 0.028188772 sec. (5427.693 rows/sec., 127.21 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.519732 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 86 to 1 rows (from 2.02 KiB) in 0.033383269 sec. (2576.141 rows/sec., 60.38 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.519757 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 05:55:36.521220 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Information> executeQuery: Read 491993 rows, 25.34 MiB in 0.150027678 sec., 3279348 rows/sec., 168.88 MiB/sec.\r\n[localhost.localdomain] 2021.04.14 05:55:36.521332 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 17:23:27.969174 [ 11033 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 17:23:27.969311 [ 11033 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 7.074e-05 sec. (14136.274 rows/sec., 441.76 KiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n[localhost.localdomain] 2022.06.24 17:23:27.984527 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Information> executeQuery: Read 1013387 rows, 52.18 MiB in 0.31498153 sec., 3217290 rows/sec., 165.65 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 17:23:27.984622 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.318 sec. Processed 1.01 million rows, 54.71 MB (3.19 million rows/s., 172.25 MB/s.)\r\n```\r\n\r\n```sql\r\nlocalhost :) select imsi, count(), toDateTime(today())\u3000from xdr_fullaudit_monitor\u3000where phonenumber='8613910000009'\u3000and toDate(gathertime) < today()\u3000group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < today())\r\nGROUP BY imsi\r\n\r\nQuery id: bee7b3ed-3ea8-4cbb-b914-2494815fb3a4\r\n\r\n[localhost.localdomain] 2022.06.24 17:23:34.173028 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> executeQuery: (from 192.168.70.74:42489, user: ck) select imsi, count(), toDateTime(today()) from xdr_fullaudit_monitor where phonenumber='8613910000009' and toDate(gathertime) < today() group by imsi;\r\n[localhost.localdomain] 2022.06.24 17:23:34.174579 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:34.175579 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:34.176518 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < today()\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 17:23:34.176904 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 17:23:34.177044 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 17:23:34.177185 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 17:23:34.177728 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 19166]), unknown, and, (toDate(column 0) in (-inf, 19166]), and\r\n[localhost.localdomain] 2022.06.24 17:23:34.178077 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 17:23:34.178298 [ 11095 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178302 [ 11062 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178313 [ 11068 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_186_38 with 4 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178361 [ 11074 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_12 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178478 [ 11068 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178548 [ 11095 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178600 [ 11068 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178588 [ 11062 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178684 [ 11074 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178673 [ 11062 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178884 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 10/10 parts by partition key, 8 parts by primary key, 70/72 marks by primary key, 70 marks to read from 8 ranges\r\n[localhost.localdomain] 2022.06.24 17:23:34.179296 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2022.06.24 17:23:34.185100 [ 11087 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.185131 [ 11087 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.185845 [ 11067 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.185881 [ 11067 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.189158 [ 11087 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 886 to 1 rows (from 20.77 KiB) in 0.009313853 sec. (95127.119 rows/sec., 2.18 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:51:17.854372 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> executeQuery: (from 192.168.70.77:44493, user: ck, initial_query_id: bee7b3ed-3ea8-4cbb-b914-2494815fb3a4) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < today()) GROUP BY imsi\r\n[localhost.localdomain] 2021.04.14 05:51:17.855652 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < today()\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 05:51:17.856176 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 05:51:17.856394 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 05:51:17.857024 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 18730]), unknown, and, (toDate(column 0) in (-inf, 18730]), and\r\n[localhost.localdomain] 2021.04.14 05:51:17.857514 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 05:51:17.857762 [ 17820 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857803 [ 17817 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857781 [ 23460 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857872 [ 17820 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857900 [ 23460 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857962 [ 17820 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857942 [ 17817 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857996 [ 23460 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.858295 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Selected 8/8 parts by partition key, 0 parts by primary key, 0/65 marks by primary key, 0 marks to read from 0 ranges\r\n[localhost.localdomain] 2021.04.14 05:51:17.858954 [ 23505 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.000386176 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 05:51:17.858973 [ 23505 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 05:51:17.859760 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 17:23:34.189950 [ 11056 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.189974 [ 11056 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.192728 [ 11056 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.012888998 sec. (2017.224 rows/sec., 47.28 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:34.192884 [ 11067 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 97 to 1 rows (from 2.27 KiB) in 0.013075659 sec. (7418.364 rows/sec., 173.87 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:34.195163 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.195193 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.212035 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.03221143 sec. (1459.109 rows/sec., 34.20 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:34.212064 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2022.06.24 17:23:34.212248 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 17:23:34.212306 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 2.8162e-05 sec. (35508.842 rows/sec., 1.08 MiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n[localhost.localdomain] 2022.06.24 17:23:34.213545 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Information> executeQuery: Read 521394 rows, 26.84 MiB in 0.040438327 sec., 12893560 rows/sec., 663.70 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 17:23:34.213640 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.044 sec. Processed 521.39 thousand rows, 28.14 MB (11.85 million rows/s., 639.84 MB/s.)\r\n\r\nlocalhost :)\r\n```"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T09:54:32Z",
        "body": "> Do\r\n> \r\n> ```\r\n> SET send_logs_level='trace' \r\n> ```\r\n> \r\n> after that repeat your queries, and share the output (text please, no screenshots).\r\n\r\nSorry for the inconvenience caused by screenshots. Already updated."
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T12:49:05Z",
        "body": ">    `gathertime` Int64,\r\n\r\nWhat do you store in `gathertime` ? Number of days after 1970-01-01 ?  19166 ?"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:02:18Z",
        "body": "> > `gathertime` Int64,\r\n> \r\n> What do you store in `gathertime` ? Number of days after 1970-01-01 ? 19166 ?\r\n\r\n```sql\r\nlocalhost :) SELECT\r\n:-]     min(gathertime), max(gathertime)\r\n:-] FROM xdr_fullaudit_monitor\r\n:-] WHERE (phonenumber = '8613910000009');\r\n\r\nSELECT\r\n    min(gathertime),\r\n    max(gathertime)\r\nFROM xdr_fullaudit_monitor\r\nWHERE phonenumber = '8613910000009'\r\n\r\nQuery id: 6551ce6c-2265-4e52-bd92-f5272e036742\r\n\r\n\u250c\u2500min(gathertime)\u2500\u252c\u2500max(gathertime)\u2500\u2510\r\n\u2502      1655776844 \u2502      1656035304 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.030 sec. Processed 1.01 million rows, 26.60 MB (34.10 million rows/s., 894.88 MB/s.)\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T14:15:05Z",
        "body": "try `(toDate(toDateTime(gathertime))` instead of `(toDate(gathertime)`"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:16:47Z",
        "body": "> try `(toDate(toDateTime(gathertime))` instead of `(toDate(gathertime)`\r\n\r\n```sql\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < '2022-06-24 00:00:00')\r\nGROUP BY imsi\r\n\r\nQuery id: 01207a60-d984-496f-b2e5-e97a1ddd970b\r\n\r\n[localhost.localdomain] 2022.06.24 22:10:48.285841 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < '2022-06-24 00:00:00') GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:10:48.288148 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:48.289361 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:48.290817 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < '2022-06-24 00:00:00'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:10:48.291183 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:10:48.291339 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:10:48.291623 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:10:48.292359 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1655999999]), unknown, and, (toDateTime(column 0) in (-inf, 1655999999]), and\r\n[localhost.localdomain] 2022.06.24 22:10:48.293721 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:10:48.293915 [ 11023 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.293923 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.293975 [ 11079 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294139 [ 11023 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_186_39 with 24 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294170 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294291 [ 11023 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294313 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294451 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294499 [ 11079 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294729 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:10:48.295070 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2022.06.24 22:10:48.300500 [ 11101 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.300551 [ 11101 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.302194 [ 11025 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.302262 [ 11025 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.302346 [ 11025 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 8 to 1 rows (from 192.00 B) in 0.006671096 sec. (1199.203 rows/sec., 28.11 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:48.302605 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.302646 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.304267 [ 11019 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.304315 [ 11019 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.306937 [ 11019 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.011292142 sec. (2302.486 rows/sec., 53.96 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.971501 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> executeQuery: (from 192.168.70.77:60182, user: ck, initial_query_id: 01207a60-d984-496f-b2e5-e97a1ddd970b) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < '2022-06-24 00:00:00') GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:10:48.309218 [ 11101 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 975 to 1 rows (from 22.85 KiB) in 0.013550795 sec. (71951.498 rows/sec., 1.65 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:48.315363 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.019682012 sec. (2387.967 rows/sec., 55.97 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:48.315384 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:38:31.973452 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < '2022-06-24 00:00:00'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:38:31.974396 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:38:31.974691 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:38:31.975607 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1655999999]), unknown, and, (toDateTime(column 0) in (-inf, 1655999999]), and\r\n[localhost.localdomain] 2021.04.14 10:38:31.976026 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:38:31.976286 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 22 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976298 [ 23484 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 3 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976326 [ 23492 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 4 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976447 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 7 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976490 [ 23492 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976540 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976683 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976516 [ 23511 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 46 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.977041 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Selected 8/8 parts by partition key, 6 parts by primary key, 63/65 marks by primary key, 63 marks to read from 6 ranges\r\n[localhost.localdomain] 2021.04.14 10:38:31.977720 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Reading approx. 491993 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:38:31.981474 [ 23512 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.981538 [ 23512 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.982765 [ 23497 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.982800 [ 23497 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.983295 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.983318 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.985750 [ 23479 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.985869 [ 23479 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.986257 [ 23512 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 705 to 1 rows (from 16.52 KiB) in 0.007242869 sec. (97337.119 rows/sec., 2.23 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.987588 [ 23479 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 23 to 1 rows (from 552.00 B) in 0.008576776 sec. (2681.660 rows/sec., 62.85 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.988711 [ 23497 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 91 to 1 rows (from 2.13 KiB) in 0.009722873 sec. (9359.374 rows/sec., 219.36 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.999838 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 125 to 1 rows (from 2.93 KiB) in 0.020862252 sec. (5991.683 rows/sec., 140.43 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.999870 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:38:32.001901 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Information> executeQuery: Read 491993 rows, 25.34 MiB in 0.030313373 sec., 16230229 rows/sec., 835.82 MiB/sec.\r\n[localhost.localdomain] 2021.04.14 10:38:32.001994 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 22:10:48.330437 [ 11026 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:10:48.330591 [ 11026 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 5.7341e-05 sec. (17439.528 rows/sec., 544.99 KiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2190 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2196 Progress: 1.01 million rows, 54.71 MB (20.21 million rows/s., 1.09 GB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:10:48.332382 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Information> executeQuery: Read 1013387 rows, 52.18 MiB in 0.046429682 sec., 21826274 rows/sec., 1.10 GiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:10:48.332568 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.050 sec. Processed 1.01 million rows, 54.71 MB (20.12 million rows/s., 1.09 GB/s.)\r\n```\r\n\r\n```sql\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < toDateTime(today()))\r\nGROUP BY imsi\r\n\r\nQuery id: 763fd9d2-369a-4a30-b9c5-220b9989d9f3\r\n\r\n[localhost.localdomain] 2022.06.24 22:10:54.336773 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < toDateTime(today())) GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:10:54.338210 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:54.339737 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:54.340636 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < toDateTime(today())\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:10:54.341057 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:10:54.341218 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:10:54.341392 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:10:54.341966 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1655999999]), unknown, and, (toDateTime(column 0) in (-inf, 1655999999]), and\r\n[localhost.localdomain] 2022.06.24 22:10:54.342294 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:10:54.342574 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342572 [ 11061 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342541 [ 11048 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342620 [ 11055 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_186_39 with 24 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342792 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342868 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342884 [ 11061 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342866 [ 11048 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342948 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.343196 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:10:54.343554 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:43:02.899896 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> executeQuery: (from 192.168.70.77:59473, user: ck, initial_query_id: 763fd9d2-369a-4a30-b9c5-220b9989d9f3) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < toDateTime(today())) GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:10:54.349894 [ 11077 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.349936 [ 11077 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.349972 [ 11053 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.350058 [ 11053 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.350232 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.350257 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.351914 [ 11077 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 370 to 1 rows (from 8.67 KiB) in 0.007825998 sec. (47278.315 rows/sec., 1.08 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:43:02.901143 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < toDateTime(today())\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:43:02.901724 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:43:02.901857 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:43:02.902526 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1618329599]), unknown, and, (toDateTime(column 0) in (-inf, 1618329599]), and\r\n[localhost.localdomain] 2021.04.14 10:43:02.902908 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:43:02.903159 [ 18980 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903160 [ 19829 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903184 [ 19084 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903192 [ 19083 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903259 [ 18980 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903479 [ 19083 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903479 [ 19084 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903465 [ 19829 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903709 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 0 parts by primary key, 0/65 marks by primary key, 0 marks to read from 0 ranges\r\n[localhost.localdomain] 2021.04.14 10:43:02.904263 [ 21674 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.000352989 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 10:43:02.904307 [ 21674 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:43:02.904942 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 22:10:54.357201 [ 11053 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 613 to 1 rows (from 14.37 KiB) in 0.013134431 sec. (46671.226 rows/sec., 1.07 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:54.357597 [ 11076 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.357641 [ 11076 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.360258 [ 11076 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.016166042 sec. (1608.310 rows/sec., 37.69 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:54.363220 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.01917536 sec. (2451.062 rows/sec., 57.45 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:54.363250 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2022.06.24 22:10:54.363453 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:10:54.363517 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 2.9546e-05 sec. (33845.529 rows/sec., 1.03 MiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2191 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2197 Progress: 521.39 thousand rows, 28.14 MB (16.87 million rows/s., 910.41 MB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:10:54.364907 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Information> executeQuery: Read 521394 rows, 26.84 MiB in 0.028065154 sec., 18577984 rows/sec., 956.31 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:10:54.365003 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.031 sec. Processed 521.39 thousand rows, 28.14 MB (16.74 million rows/s., 903.42 MB/s.)\r\n\r\nlocalhost :) \r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T14:17:09Z",
        "body": "BTW\r\n\r\n    `srcip4` Int64,    ---- should be UInt32\r\n    `srcport` Int32,   ---- should be UInt16\r\n    `destip4` Int64,   ---- should be UInt32\r\n    `destport` Int32,   --- should be UInt16\r\n    `gathertime` Int64,   ---- should be UInt32\r\n"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:19:59Z",
        "body": "> BTW\r\n> \r\n> ```\r\n> `srcip4` Int64,    ---- should be UInt32\r\n> `srcport` Int32,   ---- should be UInt16\r\n> `destip4` Int64,   ---- should be UInt32\r\n> `destport` Int32,   --- should be UInt16\r\n> `gathertime` Int64,   ---- should be UInt32\r\n> ```\r\n\r\nThx for your advice. I'll jot it down and forward to that guy in charge."
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T14:22:23Z",
        "body": "Try\r\n\r\n```sql\r\ngathertime < toInt64(toDateTime('2022-06-24 00:00:00'))\r\n\r\ngathertime < toInt64(toDateTime(today()))\r\n```\r\n"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:24:47Z",
        "body": "> ```sql\r\n> gathertime < toInt64(toDateTime(today()))\r\n> ```\r\n\r\n```sql\r\n[BEGIN] 2022/6/24 22:23:29\r\nSELECT\r\n:-]     imsi,\r\n:-]     count(),\r\n:-]     toDateTime(today())\r\n:-] FROM xdr_fullaudit_monitor\r\n:-] WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime('2022-06-24 00:00:00'))\r\n:-] GROUP BY imsi;GROUP BY imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime('2022-06-24 00:00:00')))\r\nGROUP BY imsi\r\n\r\nQuery id: 0c8a8095-b9be-4db7-94ad-3a583cb221fd\r\n\r\n[localhost.localdomain] 2022.06.24 22:23:02.045340 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime('2022-06-24 00:00:00')) GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:23:02.046777 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:02.047504 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:02.048228 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:23:02.048615 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:23:02.048851 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:23:02.049003 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:23:02.049596 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:02.049957 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:02.050122 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_81_81_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050112 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_0_80_41 (6 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050129 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_187_187_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050148 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050161 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050159 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050172 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050198 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 6\r\n[localhost.localdomain] 2022.06.24 22:23:02.050185 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050219 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050235 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 5 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050249 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050333 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_0_186_39 (18 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050372 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_0_31_7 (30 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050383 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_32_61_6 (16 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050420 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050402 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050450 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050486 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 30\r\n[localhost.localdomain] 2022.06.24 22:23:02.050535 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050540 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 16\r\n[localhost.localdomain] 2022.06.24 22:23:02.050539 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 18\r\n[localhost.localdomain] 2022.06.24 22:23:02.050603 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 8 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050627 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_62_68_2 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050621 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050663 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050709 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050727 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050760 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050763 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050777 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:02.050852 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050846 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:02.050898 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050932 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.051002 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.051238 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:23:02.051525 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:55:10.607619 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> executeQuery: (from 192.168.70.77:59473, user: ck, initial_query_id: 0c8a8095-b9be-4db7-94ad-3a583cb221fd) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime('2022-06-24 00:00:00'))) GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:23:02.055867 [ 11037 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.055909 [ 11037 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.057562 [ 11054 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.057601 [ 11054 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.057679 [ 11037 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 370 to 1 rows (from 8.67 KiB) in 0.005648689 sec. (65501.924 rows/sec., 1.50 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.058126 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.058154 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.058173 [ 11089 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.058207 [ 11089 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.061850 [ 11089 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.009813103 sec. (2649.519 rows/sec., 62.10 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.064742 [ 11054 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 613 to 1 rows (from 14.37 KiB) in 0.01273508 sec. (48134.758 rows/sec., 1.10 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.066856 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.014862219 sec. (3162.381 rows/sec., 74.12 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.066875 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:55:10.608935 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:55:10.609372 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:55:10.609584 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:55:10.610224 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:10.610691 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:10.610921 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220621_0_79_18 (3 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.610960 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.610928 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_0_56_11 (15 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.610964 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_0_44_8 (38 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611005 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 3\r\n[localhost.localdomain] 2021.04.14 10:55:10.610958 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_57_199_30 (4 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611047 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 3 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611042 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611035 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611074 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611122 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 38\r\n[localhost.localdomain] 2021.04.14 10:55:10.611124 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 15\r\n[localhost.localdomain] 2021.04.14 10:55:10.611176 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 11 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611144 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 4\r\n[localhost.localdomain] 2021.04.14 10:55:10.611209 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 7 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611247 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 4 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611396 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_74_74_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611426 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_45_73_14 (7 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611418 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611476 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611430 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611482 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611510 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:10.611531 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 7\r\n[localhost.localdomain] 2021.04.14 10:55:10.611497 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:10.611515 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:10.611589 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 5 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611573 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611591 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:10.611587 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:10.611698 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611697 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.612045 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 6 parts by primary key, 63/65 marks by primary key, 63 marks to read from 6 ranges\r\n[localhost.localdomain] 2021.04.14 10:55:10.612370 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Reading approx. 491993 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:55:10.616094 [ 19081 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.616143 [ 19081 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.617472 [ 19828 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.617500 [ 19828 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.617593 [ 19827 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.617655 [ 19827 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.618410 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.618478 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.618796 [ 19828 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 23 to 1 rows (from 552.00 B) in 0.005875676 sec. (3914.443 rows/sec., 91.74 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.619917 [ 19081 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 705 to 1 rows (from 16.52 KiB) in 0.007012315 sec. (100537.412 rows/sec., 2.30 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.624391 [ 19827 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 91 to 1 rows (from 2.13 KiB) in 0.011497861 sec. (7914.516 rows/sec., 185.50 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.630753 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 125 to 1 rows (from 2.93 KiB) in 0.017862316 sec. (6997.973 rows/sec., 164.01 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.630771 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:55:10.633439 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Information> executeQuery: Read 491993 rows, 19.27 MiB in 0.025711574 sec., 19135079 rows/sec., 749.33 MiB/sec.\r\n[localhost.localdomain] 2021.04.14 10:55:10.633533 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> MemoryTracker: Peak memory usage (for query): 8.29 MiB.\r\n[localhost.localdomain] 2022.06.24 22:23:02.081227 [ 11098 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:23:02.081303 [ 11098 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 3.213e-05 sec. (31123.561 rows/sec., 972.61 KiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2192 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2198 Progress: 1.01 million rows, 40.26 MB (23.98 million rows/s., 952.67 MB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:23:02.083091 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Information> executeQuery: Read 1013387 rows, 38.40 MiB in 0.037657394 sec., 26910704 rows/sec., 1019.70 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:23:02.083183 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> MemoryTracker: Peak memory usage (for query): 9.10 MiB.\r\n\r\n1 rows in set. Elapsed: 0.042 sec. Processed 1.01 million rows, 40.26 MB (23.84 million rows/s., 947.08 MB/s.)\r\n\r\nlocalhost :) SELECT\r\n:-]     imsi,\r\n:-]     count(),\r\n:-]     toDateTime(today())\r\n:-] FROM xdr_fullaudit_monitor\r\n:-] WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime(today()))\r\n:-] GROUP BY imsi;GROUP BY imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime(today())))\r\nGROUP BY imsi\r\n\r\nQuery id: 2d446fff-6daa-40e8-863e-b25bbb4a8e00\r\n\r\n[localhost.localdomain] 2022.06.24 22:23:14.195639 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime(today())) GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:23:14.197818 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:14.199770 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:14.200721 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:23:14.201161 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:23:14.201313 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:23:14.201522 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:23:14.202061 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:14.202412 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:14.202612 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_81_81_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202616 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_187_187_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202638 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202625 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_0_80_41 (6 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202638 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202658 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.202678 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.202683 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202682 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202701 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202713 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 6\r\n[localhost.localdomain] 2022.06.24 22:23:14.202731 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 5 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202768 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_0_186_39 (18 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202798 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202804 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_32_61_6 (16 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202826 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202821 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 18\r\n[localhost.localdomain] 2022.06.24 22:23:14.202798 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_0_31_7 (30 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202845 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 16\r\n[localhost.localdomain] 2022.06.24 22:23:14.202846 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202862 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 8 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202864 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202906 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_62_68_2 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202924 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202908 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 30\r\n[localhost.localdomain] 2022.06.24 22:23:14.202939 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.202939 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202954 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202967 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:14.202953 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202987 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.203008 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.203010 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.203029 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:14.203042 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.203057 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.203254 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:23:14.203793 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:55:22.762581 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> executeQuery: (from 192.168.70.77:59473, user: ck, initial_query_id: 2d446fff-6daa-40e8-863e-b25bbb4a8e00) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime(today()))) GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:23:14.210527 [ 2350 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.210562 [ 2350 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.212659 [ 11070 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.212702 [ 11070 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.212703 [ 11085 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.212764 [ 11085 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.213696 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.213739 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.214211 [ 11085 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 370 to 1 rows (from 8.67 KiB) in 0.009333972 sec. (39640.145 rows/sec., 929.07 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:22.763663 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:55:22.764097 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:55:22.764250 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:55:22.764729 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1618329599]), and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:22.765075 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:22.765290 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_0_44_8 (38 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765280 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220621_0_79_18 (3 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765308 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_57_199_30 (4 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765315 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 37\r\n[localhost.localdomain] 2021.04.14 10:55:22.765317 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_0_56_11 (15 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765335 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 38\r\n[localhost.localdomain] 2021.04.14 10:55:22.765331 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 3\r\n[localhost.localdomain] 2021.04.14 10:55:22.765354 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 6 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765345 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 14\r\n[localhost.localdomain] 2021.04.14 10:55:22.765358 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 4\r\n[localhost.localdomain] 2021.04.14 10:55:22.765322 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765373 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 15\r\n[localhost.localdomain] 2021.04.14 10:55:22.765383 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 2 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765392 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 4 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765386 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 3\r\n[localhost.localdomain] 2021.04.14 10:55:22.765413 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 2 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765434 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_45_73_14 (7 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765464 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 6\r\n[localhost.localdomain] 2021.04.14 10:55:22.765499 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 7\r\n[localhost.localdomain] 2021.04.14 10:55:22.765515 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 3 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765647 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765657 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765669 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_74_74_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765682 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:22.765675 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:22.765700 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765704 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:22.765720 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765722 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765756 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765708 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765795 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.766024 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 0 parts by primary key, 0/65 marks by primary key, 0 marks to read from 0 ranges\r\n[localhost.localdomain] 2021.04.14 10:55:22.766662 [ 19084 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.000411476 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:22.766680 [ 19084 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:55:22.767622 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 22:23:14.216135 [ 11070 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.011253869 sec. (2310.317 rows/sec., 54.15 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:14.219585 [ 2350 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.014760929 sec. (3184.081 rows/sec., 74.63 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:14.221452 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 613 to 1 rows (from 14.37 KiB) in 0.016595627 sec. (36937.441 rows/sec., 865.72 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:14.221472 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2022.06.24 22:23:14.221645 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:23:14.221705 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 2.6937e-05 sec. (37123.659 rows/sec., 1.13 MiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2193 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2199 Progress: 521.39 thousand rows, 20.06 MB (16.25 million rows/s., 625.29 MB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:23:14.223533 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Information> executeQuery: Read 521394 rows, 19.13 MiB in 0.027823368 sec., 18739427 rows/sec., 687.65 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:23:14.223627 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> MemoryTracker: Peak memory usage (for query): 9.10 MiB.\r\n\r\n1 rows in set. Elapsed: 0.032 sec. Processed 521.39 thousand rows, 20.06 MB (16.13 million rows/s., 620.70 MB/s.)\r\n\r\nlocalhost :) \r\n[END] 2022/6/24 22:24:05\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T17:55:26Z",
        "body": "Do you have 2 servers/shards?  It seems you have different time\r\n\r\n```\r\ntoInt64(toDateTime('2022-06-24 00:00:00'))\r\nKey condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\nKey condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n\r\ngathertime < toInt64(toDateTime(today()))\r\nKey condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\nKey condition: unknown, (column 0 in (-inf, 1618329599]), and, unknown, and\r\n\r\nSELECT toDateTime(1655999999)\r\n\u250c\u2500toDateTime(1655999999)\u2500\u2510\r\n\u2502    2022-06-23 15:59:59 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT toDateTime(1618329599)\r\n\u250c\u2500toDateTime(1618329599)\u2500\u2510\r\n\u2502    2021-04-13 15:59:59 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\nthe first server calculates `toInt64(toDateTime(today())` to 1655999999 == 2022-06-23 15:59:59\r\nthe second server calculates `toInt64(toDateTime(today())` to 1618329599 == 2021-04-13 15:59:59\r\n"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-25T00:49:23Z",
        "body": "> Do you have 2 servers/shards? It seems you have different time\r\n> \r\n> ```\r\n> toInt64(toDateTime('2022-06-24 00:00:00'))\r\n> Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n> Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n> \r\n> gathertime < toInt64(toDateTime(today()))\r\n> Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n> Key condition: unknown, (column 0 in (-inf, 1618329599]), and, unknown, and\r\n> \r\n> SELECT toDateTime(1655999999)\r\n> \u250c\u2500toDateTime(1655999999)\u2500\u2510\r\n> \u2502    2022-06-23 15:59:59 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> SELECT toDateTime(1618329599)\r\n> \u250c\u2500toDateTime(1618329599)\u2500\u2510\r\n> \u2502    2021-04-13 15:59:59 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> ```\r\n> \r\n> the first server calculates `toInt64(toDateTime(today())` to 1655999999 == 2022-06-23 15:59:59 the second server calculates `toInt64(toDateTime(today())` to 1618329599 == 2021-04-13 15:59:59\r\n\r\nYes. I have confirmed that time is inconsistent on the two servers.\r\n\r\n```sql\r\nClickHouse client version 21.8.14.5 (official build).\r\nConnecting to database falcon0325 at 192.168.70.75:9000 as user ck.\r\nConnected to ClickHouse server version 21.8.14 revision 54449.\r\n\r\nlocalhost :) select toInt64(toDateTime(today()));\r\n\r\nSELECT toInt64(toDateTime(today()))\r\n\r\nQuery id: d91ef3d6-cd24-45de-bb12-ff3e75128a8b\r\n\r\n\u250c\u2500toInt64(toDateTime(today()))\u2500\u2510\r\n\u2502                   1618329600 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.004 sec. \r\n\r\nlocalhost :) select toDateTime(1618329600);\r\n\r\nSELECT toDateTime(1618329600)\r\n\r\nQuery id: 5198240c-832f-404a-8c31-53bbad7e25d8\r\n\r\n\u250c\u2500toDateTime(1618329600)\u2500\u2510\r\n\u2502    2021-04-14 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec. \r\n\r\n```\r\n\r\n```sql\r\nClickHouse client version 21.8.14.5 (official build).\r\nConnecting to database falcon0325 at 192.168.70.77:9000 as user ck.\r\nConnected to ClickHouse server version 21.8.14 revision 54449.\r\n\r\nlocalhost :) select toInt64(toDateTime(today()));\r\n\r\nSELECT toInt64(toDateTime(today()))\r\n\r\nQuery id: 206a391c-1524-4f1b-8cb5-42e21ed39d13\r\n\r\n\u250c\u2500toInt64(toDateTime(today()))\u2500\u2510\r\n\u2502                   1656086400 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.004 sec.\r\nlocalhost :) select toDateTime(1656086400);\r\n\r\nSELECT toDateTime(1656086400)\r\n\r\nQuery id: 6d1cd4a8-f42e-4798-8ab7-614ffbe8d12b\r\n\r\n\u250c\u2500toDateTime(1656086400)\u2500\u2510\r\n\u2502    2022-06-25 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.004 sec. \r\n```"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-25T01:00:47Z",
        "body": "> \r\n\r\n\r\n\r\n> Do you have 2 servers/shards? It seems you have different time\r\n> \r\n> ```\r\n> toInt64(toDateTime('2022-06-24 00:00:00'))\r\n> Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n> Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n> \r\n> gathertime < toInt64(toDateTime(today()))\r\n> Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n> Key condition: unknown, (column 0 in (-inf, 1618329599]), and, unknown, and\r\n> \r\n> SELECT toDateTime(1655999999)\r\n> \u250c\u2500toDateTime(1655999999)\u2500\u2510\r\n> \u2502    2022-06-23 15:59:59 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> SELECT toDateTime(1618329599)\r\n> \u250c\u2500toDateTime(1618329599)\u2500\u2510\r\n> \u2502    2021-04-13 15:59:59 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> ```\r\n> \r\n> the first server calculates `toInt64(toDateTime(today())` to 1655999999 == 2022-06-23 15:59:59 the second server calculates `toInt64(toDateTime(today())` to 1618329599 == 2021-04-13 15:59:59\r\n\r\nThanks. I changed the server time and the results from two queries are consistent now."
      }
    ]
  },
  {
    "number": 38358,
    "title": "When final query processing is done at Initiator node?",
    "created_at": "2022-06-23T19:07:23Z",
    "closed_at": "2022-06-23T20:56:23Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38358",
    "body": " In terms of memory usage , \r\nIs there any difference when final query processing is done on the initiator node and when it is done on the shard and initiator only proxy the data.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38358/comments",
    "author": "HeenaBansal2009",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-06-23T20:43:32Z",
        "body": "Distributed table is unable to perform FINAL agains shards query results. It's impossible."
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-23T20:46:35Z",
        "body": "@den-crane , I am not sure if I understand your answer . \r\nMy question is around the below comments in doc:\r\n\r\n0 \u2014 Disabled (final query processing is done on the initiator node).\r\n1 - Do not merge aggregation states from different servers for distributed query processing (query completelly processed on the shard, initiator only proxy the data), can be used in case it is for certain that there are different keys on different shards.\r\n\r\nKeeping this value to default , does it make any difference in terms of memory usage ?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-23T20:50:55Z",
        "body": "I see. I thought you were asking about FROM ... FINAL."
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-23T20:53:55Z",
        "body": "> I see. I thought you were asking about FROM ... FINAL.\r\n\r\nNo , My bad I was not clear on the very first stage.  My question is around distributed_group_by_no_merge. \r\nDoes toggling its value make difference w.r.t memory usage for query processing ?\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-23T20:55:46Z",
        "body": "`1 - Do not merge aggregation` is faster and uses less memory. In this case initator just proxies the results from shards to the client (there is more settings to do sort at the initiator without aggregation)\r\n\r\n```\r\n  --distributed_group_by_no_merge arg                                               \r\n    If 1, Do not merge aggregation states from different servers for distributed queries (shards will process query up to the Complete stage, initiator just proxies the data\r\n                                                                                     from the shards).\r\n    If 2 the initiator will apply ORDER BY and LIMIT stages (it is not in case when shard process query up to the Complete stage)\r\n```\r\n\r\nFinal Aggregation is quite expensive it better to avoid it if possible.\r\n\r\nThere is a new feature\r\n\r\n```\r\n --optimize_distributed_group_by_sharding_key arg                                  \r\n Optimize GROUP BY sharding_key queries (by avoiding costly aggregation on the initiator server).\r\n```\r\nIt disables Final Aggregation automatically if groupby is suitable for sharding key."
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-23T21:00:17Z",
        "body": "So In my environment , When distributed_group_by_no_merge=0, I am getting OOM exception \r\nand when I a m setting to distributed_group_by_no_merge=1 ,  my query is processed fully without error.\r\n\r\nDo you think any possible reasons behind ?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-23T21:02:16Z",
        "body": ">Do you think any possible reasons behind ?\r\n\r\nYes, distributed_group_by_no_merge=0 requires more resources.\r\n\r\nI already answered.\r\n\r\n> 1 - Do not merge aggregation is faster and **uses less memory.** "
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-24T13:47:16Z",
        "body": "> > Do you think any possible reasons behind ?\r\n> \r\n> Yes, distributed_group_by_no_merge=0 requires more resources.\r\n> @den-crane , I agree that executor nodes only processed subquery of initial query and merging of partial aggregation happens at initiator node. Hence this scenario consumes more memory.\r\nHowever , executing the same query on same set of data on standalone node works fine. \r\n\r\n> I already answered.\r\n> \r\n> > 1 - Do not merge aggregation is faster and **uses less memory.**\r\n\r\n"
      }
    ]
  },
  {
    "number": 38193,
    "title": "Error after changing clickhouse data dirs",
    "created_at": "2022-06-18T11:56:03Z",
    "closed_at": "2024-02-27T10:42:09Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38193",
    "body": "Hi,\r\n\r\nClickhouse by default storing metadata/data into /var/lib/clickhouse. '/var/lib' is typically used for s/w install. I moved contents in /var/lib/clickhouse/* to ~/clickhouse after stopping server\r\n\r\nAlso ownership of ~/clickhouse was given to clickhouse:clickhouse recursively and also updated paths in config.xml \r\n\r\nMove is done as below \r\n\r\ncp -al /var/lib/clickhouse/* ~/clickhouse/\r\nrm -rf /var/lib/clickhouse\r\n\r\n\r\nServer is up after above activity and also client is connected and also executing sql successfully but there are lot of errors in server log. \r\n\r\nPlease suggest\r\n\r\nThank you\r\n\r\n\r\n```\r\n2022.06.18 19:38:45.358360 [ 19006 ] {} <Error> void DB::BackgroundJobsAssignee::threadFunc(): Code: 214. DB::ErrnoException: Could not calculate available disk space (statvfs), errno: 2, strerror: No such file or directory. (CANNOT_STATVFS), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb8a147a in /usr/bin/clickhouse\r\n1. DB::throwFromErrnoWithPath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int) @ 0xb8a252a in /usr/bin/clickhouse\r\n2. DB::getStatVFS(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xb8e4e5a in /usr/bin/clickhouse\r\n3. DB::DiskLocal::getAvailableSpace() const @ 0x15970be1 in /usr/bin/clickhouse\r\n4. DB::DiskLocal::getUnreservedSpace() const @ 0x15970d7a in /usr/bin/clickhouse\r\n5. DB::StoragePolicy::getMaxUnreservedFreeSpace() const @ 0x15ca8819 in /usr/bin/clickhouse\r\n6. DB::MergeTreeDataMergerMutator::getMaxSourcePartsSizeForMerge(unsigned long, unsigned long) const @ 0x16bf9cfa in /usr/bin/clickhouse\r\n7. DB::StorageMergeTree::selectPartsToMerge(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, std::__1::unique_lock<std::__1::mutex>&, std::__1::shared_ptr<DB::MergeTreeTransaction> const&, bool, DB::SelectPartsDecision*) @ 0x169808cd in /usr/bin/clickhouse\r\n8. DB::StorageMergeTree::scheduleDataProcessingJob(DB::BackgroundJobsAssignee&) @ 0x16984c3e in /usr/bin/clickhouse\r\n9. DB::BackgroundJobsAssignee::threadFunc() @ 0x16acab47 in /usr/bin/clickhouse\r\n10. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x1556c2f8 in /usr/bin/clickhouse\r\n11. DB::BackgroundSchedulePool::threadFunction() @ 0x1556f5b6 in /usr/bin/clickhouse\r\n12. ? @ 0x1557042e in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xb94d0b7 in /usr/bin/clickhouse\r\n14. ? @ 0xb9504dd in /usr/bin/clickhouse\r\n15. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n16. /build/glibc-CVJwZb/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: clone @ 0x12161f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n (version 22.6.1.1985 (official build))\r\n2022.06.18 19:38:45.791476 [ 19010 ] {} <Error> void DB::BackgroundJobsAssignee::threadFunc(): Code: 214. DB::ErrnoException: Could not calculate available disk space (statvfs), errno: 2, strerror: No such file or directory. (CANNOT_STATVFS), Stack trace (when copying this message, always include the lines below):\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38193/comments",
    "author": "sigirisetti",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-06-20T17:01:19Z",
        "body": "`~/clickhouse/` is relative path. \r\n`~` == home folder. Home folder is different for different users.\r\nAvoid relative paths. Use absolute path."
      },
      {
        "user": "sigirisetti",
        "created_at": "2022-07-04T11:47:34Z",
        "body": "Ok. Changing to data drive worked fine. Thanks"
      },
      {
        "user": "danieladoghe",
        "created_at": "2022-07-08T13:08:07Z",
        "body": "Solution:\r\n\tUse absolute paths instead of relative paths"
      }
    ]
  },
  {
    "number": 37776,
    "title": "\u3010Query Limit related\u3011will system.processes table reflect all the queries in the very moment?",
    "created_at": "2022-06-02T09:14:56Z",
    "closed_at": "2022-06-16T13:20:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37776",
    "body": "- We are on 21.8.5.7-lts now\r\n- When we encounter an \"Too many simultaneous queries. Maximum:  150\" exception,  we immediately log into the server  and execute select count(*) from system.processes. but it seems that it seldom give us the expected result. some time it is far away from the limit which we set (150 max_concurrent_queries)\r\n- The processes table seems to  reflects the queries in the very moment, but isn't it weird that we never find the total items in it  is in a reasonable range ?\r\nThank you guys!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37776/comments",
    "author": "mo-avatar",
    "comments": [
      {
        "user": "e-mars",
        "created_at": "2022-06-10T17:39:48Z",
        "body": "hi @mo-avatar ,\r\nThe queries are likely very quick and transient so by the time you can switch and do a count, the peak has likely gone.\r\nMight be able to get an idea of where peaks are happening by using the `query_log`.\r\n\r\nyou can use something like this example to get an idea of where the query peaks were at and start to narrow down what queries are causing it to go over max. \r\nthis example uses just one day and is bucketed by seconds, you'd just need to adjust below for the timeframe you want to look at and bar params according to the volume of queries.\r\n\r\n```\r\nclickhouse :) select event_time, count(*) as my_count, bar(my_count, 0,200,50) as bar FROM system.query_log where toDate(event_time) = '2022-06-03' group by event_time order by event_time;\r\n\r\nSELECT\r\n    event_time,\r\n    count(*) AS my_count,\r\n    bar(my_count, 0, 200, 50) AS bar\r\nFROM system.query_log\r\nWHERE toDate(event_time) = '2022-06-03'\r\nGROUP BY event_time\r\nORDER BY event_time ASC\r\n\r\nQuery id: eea74f45-3d15-4231-a853-6e49b12b664e\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500event_time\u2500\u252c\u2500my_count\u2500\u252c\u2500bar\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 2022-06-03 00:25:21 \u2502      103 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b     \u2502\r\n\u2502 2022-06-03 00:25:22 \u2502       95 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b       \u2502\r\n\u2502 2022-06-03 00:25:23 \u2502       76 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            \u2502\r\n\u2502 2022-06-03 00:25:24 \u2502      118 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c \u2502\r\n\u2502 2022-06-03 00:25:25 \u2502      114 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  \u2502\r\n\u2502 2022-06-03 00:25:26 \u2502      117 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e \u2502\r\n\u2502 2022-06-03 00:25:27 \u2502      114 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  \u2502\r\n\u2502 2022-06-03 00:25:28 \u2502      110 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c   \u2502\r\n\u2502 2022-06-03 00:25:29 \u2502       99 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b      \u2502\r\n\u2502 2022-06-03 00:25:30 \u2502      109 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e   \u2502\r\n\u2502 2022-06-03 00:25:31 \u2502      104 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     \u2502\r\n\u2502 2022-06-03 00:25:32 \u2502      115 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  \u2502\r\n\u2502 2022-06-03 00:25:33 \u2502      115 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  \u2502\r\n\u2502 2022-06-03 00:25:34 \u2502      118 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c \u2502\r\n\u2502 2022-06-03 00:25:35 \u2502      113 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  \u2502\r\n\u2502 2022-06-03 00:25:36 \u2502       97 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e      \u2502\r\n\u2502 2022-06-03 00:25:37 \u2502      108 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502\r\n\u2502 2022-06-03 00:25:38 \u2502      112 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   \u2502\r\n\u2502 2022-06-03 00:25:39 \u2502       63 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b               \u2502\r\n\u2502 2022-06-03 18:20:48 \u2502        4 \u2502 \u2588                              \u2502\r\n\u2502 2022-06-03 18:20:55 \u2502        2 \u2502 \u258c                              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nthanks.\r\n\r\n\r\n"
      },
      {
        "user": "mo-avatar",
        "created_at": "2022-06-16T07:02:08Z",
        "body": "Thanks for your answer,  it helps a lot!"
      }
    ]
  },
  {
    "number": 37692,
    "title": "Merge Map by GROUP BY",
    "created_at": "2022-05-31T12:40:58Z",
    "closed_at": "2022-06-16T01:22:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37692",
    "body": "There is a table with map data\r\n```sql\r\nCREATE TABLE test.table1\r\n(\r\n    `id` String,\r\n    `test_map` Map(String, String)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nINSERT INTO test.table1 VALUES \r\n(1, {'a': '1','b': 'b', 'c': '2'})\r\n(2, {'d': 'd', 'a': '2'})\r\n(1, {'d': 'd', 'a': '2'});\r\n```\r\nCan I get the following data by `GROUP BY` id?\r\n```\r\nid      test_map\r\n1\t{'a':'2','b':'b','c':'2','d':'d'}\r\n2\t{'d':'d','a':'2'}\r\n```\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37692/comments",
    "author": "Onehr7",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2022-05-31T12:53:03Z",
        "body": "```\r\n\r\nSELECT\r\n    id,\r\n    maxMap(test_map)\r\nFROM test.table1\r\nGROUP BY id\r\n\r\nQuery id: 931d181a-0807-4684-ac67-f07d17a65831\r\n\r\n\u250c\u2500id\u2500\u252c\u2500maxMap(test_map)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 2  \u2502 {'a':'2','d':'d'}                 \u2502\r\n\u2502 1  \u2502 {'a':'2','b':'b','c':'2','d':'d'} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "Onehr7",
        "created_at": "2022-05-31T13:04:04Z",
        "body": "@UnamedRus Thanks for your help. but the maxMap function needs two parameters(key and value).Furthermore, maxMap seems work only for integers. is there any other aggregation map function."
      },
      {
        "user": "UnamedRus",
        "created_at": "2022-05-31T13:58:41Z",
        "body": "> but the maxMap function needs two parameters(key and value).\r\n\r\nNo, it also work for map data type\r\n\r\n> Furthermore, maxMap seems work only for integers. \r\n\r\nNo, in my example it works for Strings as well\r\n\r\nWhich version you are using?\r\n\r\n```\r\nSELECT version();\r\n```\r\n\r\n"
      },
      {
        "user": "Onehr7",
        "created_at": "2022-06-01T01:42:13Z",
        "body": "It's  21.11.6.7 "
      },
      {
        "user": "Onehr7",
        "created_at": "2022-06-01T02:15:53Z",
        "body": "@UnamedRus It works after I upgrade my clickhouse to 22.2.2.1. Thank you!"
      }
    ]
  },
  {
    "number": 37682,
    "title": "Get max date - partition key",
    "created_at": "2022-05-31T09:41:43Z",
    "closed_at": "2022-06-01T13:12:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37682",
    "body": "Hi,\r\ncould someone explain me why query like ``SELECT max(date) from db.table`` doesn't use index and isn't instantly while column date is partition key? For example\r\n```\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2502         Indexes:                                                              \u2502\r\n\u2502           MinMax                                                              \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 100/100                                                    \u2502\r\n\u2502             Granules: 61718/61718                                             \u2502\r\n\u2502           Partition                                                           \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 100/100                                                    \u2502\r\n\u2502             Granules: 61718/61718                                             \u2502\r\n\u2502           PrimaryKey                                                          \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 100/100                                                    \u2502\r\n\u2502             Granules: 61718/61718                                             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nDon't understand why it is looking in all partitions. It seems to be obvious where it should look for.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37682/comments",
    "author": "rkozlo",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-05-31T11:29:18Z",
        "body": "It's unclear what is you mean. \r\n`max( partition_key )` uses min_max partition index (virtual projection). \r\nBut it reads all records of min_max index because min_max index is unordered and it needs to scan it to find `max`.\r\n\r\n```\r\ncreate table A( date Date, S String) Engine=MergeTree partition by  date order by S;\r\ninsert into A select today()+1, '' from numbers(1000);\r\n\r\nSELECT max(date) FROM A\r\n\u250c\u2500\u2500max(date)\u2500\u2510\r\n\u2502 2022-06-01 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nEXPLAIN SELECT max(date) FROM A\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                         \u2502\r\n\u2502   SettingQuotaAndLimits (Set limits and quota after reading from storage)           \u2502\r\n\u2502     ReadFromStorage (MergeTree(with Aggregate projection _minmax_count_projection)) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nVS without index\r\n```\r\nSELECT max(identity(date))FROM A\r\n\u250c\u2500max(identity(date))\u2500\u2510\r\n\u2502          2022-06-01 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 row in set. Elapsed: 0.004 sec. Processed 1.00 thousand rows, 2.00 KB (268.62 thousand rows/s., 537.23 KB/s.)\r\n\r\nEXPLAIN SELECT max(identity(date)) FROM A\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree (default.A)                                         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "rkozlo",
        "created_at": "2022-06-01T06:42:45Z",
        "body": "I understand what you mean. I'lll give a an example. Have similar table like you, here is engine definition\r\n`\r\nENGINE = MergeTree                                                                                                                     \r\nPARTITION BY date                                                                                                                      \r\nORDER BY date\r\n`\r\nAnd then the same explains\r\n\r\n```\r\nEXPLAIN\r\nSELECT max(identity(date))\r\nFROM db.table\r\n\r\nQuery id: 9777adee-b92c-438b-9817-10278cda3ffb\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n5 rows in set. Elapsed: 0.002 sec. \r\n\r\nclickhouse :) explain select max(date) from db.table\r\n\r\nEXPLAIN\r\nSELECT max(date)\r\nFROM db.table\r\n\r\nQuery id: 026609f3-8c94-4736-aa00-a6c2d7b60bfa\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n5 rows in set. Elapsed: 0.004 sec. \r\n\r\nclickhouse :) explain indexes=1 select max(date) from db.table\r\n\r\nEXPLAIN indexes = 1\r\nSELECT max(date)\r\nFROM db.table\r\n\r\nQuery id: c13c2808-3019-4b13-b572-79c4e14af255\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2502         Indexes:                                                              \u2502\r\n\u2502           MinMax                                                              \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 648/648                                                    \u2502\r\n\u2502             Granules: 75668/75668                                             \u2502\r\n\u2502           Partition                                                           \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 648/648                                                    \u2502\r\n\u2502             Granules: 75668/75668                                             \u2502\r\n\u2502           PrimaryKey                                                          \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 648/648                                                    \u2502\r\n\u2502             Granules: 75668/75668                                             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n18 rows in set. Elapsed: 0.015 sec.\r\n\r\nclickhouse :) select max(date), min(date) from db.table\r\n\r\nSELECT\r\n    max(date),\r\n    min(date)\r\nFROM db.table\r\n\r\nQuery id: 05131eaa-499b-49d2-ae37-eb68eb3d7688\r\n\r\n\u250c\u2500\u2500max(date)\u2500\u252c\u2500\u2500min(date)\u2500\u2510\r\n\u2502 2021-08-18 \u2502 2020-09-25 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.093 sec. Processed 453.63 million rows, 907.26 MB (4.90 billion rows/s., 9.79 GB/s.)\r\n\r\nclickhouse :) select count() from db.table\r\n\r\nSELECT count()\r\nFROM db.table\r\n\r\nQuery id: 114b4125-a58c-467d-9670-a66f30351bbc\r\n\r\n\u250c\u2500\u2500\u2500count()\u2500\u2510\r\n\u2502 453630412 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nIt is a bit confusing. It goes through all rows in table. Btw clickhouse version is 21.8.15"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-01T13:12:09Z",
        "body": ">Btw clickhouse version is 21.8.15\r\n\r\nVirtual projections were introduced in 22.2 or 22.3. You need to upgrade."
      },
      {
        "user": "rkozlo",
        "created_at": "2022-06-01T13:28:55Z",
        "body": "@den-crane thanks"
      }
    ]
  },
  {
    "number": 37669,
    "title": "\u3010Atomic database related\u3011will the mix usage of atomic database and ordinary database in the same shard cause any problem?",
    "created_at": "2022-05-31T02:05:59Z",
    "closed_at": "2022-05-31T11:57:50Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37669",
    "body": "- The clickhouse version is 21.8.5.7\r\n- The database type is originally  Ordinary. \r\n- During the replacement of some fault node in certain shard, we need to re-create the database and table.\r\n- Because clickhouse have updated the default database type to Atomic, now we has a mixed database type of the same database on \r\ndifferent replica.\r\n- will it cause some potential problem? do we need to re create the database as an Ordinary one and re create all the tables  and let the part replication process execute again ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37669/comments",
    "author": "mo-avatar",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-05-31T11:17:29Z",
        "body": "No, it should not cause any problems. \r\n\r\nHowever, it may be useful to convert other databases to Atomic:\r\n```\r\nCREATE DATABASE db_name_atomic ENGINE=Atomic;\r\nRENAME TABLE db_name.table1 TO db_name_atomic.table1;\r\nRENAME TABLE db_name.table2 TO db_name_atomic.table2;\r\n-- (rename all tables and dictionaries)\r\nDROP DATABASE db_name;    -- make sure it's empty before running this\r\nRENAME DATABASE db_name_atomic to db_name;\r\n```\r\n(but it's not necessary)"
      },
      {
        "user": "mo-avatar",
        "created_at": "2022-06-02T01:35:46Z",
        "body": "Got it  thanks for all your answer."
      },
      {
        "user": "mo-avatar",
        "created_at": "2022-06-02T01:35:50Z",
        "body": "> No, it should not cause any problems.\r\n> \r\n> However, it may be useful to convert other databases to Atomic:\r\n> \r\n> ```\r\n> CREATE DATABASE db_name_atomic ENGINE=Atomic;\r\n> RENAME TABLE db_name.table1 TO db_name_atomic.table1;\r\n> RENAME TABLE db_name.table2 TO db_name_atomic.table2;\r\n> -- (rename all tables and dictionaries)\r\n> DROP DATABASE db_name;    -- make sure it's empty before running this\r\n> RENAME DATABASE db_name_atomic to db_name;\r\n> ```\r\n> \r\n> (but it's not necessary)\r\n\r\nGot it  thanks for your answer.\r\n"
      }
    ]
  },
  {
    "number": 37310,
    "title": "Question about ALTER TABLE ADD COLUMN IF NOT EXISTS ... , modify order by () ....",
    "created_at": "2022-05-18T03:44:09Z",
    "closed_at": "2022-05-18T08:21:29Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37310",
    "body": "While i create a table on clickhouse\r\n\r\n```\r\nCREATE TABLE TEST\r\n(\r\n\tID String,\r\n    NAME String\r\n)\r\nENGINE = MergeTree\r\nORDER BY (ID)\r\n```\r\n\r\nnow i want to **add column** and modify to **order by**\r\n\r\n```\r\nalter table TEST add column if not exists VERSION String, modify order by (ID, VERSION)\r\n\r\nalter table TEST add column if not exists HELLO String, modify order by (ID, VERSION, HELLO)\r\n```\r\n\r\nok\r\n\r\nThe table structure at this time is as follows\r\n\r\n```\r\nCREATE TABLE TEST\r\n(\r\n\r\n    ID String,\r\n\r\n    NAME String,\r\n\r\n    VERSION String,\r\n\r\n    HELLO String\r\n)\r\nENGINE = MergeTree\r\nPRIMARY KEY ID\r\nORDER BY (ID,VERSION,HELLO)\r\n```\r\n\r\nbut, When I execute the following statement again\r\n\r\n`alter table TEST add column if not exists VERSION String, modify order by (ID, VERSION)`\r\n\r\nok\r\n\r\nThe table structure at this time is as follows\r\n\r\n```\r\nCREATE TABLE TEST\r\n(\r\n\r\n    ID String,\r\n\r\n    NAME String,\r\n\r\n    VERSION String,\r\n\r\n    HELLO String\r\n)\r\nENGINE = MergeTree\r\nPRIMARY KEY ID\r\nORDER BY (ID,VERSION)\r\n```\r\n\r\n**My question is**, if the condition of **IF NOT EXISTES** is not established, will it not take effect for the following **modify order by** statement?\r\n\r\n\r\nWhat should I do so that the **column exists** without modification **order by**\r\n\r\nplease help me, thanks.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37310/comments",
    "author": "PatrickUncle",
    "comments": [
      {
        "user": "genzgd",
        "created_at": "2022-05-18T08:21:29Z",
        "body": "Multiple operations in a single `ALTER TABLE` command are normally independent, so you cannot make the second operation depend on the first.  Adding an `ORDER BY` column is a special case where the column itself must be ADDed in the same `ALTER TABLE` statement to preserve the correct sorting of both old and new data parts, but that doesn't change the general rule.\r\n\r\nYou could submit a feature request to support this use case but since modifying the sorting key is a rare operation it would probably not be a high priority for the community.\r\n\r\nIf you really must change the ORDER BY statement depending on whether the column exists, you can write a script testing for the existence of the column and then execute the ALTER TABLE statement if it does not.  A really ugly example:\r\n\r\n```\r\n#!/usr/bin/env bash\r\nHAS_COL=`clickhouse-client --query=\"SELECT count() FROM system.columns WHERE database='default' and table='table' and name='new_col'\"`\r\nif [ $HAS_COL == '0' ]\r\nthen\r\n  clickhouse-client --query=\"ALTER TABLE default.table ADD COLUMN new_col String, MODIFY ORDER BY (key, new_col)\"\r\nfi\r\n```\r\n\r\n"
      },
      {
        "user": "PatrickUncle",
        "created_at": "2022-05-18T08:33:03Z",
        "body": "Thanks"
      },
      {
        "user": "den-crane",
        "created_at": "2022-05-19T17:51:16Z",
        "body": "@PatrickUncle it's normal / designed behavior. You can reduce ORDERBY by columns from the tail not included into PRIMARYKEY. It does not change ORDER of the rows in the table and does not change Index.\r\n\r\n```\r\nCREATE TABLE TEST(a int,b int,c int,d int,e int, f int)\r\nENGINE = MergeTree PRIMARY KEY (a,b)\r\nORDER BY (a,b,c,d,e,f);\r\n\r\nalter table TEST  modify order by (a,b,c,d,e);\r\n\r\nalter table TEST  modify order by (a,b);\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-05-19T17:54:05Z",
        "body": "And `alter table` allows multiple commands.\r\n\r\n```\r\nalter table TEST add column z int, drop column e, modify column f String, drop partition tuple(), freeze;\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.014 sec.\r\n```"
      }
    ]
  },
  {
    "number": 37180,
    "title": "how often does clickhouse do sum in summingmergetree?",
    "created_at": "2022-05-13T08:47:46Z",
    "closed_at": "2022-05-13T12:41:01Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37180",
    "body": "From document\r\n\r\n> ClickHouse merges the inserted parts of data periodically...\r\n\r\n**My questions is how often do it merge? is therey any way to control it?**\r\n\r\nI'm considering using summingmergetree for my analysis flow, about 10 million events a day, each event is ~1kb, summingmergetree seems a perfect solution for saving disk space and speed performance, the only concern is when data is not mergeed, I have do `optimize table`, which is costly, especially in large table.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37180/comments",
    "author": "jasonbigl",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-05-13T12:36:24Z",
        "body": "When it necessary. Sum is a byproduct of merges. Usually CH merges after 6 inserts. In average I see 22 parts in a partition.\r\n\r\nMerges are eventual and may never happen. It depends on the number of inserts that happened after, the number of parts in the partition, size of parts. If the total size of input parts are greater than the maximum part size then they will never be merged.\r\n\r\nYou should not use `optimize table`. \r\nAll queries to summingmergetree should finalize aggregation using `sum / groupby`"
      },
      {
        "user": "jasonbigl",
        "created_at": "2022-05-13T12:41:01Z",
        "body": "@den-crane clear, informational answers!! thank you very much"
      }
    ]
  },
  {
    "number": 37062,
    "title": "column and table access management ",
    "created_at": "2022-05-10T07:30:28Z",
    "closed_at": "2022-05-10T09:54:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37062",
    "body": "In the official documentation and other documents, I could only find access management of database/row in the following formats (if not using sql)\r\n`<user1>\r\n    <databases>\r\n        <database_name>\r\n            <table1>\r\n                <filter>id = 1000</filter>\r\n            </table1>\r\n        </database_name>\r\n    </databases>\r\n</user1>`\r\n\r\n`        <allow_databases>\r\n           <database>test</database>\r\n        </allow_databases>`\r\n\r\nis there any way to manage table/column access using xml? eg. granting user with access to only databaseA.tableA and not databaseA.tableB.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37062/comments",
    "author": "Ravojay",
    "comments": [
      {
        "user": "melvynator",
        "created_at": "2022-05-10T09:44:03Z",
        "body": "Thank you for creating this GitHub issue.\r\n\r\nIt's not implemented in XML. Using XML you won't be able to create a column or table-level security.\r\n\r\nIf you want to define such a thing you will need to use SQL queries. \r\n\r\n"
      },
      {
        "user": "melvynator",
        "created_at": "2022-05-10T09:44:50Z",
        "body": "@Ravojay Why would you want to do it in XML rather than using SQL queries?"
      },
      {
        "user": "Ravojay",
        "created_at": "2022-05-10T09:53:03Z",
        "body": "> @Ravojay Why would you want to do it in XML rather than using SQL queries?\r\n@melvynator \r\nwe are using clickhouse on containers, so need to copy the access control file/system tables if the containers migrate in the self-healing process. would be possible, need development though.\r\nThe xml files are generated by service inside our company based on some configuration file we provide. So would be easier for us this way.\r\nLooks like we have to adopt the sql driven way if we want access control on all granularities.\r\n\r\nThanks for the reply!"
      }
    ]
  },
  {
    "number": 36927,
    "title": "Clickhouse use multiple columns in group by clause",
    "created_at": "2022-05-05T06:14:56Z",
    "closed_at": "2022-05-05T16:09:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36927",
    "body": "\r\nHi All, I am using the below query to generate materialized view for clickhouse but i want the result to be seperate by both the names ( name_top_apps, name_remote_top_emdpoints) can anyone help on this one\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS analytics.uflow_topapps_bytes_flowdir_or_013_mv\r\n  ENGINE = SummingMergeTree\r\n  PARTITION BY toYYYYMMDD ( Timestamp )\r\n  ORDER BY (Timestamp)\r\n  POPULATE\r\n  AS SELECT\r\n  toString(AppId) as name_top_apps,\r\n  IPv6NumToString(DstIP) as name_remote_top_emdpoints,\r\n  sum(FlowStatsBytesFwd) as upload_bytes,\r\n  sum(FlowStatsBytesRev) as download_bytes, \r\n  sum(FlowStatsBytesFwd + FlowStatsBytesRev) as cumulative_bytes,\r\n  sum(FlowStatsPktsFwd) as upload_flows,\r\n  sum(FlowStatsPktsFwd) as download_flows,\r\n  sum(FlowStatsPktsFwd + FlowStatsPktsRev) as cumulative_flows,\r\n  toInt64((sum(FlowStatsBytesFwd)* 8)/least(sum(Duration),60)) as upload_rate,\r\n  toInt64((sum(FlowStatsBytesRev)* 8)/least(sum(Duration),60)) as download_rate,\r\n  toInt64(((sum(FlowStatsBytesRev) + sum(FlowStatsBytesFwd)) * 8)/least(sum(Duration),60)) as cumulative_rate,\r\n  toStartOfInterval(`Timestamp`, INTERVAL 300 second) AS Timestamp,\r\n  CpeCNID as  CpeCNID\r\n  from analytics.sampled_uflow where AccessDenied = 0 and ( FlowDir == 2 or FlowDir == 0 or FlowDir == 3 )\r\n  Group by ( CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36927/comments",
    "author": "dhruvanand96",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-05-05T16:09:41Z",
        "body": "should be  `ORDER BY (CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)`\r\n\r\nsee:\r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS analytics.uflow_topapps_bytes_flowdir_or_013_mv\r\nENGINE = SummingMergeTree PARTITION BY toYYYYMMDD ( Timestamp )\r\nORDER BY (CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)\r\nPOPULATE\r\nAS SELECT\r\n   toString(AppId) as name_top_apps,\r\n   IPv6NumToString(DstIP) as name_remote_top_emdpoints,\r\n   sum(FlowStatsBytesFwd) as upload_bytes,\r\n   sum(FlowStatsBytesRev) as download_bytes,\r\n   sum(FlowStatsBytesFwd + FlowStatsBytesRev) as cumulative_bytes,\r\n   sum(FlowStatsPktsFwd) as upload_flows,\r\n   sum(FlowStatsPktsFwd) as download_flows,\r\n   sum(FlowStatsPktsFwd + FlowStatsPktsRev) as cumulative_flows,\r\n   toInt64((sum(FlowStatsBytesFwd)* 8)/least(sum(Duration),60)) as upload_rate,\r\n   toInt64((sum(FlowStatsBytesRev)* 8)/least(sum(Duration),60)) as download_rate,\r\n   toInt64(((sum(FlowStatsBytesRev) + sum(FlowStatsBytesFwd)) * 8)/least(sum(Duration),60)) as cumulative_rate,\r\n   toStartOfInterval(Timestamp, INTERVAL 300 second) AS Timestamp,\r\n   CpeCNID as CpeCNID\r\nfrom analytics.sampled_uflow where AccessDenied = 0 and ( FlowDir == 2 or FlowDir == 0 or FlowDir == 3 )\r\nGroup by  CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp\r\n```\r\n\r\ndon't use `( )` in groupBY, there is a bug."
      },
      {
        "user": "dhruvanand96",
        "created_at": "2022-05-06T17:49:38Z",
        "body": "thanks @den-crane "
      }
    ]
  },
  {
    "number": 36903,
    "title": "Truncate replicated cluster",
    "created_at": "2022-05-04T12:06:34Z",
    "closed_at": "2022-05-04T14:00:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36903",
    "body": "What is the point of this error? \r\n```\r\ntruncate table numbers on cluster stage_cluster\r\nCode: 371, e.displayText() = DB::Exception: For a distributed DDL on circular replicated cluster its table name must be qualified by database name. (version 21.8.14.5 (official build))\r\n```\r\nstage cluster has 2 replicas per shard. Wanted to truncate tables in db1.numbers and db2.numbers\r\n\r\nApplication during its run is writing data to table for other applications. After some time it is to be cleared and feeded again. For long time it was solved by clearing one replica and it was cleared on other replicas in shard. This solution is not good because one node can be not present and then data will not be cleared or will be delayed causing weird situations. So it is not reliable at all.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36903/comments",
    "author": "rkozlo",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-05-04T12:34:25Z",
        "body": "It literally means that table name must be qualified by database name if circular replication is used:\r\n```\r\ntruncate table db1.numbers on cluster stage_cluster;\r\ntruncate table db2.numbers on cluster stage_cluster;\r\n```"
      },
      {
        "user": "rkozlo",
        "created_at": "2022-05-04T13:00:31Z",
        "body": "That's what i see ;). I was asking more likely a reason for this fuse. Any dangerous?\r\n\r\n\r\n> truncate table db1.numbers on cluster stage_cluster;\r\ntruncate table db2.numbers on cluster stage_cluster;\r\n\r\nThat's how i replaced it now. Just wondered if it can be done with single query.\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2022-05-04T13:57:09Z",
        "body": "Sorry, I misunderstood your question :)\r\nWhen circular replication is used each host is responsible for two replicas (db1.numbers and db2.numbers), so each host should execute query twice with different databases. It's not implemented, currently each host can execute only one query per one distributed DDL task. And when database name is not specified, then it's not clear which one to choose, so it just fails with the error like this."
      },
      {
        "user": "rkozlo",
        "created_at": "2022-05-04T14:00:39Z",
        "body": "Thanks for explanation!"
      }
    ]
  },
  {
    "number": 36823,
    "title": "INSERT INTO ... SELECT shows Memory limit (for query) exceeded",
    "created_at": "2022-04-30T10:50:07Z",
    "closed_at": "2022-04-30T22:45:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36823",
    "body": "I'm running a simple query that:\r\n\r\n\r\nINSERT INTO order_book_2022_04_20 SELECT *\r\nFROM stock.order_book\r\nWHERE TradingDay = '2022-04-20'\r\n\r\nwhere the destination table `order_book_2022_04_20` is a temporary table and source table stock.order_book is a distributed table.\r\n\r\nIt shows `DB::Exception: Memory limit (for query) exceeded: would use 9.32 GiB (attempt to allocate chunk of 4228000 bytes), maximum: 9.31 GiB.` by default.\r\n\r\nWhen I adjust max_memory_size to zero(unlimited)\r\nIt shows that the peak memory usage is event higher than the query result.\r\n\r\nI've tried to adjust max_insert_block_size/max_block_size etc but no one take effects. I also tried to add --max-insert-block-size 1024 to  clickhouse-client but no helps.\r\n\r\nHere's the debug log:\r\n\r\n```\r\nch-shard-2-rep-1 :) set max_memory_usage=42949672960\r\n\r\nSET max_memory_usage = 42949672960\r\n\r\nQuery id: 5476a5e8-543b-40c0-b569-fc7bbfc7a55b\r\n\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:48.992649 [ 99630 ] {5476a5e8-543b-40c0-b569-fc7bbfc7a55b} <Debug> executeQuery: (from 10.20.140.3:43396) set max_memory_usage=42949672960\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:48.992765 [ 99630 ] {5476a5e8-543b-40c0-b569-fc7bbfc7a55b} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\nch-shard-2-rep-1 :) insert into order_book_2022_04_20 select * from stock.order_book where TradingDay = '2022-04-20'\r\n\r\nINSERT INTO order_book_2022_04_20 SELECT *\r\nFROM stock.order_book\r\nWHERE TradingDay = '2022-04-20'\r\n\r\nQuery id: 6d125a8a-3b1d-4277-a039-3ff9ab0157d5\r\n\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.342436 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> executeQuery: (from 10.20.140.3:43396) insert into order_book_2022_04_20 select * from stock.order_book where TradingDay = '2022-04-20'\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.342552 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: INSERT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON _temporary_and_external_tables.`_tmp_f3c2b45f-b805-4e73-8a5b-8bf9e5c44e58`\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.342916 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343211 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343749 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343921 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343979 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344100 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> InterpreterSelectQuery: Complete -> Complete\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344250 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Key condition: (column 0 in [19102, 19102])\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344489 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): MinMax index condition: (column 0 in [19102, 19102])\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344630 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1107_1137_2_1138 (2506 marks)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344623 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_962_1106_3_1138 (11817 marks)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344653 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344655 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 2097\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344671 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 11817\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344684 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 28 steps\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344667 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2506\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344695 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344793 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Selected 2/203 parts by partition key, 2 parts by primary key, 12224/14321 marks by primary key, 12224 marks to read from 2 ranges\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.345003 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Reading approx. 99771210 rows with 24 streams\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.347756 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> Connection (10.20.131.97:9000): Connecting. Database: (not specified). User: default\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.348851 [ 64248 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.128.197:9000): Sent data for 2 scalars, total 2 rows in 3.9413e-05 sec., 49820 rows/sec., 68.00 B (1.60 MiB/sec.), compressed 0.4594594594594595 times to 148.00 B (3.48 MiB/sec.)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.349023 [ 64248 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.128.197:9000): Sent data for 1 external tables, total 0 rows in 0.000104251 sec., 0 rows/sec., 384.00 B (3.49 MiB/sec.), compressed 1.1671732522796352 times to 329.00 B (2.99 MiB/sec.)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.349157 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> Connection (10.20.131.97:9000): Connected to ClickHouse server version 22.3.2.\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.352876 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> executeQuery: (from 10.20.131.199:56640, initial_query_id: 6d125a8a-3b1d-4277-a039-3ff9ab0157d5) SELECT `order_book_local`.`TradingDay`, `order_book_local`.`Channel`, `order_book_local`.`ID`, `order_book_local`.`OrderID`, `order_book_local`.`ExchTimeOffsetUs`, `order_book_local`.`Symbol`, `order_book_local`.`Volume`, `order_book_local`.`Price`, `order_book_local`.`OrderKind`, `order_book_local`.`FunctionCode`, `order_book_local`.`TradeFlag`, `order_book_local`.`BidOrderID`, `order_book_local`.`AskOrderID`, `order_book_local`.`Type`, `order_book_local`.`LocalTimeStamp` FROM `stock`.`order_book_local` WHERE `TradingDay` = '2022-04-20'\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.354820 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.131.97:9000): Sent data for 2 scalars, total 2 rows in 2.5096e-05 sec., 78137 rows/sec., 68.00 B (2.53 MiB/sec.), compressed 0.4594594594594595 times to 148.00 B (5.49 MiB/sec.)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.354942 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.131.97:9000): Sent data for 1 external tables, total 0 rows in 6.4903e-05 sec., 0 rows/sec., 384.00 B (5.61 MiB/sec.), compressed 1.1671732522796352 times to 329.00 B (4.81 MiB/sec.)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.360077 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> executeQuery: (from 10.20.131.199:40512, initial_query_id: 6d125a8a-3b1d-4277-a039-3ff9ab0157d5) SELECT `order_book_local`.`TradingDay`, `order_book_local`.`Channel`, `order_book_local`.`ID`, `order_book_local`.`OrderID`, `order_book_local`.`ExchTimeOffsetUs`, `order_book_local`.`Symbol`, `order_book_local`.`Volume`, `order_book_local`.`Price`, `order_book_local`.`OrderKind`, `order_book_local`.`FunctionCode`, `order_book_local`.`TradeFlag`, `order_book_local`.`BidOrderID`, `order_book_local`.`AskOrderID`, `order_book_local`.`Type`, `order_book_local`.`LocalTimeStamp` FROM `stock`.`order_book_local` WHERE `TradingDay` = '2022-04-20'\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354036 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354209 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354279 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354476 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Key condition: (column 0 in [19102, 19102])\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354768 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): MinMax index condition: (column 0 in [19102, 19102])\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354985 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1122_1127_1_1138 (493 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354993 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1084_1115_2_1138 (2616 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355021 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354993 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1128_1133_1_1138 (492 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355038 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1055_1083_2_1138 (2370 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355052 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 493\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354995 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1134_1137_1_1138 (301 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355078 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355100 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355107 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2370\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355120 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 301\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355125 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355141 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355000 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_849_1021_3_1138 (14103 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355030 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354993 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1116_1121_1_1138 (492 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355058 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355221 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355079 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355234 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354996 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1022_1054_2_1138 (2697 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355185 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 11324\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355284 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355297 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 14103\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355301 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2697\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355209 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355311 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355308 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 26 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355211 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2616\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355317 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355332 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355329 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355516 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Selected 8/215 parts by partition key, 8 parts by primary key, 12232/23556 marks by primary key, 12232 marks to read from 8 ranges\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355933 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Reading approx. 99771761 rows with 24 streams\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.363439 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.364077 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.364290 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.364894 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Key condition: (column 0 in [19102, 19102])\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.365509 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): MinMax index condition: (column 0 in [19102, 19102])\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366027 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1025_1058_2_1138 (2778 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366159 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366123 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1126_1131_1_1138 (492 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366182 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1059_1087_2_1138 (2369 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366216 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_848_1024_3_1138 (14383 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366282 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366226 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2778\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366185 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1120_1125_1_1138 (492 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366365 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366234 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366401 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366445 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366232 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1132_1137_1_1138 (464 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366481 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366472 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366057 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1088_1119_2_1138 (2616 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366310 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 11358\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366632 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366324 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2369\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366691 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2616\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366712 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366744 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366535 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366650 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 14383\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366527 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367013 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 26 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367072 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 464\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367114 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367878 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Selected 7/198 parts by partition key, 7 parts by primary key, 12229/23587 marks by primary key, 12229 marks to read from 7 ranges\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.369109 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Reading approx. 99773104 rows with 24 streams\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.873240 [ 9201 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 1.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:51.458831 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 2.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:52.044369 [ 44900 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 3.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:52.628235 [ 9201 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 4.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:53.217460 [ 2274 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 5.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:53.835947 [ 9201 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 6.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:54.467806 [ 44883 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 7.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:55.080716 [ 2274 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 8.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:55.727570 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 9.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:56.367678 [ 9181 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 10.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:57.003147 [ 9166 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 11.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:57.610519 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 12.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:58.239731 [ 9166 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 13.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:59.228963 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 14.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:00.932248 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 15.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:02.895774 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 16.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:04.811238 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 17.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:06.595207 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 18.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:08.389497 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 19.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:10.052112 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 20.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:11.793655 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 21.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:13.650309 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 22.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:15.414734 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 23.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:17.309921 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 24.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:19.041601 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 25.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:20.814243 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 26.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:22.551550 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 27.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:24.539744 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 28.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:26.432611 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 29.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:28.284181 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 30.01 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:30.169253 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 31.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:32.018803 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 32.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:33.605974 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 33.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:35.421458 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 34.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:37.094259 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 35.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:38.976578 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 36.01 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:40.617896 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 37.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:42.465243 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 38.00 GiB.\r\n[ch-shard-3-rep-2] 2022.04.30 18:42:42.514774 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Information> executeQuery: Read 99773104 rows, 8.76 GiB in 52.154523129 sec., 1913028 rows/sec., 172.05 MiB/sec.\r\n[ch-shard-3-rep-2] 2022.04.30 18:42:42.528780 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> MemoryTracker: Peak memory usage (for query): 178.93 MiB.\r\n[ch-shard-1-rep-1] 2022.04.30 18:42:43.436844 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Information> executeQuery: Read 99771761 rows, 8.68 GiB in 53.083883545 sec., 1879511 rows/sec., 167.47 MiB/sec.\r\n[ch-shard-1-rep-1] 2022.04.30 18:42:43.443990 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> MemoryTracker: Peak memory usage (for query): 178.63 MiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:43.680592 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Information> executeQuery: Read 299316075 rows, 27.01 GiB in 53.338099769 sec., 5611674 rows/sec., 518.56 MiB/sec.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:43.690263 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Peak memory usage (for query): 38.41 GiB.\r\nOk.\r\n\r\n0 rows in set. Elapsed: 53.350 sec. Processed 299.32 million rows, 29.00 GB (5.61 million rows/s., 543.63 MB/s.)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36823/comments",
    "author": "variantf",
    "comments": [
      {
        "user": "variantf",
        "created_at": "2022-04-30T11:04:52Z",
        "body": "does the size of temporary table also counted for the query memory usage?"
      },
      {
        "user": "genzgd",
        "created_at": "2022-04-30T22:45:19Z",
        "body": "Yes.  Memory tracking operates by having a MemoryTracker object associated with the current query and all allocations and deallocations (C++ `new` and `delete`) are proxied through that MemoryTracker object.  That includes allocations for temporary tables which are of course entirely in memory.  Note that many aggregate functions also generate temporary tables to calculate results, which are similarly tracked.\r\n\r\nIt looks like you are putting 299 million rows of data in your temporary table (which are normally uncompressed).\r\n\r\nYou should consider just a regular MergeTree table for something like this.  In several ways MergeTree tables are superior to Memory tables for many operations (compression being one of them)."
      },
      {
        "user": "variantf",
        "created_at": "2022-05-06T11:00:51Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 36709,
    "title": "Use named connection in  remote(...) ",
    "created_at": "2022-04-27T14:02:08Z",
    "closed_at": "2022-04-27T14:40:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36709",
    "body": "Can we use named connection in  remote(...) ?\r\n\r\nI try:\r\n`SELECT count() FROM remote(mxch, db='mx_master', table='health_watch');\r\n`\r\nbut get:\r\n\r\n`Code: 36. DB::Exception: Unexpected key-value argument.Got: db, but expected: sharding_key. (BAD_ARGUMENTS) (version 22.3.3.44 (official build))`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36709/comments",
    "author": "oleg-savko",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2022-04-27T14:22:31Z",
        "body": "Please try `database` instead of `db`"
      },
      {
        "user": "oleg-savko",
        "created_at": "2022-04-27T14:35:04Z",
        "body": "\r\n\r\n\r\n> Please try `database` instead of `db`\r\n\r\nthanks thats work! It whould be grate if that will be in docs)"
      }
    ]
  },
  {
    "number": 36527,
    "title": "intersect in clickhouse",
    "created_at": "2022-04-22T03:43:40Z",
    "closed_at": "2022-04-22T03:55:47Z",
    "labels": [
      "question",
      "obsolete-version",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36527",
    "body": "Does clickhouse support interest keywords?\r\n\r\nWhen I use intersect in clickhouse, there is a syntax error, it seems that clickhouse can only support union all?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36527/comments",
    "author": "yst001",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2022-04-22T03:49:49Z",
        "body": "`interest  -> intersect` ?"
      },
      {
        "user": "yst001",
        "created_at": "2022-04-22T03:51:47Z",
        "body": "> `interest -> intersect` ?\r\n\r\nSorry, it's INTERSECT, I made a typo in the question"
      },
      {
        "user": "yst001",
        "created_at": "2022-04-22T03:53:12Z",
        "body": "In the process of using INTERSECT, there is a syntax error, the click house version is 21.4.6.55"
      },
      {
        "user": "amosbird",
        "created_at": "2022-04-22T03:55:47Z",
        "body": "`INTERSECT` is available in newer versions."
      },
      {
        "user": "yst001",
        "created_at": "2022-04-22T04:23:17Z",
        "body": "> `INTERSECT` is available in newer versions.\r\n\r\nCan you recommend me a stable version of clickhouse that supports intersect syntax? "
      },
      {
        "user": "amosbird",
        "created_at": "2022-04-22T06:15:10Z",
        "body": "The latest LTS version (22.3-LTS) is a good candidate to try."
      },
      {
        "user": "yst001",
        "created_at": "2022-04-22T06:29:42Z",
        "body": "> The latest LTS version (22.3-LTS) is a good candidate to try.\r\n\r\nthank you"
      }
    ]
  },
  {
    "number": 36105,
    "title": "What's the process in Memory Engine for update/delete mutation operation?",
    "created_at": "2022-04-10T13:54:58Z",
    "closed_at": "2022-04-10T16:44:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36105",
    "body": "I would use Memory engine for some special requirement. After the 'ALTER UPDATE/DELET' operation, can I query this table immediately?  Is the mutation operation in Memory engine is also an asyc action? How can I check whether the \"UPDATE/DELETE\" action is done for Memory Engine/Table?  Thanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36105/comments",
    "author": "taotaizhu-pw",
    "comments": [
      {
        "user": "ucasfl",
        "created_at": "2022-04-10T15:31:22Z",
        "body": "The execution of mutations for `Memory` engine is synchronous, you can query the table immediately after `alter` operation done."
      },
      {
        "user": "ucasfl",
        "created_at": "2022-04-10T15:33:59Z",
        "body": "But `Memory` engine is not for production usage, and the mutation process is single-threaded."
      },
      {
        "user": "taotaizhu-pw",
        "created_at": "2022-04-10T16:25:01Z",
        "body": "> But `Memory` engine is not for production usage, and the mutation process is single-threaded.\r\n\r\nNeed I  manually to set the \"max_threads = 1\"?  Or it would be set by system when execute mutation operation? Thanks"
      },
      {
        "user": "ucasfl",
        "created_at": "2022-04-10T16:26:39Z",
        "body": "Default by system."
      },
      {
        "user": "taotaizhu-pw",
        "created_at": "2022-04-10T16:44:08Z",
        "body": "> Default by system.\r\n\r\nThanks! :)"
      }
    ]
  },
  {
    "number": 35896,
    "title": "How many tables can be created in ClickHouse?",
    "created_at": "2022-04-04T03:48:40Z",
    "closed_at": "2022-04-04T04:39:51Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35896",
    "body": "VERSION: 21.3\r\nQUESTION: How many tables can be created in ClickHouse? For the whole, or one database\r\n\r\nI have a demand to build one table for one goods every, and the number of goods may have tens of thousands to hundreds of millions, so I need to know the relevant situation.\r\n\r\nPlease give me some advice. Fuzzy is OK also. Only give the order of magnitude is OK also.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35896/comments",
    "author": "LGDHuaOPER",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-04-04T04:39:40Z",
        "body": "You should better create single table and put all goods there. Use ORDER BY key for fast queries on goods.\r\n\r\nPS. Answering your question:\r\n\r\nYou can have around 10 000 ReplicatedMergeTree tables, around 100 000 non replicated MergeTree tables and around \r\n1 000 000 StripeLog tables per server."
      },
      {
        "user": "LGDHuaOPER",
        "created_at": "2022-04-04T04:48:45Z",
        "body": "> You should better create single table and put all goods there. Use ORDER BY key for fast queries on goods.\r\n> \r\n> PS. Answering your question:\r\n> \r\n> You can have around 10 000 ReplicatedMergeTree tables, around 100 000 non replicated MergeTree tables and around 1 000 000 StripeLog tables per server.\r\n\r\nThank you very much for your reply! Let me ask one more question. Whether each server refers to the limitations of the program? Are there any restrictions on the same database?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-04-04T05:01:30Z",
        "body": "ClickHouse has no limits on the number of tables.\r\n\r\nToo large number of tables will result in bad performance, long startup time, high memory consumption, high network traffic and CPU usage.\r\n\r\nThe numbers are from my practice."
      }
    ]
  },
  {
    "number": 35829,
    "title": "Is there any transaction isolation in clickhouse?",
    "created_at": "2022-04-01T07:43:48Z",
    "closed_at": "2022-04-01T20:41:50Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35829",
    "body": "for example, there is a merge table with 100 columns.\r\n\r\nwhen client A insert some rows, and client B query same row at same time, does client B would get only 50 columns avaiable? \r\n\r\nduring the mutation (delete or update data) process, does client would get outdated columns with uptodate columns?  ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35829/comments",
    "author": "ymmihw",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2022-04-01T08:28:52Z",
        "body": "> when client A insert some rows, and client B query same row at same time, does client B would get only 50 columns avaiable?\r\n\r\nNo.\r\n\r\n> during the mutation (delete or update data) process, does client would get outdated columns with uptodate columns?\r\n\r\nYes, it's possible to see some updated and not updated data yet."
      },
      {
        "user": "ymmihw",
        "created_at": "2022-04-02T12:38:52Z",
        "body": "> > during the mutation (delete or update data) process, does client would get outdated columns with uptodate columns?\r\n> \r\n> Yes, it's possible to see some updated and not updated data yet.\r\n\r\nThanks for your reply. \r\n\r\nWhat about the background merge process? Is it possible to see some updated and not updated data yet in ONE row?"
      },
      {
        "user": "UnamedRus",
        "created_at": "2022-04-02T19:34:22Z",
        "body": "> Is it possible to see some updated and not updated data yet in ONE row?\r\n\r\nNo, part being added to table only when it will be written completely."
      },
      {
        "user": "ymmihw",
        "created_at": "2022-04-03T03:15:04Z",
        "body": "> > Is it possible to see some updated and not updated data yet in ONE row?\r\n> \r\n> No, part being added to table only when it will be written completely.\r\n\r\n> > during the mutation (delete or update data) process, does client would get outdated columns with uptodate columns?\r\n> \r\n> Yes, it's possible to see some updated and not updated data yet.\r\n\r\nSo the 'updated and not updated yet data' is in different rows, not in one row. Am I right?\r\n\r\nThanks!!\r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 35605,
    "title": "Details about how distribute_group_by_no_merge works",
    "created_at": "2022-03-25T03:10:24Z",
    "closed_at": "2022-03-26T14:59:53Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35605",
    "body": "I'm wondering how distribute_group_by_no_merge = 1 works. \r\n\r\nI'm using distribute_group_by_no_merge=1 to optimize the following query\r\n\r\noriginal query: \r\n\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\nFROM (\r\n  SELECT client_id\r\n  , Date(ext_ingest_time) AS ext_ingest_date\r\n  FROM test\r\n  WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nAnd I wrote two queries using distributed_group_by_no_merge = 1 as optimized versions\r\n\r\nA:\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\nFROM (\r\n    SELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\n    FROM (\r\n      SELECT client_id\r\n      , Date(ext_ingest_time) AS ext_ingest_date\r\n      FROM test\r\n      WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\n    )\r\n    GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nB: \r\n```\r\nWITH Date(now()) AS in_endDate\r\n    SELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\n    FROM (\r\n        SELECT\r\n            COUNT(DISTINCT client_id) AS tmp\r\n            , Date(ext_ingest_time) AS ext_ingest_date\r\n            FROM test_table\r\n        WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n        AND ext_ingest_time <= in_endDate\r\n        GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nAnd I found the distributed_group_by_no_merge = 1 only works for B.  It seems because I imposed distributed_group_by_no_merge = 1 to a subquery in query A so it doesn't work. \r\n\r\nBut I need professional interpretation about the difference between A and B, as well as how distributed_group_by_no_merge = 1  influences these two queries.\r\n\r\nBesides, I have a few more questions:\r\n1. I know distributed_group_by_no_merge = 1  can be used to optimize `count distinct`. Does it also work for other aggregations like Min, Max, Sum, and Count?\r\n2. Can we use distributed_group_by_no_merge = 1  along with window functions?\r\n\r\nWait for your response! Thanks a lot!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35605/comments",
    "author": "catwang01",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-03-25T18:22:17Z",
        "body": "This is expected. Because of `( )` `select_at_initator ( select distributed ) `\r\n\r\nOnly part of the query inside `( .... )` is executed at shards.\r\n\r\n\r\n\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\nFROM (\r\n    SELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\n    FROM (                                                          --- executed on initator\r\n      SELECT client_id                                              --- executed on shards\r\n      , Date(ext_ingest_time) AS ext_ingest_date\r\n      FROM test\r\n      WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\n    )\r\n    GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nby ` SELECT client_id`  you are fetching all data to initator, after that `distributed_group_by_no_merge` loses sense because all data at the iniator."
      },
      {
        "user": "den-crane",
        "created_at": "2022-03-25T18:27:01Z",
        "body": ">I know distributed_group_by_no_merge = 1 can be used to optimize count distinct.\r\n> Does it also work for other aggregations like Min, Max, Sum, and Count?\r\n\r\nyes it works with all agg.functions, also check \r\n\r\n```\r\n--optimize_distributed_group_by_sharding_key arg                     Optimize GROUP BY sharding_key queries (by avoiding costly aggregation on the initiator server).\r\n```\r\nit's the automatic mode for `distributed_group_by_no_merge`\r\n\r\n------\r\n\r\n>Can we use distributed_group_by_no_merge = 1 along with window functions?\r\n\r\nNo, window functions work at the query initator now. Use `optimize_distributed_group_by_sharding_key`."
      },
      {
        "user": "catwang01",
        "created_at": "2022-03-27T13:03:40Z",
        "body": "Thanks @den-crane ! That helps a lot!"
      }
    ]
  },
  {
    "number": 35444,
    "title": "data_compressed_bytes and data_uncompressed_bytes in system.columns table are zero",
    "created_at": "2022-03-20T14:23:37Z",
    "closed_at": "2022-03-20T14:45:30Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35444",
    "body": "Clickhouse version: 22.3.2.1\r\n\r\nI create a MergeTree table and add some data to it, then I want to see how much space is used by its columns querying `system.columns` table, but it returns 0 for both `data_compressed_bytes` and `data_uncompressed_bytes`. I believe it worked a few months ago.\r\n\r\n```sql\r\ncreate table t (number Int32) engine = MergeTree() order by number as\r\nselect * from numbers(100000);\r\n\r\nselect name, type, data_compressed_bytes, data_uncompressed_bytes, compression_codec\r\nfrom system.columns where table = 't';\r\n```\r\nI get the following results:\r\n| name | type | data\\_compressed\\_bytes | data\\_uncompressed\\_bytes | compression\\_codec |\r\n| :--- | :--- | :--- | :--- | :--- |\r\n| number | Int32 | 0 | 0 |  |",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35444/comments",
    "author": "stas-sl",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-03-20T14:45:30Z",
        "body": "It's expected/intended behaviour for compact parts.\r\n\r\n```\r\nselect table, name, part_type from system.parts where table = 't';\r\n\u250c\u2500table\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500part_type\u2500\u2510\r\n\u2502 t     \u2502 all_1_1_0 \u2502 Compact   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ncreate table t (number Int32) engine = MergeTree() order by number settings min_bytes_for_wide_part=0 as\r\nselect * from numbers(100000);\r\n\r\nselect table, name, part_type from system.parts where table = 't';\r\n\u250c\u2500table\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500part_type\u2500\u2510\r\n\u2502 t     \u2502 all_1_1_0 \u2502 Wide      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect name, type, data_compressed_bytes, data_uncompressed_bytes, compression_codec\r\nfrom system.columns where table = 't';\r\n\u250c\u2500name\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500data_compressed_bytes\u2500\u252c\u2500data_uncompressed_bytes\u2500\u252c\u2500compression_codec\u2500\u2510\r\n\u2502 number \u2502 Int32 \u2502                401725 \u2502                  400000 \u2502                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      },
      {
        "user": "stas-sl",
        "created_at": "2022-03-20T14:56:32Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 35402,
    "title": "Error create database with ENGINE = PostgreSQL",
    "created_at": "2022-03-18T10:01:37Z",
    "closed_at": "2022-03-18T10:12:29Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35402",
    "body": "hi!\r\non clickhouse-server version  21.4.6.55 i am successfully create database with engine postgresql\r\nCREATE DATABASE b2b ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\nbut on clickhouse-server version 22.2.2.1 i get an error:\r\n```\r\n<Error> executeQuery: Code: 170. DB::Exception: Bad get: has UInt64, requested String. (BAD_GET) (version 22.2.2.1) (from 12\r\n7.0.0.1:35954) (in query: CREATE DATABASE b2b2 ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xaebed1a in /usr/bin/clickhouse\r\n1. DB::Exception::Exception<std::__1::basic_string_view<char, std::__1::char_traits<char> >, DB::Field::Types::Which const&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allo\r\ncator<char> > const&, std::__1::basic_string_view<char, std::__1::char_traits<char> >&&, DB::Field::Types::Which const&) @ 0xafdf5a0 in /usr/bin/clickhouse\r\n2. auto& DB::Field::safeGet<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >() @ 0xba85663 in /usr/bin/clickhouse\r\n3. ? @ 0x14bcd930 in /usr/bin/clickhouse\r\n4. DB::DatabaseFactory::getImpl(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>) @ 0x14bc\r\nb7eb in /usr/bin/clickhouse\r\n5. DB::DatabaseFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>) @ 0x14bc8f47\r\n in /usr/bin/clickhouse\r\n6. DB::InterpreterCreateQuery::createDatabase(DB::ASTCreateQuery&) @ 0x14bb0237 in /usr/bin/clickhouse\r\n7. DB::InterpreterCreateQuery::execute() @ 0x14bc5e1b in /usr/bin/clickhouse\r\n8. ? @ 0x14ee8a79 in /usr/bin/clickhouse\r\n9. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x14ee65f5 in\r\n /usr/bin/clickhouse\r\n10. DB::TCPHandler::runImpl() @ 0x159ef43a in /usr/bin/clickhouse\r\n11. DB::TCPHandler::run() @ 0x15a03419 in /usr/bin/clickhouse\r\n12. Poco::Net::TCPServerConnection::start() @ 0x18667a0f in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerDispatcher::run() @ 0x18669e61 in /usr/bin/clickhouse\r\n14. Poco::PooledThread::run() @ 0x1881a549 in /usr/bin/clickhouse\r\n15. Poco::ThreadImpl::runnableEntry(void*) @ 0x18817c40 in /usr/bin/clickhouse\r\n16. start_thread @ 0x817a in /usr/lib64/libpthread-2.28.so\r\n17. __clone @ 0xfcdc3 in /usr/lib64/libc-2.28.so\r\n```\r\nbut if i am manually create database and create tables with engine postgres, everything is fine and I don't get any errors\r\nfor example create table like this work fine:\r\n```\r\nCREATE TABLE b2b.intouch_district(    id Int32,    name String,    city_id Int32)ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'intouch_district', 'login', 'password');\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35402/comments",
    "author": "TipaOpa",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2022-03-18T10:10:33Z",
        "body": "> PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\n\r\n1 is the 6th parameter."
      },
      {
        "user": "kssenii",
        "created_at": "2022-03-18T10:12:03Z",
        "body": "> PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\n1 is the 6th parameter.\r\n\r\nexample:\r\n```\r\nCREATE DATABASE test_database ENGINE = PostgreSQL('postgres1:5432', 'test_database', 'postgres', 'mysecretpassword', '', 1)\")\r\n```"
      },
      {
        "user": "TipaOpa",
        "created_at": "2022-03-18T13:32:36Z",
        "body": "yes it works, thanks\r\nthe syntax has changed in the new version and when I updated the clickhouse-server it gave an error"
      }
    ]
  },
  {
    "number": 35342,
    "title": "AggregateFunction is not backward compatible",
    "created_at": "2022-03-16T18:32:44Z",
    "closed_at": "2022-03-16T20:46:45Z",
    "labels": [
      "question",
      "backward compatibility",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35342",
    "body": "**Describe the issue**\r\nWe were upgrading our Clickhouse cluster from **21.8.3.44** to **22.1.3.7** and found a lot of our queries are failing due to an error explained below \r\n\r\n**How to reproduce**\r\nCreate table 1\r\n```\r\nCREATE TABLE test.table_1 on cluster '{cluster}'\r\n(\r\n  `date` Date,\r\n  `uniques` AggregateFunction(uniqCombined64(17), Nullable(String))\r\n)\r\nENGINE = MergeTree()\r\nORDER BY date\r\n```\r\n\r\nCreate table 2\r\n```\r\nCREATE TABLE test.table_2 on cluster '{cluster}'\r\n(\r\n  `date` Date,\r\n  `id`  Nullable(String)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY date\r\n```\r\n\r\nInsert data in table 2\r\n```\r\nINSERT INTO test.table_2 (*) VALUES ('2022-04-01', null), ('2022-04-01', '1'), ('2022-04-01', '2'), ('2022-04-01', '3'), ('2022-04-01', '3'), ('2022-04-01', '4'), ('2022-04-01', '5');\r\n```\r\n\r\nInsert data into table 1 using table 1 data\r\n```\r\nINSERT INTO test.table_1 \r\nSELECT\r\n  date,\r\n  uniqCombined64State(17)(id)\r\nFROM test.table_2\r\nGROUP BY date\r\n```\r\n\r\nRun aggregate query on table 1\r\n```\r\nSELECT\r\n  date,\r\n  coalesce(uniqCombined64Merge(uniques), 0) AS uniques\r\nFROM test.table_1\r\nGROUP BY date \r\n```\r\n\r\nVersion **21.8.3.44** gives result\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500uniques\u2500\u2510\r\n\u2502 2022-04-01 \u2502       5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nVersion **22.1.3.7** throws error\r\n```\r\nCode: 43. DB::Exception: Received from localhost:9000. DB::Exception: Illegal type AggregateFunction(uniqCombined64(17), Nullable(String)) of argument for aggregate function uniqCombined64Merge, expected AggregateFunction(uniqCombined64, Nullable(String)) or equivalent type. (ILLEGAL_TYPE_OF_ARGUMENT)\r\n```\r\n\r\n\r\n**Additional context**\r\nThis is blocking us to upgrade to newer version.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35342/comments",
    "author": "piyushsriv",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-03-16T18:56:30Z",
        "body": ">AggregateFunction(uniqCombined64(17),\r\n> uniqCombined64Merge(uniques)\r\n\r\nThis is a mis-usage which leads to incorrect results with any version.\r\nYou should use `uniqCombined64Merge(17)(uniques)` with any version of CH."
      },
      {
        "user": "piyushsriv",
        "created_at": "2022-03-16T20:46:45Z",
        "body": "Thanks for clarifying this."
      }
    ]
  },
  {
    "number": 35246,
    "title": "How to compile the clickhouse-v20.11.4.13-stable with gcc and libstdc++ instead of libc++",
    "created_at": "2022-03-13T13:44:29Z",
    "closed_at": "2022-03-13T18:30:39Z",
    "labels": [
      "question",
      "not planned",
      "build",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35246",
    "body": null,
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35246/comments",
    "author": "starrysky9959",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-03-13T18:30:39Z",
        "body": "It is not possible."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-03-13T18:36:35Z",
        "body": "The main asset of ClickHouse is our set of test suites and continuous integration system.\r\nWith our tests, including randomized tests, we find and fix bugs in most of third-party open-source libraries.\r\n\r\nTo ensure that ClickHouse is stable and secure, the only way is to build it with exactly the same versions of libraries with exactly the same patches as we do.\r\n\r\nIf you don't know, it's difficult to imagine how many bugs (race conditions, memory safety issues) exist in C++ libraries."
      },
      {
        "user": "starrysky9959",
        "created_at": "2022-03-14T02:17:17Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 34987,
    "title": "Confuse about the difference between background_schedule_pool_size  and background_fetches_pool_size ",
    "created_at": "2022-03-02T10:10:48Z",
    "closed_at": "2022-03-03T14:25:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34987",
    "body": "I think ReplicatedMerge table has two type background tasks , one is merge and another is fetch parts from another replica. \r\nIn my opinion, background_fetches_pool_size is for fetch and background_pool_size  is for merge, so i confuse why the document for clickhouse say background_schedule_pool_size is about background task for replicated task. Is there another backgraound task for replicated table ? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34987/comments",
    "author": "Yanbuc",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2022-03-02T14:00:44Z",
        "body": "There are fetches and merges which are background tasks. Recently there were a single pool for both type of tasks, but now we use different pools for better tuning."
      },
      {
        "user": "Yanbuc",
        "created_at": "2022-03-03T12:44:54Z",
        "body": "> There are fetches and merges which are background tasks. Recently there were a single pool for both type of tasks, but now we use different pools for better tuning.\r\n\r\nI see. Thank you ."
      }
    ]
  },
  {
    "number": 34861,
    "title": "Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build))",
    "created_at": "2022-02-24T07:27:33Z",
    "closed_at": "2022-02-24T14:26:42Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34861",
    "body": "\r\nCode: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build))\r\n\r\nhello,\r\nwe build a clickhouse cluster in three machine, after insert data to distribution table,we use cmd 'optimize table dis_alerts on cluster default_cluster FINAL' to merge repeated record,but errors below:\r\n`\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 9.0.16.11 \u2502 9000 \u2502     48 \u2502 Code: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build)) \u2502                   2 \u2502                0 \u2502\r\n\u2502 9.0.16.17 \u2502 9000 \u2502     48 \u2502 Code: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build)) \u2502                   1 \u2502                0 \u2502\r\n\u2502 9.0.16.4  \u2502 9000 \u2502     48 \u2502 Code: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build)) \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n`\r\nfollowing:\r\n\r\n`\r\ndisk table:\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE dis_test.test_alerts\r\n(\r\n    `tenant_id` UInt32,\r\n    `alert_id` String,\r\n    `timestamp` DateTime CODEC(Delta(4), LZ4),\r\n    `alert_data` String,\r\n    `acked` UInt8 DEFAULT 0,\r\n    `ack_time` DateTime DEFAULT toDateTime(0),\r\n    `ack_user` LowCardinality(String) DEFAULT ''\r\n)\r\nENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{layer}-{shard}/dis_test/test_alerts', '{replica}', ack_time)\r\nPARTITION BY tenant_id % 10\r\nORDER BY (tenant_id, timestamp, alert_id)\r\nSETTINGS index_granularity = 8192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nlogic table:\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE dis_test.dis_alerts\r\n(\r\n    `tenant_id` UInt32,\r\n    `alert_id` String,\r\n    `timestamp` DateTime CODEC(Delta(4), LZ4),\r\n    `alert_data` String,\r\n    `acked` UInt8 DEFAULT 0,\r\n    `ack_time` DateTime DEFAULT toDateTime(0),\r\n    `ack_user` LowCardinality(String) DEFAULT ''\r\n)\r\nENGINE = Distributed('default_cluster', 'dis_test', 'test_alerts', tenant_id) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34861/comments",
    "author": "sbbug",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-02-24T14:26:39Z",
        "body": "You should use `optimize table dis_test.test_alerts on cluster default_cluster FINAL`\r\n\r\nOnly MergeTree family supports `optimize`."
      },
      {
        "user": "sbbug",
        "created_at": "2022-02-25T09:24:23Z",
        "body": "Nice!\r\nIt's ok and thank you!"
      }
    ]
  },
  {
    "number": 34474,
    "title": "Breaking a source table row into multiple rows using materialized views",
    "created_at": "2022-02-09T19:36:38Z",
    "closed_at": "2022-02-10T04:09:51Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34474",
    "body": "Is it possible to create two rows in a MV from a single row in a source table?\r\n\r\nExample:\r\nInsert in source table:\r\n|when | col_1 | col_2|\r\n|-----|-------|------|\r\n|datetime | val_1 | val_2|\r\n\r\nI would like to end up with something like this in the Materialized view:\r\n| when | col_x |\r\n|------|-------|\r\n|datetime | val_1|\r\n|datetime | val_2|\r\n\r\nIs it possible to do that? Thanks!!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34474/comments",
    "author": "a-dot",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-02-09T21:05:19Z",
        "body": "Yes it's possible `arrayJoin( [ col_1 , col_2 ] ) `\r\n\r\n```sql\r\nSELECT arrayJoin([col1, col2]) AS colx\r\nFROM\r\n(\r\n    SELECT\r\n        1 AS col1,\r\n        2 AS col2\r\n)\r\n\u250c\u2500colx\u2500\u2510\r\n\u2502    1 \u2502\r\n\u2502    2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n\r\ncreate materialized view ...\r\n...\r\nselect arrayJoin( [ col_1 , col_2 ] ) as col_x, \r\nfrom ...\r\n```"
      },
      {
        "user": "a-dot",
        "created_at": "2022-02-10T01:37:35Z",
        "body": "Thanks den-crane, as always!\r\nOne follow up question if I may... the array approach is stumping me and I simply can't figure out how to do this. In my first post I gave you a simple example but what I'm trying to do is slightly more complicated.. I want to unfold col1 and col2 but I want to assign val_3 as such:\r\n\r\nBefore:\r\n|when|col_1|col_2|col_3|\r\n|------|-----|-----|------|\r\n| datetime | val_1 | val_2 | val_3|\r\n\r\nAfter:\r\n|when|col_x|col_y|col_z|\r\n|-----|-----|------|-----|\r\n|datetime| val_1|val_3|0|\r\n|datetime|val_2|0|val_3|\r\n\r\nHopefully you can still help! Thanks again!!"
      },
      {
        "user": "den-crane",
        "created_at": "2022-02-10T01:50:19Z",
        "body": "```sql\r\nSELECT col_x, col_y, col_z\r\nFROM\r\n(\r\n    SELECT\r\n        'val_1' AS col_1,\r\n        'val_2' AS col_2,\r\n        'val_3' col_3\r\n)\r\narray join [col_1, col_2] AS col_x, [col_3, '0'] as col_y, ['0', col_3] as col_z\r\n\u250c\u2500col_x\u2500\u252c\u2500col_y\u2500\u252c\u2500col_z\u2500\u2510\r\n\u2502 val_1 \u2502 val_3 \u2502 0     \u2502\r\n\u2502 val_2 \u2502 0     \u2502 val_3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n```sql\r\nSELECT\r\n    (arrayJoin(arrayZip([col_1, col_2], [col_3, '0'], ['0', col_3])) AS xx).1 AS col_x,\r\n    xx.2 AS col_y,\r\n    xx.3 AS col_z\r\nFROM\r\n(\r\n    SELECT\r\n        'val_1' AS col_1,\r\n        'val_2' AS col_2,\r\n        'val_3' AS col_3\r\n)\r\n\u250c\u2500col_x\u2500\u252c\u2500col_y\u2500\u252c\u2500col_z\u2500\u2510\r\n\u2502 val_1 \u2502 val_3 \u2502 0     \u2502\r\n\u2502 val_2 \u2502 0     \u2502 val_3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "a-dot",
        "created_at": "2022-02-10T01:58:13Z",
        "body": "thank you so much!"
      }
    ]
  },
  {
    "number": 34093,
    "title": "EXPLAIN SYNTAX doesn't report more than one column in GROUP BY",
    "created_at": "2022-01-28T12:21:32Z",
    "closed_at": "2022-01-28T13:07:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34093",
    "body": "For instance a query like this:\r\n\r\n```sql\r\nexplain syntax (SELECT sum(number) _number, count(), number, toDate(now()) date FROM numbers(10) GROUP BY number, date);\r\n\r\nEXPLAIN SYNTAX\r\nSELECT\r\n    sum(number) AS _number,\r\n    count(),\r\n    number,\r\n    toDate(now()) AS date\r\nFROM numbers(10)\r\nGROUP BY\r\n    number,\r\n    date\r\n\r\nQuery id: 985a47d6-644e-4821-8a6d-83de0925cfdd\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 SELECT                      \u2502\r\n\u2502     sum(number) AS _number, \u2502\r\n\u2502     count(),                \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     toDate(now()) AS date   \u2502\r\n\u2502 FROM numbers(10)            \u2502\r\n\u2502 GROUP BY number             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nshould report: `GROUP BY number, date` in the last line.\r\n\r\nTested on these versions with the same result: 20.7.2.30, 21.7.4.18, 21.9.5.16, 21.12.3.32, 22.1.2.2",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34093/comments",
    "author": "alrocar",
    "comments": [
      {
        "user": "CurtizJ",
        "created_at": "2022-01-28T12:34:33Z",
        "body": "`toDate(now())` is a constant and constants are eliminated from `GROUP BY` keys."
      },
      {
        "user": "alrocar",
        "created_at": "2022-01-28T13:07:36Z",
        "body": "Oh I see, a non constant date works:\r\n\r\n```sql\r\nexplain syntax (SELECT sum(number) _number, count(), number, toDate(rand()) date FROM numbers(10) GROUP BY number, date);\r\n\r\nEXPLAIN SYNTAX\r\nSELECT\r\n    sum(number) AS _number,\r\n    count(),\r\n    number,\r\n    toDate(rand()) AS date\r\nFROM numbers(10)\r\nGROUP BY\r\n    number,\r\n    date\r\n\r\nQuery id: 538725c3-0996-4018-a764-fac2c1e11933\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 SELECT                      \u2502\r\n\u2502     sum(number) AS _number, \u2502\r\n\u2502     count(),                \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     toDate(rand()) AS date  \u2502\r\n\u2502 FROM numbers(10)            \u2502\r\n\u2502 GROUP BY                    \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     date                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 34009,
    "title": "Do we consider add alignment for `ThreadStatus`",
    "created_at": "2022-01-26T11:01:34Z",
    "closed_at": "2022-01-26T12:42:18Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34009",
    "body": "Do we consider add alignment for class `ThreadStatus` like this to reduce false sharing of cpu cache between multiple threads ? \r\n``` cpp\r\nclass __attribute__((__aligned__(64))) ThreadStatus : public boost::noncopyable\r\n``` \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34009/comments",
    "author": "taiyang-li",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-01-26T11:46:55Z",
        "body": "It is already allocated on thread stack, so no issue with false sharing should exist, as thread stacks are already far away of each other."
      },
      {
        "user": "taiyang-li",
        "created_at": "2022-01-26T11:54:13Z",
        "body": "Thanks for your explaination. @alexey-milovidov "
      }
    ]
  },
  {
    "number": 33498,
    "title": "Unexpected behaviour of AggregatingMergeTree",
    "created_at": "2022-01-10T10:49:03Z",
    "closed_at": "2022-01-15T21:53:19Z",
    "labels": [
      "question",
      "unexpected behaviour",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/33498",
    "body": "**Describe what's wrong**\r\n\r\n> **AggregatingMergeTree** does not respect expected behaviour for fields of type `SimpleAggregateFunction(max, Array(Tuple(url String, status_code UInt16)))`. \r\n\r\n**Does it reproduce on recent release?**\r\n\r\n> Tested on Clickhouse 21.13.1.1 \r\n\r\n**How to reproduce**\r\n\r\n* ClickHouse server version: 21.13.1.1\r\n* Create a table \r\n> `CREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Tuple(\r\n        url String,\r\n        status_code UInt16\r\n    )))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id`\r\n\r\n* Inspect created table: `SHOW CREATE TABLE test_tuple_aggreg`\r\n> Result: `CREATE TABLE default.test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples.url` Array(String),\r\n    `tuples.status_code` Array(UInt16)\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id\r\nSETTINGS index_granularity = 8192`\r\n\r\nWe loose track of `SimpleAggregateFunction`.\r\n\r\n* Populate the table to see if correct aggregations are used\r\n>  * `INSERT INTO `test_tuple_aggreg` VALUES (1, '2022-01-02', ['url1'], [500])`\r\n>  * `INSERT INTO `test_tuple_aggreg` VALUES (1, '2022-01-03', ['url2'], [404])`\r\n>  * `INSERT INTO `test_tuple_aggreg` VALUES (1, '2022-01-04', ['url3'], [503])`\r\n\r\n* When we inspect the data we see all 3 rows\r\n> select * from test_tuple_aggreg \r\n   `\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1']   \u2502 [500]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1']   \u2502 [500]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-04 \u2502 ['url3']   \u2502 [503]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518`\r\n\r\n* When we optimise it we see that wrong aggregation is picked for `tuples` fields\r\n> select * from test_tuple_aggreg final\r\n   `\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1']   \u2502 [500]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518`\r\n\r\n**Expected behavior**\r\n\r\nPreserved schema with `SimpleAggregateFunction` and correct aggregation of rows according to created schema `groupUniqArrayArray(2)`.\r\n\r\nExpected aggregation result:\r\n`\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1', 'url2']   \u2502 [500, 404]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/33498/comments",
    "author": "gontarzpawel",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-01-10T16:05:21Z",
        "body": "There is 2 independent issues.\r\n\r\n>PARTITION BY session_date\r\n\r\nClickhouse merges rows only over a partition. ! You inserted data into different partitions  '2022-01-02' '2022-01-03' '2022-01-04' !\r\n\r\n------\r\n\r\nAlso there is an issue in CH\r\n\r\n```sql \r\nCREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Tuple(url String, status_code UInt16)))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id;\r\n\r\nshow create  table test_tuple_aggreg;\r\n\r\nCREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples.url` Array(String),    ----- SimpleAggregateFunction is lost\r\n    `tuples.status_code` Array(UInt16)   ---- SimpleAggregateFunction is lost\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id\r\nSETTINGS index_granularity = 8192\r\n\r\n\r\n\r\n\r\nset flatten_nested=0;\r\n\r\n\r\n\r\n\r\nCREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Tuple(url String, status_code UInt16)))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id;\r\n\r\nshow create  table test_tuple_aggreg;\r\n\r\nCREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Tuple(url String, status_code UInt16)))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id\r\nSETTINGS index_granularity = 8192;\r\n\r\n```\r\n\r\n-----\r\n\r\n\r\n```sql\r\nset flatten_nested=0;       ----<<<<<<<-----\r\n\r\nCREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Tuple(url String, status_code UInt16)))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY toYYYYMM(session_date) ----<<<<<<<-----\r\nORDER BY id\r\nSETTINGS index_granularity = 8192;\r\n\r\nINSERT INTO test_tuple_aggreg VALUES (1, '2022-01-02', [('url1', 500)]);\r\nINSERT INTO test_tuple_aggreg VALUES (1, '2022-01-03', [('url2', 404)]);\r\nINSERT INTO test_tuple_aggreg VALUES (1, '2022-01-04', [('url3', 503)]);\r\n\r\noptimize table test_tuple_aggreg final;\r\n\r\nselect * from test_tuple_aggreg;\r\n\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 [('url2',404),('url1',500)] \u2502  !!!!! session_date calculated by ANY because it's a measurement in this table definition  !!!!! \r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      },
      {
        "user": "gontarzpawel",
        "created_at": "2022-01-11T16:18:40Z",
        "body": "Hello @den-crane ,\r\nThank you for you reply. Indeed I used different dates, mainly because it was for tests and I run query with **final** ignoring partitioning.\r\n\r\nI see your workaround, thanks. \r\n\r\nI've also spotted weird behaviour while working with `Nested` fields. Example:\r\n```\r\nCREATE TABLE default.nested_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `nested` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Nested( <------------- nested field\r\n        url String,\r\n        status_code UInt16\r\n    )))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id\r\n\r\n--------- when trying to insert data into it\r\n\r\nINSERT INTO `test_nested_aggreg` VALUES (1, '2022-01-02', ['url1'], [500])\r\n\r\n------- I get\r\n\r\nCode: 62. DB::Exception: Cannot parse expression of type SimpleAggregateFunction(groupUniqArrayArray(2), Array(Nested(url String, status_code UInt16))) here: ['url1'], [500]): While executing ValuesBlockInputFormat: data for INSERT was parsed from query. (SYNTAX_ERROR)\r\n\r\n```\r\nIs it another issue or am I doing something wrong?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-01-11T18:27:53Z",
        "body": ">Is it another issue or am I doing something wrong?\r\n\r\nClickhouse creates 2 different structures depending of `flatten_nested` -- **columnS** with Arrays VS a **column** with Arrays of Tuples.\r\nSo you should insert accordingly.\r\n\r\ntwo columns `Array(String), Array(String)`\r\n`INSERT INTO test_nested_aggreg VALUES (1, '2022-01-02', ['url1'], [500])` \r\n\r\n one column `Array(Tuple(String,String))`\r\n`INSERT INTO test_nested_aggreg VALUES (1, '2022-01-04', [('url1', 500)])`"
      }
    ]
  },
  {
    "number": 33029,
    "title": "Why can not the \u201cmodify setting\u201d operation be synchronized to the other replica",
    "created_at": "2021-12-22T02:25:38Z",
    "closed_at": "2021-12-22T04:20:35Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/33029",
    "body": "If I modify setting in one replica, the other replica will not be modified synchronously.\r\n\r\n```\r\nvoid StorageReplicatedMergeTree::alter(\r\n    const AlterCommands & commands, const Context & query_context, TableLockHolder & table_lock_holder)\r\n{\r\n    assertNotReadonly();\r\n\r\n    auto table_id = getStorageID();\r\n\r\n    if (commands.isSettingsAlter()) \r\n    {\r\n        /// We don't replicate storage_settings_ptr ALTER. It's local operation.\r\n        /// Also we don't upgrade alter lock to table structure lock.\r\n        StorageInMemoryMetadata future_metadata = getInMemoryMetadata();\r\n        commands.apply(future_metadata, query_context);\r\n\r\n        merge_strategy_picker.refreshState();\r\n\r\n        changeSettings(future_metadata.settings_changes, table_lock_holder);\r\n\r\n        DatabaseCatalog::instance().getDatabase(table_id.database_name)->alterTable(query_context, table_id, future_metadata);\r\n        return;\r\n    }\r\n ...\r\n}\r\n```\r\n\r\nCould you tell me why not the \u201cmodify setting\u201d operation be synchronized to the other replica.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/33029/comments",
    "author": "zhanghuajieHIT",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-12-22T04:20:35Z",
        "body": "Not any setting, but only storage settings. \r\nOn the purpose. Storage can be different at replicas. \r\nOne replica can have all data locally at NVME, other replica can have tiered storage NVME -> HDD -> S3\r\n\r\n"
      },
      {
        "user": "zhanghuajieHIT",
        "created_at": "2021-12-22T04:47:37Z",
        "body": "supporting synchronization between replicas may be more convenient"
      },
      {
        "user": "den-crane",
        "created_at": "2021-12-22T04:51:47Z",
        "body": "> supporting synchronization between replicas may be more convenient\r\n\r\nAll settings are synchronized except  storage settings. \r\nDeliberately. Other users need different storage configurations for replicas."
      },
      {
        "user": "zhanghuajieHIT",
        "created_at": "2021-12-22T06:05:53Z",
        "body": "ok, thanks"
      }
    ]
  },
  {
    "number": 32813,
    "title": "Clickhouse Materialized Views select column with different date",
    "created_at": "2021-12-15T18:08:29Z",
    "closed_at": "2021-12-15T19:19:54Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/32813",
    "body": "Hi guys,\r\n\r\ni have an issue when select MV column with different date period, example:\r\n\r\nsource table\r\n```\r\ncreate table if not exists source(\r\n      `original_timestamp` DateTime64(3) DEFAULT parseDateTimeBestEffort('0001-1-1 23:00:00') CODEC(DoubleDelta, LZ4),\r\n      `event` LowCardinality(String) DEFAULT '',\r\n      `identity` String DEFAULT ''\r\n    ) \r\n    ENGINE = ReplacingMergeTree()\r\n    partition by toWeek(identity,original_timestamp)\r\n        order by (id)\r\n        SETTINGS index_granularity = 8192\r\n```\r\ntarget table\r\n```\r\ncreate table if not exists target(\r\n     day DateTime,\r\n     identity String,\r\n     login AggregateFunction(countIf, String, UInt8),\r\n     register AggregateFunction(countIf, String, UInt8)\r\n    ) \r\n    ENGINE = AggregatingMergeTree()\r\n    partition by toWeek(day)\r\n        order by (day,identity)\r\n        SETTINGS index_granularity = 8192\r\n```\r\nmv table\r\n```\r\nCREATE MATERIALIZED VIEW if not exists target_mv \r\n    to target as\r\n    select\r\n    toStartOfDay(original_timestamp) as day,\r\n    identity,\r\n    countIf(event, event='login') as login,\r\n    countIf(event, event='register') as register\r\n    from source\r\n    group by identity, day\r\n```\r\n\r\nquery:\r\nselect\r\ncountIfMerge(login) as totalLoginEvent, //login event in the last 3 days\r\ncountIfMerge(register) as totalRegisterEvent //register event in the last 7 days\r\nfrom target\r\ngroup by identity,day\r\n\r\nexample expected result: totalLoginEvent is 10 (in the last 3 days) and totalRegisterEvent is 40 event in the last 7 days\r\n\r\nplease help, thanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/32813/comments",
    "author": "Kev1ntan",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-12-15T19:19:54Z",
        "body": "you don't need AggregateFunction(countIf\r\n\r\n```sql\r\nCREATE TABLE IF NOT EXISTS source\r\n(\r\n    `original_timestamp` DateTime64(3) DEFAULT parseDateTimeBestEffort('0001-1-1 23:00:00') CODEC(DoubleDelta, LZ4),\r\n    `event` LowCardinality(String) DEFAULT '',\r\n    `identity` String DEFAULT ''\r\n)\r\nENGINE = ReplacingMergeTree\r\nPARTITION BY toWeek(original_timestamp)\r\nORDER BY identity\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS target\r\n(\r\n    `day` DateTime,\r\n    `identity` String,\r\n    `login` SimpleAggregateFunction(sum, UInt64),\r\n    `register` SimpleAggregateFunction(sum, UInt64)\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY toWeek(day)\r\nORDER BY (day, identity)\r\nSETTINGS index_granularity = 8192\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS target_mv TO target AS\r\nSELECT\r\n    toStartOfDay(original_timestamp) AS day,\r\n    identity,\r\n    countIf(event = 'login') AS login,\r\n    countIf(event = 'register') AS register\r\nFROM source\r\nGROUP BY\r\n    identity,\r\n    day\r\n\r\n\r\ninsert into source select now() - interval 2 day , 'login', 1 from  numbers(10);\r\ninsert into source select now() - interval 20 day , 'login', 1 from  numbers(100);\r\n\r\ninsert into source select now() - interval 6 day , 'register', 1 from  numbers(40);\r\ninsert into source select now() - interval 20 day , 'register', 1 from  numbers(100);\r\n\r\n\r\nSELECT\r\n    sumIf(login, day >= (today() - 3)) AS totalLoginEvent,\r\n    sumIf(register, day >= (today() - 7)) AS totalRegisterEvent\r\nFROM target\r\nWHERE day >= (today() - 7)\r\n\r\n\u250c\u2500totalLoginEvent\u2500\u252c\u2500totalRegisterEvent\u2500\u2510\r\n\u2502              10 \u2502                 40 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "Kev1ntan",
        "created_at": "2021-12-16T01:42:47Z",
        "body": "thank you"
      }
    ]
  },
  {
    "number": 31306,
    "title": "Is shard weight change applied dynamically?",
    "created_at": "2021-11-11T16:44:59Z",
    "closed_at": "2021-11-18T04:35:01Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/31306",
    "body": "Hi all!\r\nI have a question relating to the re-sharding of the cluster. We have expanded our cluster by adding one more shard. Since we want the new shard to handle more load in terms of writes, we adjusted our weights in the configs for each pertaining shard, e.g., \r\n`<weight>9</weight>`. Everything seems working fine; however, we wanted to know whether the restart is required for this to take into effect or is it picked up and applied dynamically? Please advise.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/31306/comments",
    "author": "malikas05",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-11-11T16:50:47Z",
        "body": "Restart is not required.\r\nIt's picked up and applied dynamically.\r\nYou can see it in `system.clusters`"
      },
      {
        "user": "malikas05",
        "created_at": "2021-11-11T17:00:01Z",
        "body": "@den-crane: Thanks for your response. Much appreciated!"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-11-18T04:35:12Z",
        "body": "TLDR: Yes."
      }
    ]
  },
  {
    "number": 30814,
    "title": "How to add versioning column for existing ReplicatedReplacingMergedTree-engine table",
    "created_at": "2021-10-28T15:29:49Z",
    "closed_at": "2021-11-18T04:36:13Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/30814",
    "body": "Hi guys,\r\n\r\nI created a table with ReplicatedReplacingMergedTree engine (no versioning column specifed).\r\n\r\nI would like to ask if there is a way to ALTER the table and add a versioning column? \r\n\r\nI have 500GB of data on the table already so it would be not easy to create a new table and insert everything again.\r\n\r\nMany thanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/30814/comments",
    "author": "tranvinhthuy",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-10-28T15:32:11Z",
        "body": "ReplacingMergedTree or ReplicatedReplacingMergedTree ?"
      },
      {
        "user": "tranvinhthuy",
        "created_at": "2021-10-28T16:00:15Z",
        "body": "> ReplacingMergedTree or ReplicatedReplacingMergedTree ?\r\n\r\nsorry I specified more, it's ReplicatedReplacingMergedTree "
      },
      {
        "user": "den-crane",
        "created_at": "2021-10-28T17:19:52Z",
        "body": "There is no easy/safe way for Replicated. \r\n\r\nYou can use `attach partition from`. This command almost instant and almost free and does not copy data on a disk, but clones the data using hardlinks (it's possible with tables with any size).\r\n\r\n```sql\r\n-- test data\r\nCREATE TABLE test (key Int64, ts DateTime, s String)\r\nENGINE = ReplicatedReplacingMergeTree ('/clickhouse/{cluster}/{database}/tables/{shard}/sometest','{replica}')\r\nPARTITION BY toYYYYMM(ts)\r\nORDER BY (key);\r\n\r\ninsert into test select number, today(), '' from numbers(10000);\r\n\r\n-- new table with ver column\r\nCREATE TABLE new on cluster .... (key Int64, ts DateTime, s String)\r\nENGINE = ReplicatedReplacingMergeTree ('/clickhouse/{cluster}/{database}/tables/{shard}/sometest_v2','{replica}', ts)\r\nPARTITION BY toYYYYMM(ts)\r\nORDER BY (key);\r\n\r\n-- generate alters\r\nselect concat('alter table new', ' attach partition id \\'', partition_id, '\\' from test;')\r\nfrom system.parts \r\nwhere active = 1 and table like 'test' \r\ngroup by database,table,partition_id\r\norder by database,table,partition_id \r\n\r\nalter table new attach partition id '202110' from test;\r\n\r\nselect count() from new\r\n--\r\n10000\r\n\r\nrename table test to old, new to test;\r\n\r\n--eventually\r\ndrop table old.\r\n```"
      },
      {
        "user": "tranvinhthuy",
        "created_at": "2021-10-28T17:52:44Z",
        "body": "> ```sql\r\n> partition_id\r\n> ```\r\n\r\nThank you very much for your answer.\r\n\r\nI would like to ask what could be unsafe during the `attach partition from` operation? So we could prepare before executing this.\r\n\r\n(FYI, in our setup we only have 1 main CH instance and 1 replica instance + zookeeper. We can stop the replication any time and/or recreate the replica instance during table altering).\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2021-10-28T18:15:31Z",
        "body": ">I would like to ask what could be unsafe during the\r\n\r\nNothing unsafe in this way. And you can make a backup just in case `alter table test freeze`.\r\nObviously you need to stop ingestion because you will insert into the old table and the new table will miss these during maintenance inserts.\r\n\r\n------\r\n\r\n>There is no easy/safe way for Replicated.\r\n\r\nI meant there is another way. Edit .sql files and all ZK metadata records for existing table. \r\nFor example for non-replicated tables IN THIS CASE you can execute `detach table`, edit .sql file, `attach table`. No need for the second table and attach ... and other."
      },
      {
        "user": "den-crane",
        "created_at": "2021-10-28T18:19:13Z",
        "body": ">(FYI, in our setup we only have 1 main CH instance and 1 replica instance + zookeeper. We can stop the replication any >time and/or recreate the replica instance during table altering).\r\n\r\nIf you create the new table on all replicas then everything will be consistent.  Replicated table executes `alter table attach from` on all replicas. After that you need to execute `rename table test to old, new to test;` on all replicas or on_cluster (not sure is it possible with rename multiple tables or not. Probably `rename table xxx to yyy on cluster; rename table zzz to qqqq on cluster ` should work."
      },
      {
        "user": "tranvinhthuy",
        "created_at": "2021-10-28T20:56:01Z",
        "body": "Thank you so much for the help!\r\n\r\nI will proceed with this and close this issue when it is done."
      }
    ]
  },
  {
    "number": 29975,
    "title": "Is it possible to calculate number of distinct keys in map?",
    "created_at": "2021-10-11T01:49:23Z",
    "closed_at": "2021-10-11T09:36:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29975",
    "body": "I have a map type field. Is there a way to calculate how many times each of distinct keys encounter in the whole table?\r\n\r\nSay I have a table from the example in the docs:\r\n\r\n`CREATE TABLE table_map (a Map(String, UInt64)) ENGINE=Memory;`\r\n`INSERT INTO table_map VALUES ({'key1':1, 'key2':10}), ({'key1':2,'key2':20}), ({'key1':3,'key2':30});`\r\n\r\nCan I write a query like this?\r\n\r\n`SELECT\r\n    a.keys,\r\n    count(a.keys) AS cnt\r\nFROM table_map\r\nGROUP BY a.keys\r\nORDER BY cnt DESC`\r\n\r\nThis query counts distinct set of keys, but can it be rewritten to count keys?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29975/comments",
    "author": "sogawa-sps",
    "comments": [
      {
        "user": "vdimir",
        "created_at": "2021-10-11T08:09:19Z",
        "body": "You can use `arrayJoin` for that\r\n\r\n```sql\r\nSELECT arrayJoin(a.keys) as keys, count() as cnt FROM table_map GROUP BY keys ORDER BY cnt DESC;\r\n```\r\n\r\n```\r\n\u250c\u2500keys\u2500\u252c\u2500cnt\u2500\u2510\r\n\u2502 key2 \u2502   3 \u2502\r\n\u2502 key1 \u2502   3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nReopen this issue if this solution is not suitable for your case"
      },
      {
        "user": "sogawa-sps",
        "created_at": "2021-10-11T21:24:15Z",
        "body": "Thanks a lot!"
      }
    ]
  },
  {
    "number": 29620,
    "title": "Non standart aggregation with GROUP BY",
    "created_at": "2021-10-01T12:31:12Z",
    "closed_at": "2021-10-01T14:50:07Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29620",
    "body": "** SQL **\r\n```\r\nSELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL;\r\n```\r\nCurrent result:\r\n\r\n```\r\n\u250c\u2500max(1)\u2500\u2510\r\n\u2502      0 \r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nExpected SQL Standart behavior (MySQL, sqlite, etc):\r\n\r\n```\r\nmysql> SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL;\r\nEmpty set (0,00 sec)\r\n```\r\n\r\nBut with a little changed SQL all ok\r\n\r\n```\r\nSELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0;\r\n```\r\n\r\nClickhouse\r\n```\r\n\u250c\u2500max(1)\u2500\u2510\r\n\u2502      0\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nMysql\r\n```\r\nmysql> SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0;\r\n+--------+\r\n| MAX(1) |\r\n+--------+\r\n|   NULL |\r\n+--------+\r\n1 row in set (0,00 sec)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29620/comments",
    "author": "Grian",
    "comments": [
      {
        "user": "nvartolomei",
        "created_at": "2021-10-01T12:49:37Z",
        "body": "You should try one of the latest stable/lts releases for first query.\r\n\r\nFor the second, there is `aggregate_functions_null_for_empty` setting that you can enable.\r\n\r\n```\r\nroot@a70daef2a898:/# clickhouse local -q 'SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL'\r\nroot@a70daef2a898:/# clickhouse local -q 'SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 SETTINGS aggregate_functions_null_for_empty = true'\r\n\\N\r\nroot@a70daef2a898:/# clickhouse local -q 'select version()'\r\n21.9.4.35\r\n```"
      },
      {
        "user": "Grian",
        "created_at": "2021-10-01T12:57:20Z",
        "body": "Thanks, with new release ok. We will upgrade.\r\n\r\n```\r\nSELECT MAX(1) FROM ( SELECT 1 ) AS one\r\nWHERE 1 = 0\r\nGROUP BY NULL\r\n\r\n0 rows in set. Elapsed: 0.038 sec.\r\n```\r\n"
      }
    ]
  },
  {
    "number": 29181,
    "title": "Can't import large files to server running under docker: \"Broken pipe, while writing to socket\"",
    "created_at": "2021-09-19T21:58:06Z",
    "closed_at": "2021-09-19T23:42:23Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29181",
    "body": "I've started ClickHouse server as a docker container on my Windows machine:\r\n\r\n`docker run --restart always -d --name ch --ulimit nofile=262144:262144 -p 8123:8123 -p 9000:9000 -p 9009:9009 --volume=/e/ClickHouse:/var/lib/clickhouse yandex/clickhouse-server`\r\n\r\nThen I've opened an Ubuntu session (over WSL2)  and tried to import the data (2.1G csv file):\r\n\r\n`clickhouse-client --query \"INSERT INTO test.time_test FORMAT CSV\" --max_insert_block_size=100000 < /mnt/e/temp/time_test.csv`\r\n\r\nBut it failed:\r\n`Code: 210. DB::NetException: I/O error: Broken pipe, while writing to socket (127.0.0.1:9000)`\r\n\r\nIt reproduces for any file large enough. Tiny files are imported fine. Any ideas what could went wrong and how to diagnose it?\r\n\r\nOS: Windows 10\r\nClickHouse version: 21.9.3.30\r\nClickHouse client version: 18.16.1\r\nDocker Desktop: 20.10.8 (over WSL2)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29181/comments",
    "author": "sogawa-sps",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-19T23:37:08Z",
        "body": "18.16.1 is out of support.\r\nMost probably CH server restarts because of lack of memory.\r\nCheck `/var/log/clickhouse-server/clickhouse-server.log` for more info."
      },
      {
        "user": "sogawa-sps",
        "created_at": "2021-09-20T00:58:07Z",
        "body": "Looks like it was an issue with the client indeed. It came from official Ubuntu rep, I've updated it using ClickHouse's repository and now everything works fine. Thank you!"
      }
    ]
  },
  {
    "number": 29113,
    "title": "Quota doesn't seem to work",
    "created_at": "2021-09-17T07:19:21Z",
    "closed_at": "2021-09-22T07:36:45Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29113",
    "body": "clickhouse version: 21.8.5.7\r\n\r\nwhy user1 queries  is 265? any config missed?\r\n```sql\r\nSELECT\r\n    user,\r\n    count()\r\nFROM system.processes\r\nGROUP BY user\r\n\r\n\u250c\u2500user\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 user1                           \u2502     265 \u2502\r\n\u2502 default                         \u2502       3 \u2502\r\n\u2502 user2                           \u2502      88 \u2502\r\n\u2502 user3                           \u2502       1 \u2502\r\n\u2502 user4                           \u2502       5 \u2502\r\n\u2502 user5                           \u2502      11 \u2502\r\n\u2502 user6                           \u2502       6 \u2502\r\n\u2502 user7                           \u2502      12 \u2502\r\n\u2502 user8                           \u2502       1 \u2502\r\n\u2502 user9                           \u2502     109 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    name,\r\n    storage,\r\n    apply_to_list\r\nFROM quotas\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500storage\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500apply_to_list\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 default                                \u2502 users.xml       \u2502 ['default']                         \u2502\r\n\u2502 qps_20_user1                           \u2502 local directory \u2502 ['user1']                           \u2502\r\n\u2502 qps_20_user2                           \u2502 local directory \u2502 ['user2']                           \u2502\r\n\u2502 qps_20_user3                           \u2502 local directory \u2502 ['user3']                           \u2502\r\n\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    quota_name,\r\n    duration,\r\n    max_queries,\r\n    max_query_selects,\r\n    max_query_inserts\r\nFROM quota_limits\r\n\r\n\u250c\u2500quota_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500duration\u2500\u252c\u2500max_queries\u2500\u252c\u2500max_query_selects\u2500\u252c\u2500max_query_inserts\u2500\u2510\r\n\u2502 default                                \u2502     3600 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502              \u1d3a\u1d41\u1d38\u1d38 \u2502              \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2502 qps_20_user1                           \u2502        1 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502                20 \u2502                20 \u2502\r\n\u2502 qps_20_user2                           \u2502        1 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502                20 \u2502                20 \u2502\r\n\u2502 qps_20_user3                           \u2502        1 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502                20 \u2502                10 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nanother question. should i create a quota for each user?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29113/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-17T16:25:52Z",
        "body": "quota is accounted only for real tables\r\n\r\n```\r\ncreate user foo identified  by '123'\u3000;\r\nCREATE QUOTA OR REPLACE my_quota FOR INTERVAL 4 minute MAX queries = 5 TO foo;\r\ncreate table a(A Int64) Engine=Log;\r\n\r\nfor i in `seq 1 10`; do clickhouse-client -u foo --password=123 -q 'select * from a format Null'; done;\r\nReceived exception from server (version 21.9.1):\r\nCode: 201. DB::Exception: Received from localhost:9000. DB::Exception: Quota for user `foo` for 240s has been exceeded: queries = 6/5. Interval will end at 2021-09-17 16:20:00. Name of quota template: `my_quota`. (QUOTA_EXPIRED)\r\nReceived exception from server (version 21.9.1):\r\nCode: 201. DB::Exception: Received from localhost:9000. DB::Exception: Quota for user `foo` for 240s has been exceeded: queries = 7/5. Interval will end at 2021-09-17 16:20:00. Name of quota template: `my_quota`. (QUOTA_EXPIRED)\r\nReceived exception from server (version 21.9.1):\r\nCode: 201. DB::Exception: Received from localhost:9000. DB::Exception: Quota for user `foo` for 240s has been exceeded: queries = 8/5. Interval will end at 2021-09-17 16:20:00. Name of quota template: `my_quota`. (QUOTA_EXPIRED)\r\nReceived exception from server (version 21.9.1):\r\nCode: 201. DB::Exception: Received from localhost:9000. DB::Exception: Quota for user `foo` for 240s has been exceeded: queries = 9/5. Interval will end at 2021-09-17 16:20:00. Name of quota template: `my_quota`. (QUOTA_EXPIRED)\r\nReceived exception from server (version 21.9.1):\r\nCode: 201. DB::Exception: Received from localhost:9000. DB::Exception: Quota for user `foo` for 240s has been exceeded: queries = 10/5. Interval will end at 2021-09-17 16:20:00. Name of quota template: `my_quota`. (QUOTA_EXPIRED)\r\n\r\n\r\nfor i in `seq 1 10`; do clickhouse-client -u foo --password=123 -q 'select 1'; done;\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1\r\n\r\n```\r\n\r\n>another question. should i create a quota for each user?\r\n\r\nNo, `TO user,user1,user2`\r\n\r\n```\r\nCREATE QUOTA OR REPLACE my_quota FOR INTERVAL 4 minute MAX queries = 5 TO user,user1,user2;\r\nCREATE QUOTA OR REPLACE my_quota FOR INTERVAL 4 minute MAX queries = 5 TO role;\r\n```\r\n\r\nThere is another way to forbid number of simultaneous queries: max_concurrent_queries_for_user\r\n\r\n```\r\ncreate user foo2 identified  by '123'\u3000;\r\nCREATE SETTINGS PROFILE max_concurrent_queries_for_userp \r\nSETTINGS max_concurrent_queries_for_user = 1 READONLY TO foo2;\r\n```\r\n\r\n"
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-09-18T02:13:38Z",
        "body": "thank you.\r\n> quota is accounted only for real tables\r\n\r\nyes. 265 queries for user1 are insert query. \r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2021-09-18T15:38:33Z",
        "body": "> thank you.\r\n> \r\n> > quota is accounted only for real tables\r\n> \r\n> yes. 265 queries for user1 are insert query.\r\n\r\ndo you have reproducible example?"
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-09-22T07:36:45Z",
        "body": "> > thank you.\r\n> > > quota is accounted only for real tables\r\n> > \r\n> > \r\n> > yes. 265 queries for user1 are insert query.\r\n> \r\n> do you have reproducible example?\r\n\r\nno . but thank you"
      }
    ]
  },
  {
    "number": 28903,
    "title": "clickhouse server crashes after starts ",
    "created_at": "2021-09-11T18:16:54Z",
    "closed_at": "2021-09-11T21:51:45Z",
    "labels": [
      "invalid",
      "question",
      "alternative build"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28903",
    "body": "error message below\r\n```\r\n2021.09.12 01:23:44.437039 [ 18936 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.09.12 01:23:44.437087 [ 18936 ] {} <Fatal> BaseDaemon: (version 20.8.3.18, no build id) (from thread 18818) (no query) Received signal Segmentation fault (11)\r\n2021.09.12 01:23:44.437113 [ 18936 ] {} <Fatal> BaseDaemon: Address: 0x7f78fb378000 Access: write. Attempted access has violated the permissions assigned to the memory area.\r\n2021.09.12 01:23:44.437126 [ 18936 ] {} <Fatal> BaseDaemon: Stack trace: 0x14b99c2f 0x14b9e7ad 0x14b9fd36 0x14b98e78 0x14ba17ef 0x14ba1b33 0x9e7e7ea 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d\r\n2021.09.12 01:23:44.437163 [ 18936 ] {} <Fatal> BaseDaemon: 3. ? @ 0x14b99c2f in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437171 [ 18936 ] {} <Fatal> BaseDaemon: 4. ? @ 0x14b9e7ad in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437177 [ 18936 ] {} <Fatal> BaseDaemon: 5. ? @ 0x14b9fd36 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437186 [ 18936 ] {} <Fatal> BaseDaemon: 6. unw_step @ 0x14b98e78 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437193 [ 18936 ] {} <Fatal> BaseDaemon: 7. ? @ 0x14ba17ef in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437203 [ 18936 ] {} <Fatal> BaseDaemon: 8. _Unwind_Resume @ 0x14ba1b33 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437212 [ 18936 ] {} <Fatal> BaseDaemon: 9. ? @ 0x9e7e7ea in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437224 [ 18936 ] {} <Fatal> BaseDaemon: 10. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437236 [ 18936 ] {} <Fatal> BaseDaemon: 11. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437259 [ 18936 ] {} <Fatal> BaseDaemon: 12. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437267 [ 18936 ] {} <Fatal> BaseDaemon: 13. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437274 [ 18936 ] {} <Fatal> BaseDaemon: 14. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437281 [ 18936 ] {} <Fatal> BaseDaemon: 15. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437288 [ 18936 ] {} <Fatal> BaseDaemon: 16. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437295 [ 18936 ] {} <Fatal> BaseDaemon: 17. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437302 [ 18936 ] {} <Fatal> BaseDaemon: 18. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437309 [ 18936 ] {} <Fatal> BaseDaemon: 19. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437315 [ 18936 ] {} <Fatal> BaseDaemon: 20. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437322 [ 18936 ] {} <Fatal> BaseDaemon: 21. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437329 [ 18936 ] {} <Fatal> BaseDaemon: 22. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437341 [ 18936 ] {} <Fatal> BaseDaemon: 23. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437348 [ 18936 ] {} <Fatal> BaseDaemon: 24. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437355 [ 18936 ] {} <Fatal> BaseDaemon: 25. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437362 [ 18936 ] {} <Fatal> BaseDaemon: 26. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437369 [ 18936 ] {} <Fatal> BaseDaemon: 27. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437376 [ 18936 ] {} <Fatal> BaseDaemon: 28. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437382 [ 18936 ] {} <Fatal> BaseDaemon: 29. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437389 [ 18936 ] {} <Fatal> BaseDaemon: 30. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437396 [ 18936 ] {} <Fatal> BaseDaemon: 31. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n\r\n```\r\nScenario\uff1aour clickhouse cluster runs stably for a time, yesterday a beginner developer runs a lot of \"alter table\" query try to \"update\" a field in a big table, the server crashed. then we add \r\n```\r\n<cleanup_delay_period>60</cleanup_delay_period>\r\n<task_max_lifetime>60</task_max_lifetime>\r\n<max_tasks_in_queue>200</max_tasks_in_queue>\r\n```\r\nthis to the clickhouse config to clean up the DDL queue. after that, some of clickhouse servers could not start up.\r\nAny ideas",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28903/comments",
    "author": "liu316484231",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T20:53:33Z",
        "body": "It's not official build."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T21:47:14Z",
        "body": "About how to fix your issue: it has caused by large number of ALTER mutations. You should find and delete the offending mutation record. Its location depends on whether you are using MergeTree or ReplicatedMergeTree."
      },
      {
        "user": "liu316484231",
        "created_at": "2021-09-11T23:01:21Z",
        "body": "> About how to fix your issue: it has caused by large number of ALTER mutations. You should find and delete the offending mutation record. Its location depends on whether you are using MergeTree or ReplicatedMergeTree.\r\n\r\nhow to find and delete these mutations?we use MergeTree. I didnt see any log related to the alter mutation after modify the config before crashes"
      },
      {
        "user": "liu316484231",
        "created_at": "2021-09-11T23:03:54Z",
        "body": "> It's not official build.\r\n\r\nyou mean the version we use (version 20.8.3.18, no build id) is not official recommanded?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T23:05:19Z",
        "body": "Mutations are located in `mutation_*.txt` files. You can delete them.\r\n\r\n> you mean the version we use (version 20.8.3.18, no build id) is not official recommanded?\r\n\r\nYes. And it is not our build. It is somehow modified or you have built it manually."
      },
      {
        "user": "liu316484231",
        "created_at": "2021-09-13T02:29:35Z",
        "body": "> Mutations are located in `mutation_*.txt` files. You can delete them.\r\n> \r\n> > you mean the version we use (version 20.8.3.18, no build id) is not official recommanded?\r\n> \r\n> Yes. And it is not our build. It is somehow modified or you have built it manually.\r\n\r\nThank you for your reply, we fixed the problem"
      }
    ]
  },
  {
    "number": 28849,
    "title": "What if version column in ReplacingMergeTree overflows?",
    "created_at": "2021-09-10T07:29:11Z",
    "closed_at": "2021-09-10T18:32:52Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28849",
    "body": "I know there is a possibility to use type `UInt256` for version column in `ReplacingMergeTree`, in which the overflow is probably unrealistic, but still possible. What happens then, when I reach the maximum \"version\" and I need to create a new one? How does ClickHouse handle it? Or how should I handle it? Thank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28849/comments",
    "author": "grongor",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-10T18:32:52Z",
        "body": "Even UInt16 or UInt32 should be enough. \r\n4294967295 is insane number of versions \"updates\"\r\n\r\nIf UInt16 is not enough for you, you are doing something weird. "
      },
      {
        "user": "grongor",
        "created_at": "2021-09-10T19:00:13Z",
        "body": "Well, that doesn't really answer the question. And it depends on the use cases... I'm for example trying to track all network prefixes and related data from BGP available in our network, in real time, which means  thousands of updates per second. So, when the counter overflows, there is no sane way to continue I guess. "
      },
      {
        "user": "den-crane",
        "created_at": "2021-09-10T19:16:19Z",
        "body": ">Well, that doesn't really answer the question.\r\n\r\nWhen numeric type is overflow then a number starts again with 0.\r\nReplacingMergeTree treats overflowed 0 as a version which is less than MAX_NUMBER.\r\nIt's your responsibility to deal with it.\r\n\r\n>which means thousands of updates per second. \r\n\r\nThis is insane. \r\n\r\nDo you understand that you can omit version in ReplacingMergeTree? \r\nIn this case ReplacingMergeTree will use internal/natural order of blocks (order of inserts)."
      },
      {
        "user": "grongor",
        "created_at": "2021-09-11T06:50:06Z",
        "body": "> It's your responsibility to deal with it.\r\n\r\nThanks. That's what I needed to know, if ClickHouses accounts for the overflow, or if I have to work around it somehow.\r\n\r\n> This is insane.\r\n\r\nYeah, but it must be done :D \r\n\r\n> Do you understand that you can omit version in ReplacingMergeTree?\r\n\r\nYes, but thank you for mentioning it. There will be parallel writes, that's why I went with the version column.\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T06:52:36Z",
        "body": "UInt64 is good enough for 1000 updates per second (with this rate it will require ~ 100 million years to overflow)."
      },
      {
        "user": "grongor",
        "created_at": "2021-09-11T06:56:17Z",
        "body": "Sure, I knew I would probably be okay with one of the UInt types :) The question was more or less academic - I just wanted to know how ClickHouse would handle this, if it ever were to happen :)"
      }
    ]
  },
  {
    "number": 28681,
    "title": "How to prevent ATTACH in old versions previous to DETACH ... PERMANENTLY",
    "created_at": "2021-09-07T10:19:56Z",
    "closed_at": "2021-09-07T12:36:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28681",
    "body": "Hello,\r\nI have a problem with an old CH version, where old materializations are reattached after each startup.\r\nHow to remove a materialized view permanently (so it doesn\u2019t reattach on startup) in older ch versions where detach \u2026 permanently is not present? Should I just delete it from the `metadata` folder after DETACH?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28681/comments",
    "author": "inakisoriamrf",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-07T12:36:20Z",
        "body": "> Should I just delete it from the metadata folder after DETACH?\r\n\r\nYes, you can delete .sql file from metadata folder."
      },
      {
        "user": "inakisoriamrf",
        "created_at": "2021-09-08T07:46:54Z",
        "body": "Thank you @den-crane =)"
      }
    ]
  },
  {
    "number": 28608,
    "title": "How to load csv contain json column into clickhouse?",
    "created_at": "2021-09-04T10:32:39Z",
    "closed_at": "2021-09-05T03:36:00Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28608",
    "body": "csv\r\n```\r\n\"id\",\"operateDate\",\"operateTime\",\"userid\",\"usertype\",\"targetId\",\"targetName\",\"logType\",\"logSmallType\",\"operateType\",\"clientIp\",\"oldValues\",\"newValues\",\"description\",\"params\",\"logTypeLabel\",\"logSmallTypeLabel\",\"belongtype\",\"belongTypeLabel\",\"belongTypeTargetId\",\"belongTypeTargetName\",\"isDetail\",\"mainId\",\"belongMainId\",\"groupId\",\"groupNameLabel\",\"operateAuditType\",\"isArchived\",\"deviceType\"\r\n1,\"2020-03-16\",\"11:33:14\",1,1,\"1\",\"sysadmin\",3,16,\"LOGIN\",\"192.168.42.51\",\"\",\"\",\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\",\"{\\\"deviceType\\\":1,\\\"lastName\\\":\\\"sysadmin\\\",\\\"loginId\\\":\\\"sysadmin\\\",\\\"clientIp\\\":\\\"192.168.42.51\\\",\\\"userId\\\":1,\\\"desc\\\":\\\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\\\"}\",179,506994,16,506994,\\N,\\N,0,\\N,\\N,\\N,0,\"ERROR\",\\N,\"PC\"\r\n```\r\nThe params column in mysql is of type text\r\n```\r\n| params               | text          | YES  |     | NULL    |                |\r\n```\r\n\r\nThe params column in clickhouse is of type String\r\n```\r\n`params` String COMMENT '\u53c2\u6570',\r\n```\r\nhow can i load this csv into clickhouse?\r\n\r\n```\r\n# clickhouse-client -u default --password superpass --host 127.0.0.1 --port 9000 --format_csv_delimiter=\",\" --query \"INSERT INTO rd_auditlog.ecology_biz_log FORMAT CSVWithNames\" --max_insert_block_size=100000 < infra_fanweioa.ecology_biz_log.000000000.csv \r\nCode: 27. DB::ParsingException: Cannot parse input: expected ',' before: 'deviceType\\\\\":1,\\\\\"lastName\\\\\":\\\\\"sysadmin\\\\\",\\\\\"loginId\\\\\":\\\\\"sysadmin\\\\\",\\\\\"clientIp\\\\\":\\\\\"192.168.42.51\\\\\",\\\\\"userId\\\\\":1,\\\\\"desc\\\\\":\\\\\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\\\\\"}\",179,506994,16,50': \r\nRow 1:\r\nColumn 0,   name: id,                   type: Int64,    parsed text: \"1\"\r\nColumn 1,   name: operateDate,          type: String,   parsed text: \"<DOUBLE QUOTE>2020-03-16<DOUBLE QUOTE>\"\r\nColumn 2,   name: operateTime,          type: String,   parsed text: \"<DOUBLE QUOTE>11:33:14<DOUBLE QUOTE>\"\r\nColumn 3,   name: userid,               type: Int32,    parsed text: \"1\"\r\nColumn 4,   name: usertype,             type: Int32,    parsed text: \"1\"\r\nColumn 5,   name: targetId,             type: String,   parsed text: \"<DOUBLE QUOTE>1<DOUBLE QUOTE>\"\r\nColumn 6,   name: targetName,           type: String,   parsed text: \"<DOUBLE QUOTE>sysadmin<DOUBLE QUOTE>\"\r\nColumn 7,   name: logType,              type: String,   parsed text: \"3\"\r\nColumn 8,   name: logSmallType,         type: String,   parsed text: \"16\"\r\nColumn 9,   name: operateType,          type: String,   parsed text: \"<DOUBLE QUOTE>LOGIN<DOUBLE QUOTE>\"\r\nColumn 10,  name: clientIp,             type: String,   parsed text: \"<DOUBLE QUOTE>192.168.42.51<DOUBLE QUOTE>\"\r\nColumn 11,  name: oldValues,            type: String,   parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nColumn 12,  name: newValues,            type: String,   parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nColumn 13,  name: description,          type: String,   parsed text: \"<DOUBLE QUOTE>\u6388\u6743\u4fe1\u606f\u9519\u8bef<DOUBLE QUOTE>\"\r\nColumn 14,  name: params,               type: String,   parsed text: \"<DOUBLE QUOTE>{<BACKSLASH><DOUBLE QUOTE>\"\r\nERROR: There is no delimiter (,). \"d\" found instead.\r\n\r\n: data for INSERT was parsed from stdin: (at row 1)\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28608/comments",
    "author": "Fanduzi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-05T03:36:00Z",
        "body": "`\"` (double quote) must be escaped by double quote in CH CSV. \r\n\r\n`\\\"` -- incorrect.\r\n`\"\"` -- correct.\r\n\r\n```sql\r\ncreate table test(s String) Engine=Memory;\r\n\r\n:) insert into test format CSV \"{\"\"deviceType\"\":1,\"\"lastName\"\":\"\"sysadmin\"\",\"\"loginId\"\":\"\"sysadmin\"\",\"\"clientIp\"\":\"\"192.168.42.51\"\",\"\"userId\"\":1,\"\"desc\"\":\"\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\"\"}\"\r\n:-] ;\r\n\r\nINSERT INTO test FORMAT CSV\r\n\r\nQuery id: 1b12315e-5cb8-45b5-8b0f-3733068079e6\r\n\r\nOk.\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n\r\n\r\n\r\n:) select * from test;\r\n\r\nSELECT *\r\nFROM test\r\n\r\nQuery id: d82db78b-e766-493a-a499-fe58bab8238e\r\n\r\n\u250c\u2500s\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 {\"deviceType\":1,\"lastName\":\"sysadmin\",\"loginId\":\"sysadmin\",\"clientIp\":\"192.168.42.51\",\"userId\":1,\"desc\":\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\"} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2021-09-05T03:38:49Z",
        "body": "one more option:\r\n\r\n```sql\r\n:) insert into test format CSV '{\"deviceType\":1,\"lastName\":\"sysadmin\",\"loginId\":\"sysadmin\",\"clientIp\":\"192.168.42.51\",\"userId\":1,\"desc\":\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\"}'\r\n:-] ;\r\n\r\nINSERT INTO test FORMAT CSV\r\n\r\nQuery id: e3fe5273-554f-4827-99bb-e54319ea930a\r\n\r\nOk.\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n\r\n\r\n:) select * from test;\r\n\r\nSELECT *\r\nFROM test\r\n\r\nQuery id: 9bdad3d8-5650-4bc3-8604-91919865887f\r\n\r\n\u250c\u2500s\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 {\"deviceType\":1,\"lastName\":\"sysadmin\",\"loginId\":\"sysadmin\",\"clientIp\":\"192.168.42.51\",\"userId\":1,\"desc\":\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\"} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500s\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 {\"deviceType\":1,\"lastName\":\"sysadmin\",\"loginId\":\"sysadmin\",\"clientIp\":\"192.168.42.51\",\"userId\":1,\"desc\":\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\"} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "Fanduzi",
        "created_at": "2021-09-05T05:41:05Z",
        "body": "Thank you so much, it solved a big problem for me!"
      }
    ]
  },
  {
    "number": 28506,
    "title": "i got a exception when create a table use RabbitMQ engine",
    "created_at": "2021-09-02T09:17:27Z",
    "closed_at": "2021-09-03T01:05:51Z",
    "labels": [
      "question",
      "question-answered",
      "comp-rabbitmq"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28506",
    "body": "the clickhouse run in docker\r\n```\r\nsudo mkdir /var/docker/clickhouse/\r\nsudo mkdir /var/docker/clickhouse/config\r\nsudo mkdir /var/docker/clickhouse/config/config.d\r\necho \"<yandex>\r\n     <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->\r\n    <listen_host>::</listen_host>\r\n    <listen_host>0.0.0.0</listen_host>\r\n    <listen_try>1</listen_try>\r\n\r\n    <!--\r\n    <logger>\r\n        <console>1</console>\r\n    </logger>\r\n    -->\r\n</yandex>\" | sudo tee /var/docker/clickhouse/config/config.d/docker_related_config.xml\r\necho \"<yandex>\r\n <rabbitmq>\r\n    <username>guest</username>\r\n    <password>guest</password>\r\n </rabbitmq>\r\n</yandex>\" | sudo tee /var/docker/clickhouse/config/config.d/rabbit.xml\r\n\r\ndocker container stop clickhouse && docker container rm clickhouse\r\ndocker run -d \\\r\n  --name clickhouse \\\r\n  --restart on-failure \\\r\n  --ulimit nofile=262144:262144 \\\r\n  -p 8123:8123 \\\r\n  -p 9000:9000 \\\r\n  --volume=/var/docker/clickhouse:/var/lib/clickhouse \\\r\n  --volume=/var/docker/clickhouse/config/users.d:/etc/clickhouse-server/users.d \\\r\n  --volume=/var/docker/clickhouse/config/config.d:/etc/clickhouse-server/config.d \\\r\n  yandex/clickhouse-server\r\n\r\n```\r\nrabbitMQ is docker also\r\n```\r\nsudo docker run \\\r\n  -d \\\r\n  --name rabbitmq \\\r\n  -p 5672:5672 \\\r\n  -p 15672:15672 \\\r\n  rabbitmq:management\r\n```\r\n\r\nthen i create a teble and query it\r\n```\r\nCREATE TABLE queue\r\n(\r\n\tkey   UInt64,\r\n\tvalue UInt64,\r\n\tdate  DateTime\r\n) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672',\r\n\trabbitmq_exchange_name = 'exchange1',\r\n\trabbitmq_format = 'JSONEachRow',\r\n\trabbitmq_num_consumers = 5,\r\n\tdate_time_input_format = 'best_effort';\r\n\r\nselect *\r\nfrom queue;\r\n```\r\n\r\ni got a exception\r\n`Code: 530, e.displayText() = DB::Exception: RabbitMQ setup not finished. Connection might be lost (version 21.8.4.51 (official build))`\r\n\r\nthe log of clickhouse\r\n```\r\n2021.09.02\u00a009:10:26.877248\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0RabbitMQConnectionTask:\u00a0Execution\u00a0took\u00a04002\u00a0ms.\r\n2021.09.02\u00a009:10:27.377358\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Trying\u00a0to\u00a0restore\u00a0connection\u00a0to\u00a0localhost:5672\r\n2021.09.02\u00a009:10:27.577754\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Error>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Library\u00a0error\u00a0report:\u00a0connection\u00a0lost\r\n2021.09.02\u00a009:10:31.379727\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0RabbitMQConnectionTask:\u00a0Execution\u00a0took\u00a04002\u00a0ms.\r\n2021.09.02\u00a009:10:31.879822\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Trying\u00a0to\u00a0restore\u00a0connection\u00a0to\u00a0localhost:5672\r\n2021.09.02\u00a009:10:32.080192\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Error>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Library\u00a0error\u00a0report:\u00a0connection\u00a0lost\r\n```\r\n\r\nclickhouse :\r\nselect version();=21.8.4.51\r\nrabbitmq is version 3.9.5\r\n\r\nsystem: windows 10 wsl debian\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28506/comments",
    "author": "AiSY-Yang",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2021-09-02T10:28:06Z",
        "body": "I assume it does not work becuase you have both clickhouse and rabbitmq in different docker containers and then pass `localhost` to RabbitMQ engine. It should not be localhost."
      },
      {
        "user": "AiSY-Yang",
        "created_at": "2021-09-03T01:05:51Z",
        "body": "> I assume it does not work becuase you have both clickhouse and rabbitmq in different docker containers and then pass `localhost` to RabbitMQ engine. It should not be localhost.\r\n\r\nthanks\r\nyou are right \r\ni add the argument  `  --link rabbitmq:rabbitmq`  when i run clickhouse\r\n\r\nset rabbitmq_host_port = 'rabbitmq:5672'\r\n\r\nI successfully created the table"
      }
    ]
  },
  {
    "number": 28005,
    "title": "MergeTreeThread has feature spill data to disk?",
    "created_at": "2021-08-23T03:29:16Z",
    "closed_at": "2021-08-23T04:48:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28005",
    "body": "when use sql to query distributed tables \"select * from dbname.tablename_all\", no limitations, it is easy to appear such exception:\r\n\"_DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 93.38 GiB (attempt to allocate chunk of 362340215 bytes), maximum: 93.13 GiB: **While executing MergeTreeThread.**_\"\r\n\r\nwhen the memory is not enough, should be spill on disk, does it has this feature \"While executing MergeTreeThread\" ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28005/comments",
    "author": "aaawuanjun",
    "comments": [
      {
        "user": "aaawuanjun",
        "created_at": "2021-08-23T03:32:02Z",
        "body": "Although this it not a good sql, but my goal is to export data to other clusters"
      },
      {
        "user": "den-crane",
        "created_at": "2021-08-23T03:40:20Z",
        "body": "How many columns in your table?\r\nClickhouse version? `select version()`?\r\nCan you share `set send_logs_level='trace'; select * from dbname.tablename_all format Null;`\r\nTry `set max_threads=1, max_block_size=8192`"
      },
      {
        "user": "aaawuanjun",
        "created_at": "2021-08-23T04:47:35Z",
        "body": "clickhouse version: 21.5.5.12\r\ncolumns:34\r\nrows:644320978\r\n\r\nok, I am trying \r\n> set max_threads=1, max_block_size=8192\r\n\r\nat present , no error occur, I successfully to export data to local.\r\n"
      }
    ]
  },
  {
    "number": 28004,
    "title": "build failed error:FAILED: src/libdbms.so ",
    "created_at": "2021-08-23T02:03:36Z",
    "closed_at": "2021-08-25T10:58:00Z",
    "labels": [
      "question",
      "build",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28004",
    "body": "Hi\uff0cI compile cl on ubuntu 20.04, and always encounter this error. I don\u2019t know why. I also agree with the error on other debians.\r\n\r\n```\r\n12/672] Creating preprocessed file...c/lib/gssapi/krb5/gssapi_err_krb5.c\r\n+ /usr/bin/awk -f /home/hrp/Click/ClickHouse/contrib/krb5/src/util/et/et_h.awk outfile=gssapi_err_krb5.h /home/hrp/Click/ClickHouse/contrib/krb5/src/lib/gssapi/krb5/gssapi_err_krb5.et\r\n+ /usr/bin/awk -f /home/hrp/Click/ClickHouse/contrib/krb5/src/util/et/et_c.awk outfile=gssapi_err_krb5.c textdomain= localedir= /home/hrp/Click/ClickHouse/contrib/krb5/src/lib/gssapi/krb5/gssapi_err_krb5.et\r\n[421/672] Linking CXX shared library src/libdbms.so\r\nFAILED: src/libdbms.so\r\n: && /usr/bin/c++ -fPIC -fdiagnostics-color=always -fsized-deallocation  -\r\n\r\n```\r\n\r\nIs this file missing?  src/libdbms.so\uff1f\r\n\r\nI downloaded the source code  with submodules directly\uff0cis right\uff1f\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28004/comments",
    "author": "rouse2617",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-23T10:31:21Z",
        "body": "Check the amount of free space on your machine."
      },
      {
        "user": "rouse2617",
        "created_at": "2021-08-24T01:15:06Z",
        "body": "Thank you for your suggestion, I tried to expand the memory is compiled, but when packaging deb I encountered this situation, in addition, I would like to ask, packaging deb can be continued, each time have to wait a long time, to the end reported wrong, but also to come again, thank you\r\n\r\nthis is package deb error\r\n```\r\n\r\n/home/hrp/ClickHouse/programs/main.cpp:29:5: error: 'ENABLE_CLICKHOUSE_SERVER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_SERVER\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:32:5: error: 'ENABLE_CLICKHOUSE_CLIENT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_CLIENT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:35:5: error: 'ENABLE_CLICKHOUSE_LOCAL' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_LOCAL\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:38:5: error: 'ENABLE_CLICKHOUSE_BENCHMARK' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_BENCHMARK\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:41:5: error: 'ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:44:5: error: 'ENABLE_CLICKHOUSE_COMPRESSOR' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_COMPRESSOR\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:47:5: error: 'ENABLE_CLICKHOUSE_FORMAT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_FORMAT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:50:5: error: 'ENABLE_CLICKHOUSE_COPIER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_COPIER\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:53:5: error: 'ENABLE_CLICKHOUSE_OBFUSCATOR' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_OBFUSCATOR\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:56:5: error: 'ENABLE_CLICKHOUSE_GIT_IMPORT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_GIT_IMPORT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:59:5: error: 'ENABLE_CLICKHOUSE_KEEPER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_KEEPER\r\n```\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-24T12:38:52Z",
        "body": "This is wrong. These options should be enabled.\r\n\r\nI recommend to:\r\n- do static build (as default) instead of shared;\r\n- use all the defaults, don't specify any parameters;\r\n- if you need to build Debian packages, use `docker/packager`."
      },
      {
        "user": "rouse2617",
        "created_at": "2021-08-26T01:51:09Z",
        "body": "Thank you, I made it"
      }
    ]
  },
  {
    "number": 27470,
    "title": "hdfs engine with hive default delimiter '0x01'",
    "created_at": "2021-08-09T12:49:30Z",
    "closed_at": "2021-09-01T08:24:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/27470",
    "body": "```sql\r\ncreate table hdfs_engine_table_1 on cluster datacenter\r\n(\r\n    name String,\r\n    address String\r\n)\r\n    engine = HDFS('hdfs://ns/user/hive/warehouse/a/b/*', 'CSV');\r\n```\r\n\r\nwhat format should i use?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/27470/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-16T00:33:04Z",
        "body": "Run this query before importing data: `SET format_csv_delimiter = '\\x01'`"
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-09-01T08:24:22Z",
        "body": "thank you."
      }
    ]
  },
  {
    "number": 26544,
    "title": "Failed to use mysql engine when creating database with mysql engine",
    "created_at": "2021-07-20T03:54:27Z",
    "closed_at": "2021-07-21T13:24:43Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/26544",
    "body": "clickhouse's version: 20.8.3.\r\nwhen i type the sql statment:\r\n**_create database  test ENGINE = MYSQL('172.16.0.55:3306','test','root','passw0rd');_**\r\nbut response message:\r\n_**Received exception from server (version 20.8.3):\r\nCode: 36. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Database engine MYSQL cannot have arguments. \r\n0 rows in set. Elapsed: 0.006 sec.**_ \r\n\r\nso what happened about this?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/26544/comments",
    "author": "prudens",
    "comments": [
      {
        "user": "abyss7",
        "created_at": "2021-07-20T10:18:35Z",
        "body": "The engine name is case-sensitive: we have engine `MySQL`, but not `MYSQL`. The error message is misleading in this way. Try and report, if it helps."
      },
      {
        "user": "prudens",
        "created_at": "2021-07-20T11:44:02Z",
        "body": "> The engine name is case-sensitive: we have engine `MySQL`, but not `MYSQL`. The error message is misleading in this way. Try and report, if it helps.\r\n\r\nyes\uff0crename the engine name as MySQL\uff0cit work well\uff01thank you.\r\n\r\n"
      }
    ]
  },
  {
    "number": 26528,
    "title": "difference between truncate table default.SampleTable vs delete /var/lib/clickhouse/data/default/SampleTable",
    "created_at": "2021-07-19T20:59:31Z",
    "closed_at": "2021-07-19T21:19:58Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/26528",
    "body": "Hello all,\r\n\r\nTo remove all data from a table(i.e.  default.SampleTable),\r\n\r\n```\r\ntruncate table default.SampleTable;\r\n\r\nrm -fr /var/lib/clickhouse/data/default/SampleTable/*\r\n```\r\n\r\nIs there any difference between the above two operations?\r\n\r\nThank you\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/26528/comments",
    "author": "Jack012a",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-19T21:19:14Z",
        "body": "Second operation is illegal - server will be unaware that the files are gone.\r\nIt works only for very basic table engines: Log, TinyLog, StripeLog.\r\nIf you do this for MergeTree, server will throw exception on next query and you will have to restart server to apply changes.\r\nIf you do this for ReplicatedMergeTree, server will notice that the data is gone and repair itself from replica.\r\nAlso manual files removal is not atomic in presense of concurrent writes and background merges."
      },
      {
        "user": "Jack012a",
        "created_at": "2021-07-20T01:11:11Z",
        "body": "I only do the second operation while I turn off the clickhouse-server. The main reason I need to do this is to transfer some data from one machine to another machine so I don't have to import all data again.\r\n\r\nThank you"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-20T10:57:48Z",
        "body": "Yes, you can manually move data between offline servers - it is legal :)\r\nAnd to online server as well (with moving to detached directory and ATTACH PART queries)."
      }
    ]
  },
  {
    "number": 26493,
    "title": "[PART AND PARTITION] Would all the parts of one partition be merged into a single part finally? ",
    "created_at": "2021-07-19T08:28:00Z",
    "closed_at": "2021-08-02T02:15:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/26493",
    "body": "We are trying to use  clickhouse-local to do pre-computing of part\u3002 \r\n\r\nAs we all know\uff0cone partition has many parts\uff0c but will all the parts in one partition be merged into one part finally\uff1f\r\n\r\nIf they will  be merged into one part some how,  will single execution of  \"OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition | PARTITION ID 'partition_id'] [FINAL]\" command give the final result ?\r\n\r\nThx!\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/26493/comments",
    "author": "mo-avatar",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-07-19T13:02:24Z",
        "body": "Merges are eventual and may never happen. \r\nIt depends on the number of inserts that happened after, the number of parts in the partition, size of parts.\r\nIf the total size of input parts are greater than the maximum part size then they will never be merged.\r\n\r\nOPTIMIZE ... FINAL tries to merge all part into a single part if it has enough resources in a merge pool (free disk space) without noting maximum part size limit."
      },
      {
        "user": "mo-avatar",
        "created_at": "2021-07-21T10:47:11Z",
        "body": "@den-crane Thanks a lot\uff0cafter reading your reply\uff0cI re-read the related code, and find function like this:\r\nUInt64 MergeTreeDataMergerMutator::getMaxSourcePartsSizeForMerge(size_t pool_size, size_t pool_used) const\r\n{\r\n    if (pool_used > pool_size)\r\n        throw Exception(\"Logical error: invalid arguments passed to getMaxSourcePartsSize: pool_used > pool_size\", ErrorCodes::LOGICAL_ERROR);\r\n\r\n    size_t free_entries = pool_size - pool_used;\r\n    const auto data_settings = data.getSettings();\r\n\r\n    /// Always allow maximum size if one or less pool entries is busy.\r\n    /// One entry is probably the entry where this function is executed.\r\n    /// This will protect from bad settings.\r\n    UInt64 max_size = 0;\r\n    if (pool_used <= 1 || free_entries >= data_settings->number_of_free_entries_in_pool_to_lower_max_size_of_merge)\r\n        max_size = data_settings->max_bytes_to_merge_at_max_space_in_pool;\r\n    else\r\n        max_size = interpolateExponential(\r\n            data_settings->max_bytes_to_merge_at_min_space_in_pool,\r\n            data_settings->max_bytes_to_merge_at_max_space_in_pool,\r\n            static_cast<double>(free_entries) / data_settings->number_of_free_entries_in_pool_to_lower_max_size_of_merge);\r\n\r\n    return std::min(max_size, static_cast<UInt64>(data.getStoragePolicy()->getMaxUnreservedFreeSpace() / DISK_USAGE_COEFFICIENT_TO_SELECT));\r\n}\r\nAgain, thanks for your help!"
      }
    ]
  },
  {
    "number": 26242,
    "title": "how to write the sql using clickhouse, if i want to get the aggregate results fo each items select from table?",
    "created_at": "2021-07-12T10:16:19Z",
    "closed_at": "2022-10-19T21:55:26Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/26242",
    "body": "search all the ways from google to sf, no same questions.\r\n\r\nsay i have a table\uff0cwhich is about 1T large:\r\n\r\nuser_id|  pay_time | pay_info\r\n-|-|-\r\n1 | 1232323 | {'num':10, \"total\":100}\r\n1  |1232324 | {'num':11, \"total\":110}\r\n1  |1232325  |{'num':12, \"total\":120}\r\n2  |1232326 | {'num':13, \"total\":130}\r\n2  |1232327 | {'num':14, \"total\":140}\r\n2  |1232328 | {'num':15, \"total\":150}\r\n2  |1232329 | {'num':16, \"total\":160}\r\n\r\nhow i get each user' sum total or sum num when he make payments,   he has already spent, which is pay_time less than this current payment's paytime. results as follows:\r\n\r\nuser_id | pay_time | sum_num| sum_total\r\n-|-|-|-\r\n1 | 1232323  |0 |0\r\n1|  12323234 |10| 100\r\n1 | 12323234 |21 |210\r\n2 | 1232326  |0 |0 \r\n2| 1232327   |13 |130\r\n2  |1232328  |27 |270\r\n2 | 1232329  |42 |420\r\n\r\ni have read the docs, but it seems no results.  \r\n\r\nand allow_experimental_window_functions seems not working, dont know why.\r\n\r\nthanks in advance\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/26242/comments",
    "author": "aohan237",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-07-12T12:49:27Z",
        "body": ">and allow_experimental_window_functions seems not working, dont know why.\r\n\r\nClickhouse version?  `select version()`"
      },
      {
        "user": "aohan237",
        "created_at": "2021-07-13T03:24:19Z",
        "body": "> > and allow_experimental_window_functions seems not working, dont know why.\r\n> \r\n> Clickhouse version? `select version()`\r\n\r\nversion is 21.7.2.7\r\n\r\ni cant use  \"set allow_experimental_window_functions=1\" in sql query to make it work,  but i config it in user.xml in profile, it works then.  \r\n"
      },
      {
        "user": "aohan237",
        "created_at": "2021-07-13T06:26:11Z",
        "body": "@den-crane\r\nnow that i can use window function to fulfill this.\r\nbut what should i do if i want exclude the current row?\r\n\r\nit seems that exclude is still unsupported,  is there any alternative ways?"
      },
      {
        "user": "akuzm",
        "created_at": "2021-07-15T14:21:21Z",
        "body": "`EXCLUDE CURRENT ROW` is not currently supported and we don't have a timeline for this, but for frames that don't go over current row, you can emulate it by switching from e.g. `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` to `ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING`."
      },
      {
        "user": "aohan237",
        "created_at": "2021-07-16T02:53:09Z",
        "body": "> `EXCLUDE CURRENT ROW` is not currently supported and we don't have a timeline for this, but for frames that don't go over current row, you can emulate it by switching from e.g. `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` to `ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING`.\r\n\r\nthanks\r\n\r\nwhat if i want condition sum, i only want to sum pay_time less than start time,  as follows: how do i fulfill this\r\n\r\nuser_id |start_time| pay_time | pay_info\r\n-- | --|-- | --\r\n1 | 2|5 | {'num':10, \"total\":100}\r\n1 | 2|2 | {'num':11, \"total\":110}\r\n1 | 3|3 | {'num':12, \"total\":120}\r\n1 | 3|3 | {'num':13, \"total\":130}\r\n\r\nresults like this\r\n\r\nuser_id |start_time| pay_time | sum_num | sum_total\r\n-- | --|-- | -- | --\r\n1 | 2|5 | 0|0\r\n1 | 2|2 |0|0\r\n1 | 3|3 |11|100\r\n1 | 4|4 |23|230\r\n\r\n\r\n\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-02-07T01:06:03Z",
        "body": "@akuzm Could you please help with this question?"
      },
      {
        "user": "akuzm",
        "created_at": "2022-02-07T09:44:17Z",
        "body": "> @akuzm Could you please help with this question?\r\n\r\nThis needs a `FILTER` clause which CH doesn't have, but it does have `-If` combinator that has a similar effect.  So `sumIf(payInfo['total'], pay_time < start_time)`."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-10-19T21:59:18Z",
        "body": "@akuzm We have added the FILTER clause:\r\n\r\n```\r\nmilovidov-desktop :) SELECT sum(number) FILTER(WHERE number % 2 = 0) FROM numbers(10)\r\n\r\nSELECT sumIf(number, (number % 2) = 0)\r\nFROM numbers(10)\r\n\r\nQuery id: 94a0a8ac-1a93-4ff1-b9b9-f043959893c0\r\n\r\n\u250c\u2500sumIf(number, equals(modulo(number, 2), 0))\u2500\u2510\r\n\u2502                                          20 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      }
    ]
  },
  {
    "number": 25953,
    "title": "Is it possible to change zkpath for a ReplicatedMergeTree table?",
    "created_at": "2021-07-03T18:14:38Z",
    "closed_at": "2021-07-03T22:53:05Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25953",
    "body": "At first I have created a distributed table (let us name it **table_A**). And then I decided to move the data to a ReplicatedMergeTable. I did it as below:\r\n\r\n`create table table_B as table_A engine=ReplicatedMergeTree('/clickhouse/tables/01/{database}/{table}', '{replica}');`\r\n`insert into table_B select * from table_A;`\r\n\r\nSo far so good. After that, I drop the distributed table and rename the new one to the old name.\r\n\r\n`drop table table_A;`\r\n`rename table table_B to table_A;`\r\n\r\nThen I tried to insert some data into table_A, it threw an error:\r\n\r\n`Table is in readonly mode (zookeeper path: /clickhouse/tables/01/default/table_B)`\r\n\r\nI have tried to find some command to fix the zookeeper path for this table as alter table and etc. but nothing found.\r\n\r\nIs it a solution to make it out?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25953/comments",
    "author": "lucasguo",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-07-03T18:34:26Z",
        "body": "There is no good solution. ZK does not allow to rename znodes. \r\nYou can manually fix table_A.sql file in metadata folder. \r\n\r\nThis is the impossible situation in the latest CH versions. Because CH now expands {database}/{table} macros on a create command, so in the table's metadata (.sql) will be the real name `table_B`, instead of `{table}` macro.\r\n\r\nAlso the latest CH versions allow to use Atomic databases and uuid as ZK path. You  can simply omit ZK path parameter `ReplicatedMergeTree()`"
      },
      {
        "user": "lucasguo",
        "created_at": "2021-07-04T04:44:33Z",
        "body": "> There is no good solution. ZK does not allow to rename znodes.\r\n> You can manually fix table_A.sql file in metadata folder.\r\n> \r\n> This is the impossible situation in the latest CH versions. Because CH now expands {database}/{table} macros on a create command, so in the table's metadata (.sql) will be the real name `table_B`, instead of `{table}` macro.\r\n> \r\n> Also the latest CH versions allow to use Atomic databases and uuid as ZK path. You can simply omit ZK path parameter `ReplicatedMergeTree()`\r\n\r\nThanks for your fast response! After changing the table_A.sql under /clickhouse/metadata/default/ by replacing the {table} macro to the real table name,  and restarting the clickhouse service, everything goes fine now."
      }
    ]
  },
  {
    "number": 25897,
    "title": "clickhouse-local stops working on 21.4.6.55",
    "created_at": "2021-07-01T14:05:33Z",
    "closed_at": "2021-07-01T19:00:38Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25897",
    "body": "Hello all,\r\n\r\nI cannot run clickhouse-local anymore on version 21.4.6.55 but I can run it without any issue on version 18.6.0.\r\n\r\nQuestion 1> What is the issue here and How I can fix it?\r\n\r\nQuestion 2> Why the clickhouse-local requires to write into config.xml?\r\n\r\nThank you\r\n\r\n```\r\n$ echo -e \"1,2\\n3,4\" | clickhouse-local --structure \"a Int64, b Int64\" --input-format \"CSV\" --query \"SELECT * FROM table\"\r\nProcessing configuration file 'config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nCouldn't save preprocessed config to /var/lib/clickhouse/preprocessed_configs/config.xml: Access to file denied: /var/lib/clickhouse/preprocessed_configs/config.xml\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nPoco::Exception. Code: 1000, e.code() = 13, e.displayText() = Access to file denied: /var/log/clickhouse-server/clickhouse-server.log, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x12683f3c in /usr/bin/clickhouse\r\n1. Poco::FileStreamBuf::open(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned int) @ 0x126954e2 in /usr/bin/clickhouse\r\n2. Poco::FileOutputStream::FileOutputStream(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned int) @ 0x1269652d in /usr/bin/clickhouse\r\n3. Poco::LogFileImpl::LogFileImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1269f4c2 in /usr/bin/clickhouse\r\n4. Poco::FileChannel::unsafeOpen() @ 0x126892ee in /usr/bin/clickhouse\r\n5. Poco::FileChannel::open() @ 0x126891e1 in /usr/bin/clickhouse\r\n6. Loggers::buildLoggers(Poco::Util::AbstractConfiguration&, Poco::Logger&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x89c6483 in /usr/bin/clickhouse\r\n7. DB::LocalServer::initialize(Poco::Util::Application&) @ 0x89036a5 in /usr/bin/clickhouse\r\n8. Poco::Util::Application::run() @ 0x125cf046 in /usr/bin/clickhouse\r\n9. mainEntryClickHouseLocal(int, char**) @ 0x890e50b in /usr/bin/clickhouse\r\n10. main @ 0x87f1dce in /usr/bin/clickhouse\r\n11. __libc_start_main @ 0x22505 in /usr/lib64/libc-2.17.so\r\n12. _start @ 0x87bd06e in /usr/bin/clickhouse\r\n (version 21.4.6.55 (official build))\r\n\r\n$ clickhouse-local --version\r\nClickHouse client version 21.4.6.55 (official build).\r\n\r\n-rw-r--r-- 1 clickhouse clickhouse 16235 May 10 08:24 /var/lib/clickhouse/preprocessed_configs/config.xml\r\n```\r\n\r\n=============================\r\n```\r\n$ echo -e \"1,2\\n3,4\" | clickhouse-local --structure \"a Int64, b Int64\" --input-format \"CSV\" --query \"SELECT * FROM table\"\r\n1       2\r\n3       4\r\n$ clickhouse-local --version\r\nClickHouse client version 18.6.0.\r\n\r\n-rw-r--r-- 1 root root 16142 Apr 15  2019 /var/lib/clickhouse/preprocessed_configs/config.xml\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25897/comments",
    "author": "Jack012a",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-01T18:03:09Z",
        "body": "TLDR: run `clickhouse-local` from another directory, without `config.xml`.\r\n\r\nYou are running `clickhouse-local`, it looks for a config in current directory. You are running it inside a directory where the server's config is located. But `clickhouse-server` config does not make sense for `clickhouse-local`.\r\n\r\nIn most cases `clickhouse-local` does not need any config at all."
      },
      {
        "user": "Jack012a",
        "created_at": "2021-07-01T18:11:54Z",
        "body": "This is not true. I didn't run clickhouse-local from a directory where it has config.xml."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-01T18:13:26Z",
        "body": "But it managed to find\r\n`Processing configuration file 'config.xml'.`\r\nanyhow.\r\n\r\nType `ls -l` in the current working directory.\r\n"
      },
      {
        "user": "Jack012a",
        "created_at": "2021-07-01T18:37:20Z",
        "body": "@alexey-milovidov You are right. By accident, my home directory does have a copy of the config.xml. Thank you very much!"
      }
    ]
  },
  {
    "number": 25698,
    "title": "Populating a materialized view results in unexpected values",
    "created_at": "2021-06-25T06:19:21Z",
    "closed_at": "2021-06-28T10:52:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25698",
    "body": "Please forgive me if i'm missing something fairly obvious here.\r\n\r\n**Describe the unexpected behaviour**\r\nPopulating a materialized view results in unexpected values.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: 21.3.13.9\r\n\r\n```bash\r\ndocker run -d --name some-clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:21.3.13.9\r\ndocker exec -it some-clickhouse-server clickhouse-client\r\n\r\n:) CREATE TABLE tmp_aggregated\r\n(\r\n  `window_start` DateTime64 Codec(DoubleDelta, LZ4),\r\n  `metrics_name` Array(LowCardinality(String)) Codec(LZ4),\r\n  `organization_id` LowCardinality(String) Codec(LZ4)\r\n)\r\nENGINE MergeTree()\r\nPARTITION BY (organization_id) ORDER BY (window_start)\r\n\r\n:) create materialized view tmp_names (\r\n  organization_id LowCardinality(String),\r\n  metric_names SimpleAggregateFunction(groupUniqArrayArray, Array(String)),\r\n  window_start_day DateTime64\r\n)\r\nEngine=MergeTree()\r\norder by (window_start_day)\r\npopulate as select\r\n  organization_id,\r\n  groupUniqArray(metrics_name),\r\n  toStartOfDay(window_start)\r\nfrom tmp_aggregated array join metrics_name\r\ngroup by toStartOfDay(window_start), organization_id\r\n\r\n:) insert into tmp_aggregated values ('2021-06-24 07:15:09.000', ['metric1'], 'org-id');\r\n\r\n:) select * from tmp_names \\G\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\norganization_id:  org-id\r\nmetric_names:     []\r\nwindow_start_day: 1970-01-01 00:00:00\r\n\r\n:) select * from tmp_aggregated \\G\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nwindow_start:    2021-06-24 07:15:09.000\r\nmetrics_name:    ['metric1']\r\norganization_id: org-id\r\n\r\n```\r\n\r\n**Expected behavior**\r\n\r\nWhen executing `select * from tmp_names \\G` I expected values stored in tmp_names to be:\r\n\r\n```\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\norganization_id:  org-id\r\nmetric_names:      ['metric1']\r\nwindow_start_day: 2021-06-24 07:15:09.000\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25698/comments",
    "author": "shivamMg",
    "comments": [
      {
        "user": "SaltTan",
        "created_at": "2021-06-26T20:05:09Z",
        "body": "The names of the columns in the MV query and the destination table must match:\r\n\r\n as select\r\n  organization_id,\r\n  groupUniqArray(metrics_name) **as metric_names**,\r\n  toStartOfDay(window_start) **as window_start_day**\r\n\r\n"
      },
      {
        "user": "shivamMg",
        "created_at": "2021-06-28T10:52:20Z",
        "body": "Thank you."
      }
    ]
  },
  {
    "number": 25322,
    "title": "Kafka _timestamp / _timestamp_ms not working?",
    "created_at": "2021-06-16T10:57:19Z",
    "closed_at": "2021-06-16T11:21:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25322",
    "body": "**Describe the bug**\r\n\r\nAppears that _timestamp and _timestamp_ms virtual columns are returning 0 for kafka. Perhaps it is somehow due to JSONEachRow format?\r\n\r\nIssue is in docker 21.6.4.26 and 21.2.2.8 at least.\r\n\r\nI'm using amazon MSK 2.6.1\r\n\r\n**How to reproduce**\r\n\r\n```\r\ncreate table kafka ( name String ) ENGINE=Kafka() SETTINGS kafka_broker_list = '...', kafka_topic_list = 'test-events', kafka_group_name='test', kafka_format='JSONEachRow';\r\ncreate table t (time DateTime64(3), name String) ENGINE=MergeTree() ORDER BY tuple();\r\ncreate materialized view kafka_mv to t AS select _timestamp_ms, name FROM kafka;\r\n```\r\n\r\n```\r\necho '{\"name\":\"test\"}' | bin/kafka-console-producer.sh ...\r\n```\r\n\r\nClickhouse table just looks like:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u252c\u2500name\u2500\u2510\r\n\u2502 1970-01-01 00:00:00.000 \u2502 test \u2502\r\n...\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIf checking with kafka consumer:\r\n\r\n```\r\nbin/kafka-console-consumer.sh ... --from-beginning --property print.timestamp=true\r\nCreateTime:1623839664050        {\"name\":\"test\"}\r\nCreateTime:1623839989619        {\"name\":\"test\"}\r\nCreateTime:1623840562909        {\"name\":\"test\"}\r\nCreateTime:1623840285113        {\"name\":\"test\"}\r\n```\r\n\r\nI see the same with _timestamp col",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25322/comments",
    "author": "mzealey",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2021-06-16T11:18:18Z",
        "body": "Mapping of columns in mv happens by names, not by positions.\r\n\r\nSo that should help:\r\n```\r\ncreate materialized view kafka_mv to t AS select _timestamp_ms as time, name FROM kafka;\r\n```"
      },
      {
        "user": "mzealey",
        "created_at": "2021-06-16T11:21:17Z",
        "body": "Gah yes that fixed it. Normally I thought there were errors when creating a mv or inserting into it like this, but I did not see anything on console or in the logs :-/"
      }
    ]
  },
  {
    "number": 24575,
    "title": "Question about shared_ptr_helper",
    "created_at": "2021-05-27T13:44:21Z",
    "closed_at": "2021-05-27T23:41:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24575",
    "body": "I have a question about this template class.\r\n```c++\r\n/** Allows to make std::shared_ptr from T with protected constructor.\r\n  *\r\n  * Derive your T class from shared_ptr_helper<T> and add shared_ptr_helper<T> as a friend\r\n  *  and you will have static 'create' method in your class.\r\n  */\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\n\r\nMany places use this pattern\r\nclass StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageReplicatedMergeTree>, public MergeTreeData\r\n{\r\n    friend struct ext::shared_ptr_helper<StorageReplicatedMergeTree>;\r\n..\r\n};\r\n\r\nBut I think the friend class is redundant.\r\n\r\nFor example\r\n\r\n#include <iostream>\r\n#include <memory>\r\n\r\nusing namespace std;\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\nclass A:public shared_ptr_helper<A>{\r\n    public:\r\n    int a;\r\n};\r\n\r\nint main(){\r\n    std::shared_ptr<A> aObj = A::create();\r\n    cout << aObj->a << endl;\r\n}\r\n```\r\n\r\nThis code also can use the create method. Here is the question, what does the friend class do in this pattern. \r\nThanks.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24575/comments",
    "author": "wangzhen11aaa",
    "comments": [
      {
        "user": "kitaisreal",
        "created_at": "2021-05-27T14:37:36Z",
        "body": "@wangzhen11aaa we make ext::shared_ptr_helper friend because we want to create protected or private constructors, but force clients to use shared_ptr_helper create method for object construction."
      },
      {
        "user": "wangzhen11aaa",
        "created_at": "2021-05-27T23:41:41Z",
        "body": "#include <iostream>\r\n#include <memory>\r\n\r\nusing namespace std;\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\nclass A:public shared_ptr_helper<A>{\r\n    friend shared_ptr_helper<A>;\r\n    public:\r\n    int a;\r\n    protected:\r\n        A()=default;\r\n};\r\n\r\nint main(){\r\n    std::shared_ptr<A> aObj = A::create();\r\n    cout << aObj->a << endl;\r\n}\r\nOK"
      }
    ]
  },
  {
    "number": 24251,
    "title": " DB::Exception: Aggregate function sum(postition) is found inside another aggregate function in query: While processing sum(postition) AS postition",
    "created_at": "2021-05-18T14:52:41Z",
    "closed_at": "2021-05-22T10:53:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24251",
    "body": "Hi\uff0c\r\n    When I execute this query sql :\r\nSELECT\r\n    avg(postition) AS avg,\r\n    sum(postition) AS postition\r\nFROM system.columns;\r\nand the exception happened,which was:\r\nReceived exception from server (version 21.4.4):\r\nCode: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function sum(postition) is found inside another aggregate function in query: While processing sum(postition) AS postition. \r\n\r\nBut this sql can run correctly in MySQL. This is Clickhouse's  special syntax ? \r\n        Thanks.\r\n                     Best Regards.\r\n                             Eward\r\n ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24251/comments",
    "author": "cwh2008",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2021-05-19T17:17:45Z",
        "body": "You can set  `prefer_column_name_to_alias = 1`."
      },
      {
        "user": "cwh2008",
        "created_at": "2021-05-22T10:55:07Z",
        "body": "Hi\uff0camosbird. Thanks a lot.\r\nYour solution is the key to this quetion."
      }
    ]
  },
  {
    "number": 24072,
    "title": "Should uniq or uniqCombined be used for sharded, aggregated uniques?",
    "created_at": "2021-05-12T20:21:43Z",
    "closed_at": "2021-05-13T16:11:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24072",
    "body": "the documentation for uniqCombined says:\r\n\r\n```\r\nCompared to the uniq function, the uniqCombined:\r\nConsumes several times less memory.\r\nCalculates with several times higher accuracy.\r\nUsually has slightly lower performance. In some scenarios, uniqCombined can perform better than uniq, for example, with distributed queries that transmit a large number of aggregation states over the network.\r\n```\r\n\r\nThe documentation for uniq says:\r\n\r\n```\r\nWe recommend using this function in almost all scenarios.\r\n```\r\n\r\nWe have \"distributed queries that transmit a large number of aggregation states\" -- so should we be using uniqCombined?  If so should the \"uniq\" recommendation be updated to reflect that?  Or is the uniqCombined documentation out of date?\r\n\r\n(On a side note, I don't suppose there's a way to convert uniqState to uniqCombinedState for existing data?)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24072/comments",
    "author": "genzgd",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-05-12T22:10:22Z",
        "body": ">If so should the \"uniq\" recommendation be updated to reflect that? \r\n>Or is the uniqCombined documentation out of date?\r\n\r\nMilovidov: \r\nuniqCombined with default values is somewhat more accurate than uniq.\r\nuniqCombined is more accurate and takes up less memory and disk space. But usually slower.\r\n\r\n\r\nI (Denis) suggest to try `uniqCombined64(15)` -- it boosts queries up to 10 times for my distributed queries (1Gbit network).\r\nAnd the error is <2% in comparison with uniqExact.\r\n\r\nAlso about states:\r\n```\r\ndisk space for example\r\n2.2M    uniqHLL12State\r\n58M   uniqCombinedState(20)\r\n15M   uniqCombinedState(15)\r\n140M    uniqState\r\n\r\nSpeed\r\nSELECT uniqMerge(us) AS unique FROM mvz WHERE z = '555' GROUP BY d\r\n\u250c\u2500unique\u2500\u2510\r\n\u2502 297580 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.031 sec.\r\n\r\nSELECT uniqHLL12Merge(uh) AS unique FROM mvz WHERE z = '555' GROUP BY d\r\n\u250c\u2500unique\u2500\u2510\r\n\u2502 297323 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.002 sec.\r\n\r\nSELECT uniqCombinedMerge15(uc) AS unique FROM mvz WHERE z = '555' GROUP BY d\r\n\u250c\u2500unique\u2500\u2510\r\n\u2502 298636 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.003 sec.\r\n```\r\n\r\n\r\n>(On a side note, I don't suppose there's a way to convert uniqState to uniqCombinedState for existing data?)\r\n\r\nNo way. \r\nAMilovidov thinks that it's possible in theory, he can do it :) (select states into application / recalculate / insert into another table)."
      },
      {
        "user": "genzgd",
        "created_at": "2021-05-13T16:11:18Z",
        "body": "I'll give uniqCombined(64) a shot.  Thanks for the detailed response!"
      }
    ]
  },
  {
    "number": 24034,
    "title": "how to pass client settings such like max_memory_useage for  http query",
    "created_at": "2021-05-12T01:26:43Z",
    "closed_at": "2021-05-12T05:12:15Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24034",
    "body": "in some cases,i can only run select  query via http interface ,like curl or wget,but some times it will out of memory limit\r\n\r\nhow to set these client settings just like in the clickhouse-client when using http interface? \r\n\r\n\r\nby setting  a small value in settings at the end of  select  query,it shows Memory limit (for query) exceeded\r\n\r\n\r\nSELECT\r\n    CAST('2021-05-10', 'date') AS log_date,\r\n    countDistinct(map_uid) AS uid_cnt\r\nFROM\r\n(\r\n    SELECT\r\n        map_uid,\r\n        countDistinct(type) AS type_cnt\r\n    FROM\r\n    (\r\n        SELECT\r\n            map_uid,\r\n            if(os_type = 'Mobile', 'mobile', 'desktop') AS type\r\n        FROM login\r\n        WHERE (toDate(log_time) >= subtractDays(CAST('2021-05-10', 'date'), 28)) AND (toDate(log_time) <= subtractDays(CAST('2021-05-10', 'date'), 1))\r\n        GROUP BY\r\n            map_uid,\r\n            type\r\n    )\r\n    GROUP BY map_uid\r\n)\r\nWHERE type_cnt = 2\r\nGROUP BY log_date\r\nSETTINGS max_memory_usage = 20\r\n\r\n\u2193 Progress: 12.01 million rows, 156.14 MB (37.41 million rows/s., 486.35 MB/s.)  0%\r\nReceived exception from server (version 20.9.3):\r\nCode: 241. DB::Exception: Received from  DB::Exception: Received from clickhouse_node4_4_1:9000. DB::Exception: Memory limit (for query) exceeded: would use 4.22 MiB (attempt to allocate chunk of 4421564 bytes), maximum: 20.00 B.\r\n\r\n\r\n\r\nbut i change the  max_memory_usage value to a very big value such as 10000000000000000, it doesn't work.\r\n\r\nfinaly it show the errors:\r\nmemory limit Received exception from server (version 20.9.3):\r\nCode: 241. DB::Exception: Received from 1 DB::Exception: Memory limit (for query) exceeded: would use 9.38 GiB (attempt to allocate chunk of 133939184 bytes), maximum: 9.31 GiB: While executing AggregatingTransform.\r\n\r\n\r\nand i use set max_memory_usage=10000000000000;  then run the query it works.\r\n\r\nso ,how to set the client settings via http interface ,pls help thx.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24034/comments",
    "author": "windylcx",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-05-12T02:10:07Z",
        "body": "In URL parameter:\r\n\r\n`...&max_memory_usage=10000000000000`"
      },
      {
        "user": "windylcx",
        "created_at": "2021-05-12T03:57:54Z",
        "body": "> In URL parameter:\r\n> \r\n> `...&max_memory_usage=10000000000000`\r\n\r\nit works, thank you !"
      }
    ]
  },
  {
    "number": 24014,
    "title": "How to append an element after each array in a 2-D Array",
    "created_at": "2021-05-11T09:36:23Z",
    "closed_at": "2021-05-14T06:50:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24014",
    "body": "Suppose we have two arrays with the same length.  \r\nA = [[1,2],[3,4]].  \r\nB = [5,6].  \r\nIs there a easy way to get a new array C = [[1,2,5],[3,4,6]]?    \r\n     \r\nGenerally,   \r\nif A is with a shape of (n,a) and B is with a shape of (n,b),    \r\ncan we get a new array C with a shape of (n,a+b)?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24014/comments",
    "author": "JIANCHUJUN",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2021-05-11T11:11:49Z",
        "body": "arrayMap(x,y->arrayConcat(x,[y]), [[1,2],[3,4]],[5,6])"
      },
      {
        "user": "l1t1",
        "created_at": "2021-05-11T11:20:03Z",
        "body": "select arrayMap(x,y->arrayPushBack(x,y), [[1,2],[3,4]],[5,6])"
      },
      {
        "user": "JIANCHUJUN",
        "created_at": "2021-05-14T06:50:52Z",
        "body": "Thanks! It solves the prolem."
      }
    ]
  },
  {
    "number": 23958,
    "title": "Dictionary with postgres source fails",
    "created_at": "2021-05-08T12:02:31Z",
    "closed_at": "2021-05-12T07:50:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/23958",
    "body": "Hi,\r\n\r\nI created a dictionary with a postgres source and creation works fine. But when I try to query it, it says that the source table in postgres doesn't exist.\r\n\r\n```\r\n SQL Error [156]: ClickHouse exception, code: 156, host: my.clickhouse.server, port: 8123; Code: 156, e.displayText() = DB::Exception: Failed to load dictionary 'chdatabase.dictionary_table': std::exception. Code: 1001, type: pqxx::undefined_table, e.what() = ERROR:  relation \"pgschema.pgtable\" does not exist\r\nLINE 1: ...e\", \"iso3_code\", \"continent_id\", \"adjective\" FROM \"analytics...\r\n                                                             ^\r\n, (version 21.3.9.83 (official build))\r\n```\r\n\r\nThe dictionary is configured using a DDL statement:\r\n\r\n```\r\nCREATE DICTIONARY chdatabase.dictionary_table (\r\n\tid Int32,\r\n\tname String,\r\n\tiso2_code String,\r\n\tiso3_code String\r\n)\r\nPRIMARY KEY id\r\nSOURCE(POSTGRESQL(\r\n  port 5432 \r\n  host 'postgreshost' \r\n  user  'postgresuser'\r\n  password 'postgrespassword'\r\n  db 'pgdatabase'\r\n  table 'pgschema.pgtable'\r\n  )\r\n)\r\nLIFETIME(MIN 300 MAX 360)\r\nLAYOUT(COMPLEX_KEY_HASHED());\r\n```\r\n\r\nI've confirmed that the user defined in in the postgresql source has access to the table and can query it.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/23958/comments",
    "author": "electrical",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2021-05-08T12:48:16Z",
        "body": "@electrical, it does not work because ability to specify postgres `schema` of table was added only for table engine / table function, not for dictionary source. I'll add it for dictionary also.."
      },
      {
        "user": "electrical",
        "created_at": "2021-05-08T13:02:22Z",
        "body": "@kssenii Ahh okay :-) Thank you.\r\nI made the wrong assumption it was in there.\r\nDo you have an idea if this will be in an LTS release soon or should i revert back to the ODBC driver?"
      },
      {
        "user": "kssenii",
        "created_at": "2021-05-08T13:20:34Z",
        "body": "@electrical I suppose it will not get into LTS, because it is not a bug fix.."
      },
      {
        "user": "electrical",
        "created_at": "2021-05-08T13:42:06Z",
        "body": "@kssenii I see. \r\nI would partially disagree with since it's a missing function for the postgresql source in dictionaries while, as you say, it's there for the table engine / table function."
      },
      {
        "user": "kssenii",
        "created_at": "2021-05-08T13:46:22Z",
        "body": "@electrical hm, I agree, may be it will get into lts."
      },
      {
        "user": "kssenii",
        "created_at": "2021-05-14T08:59:43Z",
        "body": "@electrical, the fix is in 21.3."
      },
      {
        "user": "electrical",
        "created_at": "2021-05-14T09:23:44Z",
        "body": "@kssenii thank you so much!"
      }
    ]
  },
  {
    "number": 23914,
    "title": "all_part merge without insert or alter",
    "created_at": "2021-05-06T06:39:25Z",
    "closed_at": "2021-05-06T13:45:54Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/23914",
    "body": "I did not perform any INSERT and ALTER operations. Today, I monitored several background parts to perform a merge operation. Why is this merge performed? Does this operation have any effect? Is there any way to avoid this operation, because when you merge the disk space will suddenly surge and then fall?\r\n\r\nI hope someone can help me solve this problem, thank you very much\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/23914/comments",
    "author": "sileiH",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-05-06T12:40:01Z",
        "body": "MergeScheduler wakes up every second and computes does it need to merge parts or not, and if it does need, then scheduler a new merge and merge parts."
      },
      {
        "user": "sileiH",
        "created_at": "2021-05-06T12:45:31Z",
        "body": "Sometimes I don't need this merge, and it causes the disks to explode and crash. Will he merge the part from different disks? My each disk capacity is relatively small, is there any way to avoid this?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-05-06T13:45:22Z",
        "body": ">Sometimes I don't need this merge, and it causes the disks to explode and crash. \r\n\r\nCH MergeScheduler is hardcoded and does not have such parameters. Try to reduce background_pool_size (users / default profile), and set it to 8\r\n```\r\nbackground_pool_size arg                                       Number of threads performing background work for tables (for example, merging in merge tree). Only has meaning at server\r\n```\r\n\r\n>Will he merge the part from different disks? \r\n\r\nIt depends on how you configured storage policy.\r\n\r\n>My each disk capacity is relatively small, is there any way to avoid this?\r\n\r\nNo\r\n"
      },
      {
        "user": "sileiH",
        "created_at": "2021-05-07T01:39:21Z",
        "body": "Thank you"
      }
    ]
  },
  {
    "number": 23698,
    "title": "Kernel panic: \"Bad page state in process BgSchPool\" unsing KafkaEngine",
    "created_at": "2021-04-27T14:38:05Z",
    "closed_at": "2021-04-29T14:04:00Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/23698",
    "body": "**Describe the bug**\r\nSpontaneous kernel panic using tables with Kafka engine\r\n\r\n**How to reproduce**\r\n* ClickHouse server version 20.8.6 revision 54438\r\n* Clickhouse cluster of 3 replicas\r\n* Kafka cluster of 3 replicas\r\n* 60 tables with kafka engine + materialized views to ReplicatedMergeTree target tables\r\n* changed background_schedule_pool_size=100\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2021-04-26 @ 16:18:53.709513,935,843\r\nhost\r\n2021.04.26 16:18:53.709596 [ 28230 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128404_31024 covered by 202104_0_128407_31027 (state Committed)\r\n2021-04-26 @ 16:18:55.479513,936,026\r\nhost\r\n2021.04.26 16:18:55.479125 [ 28239 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128408_31028 covered by 202104_0_128411_31031 (state Committed)\r\n2021-04-26 @ 16:20:42.197513,936,209\r\nhost\r\n2021.04.26 16:20:42.197118 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Checking part 202104_0_128603_31217\r\n2021-04-26 @ 16:20:42.212513,936,362\r\nhost\r\n2021.04.26 16:20:42.212608 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 202104_0_128603_31217.\r\n2021-04-26 @ 16:20:42.217513,936,541\r\nhost\r\n2021.04.26 16:20:42.217504 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 202104_0_128603_31217. Hoping that it will eventually appear as a result of a merge.\r\n2021-04-26 @ 16:20:42.256513,936,827\r\nhost\r\n2021.04.26 16:20:42.256184 [ 28228 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128601_31215 covered by 202104_0_128603_31217 (state Committed)\r\n2021.04.26 16:26:49.174485 [ 17341 ] {} <Error> ServerErrorHandler: Poco::Exception. Code: 1000, e.code() = 104, e.displayText() = Connection reset by peer, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::IOException::IOException(int) @ 0x18bc80ff in /usr/bin/clickhouse\r\n1. Poco::Net::ConnectionResetException::ConnectionResetException(int) @ 0x18ab943d in /usr/bin/clickhouse\r\n2. ? @ 0x18ad61ea in /usr/bin/clickhouse\r\n3. Poco::Net::SocketImpl::receiveBytes(void*, int, int) @ 0x18ad48e3 in /usr/bin/clickhouse\r\n4. Poco::Net::HTTPSession::receive(char*, int) @ 0x18aa2c58 in /usr/bin/clickhouse\r\n5. Poco::Net::HTTPSession::get() @ 0x18aa2cc3 in /usr/bin/clickhouse\r\n6. Poco::Net::HTTPHeaderStreamBuf::readFromDevice(char*, long) @ 0x18a9787a in /usr/bin/clickhouse\r\n7. Poco::BasicBufferedStreamBuf<char, std::__1::char_traits<char>, Poco::Net::HTTPBufferAllocator>::underflow() @ 0x18a941d8 in /usr/bin/clickhouse\r\n8. std::__1::basic_streambuf<char, std::__1::char_traits<char> >::uflow() @ 0x19b3ef0e in ?\r\n9. std::__1::basic_istream<char, std::__1::char_traits<char> >::get() @ 0x19b46506 in ?\r\n10. Poco::Net::HTTPRequest::read(std::__1::basic_istream<char, std::__1::char_traits<char> >&) @ 0x18a9a957 in /usr/bin/clickhouse\r\n11. Poco::Net::HTTPServerRequestImpl::HTTPServerRequestImpl(Poco::Net::HTTPServerResponseImpl&, Poco::Net::HTTPServerSession&, Poco::Net::HTTPServerParams*) @ 0x18aa0d85 in /usr/bin/clickhouse\r\n12. Poco::Net::HTTPServerConnection::run() @ 0x18a9f74c in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerConnection::start() @ 0x18add87b in /usr/bin/clickhouse\r\n14. Poco::Net::TCPServerDispatcher::run() @ 0x18addd0b in /usr/bin/clickhouse\r\n15. Poco::PooledThread::run() @ 0x18c5c7e6 in /usr/bin/clickhouse\r\n16. Poco::ThreadImpl::runnableEntry(void*) @ 0x18c57be0 in /usr/bin/clickhouse\r\n17. start_thread @ 0x7fa3 in /usr/lib/x86_64-linux-gnu/libpthread-2.28.so\r\n18. __clone @ 0xf94cf in /usr/lib/x86_64-linux-gnu/libc-2.28.so\r\n (version 20.8.6.6 (official build))\r\n\r\n2021-04-26 @ 16:26:49.1305200172.356694] BUG: Bad page state in process BgSchPool  pfn:1fcdb35\r\n2021-04-26 @ 16:26:49.1305200172.394974] page:ffffe7c0ff36cd40 count:0 mapcount:0 mapping:ffff9324edb25b00 index:0x0\r\n2021-04-26 @ 16:26:49.1305200172.444695] flags: 0x17fffc000000000()\r\n2021-04-26 @ 16:26:49.1305200172.468918] raw: 017fffc000000000 dead000000000100 dead000000000200 ffff9324edb25b00\r\n2021-04-26 @ 16:26:49.1305200172.517059] raw: 0000000000000000 0000000000270027 00000000ffffffff ffff93406e2dc000\r\n2021-04-26 @ 16:26:49.1305200172.565211] page dumped because: page still charged to cgroup\r\n2021-04-26 @ 16:26:49.1305200172.601395] page->mem_cgroup:ffff93406e2dc000\r\n2021-04-26 @ 16:26:49.1305200172.629259] Modules linked in: drbg ansi_cprng authenc echainiv xfrm6_mode_tunnel xfrm4_mode_tunnel binfmt_misc xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo ifb sch_fq_codel bonding ip6table_mangle nf_log_ipv6 ip6t_REJECT nf_reject_ipv6 iptable_nat nf_nat_ipv4 nf_nat xt_connmark xt_DSCP xt_length xt_dscp iptable_mangle nf_log_ipv4 nf_log_common xt_set ipt_REJECT nf_reject_ipv4 xt_owner xt_multiport xt_LOG xt_limit xt_policy xt_conntrack xt_tcpudp ip_set_hash_ip ip_set nfnetlink ip6table_filter ip6_tables iptable_filter nls_ascii nls_cp437 vfat fat intel_rapl sb_edac x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass mgag200 crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel intel_cstate drm_kms_helper mxm_wmi intel_uncore efi_pstore mei_me intel_rapl_perf\r\n2021-04-26 @ 16:26:49.1305200173.070265]  iTCO_wdt pcc_cpufreq drm efivars pcspkr joydev evdev mei iTCO_vendor_support sg ioatdma ipmi_ssif wmi acpi_power_meter acpi_pad button nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 libcrc32c ipmi_si ipmi_devintf ipmi_msghandler efivarfs ip_tables x_tables autofs4 ext4 crc16 mbcache jbd2 crc32c_generic fscrypto ecb dm_mod hid_generic usbhid hid sd_mod ahci libahci xhci_pci ehci_pci xhci_hcd ehci_hcd crc32c_intel libata megaraid_sas aesni_intel aes_x86_64 crypto_simd i40e(OE) usbcore igb cryptd i2c_algo_bit scsi_mod usb_common lpc_ich i2c_i801 glue_helper mfd_core dca\r\n2021-04-26 @ 16:26:49.1305200173.070307] CPU: 20 PID: 28333 Comm: BgSchPool Tainted: G    B   W  OEL    4.19.0-13-amd64 #1 Debian 4.19.160-2\r\n2021-04-26 @ 16:26:49.1305200173.070312] Hardware name: Intel Corporation S2600WT2R/S2600WT2R, BIOS SE5C610.86B.01.01.0020.122820161512 12/28/2016\r\n2021-04-26 @ 16:26:49.1305200173.516116] Call Trace:\r\n2021-04-26 @ 16:26:49.1305200173.533853]  dump_stack+0x66/0x90\r\n2021-04-26 @ 16:26:49.1305200173.556753]  bad_page.cold.116+0x7f/0xb2\r\n2021-04-26 @ 16:26:49.1305200173.583290]  free_pcppages_bulk+0x4b4/0x660\r\n2021-04-26 @ 16:26:49.1305200173.611387]  free_unref_page_list+0x111/0x190\r\n2021-04-26 @ 16:26:49.1305200173.640536]  release_pages+0x215/0x450\r\n2021-04-26 @ 16:26:49.1305200173.666167]  tlb_flush_mmu_free+0x3d/0x60\r\n2021-04-26 @ 16:26:49.1305200173.693343]  arch_tlb_finish_mmu+0x89/0x100\r\n2021-04-26 @ 16:26:49.1305200173.721564]  tlb_finish_mmu+0x1f/0x30\r\n2021-04-26 @ 16:26:49.1305200173.746536]  zap_page_range+0xde/0x140\r\n2021-04-26 @ 16:26:49.1305200173.772146]  ? do_futex+0xc8/0xbe0\r\n2021-04-26 @ 16:26:49.1305200173.795671]  __x64_sys_madvise+0x663/0x7c0\r\n2021-04-26 @ 16:26:49.1305200173.823237]  ? do_syscall_64+0x53/0x110\r\n2021-04-26 @ 16:26:49.1305200173.849265]  ? __ia32_sys_madvise+0x7c0/0x7c0\r\n2021-04-26 @ 16:26:49.1305200173.878400]  do_syscall_64+0x53/0x110\r\n2021-04-26 @ 16:26:49.1305200173.903382]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\r\n2021-04-26 @ 16:26:49.1305200173.936667] RIP: 0033:0x7f126714c2d7\r\n2021-04-26 @ 16:26:49.1305200173.961237] Code: ff ff ff ff c3 48 8b 15 b7 6b 0c 00 f7 d8 64 89 02 b8 ff ff ff ff eb bc 66 2e 0f 1f 84 00 00 00 00 00 90 b8 1c 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 89 6b 0c 00 f7 d8 64 89 01 48\r\n2021-04-26 @ 16:26:49.1305200174.078867] RSP: 002b:00007f12082ac648 EFLAGS: 00000206 ORIG_RAX: 000000000000001c\r\n2021-04-26 @ 16:26:49.1305200174.127252] RAX: ffffffffffffffda RBX: 0000000000008000 RCX: 00007f126714c2d7\r\n2021-04-26 @ 16:26:49.1305200174.173033] RDX: 0000000000000004 RSI: 000000000003c000 RDI: 00007f0aa007e000\r\n2021-04-26 @ 16:26:49.1305200174.218905] RBP: 00007f12082ada60 R08: 000000000003c000 R09: 0000000000000014\r\n2021-04-26 @ 16:26:49.1305200174.264703] R10: 0000000000000000 R11: 0000000000000206 R12: 00007f0763210540\r\n2021-04-26 @ 16:26:49.1305200174.310591] R13: 00007f12082ac6d0 R14: 00007f123c0008c0 R15: 00007f123c004510\r\n```\r\n\r\n**Additional context**\r\nWe faced the problem twice, but unfortunately we cannot reproduce it. The problem occurs spontaneously and rarely, but because of this, our server crashes.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/23698/comments",
    "author": "suvorovis",
    "comments": [
      {
        "user": "azat",
        "created_at": "2021-04-27T19:49:51Z",
        "body": "I doubt that the clickhouse is the culprit, I would say that it is either hardware or the kernel.\r\n\r\nCan you please answer the following questions:\r\n- seems that you are running clickhouse in containers (docker/lxc/...) ?\r\n- which version of the kernel do you have? can you upgrade it to latest?\r\n- do you have memory with ECC?"
      },
      {
        "user": "suvorovis",
        "created_at": "2021-04-28T09:05:36Z",
        "body": "Thank you for the reply. Here are the answers you asked for:\r\n- we don't use any type of containers with clickhouse\r\n- kernel version 4.19.0-13-amd64\r\n- memory with ECC"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-28T09:24:50Z",
        "body": "Does the issue reproduce on all three replicas consistently or only on a subset of replicas?"
      },
      {
        "user": "azat",
        "created_at": "2021-04-28T18:16:29Z",
        "body": ">we don't use any type of containers with clickhouse\r\n\r\nInteresting\r\nCan you share kernel config, `/proc/cmdline` and `systemctl show clickhouse-server`?\r\n\r\n> kernel version 4.19.0-13-amd64\r\n\r\nI would recommend you to upgrade the kernel (I can't say which stable release do you have since I can't find any information about your distro specific versioning, but looks like some old debian kernel), is it possible?"
      },
      {
        "user": "suvorovis",
        "created_at": "2021-04-29T11:23:37Z",
        "body": "Issue was reproduced sequentially with the two days gap on two (#1 and #2) of three replicas once, and on one replica (#1) a second time. The kernel on replica #2 was upgraded and after that the server worked without this problem (about a month). So, maybe you are right about the reason.\r\nWe are going to upgrade the kernel on the first replica and watch the situation.\r\nThank you for your help!"
      }
    ]
  },
  {
    "number": 23356,
    "title": "A \"versioned\" version of AggregatingMergeTree",
    "created_at": "2021-04-20T11:09:57Z",
    "closed_at": "2021-04-24T13:51:10Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/23356",
    "body": "We have a very specific criteria for storing data which we cannot find a very good solution for. We would like to do something like a versioned AggregatingMergeTree but it doesn't exist so is there maybe a different solution?\r\n\r\nWe have a setup that looks like this:\r\n```\r\ncreate table if not exists test.test\r\n(\r\n    country        LowCardinality(String),\r\n    text           String CODEC (ZSTD(3)),\r\n    number         SimpleAggregateFunction( max, UInt32),\r\n    related_number Float32,\r\n    first_seen     SimpleAggregateFunction( min, Date)\r\n)\r\n    engine = AggregatingMergeTree()\r\n        PARTITION BY country\r\n        ORDER BY (country, text)\r\n        SETTINGS index_granularity = 256;\r\n\r\ninsert into test\r\nvalues ('us', 'foo', 1, 30, '2020-01-01'),\r\n       ('us', 'foo', 2, 20, '2021-01-02'),\r\n       ('us', 'foo', 3, 10, '2021-01-03');\r\n```\r\nIf we do select all data with final (`select * from test final`) we get:\r\n\r\n| country | text | number | related_number | first_seen |\r\n| ------------- | ------------- | ------------- | ------------- | ------------- |\r\n| us | foo  | 3  | 30 | 2020-01-01  |\r\n\r\nwhich is what one would expect. \r\n\r\nBut we would like \"save\" the `related_number` that correspond to max `number` while maintaining the lowest `first_seen` date as shown in the table below (with the _**bold italic records highligthed**_).\r\n\r\n(`select * from test`)\r\n| country | text | number | related_number | first_seen |\r\n| ------------- | ------------- | ------------- | ------------- | ------------- |\r\n| us | foo  | 1  | 30 | **_2020-01-01_**  |\r\n| us | foo  | 2  | 20 | 2020-01-02  |\r\n| **_us_** | **_foo_**  | **_3_**  | **_10_** | 2020-01-03  |\r\n| us | foo  | 1  | 30 | 2020-01-04  |\r\n\r\nSo essentially we are looking for the behavior of the combination of a ReplacingMergeTree (with `number` as the version column) and a AggregatingMergeTree -> a VersionedAggregatingMergeTree.\r\n\r\nAny idea how we could solve this?\r\n\r\nedit: changed the last table to better show what we want",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/23356/comments",
    "author": "esbenkolsbjerg",
    "comments": [
      {
        "user": "inakivb",
        "created_at": "2021-04-20T17:42:37Z",
        "body": "related_number SimpleAggregateFunction(anyLast,Float32) should work if the numbers are ascending. This will keep the last record sent"
      },
      {
        "user": "esbenkolsbjerg",
        "created_at": "2021-04-21T06:37:20Z",
        "body": "Sadly both `number` and `related_number` are not ascending or (increasing for that matter). The idea is that `related_number` fits together with `number`, hence we want them as a pair but we only wanna keep the record of the highest `number` as it is the important one. I know we can do this if we split the logic into two tables but we would prefer if it was just all in one.\r\n\r\nI edited the post above to better show what we would like."
      },
      {
        "user": "den-crane",
        "created_at": "2021-04-22T14:47:33Z",
        "body": "But you should simply use `SimpleAggregateFunction` Or `AggregateFunction` ?\r\n\r\nWhat CH version do you use?\r\n\r\n\r\n```sql\r\ncreate table if not exists test\r\n(    country        LowCardinality(String),\r\n    text           String CODEC (ZSTD(3)),\r\n    number         SimpleAggregateFunction( max, UInt32),\r\n    related_number SimpleAggregateFunction(argMax, Tuple(Float32, Tuple(UInt32, Int16))),\r\n    first_seen     SimpleAggregateFunction( min, Date)\r\n)\r\n    engine = AggregatingMergeTree()\r\n        PARTITION BY country\r\n        ORDER BY (country, text)\r\n        SETTINGS index_granularity = 256;\r\n\r\ncreate table if not exists test_ingestor\r\n(   country        LowCardinality(String),\r\n    text           String,\r\n    number         UInt32,\r\n    related_number Float32,\r\n    first_seen     Date)\r\nengine = Null;    \r\n\r\ncreate materialized view test_ingestor_mv to test\r\nas \r\nselect country, text, number, (related_number,(number,-toInt16(first_seen))) as related_number, first_seen\r\nfrom test_ingestor\r\n\r\ninsert into test_ingestor values ('us', 'foo', 1, 30, '2020-01-01'), ('us', 'foo', 2, 20, '2021-01-02'), ('us', 'foo', 3, 10, '2021-01-03');\r\n\r\nselect * from test final\u3000;\r\n\u250c\u2500country\u2500\u252c\u2500text\u2500\u252c\u2500number\u2500\u252c\u2500related_number\u2500\u2500\u252c\u2500first_seen\u2500\u2510\r\n\u2502 us      \u2502 foo  \u2502      3 \u2502 (10,(3,-18630)) \u2502 2020-01-01 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect country, text, number, related_number.1 as related_number, first_seen from test final\u3000;\r\n\u250c\u2500country\u2500\u252c\u2500text\u2500\u252c\u2500number\u2500\u252c\u2500related_number\u2500\u252c\u2500first_seen\u2500\u2510\r\n\u2502 us      \u2502 foo  \u2502      3 \u2502             10 \u2502 2020-01-01 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n"
      },
      {
        "user": "esbenkolsbjerg",
        "created_at": "2021-04-26T09:25:41Z",
        "body": "Thanks for the answer "
      }
    ]
  },
  {
    "number": 23299,
    "title": "The performance diff between Map and Array",
    "created_at": "2021-04-19T09:32:31Z",
    "closed_at": "2021-04-20T03:07:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/23299",
    "body": "Hi, I want to know the query performance between `Map` and `Array`. If I want to store multiple-value in one column, I can choose `Array` or `Map`, and use `has()` or `mapContains()` to filter the data in query. Which one will be faster, I found no obvious difference during the test.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/23299/comments",
    "author": "ruanwenjun",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2021-04-19T11:02:39Z",
        "body": "please post your test"
      },
      {
        "user": "ruanwenjun",
        "created_at": "2021-04-19T11:18:16Z",
        "body": "```sql\r\nCREATE TABLE test_map\r\n(\r\n      `itemId` Int64,\r\n       `userId` Map[Int64,Int32],\r\n       `ts` DateTime('America/Phoenix')\r\n)\r\nENGINE = SummingMergeTree\r\nPARTITION BY toYYYYMMDD(ts)\r\nORDER BY (itemId, userId, ts)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE test_array\r\n(\r\n      `itemId` Int64,\r\n       `userId` Array(Int64),\r\n       `ts` DateTime('America/Phoenix')\r\n)\r\nENGINE = SummingMergeTree\r\nPARTITION BY toYYYYMMDD(ts)\r\nORDER BY (itemId, userId, ts)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\nImport 1000w rows of data, and query sql is \r\n```sql\r\nSelect * from test_map where mapContains(userId, 123233)\r\nSelect * from test_array where has(userId, 123233)\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2021-04-20T00:50:40Z",
        "body": "`userId` Map[Int64,Int32], should be `userId` Map(Int64,Int32),\r\n\r\nand the insert sql?\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-20T01:21:24Z",
        "body": "`Map` data type is implemented with linear array. There is no advantage in performance over `Array`."
      },
      {
        "user": "ruanwenjun",
        "created_at": "2021-04-20T03:03:02Z",
        "body": "> `Map` data type is implemented with linear array. There is no advantage in performance over `Array`.\r\n\r\nThank you."
      }
    ]
  },
  {
    "number": 22985,
    "title": "Conflicts and unexpected result of CAST in where condition",
    "created_at": "2021-04-12T04:22:14Z",
    "closed_at": "2021-04-12T08:04:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22985",
    "body": "Dear authors,\r\n\r\nIn my case, values of colomn _note_ is always **NULL**, but got unexpected query result:\r\n\r\nquery: `select record_id, note from demo where CAST(note AS DECIMAL(18,0)) is not null limit 3;`\r\nresult is:\r\n```\r\n\u250c\u2500record_id\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500note\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 2_120000_120000 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2502 2_120000_120001 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2502 2_120000_120002 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nI think the result should be 0 rows, but got 3? Obviously, it is different from Mysql.\r\n\r\nAnd then I try to find out the value of cast by:\r\nquery: `select CAST(note AS DECIMAL(18,0)), record_id from demo where CAST(note AS DECIMAL(18,0)) is not null limit 3;`\r\nbut got: `DB::Exception: Cannot convert NULL value to non-Nullable type`\r\n\r\nSo if the convertion would fail, why it is not blocked by cast condition in advance?\r\nand vise versa, If where-cast condition tells it is not null, why the convertion failed, it should assign certain value to results.\r\n\r\nIf it's not a bug, I think there must be some reasons for the design of this feature.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22985/comments",
    "author": "SE2AI",
    "comments": [
      {
        "user": "SE2AI",
        "created_at": "2021-04-12T07:15:17Z",
        "body": "I try with Nullable type, it solves the problem\r\n\r\n`select record_id, note from demo where CAST(note AS Nullable(DECIMAL(18,0))) is not null limit 3;`"
      },
      {
        "user": "UnamedRus",
        "created_at": "2021-04-12T07:48:49Z",
        "body": "There is setting for that:\r\n\r\n```\r\nset cast_keep_nullable=1;\r\n```"
      },
      {
        "user": "SE2AI",
        "created_at": "2021-04-12T08:04:23Z",
        "body": "@UnamedRus Thanks, it's more convenient, and really a flexible way."
      }
    ]
  },
  {
    "number": 22483,
    "title": "Modify ttl does not affects old parts.",
    "created_at": "2021-04-02T02:49:00Z",
    "closed_at": "2021-04-06T06:59:58Z",
    "labels": [
      "question",
      "comp-ttl",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22483",
    "body": "To my invesigation, ClickHouse uses old parts' ttl as a new part's ttl when merging, so it would not delete old parts desipte modifying table's ttl.\r\nMaybe clickhouse can use modified table's ttl as new part's ttl when merging to delete old parts\u2018 data.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22483/comments",
    "author": "vsop-479",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-04-02T03:44:29Z",
        "body": "What Clickhouse version do you use?\r\nModern Clickhouse recalculate TTL when alter table modify ttl applied.\r\n\r\nAlso use `ALTER TABLE ... MATERIALIZE TTL` to materialize ttl."
      },
      {
        "user": "vsop-479",
        "created_at": "2021-04-02T06:37:01Z",
        "body": "My Clickhouse version is 20.6.\r\nI will test it on a new version."
      },
      {
        "user": "vsop-479",
        "created_at": "2021-04-22T06:06:33Z",
        "body": "20.10.6.27 can delete old parts."
      }
    ]
  },
  {
    "number": 22269,
    "title": "insert into select from hdfs engine table can be parallel ?",
    "created_at": "2021-03-29T08:11:17Z",
    "closed_at": "2021-03-30T03:33:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22269",
    "body": "",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22269/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2021-03-29T09:46:43Z",
        "body": "Hi!\r\nRead from `hdfs` will be parallel if you read from several files. Reading from single hdfs file is in single thread so far.\r\n`insert select` is parallel when `max_insert_threads` is more then 1.\r\nYou need both for your case."
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-03-30T03:33:43Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 22267,
    "title": " clickhouse-client --format_csv_delimiter='@@@' or --format_csv_delimiter=$'\\@\\@\\@' got Exception",
    "created_at": "2021-03-29T06:46:29Z",
    "closed_at": "2021-03-29T06:52:30Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22267",
    "body": "(you don't have to strictly follow this form)\r\n\r\nimport csv data into database ,like this\r\nclickhouse-client --format_csv_delimiter='@@@' --query=\"insert into default.tb_name select col1,col2,col3 from file('csv_file_name.csv','CSVWithNames','col1 String,col2 String,col3 String')\"\r\n\r\ngot \r\nCode: 19. DB::Exception: A setting's value string has to be an exactly one character long\r\n\r\nhow to translate Symbol @ ,I try \\@ not working too.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22267/comments",
    "author": "DreamUFO",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2021-03-29T06:49:45Z",
        "body": "Just like exception says: CSV supports only single character separators (i.e. single `@` could be ok, but `@@@` - not). \r\n\r\nYou can try to use `format Template`, `format  Regexp` or just preprocess your input with smth like `sed`\r\n\r\n"
      },
      {
        "user": "DreamUFO",
        "created_at": "2021-03-29T06:52:26Z",
        "body": "ok,thanks.I got.\r\nI'll deal with multi Symbol delimiter befor import."
      }
    ]
  },
  {
    "number": 22141,
    "title": "Top N of unique string",
    "created_at": "2021-03-25T16:33:28Z",
    "closed_at": "2021-04-06T09:28:10Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22141",
    "body": "Hi,\r\n\r\nIn the main table, there is a string column for `ip` which is unique (mostly) per document. I want to return the top 10 IPs for this table (few billions of documents). Is there any performant way to do so?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22141/comments",
    "author": "hatrena",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-25T16:39:46Z",
        "body": "What do you mean top10 if they are uniq? "
      },
      {
        "user": "hatrena",
        "created_at": "2021-03-25T16:41:57Z",
        "body": "to return something like:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500count()\u2500\u252c\u2500click_ip\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 10024205 \u2502 47.253.44.xxx \u2502\r\n\u2502  9929538 \u2502 47.253.32.xxx \u2502\r\n\u2502  9927342 \u2502 47.253.32.xxx \u2502\r\n\u2502  9886397 \u2502 47.90.248.xxx \u2502\r\n\u2502  9876835 \u2502 47.253.33.xxx \u2502\r\n\u2502  9866026 \u2502 47.253.40.xxx \u2502\r\n\u2502  9850891 \u2502 47.253.44.xxx \u2502\r\n\u2502  9832420 \u2502 47.89.183.xxx \u2502\r\n\u2502  9830460 \u2502 47.253.47.xxx \u2502\r\n\u2502  9763984 \u2502 47.252.11.xxx \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "hatrena",
        "created_at": "2021-03-25T16:44:48Z",
        "body": "something similar to `topK(N)` with count as well but performant. "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-05T14:32:54Z",
        "body": "Naive solution is:\r\n\r\n```\r\nSELECT click_ip, count() AS c FROM table GROUP BY click_ip ORDER BY c DESC LIMIT 10\r\n```\r\n\r\nIt may not fit in memory, but you can raise `max_memory_usage` and enable `max_bytes_before_external_group_by`.\r\nThen the query will be able to proceed even with multiple billion records. But it can be slow.\r\n\r\nOptimized variant is:\r\n\r\n```\r\nSELECT click_ip, count() AS c FROM table\r\nWHERE click_ip IN\r\n(\r\n    SELECT click_ip FROM table ORDER BY rand() LIMIT 1000000\r\n)\r\nGROUP BY click_ip ORDER BY c DESC LIMIT 10\r\n```\r\n\r\nIt relies on assumption that top 10 IP addresses most likely will be present in random sample of a million records from a table. It's not guaranteed to be true but it is almost always true.\r\n\r\nThis query can be slightly more optimal:\r\n\r\n```\r\nSELECT click_ip, count() AS c FROM table\r\nWHERE click_ip IN\r\n(\r\n    SELECT click_ip FROM table WHERE rand() % 10000 = 123\r\n)\r\nGROUP BY click_ip ORDER BY c DESC LIMIT 10\r\n```\r\n\r\nAnd if data in your table is uniformly distributed, you can also use this trick:\r\n```\r\nSET max_rows_to_group_by = 1000000, group_by_overflow_mode = 'any';\r\nSELECT click_ip, count() AS c FROM table GROUP BY click_ip ORDER BY c DESC LIMIT 10;\r\n```\r\n"
      },
      {
        "user": "hatrena",
        "created_at": "2021-04-06T07:19:47Z",
        "body": "The average response from the naive solution is `10 rows in set. Elapsed: 15.929 sec. Processed 204.23 million rows, 20.28 GB (12.82 million rows/s., 1.27 GB/s.)` which is much better than the other solutions somehow. those take minutes."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-06T09:28:10Z",
        "body": "Great!\r\n\r\nBy tuning the constants of the optimized variants (with subqueries) you can get better response time."
      }
    ]
  },
  {
    "number": 21894,
    "title": "Distributed Tables missing records. \"Exception: Too large string size.\" in logs.",
    "created_at": "2021-03-18T21:48:33Z",
    "closed_at": "2021-03-19T02:38:48Z",
    "labels": [
      "question",
      "obsolete-version",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21894",
    "body": "Setup:\r\n```\r\n         <shard>\r\n              <weight>1</weight>\r\n              <internal_replication>false</internal_replication>\r\n              <replica>\r\n                  <host><server1domainname></host>\r\n                  <port>9000</port>\r\n              </replica>\r\n              <replica>\r\n                  <host><server2domainname></host>\r\n                  <port>9000</port>\r\n              </replica>\r\n          </shard>\r\n\r\n```\r\nOur code writes to a Distributed table on either server, and then clickhouse writes the record to the underlying real tables on both servers. \r\n\r\nNo zookeeper.\r\n\r\nThis has been working for a few years.\r\n\r\nIncident.\r\nTo get multivolume support, I upgraded from 18.14.19 to 20.8.12.2. I had a number of problems, and ran out of time to debug/fix, so I rolled back to 18.14.19. My problem probably started then.\r\n\r\nNow we have discovered that some records are not getting replicated to one of the servers. Or maybe replication is working from server1 -> server2, but not the other way. I'm not 100% sure. When reads randomly choose a server, they get different results.\r\n\r\nI have these error messages constantly on one server:\r\n\r\n`2021.03.19 02:35:13.294443 [ 39 ] {} <Error> <table>.Distributed.DirectoryMonitor: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = Exception: Too large string size., e.what() = Exception\r\n`\r\n\r\nI also noticed a small number of these errors on the other server, but they are not constant. Only a handful from a few hours ago.\r\n\r\n`2021.03.18 19:22:36.647630 [ 38 ] {} <Error> <table>.Distributed.DirectoryMonitor: Code: 252, e.displayText() = DB::Exception: Received from <serverdomainname>:9000, <ipaddress> DB::Exception: Too many parts (304). Merges are processing significantly slower than inserts..\r\n`\r\n\r\nI did some research into the first error, but I don't really understand which directory, or which string it is referring to.\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21894/comments",
    "author": "entropical",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-18T22:47:41Z",
        "body": "You don't have actual Replication. \r\nDistributed table propagates inserts into 2 CH nodes. Sometimes inserts can fail into one replica and succeed into another.\r\nThere is no surprise that Replicas has different data. It's documented behavior for this setup.\r\n\r\n >Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = Exception: Too large string size\r\n\r\nYou should stop insertion and check what is going in Distributed table catalog. \r\nProbably drop Distributed table. Remove all remains of Distributed folder (/var/lib/clickhouse/data/db/distributedtablename/) and create Distributed table back.\r\n\r\n> Too many parts (304). Merges are\r\n\r\nNeed to check what is going on with `<serverdomainname>:9000` in this server logs. And in `select * from system.merges`\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-18T22:48:48Z",
        "body": ">I did some research into the first error, but I don't really understand which directory, or which string it is referring to.\r\n\r\nIt's about `/var/lib/clickhouse/data/db/distributedtablename/.....`"
      },
      {
        "user": "entropical",
        "created_at": "2021-03-18T23:10:29Z",
        "body": "> You don't have actual Replication.\r\n> It's documented behavior for this setup.\r\n\r\nYeah, I did know this, but up until very recently it has been sufficient for our purpose.\r\n\r\n> It's about /var/lib/clickhouse/data/db/distributedtablename/.....\r\n\r\nRight! I noticed this directory when I had the failed upgrade.\r\n\r\nI have two directories here, but I think I should only have one. One has IP, one has domainname. I'm pretty sure this is related to the version change.\r\n\r\n/var/lib/clickhouse/data/noc/distributedtablename/\r\ndefault@111%2E222%2E333%2E444:9000/\r\ndefault@domainname%2Etld%2Etld%2Etld%2Ecom:9000/\r\n\r\n> Probably drop Distributed table. Remove all remains of Distributed folder (/var/lib/clickhouse/data/db/distributedtablename/) and create Distributed table back.\r\n\r\nYeah, this sounds like it might help. This is safe, right, because the Distributed table doesn't actually contain data? Trying not to lose any (more...) data."
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-18T23:44:29Z",
        "body": ">This is safe, right, because the Distributed table doesn't actually contain data? \r\n\r\nYes. It is safe. It contains only not propagated inserts. That's why I suggest to stop insertion.\r\nCheck that it stops to propagate. \r\nCheck the size of these subdirectories: `du -sh /var/lib/clickhouse/data/noc/distributedtablename/*`\r\nIf it's huge, check how many .bin files inside. Check the server logs."
      },
      {
        "user": "entropical",
        "created_at": "2021-03-19T00:20:21Z",
        "body": "On one server, both of these directories are very small.\r\nOn the other server, the one with the IP address in as the name is small. The one with the domain name appears to be so huge I can't even ls or du it.\r\n\r\n> Remove all remains of Distributed folder\r\n\r\nHypothetically, if there are orphaned bin files here, can I reintroduce them after I recreate the Distributed table, and it is working? I'm not _too_ concerned if not, just working out my options.\r\n\r\nThanks for your excellent help by the way."
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-19T00:53:01Z",
        "body": ">Hypothetically, if there are orphaned bin files here, \r\n>can I reintroduce them after I recreate the Distributed table, and it is working?\r\n> I'm not too concerned if not, just working out my options.\r\n\r\nIf you sure that both directories are the same destination you can move files from the one to anther.\r\nAnd yes, you can move files / rename (move) the whole  directory to somewhere to /var/lib/clickhouse\r\nrecreate the distr. table and move .bin files one by one or  1000 by 1000 ( batches ).\r\n\r\n"
      },
      {
        "user": "entropical",
        "created_at": "2021-03-19T01:33:53Z",
        "body": "I stopped my writers, and am now dropping the replicatedtable. It has taken about 10 minutes so far... "
      },
      {
        "user": "entropical",
        "created_at": "2021-03-19T02:38:48Z",
        "body": "The drop was taking forever. I think it was attempting to catch up on the pending propogations first.\r\n\r\nI moved the directory to a safe location, and the drop completed a few seconds later. I then recreated the distributed table, and started my writers. For the last hour it looks like the propagations are working fine.\r\n\r\nI'll now try to reparent my orphan bin files.\r\n\r\nThanks again for your help @den-crane."
      }
    ]
  },
  {
    "number": 21575,
    "title": "How to import csv file with  delimiter character ascll ",
    "created_at": "2021-03-10T03:20:15Z",
    "closed_at": "2021-03-10T06:51:30Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21575",
    "body": "How to import csv file with ascll delimiter character, example:\r\n\r\nLO\u0003OG_NFO\u0003110\u0003OU07\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nLN\u0003OG_NFO\u0003110\u0003OU0705\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nAN\u0003OG_NFO\u0003110\u0003OU075\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nLN\u0003OG_FO\u0003110\u0003OU005\u00030\u0003\u00032014-03-21-01.57.30.000000\r\n\r\ndelimiter character \"\u0003\" is \"0X03\",\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21575/comments",
    "author": "joakapp",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-10T04:28:58Z",
        "body": "--format_csv_delimiter=$'\\x03'"
      },
      {
        "user": "joakapp",
        "created_at": "2021-03-10T06:51:30Z",
        "body": "Thanks a lot"
      }
    ]
  },
  {
    "number": 21503,
    "title": "Date value is inconsistency between format",
    "created_at": "2021-03-07T09:14:42Z",
    "closed_at": "2021-03-08T06:01:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21503",
    "body": "I have a schema:\r\n```sql\r\nCREATE TABLE test(\r\n `field1` Int64,\r\n `field2` Int32,\r\n `field3` String,\r\n `field4` String,\r\n `field5` String,\r\n `field6` Int32,\r\n `field7` Int64,\r\n `field8` String,\r\n `time` Int64,\r\n `field9` Int64,\r\n `field10` Int64,\r\n `field11` Int32,\r\n `field12` String,\r\n `field13` Int32,\r\n `field14` Int32,\r\n `field15` Int64,\r\n `field16` Int64,\r\n `field17` Int64,\r\n `field18` Int64,\r\n `field19` String,\r\n `field20` String,\r\n `field21` Int64,\r\n `field22` Int32,\r\n `field23` String,\r\n `field24` Int32,\r\n `field25` Int32,\r\n `field26` Int64,\r\n `field27` Int64,\r\n `field28` Int64,\r\n `field29` Int64,\r\n `field30` String,\r\n `field31` String,\r\n `date_time` DateTime64\r\n) ENGINE =  ReplicatedMergeTree('/data/clickhouse/replicated/test_repl', 'replica_1')\r\nPARTITION BY toYYYYMM(date_time)\r\nORDER BY (time, field1, field2) SETTINGS index_granularity = **8192**\r\n```\r\nI perform SQL:\r\n\r\n```sql\r\nSELECT toDate(date_time), toYYYYMM(date_time), date_time FROM test\r\nWHERE toYYYYMM(date_time) BETWEEN 202101 AND 202102\r\nAND voucher_id=123456789;\r\n```\r\nThe result:\r\n| toDate      | toYYYYMM | date_time\r\n| ----------- | ----------- | ----------- |\r\n| 2021-02-01      | 202102       | 2021-01-31T17:00:00+00:00\r\n\r\nI confused about this result. Why toDate and toYYYYMM different  date_time? \r\nI expected toDate: 2021-31-01 and toYYYYMM: 202001.\r\n \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21503/comments",
    "author": "phamtai97",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-07T13:35:56Z",
        "body": "This result is from JDBC application? Right?\r\n\r\nCan you show the result of\r\n```\r\nselect timezone(), now(), toString(now());\r\n```"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T14:09:46Z",
        "body": "The result of your SQL:\r\n\r\n|timezone()| now() |toString(now())|\r\n| ----------- | ----------- | ----------- |\r\n|Asia/Ho_Chi_Minh   | 2021-03-07T14:08:03+00:00       | 2021-03-07 21:08:03\r\n\r\n\r\n"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T14:11:57Z",
        "body": "I use lib:\r\n\r\n```pom\r\n<dependency>\r\n    <groupId>ru.yandex.clickhouse</groupId>\r\n    <artifactId>clickhouse-jdbc</artifactId>\r\n</dependency>\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-07T15:19:00Z",
        "body": "What tool do use to query data using JDBC ? DataGrip?\r\nWhat timezone at you local computer ? Windows/MACos -- Asia/Ho_Chi_Minh ?"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T17:03:29Z",
        "body": "I develop aplication by Java anh use lib Clickhouse JDBC. Then, I deploy app on Linux server."
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T17:05:08Z",
        "body": "I try query this SQL on superset tool on Mac PC, the result is not change."
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T17:13:13Z",
        "body": "So how the data is actually stored? Is it because lib JDBC has converted the time to the correct local time?\r\nWhat SQL do I have to use to compare the date_time properly?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-07T18:04:00Z",
        "body": ">So how the data is actually stored? \r\n\r\nit depends on how you insert data.\r\n\r\n>Is it because lib JDBC has converted the time to the correct local time?\r\n\r\nyes. JDBC converts datetime to the local TZ. It should work OK if you run JAVA app at the server with Asia/Ho_Chi_Minh.\r\n\r\n>What SQL do I have to use to compare the date_time properly?\r\n\r\nWHERE date_time >= toDateTime('2021-01-01 00:00:00', 'Asia/Ho_Chi_Minh') \r\n       AND date_time < toDateTime('2021-01-03 00:00:00', 'Asia/Ho_Chi_Minh')\r\n\r\n\r\n"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T23:10:36Z",
        "body": "Thank you, I understand this issue."
      }
    ]
  },
  {
    "number": 21473,
    "title": "How to downgrade from version 20.12.7.3 to 20.4.4.18 with Atomic database created",
    "created_at": "2021-03-05T10:27:05Z",
    "closed_at": "2021-03-08T06:18:26Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21473",
    "body": "I created some Atomic databases in 20.12.7.3, when I need to downgrade the server to 20.4.4.18 there's always errors like below: \r\n`{} <Error> Application: DB::Exception: Syntax error (in file /var/lib/clickhouse/metadata/default.sql): failed at position 19 (line 1, col 19): UUID '6ef8d876-bd4e-44bc-bc44-2f2e950e3f20'\r\nENGINE = Atomic\r\n. Expected one of: storage definition, ENGINE, ON\r\n`\r\nHow can I smoothly do the downgrade? \r\nI tried to set allow_experimental_database_atomic=1 in users.xml, but it doesn't work. ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21473/comments",
    "author": "Zhile",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-05T13:03:47Z",
        "body": "in 20.12.7.3 You can move all tables to ordinary database by rename.\r\n\r\n```\r\ncreate database atomic_db Engine=Atomic;\r\ncreate database ordinary_db Engine=Ordinary;\r\ncreate table  atomic_db.x(A Int64) Engine=MergeTree order by A;\r\ninsert into atomic_db.x select number from numbers(100000);\r\nrename table atomic_db.x to ordinary_db.x;\r\nls -1 /var/lib/clickhouse/data/ordinary_db/x\r\nall_1_1_0\r\ndetached\r\nformat_version.txt\r\ndrop database atomic_db;\r\ndetach database ordinary_db;\r\nmv /var/lib/clickhouse/metadata/ordinary_db.sql /var/lib/clickhouse/metadata/atomic_db.sql\r\nvi /var/lib/clickhouse/metadata/atomic_db.sql\r\nmv /var/lib/clickhouse/metadata/ordinary_db /var/lib/clickhouse/metadata/atomic_db\r\nmv /var/lib/clickhouse/data/ordinary_db /var/lib/clickhouse/data/atomic_db\r\nattach database atomic_db;\r\nselect count() from atomic_db.x\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502  100000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nshow create database atomic_db\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE DATABASE atomic_db\r\nENGINE = Ordinary \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nYou can generate rename by \r\n```\r\nselect 'rename table ....atomdb.' ||name||' to ordin.'||name||';' from system.tables where db = atomdb and engine =\r\n```"
      },
      {
        "user": "Zhile",
        "created_at": "2021-03-08T06:18:23Z",
        "body": "Thanks @den-crane , that's really helpful!"
      }
    ]
  },
  {
    "number": 21177,
    "title": "How do I enable the compilation option -pie?",
    "created_at": "2021-02-25T08:22:21Z",
    "closed_at": "2021-06-13T21:33:28Z",
    "labels": [
      "question",
      "build",
      "st-fixed"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21177",
    "body": "In CMakeLists.txt\uff0cthere are following compilation options by default:\r\n_set (CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO} -fno-pie\")\r\nset (CMAKE_C_FLAGS_RELWITHDEBINFO \"${CMAKE_C_FLAGS_RELWITHDEBINFO} -fno-pie\")\r\nset (CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-no-pie\")_\r\n\r\nBecause I want to compile clickhouse in a more secure mode\uff0cI need to use \"-fpie\" or \"-pie\" compilation options in compiling\u3002But if I change \"-fno-pie\" to \"-fpie\" and \"-no-pie\" to \"-pie\",I can not complie clickhouse successfully,the following is my compilation command:\r\n_cmake .. -DUSE_INTERNAL_BOOST_LIBRARY=1  -DENABLE_READLINE=1 -DCMAKE_BUILD_TYPE=Release -DENABLE_MYSQL=0 -DENABLE_DATA_SQLITE=0 -DPOCO_ENABLE_SQL_SQLITE=0 -DENABLE_JEMALLOC=ON -DENABLE_EMBEDDED_COMPILER=1  -DENABLE_PARQUET=1  -DENABLE_ORC=1 -DENABLE_PROTOBUF=1 -DENABLE_ODBC=0 -DENABLE_SSL=1  -DNO_WERROR=1 -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DUSE_INTERNAL_ODBC_LIBRARY=1 -DMAKE_STATIC_LIBRARIES=1_\r\n\r\nThe following is error info:\r\n\r\n-- Performing Test HAVE_PTRDIFF_T\r\n-- Performing Test HAVE_PTRDIFF_T - Failed\r\n-- Check size of void *\r\n-- Check size of void * - failed\r\n-- sizeof(void *) is  bytes\r\nCMake Error at contrib/zlib-ng/CMakeLists.txt:419 (message):\r\n  sizeof(void *) is neither 32 nor 64 bit\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\nSo how can I config my compilation options to compile clickhouse with pie enabled successfully? Thank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21177/comments",
    "author": "wallace-clickhouse",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-28T22:55:19Z",
        "body": "Just remove `-fno-pie` and `-Wl,-no-pie`."
      },
      {
        "user": "wallace-clickhouse",
        "created_at": "2021-03-29T08:57:10Z",
        "body": "It works\uff0cthanks\uff01"
      }
    ]
  },
  {
    "number": 21099,
    "title": "ALTER TABLE UPDATE referencing field from table being updated",
    "created_at": "2021-02-23T09:31:42Z",
    "closed_at": "2022-04-11T15:12:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21099",
    "body": "Hello. Thanks for the amazing software, I \u2764\ufe0f Clickhouse.\r\n\r\nWe had a bug in our app code that generated some bad data in the sessions table. I need to regenerate the `exit_page` field for all the historical data in our sessions table (CollapsingMergeTree).\r\n\r\nOur table layout is simpler but similar to the Yandex.Metrica example. Some names are different but you should get the idea `hits -> events, visits -> sessions`. Not that much data yet, less than a billion rows in the sessions table.\r\n\r\nComing from regular SQL, this was my first instinct:\r\n```sql\r\nALTER TABLE sessions UPDATE exit_page=(SELECT anyLast(pathname) FROM events WHERE events.session_id=sessions.session_id ORDER BY timestamp);\r\n```\r\nbut I get the following error:\r\n```\r\nCode: 47. DB::Exception: Received from clickhouse-server:9000. DB::Exception: Missing columns: 'sessions.session_id' while processing query: 'SELECT anyLast(pathname) FROM plausible_dev.events WHERE (domain = 'localtest.me') AND (session_id = sessions.session_id)', required columns: 'domain' 'pathname' 'session_id' 'sessions.session_id', source columns: [...]\r\n```\r\n\r\nIn regular SQL I am used to being able to reference columns from the row that I'm updating. I realize Clickhouse has very different semantics for updating and this might not be supported. Any other ideas how one might go about updating a field like this?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/comments",
    "author": "ukutaht",
    "comments": [
      {
        "user": "ILYenBui",
        "created_at": "2022-04-10T03:26:33Z",
        "body": "Hi,\r\n\r\nClickHouse does not support updating the table's column like that. However, there is a workaround solution that you can have a look at:\r\n\r\n```\r\nCREATE TABLE exit_pages (session_id UInt64, exit_page String) Engine = Join(ANY, LEFT, session_id);\r\n\r\nINSERT INTO exit_pages SELECT session_id, anyLast(pathname) as exit_page from events group by session_id;\r\n\r\nALTER TABLE sessions \r\nUPDATE exit_page = joinGet('exit_pages', 'exit_page', session_id);\r\n\r\nDROP TABLE exit_pages; \r\n```\r\n"
      },
      {
        "user": "ukutaht",
        "created_at": "2022-04-11T15:12:16Z",
        "body": "Thanks! That's exactly what I ended up doing, wasn't too painful"
      }
    ]
  },
  {
    "number": 20612,
    "title": "Exception: Nested type Array(String) cannot be inside Nullable type (version 20.9.2.20 (official build))",
    "created_at": "2021-02-17T09:36:32Z",
    "closed_at": "2021-02-23T11:44:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20612",
    "body": "i was created table:\r\nCREATE TABLE compare.test1\r\n(\r\n\tevent Nullable(String)\r\n) \r\nENGINE = MergeTree ORDER BY tuple();\r\n\r\nwhen i have construction as \r\nSELECT\r\n\tJSONExtractString(event, 'event_time') as s\r\n\t,event\r\n\t,splitByChar('T', JSONExtractString(event, 'event_time'))[1]\r\nfrom compare.test1\r\n\r\ni have error:\r\nException: Nested type Array(String) cannot be inside Nullable type (version 20.9.2.20 (official build))\r\n\r\nWhen in table i implement event String insted of Nullable(String) it's ok, i don't have any error",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20612/comments",
    "author": "zav379",
    "comments": [
      {
        "user": "vdimir",
        "created_at": "2021-02-17T10:42:09Z",
        "body": "Now `Nullabe(String)` not supported by function `splitByChar`. Workaround is to covert `Nullable(String)` into `String` e.g. with `COALESCE` and (maybe you also want to add `WHERE event is not NULL`) :\r\n```\r\nSELECT\r\n    JSONExtractString(event, 'event_time') AS s,\r\n    event,\r\n    splitByChar('T', JSONExtractString(COALESCE(event, ''), 'event_time'))[1]\r\nFROM test1\r\nWHERE event is not NULL\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-17T14:36:43Z",
        "body": "splitByChar('T', assumeNotNull( ... ) ) "
      },
      {
        "user": "zav379",
        "created_at": "2021-02-18T09:19:11Z",
        "body": "thanks "
      },
      {
        "user": "BlackSinny",
        "created_at": "2022-01-26T03:52:56Z",
        "body": "when i run \r\n\r\n```sql\r\nALTER table ${databaseName} ON CLUSTER ${tmp } ADD COLUMN asset_get_id_list Nullable(Array(String)) DEFAULT NULL;\r\n```\r\n\r\nreceive\r\n```sql\r\nCode: 43. DB::Exception: Nested type Array(String) cannot be inside Nullable type. (ILLEGAL_TYPE_OF_ARGUMENT\r\n```\r\n\r\ncan you  help me...."
      }
    ]
  },
  {
    "number": 20553,
    "title": "Select * memory usage optimization",
    "created_at": "2021-02-16T10:28:32Z",
    "closed_at": "2021-02-24T19:06:38Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20553",
    "body": "I use ClickHouse 20.12.3.3 to store normalized logs. A table consists of 160 columns, primary key and order by key is Timestamp (int64).\r\n\r\nOften I need to find some logs extracting all table columns. When I use the following query to find logs in 1 month period (approx. 2 TB of compressed data) on a single node - it consumes 20+ GB of memory:\r\n\r\nSELECT * FROM events WHERE Column1 = 'value1' AND Column2 = 'value2' AND Timestamp > [START] AND Timestamp < [END] ORDER BY Timestamp DESC LIMIT **250**;\r\n\r\nThe wider search period is, the more RAM is consumed. The less columns appear in SELECT, the less memory consumed.\r\n\r\nFolks from CH telegram group told me that part of this memory is allocated for column buffers (1MB for each column that appears in SELECT) by each thread. Let's say query is executed by 32 threads: 32t * 160c * 1MB = 5 GB RAM. So it is not clear why CH needs another 15 GB to execute this query. \r\n\r\nIs there a way to use a pipeline like the following one?\r\n\r\n1. Read only columns that appear in WHERE and ORDER BY clauses from disk;\r\n2. Mark locations of each row that satisfy WHERE clause, heap sort on-the-fly;\r\n3. Extract marked rows after scan is finished.\r\n\r\nThis way I wouldn't need tens of GB of RAM to perform deep \"historical\" searches.\r\n\r\nIf that is not possible at the moment, are there any plans to introduce such a pipeline in a future releases?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20553/comments",
    "author": "tephrocactus",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-19T17:12:19Z",
        "body": "1. Let's check `EXPLAIN` output. It should use \"read in order\" query plan if `Timestamp` column is the first column in primary key. Otherwise large amount of memory will be used for sorting.\r\n2. Maybe lowering block size will help: `SET max_block_size = 8192` or lower.\r\n\r\n```\r\nIs there a way to use a pipeline like the following one?\r\n\r\n    Read only columns that appear in WHERE and ORDER BY clauses from disk;\r\n    Mark locations of each row that satisfy WHERE clause, heap sort on-the-fly;\r\n    Extract marked rows after scan is finished.\r\n```\r\n\r\nThis algorithm is not implemented in ClickHouse.\r\n\r\n> If that is not possible at the moment, are there any plans to introduce such a pipeline in a future releases?\r\n\r\nIt's possible but non-trivial."
      },
      {
        "user": "tephrocactus",
        "created_at": "2021-02-19T23:40:38Z",
        "body": "Thank you for an answer. \r\n\r\nHere is the output of EXPLAIN for similar query against a distributed table:\r\n\r\n```EXPLAIN\r\nSELECT *\r\nFROM events\r\nWHERE (Timestamp >= 1613573815452) AND (Timestamp <= 1613576700442)\r\nORDER BY Timestamp DESC\r\nLIMIT 250\r\n\r\nQuery id: b605856a-d07b-49ce-b544-4bf33c2eb278\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression (Projection)                                                       \u2502\r\n\u2502   Limit (preliminary LIMIT)                                                   \u2502\r\n\u2502     MergingSorted (Merge sorted streams for ORDER BY)                         \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         Union                                                                 \u2502\r\n\u2502           ReadFromPreparedSource (Read from remote replica)                   \u2502\r\n\u2502           ReadFromPreparedSource (Read from delayed local replica)            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd EXPLAIN for the same query against a local replicated table:\r\n\r\n```\r\nEXPLAIN\r\nSELECT *\r\nFROM events_local\r\nWHERE (Timestamp >= 1613573815452) AND (Timestamp <= 1613576700442)\r\nORDER BY Timestamp DESC\r\nLIMIT 250\r\n\r\nQuery id: 0c3879ad-462e-4525-ac96-d4a16b650978\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression (Projection)                                                           \u2502\r\n\u2502   Limit (preliminary LIMIT)                                                       \u2502\r\n\u2502     FinishSorting                                                                 \u2502\r\n\u2502       Expression (Before ORDER BY and SELECT)                                     \u2502\r\n\u2502         Filter (WHERE)                                                            \u2502\r\n\u2502           SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502             Union                                                                 \u2502\r\n\u2502               ReverseRows                                                         \u2502\r\n\u2502                 ReadFromStorage (MergeTree  with order)                           \u2502\r\n\u2502               ReverseRows                                                         \u2502\r\n\u2502                 ReadFromStorage (MergeTree  with order)                           \u2502\r\n\u2502               ReverseRows                                                         \u2502\r\n\u2502                 ReadFromStorage (MergeTree  with order)                           \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nOptimize read in order is enabled:\r\n\r\n```\r\nSELECT *\r\nFROM system.settings\r\nWHERE name = 'optimize_read_in_order'\r\n\r\nQuery id: 059bb96f-03f5-4478-9f66-56d65e3dc73e\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname:        optimize_read_in_order\r\nvalue:       1\r\n\r\n```\r\n\r\nTimestamp column is a primary key. A fragment of output of a query against system.tables:\r\n\r\n```\r\nname:                       events_local\r\n...\r\npartition_key:              (toYYYYMMDD(toDateTime(Timestamp / 1000)), SpaceID)\r\nsorting_key:                Timestamp\r\nprimary_key:                Timestamp\r\nsampling_key:               Timestamp\r\nstorage_policy:             default\r\n....\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-20T05:55:39Z",
        "body": "Ok. The query plan is correct.\r\n\r\nNow let's go to\r\n> 2. Maybe lowering block size will help: SET max_block_size = 8192 or lower.\r\n\r\nMaybe there are very large values in the table?"
      },
      {
        "user": "tephrocactus",
        "created_at": "2021-02-20T13:56:36Z",
        "body": "Setting max_block_size to 8192 helps to reduce memory usage. Here is a query for 2 month period:\r\n\r\n```\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nis_initial_query:     1\r\nuser:                 default\r\nquery_id:             d0e4e907-f2df-4d44-a8d5-5765aa7735d3\r\nelapsed:              889.8091651\r\nis_cancelled:         0\r\nread_rows:            0\r\nread_bytes:           0\r\ntotal_rows_approx:    0\r\nwritten_rows:         0\r\nwritten_bytes:        0\r\nmemory_usage:         1539539856\r\npeak_memory_usage:    1539539856\r\nquery:                SELECT * FROM `events_local` WHERE Timestamp >= 1609448400000 AND Timestamp <= 1613825407279 AND (DeviceProduct = \u2018...' OR DeviceProduct = \u2018...' OR ServiceID = '3bf38e66-dd86-47bb-8839-fc8c3c12505b' OR ServiceID = '89d4716c-1666-4745-9e59-9ee4a5ccd71c' OR ServiceID = '5d2a94e6-75d3-486a-90bf-dcd1b38eadc6' OR ServiceID = '2d1f5f7b-33b2-4dda-adf8-aef8f94976e9' OR DeviceProduct = \u2018...') ORDER BY Timestamp DESC LIMIT 250\r\nProfileEvents.Names:  ['Query','SelectQuery','FileOpen','Seek','ReadBufferFromFileDescriptorRead','ReadBufferFromFileDescriptorReadBytes','ReadCompressedBytes','CompressedReadBufferBlocks','CompressedReadBufferBytes','IOBufferAllocs','IOBufferAllocBytes','ArenaAllocChunks','ArenaAllocBytes','FunctionExecute','MarkCacheHits','MarkCacheMisses','CreatedReadBufferOrdinary','DiskReadElapsedMicroseconds','SelectedParts','SelectedRanges','SelectedMarks','ContextLock','RWLockAcquiredReadLocks','RealTimeMicroseconds','UserTimeMicroseconds','SystemTimeMicroseconds','SoftPageFaults','OSCPUWaitMicroseconds','OSCPUVirtualTimeMicroseconds','OSReadChars','OSWriteChars']\r\nProfileEvents.Values: [1,1,268153,131530,263061,33439521600,36,1,10,268154,35968729162,4,16384,2860,5092,131530,268153,738458090,928,928,8786329,2836,1,5161249,4502249,139672,3,2672,4634931,482030,744]\r\nSettings.Names:       ['max_block_size','use_uncompressed_cache','load_balancing','distributed_aggregation_memory_efficient','max_bytes_before_external_group_by','max_bytes_before_external_sort','max_memory_usage']\r\nSettings.Values:      ['8192','0','random','1','20000000000','20000000000','40000000000\u2019]\r\n```\r\nQuery processing takes about 15 minutes. But memory usage is unstable. The same query for a 1 month period sometimes consumes about 5-7 GB RAM and completes in 60 seconds (cache?). But query for 2 month often does not cross 1 GB boundary with identical max_block_size.\r\n\r\n`Maybe there are very large values in the table?`\r\n\r\nOne column has maximum size of 16 KB (application-level limit). Other string columns are limited to 1 KB. But this is a rare case. Most log records (when serialized to JSON) are about 500-1000 bytes long."
      }
    ]
  },
  {
    "number": 20471,
    "title": "parseDateTimeBestEffortUS doesn't support OrNull modifier",
    "created_at": "2021-02-13T22:35:55Z",
    "closed_at": "2021-02-13T22:53:46Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20471",
    "body": "Works:\r\n```sql\r\nselect parseDateTimeBestEffortUS('30/01/2021')\r\n```\r\nWorks:\r\n```sql\r\nselect parseDateTimeBestEffortOrNull('30/01/2021')\r\n```\r\nDoesn't work:\r\n```sql\r\nselect parseDateTimeBestEffortUSOrNull('30/01/2021')\r\n```\r\n> DB::Exception: Unknown function parseDateTimeBestEffortUSOrNull. Maybe you meant: ['parseDateTimeBestEffortOrNull', 'parseDateTime64BestEffortOrNull']: While processing parseDateTimeBestEffortUSOrNull('30/01/2021')",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20471/comments",
    "author": "stas-sl",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-02-13T22:52:55Z",
        "body": "```\r\nSELECT parseDateTimeBestEffortUSOrNull('30/01/2021')\r\n\r\nQuery id: fcbb75e7-8b59-47c1-b6c8-3db067ad65cb\r\n\r\n\u250c\u2500parseDateTimeBestEffortUSOrNull('30/01/2021')\u2500\u2510\r\n\u2502                           2021-01-30 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-13T22:53:46Z",
        "body": "ClickHouse release v21.2.2.8-stable, 2021-02-07\r\nAdded functions parseDateTimeBestEffortUSOrZero, parseDateTimeBestEffortUSOrNull. #19712 (Maksim Kita)."
      },
      {
        "user": "stas-sl",
        "created_at": "2021-02-13T22:57:12Z",
        "body": "Cool! Sorry for bothering"
      }
    ]
  },
  {
    "number": 20273,
    "title": "how to kill long query?",
    "created_at": "2021-02-10T04:26:47Z",
    "closed_at": "2021-02-10T04:34:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20273",
    "body": "KILL QUERY WHERE query_id='xxx' doesn't work, it just return waiting. \r\n\r\nIs it true that a sql cannot be killed unless it is executed? But what is the point of killing if all executions are completed? The CPU is full, and the purpose of kill query is to immediately stop the executing sql and reduce the CPU usage\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20273/comments",
    "author": "Fanduzi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-02-10T04:31:17Z",
        "body": "Unfortunately not all queries can be killed.\r\nKILL QUERY only sets a flag that must be checked by the query.\r\nA query pipeline is checking this flag  before a switching to next block. If the pipeline has stuck somewhere in the middle it cannot be killed.\r\nIf a query does not stop, the only way to get rid of it is to restart ClickHouse."
      },
      {
        "user": "Fanduzi",
        "created_at": "2021-02-10T04:33:56Z",
        "body": "> Unfortunately not all queries can be killed.\r\n> KILL QUERY only sets a flag that must be checked by the query.\r\n> A query pipeline is checking this flag before a switching to next block. If the pipeline has stuck somewhere in the middle it cannot be killed.\r\n> If a query does not stop, the only way to get rid of it is to restart ClickHouse.\r\n\r\nThank you @den-crane , I understand, I will restart after waiting, you guys response time is too fast!  Happy Chinese New Year :)"
      },
      {
        "user": "amosbird",
        "created_at": "2021-02-10T04:38:42Z",
        "body": "I feel like we can extend the `KILL` query so that it can kill\r\n1. long queries\r\n2. queries of given table, database\r\n3. queries of given function\r\netc.."
      },
      {
        "user": "SaltTan",
        "created_at": "2021-02-10T19:06:33Z",
        "body": "Any column from system.processes can be used in the WHERE section of KILL QUERY"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-10T19:17:01Z",
        "body": "yeah, like\r\n\r\nKILL QUERY WHERE elapsed > 600"
      }
    ]
  },
  {
    "number": 20162,
    "title": "Do we need to still use GROUP BY in MATERIALIZED VIEWS?",
    "created_at": "2021-02-06T20:20:59Z",
    "closed_at": "2021-02-08T13:00:15Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20162",
    "body": "Following on from #16954, does it still make sense to write:\r\n\r\n```\r\nCREATE TABLE agg (\r\n) ENGINE = AggregatingMergeTree()\r\n\r\nCREATE MATERIALIZED VIEW mv TO agg AS SELECT ... FROM src GROUP BY ...\r\n```\r\n\r\nis it basically identical performance-wise to write it without the `GROUP BY` without any overheads or other costs (and probably making it much easier to insert bulk data without having to go via a Null table per #17239) ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20162/comments",
    "author": "mzealey",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-06T23:50:55Z",
        "body": "Yes, after `optimize_on_insert` is enabled there is no reason to write GROUP BY in mat.view definition."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-06T23:52:13Z",
        "body": "Maybe it is still good for illustrative purposes.\r\n\r\nAlso maybe there are cases when GROUP BY in mat.view differs to the ORDER BY key of AggregatingMergeTree but I cannot imagine if they are useful."
      },
      {
        "user": "mzealey",
        "created_at": "2021-02-07T12:35:00Z",
        "body": "Seems to work perfectly in my testing - no need for Null engines or GROUP BY's any more. Great work!"
      },
      {
        "user": "mzealey",
        "created_at": "2021-02-08T12:21:28Z",
        "body": "Hm actually I just came across a case where this is not working so well. I'm using cloudflare goflow and have an agg table like:\r\n\r\n```\r\nCREATE TABLE netflow_sum_proto_port (\r\n    Time DateTime CODEC(DoubleDelta, LZ4),\r\n    SamplerAddress IPv4 CODEC(ZSTD),\r\n    ProtoName Enum8('ICMP'=1,'TCP'=6, 'UDP'=17,'other'=0),\r\n    Port Int32,\r\n    Flows SimpleAggregateFunction(sum, Float64),\r\n    Packets SimpleAggregateFunction(sum, Float64),\r\n    Bytes SimpleAggregateFunction(sum, Float64)\r\n)\r\nENGINE = AggregatingMergeTree()\r\nPARTITION BY toDate(Time)\r\nORDER BY (Time, SamplerAddress, ProtoName, Port);\r\n\r\ndrop view if exists netflow_sum_proto_port_mv;\r\nCREATE MATERIALIZED VIEW netflow_sum_proto_port_mv TO netflow_sum_proto_port\r\nAS SELECT\r\n    toStartOfInterval(time, INTERVAL 30 SECOND) AS Time,\r\n    reinterpretAsUInt32(reverse(SamplerAddress)) AS SamplerAddress,\r\n    ProtoName,\r\n    IF(Port in (443,80,0,445,993,143,2003,1443,3306,110,3389,22,21,23,995,25,53,19,587,2083,7000,1194,5060,4001,1198,389,123,1900,161,623,11211,131,132,133,134,135,136,137,138,139), Port, -1) Port,\r\n    sum(Flows) AS Flows,\r\n    sum(Packets) AS Packets,\r\n    sum(Bytes) AS Bytes\r\nFROM (\r\n    SELECT\r\n        toUInt32(TimeFlowEnd-TimeFlowStart+1) AS duration\r\n        , arrayJoin(timeSlots(TimeFlowStart, duration, 1)) AS time\r\n        , SamplerAddress\r\n        , IF( Proto in (1,6,17), Proto, 0 ) AS ProtoName\r\n        , arrayJoin([SrcPort, DstPort]) AS Port\r\n        , 1.0 / 2 / duration AS Flows\r\n        , toFloat32(Packets) / 2 / duration AS Packets\r\n        , toFloat32(Bytes) / 2 / duration AS Bytes\r\n    FROM netflow_flows\r\n    WHERE Type = 'Netflow_9'\r\n) t;\r\n```\r\n\r\nThis is working fine, but if I truncate + insert historic data it goes very slow without the group by - if I add `GROUP BY Time, SamplerAddress, ProtoName, Port` on the end it goes about 20* faster inserting. I'm only using `max_insert_threads=1` so perhaps that is the difference as when doing the GROUP BY it seems to use quite a bit more CPU."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-08T12:59:44Z",
        "body": "Yes, it is expected: GROUP BY implementation (it's the same as used for SELECT queries) is faster than merging (including merge-on-insert)."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-08T13:00:15Z",
        "body": "So, the answer is: it still makes sense :)"
      }
    ]
  },
  {
    "number": 19955,
    "title": "How to decrease the number of ClickHouse operations on ZooKeeper?",
    "created_at": "2021-02-02T02:58:26Z",
    "closed_at": "2021-02-02T07:33:22Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19955",
    "body": "```\r\njava.lang.Exception: shutdown Leader! reason: zxid lower 32 bits have rolled over, forcing re-election, and therefore new epoch start\r\n```\r\n\r\nIs there any other way to restart ZooKeeper on a regular basis?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19955/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-02T07:29:27Z",
        "body": "Group INSERTs to larger batches to lower the amount of INSERT queries."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-02T07:30:44Z",
        "body": "Other caveats:\r\n- useless partitioning that leads to large number of partitions;\r\n- large number of tables with identical structure."
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-02-02T07:33:22Z",
        "body": "> Other caveats:\r\n> \r\n> * useless partitioning that leads to large number of partitions;\r\n> * large number of tables with identical structure.\r\nthanks\r\n"
      }
    ]
  },
  {
    "number": 19943,
    "title": "Will the data be cached when clickhouse-benchmark tool is used for multiple queries?",
    "created_at": "2021-02-01T15:53:29Z",
    "closed_at": "2021-02-04T14:44:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19943",
    "body": "Like `select * from table`,`select count(*) from table `",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19943/comments",
    "author": "nautaa",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-02-01T16:37:55Z",
        "body": "yes, \r\n\r\n`select count(*) from table ` -- uses optimization and reads in-memory parts structure.\r\n\r\n--optimize_trivial_count_query arg                               Process trivial 'SELECT count() FROM table' query from metadata.\r\n\r\n`select * from table` -- also cached because CH relays on Linux file cache and does not control it.\r\nbut you can use \r\n--min_bytes_to_use_direct_io arg                                 The minimum number of bytes for reading the data with O_DIRECT option during SELECT queries execution. 0 - disabled.\r\n\r\nSo you can try\r\n\r\n```\r\ncreate table table10kRows(A Int64) Engine =MergeTree() order by A;\r\ninsert into table10kRows select number from numbers(10000);\r\n\r\nclickhouse-benchmark -c 16 --database=default <<< 'select count() from table10kRows'\r\nQPS: 4983.314\r\n\r\n\r\nclickhouse-benchmark -c 16 --optimize_trivial_count_query=0 --database=default <<< 'select count() from table10kRows' \r\nQPS: 10597.303\r\n\r\nclickhouse-benchmark -c 16 --optimize_trivial_count_query=0 --min_bytes_to_use_direct_io=1 --database=default <<< 'select count() from table10kRows'\r\nQPS: 11212.843\r\n```\r\n\r\nfunny that optimize_trivial_count_query=0 gives a boost, probably it's only for a small tables.\r\n"
      },
      {
        "user": "nautaa",
        "created_at": "2021-02-02T02:22:06Z",
        "body": "So if I don't want to use the cache for testing disk io, can I use it `--optimize_trivial_count_query=0 --min_bytes_to_use_direct_io=1`, and why it is faster? @den-crane "
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-04T14:44:36Z",
        "body": ">So if I don't want to use the cache for testing disk io, \r\n>can I use it --optimize_trivial_count_query=0 --min_bytes_to_use_direct_io=1\r\n\r\nYes.\r\n\r\n>and why it is faster?\r\n\r\nIt's only for my test. \r\nBecause of my hardware + table table10kRows is too small."
      }
    ]
  },
  {
    "number": 19878,
    "title": "Altering Enum in ordering key doesn't work on ReplicatedMergeTree",
    "created_at": "2021-01-31T11:07:29Z",
    "closed_at": "2021-02-02T08:54:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19878",
    "body": "**Describe the bug**\r\nReproduces at 20.8.12.2 Clickhouse version.\r\n\r\n```\r\nCREATE TABLE t ON CLUSTER replicated\r\n(\r\n    `a` UInt64,\r\n    `b` Enum8('a' = 1, 'b' = 2)\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/default/{shard}/t', '{replica}')\r\nORDER BY (b, a)\r\n\r\nINSERT INTO t VALUES (1,1),(2,2)\r\n\r\nALTER TABLE t ON CLUSTER replicated MODIFY COLUMN b Enum8('a' = 1)\r\n```\r\nThe last statement falls into error: `Cannot execute replicated DDL query, maximum retires exceeded`\r\n\r\nIn case of local alter: \r\n`ALTER TABLE t ON CLUSTER replicated MODIFY COLUMN b Enum8('a' = 1)`\r\n\r\nthe error is next:\r\n```\r\nReceived exception from server (version 20.8.12):\r\nCode: 524. DB::Exception: Received from localhost:9100. DB::Exception: ALTER of key column b from type Enum8('a' = 1, 'b' = 2) to type Enum8('a' = 1) must be metadata-only.\r\n```\r\n\r\n\r\n**Expected behavior**\r\nThe same as on non-Replicated MergeTree",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19878/comments",
    "author": "Blackmorse",
    "comments": [
      {
        "user": "alesapin",
        "created_at": "2021-02-01T13:23:52Z",
        "body": "Hi!\r\n\r\nAlter of the key columns in ClickHouse is quite limited. You can alter enums but only add new values. Removing old values can change the representation of the primary key. \r\n```\r\n:) ALTER TABLE t\r\n    MODIFY COLUMN `b` Enum8('a' = 1)\r\n\r\nReceived exception from server (version 21.2.1):\r\nCode: 524. DB::Exception: Received from localhost:9000. DB::Exception: ALTER of key column b from type Enum8('a' = 1, 'b' = 2) to type Enum8('a' = 1) is not safe because it can change the representation of primary key. \r\n```\r\nbut the extension is allowed:\r\n```\r\n\r\nALTER TABLE t\r\n    MODIFY COLUMN `b` Enum8('a' = 1, 'b' = 2, 'c' = 3)\r\n\r\nOk.\r\n\r\n```\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-01T14:03:43Z",
        "body": "> The same as on non-Replicated MergeTree\r\n\r\nBut how it's related to Replicated / non-Replicated MergeTree?"
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-01T16:26:59Z",
        "body": "@alesapin , \r\n\r\n>  You can alter enums but only add new values\r\n\r\nBut documentation says:\r\n\r\n> The Enum type can be changed without cost using ALTER, if only the set of values is changed. It is possible to both add and remove members of the Enum using ALTER (removing is safe only if the removed value has never been used in the table)\r\n\r\n"
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-01T16:31:19Z",
        "body": "@alexey-milovidov \r\n\r\n> But how it's related to Replicated / non-Replicated MergeTree?\r\n\r\nSorry, I was sure that the same operation works on simple MergeTree(). My bad"
      },
      {
        "user": "alesapin",
        "created_at": "2021-02-02T08:27:27Z",
        "body": "> @alesapin ,\r\n> \r\n> > You can alter enums but only add new values\r\n> \r\n> But documentation says:\r\n> \r\n> > The Enum type can be changed without cost using ALTER, if only the set of values is changed. It is possible to both add and remove members of the Enum using ALTER (removing is safe only if the removed value has never been used in the table)\r\n\r\nAs the documentation says:\r\n`> removing is safe only if the removed value has never been used in the table`\r\n\r\nWe restrict unsafe operations for the primary key columns because it can lead not only to the single-column corruption but to the whole table became broken."
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-02T08:41:03Z",
        "body": "@alesapin , So , It is correct behaviour, right?"
      },
      {
        "user": "alesapin",
        "created_at": "2021-02-02T08:43:21Z",
        "body": "> @alesapin , So , It is correct behaviour, right?\r\n\r\nYes, it's expected, maybe we have to improve docs."
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-02T08:54:35Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 19797,
    "title": "Is it possible to disable using timezone and set only UTC for DateTime64 format?",
    "created_at": "2021-01-29T07:18:26Z",
    "closed_at": "2021-01-29T13:01:41Z",
    "labels": [
      "question",
      "comp-datetime"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19797",
    "body": "\r\nIs it possible to disable using timezone and set only UTC for DateTime64 format?\r\n\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19797/comments",
    "author": "lessenko",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2021-01-29T11:57:22Z",
        "body": "You can just set up default timezone to UTC system-wide, or in clickhouse configuration. "
      },
      {
        "user": "lessenko",
        "created_at": "2021-01-29T13:01:41Z",
        "body": "@filimonov,\r\nThank you. It can be closed. "
      }
    ]
  },
  {
    "number": 19658,
    "title": "Execute Clickhouse compressor -- decompress to return xshell",
    "created_at": "2021-01-26T13:14:08Z",
    "closed_at": "2021-01-27T03:05:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19658",
    "body": "SELECT *\r\nFROM mt2\r\n\r\n\u250c\u2500a\u2500\u252c\u2500\u2500b\u2500\u252c\u2500\u2500c\u2500\u2510\r\n\u2502 3 \u2502  4 \u2502 10 \u2502\r\n\u2502 3 \u2502  5 \u2502  9 \u2502\r\n\u2502 3 \u2502  6 \u2502  8 \u2502\r\n\u2502 3 \u2502  7 \u2502  7 \u2502\r\n\u2502 3 \u2502  8 \u2502  6 \u2502\r\n\u2502 3 \u2502  9 \u2502  5 \u2502\r\n\u2502 3 \u2502 10 \u2502  4 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\r\n\r\nThe data directory is as follows\r\n\r\n[root@ck mt2]# tree\r\n.\r\n\u251c\u2500\u2500 3_1_1_0\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 a.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 a.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 b.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 b.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 checksums.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 columns.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 count.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 minmax_a.idx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 partition.dat\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 primary.idx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 skp_idx_idx_c.idx\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 skp_idx_idx_c.mrk\r\n\u251c\u2500\u2500 detached\r\n\u2514\u2500\u2500 format_version.txt\r\n\r\nExecute clickhouse-compressor like this\r\n\r\n[root@ck mt2]# clickhouse-compressor --decompress < 3_1_1_0/b.bin2                                                                                                                \t\r\n[root@ck mt2]# Xshell\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19658/comments",
    "author": "xiedeyantu",
    "comments": [
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-26T13:46:08Z",
        "body": "[root@ck mt2]# clickhouse-compressor --decompress < 3_1_1_0/b.bin\r\n[root@ck mt2]# Xshell\r\n\r\nnot b.bin2"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-26T18:55:35Z",
        "body": "That is correct and you just read some binary data from your table into your terminal."
      },
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-27T02:19:31Z",
        "body": "Why show Xshell instead of data\uff1fI want to see the structure of the bin file"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-27T02:38:18Z",
        "body": "Binary data with ANSI escape sequences can be interpreted by terminal."
      },
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-27T02:44:34Z",
        "body": "Can you give me a shell command? How to operate?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-27T02:53:18Z",
        "body": "Could you please tell me how to reproduce this result?\r\n\r\nE.g. CREATE TABLE statement, INSERT..."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-27T02:53:49Z",
        "body": "To display binary data I also recommend `xxd` tool:\r\n\r\n`clickhouse-compressor --decompress < 3_1_1_0/b.bin | xxd`"
      },
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-27T03:05:24Z",
        "body": "This shell command is easy to use. Thank you very much"
      }
    ]
  },
  {
    "number": 19531,
    "title": "Data stored in store directory in version 21?",
    "created_at": "2021-01-24T16:38:54Z",
    "closed_at": "2021-01-24T17:03:37Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19531",
    "body": "Recently I have installed a clickhouse of version 21.1.2.15. I found that the data is stored in the store directory(CLICKHOUSEPATH/store) with the name of random string, but not in the data directory as before. \r\n\r\nInstead, in the data directory there are some symbolic links to the data directory in the store directory. \r\nIs it a new feature of the new version?\r\n\r\nWhen I tried to freeze the table, I found the directory with random name in the shadow directory(same as the one in store directory) but not like \"/database/tablename\" as before. \r\nIt seems make the restore from the freeze file more complicated and makes no benefits.\r\n\r\nIs there any suggestion with the backup/restore work in the new version? Thanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19531/comments",
    "author": "winoenix",
    "comments": [
      {
        "user": "winoenix",
        "created_at": "2021-01-24T16:48:53Z",
        "body": "It seems that the default database comes to Atomic from Ordinary?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-24T17:01:17Z",
        "body": "That's true. Atomic database is default from version 20.10.\r\n\r\nIt gives the following benefits: lock-free table CREATE/DROP/RENAME, allows EXCHANGE and REPLACE queries.\r\n\r\nDrawbacks: some external tools may not be ready for data layout on filesystem.\r\n\r\nYou can also create database with ENGINE specified explicitly, e.g. `CREATE DATABASE db ENGINE = Ordinary` or change the default in users profile with `default_database_engine` setting."
      },
      {
        "user": "winoenix",
        "created_at": "2021-01-24T17:03:34Z",
        "body": "> That's true. Atomic database is default from version 20.10.\r\n> \r\n> It gives the following benefits: lock-free table CREATE/DROP/RENAME, allows EXCHANGE and REPLACE queries.\r\n> \r\n> Drawbacks: some external tools may not be ready for data layout on filesystem.\r\n> \r\n> You can also create database with ENGINE specified explicitly, e.g. `CREATE DATABASE db ENGINE = Ordinary` or change the default in users profile with `default_database_engine` setting.\r\n\r\nthank very much"
      }
    ]
  },
  {
    "number": 19315,
    "title": "ALTER DELETE not working",
    "created_at": "2021-01-20T13:15:53Z",
    "closed_at": "2021-08-17T07:27:13Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19315",
    "body": "I have a table that I want to mutate using the following query:\r\n\r\n```sql\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE (ProjectId, UserId, SessionId) NOT IN (\r\n        SELECT ProjectId, UserId, SessionId\r\n        FROM clarity.page_data_enrich\r\n        GROUP BY ProjectId, UserId, SessionId\r\n        HAVING argMax(IsFavorite, RowVersion)\r\n    );\r\n```\r\n\r\nWhen I run the query, I get the following exception in `system.mutations` table:\r\n\r\n```\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_10.txt\r\ncommand:                    DELETE WHERE (ProjectId, UserId, SessionId) NOT IN (SELECT ProjectId, UserId, SessionId FROM clarity.page_data_enrich GROUP BY ProjectId, UserId, SessionId HAVING argMax(IsFavorite, RowVersion))\r\ncreate_time:                2021-01-20 15:08:28\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [10]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-01-20 15:08:30\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 20.12.5.14 (official build))\r\n```\r\n\r\nEven though the following query runs with no problem:\r\n\r\n```sql\r\nSELECT *\r\nFROM clarity.page_data\r\nWHERE (ProjectId, UserId, SessionId) NOT IN\r\n(\r\n    SELECT ProjectId, UserId, SessionId\r\n    FROM clarity.page_data_enrich\r\n    GROUP BY ProjectId, UserId, SessionId\r\n    HAVING argMax(IsFavorite, RowVersion)\r\n)\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500Timestamp\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500Date\u2500\u252c\u2500ProjectId\u2500\u252c\u2500UserId\u2500\u252c\u2500SessionId\u2500\u252c\u2500PageNum\u2500\u2510\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       1 \u2502\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       2 \u2502\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nI am not sure what is wrong with the `ALTER DELETE` query!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19315/comments",
    "author": "OmarBazaraa",
    "comments": [
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-03T15:17:43Z",
        "body": "I even tried to concatenate the fields instead of comparing tuples, but I get the exact same error:\r\n\r\n```sql\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE concat(toString(ProjectId), toString(UserId), toString(SessionId)) NOT IN (\r\n        SELECT concat(toString(ProjectId), toString(UserId), toString(SessionId))\r\n        FROM clarity.page_data_enrich\r\n        GROUP BY ProjectId, UserId, SessionId\r\n        HAVING argMax(IsFavorite, RowVersion)\r\n    );\r\n```\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\nORDER BY create_time DESC\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_16.txt\r\ncommand:                    DELETE WHERE concat(toString(ProjectId), toString(UserId), toString(SessionId)) NOT IN (SELECT concat(toString(ProjectId), toString(UserId), toString(SessionId)) FROM clarity.page_data_enrich GROUP BY ProjectId, UserId, SessionId HAVING argMax(IsFavorite, RowVersion))\r\ncreate_time:                2021-02-03 17:11:40\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [16]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-02-03 17:11:44\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 21.1.2.15 (official build))\r\n```\r\n\r\nIt's stating that the number of columns in section IN doesn't match!\r\n\r\nAny ideas what is going wrong?!"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-03T16:40:32Z",
        "body": "I think mutations are not designed to handle such `where subqueries`\r\n\r\nas a WA I would create a table Engine=Join and inserted into this Join table IDs which should be deleted using `insert select` \r\nthen run delete like this \r\n```\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE joinHas(, , (ProjectId, serId, SessionId) )"
      },
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-04T13:37:39Z",
        "body": "Thanks @den-crane for your suggestion!\r\n\r\nI tried it but it's giving me the same error...\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\nORDER BY create_time DESC\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_17.txt\r\ncommand:                    DELETE WHERE isNotNull(joinGet('clarity.page_data_retained', 'RowVersion', ProjectId, UserId, SessionId))\r\ncreate_time:                2021-02-04 15:33:34\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [17]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-02-04 15:33:52\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 21.1.2.15 (official build))\r\n```\r\n\r\nAny other possible alternatives to retain/TTL records based on values from other tables?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-04T14:35:41Z",
        "body": "@OmarBazaraa \r\n\r\nHMm, I think this error from the previous mutations.\r\nTry remove failed mutations first:\r\n\r\n```\r\nkill mutation where not is_done;\r\nALTER TABLE clarity.page_data DELETE WHERE joinHas(, , (ProjectId, serId, SessionId) )\r\n```"
      },
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-04T14:42:48Z",
        "body": "> Try remove failed mutations first\r\n\r\nThanks @den-crane, it worked!\r\n\r\nAnd what is more interesting now is that my original query is working now too without having to use `Join` table.\r\nAlso, the column `latest_fail_reason` of `system.mutations` table has been cleared, I can no longer find the old error messages."
      }
    ]
  },
  {
    "number": 19223,
    "title": "Clickhouse failed to start, permission denied",
    "created_at": "2021-01-18T03:02:17Z",
    "closed_at": "2021-01-18T06:58:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19223",
    "body": "Hello\uff1a\r\nCentOS Linux release 7.9\r\nClickHouse server version 20.12.5.14 (official build).\r\nyum installed.\r\n\r\nwhen I use command \"systemctl start clickhouse-server\"   to start clickhouse , and Failed to start , \r\nclickhouse-server.err.log and clickhouse-server.log is empty .\r\n\r\nin /var/log/message\uff0cis\uff1a\r\nclickhouse-server: Processing configuration file '/etc/clickhouse-server/config.xml'.\r\nclickhouse-server: std::exception. Code: 1001, type: std::__1::__fs::filesystem::filesystem_error, e.what() = filesystem error: in posix_stat:  failed to determine attributes for the specified path: Permission denied [/etc/clickhouse-server/config.xml], Stack trace (when copying this message, always include the   \r\nlines below):\r\nclickhouse-server: 0. std::__1::system_error::system_error(std::__1::error_code, std::__1::basic_string<char, std::__1::char_traits<char>, std::    \r\n__1::allocator<char> > const&) @ 0x123f8d83 in ?\r\n\r\nI checked the directory and file permissions\uff0cis OK\r\n-rw-rw---- 1 clickhouse clickhouse 33809 Jan 18 09:06 config.xml\r\n\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 access\r\ndrwxr-x--- 10 clickhouse clickhouse 4096 Jan 18 09:30 data\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 format_schemas\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:30 log\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 tmp\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 user_files\r\n\r\nBut when I use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start \uff0cis OK\r\n\r\nCan you take a look for me? Thank you very much.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19223/comments",
    "author": "Goolen",
    "comments": [
      {
        "user": "zhangjmruc",
        "created_at": "2021-01-18T03:50:37Z",
        "body": "Would you please show us how you use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start?\r\nNormally, you should only use clickhouse user to manually start clickhouse server. \r\nsudo -u clickhouse /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml"
      },
      {
        "user": "hexiaoting",
        "created_at": "2021-01-18T04:06:11Z",
        "body": "check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T05:29:52Z",
        "body": "> check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)\r\n\r\n# ll -d /etc/\r\ndrwxr-xr-x. 110 root root 8192 Jan 15 15:13 /etc/\r\n\r\n# ll -d /etc/clickhouse-server\r\ndrw-rw---- 4 clickhouse clickhouse 183 Jan 18 09:06 /etc/clickhouse-server"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T05:31:46Z",
        "body": "> Would you please show us how you use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start?\r\n> Normally, you should only use clickhouse user to manually start clickhouse server.\r\n> sudo -u clickhouse /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\r\n\r\n# ps aux | grep click\r\nroot     15751  0.0  0.0 112828  2292 pts/0    S+   13:30   0:00 grep --color=auto click\r\n# /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\r\n# ps aux | grep click\r\nroot     15803  6.0  0.4 804000 133620 ?       DLsl 13:30   0:00 /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T05:32:58Z",
        "body": "[root@db1074 ~]# ps aux | grep click\r\nroot     15984  0.0  0.0 112828  2196 pts/0    S+   13:32   0:00 grep --color=auto click\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# systemctl start clickhouse-server\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# ps aux | grep click              \r\nroot     16021  0.0  0.0 112828  2244 pts/0    S+   13:32   0:00 grep --color=auto click"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T05:33:22Z",
        "body": "[root@db1074 ~]# ps aux | grep click\r\nroot     15751  0.0  0.0 112828  2292 pts/0    S+   13:30   0:00 grep --color=auto click\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# ps aux | grep click\r\nroot     15803  6.0  0.4 804000 133620 ?       DLsl 13:30   0:00 /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\r\nroot     15809  0.0  0.0 112828  2240 pts/0    S+   13:30   0:00 grep --color=auto click\r\n[root@db1074 ~]# "
      },
      {
        "user": "hexiaoting",
        "created_at": "2021-01-18T06:07:10Z",
        "body": "> > check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)\r\n> \r\n> # ll -d /etc/\r\n> drwxr-xr-x. 110 root root 8192 Jan 15 15:13 /etc/\r\n> \r\n> # ll -d /etc/clickhouse-server\r\n> drw-rw---- 4 clickhouse clickhouse 183 Jan 18 09:06 /etc/clickhouse-server\r\n\r\nchown /etc to owner clickhouse, and use clickhouse users to start server"
      },
      {
        "user": "zhangjmruc",
        "created_at": "2021-01-18T06:26:17Z",
        "body": "change the permission for /etc/clickhouse-server to drwxr-xr-x, as blow:\r\ndrwxr-xr-x   4 root                   root                     4096 Oct 26 10:15 clickhouse-server/\r\n\r\nchmod 755 /etc/clickhouse-server\r\n\r\n=== clickhouse-server should be ran with user clickhouse.====\r\n$ systemctl start clickhouse-server\r\n~$ ps -ef | grep clickhouse\r\n**clickho+**  53126      1 48 14:24 ?        00:00:04 /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --pid-file=/run/clickhouse-server/clickhouse-server.pid\r\n"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T06:53:17Z",
        "body": "> /etc/clickhouse-serv\r\n\r\nthank you first\r\nAfter changing the permissions to 755 , start is OK.\r\n\r\nThere is no permission to change this directory after installation\uff0cI don't know why it became 660.\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T06:55:22Z",
        "body": "> > > check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)\r\n> > \r\n> > \r\n> > # ll -d /etc/\r\n> > drwxr-xr-x. 110 root root 8192 Jan 15 15:13 /etc/\r\n> > # ll -d /etc/clickhouse-server\r\n> > drw-rw---- 4 clickhouse clickhouse 183 Jan 18 09:06 /etc/clickhouse-server\r\n> \r\n> chown /etc to owner clickhouse, and use clickhouse users to start server\r\n\r\nThank you.\r\nThe problem has been solved\uff0creference @zhangjmruc \r\n\r\n"
      },
      {
        "user": "krafter",
        "created_at": "2023-03-26T06:19:22Z",
        "body": "setting WorkingDirectory=/<config_file_dir>/<owned_by_clickhouse_user>/ in systemctl service helped me"
      },
      {
        "user": "amolsr",
        "created_at": "2023-04-04T06:04:37Z",
        "body": "what is the password for clickhouse ubuntu user.? I don't have access to root user of the machine."
      }
    ]
  },
  {
    "number": 19102,
    "title": "what 's real mean ClickHouseProfileEvents_MergeTreeDataWriterRows",
    "created_at": "2021-01-15T09:30:04Z",
    "closed_at": "2021-01-15T10:28:34Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19102",
    "body": "    HELP ClickHouseProfileEvents_MergeTreeDataWriterRows Number of rows INSERTed to MergeTree tables.\r\n    TYPE ClickHouseProfileEvents_MergeTreeDataWriterRows counter\r\n\r\nonly one table at my cluster. and replicatedMergeTree table.  two replicas\r\n\r\nthis is my query result.\r\n```SQL\r\nSELECT \r\n    count(1),\r\n    toStartOfMinute(ptime) AS time\r\nFROM xxx\r\nGROUP BY time\r\nORDER BY time DESC\r\nLIMIT 10\r\n\r\n\u250c\u2500count(1)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u2510\r\n\u2502  7088853 \u2502 2021-01-15 17:23:00 \u2502\r\n\u2502 10248679 \u2502 2021-01-15 17:22:00 \u2502\r\n\u2502  8418958 \u2502 2021-01-15 17:21:00 \u2502\r\n\u2502  5623445 \u2502 2021-01-15 17:20:00 \u2502\r\n\u2502  7268165 \u2502 2021-01-15 17:19:00 \u2502\r\n\u2502  7144866 \u2502 2021-01-15 17:18:00 \u2502\r\n\u2502  2571437 \u2502 2021-01-15 17:17:00 \u2502\r\n\u2502  3132464 \u2502 2021-01-15 17:16:00 \u2502\r\n\u2502  4344607 \u2502 2021-01-15 17:15:00 \u2502\r\n\u2502  7879506 \u2502 2021-01-15 17:14:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nand ClickHouseProfileEvents_MergeTreeDataWriterRows between 1 minute 200Mil.\r\nso anybody help me ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19102/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-15T09:36:35Z",
        "body": "Rows written while merge are also counted."
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-01-15T10:28:34Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 19030,
    "title": "MaterializeMySQL will get exception when create database on cluster",
    "created_at": "2021-01-14T07:29:44Z",
    "closed_at": "2021-01-18T10:39:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19030",
    "body": "**Describe the bug**\r\nWhen i create MaterializeMySQL engine database on cluster,I will get the following execption,but i already set allow_experimental_database_materialize_mysql = 1 on every ClickHouse node.\r\nIf i create MaterializeMySQL engine database on single ClickHouse node,it will be successed.\r\n```\r\n2021.01.14 15:09:58.759295 [ 25034 ] {ee3c8887-ef0f-454b-ac10-5089e001fcae} <Error> DDLWorker: Query CREATE DATABASE test_mysql_new UUID '34f48f87-411b-4534-b64e-3bd646dd91f8' ENGINE = MaterializeMySQL('clickhouse2:3307', 'test', 'root', 'root123') wasn't finished successfully: Code: 336, e.displayText() = DB::Exception: MaterializeMySQL is an experimental database engine. Enable allow_experimental_database_materialize_mysql to use it., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::InterpreterCreateQuery::createDatabase(DB::ASTCreateQuery&) @ 0xdc75c5f in /usr/bin/clickhouse\r\n1. DB::InterpreterCreateQuery::execute() @ 0xdc81317 in /usr/bin/clickhouse\r\n2. ? @ 0xe060347 in /usr/bin/clickhouse\r\n3. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>) @ 0xe063377 in /usr/bin/clickhouse\r\n4. DB::DDLWorker::tryExecuteQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::DDLTask const&, DB::ExecutionStatus&) @ 0xda5ec43 in /usr/bin/clickhouse\r\n5. DB::DDLWorker::processTask(DB::DDLTask&) @ 0xda60eb9 in /usr/bin/clickhouse\r\n6. DB::DDLWorker::enqueueTask(std::__1::unique_ptr<DB::DDLTask, std::__1::default_delete<DB::DDLTask> >) @ 0xda5f5e2 in /usr/bin/clickhouse\r\n7. ? @ 0xda6f64d in /usr/bin/clickhouse\r\n8. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x7dc870d in /usr/bin/clickhouse\r\n9. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x7dcac6f in /usr/bin/clickhouse\r\n10. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x7dc5b3d in /usr/bin/clickhouse\r\n11. ? @ 0x7dc96f3 in /usr/bin/clickhouse\r\n12. start_thread @ 0x7dc5 in /usr/lib64/libpthread-2.17.so\r\n13. __clone @ 0xf61cd in /usr/lib64/libc-2.17.so\r\n (version 20.12.5.14 (official build))\r\n\r\n```\r\n**How to reproduce**\r\nClickHouse server version is 20.12.5.14\r\nMySQL server version is 8.0.22\r\nExecute\r\n```\r\ncreate database test_materializemysql on cluster test_cluster engine = MaterializeMySQL('mysqlhost:port', 'test', 'test', 'test');\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19030/comments",
    "author": "Hedwiglzy",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2021-01-18T10:38:52Z",
        "body": "It's because `SET` query changes setting value in current session. Distributed DDL queries are executed in separate session and `SET` query does not affect it. You should enable `allow_experimental_database_materialize_mysql` setting globally in server config to make it work with `ON CLUSTER` queries. Example:\r\n```\r\n    <profiles>\r\n            <default>\r\n                    <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n                    ...\r\n            </default>\r\n            ...\r\n    </profiles>\r\n```\r\n\r\nAnother option is to configure distributed DDL queries to use separate settings profile with all settings you need:\r\n```\r\n    <profiles>\r\n            <default>\r\n                    ...\r\n            </default>\r\n            <dddl>\r\n                    <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n            </dddl>\r\n            ...\r\n    </profiles>\r\n   ...\r\n   <distributed_ddl>\r\n        <path>/clickhouse/task_queue/ddl</path>\r\n        <profile>dddl</profile>\r\n    </distributed_ddl>\r\n```"
      },
      {
        "user": "Hedwiglzy",
        "created_at": "2021-01-25T02:26:16Z",
        "body": "> > It's because `SET` query changes setting value in current session. Distributed DDL queries are executed in separate session and `SET` query does not affect it. You should enable `allow_experimental_database_materialize_mysql` setting globally in server config to make it work with `ON CLUSTER` queries. Example:\r\n> > ```\r\n> >     <profiles>\r\n> >             <default>\r\n> >                     <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n> >                     ...\r\n> >             </default>\r\n> >             ...\r\n> >     </profiles>\r\n> > ```\r\n> > \r\n> > \r\n> > Another option is to configure distributed DDL queries to use separate settings profile with all settings you need:\r\n> > ```\r\n> >     <profiles>\r\n> >             <default>\r\n> >                     ...\r\n> >             </default>\r\n> >             <dddl>\r\n> >                     <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n> >             </dddl>\r\n> >             ...\r\n> >     </profiles>\r\n> >    ...\r\n> >    <distributed_ddl>\r\n> >         <path>/clickhouse/task_queue/ddl</path>\r\n> >         <profile>dddl</profile>\r\n> >     </distributed_ddl>\r\n> > ```\r\n> \r\n> I still got the error after setting the server config. Any suggestion else?\r\n\r\nI follow the above method to config clickhouse is OK,you need config two files,<allow_experimental_database_materialize_mysql> in user.xml,<distributed_ddl> in config.xml,did you configuration both?"
      },
      {
        "user": "sliontc",
        "created_at": "2021-01-25T07:01:27Z",
        "body": "Yes, I figured it out that the clickhouse service have to be restarted finally."
      },
      {
        "user": "tavplubix",
        "created_at": "2021-01-25T10:12:24Z",
        "body": "@sliontc you should change the password. Please, be careful when posting screenshots, logs, configs and other stuff, which may contain sensitive data."
      },
      {
        "user": "sliontc",
        "created_at": "2021-01-26T00:47:47Z",
        "body": "> @sliontc you should change the password. Please, be careful when posting screenshots, logs, configs and other stuff, which may contain sensitive data.\r\n\r\nOMG, could you please delete the image? I already delete my post."
      },
      {
        "user": "naveedmm",
        "created_at": "2021-11-03T04:40:57Z",
        "body": "Server doesn't restart after doing to suggested changes\r\n                    `<allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>`\r\nThis is causing the restart to fail"
      },
      {
        "user": "tavplubix",
        "created_at": "2021-11-03T10:02:08Z",
        "body": "> Server doesn't restart after doing to suggested changes `<allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>` This is causing the restart to fail\r\n\r\nCheck server log, it contains error messages explaining why server cannot start. Most likely it's because the setting `allow_experimental_database_materialize_mysql` was renamed to `allow_experimental_database_materialized_mysql`, so server does not start due to unknown setting."
      }
    ]
  },
  {
    "number": 18872,
    "title": "apply max_execution_time but without using an estimate?",
    "created_at": "2021-01-08T16:41:16Z",
    "closed_at": "2021-01-08T17:05:55Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18872",
    "body": "**Describe the bug**\r\n\r\nI have been using `max_execution_time` to limit query complexity. Normally this works great but there are some situations where very complex queries (1 page of sql so won't include below) are estimated at 1800s but actually complete in 4s. Because the estimate was 1800s I get the error message \"Estimated query execution time (1807.380046338318 seconds) is too long. Maximum: 60. Estimated rows to process: 2871497: While executing MergeTreeThread (version 20.11.3.3 (official build))\".\r\n\r\nAs there will always be issues around estimating query execution time, my request is to have an option alongside max_execution_time which will say whether to rely on estimates or whether to just run it and abort after the specified time.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18872/comments",
    "author": "mzealey",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-01-08T16:52:08Z",
        "body": "you need to set `timeout_before_checking_execution_speed=0` this disables estimation"
      },
      {
        "user": "mzealey",
        "created_at": "2021-01-08T17:05:55Z",
        "body": "ok thank you. i saw some comments around this but assumed it was only applicable for the `min_execution_speed` type options"
      }
    ]
  },
  {
    "number": 18854,
    "title": "why select count(*) from numbers(10000000) cannot run in readonly mode?",
    "created_at": "2021-01-08T01:18:28Z",
    "closed_at": "2021-01-08T14:04:31Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18854",
    "body": "Code: 164, e.displayText() = DB::Exception: play: Cannot execute query in readonly mode (version 20.13.1.5552 (official build))\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18854/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-01-08T01:36:10Z",
        "body": "All table functions require RW privileges by design.\r\nYou can use `select count(*) from system.numbers where number <= 10000000` instead."
      },
      {
        "user": "l1t1",
        "created_at": "2021-01-08T04:21:43Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 18759,
    "title": "ClickHouseProfileEvents_ZooKeeperWaitMicroseconds very big",
    "created_at": "2021-01-05T03:34:12Z",
    "closed_at": "2021-01-06T09:18:37Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18759",
    "body": "ClickHouseProfileEvents_ZooKeeperWaitMicroseconds 6564505184722\r\nafter 1min\r\nClickHouseProfileEvents_ZooKeeperWaitMicroseconds 6567706573267\r\n\r\nAbout 3000 seconds\r\n\r\nplease help.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18759/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-05T17:29:55Z",
        "body": "3000 seconds spent for one minute means that you have at least 50 concurrent requests to ZooKeeper which is normal if you have at least a few 10s of Replicated tables."
      },
      {
        "user": "gj-zhang",
        "created_at": "2021-01-06T09:18:37Z",
        "body": "> 3000 seconds spent for one minute means that you have at least 50 concurrent requests to ZooKeeper which is normal if you have at least a few 10s of Replicated tables.\r\n\r\nok thank you"
      },
      {
        "user": "zhoupengbo",
        "created_at": "2022-04-26T06:26:50Z",
        "body": "What does this indicator mean?  ClickHouseProfileEvents_ZooKeeperWaitMicroseconds \r\n@alexey-milovidov my nums is about 10 hours, and increacing...\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-04-27T02:50:18Z",
        "body": "It means that 10 hours has been spent in waiting for responses from ZooKeeper since server startup.\r\nThe waiting is done asynchronously and in parallel, so you'd better not to care."
      }
    ]
  },
  {
    "number": 18551,
    "title": "[Guidance] Table Migration (due to changing primary key)",
    "created_at": "2020-12-27T13:31:36Z",
    "closed_at": "2020-12-27T22:40:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18551",
    "body": "Working with Clickhouse and all is going well, but today I ran into the problem of needing to a change the name of a primary key on a table. \r\n\r\nAfter some research, it appears that currently a migration of data to a new table is needed to do this. It would be helpful to have some basic syntax and guidelines for how a table migration should happen with Clickhouse. \r\n\r\n- What is the preferred approach for a non-zookeeper managed server?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18551/comments",
    "author": "arpowers",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-27T16:13:19Z",
        "body": "Create a new table.` insert into new select * from old`\r\n"
      },
      {
        "user": "arpowers",
        "created_at": "2020-12-27T16:54:19Z",
        "body": "@den-crane simple enough! thanks"
      }
    ]
  },
  {
    "number": 18346,
    "title": "Subquery optimization question",
    "created_at": "2020-12-22T04:08:19Z",
    "closed_at": "2021-01-27T16:27:32Z",
    "labels": [
      "question",
      "comp-optimizers"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18346",
    "body": "I tried to look at the docs and an explain query but couldn't tell... but if I have a query that contains a subquery and the outer query doesn't use all of the fields of the subquery, are those columns ever read?\r\n\r\nFor example:\r\n\r\n```sql\r\nSELECT count(id)\r\nFROM\r\n(\r\n   SELECT id,\r\n    any(country) as country,\r\n    any(state)  as state\r\n  FROM items\r\n  GROUP BY id\r\n) x\r\n```\r\n\r\nWould the country and state fields be read by clickhouse even though the outer query never uses them, i.e. would they be pruned?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18346/comments",
    "author": "mauidude",
    "comments": [
      {
        "user": "mauidude",
        "created_at": "2021-01-24T17:30:19Z",
        "body": "Following up on this... anyone know?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-01-24T21:35:10Z",
        "body": "\r\n\r\n```sql\r\ncreate table items ( id Int64, country String, state String) Engine=MergeTree order by tuple();\r\ninsert into items select number , toString(cityHash64(number)) , \r\ntoString(cityHash64(number)) from numbers(200000000);\r\n\r\nSELECT id,any(country) as country, any(state)  as state \r\nFROM items GROUP BY id format Null  \r\nElapsed: 38.773 sec.\r\n\r\n\r\nSELECT id\r\nFROM items GROUP BY id format Null  \r\nElapsed: 7.238 sec.\r\n\r\n\r\nSELECT count(id)\r\nFROM( SELECT id, any(country) as country, any(state)  as state\r\n    FROM items GROUP BY id\r\n) x format Null  \r\nElapsed: 7.241 sec.\r\n"
      },
      {
        "user": "mauidude",
        "created_at": "2021-01-27T16:27:32Z",
        "body": "thank you! that was helpful!"
      }
    ]
  },
  {
    "number": 18123,
    "title": "What's the difference between Atomic and Ordinary database engine",
    "created_at": "2020-12-16T03:08:25Z",
    "closed_at": "2020-12-16T17:09:59Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18123",
    "body": "I've searched the official doc and hasn't found anything about this.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18123/comments",
    "author": "Fullstop000",
    "comments": [
      {
        "user": "taiyang-li",
        "created_at": "2020-12-16T03:19:28Z",
        "body": "I found this comment in source file: `src/Databases/DatabaseAtomic.h`\r\n```\r\n/// All tables in DatabaseAtomic have persistent UUID and store data in\r\n/// /clickhouse_path/store/xxx/xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy/\r\n/// where xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy is UUID of the table.\r\n/// RENAMEs are performed without changing UUID and moving table data.\r\n/// Tables in Atomic databases can be accessed by UUID through DatabaseCatalog.\r\n/// On DROP TABLE no data is removed, DatabaseAtomic just marks table as dropped\r\n/// by moving metadata to /clickhouse_path/metadata_dropped/ and notifies DatabaseCatalog.\r\n/// Running queries still may use dropped table. Table will be actually removed when it's not in use.\r\n/// Allows to execute RENAME and DROP without IStorage-level RWLocks\r\n```"
      },
      {
        "user": "Fullstop000",
        "created_at": "2020-12-16T03:24:29Z",
        "body": "@taiyang-li These comments are really helpful! Hope the doc can be updated soon."
      },
      {
        "user": "tavplubix",
        "created_at": "2020-12-16T12:04:31Z",
        "body": "See also #17906"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-12-16T17:09:59Z",
        "body": "Motivation: #6787"
      }
    ]
  },
  {
    "number": 18085,
    "title": "What is the difference between attach/detach and move partition",
    "created_at": "2020-12-15T03:45:35Z",
    "closed_at": "2020-12-26T16:53:22Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18085",
    "body": "Hi Team,\r\nMy requirement is move partition from one table and another table. \r\nBoth `ATTACH PARTITION FROM` and `MOVE PARTITION TO TABLE` could meet my requirement, but what is the difference and which one has better performance?\r\n\r\nThanks!\r\nWenjun",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18085/comments",
    "author": "RangerWolf",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2020-12-17T18:46:43Z",
        "body": "These queries are almost identical, except the following details:\r\n - `MOVE PARTITION TO TABLE` deletes partition from the source table, `ATTACH PARTITION FROM` doesn't.\r\n - It's not possible to `MOVE` partitions between replicated and not-replicated tables, `ATTACH` should work fine with `MergeTree` and `ReplicatedMergeTree`.\r\n\r\nThere is also `REPLACE PARTITION` query, it's similar to `ATTACH PARTITION FROM`, but it deletes partition from the destination table before  attaching.\r\n\r\n"
      },
      {
        "user": "RangerWolf",
        "created_at": "2020-12-27T12:19:28Z",
        "body": "> These queries are almost identical, except the following details:\r\n> \r\n> * `MOVE PARTITION TO TABLE` deletes partition from the source table, `ATTACH PARTITION FROM` doesn't.\r\n> * It's not possible to `MOVE` partitions between replicated and not-replicated tables, `ATTACH` should work fine with `MergeTree` and `ReplicatedMergeTree`.\r\n> \r\n> There is also `REPLACE PARTITION` query, it's similar to `ATTACH PARTITION FROM`, but it deletes partition from the destination table before attaching.\r\n\r\nThanks, but which one has better performance?  @tavplubix "
      },
      {
        "user": "tavplubix",
        "created_at": "2020-12-27T12:23:28Z",
        "body": "All of them are almost identical, so performance is the same"
      },
      {
        "user": "RangerWolf",
        "created_at": "2020-12-28T06:37:14Z",
        "body": "> All of them are almost identical, so performance is the same\r\n\r\nThanks!"
      },
      {
        "user": "ivan-tkatchev",
        "created_at": "2024-02-26T17:26:12Z",
        "body": "> All of them are almost identical, so performance is the same\r\n\r\nNot at all true.\r\nMOVE has some replication queue magic under the hood. MOV'ing a partition with lots of parts dumped tens of thousands of entries into replication_queue, after which the replication queue stopped processing and data was eventually lost. (Seems like the replication magic has lots of race conditions and bugs.)\r\n\r\nATTACH FROM just copied parts into the table. Not sure if the parts will be replicated eventually or not.\r\n\r\n(Both tables are replicated in this case.)\r\n"
      }
    ]
  },
  {
    "number": 18001,
    "title": "Columns are from different tables while processing dateDiff",
    "created_at": "2020-12-11T09:58:36Z",
    "closed_at": "2020-12-14T00:24:47Z",
    "labels": [
      "question",
      "comp-joins",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18001",
    "body": "I'm trying to calc the next day retention login user with ClickHouse.\r\n\r\nThe table structure of `t_user_login` is:\r\n\r\n```\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 user    \u2502 String                    \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2502 log_day \u2502 DateTime('Asia/Shanghai') \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd the SQL is:\r\n\r\n```sql\r\nSELECT DISTINCT log_day,a.user as user_day0,b.user as user_day1\r\nFROM (\r\n  SELECT min(log_day) as log_day, user\r\n  FROM t_user_login\r\n  GROUP BY user\r\n) a\r\nLEFT JOIN t_user_login b\r\nON dateDiff('day', b.log_day, a.log_day) = 1 AND a.user = b.user;\r\n```\r\n\r\nBut received an exception:\r\n\r\n> Received exception from server (version 20.11.4):\r\nCode: 403. DB::Exception: Received from localhost:9000. DB::Exception: Invalid columns in JOIN ON section. Columns b.log_day and log_day are from different tables.: While processing dateDiff('day', b.log_day, log_day) = 1.\r\n\r\nThis really confused me for a long time. Anyone can help me, thanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18001/comments",
    "author": "shuizhongyueming",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-11T16:15:06Z",
        "body": "toStartOfDay(b.log_day - interval 1 day) =toStartOfDay(a.log_day)\r\n\r\n```sql\r\nSELECT DISTINCT log_day,a.user as user_day0,b.user as user_day1\r\nFROM (\r\n  SELECT min(log_day) as log_day, user\r\n  FROM t_user_login\r\n  GROUP BY user\r\n) a\r\nLEFT JOIN t_user_login b\r\nON toStartOfDay(b.log_day - interval 1 day) =toStartOfDay(a.log_day) AND a.user = b.user;\r\n\r\n```"
      },
      {
        "user": "shuizhongyueming",
        "created_at": "2020-12-12T06:54:54Z",
        "body": "@den-crane It works! Thank you!\r\nBut I still want ask: did the `dateDiff` can't be use at this context?"
      },
      {
        "user": "filimonov",
        "created_at": "2020-12-13T23:25:41Z",
        "body": "Currently clickhouse supports only equijoins. "
      }
    ]
  },
  {
    "number": 17999,
    "title": "now()    timezone is client timezone not server timezone?",
    "created_at": "2020-12-11T06:37:35Z",
    "closed_at": "2020-12-18T17:40:04Z",
    "labels": [
      "question",
      "comp-datetime"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17999",
    "body": "sql \uff1aselect now(),toString(now())\r\nresult:\r\n|now()|toString(now())|\r\n|-----|---------------|\r\n|2020-12-11 06:37:09|2020-12-10 22:37:09|\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17999/comments",
    "author": "jjtjiang",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-11T16:18:58Z",
        "body": "Yes. What API do you use? JDBC ?\r\n"
      },
      {
        "user": "filimonov",
        "created_at": "2020-12-13T23:27:48Z",
        "body": "`now()` for native clients the number is send to a client and client do formatting to string.\r\n\r\n`toString(now())` server do a formatting and send string. "
      },
      {
        "user": "jjtjiang",
        "created_at": "2020-12-14T06:57:45Z",
        "body": "got it, I use JDBC .thanks @den-crane @filimonov "
      }
    ]
  },
  {
    "number": 17885,
    "title": "how to make groupArray more faster?",
    "created_at": "2020-12-08T06:07:55Z",
    "closed_at": "2020-12-17T05:29:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17885",
    "body": "My query result has 1 million and need to page it,  now implemented by groupArray, but it is slow, is there any way to make it faster ?\r\neg, when click page \"3\" , I need return No. 31~40 row. ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17885/comments",
    "author": "vegastar002",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2020-12-08T10:49:50Z",
        "body": "plz post the full sql "
      },
      {
        "user": "den-crane",
        "created_at": "2020-12-08T14:41:14Z",
        "body": "Do not use groupArray for this.\r\n\r\n```\r\ncreate table temp(A Int64) Engine=Log;\r\ninsert into temp select * from numbers_mt(100000000);\r\nselect * from temp limit 10 offset 99000000;\r\n10 rows in set. Elapsed: 0.124 sec. Processed 99.08 million rows,\r\n```"
      },
      {
        "user": "vegastar002",
        "created_at": "2020-12-17T05:29:29Z",
        "body": "yes, finally I found can use this way to page"
      }
    ]
  },
  {
    "number": 17648,
    "title": "Failed to determine user credentials",
    "created_at": "2020-12-01T02:41:01Z",
    "closed_at": "2020-12-01T20:12:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17648",
    "body": "(you don't have to strictly follow this form)\r\n\r\n**Describe the bug**\r\nThe version is : 20.11.4.13\r\nUbuntu: 18.04.5 LTS\r\nWhen run command: sudo /etc/init.d/clickhouse-server start, I got an err message:\r\nDec  1 10:35:24 apps-domain systemd[7878]: clickhouse-server.service: Failed to determine user credentials: No such process\r\nDec  1 10:35:24 apps-domain systemd[7878]: clickhouse-server.service: Failed at step USER spawning /usr/bin/clickhouse-server: No such process\r\n\r\nWhoever has encountered it\uff0ccan reply to the solution\u3002\r\nThank you very much!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17648/comments",
    "author": "gavinju",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-01T03:28:35Z",
        "body": "Do not use `/etc/init.d/clickhouse-server start`\r\n\r\nUSE: \r\n`sudo systemctl start clickhouse-server` \r\n`sudo systemctl status clickhouse-server` \r\n`sudo systemctl stop clickhouse-server`\r\n`sudo journalctl -u clickhouse-server.service`"
      },
      {
        "user": "gavinju",
        "created_at": "2020-12-01T10:24:15Z",
        "body": "hi, den-crane\r\nthanks for your replay!  I tried it, but the same error is displayed.\r\nBut, use the following can run correctly:\r\ncoclickhouse-server --config-file=/etc/clickhouse-server/config.xml"
      },
      {
        "user": "den-crane",
        "created_at": "2020-12-01T15:00:21Z",
        "body": "try \r\n\r\nsudo -u clickhouse clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n\r\nServices use clickhouse user. "
      },
      {
        "user": "gavinju",
        "created_at": "2020-12-01T20:11:36Z",
        "body": "thank you!   It's ok!"
      }
    ]
  },
  {
    "number": 17312,
    "title": "PDO integration",
    "created_at": "2020-11-23T14:58:22Z",
    "closed_at": "2020-11-23T19:52:47Z",
    "labels": [
      "question",
      "comp-mysql"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17312",
    "body": "**Describe the bug**\r\n\r\nI can't connect to ClickHouse via the mysql interface using PDO\r\n\r\n**How to reproduce**\r\n* version 20.11.2.1 (official build)\r\n* PHP 7.4.3\r\n\r\n```php\r\n$pdo=new PDO(\"mysql:host=127.0.0.1;port=8123;dbname=test\", \"default\",\"\",[PDO::ATTR_TIMEOUT => 5,PDO::ATTR_ERRMODE => PDO::ERRMODE_EXCEPTION]);\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17312/comments",
    "author": "god1dog",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-23T15:48:12Z",
        "body": "```\r\ncat /etc/clickhouse-server/config.xml|grep mysql\r\n    <mysql_port>9004</mysql_port>\r\n\r\n```\r\nYou should use port 9004 not `port=8123`;"
      },
      {
        "user": "god1dog",
        "created_at": "2020-11-23T19:52:47Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 17282,
    "title": "Last in the selection ReplacingMergeTree.",
    "created_at": "2020-11-23T02:24:07Z",
    "closed_at": "2020-11-27T03:24:35Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17282",
    "body": "I use ReplacingMergeTree, after a while old data versions lose. I want to ask how to configure that time? And I want to configure ReplacingMergeTree every time I push data to save only the latest version?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17282/comments",
    "author": "vladimirteddy",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-11-23T07:39:14Z",
        "body": "> I use ReplacingMergeTree, after a while old data versions lose. I want to ask how to configure that time?\r\n\r\nNo way for that. If you need to preserve all versions of your data row - use plain MergeTree (may be with some TTL expression), and argMax aggregate functions go access last values (or create materialized view to access last values using Replacing / Aggregating)\r\n\r\n> And I want to configure ReplacingMergeTree every time I push data to save only the latest version?\r\n\r\nNot possible. It's eventual. If your inserts happens very rare (like once per day) you can use OPTIMIZE FINAL command after your inserts. But it will not work with real-time ingestion."
      },
      {
        "user": "vladimirteddy",
        "created_at": "2020-11-24T02:15:17Z",
        "body": "Thank you @filimonov ."
      }
    ]
  },
  {
    "number": 17272,
    "title": "why query Memory engine table is slower than MergeTree table",
    "created_at": "2020-11-22T10:37:11Z",
    "closed_at": "2020-11-22T21:52:42Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17272",
    "body": "```\r\nVM_0_52_centos :) create table sfz_mem engine=Memory as select * from sfz;\r\n\r\nCREATE TABLE sfz_mem\r\nENGINE = Memory AS\r\nSELECT *\r\nFROM sfz\r\n\r\nQuery id: 0007c552-2137-4857-a0ba-d12808c4b9e7\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 1.642 sec. Processed 49.61 million rows, 396.89 MB (30.20 million rows/s., 241.64 MB/s.)\r\n\r\n\r\nVM_0_52_centos :) select sum(toInt128(code)),floor(code/1e15)b from sfz_mem group by b;\r\n\r\nSELECT\r\n    sum(toInt128(code)),\r\n    floor(code / 1000000000000000.) AS b\r\nFROM sfz_mem\r\nGROUP BY b\r\n\r\nQuery id: 668733c8-3d03-4039-a4f2-fe86e6aeed8d\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(toInt128(code))\u2500\u252c\u2500\u2500\u2500b\u2500\u2510\r\n\u2502 5481829832813867983754400 \u2502 110 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.828 sec. Processed 49.61 million rows, 396.89 MB (59.89 million rows/s., 479.13 MB/s.)\r\n\r\nVM_0_52_centos :) select sum(toInt128(code)),floor(code/1e15)b from sfz group by b;\r\n\r\nSELECT\r\n    sum(toInt128(code)),\r\n    floor(code / 1000000000000000.) AS b\r\nFROM sfz\r\nGROUP BY b\r\n\r\nQuery id: 38393b3e-10ae-4b4f-992b-a968fd84357c\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(toInt128(code))\u2500\u252c\u2500\u2500\u2500b\u2500\u2510\r\n\u2502 5481829832813867983754400 \u2502 110 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.289 sec. Processed 49.61 million rows, 396.89 MB (171.83 million rows/s., 1.37 GB/s.)\r\n\r\nVM_0_52_centos :) desc sfz;\r\n\r\nDESCRIBE TABLE sfz\r\n\r\nQuery id: 7be0cb1c-908f-4a0d-88bc-3c3630352112\r\n\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 code \u2502 Int64 \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.021 sec.\r\n\r\nVM_0_52_centos :) desc sfz_mem;\r\n\r\nDESCRIBE TABLE sfz_mem\r\n\r\nQuery id: 0b6b2fc5-d08c-45d8-af43-34b6762c10e7\r\n\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 code \u2502 Int64 \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.026 sec.\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17272/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-22T15:31:13Z",
        "body": "Because of MT internal structure (index+marks) it can be read in by many threads in parallel.\r\n\r\n```sql\r\ncreate table sfz(code Int64) Engine=MergeTree order by code;\r\ninsert into sfz select * from numbers(1000000000);\r\ncreate table sfz_mem engine=Memory as select * from sfz;\r\n\r\n\r\nselect sum(toInt128(code)),floor(code/1e15)b from sfz_mem group by b;\r\nElapsed: 4.694 sec.\r\n\r\nselect sum(toInt128(code)),floor(code/1e15)b from sfz group by b;\r\nElapsed: 1.961 sec.\r\n\r\nset max_threads=1, max_streams_to_max_threads_ratio=1;\r\n\r\nselect sum(toInt128(code)),floor(code/1e15)b from sfz_mem group by b;\r\nElapsed: 7.357 sec.\r\n\r\nselect sum(toInt128(code)),floor(code/1e15)b from sfz group by b;\r\nElapsed: 13.933 sec.\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-22T21:52:42Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 17230,
    "title": "Storing huge amount of log entries",
    "created_at": "2020-11-20T13:30:25Z",
    "closed_at": "2020-11-25T08:59:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17230",
    "body": "Hello,\r\n\r\nI would like to use ClickHouse for storing huge amount of log entries (sorted without ORDER BY) and after some time dump it to disk and send to the clients. \r\nI have tried to use tables with mergetree table engine but, as the description of mergetree engine says it was not sorted, because not all parts were merged. \r\n\r\nIs there a way how to achieve desired behavior? I would like to have sorted logs without ORDER BY expression. Is it even possible with some engines?\r\nI have look into Log engine but it does not support TTL and replication.\r\n\r\nThank you for help",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17230/comments",
    "author": "wutchzone",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-20T14:43:38Z",
        "body": "You should use *MergeTree. Other engines is not suggested for permanent data.\r\nCan you show \"create table xxx\" for MergeTree ?\r\nYou can use MergeTree without ORDERBY (`order by tuple()`), but usually it has no sense. At least you can add fake column with insertion date.\r\n\r\n```sql\r\nCREATE TABLE mylogs\r\n(\r\n    `Log` String,\r\n    `created_at` Date MATERIALIZED today()\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(created_at)\r\nORDER BY created_at\r\n```\r\n\r\nCan you show your supposed selects ? "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-11-21T21:06:41Z",
        "body": "> Is there a way how to achieve desired behavior? I would like to have sorted logs without ORDER BY expression. Is it even possible with some engines?\r\n\r\nLogs are typically stored in MergeTree table with ORDER BY time column."
      },
      {
        "user": "wutchzone",
        "created_at": "2020-11-25T08:59:52Z",
        "body": "@den-crane @alexey-milovidov \r\nThank you both for help. Firstly I was very confused when I used the MergeeTree engine, because I tried to dump the data too early. Propably many parts were not merged and it consumed a lot of RAM when used with ORDER BY. When I try to dump after hour when they were inserted, it does not consumed RAM at all and it performs smoothly. I was just afraid that this cannot be solved with mergee tree, but it can. Now everything is working perfectly, thank you once more."
      }
    ]
  },
  {
    "number": 17054,
    "title": "why the first 1e8 rows inserted and the 2nd 1e8 rows failed",
    "created_at": "2020-11-16T05:12:07Z",
    "closed_at": "2020-11-17T22:05:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17054",
    "body": "```\r\ncreate table sfz15y engine=MergeTree()order by id as select a.number*10000+b.number id from numbers(50000)a,numbers(10000)b;\r\ninsert into  sfz15y select (a.number+50000)*10000+b.number id from numbers(50000)a,numbers(10000)b;\r\ninsert into  sfz15y select (a.number)*10000+b.number id from numbers(50000)a,numbers(10000)b where b.number%5=0;\r\ncreate table sfzcm engine=MergeTree()order by id as select id,count(*)c from sfz15y group by id having count(*)>1;\r\n```\r\n**failed, but the table was created** , then i try to reduce the rows.\r\n```\r\ninsert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;\r\nQuery id: da659738-3916-466c-a392-718224d9e178\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 10.615 sec. Processed 120.01 million rows, 960.10 MB (11.31 million rows/s., 90.45 MB/s.)\r\n\r\nDESKTOP-RS3EG9A.localdomain :) select count(*) from sfzcm;\r\n\r\nQuery id: bd043a67-f81a-475e-90da-ebc6a44aabef\r\n\r\n\u250c\u2500\u2500count()\u2500\u2510\r\n\u2502 20000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec.\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;\r\n```\r\nReceived exception from server (version 20.11.3):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (total) exceeded: would use 11.08 GiB (attempt to allocate chunk of 6291456 bytes), maximum: 11.08 GiB: While executing AggregatingTransform.\r\n\r\n0 rows in set. Elapsed: 2.440 sec. Processed 92.93 million rows, 743.44 MB (38.08 million rows/s., 304.67 MB/s.)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17054/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-16T18:41:10Z",
        "body": "```\r\nset max_memory_usage='10G', max_bytes_before_external_group_by='3G';\r\n\r\nCREATE TABLE sfz15y\r\nENGINE = MergeTree()\r\nORDER BY id AS\r\nSELECT (a.number * 10000) + b.number AS id\r\nFROM numbers(50000) AS a\r\n, numbers(10000) AS b\r\n\r\n0 rows in set. Elapsed: 10.107 sec.\r\n\r\n\r\n\r\nINSERT INTO sfz15y SELECT (a.number * 10000) + b.number AS id\r\nFROM numbers(50000) AS a\r\n, numbers(10000) AS b\r\nWHERE (b.number % 5) = 0\r\n\r\n0 rows in set. Elapsed: 4.232 sec.\r\n\r\n\r\nCREATE TABLE sfzcm\r\nENGINE = MergeTree()\r\nORDER BY id AS\r\nSELECT\r\n    id,\r\n    count(*) AS c\r\nFROM sfz15y\r\nGROUP BY id\r\nHAVING count(*) > 1\r\n\r\n0 rows in set. Elapsed: 53.631 sec.\r\n\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;\r\n\r\n0 rows in set. Elapsed: 11.482 sec.\r\n\r\n\r\nselect count(*) from sfzcm;\r\n\r\n\u250c\u2500\u2500\u2500count()\u2500\u2510\r\n\u2502 120000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;\r\n\r\n0 rows in set. Elapsed: 11.149 sec.\r\n\r\n```\r\n"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-16T22:47:14Z",
        "body": "thanks, and I wonder why those two inserts need different memory size? "
      },
      {
        "user": "den-crane",
        "created_at": "2020-11-16T23:32:26Z",
        "body": "Which 2 ?"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-17T00:00:54Z",
        "body": "1.`insert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;`\r\n2.`insert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;`"
      },
      {
        "user": "den-crane",
        "created_at": "2020-11-17T00:29:44Z",
        "body": "```\r\nSET send_logs_level = 'debug'\r\n\r\n1 insert : MemoryTracker: Peak memory usage (for query): 4.93 GiB.\r\n2 insert : MemoryTracker: Peak memory usage (for query): 4.02 GiB.\r\n\r\nset max_memory_usage='40G', max_bytes_before_external_group_by=0\r\n\r\n1 insert : MemoryTracker: Peak memory usage (for query): 5.06 GiB.\r\n2 insert : MemoryTracker: Peak memory usage (for query): 5.06 GiB.\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-17T00:57:49Z",
        "body": "thanks, one more question\r\nhow to check the current value of  `max_memory_usage, max_bytes_before_external_group_by` etc"
      },
      {
        "user": "den-crane",
        "created_at": "2020-11-17T19:37:55Z",
        "body": "```sql\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE name IN ('max_memory_usage', 'max_bytes_before_external_group_by')\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_bytes_before_external_group_by \u2502 76027960320  \u2502\r\n\u2502 max_memory_usage                   \u2502 152055920640 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-17T22:05:18Z",
        "body": "got it. thanks"
      }
    ]
  },
  {
    "number": 16799,
    "title": "DB::Exception: Unknown function avg (version 20.9.3.45 (official build))",
    "created_at": "2020-11-09T03:18:18Z",
    "closed_at": "2020-11-09T10:35:02Z",
    "labels": [
      "question",
      "unexpected behaviour"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16799",
    "body": "```sql\r\nSELECT\r\n\t(intDiv(toUInt32(log_time), 1) * 1) * 1000 as t,\r\n\tavg(`request_time`) as a\r\nFROM\r\n\tELB_LOG.api_log\r\nWHERE\r\n\t\"log_time\" >= toDateTime(1604315080)\r\nGROUP BY\r\n\tt,\r\n\ta\r\nORDER BY\r\n\tt\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16799/comments",
    "author": "kim-up",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-11-09T09:18:05Z",
        "body": "Your query is incorrect (you trying to do group by on the column which is an aggregation function). \r\n\r\nBut the error reported is totally misleading and should be improved.\r\n\r\nMinimal testcase:\r\n```\r\nselect number a, avg(number) b from numbers(1) group by a,b;\r\n```\r\n\r\n```\r\n<=20.3:\r\n\r\nReceived exception from server (version 20.3.21):\r\nCode: 47. DB::Exception: Received from localhost:9000. DB::Exception: Unknown identifier (in GROUP BY): avg(number). Stack trace:\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xe193120 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x857071d in /usr/bin/clickhouse\r\n2. ? @ 0xc822394 in /usr/bin/clickhouse\r\n3. DB::ExpressionAnalyzer::ExpressionAnalyzer(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::SyntaxAnalyzerResult const> const&, DB::Context const&, unsigned long, bool, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::SubqueryForSet, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, DB::SubqueryForSet> > >) @ 0xc81e39a in /usr/bin/clickhouse\r\n4. ? @ 0xc658bac in /usr/bin/clickhouse\r\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xc659e09 in /usr/bin/clickhouse\r\n6. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xc65b419 in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xc865e86 in /usr/bin/clickhouse\r\n8. DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::QueryProcessingStage::Enum) @ 0xc5d09c4 in /usr/bin/clickhouse\r\n9. ? @ 0xca74785 in /usr/bin/clickhouse\r\n10. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, bool) @ 0xca77571 in /usr/bin/clickhouse\r\n11. DB::TCPHandler::runImpl() @ 0x86524e9 in /usr/bin/clickhouse\r\n12. DB::TCPHandler::run() @ 0x86534d0 in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerConnection::start() @ 0xd8cebbb in /usr/bin/clickhouse\r\n14. Poco::Net::TCPServerDispatcher::run() @ 0xd8cf03d in /usr/bin/clickhouse\r\n15. Poco::PooledThread::run() @ 0xe2212d7 in /usr/bin/clickhouse\r\n16. Poco::ThreadImpl::runnableEntry(void*) @ 0xe21d0cc in /usr/bin/clickhouse\r\n17. ? @ 0xe21ea6d in /usr/bin/clickhouse\r\n18. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n19. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n\r\n>= 20.4:\r\n\r\nReceived exception from server (version 20.4.9):\r\nCode: 46. DB::Exception: Received from localhost:9000. DB::Exception: Unknown function avg. Stack trace:\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x110e3bb0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x998493d in /usr/bin/clickhouse\r\n2. ? @ 0xd8d5fd6 in /usr/bin/clickhouse\r\n3. DB::FunctionFactory::get(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context const&) const @ 0xd8d5135 in /usr/bin/clickhouse\r\n4. ? @ 0xe198839 in /usr/bin/clickhouse\r\n5. DB::SyntaxAnalyzer::analyzeSelect(std::__1::shared_ptr<DB::IAST>&, DB::SyntaxAnalyzerResult&&, DB::SelectQueryOptions const&, std::__1::vector<DB::TableWithColumnNamesAndTypes, std::__1::allocator<DB::TableWithColumnNamesAndTypes> > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::TableJoin>) const @ 0xe19c84c in /usr/bin/clickhouse\r\n6. ? @ 0xdeeef69 in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xdef24df in /usr/bin/clickhouse\r\n8. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xdef3bd9 in /usr/bin/clickhouse\r\n9. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xe0c8d41 in /usr/bin/clickhouse\r\n10. DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::QueryProcessingStage::Enum) @ 0xde63bbf in /usr/bin/clickhouse\r\n11. ? @ 0xe2134bd in /usr/bin/clickhouse\r\n12. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, bool) @ 0xe216e85 in /usr/bin/clickhouse\r\n13. DB::TCPHandler::runImpl() @ 0x9a8ad68 in /usr/bin/clickhouse\r\n14. DB::TCPHandler::run() @ 0x9a8bd70 in /usr/bin/clickhouse\r\n15. Poco::Net::TCPServerConnection::start() @ 0x10fcf92b in /usr/bin/clickhouse\r\n16. Poco::Net::TCPServerDispatcher::run() @ 0x10fcfdbb in /usr/bin/clickhouse\r\n17. Poco::PooledThread::run() @ 0x1117d986 in /usr/bin/clickhouse\r\n18. Poco::ThreadImpl::runnableEntry(void*) @ 0x11178c40 in /usr/bin/clickhouse\r\n19. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n20. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n```"
      },
      {
        "user": "kim-up",
        "created_at": "2020-11-09T10:35:02Z",
        "body": "thx\uff0cGOOD!"
      }
    ]
  },
  {
    "number": 16695,
    "title": "how to change default CSV FILE format_csv_delimiter?",
    "created_at": "2020-11-05T07:51:09Z",
    "closed_at": "2020-11-10T11:12:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16695",
    "body": "Hello,\r\n\r\n\r\nmy setting:\r\n```\r\n# cat /etc/clickhouse-server/config.d/delimiter.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n\t<profiles>\r\n        <default>\r\n\t\t<format_csv_delimiter>|</format_csv_delimiter>\r\n        </default>\r\n    </profiles>\r\n</yandex>\r\n```\r\n\r\nbut throw exception \r\n```\r\nCode: 27. DB::Exception: Cannot parse input: expected ',' before: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: user_ip,         type: String, parsed text: \"\"\r\nERROR: Line feed found where delimiter (,) is expected. It's like your file has less columns than expected.\r\n```\r\n\r\n    please, how to change default CSV FILE format_csv_delimiter?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16695/comments",
    "author": "trollhe",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-05T14:14:00Z",
        "body": ">cat /etc/clickhouse-server/config.d/delimiter.xml\r\n\r\nWrong folder for user settings.\r\nShould be /etc/clickhouse-server/**users.d**/delimiter.xml\r\n\r\n\r\nconfig.d -- server settings (config.xml)\r\nusers.d -- users settings (user.xml)\r\nconf.d -- any (config.xml and user.xml)"
      },
      {
        "user": "trollhe",
        "created_at": "2020-11-06T02:40:08Z",
        "body": "> > cat /etc/clickhouse-server/config.d/delimiter.xml\r\n> \r\n> Wrong folder for user settings.\r\n> Should be /etc/clickhouse-server/**users.d**/delimiter.xml\r\n> \r\n> config.d -- server settings (config.xml)\r\n> users.d -- users settings (user.xml)\r\n> conf.d -- any (config.xml and user.xml)\r\n\r\nthanks den-crane,\r\n\r\ni'm try to it, file already move from  `/etc/clickhouse-server/config.d/` to `/etc/clickhouse-server/users.d/`. and  restarted to clickhouse-server service.\r\n\r\nbut exception  still.\r\n\r\ntips: i used table function file() to select /data/clickhouse/user_files/*.csv. csv delimiter is  \"|\".\r\n\r\n"
      },
      {
        "user": "trollhe",
        "created_at": "2020-11-10T11:12:07Z",
        "body": "thanks , Successfully processed. config file move to `/etc/clickhouse-server/users.d/ `"
      }
    ]
  },
  {
    "number": 16690,
    "title": "How can i obtain table comment and column comment",
    "created_at": "2020-11-05T03:40:00Z",
    "closed_at": "2020-11-05T14:28:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16690",
    "body": "hi everyone, I want to ask how can I get the table comment and column comment when I writing sql query. \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16690/comments",
    "author": "wmaa0002",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-05T04:04:39Z",
        "body": "No table comments, only columns.\r\n\r\n```sql\r\ncreate table x( a Int64 comment 'some comment') Engine=Memory;\r\n\r\n\r\ndesc table x\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 a    \u2502 Int64 \u2502              \u2502                    \u2502 some comment \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nselect name, type, comment from system.columns where table = 'x' and database = 'default'\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500comment\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a    \u2502 Int64 \u2502 some comment \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      },
      {
        "user": "wmaa0002",
        "created_at": "2020-11-05T06:07:19Z",
        "body": "thank u so much for ur answer~"
      }
    ]
  },
  {
    "number": 16537,
    "title": "Process killed by  Memory limit (total) exceeded",
    "created_at": "2020-10-30T02:45:13Z",
    "closed_at": "2020-11-03T16:22:32Z",
    "labels": [
      "question",
      "obsolete-version",
      "memory"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16537",
    "body": "i start my process 2 hours,only commit not query\u3002\r\n\r\n<max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>\r\nserver RAM :32g\r\n\r\nthe error is :\r\n2020.10.29 23:45:16.320714 [ 10473 ] {171eac9e-a25d-45b0-bd59-7ee8f6533105} <Error> executeQuery: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 24.72 GiB (attempt to allocate chunk of 5075574 bytes), maximum: 24.71 GiB (version 20.7.1.1) (from 127.0.0.1:56612) (in query: INSERT INTO \r\n\r\n0. std::exception::capture() @ 0x66c630e in /usr/bin/clickhouse\r\n1. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8bff4d4 in /usr/bin/clickhouse\r\n2. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x54039a1 in /usr/bin/clickhouse\r\n3. MemoryTracker::alloc(long) @ 0x53ec389 in /usr/bin/clickhouse\r\n4. MemoryTracker::alloc(long) @ 0x53ec3c5 in /usr/bin/clickhouse\r\n5. MemoryTracker::alloc(long) @ 0x53ec3c5 in /usr/bin/clickhouse\r\n6. MemoryTracker::alloc(long) @ 0x53ec3c5 in /usr/bin/clickhouse\r\n7. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x5413acc in /usr/bin/clickhouse\r\n8. void DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 15ul, 16ul>::resize<>(unsigned long) @ 0x549cde5 in /usr/bin/clickhouse\r\n9. ? @ 0x763ac77 in /usr/bin/clickhouse\r\n10. DB::DataTypeString::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0x763b3fc in /usr/bin/clickhouse\r\n11. DB::NativeBlockInputStream::readData(DB::IDataType const&, DB::IColumn&, DB::ReadBuffer&, unsigned long, double) @ 0x7b080a4 in /usr/bin/clickhouse\r\n12. DB::NativeBlockInputStream::readImpl() @ 0x7b087da in /usr/bin/clickhouse\r\n13. DB::IBlockInputStream::read() @ 0x75e1883 in /usr/bin/clickhouse\r\n14. DB::TCPHandler::receiveData(bool) @ 0x7a1d63b in /usr/bin/clickhouse\r\n15. DB::TCPHandler::receivePacket() @ 0x7a1dbf1 in /usr/bin/clickhouse\r\n16. DB::TCPHandler::readDataNext(unsigned long const&, int const&) @ 0x7a1e0b3 in /usr/bin/clickhouse\r\n17. DB::TCPHandler::readData(DB::Settings const&) @ 0x7a1e388 in /usr/bin/clickhouse\r\n18. DB::TCPHandler::processInsertQuery(DB::Settings const&) @ 0x7a1e586 in /usr/bin/clickhouse\r\n19. DB::TCPHandler::runImpl() @ 0x7a1f371 in /usr/bin/clickhouse\r\n20. DB::TCPHandler::run() @ 0x7a203b1 in /usr/bin/clickhouse\r\n21. Poco::Net::TCPServerConnection::start() @ 0x7e331bb in /usr/bin/clickhouse\r\n22. Poco::Net::TCPServerDispatcher::run() @ 0x7e335b1 in /usr/bin/clickhouse\r\n23. Poco::PooledThread::run() @ 0x8c2dc4a in /usr/bin/clickhouse\r\n24. Poco::ThreadImpl::runnableEntry(void*) @ 0x8c2ced4 in /usr/bin/clickhouse\r\n25. start_thread @ 0x7ea5 in /lib64/libpthread-2.17.so\r\n26. clone @ 0xfe8dd in /lib64/libc-2.17.so\r\n\r\n\r\n\r\nrun   select metric,':', formatReadableSize(value) from system.metrics where metric like 'MemoryTracking' the result is:\r\n\r\n\u250c\u2500metric\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500':'\u2500\u252c\u2500formatReadableSize(value)\u2500\u2510\r\n\u2502 MemoryTracking \u2502 :   \u2502 24.71 GiB                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n\r\nbut the real RAM usage is less than 100M, What caused the problem. thanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16537/comments",
    "author": "xiashaohua",
    "comments": [
      {
        "user": "azat",
        "created_at": "2020-10-30T08:32:23Z",
        "body": "> i start my process 2 hours,only commit not query\u3002\r\n\r\nNot sure what \"only commit\" means, but:\r\n\r\n>2020.10.29 23:45:16.320714 [ 10473 ] {171eac9e-a25d-45b0-bd59-7ee8f6533105} executeQuery: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 24.72 GiB (attempt to allocate chunk of 5075574 bytes), maximum: 24.71 GiB (version 20.7.1.1) (from 127.0.0.1:56612) (in query: INSERT INTO\r\n\r\nThere were some significant improvements in the memory tracking, the most relevant for this issue is - #16121\r\n\r\nP.S. label memory is required"
      },
      {
        "user": "xiashaohua",
        "created_at": "2020-10-30T08:56:58Z",
        "body": "> > i start my process 2 hours,only commit not query\u3002\r\n> \r\n> Not sure what \"only commit\" means, but:\r\n> \r\n> > 2020.10.29 23:45:16.320714 [ 10473 ] {171eac9e-a25d-45b0-bd59-7ee8f6533105} executeQuery: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 24.72 GiB (attempt to allocate chunk of 5075574 bytes), maximum: 24.71 GiB (version 20.7.1.1) (from 127.0.0.1:56612) (in query: INSERT INTO\r\n> \r\n> There were some significant improvements in the memory tracking, the most relevant for this issue is - #16121\r\n> \r\n> P.S. label memory is required\r\n\r\nInsert only no select\u3002and the phenomenon is not stable, sometimes appear in a few hours, sometimes not appear in a few days"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-30T12:57:48Z",
        "body": "It seems this issue will be fixed in the next stable release 20.11"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-30T13:02:01Z",
        "body": "20.7.1.1 -- is obsolete and not stable. You need to upgrade at least to 20.7.4.11. Or better to 20.8.5.45."
      },
      {
        "user": "xiashaohua",
        "created_at": "2020-10-31T08:56:44Z",
        "body": "> 20.7.1.1 -- is obsolete and not stable. You need to upgrade at least to 20.7.4.11. Or better to 20.8.5.45.\r\n\r\nthanks,i will  update to a stable version"
      }
    ]
  },
  {
    "number": 16421,
    "title": "cannot start server 20.10.2.20,reports <jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.",
    "created_at": "2020-10-27T08:42:24Z",
    "closed_at": "2020-11-25T10:14:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16421",
    "body": "the old version  19.17.5.18\r\nservice clickhouse-server stop does not work, so I  stop the old server by kill, then run rpm -Uvh to update the software, then service clickhouse-server start failed\r\n\r\n```\r\n[root@localhost ~]# service clickhouse-server stop\r\n[root@localhost ~]# clickhouse-client -m\r\nClickHouse client version 19.17.5.18 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 19.17.5 revision 54428.\r\n\r\nlocalhost :) \\q;\r\nBye.\r\n[root@localhost ~]# ps -ef|grep click\r\nroot     32900 32878  0 Sep18 ?        02:51:20 clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\nroot     50565 50287  0 15:57 pts/1    00:00:00 grep click\r\n[root@localhost ~]# kill 32900\r\n[root@localhost ~]# ps -ef|grep click\r\nroot     50651 50287  0 16:00 pts/1    00:00:00 grep click\r\n\r\n[root@localhost tmp]# rpm -q click*\r\npackage clickhouse-client-20.10.2.20-2.noarch.rpm is not installed\r\npackage clickhouse-common-static-20.10.2.20-2.x86_64.rpm is not installed\r\npackage clickhouse-server-20.10.2.20-2.noarch.rpm is not installed\r\n[root@localhost tmp]# rpm -Uvh click*\r\nwarning: clickhouse-client-20.10.2.20-2.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID e0c56bd4: NOKEY\r\nPreparing...                ########################################### [100%]\r\n   1:clickhouse-common-stati########################################### [ 33%]\r\n   2:clickhouse-client      ########################################### [ 67%]\r\n   3:clickhouse-server      ########################################### [100%]\r\n\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nPath to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/\r\n\r\n[root@localhost tmp]# service clickhouse-server start\r\nStart clickhouse-server service: <jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nPath to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/\r\nChanging owner of [/var/lib/clickhouse/] to [clickhouse:clickhouse]\r\nChanging owner of [/var/log/clickhouse-server/*] to [clickhouse:clickhouse]\r\nChanging owner of [/var/log/clickhouse-server] to [root:clickhouse]\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nUNKNOWN\r\n[root@localhost tmp]# clickhouse-client -m\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nClickHouse client version 20.10.2.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 210. DB::NetException: Connection refused (localhost:9000)\r\n\r\n[root@localhost tmp]# cat /proc/version\r\nLinux version 2.6.32-573.el6.x86_64 (mockbuild@kojibuilder-ve) (gcc version 4.4.7 20120313 (GCC) ) #1 SMP Wed Feb 24 13:34:24 CST 2016\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16421/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-10-27T14:00:42Z",
        "body": "1) please check /var/log/clickhouse-server/clickhouse-server.err.log and /var/log/clickhouse-server/clickhouse-server.log\r\n2) what is your OS? "
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:10:09Z",
        "body": "clickhouse-server.err.log\r\n```\r\n[root@localhost ~]# tail  /var/log/clickhouse-server/clickhouse-server.err.log\r\n24. 0x7fb90d9a3075 DB::NullAndDoCopyBlockInputStream::readImpl() /usr/bin/clickhouse\r\n25. 0x7fb90d8555ca DB::IBlockInputStream::read() /usr/bin/clickhouse\r\n26. 0x7fb90d84e69b DB::AsynchronousBlockInputStream::calculate() /usr/bin/clickhouse\r\n27. 0x7fb90d84ea60 ? /usr/bin/clickhouse\r\n28. 0x7fb90a64917e ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::_List_iterator<ThreadFromGlobalPool>) /usr/bin/clickhouse\r\n29. 0x7fb90a64978e ThreadFromGlobalPool::ThreadFromGlobalPool<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}&&)::{lambda()#1}::operator()() const /usr/bin/clickhouse\r\n30. 0x7fb90a646c4c ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse\r\n31. 0x7fb9103d0ec0 ? /usr/bin/clickhouse\r\n\r\n2020.10.27 15:54:07.005337 [ 1 ] {} <Error> Application: DB::Exception: Cannot lock file /var/lib/clickhouse/status. Another server instance in same directory is already running.\r\n\r\n```\r\nclickhouse-server.log\r\n```\r\n[root@localhost ~]# tail -100 /var/log/clickhouse-server/clickhouse-server.log\r\n2020.10.27 15:54:07.005337 [ 1 ] {} <Error> Application: DB::Exception: Cannot lock file /var/lib/clickhouse/status. Another server instance in same directory is already running.\r\n2020.10.27 15:54:07.005839 [ 1 ] {} <Information> Application: shutting down\r\n2020.10.27 15:54:07.005863 [ 1 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2020.10.27 15:54:07.007098 [ 2 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n2020.10.27 15:57:10.521311 [ 87 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::1]:40251\r\n2020.10.27 15:57:10.521527 [ 87 ] {} <Debug> TCPHandler: Connected ClickHouse client version 19.17.0, revision: 54428, user: default.\r\n2020.10.27 15:57:10.530121 [ 88 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::1]:40252\r\n2020.10.27 15:57:10.530263 [ 88 ] {} <Debug> TCPHandler: Connected ClickHouse client version 19.17.0, revision: 54428, user: default.\r\n2020.10.27 15:57:10.532893 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> executeQuery: (from [::1]:40252) SELECT DISTINCT arrayJoin(extractAll(name, '[\\\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.settings UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)\r\n2020.10.27 15:57:10.536672 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> Join: setSampleBlock: comb.name String String(size = 0)\r\n2020.10.27 15:57:10.539658 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> Join: setSampleBlock: comb.name String String(size = 0)\r\n2020.10.27 15:57:10.542160 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> Join: setSampleBlock: comb.name String String(size = 0)\r\n2020.10.27 15:57:10.543592 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543653 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543702 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543735 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543816 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544097 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544542 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544685 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544781 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544985 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.545070 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.545989 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Distinct\r\n  Union\r\n   Distinct \u00d7 6\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Expression\r\n        Expression\r\n         One\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Expression\r\n        CreatingSets\r\n         Lazy\r\n         Expression\r\n          Filter\r\n           Expression\r\n            One\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Limit\r\n        Expression\r\n         Expression\r\n          One\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Limit\r\n        Expression\r\n         Distinct\r\n          Expression\r\n           Tables\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Limit\r\n        Expression\r\n         Distinct\r\n          Expression\r\n           Columns\r\n\r\n2020.10.27 15:57:10.549108 [ 98 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> CreatingSetsBlockInputStream: Creating join. \r\n2020.10.27 15:57:10.550504 [ 98 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.551153 [ 98 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> CreatingSetsBlockInputStream: Created. Join with 9 entries from 9 rows. In 0.001 sec.\r\n2020.10.27 15:57:10.555171 [ 21 ] {} <Trace> SystemLog (system.trace_log): Flushing system log\r\n2020.10.27 15:57:10.555870 [ 21 ] {} <Debug> DiskSpaceMonitor: Reserving 1.00 MiB on disk `default`, having unreserved 23.59 GiB.\r\n2020.10.27 15:57:10.556061 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n2020.10.27 15:57:10.556087 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waited for threads to finish\r\n2020.10.27 15:57:10.556187 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Information> executeQuery: Read 2767 rows, 111.92 KiB in 0.023 sec., 119154 rows/sec., 4.71 MiB/sec.\r\n2020.10.27 15:57:10.556206 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> MemoryTracker: Peak memory usage (for query): 1.04 MiB.\r\n2020.10.27 15:57:10.556268 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n2020.10.27 15:57:10.556282 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waited for threads to finish\r\n2020.10.27 15:57:10.564214 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> MemoryTracker: Peak memory usage (total): 1.04 MiB.\r\n2020.10.27 15:57:10.564243 [ 21 ] {} <Trace> system.trace_log: Renaming temporary part tmp_insert_202010_63_63_0 to 202010_200_200_0.\r\n2020.10.27 15:57:10.564256 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Information> TCPHandler: Processed in 0.033 sec.\r\n2020.10.27 15:57:10.564290 [ 88 ] {} <Information> TCPHandler: Done processing connection.\r\n2020.10.27 15:57:19.166554 [ 87 ] {} <Information> TCPHandler: Done processing connection.\r\n2020.10.27 16:00:22.919041 [ 112 ] {} <Information> Application: Received termination signal (Terminated)\r\n2020.10.27 16:00:22.919329 [ 1 ] {} <Debug> Application: Received termination signal.\r\n2020.10.27 16:00:22.919434 [ 1 ] {} <Debug> Application: Waiting for current connections to close.\r\n2020.10.27 16:00:23.831890 [ 1 ] {} <Information> Application: Closed all listening sockets.\r\n2020.10.27 16:00:23.831933 [ 1 ] {} <Information> Application: Closed connections.\r\n2020.10.27 16:00:23.837146 [ 1 ] {} <Information> Application: Shutting down storages.\r\n2020.10.27 16:00:23.939019 [ 1 ] {} <Trace> BackgroundSchedulePool: Waiting for threads to finish.\r\n2020.10.27 16:00:23.939511 [ 1 ] {} <Debug> Application: Shutted down storages.\r\n2020.10.27 16:00:23.942859 [ 1 ] {} <Debug> Application: Destroyed global context.\r\n2020.10.27 16:00:23.943143 [ 1 ] {} <Information> Application: shutting down\r\n2020.10.27 16:00:23.943155 [ 1 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2020.10.27 16:00:23.943207 [ 112 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:13:02Z",
        "body": "same error if i run ```clickhouse-server --config-file=/etc/clickhouse-server/config.xml```\r\n```\r\n[root@localhost ~]# clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nLogging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nLogging trace to console\r\n2020.10.28 07:44:47.987507 [ 25804 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\n2020.10.28 07:44:47.992744 [ 25804 ] {} <Information> Pipe: Cannot get pipe capacity, errno: 22, strerror: Invalid argument. Very old Linux kernels have no support for this fcntl.\r\n2020.10.28 07:44:48.043623 [ 25804 ] {} <Information> : Starting ClickHouse 20.10.2.20 with revision 54441, no build id, PID 25804\r\n2020.10.28 07:44:48.043772 [ 25804 ] {} <Information> Application: starting up\r\n2020.10.28 07:44:48.052278 [ 25804 ] {} <Trace> Application: Will do mlock to prevent executable memory from being paged out. It may take a few seconds.\r\n2020.10.28 07:44:48.084764 [ 25804 ] {} <Trace> Application: The memory map of clickhouse executable has been mlock'ed, total 164.02 MiB\r\n2020.10.28 07:44:48.085210 [ 25804 ] {} <Error> Application: DB::Exception: Effective user of the process (root) does not match the owner of the data (clickhouse). Run under 'sudo -u clickhouse'.\r\n2020.10.28 07:44:48.085253 [ 25804 ] {} <Information> Application: shutting down\r\n2020.10.28 07:44:48.085263 [ 25804 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2020.10.28 07:44:48.087090 [ 25807 ] {} <Trace> BaseDaemon: Received signal -2\r\n2020.10.28 07:44:48.087170 [ 25807 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:18:25Z",
        "body": "run ```sudo -u clickhouse``` seems work now\r\n\r\n```\r\n[root@localhost ~]# sudo -u clickhouse  clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nLogging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nLogging trace to console\r\n2020.10.28 07:47:14.897570 [ 25888 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\n2020.10.28 07:47:14.902881 [ 25888 ] {} <Information> Pipe: Cannot get pipe capacity, errno: 22, strerror: Invalid argument. Very old Linux kernels have no support for this fcntl.\r\n2020.10.28 07:47:14.954047 [ 25888 ] {} <Information> : Starting ClickHouse 20.10.2.20 with revision 54441, no build id, PID 25888\r\n2020.10.28 07:47:14.954201 [ 25888 ] {} <Information> Application: starting up\r\n2020.10.28 07:47:14.961761 [ 25888 ] {} <Trace> Application: Will do mlock to prevent executable memory from being paged out. It may take a few seconds.\r\n2020.10.28 07:47:14.993816 [ 25888 ] {} <Trace> Application: The memory map of clickhouse executable has been mlock'ed, total 164.02 MiB\r\n2020.10.28 07:47:14.993954 [ 25888 ] {} <Debug> Application: rlimit on number of file descriptors is 262144\r\n2020.10.28 07:47:14.993971 [ 25888 ] {} <Debug> Application: Initializing DateLUT.\r\n2020.10.28 07:47:14.993983 [ 25888 ] {} <Trace> Application: Initialized DateLUT with time zone 'Asia/Shanghai'.\r\n2020.10.28 07:47:14.994009 [ 25888 ] {} <Debug> Application: Setting up /var/lib/clickhouse/tmp/ to store temporary data in it\r\n2020.10.28 07:47:14.995115 [ 25888 ] {} <Debug> Application: Configuration parameter 'interserver_http_host' doesn't exist or exists and empty. Will use 'localhost' as replica host.\r\n2020.10.28 07:47:14.996173 [ 25888 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/users.xml'\r\nProcessing configuration file '/etc/clickhouse-server/users.xml'.\r\nInclude not found: networks\r\nSaved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'.\r\n2020.10.28 07:47:14.996946 [ 25888 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performing update on configuration\r\n2020.10.28 07:47:14.997673 [ 25888 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performed update on configuration\r\n2020.10.28 07:47:14.998025 [ 25888 ] {} <Warning> Access(local directory): File /var/lib/clickhouse/access/users.list doesn't exist\r\n2020.10.28 07:47:14.998053 [ 25888 ] {} <Warning> Access(local directory): Recovering lists in directory /var/lib/clickhouse/access/\r\n2020.10.28 07:47:14.998340 [ 25888 ] {} <Information> Application: Setting max_server_memory_usage was set to 56.62 GiB (62.91 GiB available * 0.90 max_server_memory_usage_to_ram_ratio)\r\n2020.10.28 07:47:14.998379 [ 25888 ] {} <Information> Application: Loading metadata from /var/lib/clickhouse/\r\n2020.10.28 07:47:15.005439 [ 25888 ] {} <Information> DatabaseOrdinary (system): Total 1 tables and 0 dictionaries.\r\n2020.10.28 07:47:15.012366 [ 25893 ] {} <Information> BackgroundProcessingPool: Create BackgroundProcessingPool with 16 threads\r\n2020.10.28 07:47:15.013364 [ 25893 ] {} <Debug> system.trace_log: Loading data parts\r\n2020.10.28 07:47:15.059491 [ 25893 ] {} <Debug> system.trace_log: Loaded data parts (5 items)\r\n2020.10.28 07:47:15.059872 [ 25888 ] {} <Information> DatabaseOrdinary (system): Starting up tables.\r\n2020.10.28 07:47:15.063505 [ 25888 ] {} <Information> DatabaseOrdinary (datasets): Total 1 tables and 0 dictionaries.\r\n2020.10.28 07:47:15.064279 [ 25893 ] {} <Debug> datasets.ontime: Loading data parts\r\n2020.10.28 07:47:15.800211 [ 25893 ] {} <Debug> datasets.ontime: Loaded data parts (373 items)\r\n2020.10.28 07:47:15.802813 [ 25888 ] {} <Information> DatabaseOrdinary (datasets): Starting up tables.\r\n2020.10.28 07:47:15.828392 [ 25888 ] {} <Information> DatabaseOrdinary (default): Total 8 tables and 0 dictionaries.\r\n```\r\nand open another terminal\r\n```\r\n[root@localhost ~]# clickhouse-client -m\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nClickHouse client version 20.10.2.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.10.2 revision 54441.\r\n\r\nlocalhost :) show databases;\r\n\r\nSHOW DATABASES\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 _temporary_and_external_tables \u2502\r\n\u2502 datasets                       \u2502\r\n\u2502 default                        \u2502\r\n\u2502 system                         \u2502\r\n\u2502 tpch                           \u2502\r\n\u2502 tutorial                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n6 rows in set. Elapsed: 0.003 sec. \r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:24:12Z",
        "body": "but service clickhouse-server start  or sudo service clickhouse-server start still fail\r\n```\r\n[root@localhost ~]# sudo -u clickhouse service clickhouse-server start\r\n/etc/init.d/clickhouse-server: line 387: /var/lock/clickhouse-server: Permission denied\r\n[root@localhost ~]# sudo service clickhouse-server start\r\nStart clickhouse-server service: <jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nPath to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/\r\nChanging owner of [/var/log/clickhouse-server/*] to [clickhouse:clickhouse]\r\nChanging owner of [/var/log/clickhouse-server] to [root:clickhouse]\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\n\r\nUNKNOWN\r\n[root@localhost ~]# \r\n[root@localhost ~]# clickhouse-client -m\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nClickHouse client version 20.10.2.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 210. DB::NetException: Connection refused (localhost:9000)\r\n\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-28T14:41:50Z",
        "body": "the real issue is \r\n```\r\n/etc/init.d/clickhouse-server: line 387: /var/lock/clickhouse-server: Permission denied\r\n```\r\n\r\nthe issue is not related to `<jemalloc>: perCPU `\r\n\r\n"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T23:55:03Z",
        "body": "how to solve the problem?\r\n```\r\n$ cat /etc/init.d/clickhouse-server |sed -n '381,390p'\r\n(\r\n    if $FLOCK -n 9; then\r\n        main \"$@\"\r\n    else\r\n        echo \"Init script is already running\" && exit 1\r\n    fi\r\n) 9> $LOCKFILE\r\n```"
      },
      {
        "user": "filimonov",
        "created_at": "2020-11-25T10:14:14Z",
        "body": "> same error if i run clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n\r\nIt's completely different... \r\n```\r\n2020.10.28 07:44:48.085210 [ 25804 ] {} <Error> Application: DB::Exception: Effective user of the process (root) does not match the owner of the data (clickhouse). Run under 'sudo -u clickhouse'.\r\n```\r\n\r\n> sudo -u clickhouse service clickhouse-server start\r\n\r\nIf you run clickhouse as  a service, you should run it as root. i.e.:\r\n```\r\n> sudo service clickhouse-server start\r\n```\r\n\r\nThe process will be executed from clickhouse user anyway. \r\n\r\nAlso please consider updating your OS/ linux. \r\n\r\n`perCPU arena getcpu() not available` actually means your linux kernel is quite old. "
      }
    ]
  },
  {
    "number": 16254,
    "title": "Why CH always restart automatically after killing it?",
    "created_at": "2020-10-22T07:14:27Z",
    "closed_at": "2020-10-22T10:55:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16254",
    "body": "I found that CH server can always restart itself in about half a minute after stopping it, or killing it.\r\nIs there any backgound process to keep the CH server alive?\r\nI tried to remove the file from /etc/init.d/clickhouse-server, but the problem occurs.\r\nActually, It's not a problem, just wondering why...\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16254/comments",
    "author": "y45398jp123",
    "comments": [
      {
        "user": "MeteHanC",
        "created_at": "2020-10-22T07:53:46Z",
        "body": "I think this is about the unit file of ClickHouse (for ubuntu, filepath is : /etc/systemd/system/clickhouse-server.service)\r\n\r\nUnder the Service section you can see the following lines ;\r\n\r\n```\r\nRestart=always\r\nRestartSec=30\r\n```\r\n\r\nSo this is not actually a ClickHouse specific thing "
      },
      {
        "user": "Inasayang",
        "created_at": "2020-10-22T08:45:28Z",
        "body": "systemd"
      },
      {
        "user": "y45398jp123",
        "created_at": "2020-10-22T10:55:42Z",
        "body": "Got it, thanks a lot."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T00:19:17Z",
        "body": "for initV systems it's a cron-job.\r\n\r\n```\r\n# cat /etc/cron.d/clickhouse-server\r\n*/10 * * * * root (which service > /dev/null 2>&1 && (service clickhouse-server condstart ||:)) || /etc/init.d/clickhouse-server condstart > /dev/null 2>&1\r\n```"
      }
    ]
  },
  {
    "number": 16251,
    "title": "ALTER DROP doesn't consider size of a partition correctly",
    "created_at": "2020-10-22T01:27:13Z",
    "closed_at": "2020-10-23T16:37:03Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16251",
    "body": "Hi. When I try to drop partition for a certain hour:\r\n```alter table db.table drop partition '2020-10-19 18:00:00';```\r\nI sometimes get this error:\r\n```\r\n[2020-10-22 07:21:16] Code: 359, e.displayText() = DB::Exception: Table or Partition in db.table was not dropped.\r\n[2020-10-22 07:21:16] Reason:\r\n[2020-10-22 07:21:16] 1. Size (52.01 GB) is greater than max_[table/partition]_size_to_drop (50.00 GB)\r\n[2020-10-22 07:21:16] 2. File '/var/lib/clickhouse/flags/force_drop_table' intended to force DROP doesn't exist\r\n```\r\nHowever, if I run this:\r\n```select formatReadableSize(sum(bytes_on_disk)) from (select bytes_on_disk from system.parts where table = 'table' and partition = '2020-10-19 18:00:00');```\r\nI see that the size of this partition is much lower: 48.43 GiB. So why does it tell me that I'm dropping too big partition and why does CH allow such big partitions at all if it prohibits dropping them in the end?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16251/comments",
    "author": "keliss",
    "comments": [
      {
        "user": "keliss",
        "created_at": "2020-10-22T01:41:17Z",
        "body": "Also, the message telling me to \"increase (or set to zero) max_[table/partition]_size_to_drop in server config and restart ClickHouse\" seems to be misleading - I don't have to restart CH for these settings to apply. Or it's just some CH magic that made the partitions smaller so I was able to drop them normally (I've already seen such behaviour but I thought it is impossible for a partition to become smaller without dropping any of its parts)."
      },
      {
        "user": "abyss7",
        "created_at": "2020-10-22T18:10:38Z",
        "body": "Can you provide please the `SHOW CREATE TABLE` result for table in question? And please provide the CH version."
      },
      {
        "user": "keliss",
        "created_at": "2020-10-22T18:17:05Z",
        "body": "Of course:\r\n```\r\nCREATE TABLE db.table (`writeTime` DateTime DEFAULT now(), ...) ENGINE = MergeTree() PARTITION BY toStartOfHour(writeTime) ORDER BY tuple() SETTINGS index_granularity = 8192;\r\n```\r\n20.9.3.45"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-22T18:19:41Z",
        "body": "> Also, the message telling me to \"increase (or set to zero) max_[table/partition]_size_to_drop in server config and restart ClickHouse\" seems to be misleading - I don't have to restart CH for these settings to apply. Or it's just some CH magic that made the partitions smaller so I was able to drop them normally (I've already seen such behaviour but I thought it is impossible for a partition to become smaller without dropping any of its parts).\r\n\r\nmax_partition_size_to_drop reload/apply without restart was implemented recently and this message should be corrected."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T13:01:53Z",
        "body": "As I understand you just finished to insert data to this partition '2020-10-19 18:00:00'.\r\nIt is possible that when you checked the size by select `inactive` parts were deleted already. "
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T13:27:56Z",
        "body": "No, this partition remained intact for sure, we don't insert data for some past period of time."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T14:12:50Z",
        "body": "Ah, it is two different units Gib vs GB 1024 vs 1000.\r\n\r\n```    \r\n<max_partition_size_to_drop>5000000000</max_partition_size_to_drop> \r\n\r\nSELECT\r\n    formatReadableSize(sum(bytes_on_disk)),\r\n    round(((sum(bytes_on_disk) / 1000) / 1000) / 1000, 2) AS GB\r\nFROM system.parts\r\nWHERE table = 'XX'\r\n\r\n\u250c\u2500formatReadableSize(sum(bytes_on_disk))\u2500\u252c\u2500\u2500\u2500\u2500GB\u2500\u2510\r\n\u2502 9.50 GiB                               \u2502 10.21 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nalter table XX drop partition tuple();\r\n\r\n1. Size (10.21 GB) is greater than max_[table/partition]_size_to_drop (5.00 GB)\r\n\r\n```"
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T14:37:31Z",
        "body": "But even in this case the exception is triggered by a lower amount of disk space than the limit :)\r\n48.43 GiB * 1024 / 1000 = 49.59 GB."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T14:42:11Z",
        "body": "No. The limit is also in GB(1000). Check my message \r\n\r\nmax_partition_size_to_drop = 5000000000\r\n\r\n1. Size (10.21 GB) is greater than max_[table/partition]_size_to_drop (5.00 GB)"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T14:46:14Z",
        "body": "and \r\n\r\n`48.43 * (1024 * 1024 * 1024) / (1000*1000*1000) = 52.00`\r\n\r\n>1. Size (52.01 GB) is greater than max_[table/partition]_size_to_drop (50.00 GB)"
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T15:03:25Z",
        "body": "Oh, sorry, I calculated incorrectly. Is there any particular reason to keep some limit for DROP queries at all? I can't imagine a use-case for this setting."
      },
      {
        "user": "abyss7",
        "created_at": "2020-10-23T15:40:05Z",
        "body": "> Oh, sorry, I calculated incorrectly. Is there any particular reason to keep some limit for DROP queries at all? I can't imagine a use-case for this setting.\r\n\r\nFrom documentation:\r\n> In many cases mistakes like these will affect all replicas. ClickHouse has built-in safeguards to prevent some types of mistakes \u2014 for example, by default you can\u2019t just drop tables with a MergeTree-like engine containing more than 50 Gb of data."
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T16:37:03Z",
        "body": "Ok, got it. Thanks for your help."
      }
    ]
  },
  {
    "number": 16220,
    "title": "Query from distributed tables with sharded data return separate result for each shard",
    "created_at": "2020-10-21T10:58:43Z",
    "closed_at": "2020-11-08T12:56:09Z",
    "labels": [
      "question",
      "st-need-info",
      "st-need-repro"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16220",
    "body": "**Describe the bug**\r\nSelect aggregates from disributed table produce unexpected results.\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\nTested on \r\n1) \r\n```\r\n+----------------+----------------------------------------+\r\n|name            |value                                   |\r\n+----------------+----------------------------------------+\r\n|VERSION_FULL    |ClickHouse 20.3.11.97                   |\r\n|VERSION_DESCRIBE|v20.3.11.97-stable                      |\r\n|VERSION_INTEGER |20003011                                |\r\n|VERSION_GITHASH |952efc395509c10081e8c1b836ebb5c2e0f898fa|\r\n|VERSION_REVISION|54433                                   |\r\n+----------------+----------------------------------------+\r\n\r\n```\r\n\r\n2)\r\n```\r\n+----------------+----------------------------------------+\r\n|name            |value                                   |\r\n+----------------+----------------------------------------+\r\n|VERSION_FULL    |ClickHouse 20.7.2.30                    |\r\n|VERSION_DESCRIBE|v20.7.2.30-stable                       |\r\n|VERSION_INTEGER |20007002                                |\r\n|VERSION_GITHASH |e0529c753f9dd4fc27e38f2f25a14d50beedda65|\r\n|VERSION_REVISION|54437                                   |\r\n+----------------+----------------------------------------+\r\n```\r\n* Non-default settings, if any\r\nprefer_localhost_replica 0\r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\ncreate table test\r\n(\r\n    date Date,\r\n    key  String\r\n) engine MergeTree() partition by date order by key;\r\ncreate table test_dist as test engine Distributed('cubes', default, test, date);\r\n```\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\nOn shard 1: \r\n```\r\ninsert into test values ('2020-01-01', '1');\r\ninsert into test values ('2020-01-01', '2');\r\n```\r\nOn shard 2\r\n```\r\ninsert into test values ('2020-01-01', '1', 1);\r\ninsert into test values ('2020-01-01', '2', 1);\r\ninsert into test values ('2020-01-03', '3', 1);\r\n```\r\n* Queries to run that lead to unexpected result\r\n```\r\nselect toMonday(date) mnt, uniq(key)\r\nfrom test_dist\r\nwhere mnt = '2019-12-30'\r\ngroup by mnt;\r\n```\r\n\r\nResult:\r\n```\r\n+----------+---------+\r\n|mnt       |uniq(key)|\r\n+----------+---------+\r\n|2019-12-30|2        |\r\n|2019-12-30|3        |\r\n+----------+---------+\r\n\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nExpected single row to return with 3 uniq(key)\r\n```\r\n+----------+---------+\r\n|mnt       |uniq(key)|\r\n+----------+---------+\r\n|2019-12-30|3        |\r\n+----------+---------+\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16220/comments",
    "author": "dmgburg",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-21T13:39:56Z",
        "body": "Can you  show cluster `cubes` description?\r\n\r\nAnd `select *, _shard_num from test_dist`"
      },
      {
        "user": "SaltTan",
        "created_at": "2020-10-21T22:03:42Z",
        "body": "Reminds me of #13331"
      },
      {
        "user": "dmgburg",
        "created_at": "2020-10-26T14:43:14Z",
        "body": "@den-crane \r\n Clusters layout\r\n```\r\n <remote_servers>\r\n        <cubes>\r\n            <shard>\r\n                <internal_replication>true</internal_replication>\r\n                <replica>\r\n                    <host>server</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n                <replica>\r\n                    <host>server2</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n        </cubes>\r\n    </remote_servers>\r\n```\r\n\r\nselect *, _shard_num from test_dist;\r\n\r\n```\r\n+----------+---+----------+\r\n|date      |key|_shard_num|\r\n+----------+---+----------+\r\n|2020-01-01|2  |1         |\r\n|2020-01-01|1  |1         |\r\n|2020-01-03|3  |1         |\r\n|2020-01-01|2  |1         |\r\n|2020-01-01|1  |1         |\r\n+----------+---+----------+\r\n\r\n```\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-26T15:00:28Z",
        "body": "@dmgburg you have confused SHARD vs REPLICA\r\n\r\nYou  config for 2 shards  must be \r\n```\r\nshard \r\n   replica\r\nshard\r\n   replica\r\n```\r\n\r\n```\r\n<remote_servers>\r\n       <cubes>\r\n           <shard>\r\n               <internal_replication>true</internal_replication>\r\n               <replica>\r\n                   <host>server</host>\r\n                   <port>9000</port>\r\n               </replica>\r\n           </shard>            \r\n           <shard>             \r\n               <replica>\r\n                   <host>server2</host>\r\n                   <port>9000</port>\r\n               </replica>\r\n           </shard>\r\n       </cubes>\r\n   </remote_servers>\r\n\r\n```\r\n\r\n---\r\n\r\nIf each of 2 shards have 3 replicas config have to be\r\n```\r\nshard \r\n   replica\r\n   replica\r\n   replica\r\nshard\r\n   replica\r\n   replica\r\n   replica\r\n```"
      },
      {
        "user": "dmgburg",
        "created_at": "2020-11-08T12:56:09Z",
        "body": "\u0414\u0430, \u0432\u0441\u0435 \u0442\u0430\u043a. \u0421 \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u043e. \u0421\u043f\u0430\u0441\u0438\u0431\u043e!"
      }
    ]
  },
  {
    "number": 15959,
    "title": "clickhouse integrations hdfs csv file",
    "created_at": "2020-10-14T10:09:11Z",
    "closed_at": "2020-10-16T06:09:33Z",
    "labels": [
      "question",
      "feature"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15959",
    "body": "Hello,How to create a CSV table split by '|'\uff1f\r\n\r\nhdfs file content\r\n```\r\nssss|aaaa|pppp|\r\n```\r\n\r\nclickhouse create table:\r\n```\r\ncreate table xxx.xxxx\r\n(\r\n...\r\n)\r\nENGINE = HDFS('hdfs://xxx:9000/ext/ntbcp/*', 'CSV');\r\n```\r\n\r\n**how to setting format_csv_delimiter '|' ?????**\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15959/comments",
    "author": "trollhe",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-14T13:07:18Z",
        "body": "try to set format_csv_delimiter in default profile.\r\n```\r\ncat /etc/clickhouse-server/conf.d/z_user_substitutes.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n    <profiles>\r\n        <default>\r\n\t\t<format_csv_delimiter>|</format_csv_delimiter>\r\n        </default>\r\n    </profiles>\r\n</yandex>\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-10-14T13:13:16Z",
        "body": "We should allow all format settings in File/URL/HDFS/S3 engines.\r\nAlso we should respect query-level settings when table function is queried."
      },
      {
        "user": "trollhe",
        "created_at": "2020-10-15T02:35:52Z",
        "body": "> try to set format_csv_delimiter in default profile.\r\n> \r\n> ```\r\n> cat /etc/clickhouse-server/conf.d/z_user_substitutes.xml\r\n> <?xml version=\"1.0\"?>\r\n> <yandex>\r\n>     <profiles>\r\n>         <default>\r\n> \t\t<format_csv_delimiter>|</format_csv_delimiter>\r\n>         </default>\r\n>     </profiles>\r\n> </yandex>\r\n> ```\r\n\r\nthanks den-crane, Can the field separator be set for a specific table?\r\n"
      },
      {
        "user": "trollhe",
        "created_at": "2020-10-15T02:37:11Z",
        "body": "> We should allow all format settings in File/URL/HDFS/S3 engines.\r\n> Also we should respect query-level settings when table function is queried.\r\n\r\nyes,it`s support hdfs engine. "
      }
    ]
  },
  {
    "number": 15953,
    "title": "why my sql produces  duplicate records?",
    "created_at": "2020-10-14T02:55:07Z",
    "closed_at": "2020-10-14T11:22:02Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15953",
    "body": "i wish to produce 50000000 different records with following commands, but failed\r\n```sql\r\ncreate table sfz(code Int64)ENGINE = MergeTree()order by code;\r\ninsert into sfz select (110000+n1.number)*1E12 + (20000101+n2.number)*1E4+n3.number from numbers(1000)n1,numbers(500)n2,numbers(100)n3;\r\n\r\nSELECT count(*)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500count()\u2500\u2510\r\n\u2502 50000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT countDistinct(code)\r\nFROM sfz\r\n\r\n\u250c\u2500uniqExact(code)\u2500\u2510\r\n\u2502         3500000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT \r\n    max(code), \r\n    min(code)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u2510\r\n\u2502 110999200006000096 \u2502 110000200001010000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15953/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2020-10-14T04:16:05Z",
        "body": "if i use String data type, the result is right\r\n```sql\r\ncreate table sfz2(code String)ENGINE = MergeTree()order by code;\r\n\r\ninsert into sfz2 select toString(110000+n1.number)|| toString(20000101+n2.number)||'0'||toString(n3.number) from numbers(1000)n1,numbers(500)n2,numbers(100)n3;\r\n\r\nSELECT countDistinct(code)\r\nFROM sfz2\r\n\r\n\u250c\u2500uniqExact(code)\u2500\u2510\r\n\u2502        50000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT \r\n    max(code), \r\n    min(code)\r\nFROM sfz2\r\n\r\n\u250c\u2500max(code)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500min(code)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 11099920000600099 \u2502 1100002000010100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-14T04:22:58Z",
        "body": "i realized the problem was caused by big number\r\n```sql\r\ntruncate table sfz;\r\ninsert into sfz select (110000+n1.number)*toInt64(1E12) + (20000101+n2.number)*toInt64(1E4)+n3.number from numbers(1000)n1,numbers(500)n2,numbers(100)n3;\r\nSELECT \r\n    max(code), \r\n    min(code)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u2510\r\n\u2502 110999200006000099 \u2502 110000200001010000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT countDistinct(code)\r\nFROM sfz\r\n\r\n\u250c\u2500uniqExact(code)\u2500\u2510\r\n\u2502        50000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-10-14T11:21:00Z",
        "body": "Yes, the value may not fit in `Int64` type:\r\n\r\n```\r\nmilovidov-desktop :) SELECT toInt64(1e18)\r\n\r\nSELECT toInt64(1000000000000000000.)\r\n\r\nQuery id: a33fad40-ac0b-491b-813a-41d749f725c5\r\n\r\n\u250c\u2500toInt64(1000000000000000000.)\u2500\u2510\r\n\u2502           1000000000000000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n\r\nmilovidov-desktop :) SELECT toInt64(1e19)\r\n\r\nSELECT toInt64(10000000000000000000.)\r\n\r\nQuery id: 71e95f22-a6a1-4e01-9c0a-4f748ec09023\r\n\r\n\u250c\u2500toInt64(10000000000000000000.)\u2500\u2510\r\n\u2502           -9223372036854775808 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nmilovidov-desktop :) SELECT toUInt64(1e19)\r\n\r\nSELECT toUInt64(10000000000000000000.)\r\n\r\nQuery id: 6ad6fe7a-127d-4733-b5e6-f7ee033ba71c\r\n\r\n\u250c\u2500toUInt64(10000000000000000000.)\u2500\u2510\r\n\u2502            10000000000000000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n\r\nmilovidov-desktop :) SELECT toUInt64(1e20)\r\n\r\nSELECT toUInt64(100000000000000000000.)\r\n\r\nQuery id: ad01db14-3f82-4658-93a9-677103dc02e4\r\n\r\n\u250c\u2500toUInt64(100000000000000000000.)\u2500\u2510\r\n\u2502                                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-10-14T11:22:02Z",
        "body": "The most recent ClickHouse version has experimental support for 128 and 256 bit integers."
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-15T08:14:12Z",
        "body": "avg() 's result is wrong for Int64\r\n```sql\r\nSELECT\r\n    count(*),\r\n    max(code),\r\n    min(code),\r\n    avg(code),\r\n    substr(toString(code), 1, 4) AS k\r\nFROM sfz\r\nGROUP BY k\r\nORDER BY k ASC\r\n\r\n\u250c\u2500count()\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500avg(code)\u2500\u252c\u2500k\u2500\u2500\u2500\u2500\u2510\r\n\u2502 5000000 \u2502 110099200006000099 \u2502 110000200001010000 \u2502  114208568606.46927 \u2502 1100 \u2502\r\n\u2502 5000000 \u2502 110199200006000099 \u2502 110100200001010000 \u2502   501790570574.8905 \u2502 1101 \u2502\r\n\u2502 5000000 \u2502 110299200006000099 \u2502 110200200001010000 \u2502   889372572543.3118 \u2502 1102 \u2502\r\n\u2502 5000000 \u2502 110399200006000099 \u2502 110300200001010000 \u2502  1276954574511.7332 \u2502 1103 \u2502\r\n\u2502 5000000 \u2502 110499200006000099 \u2502 110400200001010000 \u2502  1664536576480.1545 \u2502 1104 \u2502\r\n\u2502 5000000 \u2502 110599200006000099 \u2502 110500200001010000 \u2502 -1637230236293.3347 \u2502 1105 \u2502\r\n\u2502 5000000 \u2502 110699200006000099 \u2502 110600200001010000 \u2502 -1249648234324.9133 \u2502 1106 \u2502\r\n\u2502 5000000 \u2502 110799200006000099 \u2502 110700200001010000 \u2502  -862066232356.4922 \u2502 1107 \u2502\r\n\u2502 5000000 \u2502 110899200006000099 \u2502 110800200001010000 \u2502  -474484230388.0709 \u2502 1108 \u2502\r\n\u2502 5000000 \u2502 110999200006000099 \u2502 110900200001010000 \u2502  -86902228419.64958 \u2502 1109 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n10 rows in set. Elapsed: 1.667 sec. Processed 50.00 million rows, 400.00 MB (30.00 million rows/s., 240.01 MB/s.)\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-15T08:17:17Z",
        "body": "add toFoloat64() can get right result\r\n```sql\r\nSELECT\r\n    count(*),\r\n    max(code),\r\n    min(code),\r\n    avg(toFloat64(code)),\r\n    substr(toString(code), 1, 4) AS k\r\nFROM sfz\r\nGROUP BY k\r\nORDER BY k ASC\r\n\r\n\u250c\u2500count()\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u252c\u2500avg(toFloat64(code))\u2500\u252c\u2500k\u2500\u2500\u2500\u2500\u2510\r\n\u2502 5000000 \u2502 110099200006000099 \u2502 110000200001010000 \u2502   110049700003509260 \u2502 1100 \u2502\r\n\u2502 5000000 \u2502 110199200006000099 \u2502 110100200001010000 \u2502   110149700003674400 \u2502 1101 \u2502\r\n\u2502 5000000 \u2502 110299200006000099 \u2502 110200200001010000 \u2502   110249700003453650 \u2502 1102 \u2502\r\n\u2502 5000000 \u2502 110399200006000099 \u2502 110300200001010000 \u2502   110349700003415840 \u2502 1103 \u2502\r\n\u2502 5000000 \u2502 110499200006000099 \u2502 110400200001010000 \u2502   110449700003517810 \u2502 1104 \u2502\r\n\u2502 5000000 \u2502 110599200006000099 \u2502 110500200001010000 \u2502   110549700003510430 \u2502 1105 \u2502\r\n\u2502 5000000 \u2502 110699200006000099 \u2502 110600200001010000 \u2502   110649700003511710 \u2502 1106 \u2502\r\n\u2502 5000000 \u2502 110799200006000099 \u2502 110700200001010000 \u2502   110749700003574190 \u2502 1107 \u2502\r\n\u2502 5000000 \u2502 110899200006000099 \u2502 110800200001010000 \u2502   110849700003585060 \u2502 1108 \u2502\r\n\u2502 5000000 \u2502 110999200006000099 \u2502 110900200001010000 \u2502   110949700003438200 \u2502 1109 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-15T08:47:31Z",
        "body": "sum() is also bad, if i use Int128 type, it can do right, but avg() is still not accurate\r\n```sql\r\nSELECT sum(code)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(code)\u2500\u2510\r\n\u2502 682658504670491840 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nset allow_experimental_bigint_types =1;\r\ncreate table sfz2(code Int128)ENGINE = MergeTree()order by code;\r\ninsert into sfz2 select * from sfz limt 10000;\r\nSELECT sum(code)\r\nFROM sfz2\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(code)\u2500\u2510\r\n\u2502 1100002000015050495000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT avg(code)\r\nFROM sfz2\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500avg(code)\u2500\u2510\r\n\u2502 110000200001505060 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      }
    ]
  },
  {
    "number": 15903,
    "title": "DB::Exception: Unknown data type family: DateTime64 while import from tsv, csv",
    "created_at": "2020-10-13T09:40:24Z",
    "closed_at": "2020-10-13T14:56:07Z",
    "labels": [
      "question",
      "comp-datetime",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15903",
    "body": "When I import data from external file (tsv, csv) I got error:\r\n\r\n**Code: 50. DB::Exception: Unknown data type family: DateTime64**\r\n\r\nServer version 20.9.2.\r\n\r\n```\r\n24b27b0d4af5 :) SELECT * FROM system.data_type_families WHERE name LIKE 'DateTime%';\r\n\r\nSELECT * FROM system.data_type_families WHERE name LIKE 'DateTime%'\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500case_insensitive\u2500\u252c\u2500alias_to\u2500\u2510\r\n\u2502 DateTime   \u2502                1 \u2502          \u2502\r\n\u2502 DateTime64 \u2502                1 \u2502          \u2502\r\n\u2502 DateTime32 \u2502                1 \u2502          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nDateTime64 is present.\r\n\r\nMy data:\r\n\r\n\r\n**database**\r\n```\r\nCREATE TABLE log_viewing  \r\n(\r\n  id UInt32,\r\n  ts DateTime64(6, 'Europe/Moscow'),\r\n  document_id UInt16,\r\n  user_id UInt16,\r\n  element_id_max UInt16,\r\n  element_id_max_child UInt16,\r\n  element_id_min UInt16,\r\n  element_id_min_child UInt16,\r\n  status UInt8,\r\n  tz_offset Int16,\r\n  ts_local DateTime64(6, 'Europe/Moscow'),\r\n  source UInt8,\r\n  duration UInt16\r\n) ENGINE Log;\r\n```\r\n\r\nFile **log_viewing-0.tsv**\r\n```\r\nid\tts\tdocument_id\tuser_id\telement_id_min\telement_id_min_child\telement_id_max\telement_id_max_child\tstatus\ttz_offset\tts_local\tsource\tduration\r\n1592845\t2019-07-23 12:31:31.997075\t4\t2\t1\t1\t10\t10\t2\t-180\t2019-07-23 12:31:31.997075\t1\t0\r\n1592846\t2019-07-23 12:31:33.997075\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:33.997075\t1\t2000\r\n1592847\t2019-07-23 12:31:35.497075\t4\t2\t2\t2\t6\t6\t2\t-180\t2019-07-23 12:31:35.497075\t1\t1000\r\n1592848\t2019-07-23 12:31:36.497075\t4\t2\t1\t1\t4\t4\t2\t-180\t2019-07-23 12:31:36.497075\t1\t1000\r\n1592849\t2019-07-23 12:31:37.997075\t4\t2\t2\t2\t5\t5\t2\t-180\t2019-07-23 12:31:37.997075\t1\t2000\r\n1592850\t2019-07-23 12:31:39.497075\t4\t2\t1\t1\t4\t4\t2\t-180\t2019-07-23 12:31:39.497075\t1\t1000\r\n1592851\t2019-07-23 12:31:40.497075\t4\t2\t2\t2\t4\t4\t2\t-180\t2019-07-23 12:31:40.497075\t1\t1000\r\n1592852\t2019-07-23 12:31:40.997075\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:40.997075\t1\t1000\r\n1592854\t2019-07-23 12:31:48.191737\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:48.191737\t1\t7000\r\n```\r\n\r\nCommand for import:\r\n```\r\nclickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15903/comments",
    "author": "borisovcode",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-13T12:57:45Z",
        "body": "Your clickhouse-client is outdated and does not support DateTime64.\r\nInstall clickhouse-client with version 20.9.2 \r\n\r\nIn case of \r\n`clickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv`\r\nclickhouse-client reads a stream and does a stream parsing and sends parsed data in the Native format to a server.\r\n\r\n\r\n\r\n```\r\n$ clickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv\r\n\r\n\r\nClickHouse client version 20.11.1.4897 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.11.1 revision 54441.\r\n\r\nselect count() from log_viewing\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       9 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      },
      {
        "user": "borisovcode",
        "created_at": "2020-10-13T14:56:03Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 15851,
    "title": "How to remove default value for column?",
    "created_at": "2020-10-12T08:22:40Z",
    "closed_at": "2020-10-12T13:52:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15851",
    "body": "Hello. Here's a table for example:\r\n```sql\r\nCREATE TABLE test.table1\r\n(\r\n        EventDate Date,\r\n        Id Int32,\r\n        Value String default 'strstrstr'\r\n)\r\nEngine = MergeTree()\r\nPARTITION BY toYYYYMM(EventDate)\r\nORDER BY Id;\r\n```\r\n\r\nI can modify default value for \"Value\" column like: `ALTER TABLE test.table1 MODIFY COLUMN Value DEFAULT 'mystring'`\r\nBut how can I remove this default value? Even if I execute `ALTER TABLE test.table1 MODIFY COLUMN Value DEFAULT ''`, it just defaults to an empty string.\r\nAnd also I cannot do this for types like Int32, because this will throw an error on future SELECT\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15851/comments",
    "author": "MasterGroosha",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-12T13:43:16Z",
        "body": "String type:  MODIFY COLUMN Value DEFAULT ''\r\nInt32 type:  MODIFY COLUMN Value DEFAULT 0"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-12T13:48:27Z",
        "body": "And starting from CH 20.10\r\n\r\n```sql\r\nalter table table1 \r\n   MODIFY COLUMN Value \r\n   REMOVE DEFAULT;\r\n```"
      },
      {
        "user": "MasterGroosha",
        "created_at": "2020-10-12T13:51:58Z",
        "body": "@den-crane Thank you! Looking forward to installing 20.10 as soon as it is released."
      }
    ]
  },
  {
    "number": 15848,
    "title": "Must there be enough memory to use GROUP BY?",
    "created_at": "2020-10-12T04:30:37Z",
    "closed_at": "2020-10-13T21:42:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15848",
    "body": "The computer used for testing has 16GB of RAM.\r\nThese configurations have been set:\r\n`<max_memory_usage_for_all_queries>12000000000</max_memory_usage_for_all_queries>\r\n            <max_bytes_before_external_group_by>6000000000</max_bytes_before_external_group_by>\r\n            <max_memory_usage>12000000000</max_memory_usage>`\r\nBut still got an error:\r\nDB::Exception: Memory limit (total) exceeded: would use 13.95 GiB (attempt to allocate chunk of 134217760 bytes), maximum: 13.90 GiB: While executing AggregatingTransform.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15848/comments",
    "author": "qinglok",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-12T13:31:34Z",
        "body": "What CH version do you use?\r\n\r\nTry to lower max_memory_usage  to 10G `<max_memory_usage>10000000000` and max_bytes_before_external_group_by to 4G `<max_bytes_before_external_group_by>4000000000`\r\n"
      },
      {
        "user": "qinglok",
        "created_at": "2020-10-12T16:39:43Z",
        "body": "Yes, it seems to solve the problem.\r\n\r\nAnother question about AggregatingMergeTree is why there is no data in the tables of the AggregatingMergeTree engine after the CH service is restarted?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-12T21:01:33Z",
        "body": "What CH version do you use?\r\n\r\n>Another question about AggregatingMergeTree is why there is no data in the \r\n>tables of the AggregatingMergeTree engine after the CH service is restarted?\r\n\r\nDisk corruption? \r\nCheck startup messages **grep table_name /var/log/clickhouse-server/clickhouse-server.log**"
      },
      {
        "user": "qinglok",
        "created_at": "2020-10-13T06:30:15Z",
        "body": "CH version is 20.8.2.3-2.\r\nThe disk is normal. Because I can see that there are still files in the data directory of CH.\r\nHowever, after the CH service is restarted, no data can be found by using SELECT.\r\nBut now I can\u2019t reproduce the same situation.\r\n"
      }
    ]
  },
  {
    "number": 15835,
    "title": "tokenbf_v2 index does not drop rows in aggregation query",
    "created_at": "2020-10-11T08:56:05Z",
    "closed_at": "2020-10-13T21:43:12Z",
    "labels": [
      "question",
      "comp-skipidx",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15835",
    "body": "The tokenbf_v2 index does not drop rows when used in an aggregation query. That is, according to the query trace, it is used and rows are said to be dropped, but eventually all rows are scanned anyway as if the index didn't exist.\r\n\r\nIn my case I have a (large) table like:\r\n\r\n```\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64\r\n)\r\nENGINE = ReplicatedMergeTree\r\nPARTITION BY Year,\r\nORDER BY (Month, Route, Count),\r\nINDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1,\r\nSETTINGS index_granularity = 128;\r\n```\r\n\r\nRoute is a comma separated string of id's, like '242341,345223,12341'. There can be hundreds of id's in a Route.\r\n\r\nWhen I query: \r\n\r\n`select Count(*) from MYTABLE where Year = '2020' and hasToken(Route, '12341')`\r\n\r\nthe query trace shows this:\r\n\r\nKey condition: (column 2 in ['2020, '2020']), unknown, and\r\nMinMax index condition: (column 0 in ['2020', '2020']), unknown, and\r\nIndex `route_index` has dropped 254959 granules.\r\nSelected 1 parts by date, 1 parts by key, 17961 marks to read from 7839 ranges\r\nReading approx. 2299008 rows with 2 streams\r\n\r\nwhich looks good, but then it proceeds to read all rows anyway:\r\n\r\nAggregated. 233990 to 1 rows (from 0.225 MiB) in 5.498 sec. (42560.946 rows/sec., 0.041 MiB/sec.)\r\nAggregated. 104612 to 1 rows (from 0.102 MiB) in 5.774 sec. (18119.055 rows/sec., 0.018 MiB/sec.)\r\nAggregated. 130101 to 1 rows (from 0.127 MiB) in 5.775 sec. (22529.594 rows/sec., 0.022 MiB/sec.)\r\nAggregator: Merging aggregated data\r\nexecuteQuery: Read **34925324** rows, 16.27 GiB in 5.966 sec., 5853956 rows/sec., 2.73 GiB/sec.\r\n\r\nActually if I run the query like this, which does not invoke the route_index:\r\n\r\n`select Count(*) from MYTABLE where Year = '2020' and Route LIKE '%12341%'`\r\n\r\nin this case the query will be faster! The reason is it also scans all rows but does not have the extra first step of the tokenbf index.\r\n\r\nExpected behaviour:\r\n\r\nSince the tokenbf_v2 filter was able to skip 34925324 - 2299008 rows, which is 93% of the total number of rows, I expected the hasToken query to be faster than the LIKE query which didn't use any index.\r\nI don't understand why the hasToken query, after initially dropping all those rows, proceeds to scan all rows anyway.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15835/comments",
    "author": "misja-alma",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-11T13:40:03Z",
        "body": "What CH version do you use?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-11T14:18:45Z",
        "body": "`route_index` analysis takes 200ms -- and this is expected.\r\n\r\n```sql\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64,\r\n    INDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nORDER BY tuple()\r\nSETTINGS index_granularity = 128;\r\n\r\ninsert into MY_TABLE select '2020', arrayStringConcat(arrayMap(i-> toString(intHash32(i*number)) ,range(10)),','), number\r\nfrom numbers(100000);\r\n\r\ninsert into MY_TABLE select '2020', '2299008,2299008,2299008', number from numbers(100000000);\r\n\r\nOPTIMIZE TABLE MY_TABLE FINAL\r\n\r\nselect Count(*) from MY_TABLE where hasToken(Route, '3119550599')\r\nIndex `route_index` has dropped 781655 / 782032 granules.\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.219 sec. Processed 48.26 thousand rows, 5.58 MB (220.13 thousand rows/s., 25.46 MB/s.)\r\n\r\n\r\nselect Count(*) from MY_TABLE where Route like '%3119550599%'\r\nIndex `route_index` has dropped 0 / 782032 granules.\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.643 sec. Processed 100.10 million rows, 3.21 GB (155.56 million rows/s., 4.99 GB/s.)\r\n\r\n\r\nselect Count(*) from MY_TABLE where Route like '%,3119550599,%'\r\nIndex `route_index` has dropped 781655 / 782032 granules.\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.213 sec. Processed 48.26 thousand rows, 5.58 MB (226.53 thousand rows/s., 26.20 MB/s.)\r\n\r\nalter table MY_TABLE drop index route_index;\r\nselect Count(*) from MY_TABLE where Route like '%3119550599%'\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.459 sec. Processed 100.10 million rows, 3.21 GB (217.92 million rows/s., 6.99 GB/s.)\r\n\r\n\r\nselect Count(*) from MY_TABLE where hasToken(Route, '3119550599')\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.443 sec. Processed 100.10 million rows, 3.21 GB (225.78 million rows/s., 7.24 GB/s.)\r\n\r\n```\r\n\r\nEverything works as expected.\r\n\r\nhasToken === like '%,3119550599,%'\r\n\r\n\r\nThough it's weird that `like '%3119550599%'` -- does not know that `3119550599` is a token but still uses an index.\r\n"
      },
      {
        "user": "misja-alma",
        "created_at": "2020-10-11T17:39:37Z",
        "body": "Thanks, I checked and you are right. I don't know what went wrong the first time but I recreated the table and now things seem to work correctly. Also nice that the index can also be used in LIKE queries, as long as I add the comma's.\r\n\r\nHowever, I noticed that the difference starts to become smaller when there are more partitions filled in my table. In your example there is no partitioning but my table has partitioning by YEAR. When there is only one partition filled, the query is a lot faster with the route_index. But if I add more years, the difference in speed becomes much smaller until it actually becomes much slower to query with index than without.\r\n\r\n(to be clear, querying without index I do like:\r\n\r\n`select Count(*) from MY_TABLE where Year = '2020' and Route like '%3119550599%'`\r\n\r\nand to invoke the index I add the comma's to the like )\r\n\r\nActually I noticed that the difference only appears if I call another optimize after adding the data to my table with the index.\r\nIf I only add the data and don't call optimize, the query with index remains faster."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-11T20:42:38Z",
        "body": "@misja-alma I don't understand what is you mean about partitions. Partition pruning works before index analysis.\r\n\r\n\r\n\r\n```sql\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64,\r\n    INDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\npartition by Year\r\nORDER BY tuple()\r\nSETTINGS index_granularity = 128;\r\n\r\ninsert into MY_TABLE select '2020', arrayStringConcat(arrayMap(i-> toString(intHash32(i*number)) ,range(10)),','), number\r\nfrom numbers(100000);\r\ninsert into MY_TABLE select '2020', '2299008,2299008,2299008', number from numbers(100000000);\r\n\r\ninsert into MY_TABLE select '2019', '2299008,2299008,2299008', number from numbers(100000000);\r\ninsert into MY_TABLE select '2019', arrayStringConcat(arrayMap(i-> toString(intHash32(i*number)) ,range(10)),','), number\r\nfrom numbers(10000000);\r\n\r\noptimize table MY_TABLE  final ;\r\n\r\nselect Count(*) from MY_TABLE where Year = '2020' and hasToken(Route, '3119550599');\r\n1 rows in set. Elapsed: 0.238 sec. Processed 48.26 thousand rows, 5.63 MB (202.61 thousand rows/s., 23.64 MB/s.)\r\n\r\nselect Count(*) from MY_TABLE where Year = '2020' and  Route like '%3119550599%'\r\n1 rows in set. Elapsed: 0.744 sec. Processed 100.10 million rows, 3.31 GB (134.52 million rows/s., 4.45 GB/s.)\r\n\r\nselect Count(*) from MY_TABLE where Year = '2020' and  Route like '%,3119550599,%'\r\n1 rows in set. Elapsed: 0.233 sec. Processed 48.26 thousand rows, 5.63 MB (207.27 thousand rows/s., 24.18 MB/s.)\r\n\r\n```"
      },
      {
        "user": "misja-alma",
        "created_at": "2020-10-12T06:20:34Z",
        "body": "@den-crane I just reported what I was seeing. I also don't quite understand why the index query slowed down when filling more than one partition. But I have a feeling that it might just be the amount of data that made the index grow so large that it didn't fit into memory anymore.\r\nTo test this I tried making the index_granularity larger and the index queries started to be fast again, also with multiple partitions. This despite the fact that with larger index granularity the bloom filter cannot be as precise in dropping granules."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-13T00:50:46Z",
        "body": ">This despite the fact that with larger index granularity the bloom filter cannot be as precise in dropping granules.\r\n\r\nAnalyzing skip indexes is a super-slow process and they are heavy and need to read them from disk.\r\nWhen an index granula covers many rows it speeds up the index analyzes but decreases number of dropped rows."
      }
    ]
  },
  {
    "number": 15802,
    "title": "Connecttion refused after kill the clickhouse server process",
    "created_at": "2020-10-10T02:18:38Z",
    "closed_at": "2020-10-12T03:04:30Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15802",
    "body": "I have a clickhouse table containing more than 40 billion records.  One day I want to modify a parameter in my config.xml. So I attempted to stop the server but it was stuck. I guess there were many background processes working. Then I kill the clickhouse server with \"kill -9\".\r\nHowever when I tried to restart the server and run clickhouse-client again, I got \"connection refused\". \r\nIf I remove all data partitions and the corresponding table metadata (sql), it works fine again. Once I move the table data back to its position, I got the same error \"connection refused\" again.\r\n\r\n**How to reproduce**\r\nif I tried to stop the clickhosue server process with \"kill -9\", but not \"clickhouse-server stop\". the error occurs. Especially when its a massive table.\r\n\r\n* Which ClickHouse server version to use\r\nversion 20.0.10\r\n\r\nIf any ideas? Thanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15802/comments",
    "author": "y45398jp123",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-10T02:22:20Z",
        "body": "40 billion records -- it's not big table. There are installation with 40 trillions.\r\nCH start time depends on number of parts (table partitioning).\r\n\r\nCan you show \r\n`select count() from system.parts`?\r\n`select count() from system.columns where tables = ....` ?"
      },
      {
        "user": "y45398jp123",
        "created_at": "2020-10-10T02:38:42Z",
        "body": "Thanks for your reply @den-crane \r\nthe table contains 14521 partitions and 119 columns.\r\ndo i have to wait for longer time after restart the server before trying to connect? have no idea...\r\nThanks."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-10T12:30:11Z",
        "body": ">The table contains 14521 partitions and 119 columns.\r\n\r\nThen slow start is expected. Especially on slow HDD or IOPS limited cloud disks (like AWS EBS GP1)\r\nYou should wait longer  for a start minutes. I've heard that in case of 100000 CH can start up to 40 minutes.\r\nThis is expected by design. Because CH reads reads every part (metadata) on start. \r\n\r\nYou can use disks with low latency (SSD/NVME) or reduce number of partitions (by re-desingning your table -- partition by section)."
      },
      {
        "user": "y45398jp123",
        "created_at": "2020-10-12T03:04:26Z",
        "body": "thanks so much for reply, quick and professional !"
      }
    ]
  },
  {
    "number": 15464,
    "title": "Can't Import Parquet on macOS",
    "created_at": "2020-09-30T08:45:34Z",
    "closed_at": "2020-09-30T13:10:58Z",
    "labels": [
      "question",
      "build",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15464",
    "body": "MacOS Catalina\r\n\r\n```\r\n\u279c  ~ docker ps\r\nCONTAINER ID        IMAGE                                COMMAND             CREATED             STATUS              PORTS                                                      NAMES\r\nb9d8daab2501        yandex/clickhouse-server:20.9.2.20   \"/entrypoint.sh\"    2 hours ago         Up 2 hours          0.0.0.0:8123->8123/tcp, 0.0.0.0:9000->9000/tcp, 9009/tcp   adoring_nash\r\n\r\n\u279c  ~ cat ~/Downloads/cleand.parquet | clickhouse-client --query=\"INSERT INTO xm_rspd_data FORMAT Parquet\"\r\nCode: 73. DB::Exception: Unknown format Parquet: data for INSERT was parsed from stdin\r\n\r\n\u279c  ~ clickhouse-client\r\nClickHouse client version 20.10.1.4800 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.9.2 revision 54439.\r\n\r\nClickHouse server version is older than ClickHouse client. It may indicate that the server is out of date and can be upgraded.\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15464/comments",
    "author": "pan3793",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-30T13:04:54Z",
        "body": "Parquet is excluded from MacOS build, and mysql. \r\nCheck make file. \r\n\r\nYou can use dockerized `clickhouse-client` as well.\r\ndocker run -it --rm --link some-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server"
      },
      {
        "user": "pan3793",
        "created_at": "2020-09-30T13:10:58Z",
        "body": "> Parquet is excluded from MacOS build, and mysql.\r\n> Check make file.\r\n> \r\n> You can use dockerized `clickhouse-client` as well.\r\n> docker run -it --rm --link some-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server\r\n\r\nThanks for explanation, it works in docker."
      }
    ]
  },
  {
    "number": 15431,
    "title": "How to convert ColumnPtr object to ColumnUInt64 * object?",
    "created_at": "2020-09-29T07:12:53Z",
    "closed_at": "2020-10-13T22:11:01Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15431",
    "body": "I define a class A, which has a member `ColumnUInt64 * col` in A.h file. Then, in A.cpp, I initialize the member `col` in the way:\r\n```\r\ncol = ColumnUInt64::create();\r\n```\r\nand I got a error\r\n```\r\ncan not convert COWHelper<DB::ColumnVectorHelper, DB::ColumnVector<long unsigned int> >::MutablePtr {aka \u2018COW<DB::IColumn>::mutable_ptr<DB::ColumnVector<long unsigned int> >\u2019} to non-scalar std::__1::unique_ptr<DB::ColumnVector<long unsigned int> >\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15431/comments",
    "author": "744570676",
    "comments": [
      {
        "user": "Sasasu",
        "created_at": "2020-09-29T07:29:16Z",
        "body": "`ColumnUInt64 *col = ColumnUInt64::create().detach();` if you realy want."
      },
      {
        "user": "744570676",
        "created_at": "2020-09-29T13:39:33Z",
        "body": "> `ColumnUInt64 *col = ColumnUInt64::create().detach();` if you realy want.\r\n\r\nok, thank you~ :)"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-29T14:47:32Z",
        "body": "@Sasasu @744570676 I'm afraid that's not the best answer.\r\nBecause it will release the ownership of the object.\r\n\r\nJust write `col.get()` to obtain non-owning pointer whille keeping the ownership by MutablePtr object."
      }
    ]
  },
  {
    "number": 15314,
    "title": "Fetch Single Row From Select Query Performance",
    "created_at": "2020-09-25T16:00:13Z",
    "closed_at": "2020-10-17T15:59:11Z",
    "labels": [
      "question",
      "performance",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15314",
    "body": "In a SELECT query, I want to return a single row record out of billions of rows in ClickHouse. I know that Clickhouse is not meant for single queries but here I have no other choice. I would like to enhance the performance of the query as much as possible. The query is the following:\r\n\r\n```select * from products where un_id='us-f032f8df-65c9-4f0b-8df2-ddb3a436ae7e' and organization_id='test' and request_time >= '2020-09-25 00:00:00' limit 1```\r\n\r\norganization_id and request_time are both partitioning keys. \r\n\r\nMy default settings for max_threads are 4 while for max_block_size is 65505. I have also tried setting max_threads=1, max_block_size=1024 (answer from a previous post here) but this did not really help with the speed of the query. \r\n\r\n I would like to achieve a response in less than 4 seconds. Is sth like this possible with this amount of data (billions of records)? \r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15314/comments",
    "author": "stcharitak",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-25T16:31:29Z",
        "body": "You have no choice except to move un_id to the beginning of PRIMARYINDEX.\r\nAnd try to clean un_id from 'us-' and save UUID as UUID (or as hex/FIXED String) not a String.\r\n\r\n```\r\n\r\ncreate table product (\r\n         country String, \r\n         un_id UUID, \r\n         organization_id LowCardinality(String),\r\n         request_time DateTime) \r\nEngine=MergeTree \r\npartition by (organization_id, toYYYYMM(request_time))\r\norder by (country, un_id, request_time)\r\nsettings index_granularity=1024;\r\n\r\ninsert into product select 'us', generateUUIDv4(), 'test', toDateTime('2020-01-01 00:00:00') + intDiv(number, 100)\r\nfrom numbers(1000000000);\r\n\r\n0 rows in set. Elapsed: 551.801 sec.\r\n\r\n\r\nSELECT *\r\nFROM product\r\nLIMIT 100000, 1\r\n\r\n\u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n\u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nTEST:\r\n\r\n\r\nSET max_threads = 1;\r\n\r\n\r\nSELECT *\r\nFROM product\r\nWHERE (organization_id = 'test') AND (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')\r\n\r\n\u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n\u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.004 sec. Processed 15.36 thousand rows, 491.85 KB (4.33 million rows/s., 138.74 MB/s.)\r\n\r\n\r\n\r\n\r\n```\r\n\r\nElapsed: 0.004 sec. is less than 4 sec.\r\nbecause PRIMARYindex is  `(country, un_id, request_time)` and it addresses ` (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')`\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-25T18:26:42Z",
        "body": "It's better to make ORDER BY `organization_id`, `request_time`, not PARTITION BY."
      },
      {
        "user": "stcharitak",
        "created_at": "2020-09-28T14:45:17Z",
        "body": "> You have no choice except to move un_id to the beginning of PRIMARYINDEX.\r\n> And try to clean un_id from 'us-' and save UUID as UUID (or as hex/FIXED String) not a String.\r\n> \r\n> ```\r\n> \r\n> create table product (\r\n>          country String, \r\n>          un_id UUID, \r\n>          organization_id LowCardinality(String),\r\n>          request_time DateTime) \r\n> Engine=MergeTree \r\n> partition by (organization_id, toYYYYMM(request_time))\r\n> order by (country, un_id, request_time)\r\n> settings index_granularity=1024;\r\n> \r\n> insert into product select 'us', generateUUIDv4(), 'test', toDateTime('2020-01-01 00:00:00') + intDiv(number, 100)\r\n> from numbers(1000000000);\r\n> \r\n> 0 rows in set. Elapsed: 551.801 sec.\r\n> \r\n> \r\n> SELECT *\r\n> FROM product\r\n> LIMIT 100000, 1\r\n> \r\n> \u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n> \u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> ------------------------------------------------------------------------------------------------------------------------------------------------\r\n> \r\n> TEST:\r\n> \r\n> \r\n> SET max_threads = 1;\r\n> \r\n> \r\n> SELECT *\r\n> FROM product\r\n> WHERE (organization_id = 'test') AND (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')\r\n> \r\n> \u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n> \u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> 1 rows in set. Elapsed: 0.004 sec. Processed 15.36 thousand rows, 491.85 KB (4.33 million rows/s., 138.74 MB/s.)\r\n> ```\r\n> \r\n> Elapsed: 0.004 sec. is less than 4 sec.\r\n> because PRIMARYindex is `(country, un_id, request_time)` and it addresses ` (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')`\r\n\r\nthanks for the detailed answer. I already test it and I get really fast results."
      },
      {
        "user": "stcharitak",
        "created_at": "2020-10-01T16:04:23Z",
        "body": "Unfortunately, I am still unable to make fast SELECT queries after 150.000.000 inserts. I have the following table:\r\n\r\n```\r\nCREATE TABLE default.products_sharded\r\n(\r\n    `request_time` DateTime DEFAULT now(),\r\n    `un_id` UUID,\r\n    `organization_id` LowCardinality(String),\r\n    `investor` String,\r\n    `provider` String,\r\n    `publisher` String,\r\n    `creator` String,\r\n    `code` String,\r\n    `description` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/products_sharded', '{replica}')\r\nPARTITION BY (organization_id, toMonday(request_time))\r\nORDER BY (un_id)\r\nTTL request_time + INTERVAL + 30 DAY\r\nSETTINGS index_granularity = 1024;\r\n```\r\n\r\nThen I insert data to the table like this:\r\n```\r\nINSERT INTO products_sharded (request_time, un_id, organization_id, investor, provider, publisher, creator, code, description) SELECT\r\n    toDateTime(now()) + toInt64(number) AS request_time,\r\n    generateUUIDv4() AS un_id,\r\n    'test_1' AS organization_id,\r\n    concat('investor_', toString((number % 50) + 1)) AS investor,\r\n    concat('provider_', toString((number % 50) + 1)) AS provider,\r\n    concat('publisher_', toString((number % 50) + 1)) AS publisher,\r\n    concat('creator_', toString((number % 50) + 1)) AS creator,\r\n    'C98A1D7F-30EC-1016-9C72-43350A39E86C' AS code,\r\n    'some description for the product' AS description\r\nFROM system.numbers\r\nLIMIT 10000000\r\n```\r\n\r\nThen I would like to do the following query:\r\n```\r\nSELECT * FROM products_distributed WHERE un_id='0b0ed88e-e645-4d49-9b43-1a6b3f8fac4e ' limit 1\r\n```\r\n\r\nWhen I make the query I also know the organization_id, investor, provider, and publisher. So I could add those fields to the query if it is needed to make it faster. Unfortunately, I cannot know the request time. So I cannot include it in the query. This is why also I have set a TTL for my table to 30 days. The organization_id is part of the partition so I guess it should make the query much faster when there are 50 organizations for example. I would like the query to be also fast if the `un_id` does not exist in the table. \r\n\r\nWhen my table contains 100.000.000, the query is quite fast.\r\n\r\n```\r\nSET max_threads=1\r\n\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = '661ca451-3f9a-405e-8000-351a80a1cb7c'\r\nLIMIT 1\r\n\r\n0 rows in set. Elapsed: 0.155 sec. Processed 227.33 thousand rows, 42.35 MB (1.46 million rows/s., 272.81 MB/s.)\r\n\r\nand even better when `un_id` exists\r\n\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = 'a6c90405-ca2a-4a90-9a6a-77edc629055f'\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nrequest_time:    2020-10-01 15:37:07\r\nun_id:           a6c90405-ca2a-4a90-9a6a-77edc629055f\r\norganization_id: test_1\r\ninvestor:        investor_1\r\nprovider:        provider_1\r\npublisher:       publisher_1\r\ncreator:         creator_1\r\ncode:            C98A1D7F-30EC-1016-9C72-43350A39E86C\r\ndescription:     some description for the product\r\n\r\n1 rows in set. Elapsed: 0.108 sec. Processed 126.98 thousand rows, 23.66 MB (1.18 million rows/s., 219.00 MB/s.)\r\n```\r\n\r\nIf I repeat the process after adding more than 200.000.000 records for one organization_id (here partitioning cannot help, for now, it can be useful later with more organization_ids. But an organization_id might have 1 billion of products) then the query is really slow:\r\n\r\n```\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = '8aa40f91-119c-4e26-9afb-81b538caddbc'\r\nLIMIT 1\r\n\r\n0 rows in set. Elapsed: 13.764 sec. Processed 435.20 thousand rows, 81.08 MB (31.62 thousand rows/s., 5.89 MB/s.)\r\n\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = '6b8be299-bdb9-4632-871d-f5d3ca530bf7'\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nrequest_time:    2020-10-01 15:48:40\r\nun_id:           6b8be299-bdb9-4632-871d-f5d3ca530bf7\r\norganization_id: test_1\r\ninvestor:        investor_1\r\nprovider:        provider_1\r\npublisher:       publisher_1\r\ncreator:         creator_1\r\ncode:            C98A1D7F-30EC-1016-9C72-43350A39E86C\r\ndescription:     some description for the product\r\n\r\n1 rows in set. Elapsed: 5.748 sec. Processed 272.38 thousand rows, 50.75 MB (47.39 thousand rows/s., 8.83 MB/s.)\r\n\r\n```\r\n\r\nHere I would like to point out that if I do the following:\r\n\r\n```\r\nSELECT *\r\nFROM products_distributed\r\nLIMIT 100000, 1\r\n```\r\n\r\nand get the `un_id` from the result and query by that `un_id` of course this will be really fast since I guess there has sth to do with Clickhouse caching. \r\n\r\nI think the proper way to test this is to manually insert a new record with known `un_id` then insert a few millions of records and then make the query with the known `un_id`\r\n\r\nI have tried many combinations e.g. removing request time from partitioning adding organization_id as the primary key as well and different select queries including other fields as well. Nothing has worked so far when it comes to more than 200.000.000 records. \r\n\r\nI hope now it is clearer what I've been trying to achieve. I am wondering if I can achieve sth like that with Clickhouse. Are there any other recommendations?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-03T23:00:25Z",
        "body": "It's because you partitioned your table by `organization_id`--  `PARTITION BY (organization_id, toMonday(request_time))`\r\nBut your queries does not have `organization_id` predicate.\r\n\r\nSimply change partition key to `PARTITION BY (toMonday(request_time))`\r\nor better to `PARTITION BY (toYYYYMM(request_time))`\r\n\r\nAnd you can change granularity to 256 `index_granularity=256`\r\n\r\nAlso if you use sharding you can shard by un_id and enable `optimize_skip_unused_shards`.\r\n\r\n\r\nAnd the last thing. Why do you use CH? CH is not designed for K/V. Try K/V databases. Redis, Cassandra. You will speedup your queries 1000 times with a cluster of  Cassandra."
      }
    ]
  },
  {
    "number": 15295,
    "title": "Can I create MaterializedView over remote table?",
    "created_at": "2020-09-25T09:14:18Z",
    "closed_at": "2020-09-26T22:43:23Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15295",
    "body": "There are 2 clusters.\r\nI create table `base` and table `destination` on `cluster_1`.\r\n```\r\ncreate table base (id Int8, name String) ENGINE=MergeTree() order by id;\r\ncreate table destination (id Int8, cnt Int8) ENGINE=MergeTree() order by id;\r\n```\r\nI create materialized view `view` on `cluster_1` and it's working as expected.\r\n```\r\ncreate MATERIALIZED VIEW view to destination as select id,count(name) as cnt from test;\r\n```\r\n**And here is the problem**. I create table `destination` and materialized view `view` on `cluster_2` which is based on table `test` from `cluster_1`.\r\n```\r\ncreate table destination (id Int8, cnt Int8) ENGINE=MergeTree() order by id;\r\ncreate MATERIALIZED VIEW view to destination as select id,count(name) as cnt from remote('cluster_1',default.test,'default','') group by id;\r\n```\r\nI get exception:\r\n```\r\nReceived exception from server (version 20.4.4):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Logical error while creating StorageMaterializedView. Could not retrieve table name from select query..\r\n```\r\nIt seems like materializedView source cannot be a remote table. And then I try to create a remote materializedView on `cluster_1`.\r\n```\r\ncreate MATERIALIZED VIEW remote_view to remote('cluster_2',default.destination,'default','') as select id,count(name) as cnt from test\r\n```\r\nFail again:\r\n```\r\nSyntax error: failed at position 48:\r\nExpected one of: CONSTRAINT, identifier, column declaration, INDEX, list of elements, columns or indices declaration list, table property (column, index, constraint) declaration\r\n```\r\n\r\nDo you know how to create MaterializedView over remote tables?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15295/comments",
    "author": "winter7",
    "comments": [
      {
        "user": "winter7",
        "created_at": "2020-09-25T09:45:50Z",
        "body": "The reason I ask this question is that I want to use Kafka Engine. I wonder if I should temporarily add some ClickHouse instances with kafka engine table as consumers when Kafka traffic surges and convert data by materialized View to the original (remote) Clickhouse instances. Is this a good way to deal with the sudden increase in Kafka traffic?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-25T14:25:39Z",
        "body": "MaterializedView is an insert trigger. \r\n\r\nIt gets data from **INSERT**. \r\n\r\nIt **never** reads (selects) from table.\r\n\r\nYou must create MaterializedView at a server where inserts happen. You can re-route MaterializedView output to remote server. `create table xxx as remote(); create Materialized View .... to xxx as select .....` "
      },
      {
        "user": "winter7",
        "created_at": "2020-09-27T02:55:11Z",
        "body": "Thank you! I think `create table xxx as remote();` is what I missed in my second attempt."
      }
    ]
  },
  {
    "number": 15278,
    "title": "Add summarized column to SummingMergeTree",
    "created_at": "2020-09-25T06:50:26Z",
    "closed_at": "2020-09-26T22:44:07Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15278",
    "body": "Hello guys,I have a SummingMergeTree engine table like \r\n`ENGINE = SummingMergeTree((value_a,value_b))`\r\nNow I would like to add a `value_c` UInt64 to table,is that a way I can add value `value_c` to summarized column array?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15278/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-25T14:19:18Z",
        "body": "There is no easy way to change this list `(value_a,value_b)`. \r\nUsually it is excessive section and you can create just SummingMergeTree() then Summing will sum all columns except columns in ORDERBY list.\r\n\r\nOptions how to change it:\r\n1. detach table XXX;\r\n   vi /var/lib/clickhouse/metadata/{db}/{table}.sql # edit file manually and change that list.\r\n   attach table XXX;\r\n\r\n2. create a new table YYY with exactly the same structure as XXX but with new ((value_a,value_b)) list . \r\n    alter table YYY attach partition .... from XXX; # for each partition in XXX\r\n    rename table XXX to XXX_old, YYY to XXX;\r\n    drop table XXX_old"
      },
      {
        "user": "byx313",
        "created_at": "2020-09-27T01:27:27Z",
        "body": "thank you den!"
      }
    ]
  },
  {
    "number": 15161,
    "title": "Table system.query_log does not exist for few systems",
    "created_at": "2020-09-22T17:32:32Z",
    "closed_at": "2020-09-23T06:59:33Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15161",
    "body": "Hi,\r\nWe are using clickhouse on our production servers. I have noticed that query_log table sometimes exist on some server and on some it does not exist.\r\nLike I am running clickhouse on two servers with same default configs, no changes except for clickhouse version:\r\n1)Clickhouse version 20..6.3 (here the table exists) \r\n2)Clickhouse version 20..4.6 (here the table does not exist)\r\nQuestion: \r\n1) does query_log is enabled by default in latest version??\r\n2) Does it depend on disk space also?? i mean the table will grow over time and does it get deleted afterwards or something like that?? When does the table flushes??",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15161/comments",
    "author": "John-belt",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-22T18:34:20Z",
        "body": "It is enabled by default for all queries since version 20.5.\r\nIn older version you can enable it manually with the `log_queries` setting.\r\nIn new version, you can disable it with the same setting...\r\nThat's consistent with your report :)\r\n\r\n> Does it depend on disk space also?? i mean the table will grow over time and does it get deleted afterwards or something like that?? \r\n\r\nNo, there is no cleanup, it will grow indefinitely.\r\nUsually it is quite small nevertheless and there is no issue.\r\nOtherwise you can manually `TRUNCATE`, `ALTER ... DROP PARTITION` or `ALTER MODIFY TTL` to set automatic cleaning.\r\n\r\n> When does the table flushes??\r\n\r\nThe data is flushed from in-memory buffer to the table every 7 seconds by default."
      },
      {
        "user": "John-belt",
        "created_at": "2020-09-23T06:59:45Z",
        "body": "Yup that makes sense, thanks"
      }
    ]
  },
  {
    "number": 14881,
    "title": "clickhouse-local and table with 10K columns",
    "created_at": "2020-09-16T13:29:04Z",
    "closed_at": "2020-09-16T16:46:08Z",
    "labels": [
      "question",
      "comp-cli",
      "question-answered",
      "comp-local"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14881",
    "body": "need to convert TSV to Native but schema is too big for command-line...",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14881/comments",
    "author": "filimonov",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-16T16:44:50Z",
        "body": "```\r\nmilovidov@milovidov-desktop:~/work/tmp$ cat metadata/local/test.sql\r\nATTACH TABLE test (x UInt64 /* long list here */) ENGINE = File(TSV, stdin);\r\n\r\nmilovidov@milovidov-desktop:~/work/tmp$ ls -lR\r\n.:\r\ndrwxrwxr-x 3 milovidov milovidov 4096 \u0441\u0435\u043d 16 19:42 metadata\r\n\r\n./metadata:\r\ndrwxrwxr-x 2 milovidov milovidov 4096 \u0441\u0435\u043d 16 19:43 local\r\n\r\n./metadata/local:\r\n-rw-r--r-- 1 milovidov milovidov 77 \u0441\u0435\u043d 16 19:43 test.sql\r\n\r\nmilovidov@milovidov-desktop:~/work/tmp$ echo 123 | clickhouse-local --query \"SELECT * FROM local.test\" -- --path=.\r\n123\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-16T16:46:03Z",
        "body": "You can use `clickhouse-local` on top of predefined catalog as in the example above.\r\nIn this catalog, you can have a table with engine File and arbitrary long list of columns."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-16T16:46:44Z",
        "body": "`-- --path=.`\r\n\r\nCan be also specified with `config.xml` in current directory."
      },
      {
        "user": "filimonov",
        "created_at": "2020-09-16T21:02:10Z",
        "body": "Cool! Didn't know that. BTW - it also means clickhouse-local can produce ready to attach parts. \r\n\r\nJust a side note - may be smth like `--queries-file` (as an alternative for --query) is worth adding both for clickhouse-client and clickhouse-local "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-16T22:15:04Z",
        "body": "> it also means clickhouse-local can produce ready to attach parts\r\n\r\nYes.\r\nAlso it can be used for \"maintanence mode\" on server.\r\n\r\n> Just a side note - may be smth like --queries-file (as an alternative for --query) is worth adding both for clickhouse-client and clickhouse-local\r\n\r\nIt's a good feature request, worth doing..."
      }
    ]
  },
  {
    "number": 14830,
    "title": "Cant' execute grant SQL",
    "created_at": "2020-09-15T02:51:44Z",
    "closed_at": "2020-09-15T09:17:45Z",
    "labels": [
      "question",
      "question-answered",
      "comp-rbac"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14830",
    "body": "Clikchouse Version : 20.3.17.173\r\nProblem of RBAC:\r\nWhen i execute the follow SQL, it's oK.\r\n```\r\nCREATE ROLE devgroup;\r\nGRANT SELECT,INSERT ON *.* TO devgroup;\r\n ````\r\n  \r\nBut when I `GRANT SOURCES` ,don't work;\r\n```\r\njyw-centos7-bd04 :) GRANT SOURCES ON *.* TO devgroup;\r\n\r\nSyntax error: failed at position 15:\r\n\r\nGRANT SOURCES ON *.* TO devgroup;\r\n\r\nExpected one of: EXCEPT, Comma, At, TO, token\r\n```\r\n\r\nHow can i grant privileges  about SOURCES? Thanks for you help.\r\n       \r\n  \r\n   ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14830/comments",
    "author": "spihiker",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-09-15T05:45:38Z",
        "body": "> How can i grant privileges about SOURCES?\r\n\r\nUpdate clickhouse version. 20.3 should not be used for RBAC"
      },
      {
        "user": "spihiker",
        "created_at": "2020-09-15T07:18:17Z",
        "body": "When update 20.6 ,work well .thank you ."
      }
    ]
  },
  {
    "number": 14794,
    "title": "About performance between hash join and partial merge join ",
    "created_at": "2020-09-14T09:55:32Z",
    "closed_at": "2020-10-06T10:16:54Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14794",
    "body": "Hi ,\r\nNow we face some join case, and I had test the newest  version about the join algorithm between hash join and partial merge join:\r\n\r\n```\r\nSET join_algorithm = 'partial_merge'\r\nSELECT number * 200000 as n, j FROM numbers(5) nums ANY LEFT JOIN (     SELECT number * 2 AS n, number AS j     FROM numbers(10000000) ) js2 USING n;\r\n\r\nMemoryTracker: Peak memory usage (for query): 457.46 MiB.\r\n5 rows in set. Elapsed: 0.918 sec. Processed 10.02 million rows, 80.18 MB (10.92 million rows/s., 87.39 MB/s.) \r\n\r\nSET join_algorithm = 'hash'\r\nSELECT number * 200000 as n, j FROM numbers(5) nums ANY LEFT JOIN (     SELECT number * 2 AS n, number AS j     FROM numbers(10000000) ) js2 USING n;\r\n\r\nMemoryTracker: Peak memory usage (for query): 845.12 MiB.\r\n5 rows in set. Elapsed: 2.023 sec. Processed 10.02 million rows, 80.18 MB (4.95 million rows/s., 39.63 MB/s.)\r\n```\r\n\r\nSeems the partial merge join has two times better than hash join in respect of memory/time cost at least. And also do some test against business data,  give the same result.\r\n\r\nAfter profiling the hash join, found all the cost from building memory table for right table. But how the partial merge join work? seems no detail doc about this.  Just plan to upgrade to the partial merge join version and want make sure the partial merge join is really good for join case. big thanks.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14794/comments",
    "author": "compasses",
    "comments": [
      {
        "user": "4ertus2",
        "created_at": "2020-09-14T13:10:25Z",
        "body": "I checked 'partial_merge' vs 'hash' algo when it was implemented on data from TPC-H test. It was 3-4 times slower when all data is in memory. So in general it should be slower.\r\n\r\nAny case, It's expected that in some cases (when data in both tables is already sorted by joining key) MergeJoin would be faster than HashJoin. But it needs more code out of join algo itself to make such improvement: we should pass sort info through query pipeline to take it into account. I think you've find a special case that works even without such optimizations.\r\n\r\nWe need to investigate you question and recheck perf test results.\r\n\r\n"
      },
      {
        "user": "compasses",
        "created_at": "2020-09-14T15:34:22Z",
        "body": "@4ertus2 very appreciate for your quick answer, please check my comments \r\n\r\n> I checked 'partial_merge' vs 'hash' algo when it was implemented on data from TPC-H test. It was 3-4 times slower when all data is in memory. So in general it should be slower.\r\n\r\nHow big the TPC-H data-set? I mean build the memory hash table need cost many memory and time. Maybe the data-set not big enough?\r\n\r\n> Any case, It's expected that in some cases (when data in both tables is already sorted by joining key) MergeJoin would be faster than HashJoin. But it needs more code out of join algo itself to make such improvement: we should pass sort info through query pipeline to take it into account. I think you've find a special case that works even without such optimizations.\r\n\r\nI think that's easy. The join table, left or right do have some related each other, and the join key exist in both two tables, \r\nand the order key of the two tables both contains the join key.  So it should be what you said 'data in both tables is already sorted by joining key' ?\r\n\r\n\r\n"
      },
      {
        "user": "4ertus2",
        "created_at": "2020-09-14T16:27:34Z",
        "body": "> How big the TPC-H data-set? I mean build the memory hash table need cost many memory and time. Maybe the data-set not big enough?\r\n\r\nI do not remebmer data-set size exactly. But hash table size was about 10-16 Gb.\r\n\r\n> So it should be what you said 'data in both tables is already sorted by joining key' ?\r\n\r\nLet me describe how does 'partial_merge' join algo works. It's a variant of MergeJoin adapted to ClickHouse query pipeline. ClickHouse streams left table in blocks and join it over full-known right table. It's a way how HashJoin expects join algo (first it builds hash table, second it scans left one). For honest MergeJoin we have to sort both tables and merge sorted results. 'partial_merge' algo do not sort left table, but has build and scan phases as 'hash' one instead. At build phase 'partial_merge' sorts right table by join key in blocks (and in general it's more expensive than just to make hash table). And create min-max index for sorted blocks. At 'scan' phase it sorts parts of left table by join key and merge join them over right table. It's also uses index to skip unneded right table blocks from join.\r\n\r\nSo, 'partial_merge' join could be faster when we could avoid sorts in build and scan phase (when the data is already sorted by joining key). And it would be much more expensive to use 'partial_merge' instead of 'hash' join algo when your left table has some general distribution of join keys cause you have to join every left table part with entire right table and min-max index does not help you in this case.\r\n\r\nTo make benefits of MergeJoin we have to tell it not to sort columns.\r\nIf not we could have profit of merge join at build phase if sorting of blocks is faster then building a hash table. It could happen in joins with a few tight columns in key (when hash table memory alloacations are meaningful).\r\nAlso it's possible to have some benefits in scan phase if min-max index of right table + merge join works faster than hash table lookups. We could have such situation when left table data is near to sorted.\r\nThese are special cases. Make a uniform distribution of inputs and wide string column in joining key and hash join wins."
      },
      {
        "user": "compasses",
        "created_at": "2020-09-15T02:05:48Z",
        "body": "Big thanks, seems more clear now.\r\n\r\nIn consideration of resource cost, the  'partial_merge' algorithm will have more less memory footprint, and I think usually more memory cost always cost more time. And especially under some resource limit environment.\r\n\r\nStart from partial merge algorithm, there are many ways to optimize your SQL or data localization, but the hash join we had nothing to do but need more memory."
      },
      {
        "user": "mateng0915",
        "created_at": "2020-10-15T10:37:42Z",
        "body": "Hello, i have one question, \r\nCan I use partial_merge to avoid the problem of insufficient memory?"
      },
      {
        "user": "zhanglistar",
        "created_at": "2022-03-08T08:45:12Z",
        "body": "> \r\n\r\n@mateng0915 Yes, you can."
      }
    ]
  },
  {
    "number": 14687,
    "title": "can clickhouse-copier copy data to another cluster using different timezone?",
    "created_at": "2020-09-10T09:38:37Z",
    "closed_at": "2020-09-10T15:46:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14687",
    "body": "I don't know much about the migration principle of clickhouse-copier.\r\nI have two clusters A B, A uses Asia/Shanghai time zone, and B uses Etc/UTC.\r\nI used clickhouse-copier to migrate the data and found that the Datetime columns were actually eight hours apart.\r\n\r\nBut if I manually import it in the following way, the data is correct.\r\n```\r\nclickhouse-client -udefault -hB --query=\"select * from db.table1\" --format=CSV> table1.csv\r\nclickhouse-client -u default -hA --database=broker --query=\"INSERT INTO db.table1 FORMAT CSV\" <table1.csv\r\n```\r\nI read the log and thought clickhouse-copier was also insert after select, but the result was problematic. So I want to know the difference between clickhouse-copier's  and \"select to csv then insert\"",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14687/comments",
    "author": "Fanduzi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-10T14:02:00Z",
        "body": "Datetime. The point in time is saved as a Unix timestamp, **regardless of the time zone or daylight saving time**. \r\nAdditionally, the DateTime type can store time zone that is the same for the entire column, that affects how the values of the DateTime type values are **displayed in text format** and how the values specified **as strings are parsed** (\u20182020-01-01 05:00:01\u2019). The time zone is not stored in the rows of the table (or in resultset), but **is stored in the column metadata**.\r\n\r\n\r\nclickhouse-copier copies Datetime value as is. Number of seconds 1599746203 stays as is 1599746203 when you copy data from server A to server B. When you query 1599746203 you see different TZ string representation at A and B.\r\n\r\nWhen you use CSV you convert 1599746203 to a string and parse from a string accordingly the current TZ (for a client/server).\r\n\r\n```\r\n\r\n# TZ=UTC clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n2020-09-10 13:56:43\r\n\r\n# TZ=Asia/Shanghai clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n2020-09-10 21:56:43\r\n```"
      },
      {
        "user": "Fanduzi",
        "created_at": "2020-09-10T14:55:11Z",
        "body": "> Datetime. The point in time is saved as a Unix timestamp, **regardless of the time zone or daylight saving time**.\r\n> Additionally, the DateTime type can store time zone that is the same for the entire column, that affects how the values of the DateTime type values are **displayed in text format** and how the values specified **as strings are parsed** (\u20182020-01-01 05:00:01\u2019). The time zone is not stored in the rows of the table (or in resultset), but **is stored in the column metadata**.\r\n> \r\n> clickhouse-copier copies Datetime value as is. Number of seconds 1599746203 stays as is 1599746203 when you copy data from server A to server B. When you query 1599746203 you see different TZ string representation at A and B.\r\n> \r\n> When you use CSV you convert 1599746203 to a string and parse from a string accordingly the current TZ (for a client/server).\r\n> \r\n> ```\r\n> \r\n> # TZ=UTC clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n> 2020-09-10 13:56:43\r\n> \r\n> # TZ=Asia/Shanghai clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n> 2020-09-10 21:56:43\r\n> ```\r\n\r\nthank you.\r\nDoes this mean I cannot use clickhouse-copier to migrate data between two clusters with different time zone?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-10T15:06:31Z",
        "body": "> \r\n> Does this mean I cannot use clickhouse-copier to migrate data between two clusters with different time zone?\r\n\r\nYou can use clickhouse-copier. You need to configure the client to see results in desired timezone.\r\n\r\nexample : \r\n**TZ=Asia/Shanghai** clickhouse-client -q 'select toDateTime(1599746203)' **--use_client_time_zone=1**"
      },
      {
        "user": "Fanduzi",
        "created_at": "2020-09-10T15:46:29Z",
        "body": "> > Does this mean I cannot use clickhouse-copier to migrate data between two clusters with different time zone?\r\n> \r\n> You can use clickhouse-copier. You need to configure the client to see results in desired timezone.\r\n> \r\n> example :\r\n> **TZ=Asia/Shanghai** clickhouse-client -q 'select toDateTime(1599746203)' **--use_client_time_zone=1**\r\n\r\nThank you very much, I get it"
      }
    ]
  },
  {
    "number": 14543,
    "title": "Dump of create tables sql scripts for database",
    "created_at": "2020-09-07T12:56:13Z",
    "closed_at": "2020-09-07T13:51:14Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14543",
    "body": "Is there a standard way to get a sql script in clickhouse to create all tables in a database, or all tables and databases in general? It would be convenient to have an analogue of pg_dump to get such a script.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14543/comments",
    "author": "horoshenkiy",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2020-09-07T13:36:13Z",
        "body": "`select create_table_query from system.tables where  <your favorite filters> format TSVRaw`"
      },
      {
        "user": "horoshenkiy",
        "created_at": "2020-09-07T13:51:39Z",
        "body": "Done"
      }
    ]
  },
  {
    "number": 14456,
    "title": "toUUID function executed on false IF statement branch on select",
    "created_at": "2020-09-03T20:27:21Z",
    "closed_at": "2020-09-03T22:11:02Z",
    "labels": [
      "question",
      "unexpected behaviour",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14456",
    "body": "\r\n```\r\n\r\n\r\nSELECT if(1 = 2, 'A', 'B')\r\nFORMAT CSV\r\n\r\n\"B\"\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n```\r\n\r\nThe expected result should also be 'B' as above??  but seems to be executing toUUID\r\n```\r\nSELECT if(1 = 2, toUUID('1dd20d3aa81350af566a117a23c80aba2fbf'), NULL)\r\nFORMAT CSV\r\n\r\nReceived exception from server (version 20.7.2):\r\nCode: 6. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse string '1dd20d3aa81350af566a117a23c80aba2fbf' as UUID: syntax error at position 32 (parsed just '1dd20d3aa81350af566a117a23c80aba')\r\n```\r\n\r\nBTW the behavior of toUUID has changed from 20.5 -> 20.6, 20.7 it use to not throw an error\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14456/comments",
    "author": "shawel",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-03T20:50:07Z",
        "body": "Clickhouse  always calculates all branches (if and else) for PERFORMANCE.\r\n\r\nProbably it needs a for new function `toUUIDOrNull`\r\n\r\nBefore 20.7 it worked because\r\n```\r\nSELECT toUUID('1dd20d3aa81350af566a117a23c80aba2fbf')\r\n\r\n\u250c\u2500toUUID('1dd20d3aa81350af566a117a23c80aba2fbf')\u2500\u2510\r\n\u2502           1dd20d3a-8135-af56-a117-23c80aba2fbf \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-03T21:05:07Z",
        "body": "WA:\r\n\r\n\r\n```\r\nselect \r\n        if(1 = 2, toUUID(\r\n             case when  position(x, '-') = 0  then '00000000-0000-0000-0000-000000000000' else x end\r\n        ), NULL) y\r\nfrom (select '1dd20d3aa81350af566a117a23c80aba2fbf' x \r\n       union all select '1dd20d3a-8135-af56-a117-23c80aba2fbf')\r\n\r\n\u250c\u2500\u2500\u2500\u2500y\u2500\u2510\r\n\u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500y\u2500\u2510\r\n\u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "shawel",
        "created_at": "2020-09-03T22:11:02Z",
        "body": "I see counter-intuitive about fast performance in branches.  I will try your method. thanks @den-crane "
      }
    ]
  },
  {
    "number": 14419,
    "title": "No data in replicated table",
    "created_at": "2020-09-02T17:41:29Z",
    "closed_at": "2020-09-03T14:21:46Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14419",
    "body": "I have two replicas. Data are inserted into table in replica A. Table counterpart in replica B is empty though. Data are not replicated between replicas. I tried recreating table in replica B but it did not trigger the sync.\r\n\r\nReplication for other tables is working just fine.\r\n\r\nWhat can I do to debug the issue? `show create table` gives same result for both replicas.\r\n\r\nClickHouse version 20.6",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14419/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-02T17:47:09Z",
        "body": "check (at both nodes) : select * from system.replication_queue \r\n\r\nwrong ZK path? different shards?"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-02T17:50:07Z",
        "body": "I don't have multiple shards.\r\n\r\nreplication queue does not contain any records for the target table\r\n\r\npath is the same (reported by show create table)\r\n\r\n\ud83e\udd14 "
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-02T17:52:25Z",
        "body": "Show `grep tablename clickhouseserver.log`"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-02T17:56:41Z",
        "body": "I tried grepping `cat /var/log/clickhouse-server/clickhouse-server.log` in both replicas but for this particular table contains no logs (it does have some warnings for larger tables though)."
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-02T18:05:33Z",
        "body": "It could be anything. How did you copy/pasted / attached zk path when you created a table?\r\n\r\nFor example these two strings are different. \r\nLatin:    /ccccc/ \r\n\u0421yrrylic: /\u0441\u0441\u0441\u0441\u0441/ \r\n\r\nCheck zookeeper select * from system.zookeeper where path = '/..zk_path/replicas\r\nCheck select * from system.replicas where table = ...\r\n"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-03T13:29:12Z",
        "body": "> select * from system.zookeeper where path = '/..zk_path/replicas\r\nGives result on both replicas. \r\n\r\nBut on the B, there's `replicas\tlast added replica: A` (version: 1)\r\nFor fully replicated tables it's `replicas\tlast added replica: B` (version: 4)\r\n\r\n`system.replicas` says the `total_replicas` is 1. Seems like those tables are disconnected on both replicas. Is there a way to get in in sync somehow?\r\n\r\n> How did you copy/pasted / attached zk path when you created a table?\r\n\r\nI did it a long time ago but It was simple CREATE TABLE ON CLUSTER I believe"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-03T14:18:37Z",
        "body": "Check at both servers that zhash is identical \r\n```\r\n\r\nselect replica_name, total_replicas, active_replicas, zookeeper_path, \r\n           cityHash64(zookeeper_path) zhash, hex(zookeeper_path) \r\nfrom system.replicas\r\nwhere table = '....'\r\n```"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-03T14:21:46Z",
        "body": "Zookeeper paths differ in a small typo :/ Thank you very much for helping me debug this. I have setup a new monitoring query to catch this in the future."
      }
    ]
  },
  {
    "number": 14009,
    "title": "How to set: joined_subquery_requires_alias=0 in config.xml",
    "created_at": "2020-08-24T16:44:56Z",
    "closed_at": "2020-08-24T17:40:55Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14009",
    "body": "We recently upgraded our Clickhouse server and started getting: \r\n\r\n```\r\nDB::Exception: No alias for subquery or table function in JOIN (set joined_subquery_requires_alias=0 to disable restriction).\r\n```\r\n\r\nI can change this setting in the command line client, but I want to change it in the server's config.xml. I've tried putting it under the `<yandex>` tag, and under the default user profile but neither work. Is there some special tagging that needs to be used around this specific setting? Thanks,\r\n\r\nMatt",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14009/comments",
    "author": "mvcalder-xbk",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-08-24T17:39:20Z",
        "body": "Not config.xml\r\nThis parameter must be set in users profile in users.xml\r\n\r\nfor example\r\n```\r\n\r\ncat /etc/clickhouse-server/conf.d/any_join_distinct_right_table_keys.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n    <profiles>\r\n        <default>\r\n            <any_join_distinct_right_table_keys>1</any_join_distinct_right_table_keys>\r\n\t    <joined_subquery_requires_alias>0</joined_subquery_requires_alias>\r\n        </default>\r\n    </profiles>\r\n</yandex>\r\n```"
      },
      {
        "user": "mvcalder-xbk",
        "created_at": "2020-08-24T17:40:55Z",
        "body": "@den-crane thanks. "
      }
    ]
  },
  {
    "number": 13878,
    "title": "How can I using Json-related-format to import multi-level nested Json data?",
    "created_at": "2020-08-19T08:07:01Z",
    "closed_at": "2020-08-20T09:33:46Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13878",
    "body": "Each line of my json data looks like:\r\n```\r\n{\r\n    \"id\": 1, \r\n    \"source\": \"china\",\r\n    \"sentences\":[\r\n          { content:\"I loved apples\",\r\n            words: [ {\"content\": \"I\", \"stem\": \"i\", weight: 5}, \r\n                     {\"content\": \"loved\", \"stem\": \"love\", weight: 10}, \r\n                     {\"content\": \"apples\", \"stem\": \"apple\", weight: 1}]}, \r\n         { content:\"My parents have many apples\",\r\n            words: [ {\"content\": \"My\", \"stem\": \"my\", weight: 6}, \r\n                     {\"content\": \"parentes\", \"stem\": \"parent\", weight: 5}, \r\n                     ......\r\n                     {\"content\": \"apples\", \"stem\": \"apple\", weight: 1}]}\r\n    ]\r\n}\r\n```\r\n\r\n\"sentences\" is an array, and \"words\" is an array too. \r\n\r\n\r\nHow can I load this json data to table with JsonEachRow\uff1f\r\nBecause I want to each domain in my json schema like id, source, sentences.content, sentences.words.content, sentences.words.stem, sentences.words.weight is stored separatly. So it will use the benefits of column storage.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13878/comments",
    "author": "hexiaoting",
    "comments": [
      {
        "user": "hczhcz",
        "created_at": "2020-08-19T08:50:23Z",
        "body": "For complex JSON structures, it is good to import them as strings and use JSON functions (JSONExtract-)."
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-19T08:53:14Z",
        "body": "@hczhcz \r\n> For complex JSON structures, it is good to import them as strings and use JSON functions (JSONExtract-).\r\n\r\nYou mean that take the entire json object as a string?\r\n But in the way, it is stored as a single column and we cannot take advantage of column storage in clickhouse?"
      },
      {
        "user": "hczhcz",
        "created_at": "2020-08-19T09:08:19Z",
        "body": "@hexiaoting \r\nSimply materialize what you extract from json."
      },
      {
        "user": "filimonov",
        "created_at": "2020-08-19T10:11:13Z",
        "body": "Import them as JSONAsString, parse them with JSON functions (can be in MV)"
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-20T03:03:27Z",
        "body": "@hczhcz @filimonov \r\n```\r\nclickhouse-client -q \"create table json_as_string(field String) Engine = Memory\"\r\ncat xxx| clickhouse-client -q \"insert into json_as_string format JSONAsString\"\r\n```\r\nnow all my json data is stored in json_as_string table as a string column. \r\n\r\nBut How can I extract the values of \"sentences.content\"  and \"sentences.words.content\" into another table json_data?\r\n```\r\ncreate table json_data {\r\n    id Int32;\r\n    source String;\r\n    sentences_content Array(String);\r\n    sentences_words_content Array(Array(String));\r\n    sentences_words_stem Array(Array(String));\r\n    sentences_words_weight Array(Array(Int32));\r\n}\r\n```\r\n\r\nselect  JSONExtractRaw(field, 'sentences')) from json_as_string; returns a String type not Array. Do I need to transform the String to Array type???\r\nAnd I tried using \"select JSONExtract(field, 'sentences', Array(String))\", but it  returns null."
      },
      {
        "user": "hczhcz",
        "created_at": "2020-08-20T03:44:04Z",
        "body": "```sql\r\nwith\r\n    '{\r\n        \"id\": 1, \r\n        \"source\": \"china\",\r\n        \"sentences\": [\r\n              { \"content\": \"I loved apples\",\r\n                \"words\": [ {\"content\": \"I\", \"stem\": \"i\", \"weight\": 5}, \r\n                         {\"content\": \"loved\", \"stem\": \"love\", \"weight\": 10}, \r\n                         {\"content\": \"apples\", \"stem\": \"apple\", \"weight\": 1}]}, \r\n             { \"content\": \"My parents have many apples\",\r\n                \"words\": [ {\"content\": \"My\", \"stem\": \"my\", \"weight\": 6}, \r\n                         {\"content\": \"parentes\", \"stem\": \"parent\", \"weight\": 5},\r\n                         {\"content\": \"apples\", \"stem\": \"apple\", \"weight\": 1}]}\r\n        ]\r\n    }' as root,\r\n    JSONExtractArrayRaw(root, 'sentences') as sentences,\r\n    arrayMap(s -> JSONExtractArrayRaw(s, 'words'), sentences) as words\r\nselect\r\n    arrayMap(s -> JSONExtractString(s, 'content'), sentences) as sentences_content,\r\n    arrayMap(s -> arrayMap(w -> JSONExtractString(w, 'content'), s), words) as words_content\r\n```\r\n\r\n```sql\r\ncreate table data (\r\n    root String,\r\n    sentences Array(String) alias JSONExtractArrayRaw(root, 'sentences'),\r\n    sentences_content Array(String) materialized arrayMap(s -> JSONExtractString(s, 'content'), sentences),\r\n    ...\r\n)\r\nengine = ...\r\n```\r\n\r\nFYI."
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-20T04:02:51Z",
        "body": "@hczhcz Thanks a lot . That's what I want. ^^"
      },
      {
        "user": "ramazanpolat",
        "created_at": "2020-08-22T18:39:28Z",
        "body": "Bookmarking this for later review."
      },
      {
        "user": "ramazanpolat",
        "created_at": "2020-08-22T19:37:09Z",
        "body": "@hexiaoting How did you make it work?\r\n\r\nCan you post your DDL's here? \r\n"
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-24T03:28:25Z",
        "body": "@ramazanpolat \r\n```\r\ncreate table json1(\r\n\troot String,\r\n\tid String\r\n\t\tmaterialized JSONExtractString(root, '_id'),\r\n\tsource String\r\n\t\tmaterialized JSONExtractString(root, 'source') ,\r\n\tsentences Array(String)\r\n\t\talias JSONExtractArrayRaw(root, 'sentences'),\r\n\tsentences_content Array(String)\r\n\t\tmaterialized arrayMap(s -> JSONExtractString(s, 'content'), sentences),\r\n\twords Array(Array(String))\r\n\t\tmaterialized arrayMap(s -> JSONExtractArrayRaw(s, 'words'), sentences),\r\n\twords_content Array(Array(String))\r\n\t\tmaterialized arrayMap(s -> (arrayMap(k->JSONExtractString(k, 'content'), s)), words),\r\n\twords_stem Array(Array(String))\r\n\t\tmaterialized arrayMap(s -> (arrayMap(k->JSONExtractString(k, 'stem'), s)), words)\r\n\t) engine = MergeTree order by publish_time;\r\n\r\ncat $file.json | clickhouse-client -q \"insert into json1(root) format JSONAsString \"\r\n```"
      }
    ]
  },
  {
    "number": 13835,
    "title": "ALTER MODIFY ORDER BY does not work",
    "created_at": "2020-08-17T13:12:30Z",
    "closed_at": "2020-08-17T14:51:39Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13835",
    "body": "Hi guys,\r\n\r\nMy clickhouse version is 20.3.10.75. When altering the table order by expression, I got the exception message as follows:\r\n\r\n```\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: Existing column version is used in the expression that was added to the sorting key. You can add expressions that use only the newly added columns.\r\n```\r\n\r\nThe table is defined as follows:\r\n```\r\nCREATE TABLE default.users_online\r\n(\r\n    `when` DateTime,\r\n    `uid` UInt64,\r\n    `duration` UInt64,\r\n    `version` Int32\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\nORDER BY (uid, when)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\nThe ALTER MODIFY ORDER BY command is ```ALTER TABLE users_online MODIFY ORDER BY (uid, when, version)```.\r\n\r\nThe expected behavior is table's order by expression should be modified.\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13835/comments",
    "author": "fastio",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-08-17T13:50:36Z",
        "body": "Code: 36. DB::Exception: Received from localhost:9000. DB::Exception: Existing column version is used in the expression that was added to the sorting key. **You can add expressions that use only the newly added columns.**\r\n\r\n\r\nCREATE TABLE default.users_online\r\n(\r\n    when DateTime,\r\n    uid UInt64,\r\n    duration UInt64\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\nORDER BY (uid, when)\r\nSETTINGS index_granularity = 8192\r\n\r\nOk.\r\n\r\n\r\nALTER TABLE default.users_online  **ADD COLUMN version Int32,**  MODIFY ORDER BY (uid, when, version)\r\n\r\nOk.\r\n\r\n\r\n"
      },
      {
        "user": "fastio",
        "created_at": "2020-08-17T14:08:45Z",
        "body": "@den-crane Thank you for reply. I got it. If the ORDER BY expression modified with existing column,  the existing data of table should be re-sorted which will pay a huge cost."
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-17T14:49:52Z",
        "body": "Yes. \r\n\r\nAnd BTW, `MODIFY ORDER BY` does not change primary index, it changes only rows sorting (for new parts).\r\n\r\n\r\nSHOW CREATE TABLE default.users_online\r\n\r\nCREATE TABLE default.users_online\r\n(\r\n    when DateTime,\r\n    uid UInt64,\r\n    duration UInt64,\r\n    version Int32\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\n**PRIMARY KEY (uid, when)**\r\n**ORDER BY (uid, when, version)**\r\nSETTINGS index_granularity = 8192 \r\n\r\n"
      }
    ]
  },
  {
    "number": 13327,
    "title": "joinGet result invalid.",
    "created_at": "2020-08-04T09:57:59Z",
    "closed_at": "2020-08-07T13:21:55Z",
    "labels": [
      "question",
      "comp-joins",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13327",
    "body": "**Describe the bug**\r\nclickhouse version: 20.6.1.4066\r\n\r\n\r\n* Queries to run that lead to unexpected result\r\n select joinGet('db.T2','id',tid) as nodeId,count(*) from db.T1 where tid='1000' group by nodeId\r\n\uff08db.T2 use storageJoin engine, join type parameter: left)\r\nresult:\r\nnodeId  count(*)\r\n0\t593\r\n43\t70\r\n\r\n**Expected behavior**\r\nexpected result:\r\nnodeId  count(*)\r\n43\t663\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13327/comments",
    "author": "templarzq",
    "comments": [
      {
        "user": "templarzq",
        "created_at": "2020-08-04T09:58:35Z",
        "body": "db.T2  column id type: int32"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-04T19:14:11Z",
        "body": "@templarzq  Do you have reproducible example? \r\nDoes it work before 20.6?\r\nDo you expect that joinGet have to return something instead of 0 in case of `left` ?"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-05T03:41:05Z",
        "body": "it works before version 20.5 (include 20.5.1.1)"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-05T21:18:21Z",
        "body": "OK. And how to reproduce it?\r\n\r\n@templarzq \r\n```\r\n\r\ncreate table T1 Engine=MergeTree order by tuple() as select intDiv(number,1000) tid from numbers(1000000);\r\ncreate table T2 Engine=Join(any, left,tid) as select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\n\r\nselect joinGet('db.T2','id',tid) as nodeId,count() from T1 where tid='333' group by nodeId\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      5 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-06T03:56:48Z",
        "body": "create table xxx on cluster bench_cluster(\r\nxxx\r\n)\r\nENGINE =Join(ANY, LEFT, Id)"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-06T03:57:31Z",
        "body": "maybe the option \"on cluster xxx\"  lead to this result?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-06T04:04:22Z",
        "body": "> maybe the option \"on cluster xxx\" lead to this result?\r\n\r\nI don't see how. \r\nPlease provide reproducible example."
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-06T06:54:54Z",
        "body": "create table T1 on cluster bench_cluster Engine=MergeTree order by tuple() as select intDiv(number,1000) tid from numbers(1000000);\r\ncreate table T2 on cluster bench_cluster(\r\n  id UInt32,\r\n  tid UInt64\r\n) Engine=Join(any, left,tid);\r\ninsert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\ncreate table T3 on cluster bench_cluster as T1   ENGINE  = Distributed(bench_cluster, default,  T1, sipHash64(tid)); \r\n\r\n\r\nselect joinGet('default.T2','id',tid) as nodeId,count(*) from T3 where tid='333' group by nodeId"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-06T06:55:30Z",
        "body": "bench_cluster have more than 1 node."
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-06T13:42:50Z",
        "body": "It's because T2 (Join) is empty on shards (it's not a replicated engine). And joinGet works on shards against empty table.\r\nYou can fill T2 at all nodes with the same data or perform joinGet at the initiator using `from()`.\r\n\r\n```SQL\r\ncreate table T1 on cluster segmented (tid UInt64) Engine=MergeTree order by tuple();\r\ncreate table T3 on cluster segmented as T1 ENGINE = Distributed(segmented, currentDatabase(), T1, sipHash64(tid));\r\n\r\n-- data sharded on cluster\r\ninsert into T3  select intDiv(number,1000) tid from numbers(1000000);\r\n\r\ncreate table T2 on cluster segmented(id UInt32, tid UInt64) Engine=Join(any, left,tid);\r\n\r\n-- data only at current node in Engine=Join\r\ninsert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\n\r\n-- WRONG result\r\nSELECT\r\n    joinGet('default.T2', 'id', tid) AS nodeId,\r\n    count(*)\r\nFROM T3\r\nWHERE tid = 333\r\nGROUP BY nodeId\r\n\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      0 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n-- RIGTH result - joinGet is executed at the current node only\r\nSELECT\r\n    joinGet('default.T2', 'id', tid) AS nodeId,\r\n    count(*)\r\nFROM\r\n(\r\n    SELECT tid\r\n    FROM T3\r\n    WHERE tid = 333\r\n)\r\nGROUP BY nodeId\r\n\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      5 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n-- RIGTH result - joinGet is executed at all nodes but it has the same data at all nodes.\r\n-- execute at all nodes\r\nnode2: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\nnode3: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\nnode4: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\nnode5: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\n\r\nSELECT\r\n    joinGet('default.T2', 'id', tid) AS nodeId,\r\n    count(*)\r\nFROM T3\r\nWHERE tid = 333\r\nGROUP BY nodeId\r\n\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      5 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\nClickHouse server version 19.13.7\r\n"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-07T02:00:57Z",
        "body": "ok,thanks."
      }
    ]
  },
  {
    "number": 13256,
    "title": "\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 CREATE USER",
    "created_at": "2020-08-03T04:05:49Z",
    "closed_at": "2020-08-03T17:11:17Z",
    "labels": [
      "question",
      "question-answered",
      "comp-rbac"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13256",
    "body": "ClickHouse server version 20.5.4 revision 54435\r\n\r\n<access_management>1</access_management>\r\n\r\nCode: 514. DB::Exception: Received from localhost:9000. DB::Exception: Not found a storage to insert user",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13256/comments",
    "author": "handgunman",
    "comments": [
      {
        "user": "handgunman",
        "created_at": "2020-08-03T09:50:57Z",
        "body": "\u0432 \u0442\u0440\u0430\u043a\u0435\u0440\u043e\u0432\u043a\u0435 \u0432\u0440\u043e\u0434\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0433\u043e\r\nCode: 514. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Not found a storage to insert user `okraina`. Stack trace:\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x11b9acc0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x9f3e2cd in /usr/bin/clickhouse\r\n2. ? @ 0xe508201 in /usr/bin/clickhouse\r\n3. ? @ 0xe5000c8 in /usr/bin/clickhouse\r\n4. DB::IAccessStorage::insert(std::__1::vector<std::__1::shared_ptr<DB::IAccessEntity const>, std::__1::allocator<std::__1::shared_ptr<DB::IAccessEntity const> > > const&) @ 0xe500e9f in /usr/bin/clickhouse\r\n5. DB::InterpreterCreateUserQuery::execute() @ 0xe9cd24d in /usr/bin/clickhouse\r\n6. ? @ 0xed3c7ed in /usr/bin/clickhouse\r\n7. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xed3fe2a in /usr/bin/clickhouse\r\n8. DB::TCPHandler::runImpl() @ 0xf36443c in /usr/bin/clickhouse\r\n9. DB::TCPHandler::run() @ 0xf365190 in /usr/bin/clickhouse\r\n10. Poco::Net::TCPServerConnection::start() @ 0x11ab8aeb in /usr/bin/clickhouse\r\n11. Poco::Net::TCPServerDispatcher::run() @ 0x11ab8f7b in /usr/bin/clickhouse\r\n12. Poco::PooledThread::run() @ 0x11c37aa6 in /usr/bin/clickhouse\r\n13. Poco::ThreadImpl::runnableEntry(void*) @ 0x11c32ea0 in /usr/bin/clickhouse\r\n14. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n15. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-03T17:09:31Z",
        "body": "Check that you have configured  access_control_path in config.xml\r\n\r\n```\r\n    <!-- Path to folder where users and roles created by SQL commands are stored. -->\r\n    <access_control_path>/var/lib/clickhouse/access/</access_control_path>\r\n```"
      },
      {
        "user": "handgunman",
        "created_at": "2020-08-03T17:11:17Z",
        "body": "\u0421\u043f\u0430\u0441\u0438\u0431\u043e! \u0415\u0433\u043e \u043d\u0435 \u0431\u044b\u043b\u043e \u0432 \u0441\u0442\u0430\u0440\u043e\u043c \u043a\u043e\u043d\u0444\u0438\u0433\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b, \u043f\u043e\u0442\u043e\u043f\u0443 \u043f\u0440\u043e\u0433\u043b\u044f\u0434\u0435\u043b."
      },
      {
        "user": "draev",
        "created_at": "2020-11-06T09:59:27Z",
        "body": "> Check that you have configured access_control_path in config.xml\r\n> \r\n> ```\r\n>     <!-- Path to folder where users and roles created by SQL commands are stored. -->\r\n>     <access_control_path>/var/lib/clickhouse/access/</access_control_path>\r\n> ```\r\n\r\n\u0410 \u0421\u0435\u0440\u0432\u0435\u0440 \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0442\u044c \u043f\u043e\u0441\u043b\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043a\u043e\u043d\u0444\u0438\u0433\u0430 ?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-11-06T11:43:51Z",
        "body": "\u0421\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e \u0434\u0430."
      }
    ]
  },
  {
    "number": 13195,
    "title": "the equal function like MySQL's group_concat",
    "created_at": "2020-07-31T17:26:58Z",
    "closed_at": "2020-08-01T14:51:47Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13195",
    "body": " is there a function as MySQL's group_concat?\r\nthe example show in MySQL:\r\n>create table kpi(emp_no varchar(8),performance varchar(32),month varchar(32));\r\n>insert into kpi values (10,'A','2020-01'),(10,'A','2020-02'),(10,'C','2020-03'),(10,'B','2020-04'),(10,'A','2020-05'),(10,'A','2020-06');\r\n>insert into kpi values (20,'A','2020-01'),(20,'B','2020-02'),(20,'C','2020-03'),(20,'C','2020-04'),(20,'A','2020-05'),(20,'D','2020-06'); \r\n>insert into kpi values (30,'C','2020-03'),(30,'C','2020-04'),(30,'B','2020-05'),(30,'B','2020-06');\r\n\r\n>mysql> select emp_no,group_concat(performance order by month separator '-') kpi_list,group_concat(distinct performance order by month separator '-') kpi_uniq,group_concat(distinct performance order by month desc separator '-') kpi_uniq_desc from kpi group by emp_no;  \r\n>+--------+-------------+----------+---------------+\r\n| emp_no | kpi_list    | kpi_uniq | kpi_uniq_desc |\r\n+--------+-------------+----------+---------------+\r\n| 10     | A-A-C-B-A-A | A-C-B    | B-C-A         |\r\n| 20     | A-B-C-C-A-D | A-B-C-D  | D-C-B-A       |\r\n| 30     | C-C-B-B     | C-B      | B-C           |\r\n+--------+-------------+----------+---------------+\r\n3 rows in set (0.00 sec)\r\n\r\nBy the way i want to get each user's the count of performance level ,for examle emp_no=10 have A 4 times\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13195/comments",
    "author": "vkingnew",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-07-31T20:40:48Z",
        "body": "There is `groupArray` aggregate function, it returns array of all values.\r\nThere is `groupUniqArray` that returns array of all distinct values.\r\n\r\nAnd you can convert resulting array to string with `arrayStringConcat` function."
      },
      {
        "user": "vkingnew",
        "created_at": "2020-08-01T06:25:16Z",
        "body": "ok by your tips,i get it.\r\n>SELECT \r\n    emp_no,\r\n    groupArray(performance) AS kpi_asc,\r\n    arrayStringConcat(kpi_asc, '-') AS kpi_list,\r\n    groupUniqArray(performance) AS kpi_uniq,\r\n    countEqual(kpi_asc, 'A') AS A_cnt,\r\n    countEqual(kpi_asc, 'B') AS B_cnt,\r\n    countEqual(kpi_asc, 'C') AS C_cnt,\r\n    countEqual(kpi_asc, 'D') AS D_cnt\r\nFROM kpi\r\nGROUP BY emp_no\r\nORDER BY emp_no ASC\r\n>\u250c\u2500emp_no\u2500\u252c\u2500kpi_asc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500kpi_list\u2500\u2500\u2500\u2500\u252c\u2500kpi_uniq\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500A_cnt\u2500\u252c\u2500B_cnt\u2500\u252c\u2500C_cnt\u2500\u252c\u2500D_cnt\u2500\u2510\r\n\u2502 10     \u2502 ['A','A','C','B','A','A'] \u2502 A-A-C-B-A-A \u2502 ['B','A','C']     \u2502     4 \u2502     1 \u2502     1 \u2502     0 \u2502\r\n\u2502 20     \u2502 ['A','B','C','C','A','D'] \u2502 A-B-C-C-A-D \u2502 ['B','A','D','C'] \u2502     2 \u2502     1 \u2502     2 \u2502     1 \u2502\r\n\u2502 30     \u2502 ['C','C','B','B']         \u2502 C-C-B-B     \u2502 ['B','C']         \u2502     0 \u2502     2 \u2502     2 \u2502     0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 0.004 sec. "
      },
      {
        "user": "fzhedu",
        "created_at": "2021-05-18T14:06:58Z",
        "body": "`groupUniqArray ` and `grougArray ` just accept only only expression, but `group_concat(x,x,x...)` could accept more expressions and order by. So how Clickhouse compeletly support `group_concat(x,x,x...)` of mysql?"
      }
    ]
  },
  {
    "number": 13109,
    "title": "How does concurrency control work?",
    "created_at": "2020-07-30T13:43:56Z",
    "closed_at": "2020-07-30T14:12:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13109",
    "body": "Hi. I'd like to know how **ClickHouse** ensures *concurrency control* at table level. For example, if there's many applications writing in the same table at the same time, won't data get corrupted?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13109/comments",
    "author": "zergon321",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-07-30T14:07:40Z",
        "body": "MergeTree tables are represented by a set of immutable data parts. Data parts are never modified, only created and deleted. INSERTs create new data parts. SELECTs take snapshot of required data parts and hold the snapshot during the query.\r\nSnapshots are based on reference counting and snapshot acquire operation is very cheap.\r\nBoth SELECTs and INSERTs and even ALTERs can be executed concurrently.\r\n\r\nData parts are merged in background - a new larger data part is created as a result of merge, then source data parts are deleted.\r\nWhen data needs to be modified (ALTER query), we perform copy-on-write - create another part and remove source part.\r\nVersion number and block number are tracked for each part.\r\n\r\nWhen we write new data part, we create temporary data and then perform atomic commit (the data is appeared in a table atomically). The user cannot see partially written data part and no data corruption possible.\r\n\r\nTLDR, it's MVCC.\r\n\r\nPS. Other (rudimentary) table engines like Log, TinyLog, StripeLog are using simple read-write lock.\r\nMemory table engine is using snapshot like MergeTree."
      },
      {
        "user": "zergon321",
        "created_at": "2020-07-30T14:12:17Z",
        "body": "Thank you very much for the answer!"
      }
    ]
  },
  {
    "number": 13066,
    "title": "What does the tailing number of system.query_log_N mean?",
    "created_at": "2020-07-29T12:28:12Z",
    "closed_at": "2020-07-30T02:53:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13066",
    "body": "ClickHouse is an excellent OLAP database\r\n\r\nWhile I'm looking for the slow queries, I found many `query_log_N` and `query_thread_log_N` tables.\r\n\r\n```SQL\r\n\r\nSHOW TABLES\r\n\r\n...\r\n\u2502 query_log                      \u2502\r\n\u2502 query_log_0                    \u2502\r\n\u2502 query_log_1                    \u2502\r\n\u2502 query_log_2                    \u2502\r\n\u2502 query_log_3                    \u2502\r\n\u2502 query_log_4                    \u2502\r\n\u2502 query_thread_log               \u2502\r\n\u2502 query_thread_log_0             \u2502\r\n\u2502 query_thread_log_1             \u2502\r\n\u2502 query_thread_log_2             \u2502\r\n\u2502 query_thread_log_3             \u2502\r\n\u2502 query_thread_log_4             \u2502\r\n...\r\n```\r\n\r\nI am developing a `monitoring system for slow queries` and considered to use the query_log table,\r\n\r\nbut there are other tables with the same postfix of names.\r\n\r\nI wonder what the tailing numbers of tables mean.\r\n\r\nThank you.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13066/comments",
    "author": "achimbab",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-07-30T02:41:23Z",
        "body": "It's renamed after each CH upgrade which changes table's schema"
      },
      {
        "user": "achimbab",
        "created_at": "2020-07-30T06:37:25Z",
        "body": "@den-crane \r\nThank you for your answer."
      }
    ]
  },
  {
    "number": 12750,
    "title": "Exception with INSERT INTO ",
    "created_at": "2020-07-24T13:12:44Z",
    "closed_at": "2020-07-24T19:07:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12750",
    "body": "Hello !\r\n\r\n**Describe the bug**\r\nI try to use INSERT INTO  a table (not a view !) using the AggregatingMergeTree() engine, or in a Distributed table created \"on\" the table using the AggregatingMergeTree ; in both case I have the following exception : \r\n> Code: 20. DB::Exception: Received from localhost:9000. DB::Exception: Number of columns doesn't match.\r\n\r\n**How to reproduce**\r\nClickHouse server version 20.5.2 revision 54435\r\nClickHouse client version 20.5.2.7 (official build)\r\n\r\nI didn't change the setting,\r\n\r\n1>  I have created a Distributed table \"visits_distributed_v2\", and filled it.\r\n2> I have created the \"all_visitor\" table :\r\n```\r\nCREATE TABLE poc.all_visitor\r\n(\r\n    `VisitorCode` String,\r\n    `arrayVisitDuration` AggregateFunction(groupArray, Int64)\r\n)\r\nENGINE = AggregatingMergeTree()\r\nPARTITION BY VisitorCode\r\nORDER BY VisitorCode\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\n3> I have created the Distributed table all_visitor_distributed :\r\n```\r\nCREATE TABLE poc.all_visitor_distributed AS poc.all_visitor\r\nENGINE = Distributed(test_shard_localhost, poc, all_visitor, rand())\r\n```\r\n\r\n4> When I try to insert data from the \"visits_distributed_v2\" table in the Distributed Table or the \"source\" one, I have the issue.\r\n```\r\nINSERT INTO poc.all_visitor_distributed SELECT (VisitorCode, groupArrayState(VisitDuration))\r\nFROM poc.visits_distributed_v2\r\nGROUP BY VisitorCode\r\n```\r\n\r\n**Expected behavior**\r\nIt could totally by an error on my side, I already checked the name of the columns or their types, the number of columns seems to match but the error is misleading that's why I need help :)\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12750/comments",
    "author": "RonanMorgan",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-07-24T15:55:21Z",
        "body": ">SELECT (VisitorCode, groupArrayState(VisitDuration))\r\n\r\nbecause of ( )\r\n\r\ntry `SELECT VisitorCode, groupArrayState(VisitDuration)`\r\n\r\n( ) -- makes a special type - Tuple, syntax sugar for a  function -- tuple\r\n```\r\n\r\nselect 1, 2, tuple(1,2), (1,2)\r\n\u250c\u25001\u2500\u252c\u25002\u2500\u252c\u2500tuple(1, 2)\u2500\u252c\u2500tuple(1, 2)\u2500\u2510\r\n\u2502 1 \u2502 2 \u2502 (1,2)       \u2502 (1,2)       \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ndesc(select 1, 2, tuple(1,2) y, (1,2) x)\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\r\n\u2502 1    \u2502 UInt8               \u2502\r\n\u2502 2    \u2502 UInt8               \u2502\r\n\u2502 y    \u2502 Tuple(UInt8, UInt8) \u2502\r\n\u2502 x    \u2502 Tuple(UInt8, UInt8) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\r\n```"
      },
      {
        "user": "RonanMorgan",
        "created_at": "2020-07-24T19:07:41Z",
        "body": "Thank you very much !"
      }
    ]
  },
  {
    "number": 12694,
    "title": "how to kill optimize FINAL  in backgroud",
    "created_at": "2020-07-22T22:56:03Z",
    "closed_at": "2020-07-27T11:56:29Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12694",
    "body": "i optimize a big table (replicedReplacingMergeTree)   , it makes io  100%   ,so i want to kill this optimize FINAL  \r\nhow can i do ?\r\n\r\n<Debug> default.users (ReplicatedMergeTreeQueue): Not executing log entry MERGE_PARTS for part 20200630_0_307423_23 because source parts size (67.06 GiB) is greater than the current maximum (7.58 GiB).\r\n\r\nthanks advance",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12694/comments",
    "author": "zhlovy",
    "comments": [
      {
        "user": "zhlovy",
        "created_at": "2020-07-22T23:07:13Z",
        "body": "select *  FROM system.processes  ;   the result is null   ,  so can`t kill it"
      },
      {
        "user": "alesapin",
        "created_at": "2020-07-23T08:37:53Z",
        "body": "**For normal MergeTree:**\r\nThe only way is to call `SYSTEM STOP MERGES table_name`. **All merges** will be aborted, after that, you should start them again with `SYSTEM START MERGES table_name`. After that normal merges will start again, without FINAL merges from OPTIMIZE. \r\n\r\n**For ReplicatedMergeTree:**\r\nIt's not possible, because merges were written in the zookeeper replication log. You can just tune the amount of resources for background merges with settings:\r\n```\r\nbackground_pool_size\r\nnumber_of_free_entries_in_pool_to_lower_max_size_of_merge\r\nmax_replicated_merges_in_queue\r\n```"
      },
      {
        "user": "zhlovy",
        "created_at": "2020-07-24T01:41:14Z",
        "body": "thanks ~ \r\nThanks a lot for the trouble you've taken to help me! "
      }
    ]
  },
  {
    "number": 12565,
    "title": "Support configuration hot reload of merge_tree_settings?",
    "created_at": "2020-07-17T14:57:10Z",
    "closed_at": "2020-07-17T15:35:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12565",
    "body": "Hi team:\r\n    I found there no hot reload ability of merge_tree_settings like max_parts_in_total, parts_to_delay_insert and so on. Are there any ways to implement hot reload?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12565/comments",
    "author": "kekekedeng",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-07-17T15:03:04Z",
        "body": "you can apply it to the table \r\n```\r\nalter  table foobar modify setting max_parts_in_total  = 6000;\r\n\r\n```"
      },
      {
        "user": "kekekedeng",
        "created_at": "2020-07-17T15:35:59Z",
        "body": "@den-crane It works, thanks!"
      }
    ]
  },
  {
    "number": 11888,
    "title": "Multithreading reading the same FD problem.",
    "created_at": "2020-06-23T12:54:56Z",
    "closed_at": "2020-06-24T00:19:03Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/11888",
    "body": "```seek``` and ```read``` are not atomic operations.I didn't see the lock.Will there be concurrency issues?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/11888/comments",
    "author": "nicelulu",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-06-23T17:22:44Z",
        "body": "We don't read from one fd from multiple threads.\r\nIf we read from multiple threads, file is opened multiple times."
      },
      {
        "user": "nicelulu",
        "created_at": "2020-06-24T01:46:33Z",
        "body": "> We don't read from one fd from multiple threads.\r\n> If we read from multiple threads, file is opened multiple times.\r\n\r\nThanks for your reply, i get it."
      }
    ]
  },
  {
    "number": 11396,
    "title": "Does CK support sharding_key policy that write to all shards?",
    "created_at": "2020-06-03T03:11:47Z",
    "closed_at": "2020-06-04T18:37:22Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/11396",
    "body": "Hey guys, I'm new to CK. So now I want to build a distributed table with a sharding_key policy that when I write data to the distributed table, the data will be sent to all shards. \r\n\r\nI know CK now only supports rand() or built-in hash functions or UserID. Btw, does it mean that each time the data will only be sent to one shard?\r\n ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/11396/comments",
    "author": "AlexanderChiuluvB",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-06-03T03:33:36Z",
        "body": "Do you need data distributed among all nodes or duplicated to all nodes?\r\n\r\nFor example you have 2 servers and inserted 10 rows. Do you need 5 rows at the first node and another 5 at the second node? Or do you need data duplicated? Both nodes have all 10 rows?"
      },
      {
        "user": "AlexanderChiuluvB",
        "created_at": "2020-06-03T04:56:49Z",
        "body": "> Do you need data distributed among all nodes or duplicated to all nodes?\r\n> For example you have 2 servers and inserted 10 rows. Do you need 5 rows at the first node and another 5 at the second node? Or do you need data duplicated? Both nodes have all 10 rows?\r\n\r\nBoth nodes have all 10 rows"
      },
      {
        "user": "AlexanderChiuluvB",
        "created_at": "2020-06-03T05:03:03Z",
        "body": "> Do you need data distributed among all nodes or duplicated to all nodes?\r\n> For example you have 2 servers and inserted 10 rows. Do you need 5 rows at the first node and another 5 at the second node? Or do you need data duplicated? Both nodes have all 10 rows?\r\n> \r\n> Both nodes have all 10 rows\r\n\r\nActually I want to implement dimenstion table in CK. Suppose I have many servers, and I want to create a local small dimension table in each servers. So my idea is that I can create a distributed table, and then when I want to write data to the dimenstion table, I can directly write to the distributed table and  the data will be sent to all the local dimenstion table in each servers. The problem is how do I define the sharding_key policy?"
      },
      {
        "user": "YiuRULE",
        "created_at": "2020-06-03T07:00:07Z",
        "body": "If you want to set the same record on all of your server, why not just set a replicated table on each of your server ?"
      },
      {
        "user": "AlexanderChiuluvB",
        "created_at": "2020-06-03T08:06:51Z",
        "body": "> If you want to set the same record on all of your server, why not just set a replicated table on each of your server ?\r\n\r\nBecause replica table can only be replicated between replicas of the same shard, but can not be replicated between diffetent shards. \r\n\r\nI want that the small dimension table can be replicated between all the shards nodes, then I have a super big fact table(it's sharded) want to join the dimension table.So all the shards of the big fact table can join the complete small dimension table on each shard node.\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-06-03T19:28:59Z",
        "body": "@AlexanderChiuluvB Do you use Replicated Engine or plain MergeTree ?\r\n\r\nIf you don't use Replicated you need to describe one more cluster with with one shard many replicas.\r\n\r\nIf you use Replicated table then just omit {shard} macros in ZK table path and don't use Distributed table."
      },
      {
        "user": "AlexanderChiuluvB",
        "created_at": "2020-06-04T06:43:56Z",
        "body": "> @AlexanderChiuluvB Do you use Replicated Engine or plain MergeTree ?\r\n> If you don't use Replicated you need to describe one more cluster with with one shard many replicas.\r\n> If you use Replicated table then just omit {shard} macros in ZK table path and don't use Distributed table.\r\n\r\nalright thanks~"
      }
    ]
  },
  {
    "number": 11062,
    "title": "DB::Exception: Cannot get value from Function.",
    "created_at": "2020-05-20T06:18:31Z",
    "closed_at": "2020-05-20T07:28:36Z",
    "labels": [
      "question",
      "st-fixed",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/11062",
    "body": "Hello, are you okay with COVID-19?\r\n\r\nI am using ClickHouse very well, but I got an exception.\r\n\r\nIn the below, I wrote queries to run that lead to unexpected result.\r\n\r\nThank you.\r\n\r\n**Exception**\r\n```\r\nCode: 48. DB::Exception: Received from localhost:19000. DB::Exception: Cannot get value from Function.\r\n```\r\n\r\n**Server Version**\r\n```20.3.5.21```\r\n\r\n**Qeuries for Reproduction**\r\n```SQL\r\nCREATE TABLE test_ga\r\n(\r\n    `d` Date,\r\n    `id` UInt64,\r\n    `duration` UInt64,\r\n    `gender` String\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY d\r\nORDER BY id;\r\n\r\nINSERT INTO test_ga SELECT\r\n    number % 100,\r\n    number,\r\n    number * 1000,\r\n    if((number % 2) = 0, 'm', 'f')\r\nFROM system.numbers\r\nLIMIT 11111110;\r\n```\r\n\r\n```SQL\r\nWITH\r\n    groupArrayIf(duration / 1000, (duration > 0) AND (duration < ((1000 * 3600) * 6))) AS avg_duration_group_array,\r\n    round(medianExactIf(duration, duration > 0) / 1000) AS median_duration,\r\n    round(arrayReduce('sum', arrayFilter(x -> (x < (arrayReduce('avg', avg_duration_group_array) + (2 * arrayReduce('stddevPop', avg_duration_group_array)))), avg_duration_group_array)) / 60) AS avg_duration\r\nSELECT\r\n    median_duration,\r\n    avg_duration\r\nFROM test_ga\r\nGROUP BY gender\r\nORDER BY median_duration DESC\r\nLIMIT 100;\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/11062/comments",
    "author": "achimbab",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-20T07:21:11Z",
        "body": "@achimbab We're alright.\r\n\r\n> 20.3.5.21\r\n\r\nWe have already released four bugfixes after this version, could you please check 20.3.9?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-20T07:27:51Z",
        "body": "I have checked master version and the issue does not reproduce."
      },
      {
        "user": "achimbab",
        "created_at": "2020-05-20T08:39:35Z",
        "body": "@alexey-milovidov \r\nThank you very much.!"
      }
    ]
  },
  {
    "number": 11056,
    "title": "Get the value for the previous point of time. IOT case",
    "created_at": "2020-05-20T01:21:26Z",
    "closed_at": "2020-05-20T13:42:13Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/11056",
    "body": "I have the table with keys (time, facility). For each key, the (value) received from the sensor is stored. For instance:\r\n\r\n\u250c\u2500date\u2500\u2500\u2500\u2500\u252cfacility\u252cvalue\u252c\r\n\u2502 2017-09-09 \u2502 10002\u2502 10  \u2502\r\n\u2502 2017-09-10 \u2502 10001\u2502 12      \u2502\r\n\u2502 2017-09-12 \u2502 10002\u2502 15      \u2502\r\n\u2502 2017-09-15 \u2502 10001\u2502 17      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2518\r\n\r\nI want to calculate an increase in the current value compared to the previous one. Something like this:\r\n\r\n\u250c\u2500date\u2500\u2500\u2500\u2500\u252cfacility\u252c\u2500value\u252c\r\n\u2502 2017-09-12 \u2502 10002\u2502 15/10 \u2502 // 15 current, 10 previous for facility 10002\r\n\u2502 2017-09-15 \u2502 10001\u2502 17/12 \u2502 // 17 current, 12 previous for facility 10002\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2518\r\n\r\nTo get this result, I need to use JOIN, but as I understood from the documentation, the comparison condition in JOIN operation is only for equality, and inequalities cannot be used.\r\n\r\nOf course, I can do a Cartesian product, then do a date comparison (get all dates less than the current one) in the WHERE section and then select the maximum date from the filtered ones. But it is very time and memory consuming!\r\n\r\nPlease help me create an optimal query, because this case is typical for IOT",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/11056/comments",
    "author": "Shanginre",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-05-20T02:15:43Z",
        "body": "There is no good solution, this task is not typical for OLAP.\r\n\r\n```\r\nselect groupArray(date), facility, groupArray(value) from \r\n  (select date,  facility, value from \r\n         (select '2017-09-09' date, 10002 facility, 10 value union all\r\n          select '2017-09-10' , 10001, 12 union all\r\n          select '2017-09-12' , 10002, 15 union all\r\n          select '2017-09-15' , 10001, 17 ) \r\n   order by facility, date desc)\r\ngroup by facility\r\n\r\n\u250c\u2500groupArray(date)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500facility\u2500\u252c\u2500groupArray(value)\u2500\u2510\r\n\u2502 ['2017-09-15','2017-09-10'] \u2502    10001 \u2502 [17,12]           \u2502\r\n\u2502 ['2017-09-12','2017-09-09'] \u2502    10002 \u2502 [15,10]           \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nselect groupArray(1)(date)[1], facility, groupArray(2)(value) from \r\n  (select date,  facility, value from \r\n         (select '2017-09-09' date, 10002 facility, 10 value union all\r\n          select '2017-09-10' , 10001, 12 union all\r\n          select '2017-09-12' , 10002, 15 union all\r\n          select '2017-09-15' , 10001, 17 ) \r\n   order by facility, date desc)\r\ngroup by facility\r\n\u250c\u2500arrayElement(groupArray(1)(date), 1)\u2500\u252c\u2500facility\u2500\u252c\u2500groupArray(2)(value)\u2500\u2510\r\n\u2502 2017-09-15                           \u2502    10001 \u2502 [17,12]              \u2502\r\n\u2502 2017-09-12                           \u2502    10002 \u2502 [15,10]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "Shanginre",
        "created_at": "2020-05-20T09:55:19Z",
        "body": "thanks for the answer. I'm used to working with relational databases, this is a common case there. Please tell me in which DBMS such a query will be executed quickly for 100 million rows? Cassandra, InfluxDB, ....? I am now choosing a DBMS for IOT project."
      },
      {
        "user": "den-crane",
        "created_at": "2020-05-20T13:42:13Z",
        "body": "Yes, Cassandra, InfluxDB much better for such queries. "
      }
    ]
  },
  {
    "number": 10891,
    "title": "How to create table with MATERIALIZED column as select from",
    "created_at": "2020-05-13T14:39:34Z",
    "closed_at": "2020-05-13T14:53:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10891",
    "body": "Please? Help me.\r\n\r\nI want to know how create table with MATERIALIZED column from another select with input variables.\r\nI do next:\r\n\r\nCREATE TABLE ANALYST.DATA_TEST1 (\r\n `ids` UInt16,\r\n `timestamp` String,\r\n `code` UInt64,\r\n `id` UUID MATERIALIZED generateUUIDv4(),\r\n `timemoment` DateTime MATERIALIZED CAST(formatDateTime(toDateTime(substring(timestamp, 1, 19)), '%Y-%m-%d %T'), 'DateTime'),\r\n `adate` Date MATERIALIZED toDate(timemoment),\r\n `idate` Date MATERIALIZED toDate(now()),\r\n `moduleid` UInt16 MATERIALIZED (SELECT if((SELECT uniqExact(ANALYST.HOME.moduleid) FROM ANALYST.HOME WHERE ANALYST.HOME.id_soa = ids) > 1, 379, 339))\r\n) ENGINE = MergeTree(adate, (moduleid, timemoment, code), 8192);\r\n\r\nbut execution finshed with error DB::Exception: Missing columns: 'ids' while processing query: 'SELECT uniqExact(ANALYST.HOME.moduleid) FROM ANALYST.HOME WHERE id_soa = ids', required columns: 'ids' 'moduleid' 'id_soa', source columns: 'moduleid' 'description' 'host_min' 'id_soa' 'network' 'host_max'\r\n\r\nMaybe I'm wrong and it's impossible!? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10891/comments",
    "author": "xap9i",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-05-13T14:50:54Z",
        "body": "It's impossible. You can use `select from` in MATERIALIZED column. You can use only `dictGet<Type>`. "
      },
      {
        "user": "xap9i",
        "created_at": "2020-05-13T14:53:04Z",
        "body": "Sorry, thanks!"
      }
    ]
  },
  {
    "number": 10883,
    "title": "Multiple clusters, same servers",
    "created_at": "2020-05-13T10:26:06Z",
    "closed_at": "2020-05-15T05:00:59Z",
    "labels": [
      "question",
      "comp-distributed"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10883",
    "body": "I have  a distributed table with replication (using zookeeper).\r\n\r\n```\r\n\r\ncreate table s_actions (...)\r\nPARTITION BY toMonday(createdon)\r\nORDER BY\r\n  (createdon, user__id) SAMPLE BY user__id SETTINGS index_granularity = 8192\r\n \r\n```\r\n\r\nI have primary replicas with 3 servers with a lot of memory and cpu . Second replica  has slow ssd ,less cpu and ram and is used for replication backup and then daily backups (FREEZE PARTITION).\r\n\r\nI have distributed table like\r\n\r\n````\r\n CREATE TABLE actions (\r\n....\r\n) ENGINE = Distributed(\r\n  rep,\r\n  actions,\r\n  s_actions,\r\n  cityHash64(toString(user__id))\r\n)\r\n\r\n```` \r\nrep cluster has only one replica for each shard. So If any server from primary replica fails everything will be broken. I want to create rep_write cluster in clickhouse config with secondary replicas to allow writes to secondary or primary replicas . Reads are not needed to be protected. \r\n\r\nProblem is that I'm using hashing function instead of random to optimize performance. Is it safe to define separate clusters with same (by order) servers (with extra replicas) and use distributed tables with same hashing function? \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10883/comments",
    "author": "thyn",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-05-13T13:44:15Z",
        "body": "Yes. It is safe. If you make a new cluster with the same shards/servers order it will have the same shard numbers. \r\nYou can verify it in `select * from system.clusters`"
      },
      {
        "user": "thyn",
        "created_at": "2020-05-15T05:00:59Z",
        "body": "It's great, thank you"
      }
    ]
  },
  {
    "number": 10673,
    "title": "UInt16\" is greater than max allowed Date value",
    "created_at": "2020-05-05T09:39:38Z",
    "closed_at": "2020-05-05T14:08:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10673",
    "body": "Hi, can someone quickly clarify what error is this?\r\n\r\nI tried loading Parquet file into table i have not used UInt16 data type anywhere in the table but getting the below error\r\n\r\nERROR:  Input value 4294941729 of a column \"UInt16\" is greater than max allowed Date value, which is 49710",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10673/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-05T12:06:05Z",
        "body": "It looks like you are importing some column from Parquet into Date type in ClickHouse.\r\nDate type is represented by UInt16 containing the number of days since 1970-01-01.\r\nBut in Parquet file you have some big number (4294941729) or negative number (-25566) instead."
      },
      {
        "user": "Crazylearner30",
        "created_at": "2020-05-05T14:08:28Z",
        "body": "Yes!! we were using a wrong date 1900-01-01.\r\n\r\nWe changed it and its working fine."
      }
    ]
  },
  {
    "number": 10252,
    "title": "Partitions have  active and is_frozen  both flags at the same time",
    "created_at": "2020-04-14T11:18:48Z",
    "closed_at": "2020-07-16T14:53:32Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10252",
    "body": "Hi mates.\r\nI couldn't find any information about my trouble\r\nSo,\r\nI have big table with engine ReplicatedMergeTree. I hold the data during 90 days there.\r\nthe structure is\r\n```\r\nCREATE TABLE IF NOT EXISTS default.message_by_chid (\r\n...\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/tables/{shard}/message_by_chid', '{replica}')\r\nPARTITION BY toMonday(logs_SendDateTime)\r\nORDER BY (logs_SMSChannelId, logs_SendDateTime, prices_SMSChannelId) TTL logs_SendDateTime + toIntervalDay(90)\r\nSETTINGS index_granularity = 8192;\r\n```\r\nSo, I have a problem that old data was not deleted.\r\nI've checked  system.parts table to get more information about it. I saw that a lot partitions have flags\r\nactive and is_frozen = 1. However, I had not setup these partitions like snapshots. I have other tables  with TTL and  the same engine, but I don't see the same behavior there. \r\n\r\n```\r\n | database | table | partition | name | rows | is_frozen | active | refcount | level | data_version | modification_time | remove_time | min_time | max_time |\r\n | default | message_by_chid | 2019-12-30 | 20191230_0_639598_7365_639615 | 0 | 1 | 1 | 1 | 7365 | 639615 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 |\r\n | default | message_by_chid | 2020-01-06 | 20200106_0_606705_8405_606719 | 0 | 1 | 1 | 1 | 8405 | 606719 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 |\r\n | default | message_by_chid | 2020-01-13 | 20200113_0_537334_6644_537348 | 559835705 | 1 | 1 | 1 | 6644 | 537348 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-13 00:00:00 | 2020-01-19 23:59:59 |\r\n | default | message_by_chid | 2020-01-20 | 20200120_0_452519_8594_466711 | 684299622 | 1 | 1 | 1 | 8594 | 466711 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-20 00:00:00 | 2020-01-26 23:59:59 |\r\n | default | message_by_chid | 2020-01-20 | 20200120_452520_466699_47_466711 | 28731947 | 1 | 1 | 1 | 47 | 466711 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-21 09:00:00 | 2020-01-26 15:21:11 |\r\n | default | message_by_chid | 2020-01-27 | 20200127_0_337054_4649_337066 | 885381133 | 1 | 1 | 1 | 4649 | 337066 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-27 00:00:00 | 2020-02-02 23:59:59 |\r\n | default | message_by_chid | 2020-02-03 | 20200203_0_412266_7930_412278 | 802769920 | 1 | 1 | 1 | 7930 | 412278 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-02-03 00:00:00 | 2020-02-09 23:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_0_210652_42_412585 | 974574244 | 0 | 1 | 1 | 42 | 412585 | 2020-04-14 09:16:57 | 0000-00-00 00:00:00 | 2020-02-10 00:00:00 | 2020-02-16 18:39:43 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_210653_348867_4786_412585 | 70452324 | 0 | 1 | 1 | 4786 | 412585 | 2020-04-14 08:52:50 | 0000-00-00 00:00:00 | 2020-02-11 13:15:54 | 2020-02-16 23:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_348868_360495_36_412585 | 12137004 | 1 | 1 | 1 | 36 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-15 13:00:00 | 2020-02-16 07:10:47 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_367751_371362_35_412585 | 4304951 | 1 | 1 | 1 | 35 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 03:00:00 | 2020-02-16 10:28:42 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_360496_367750_34_412585 | 2074170 | 1 | 1 | 1 | 34 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 03:00:00 | 2020-02-16 08:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_371363_387310_37_412585 | 7889891 | 1 | 1 | 1 | 37 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 09:00:00 | 2020-02-16 14:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_387311_398516_53_412585 | 4373265 | 1 | 1 | 1 | 53 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 09:00:00 | 2020-02-16 22:59:59 |\r\n\r\n```\r\n\r\nCould  you explain a logic for that? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10252/comments",
    "author": "vvchistyakov",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-04-14T15:38:04Z",
        "body": "is_frozen -- is not related to TTL or to active.\r\n\r\nis_frozen -- means that this part's folder has `-r--r--r--` filesystem flags instead of  `-rw-rw-r--` it means that this part was fetched from other replica or a `freeze` command (backup) was executed against this part."
      },
      {
        "user": "vvchistyakov",
        "created_at": "2020-04-14T17:18:10Z",
        "body": "Hi @den-crane\r\nYep, partitions with is_frozen flag have only read permission on filesystem on my server. However. I've checked the replica for this nodes. And I've got the same flags for same partitions. Nobody had made a backup for that."
      },
      {
        "user": "den-crane",
        "created_at": "2020-04-14T19:08:09Z",
        "body": ">it means that this part was fetched from other replica \r\n\r\nanyway it's not related to TTL. You can `unfreeze` them using chmod"
      },
      {
        "user": "vvchistyakov",
        "created_at": "2020-04-15T07:57:21Z",
        "body": "Sorry @den-crane. It still is not clear for me. Is it ok when partitions have these same statuses on all replicas?\r\nCould you suggest a direction where I should search problem with TTL? I've checked  error log for few days and  haven't found any interest errors."
      },
      {
        "user": "vvchistyakov",
        "created_at": "2020-04-15T10:15:33Z",
        "body": "#6462"
      },
      {
        "user": "vvchistyakov",
        "created_at": "2020-04-15T21:52:14Z",
        "body": "Maybe, I'm wrong.\r\nI was continue investigate this trouble.\r\nI found interest thing. I have the partition _20191230_961638_1264729_10079_1279734_. And, there is ttl.txt file with content\r\n`{\"table\":{\"min\":1585753056,\"max\":1586044799}}`\r\n1585753056 = 2020-04-01T14:57:36\r\n1586044799 = 2020-04-04T23:59:59\r\nDoes it mean CH will try delete the data after 2020-06-30?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-04-15T23:25:28Z",
        "body": "it means that TTL should start to remove rows after 2020-04-01T14:57:36"
      },
      {
        "user": "vvchistyakov",
        "created_at": "2020-04-16T08:27:06Z",
        "body": "Oh...I see\r\nLook's like trouble with TTL for me is #7701\r\nSo, as I could understand using `is_frozen` flag on all replicas at the same time for partitions without creating snapshots is normal behaviour for CH.\r\nI guess I've finished my investigation here.\r\nThanks @den-crane "
      },
      {
        "user": "vvchistyakov",
        "created_at": "2020-07-16T09:40:08Z",
        "body": "> @vvchistiakov, do you have any further questions?\r\n\r\nNope:)"
      }
    ]
  },
  {
    "number": 10062,
    "title": "Duplicated primary key in materialized view",
    "created_at": "2020-04-06T09:16:00Z",
    "closed_at": "2020-04-06T18:29:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10062",
    "body": "I have a table\r\n```sql\r\nCREATE TABLE user_video_view (\r\n\tuser_id\t\t\t\tUInt64,\r\n\tvideo_id\t\t\tUInt64,\r\n\tvisitor_session_id\tUUID,\r\n\tvisitor_id\t\t\tUInt64,\r\n\tvisitor_ip\t\t\tString,\r\n\tvisitor_user_agent\tString,\r\n\tcreated_at\t\t\tDateTime\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY (toYYYYMM(created_at), user_id, video_id)\r\nORDER BY (created_at, user_id, video_id, visitor_session_id)\r\n```\r\nand materialized view based on it\r\n```sql\r\nCREATE MATERIALIZED VIEW grouped_user_video_view\r\nENGINE = SummingMergeTree()\r\nPARTITION BY week\r\nORDER BY (user_id, week)\r\nPOPULATE\r\nAS SELECT\r\n\tintDiv(toRelativeWeekNum(created_at) - toRelativeWeekNum(toDateTime('2020-03-23 00:00:00')), 2) AS week,\r\n\tuser_id,\r\n\tcount() AS view_count\r\nFROM user_video_view\r\nGROUP BY user_id, week\r\n```\r\n\r\nFor some combination of `user_id` and `week` I have duplicated rows in response:\r\n```\r\n\u250c\u2500week\u2500\u252c\u2500user_id\u2500\u252c\u2500view_count\u2500\u2510\r\n\u2502    0 \u2502     159 \u2502          1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500week\u2500\u252c\u2500user_id\u2500\u252c\u2500view_count\u2500\u2510\r\n\u2502    0 \u2502       5 \u2502          2 \u2502\r\n\u2502    0 \u2502      15 \u2502          5 \u2502\r\n\u2502    0 \u2502      16 \u2502          4 \u2502\r\n\u2502    0 \u2502      17 \u2502          1 \u2502\r\n\u2502    0 \u2502      42 \u2502          2 \u2502\r\n\u2502    0 \u2502      45 \u2502          3 \u2502\r\n\u2502    0 \u2502     159 \u2502          2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIs this an expected behavior?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10062/comments",
    "author": "grachov",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-04-06T12:41:43Z",
        "body": "Yes. SumminngMT collapses rows during the Merge. \r\nSummingMT and AggregatingMT expect that select query will do final aggregation using group by. Merges are eventual and may never happen.\r\n\r\nSo CH expects that all queries will do final summing using\r\n```\r\nselect sum(view_count) ,  ...\r\nfrom grouped_user_video_view\r\ngroup by ...\r\n```"
      },
      {
        "user": "grachov",
        "created_at": "2020-04-06T13:34:18Z",
        "body": "Thanks for explanation! Does it mean that other view (not materialized) can be created on top of it to apply grouping?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-04-06T14:01:34Z",
        "body": "> Thanks for explanation! Does it mean that other view (not materialized) can be created on top of it to apply grouping?\r\n\r\nyes.\r\n\r\n\r\n\r\nBe aware that double groupping and reading excessive columns may slow-down your queries up to 10 times.\r\n\r\n```\r\nselect sum(view_count), user_id\r\nfrom grouped_user_video_view\r\ngroup by user_id\r\n```\r\n\r\nVS\r\n```\r\n\r\nselect sum(view_count), user_id\r\nfrom ( ---  view\r\n              select sum(view_count), user_id, week\r\n              from grouped_user_video_view \r\n              group by user_id, week \r\n         ) \r\ngroup by user_id\r\n\r\n```"
      },
      {
        "user": "grachov",
        "created_at": "2020-04-06T18:23:56Z",
        "body": "Thanks again! \ud83d\udc4d "
      }
    ]
  },
  {
    "number": 9915,
    "title": "toStartHour doesn't work in MV?",
    "created_at": "2020-03-28T23:16:42Z",
    "closed_at": "2020-03-28T23:36:45Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9915",
    "body": "**Describe the bug**\r\n`toStartOfHour`  in a MV returns 0.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: `20.3.4.10 (official build)` in Ubuntu bionic\r\n* Which interface to use, if matters: `clickhouse-client -m -n`\r\n* Non-default settings, if any: none \r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\nCREATE TABLE test.a (\r\n  t DateTime\r\n)\r\nENGINE=Memory();\r\nCREATE TABLE test.b\r\n(\r\n  t DateTime\r\n)\r\nENGINE=Memory();\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t)\r\nFROM test.a;\r\n```\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\n```\r\nINSERT INTO test.a (t) VALUES ('2020-03-28 11:22:33');\r\n```\r\n* Queries to run that lead to unexpected result\r\n```\r\nSELECT * FROM test.b;\r\n```\r\n**Expected behavior**\r\nI expected one row from table `b`\r\n```\r\n2020-03-28 11:00:00\r\n```\r\nThe actual result is table `b` is empty.\r\n\r\n**Error message and/or stacktrace**\r\nNo errors in the log.\r\n\r\n**Additional context**\r\nOne row is inserted if a column is added.  The value for column `t` is still wrong.\r\n\r\n```\r\nCREATE TABLE test.a (\r\n  t DateTime,\r\n  name String\r\n)\r\nENGINE=Memory();\r\nCREATE TABLE test.b\r\n(\r\n  t DateTime,\r\n  name String\r\n)\r\nENGINE=Memory();\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t),\r\n    name\r\nFROM test.a;\r\nINSERT INTO test.a (t, name) VALUES ('2020-03-28 11:22:33', 'myname');\r\nSELECT * FROM test.b;\r\n```\r\nThe output is\r\n```\r\n0000-00-00 00:00:00\tmyname\r\n```\r\n\r\nThe same SELECT returns correct values if I run it manually in the client.\r\n\r\n```\r\nSELECT\r\n    toStartOfHour(t),\r\n    name\r\nFROM test.a\r\n\r\n\u250c\u2500\u2500\u2500\u2500toStartOfHour(t)\u2500\u252c\u2500name\u2500\u2500\u2500\u2510\r\n\u2502 2020-03-28 11:00:00 \u2502 myname \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9915/comments",
    "author": "knoguchi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-28T23:34:40Z",
        "body": "Column names must be the same in a MV select and the target (TO) table \r\n\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t) **as t**\r\nFROM test.a;\r\n\r\nMV uses column names when it does insert into the target table."
      },
      {
        "user": "knoguchi",
        "created_at": "2020-03-28T23:36:16Z",
        "body": "ouch.  I overlooked it.  Thanks!"
      }
    ]
  },
  {
    "number": 9870,
    "title": "Cannot replicate table from 19.3.3 to 20.3.3",
    "created_at": "2020-03-25T19:17:58Z",
    "closed_at": "2020-03-30T14:24:47Z",
    "labels": [
      "question",
      "backward compatibility",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9870",
    "body": "We created a new node using version 20.3.3 and tried to replicate a number of tables from a 19.3.3 node.  The initial replication worked, but upon restarting the 20.3.3 node got the following failure (this happened for several tables):\r\n\r\n```\r\nExisting table metadata in ZooKeeper differs in index granularity bytes. Stored in\r\nZooKeeper: 10485760, local: 0: Cannot attach table `<db>`.`<table>` from metadata\r\nfile /opt/data/clickhouse/metadata/<db>/<table> from query ATTACH TABLE <table>\r\n(`datetime` DateTime, `kafka_time` DateTime, `hostname` String, `message` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/<db.table>, '{replica}')\r\nPARTITION BY toDate(datetime) ORDER BY (datetime, hostname) SETTINGS\r\nindex_granularity = 8192\r\n```\r\n\r\nIndex granularity on both tables is 8192, metadata .sql file is identical.  metadata from zookeeper node:\r\n\r\n```\r\nmetadata format version: 1\r\ndate column: \r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 0\r\nsign column: \r\nprimary key: datetime, hostname\r\ndata format version: 1\r\npartition key: toDate(datetime)\r\n```\r\n\r\nStack trace:\r\n\r\n```0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x102d352c in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8f2d989 in /usr/bin/clickhouse\r\n2. ? @ 0xd94cdce in /usr/bin/clickhouse\r\n3. DB::StorageReplicatedMergeTree::checkTableStructure(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xd5b426b in /usr/bin/clickhouse\r\n4. DB::StorageReplicatedMergeTree::StorageReplicatedMergeTree(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bo\r\nol, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata const&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char\r\n> > const&, DB::MergeTreeData::MergingParams const&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool) @ 0xd5d9b4b in /usr/bin/clickhouse\r\n5. ? @ 0xd957dba in /usr/bin/clickhouse\r\n6. std::__1::__function::__func<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&), std::__1::allocator<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&)>, std::__1::shared_ptr<DB::IStorage> (DB::Sto\r\nrageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) @ 0xd95b2d3 in /usr/bin/clickhouse\r\n7. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) cons\r\nt @ 0xd4fbc4c in /usr/bin/clickhouse\r\n8. DB::createTableFromAST(DB::ASTCreateQuery, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool\r\n) @ 0xcedc09e in /usr/bin/clickhouse\r\n9. ? @ 0xced2bcf in /usr/bin/clickhouse\r\n10. ? @ 0xced3381 in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8f515e7 in /usr/bin/clickhouse\r\n12. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleI\r\nmpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x8f51c34 in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8f50b07 in /usr/bin/clickhouse\r\n14. ? @ 0x8f4f00f in /usr/bin/clickhouse\r\n15. start_thread @ 0x7e65 in /usr/lib64/libpthread-2.17.so\r\n16. clone @ 0xfe88d in /usr/lib64/libc-2.17.so\r\n (version 20.3.3.6 (official build))\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9870/comments",
    "author": "genzgd",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-25T19:25:37Z",
        "body": "20.3.3 by default uses adaptive index granularity. Such feature is not existed in 19.3.3.\r\n\r\nCreate tables without `adaptive index granularity`\r\n\r\n20.3.3\r\ncreate table ....\r\nsettings index_granularity =8192,  index_granularity_bytes = 0;\r\n\r\n19.3.3\r\ncreate table ....\r\nsettings index_granularity =8192;\r\n\r\nBut there is one problem. LZ4 compression format is incompatible < 19.7 and  >= 19.7 .\r\nYou can temporary use replication 20.3.3 <-> 19.3.3. But you need to upgrade 19.3.3 as soon as possible."
      },
      {
        "user": "genzgd",
        "created_at": "2020-03-30T15:07:32Z",
        "body": "Yes, we definitely had some weirdness because we upgrade to 19.17 at one point and then downgraded back to 19.3.3, so there was some unexpected inconsistency in zookeeper metadata definitions around index granularity.  Thanks for pointing us in the right direction!"
      }
    ]
  },
  {
    "number": 9701,
    "title": "how to make MATERIALIZED VIEW update automatically when several origin-tables were inserted",
    "created_at": "2020-03-17T06:42:49Z",
    "closed_at": "2020-03-20T06:44:38Z",
    "labels": [
      "question",
      "comp-matview"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9701",
    "body": "Create two origin-tables with only two fields, associated by id\uff1a\r\n```\r\nCREATE TABLE default.test0 (\r\n`id` String,\r\n `name` String\r\n) ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\n\r\nCREATE TABLE default.test00 (\r\n`id` String,\r\n `name2` String\r\n) ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\n```\r\ncreate the MATERIALIZED VIEW\uff1a\r\n```\r\ncreate  MATERIALIZED VIEW default.test_view  ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\nAS select t0.id,name,name2 from `default`.test0 t0 join `default`.test00 t00 on t0.id=t00.id\r\n```\r\nInsert into origin-tables\uff0ctest0 and test00\uff1a\r\n```\r\ninsert into `default`.test0 values ('1','name1')\r\n\r\ninsert into `default`.test00 values ('1','name10')\r\n```\r\nthen select from the view\uff1a\r\n`select * from default.test_view `\r\n\r\nbut the resultset is empty.\r\n\r\n> id|name|name2|\r\n> --|----|-----|\r\n\r\n\r\nBut if I create a 'MATERIALIZED VIEW' for a single table, the view can auto update after the single table was inserted.like:\r\n```\r\ncreate  MATERIALIZED VIEW default.test_view0  ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\nAS select  id,name FROM `default`.test0 \r\n```\r\n\r\n\r\nPlease tell me how to make the MATERIALIZED VIEW update automatically after origin-tables were inserted?  Thank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9701/comments",
    "author": "zhouxiujue",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-03-17T09:37:27Z",
        "body": "Materialiezed view is updated when new block of data is inserted in the left-most table. \r\nYou can think about that as AFTER INSERT trigger.\r\n\r\n> after origin-tables were inserted\r\n\r\nClickHouse can't know if you already finish inserting or plan to insert more. \r\n\r\nYou can just fire `INSERT INTO target_table SELECT .... FROM src_table1 LEFT JOIN src_table2  where condition`\r\n\r\n\r\n\r\n"
      },
      {
        "user": "zhouxiujue",
        "created_at": "2020-03-20T06:45:03Z",
        "body": "Thank you\uff01"
      }
    ]
  },
  {
    "number": 9616,
    "title": "upgrade clickhouse cluster",
    "created_at": "2020-03-12T05:17:38Z",
    "closed_at": "2020-03-13T03:57:48Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9616",
    "body": "Now version:19.16.2.2 (official build)\r\nupgrade to:20.1.6.30-2\r\n\r\nIs this ok?\r\nWill it affect zookeeper?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9616/comments",
    "author": "Inasayang",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-12T15:47:59Z",
        "body": ">Is this ok?\r\n\r\nyes\r\n\r\n>Will it affect zookeeper?\r\n\r\nno"
      },
      {
        "user": "Inasayang",
        "created_at": "2020-03-13T03:57:48Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 9541,
    "title": "Materialized View with targeting past data",
    "created_at": "2020-03-06T16:06:16Z",
    "closed_at": "2020-03-17T02:52:31Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9541",
    "body": "I am having an issue with a Materialized View which targets the past data. I know that MV works like a trigger for inserts to a table. \r\n\r\nI need a Materialized View only for yesterday. I have the following MV:\r\n\r\n```\r\nCREATE MATERIALIZED VIEW default.chart_yesterday\r\nENGINE = ReplicatedSummingMergeTree(\r\n     '/clickhouse/tables/{shard}/default/chart_yesterday',\r\n     '{replica}')\r\n     PARTITION BY toYYYYMM(date)\r\n     ORDER BY (date, hour, cityHash64(organization_id)\r\n)\r\nSAMPLE BY cityHash64(organization_id)\r\nSETTINGS index_granularity = 8192\r\nPOPULATE AS\r\nSELECT\r\n     SUM(rejected) AS clr,\r\n     (count() - clr) AS cla,\r\n     toDate(request_time) AS date,\r\n     toHour(request_time) as hour,\r\n     organization_id\r\nFROM mytable_sharded\r\nWHERE date = yesterday()\r\nGROUP BY date, hour, organization_id\r\nORDER BY hour;\r\n```\r\n\r\nAfter creating the VM, I have data only for yesterday, everything fine. but after a day, the VM has no data\r\n\r\nDoes it mean that since there is no trigger for yesterday's data, so VM doesn't get triggered, so no data?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9541/comments",
    "author": "hatrena",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-06T21:57:52Z",
        "body": ">WHERE date = yesterday()\r\n\r\nYou MV will get a new data only if `insert into mytable_sharded` will insert `date = yesterday()`\r\n\r\nYou can create MV without this condition `WHERE date = yesterday()`\r\nWith daily partitioning ` PARTITION BY toYYYYMMDD(date)`\r\nAnd remove data older than yesterday by `drop partition` or by `table TTL`\r\nthen your MV will have data only for yesterday and today."
      },
      {
        "user": "hatrena",
        "created_at": "2020-03-25T15:21:36Z",
        "body": "I used `PARTITION BY toYYYYMMDD(date)` with `table TTL`, and it worked perfectly. thanks"
      }
    ]
  },
  {
    "number": 9504,
    "title": "Error in system.replication_queue ",
    "created_at": "2020-03-04T00:38:39Z",
    "closed_at": "2020-03-04T01:20:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9504",
    "body": "version  20.2.1\r\n\r\nThere is  an error in table system.replication_queue  .\r\n\r\n Not executing log entry for part 20200213_2040_2040_1_2016 because another log entry for the same part is being processed. This shouldn't happen often.\r\n\r\n\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500replica_name\u2500\u252c\u2500position\u2500\u252c\u2500node_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500required_quorum\u2500\u252c\u2500source_replica\u2500\u252c\u2500new_part_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500parts_to_merge\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500is_detach\u2500\u252c\u2500is_currently_executing\u2500\u252c\u2500num_tries\u2500\u252c\u2500last_exception\u2500\u252c\u2500\u2500\u2500last_attempt_time\u2500\u252c\u2500num_postponed\u2500\u252c\u2500postpone_reason\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500last_postpone_time\u2500\u2510\r\n\u2502 my_sdap  \u2502 dm_user_behavior_events \u2502 replica-001  \u2502        0 \u2502 queue-0000104788 \u2502 MERGE_PARTS \u2502 2020-03-04 07:46:52 \u2502               0 \u2502 replica-001    \u2502 20200213_2040_2040_1_2016 \u2502 ['20200213_2040_2040_0_2016'] \u2502         0 \u2502                      0 \u2502         0 \u2502                \u2502 0000-00-00 00:00:00 \u2502          3801 \u2502 Not executing log entry for part 20200213_2040_2040_1_2016 because another log entry for the same part is being processed. This shouldn't happen often. \u2502 2020-03-04 08:36:43 \u2502",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9504/comments",
    "author": "onine007",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-03-04T01:18:41Z",
        "body": "This is normal system behaviour."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-03-04T01:19:07Z",
        "body": "And this is not an error but `postpone_reason`."
      },
      {
        "user": "onine007",
        "created_at": "2020-03-04T01:20:42Z",
        "body": "OK  ,thank you!"
      }
    ]
  },
  {
    "number": 9406,
    "title": "Can I controll the insert rate of syntex 'INSERT INTO SELECT'?",
    "created_at": "2020-02-27T08:31:12Z",
    "closed_at": "2020-03-01T15:29:16Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9406",
    "body": "I would like to know what is the factor that controll the insert rate of syntex 'INSERT INTO SELECT'?It depends on ClickHouse internal calculation?\r\nAny parameter can I change to increase or decrease the insert rate?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9406/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-01T15:26:17Z",
        "body": "No. \r\n\r\nCurrently `INSERT INTO SELECT` works in one thread (insert part).\r\nIn the next stable release (20.2) will be implemented **Parallel INSERT in INSERT SELECT query** #8166 and a setting `max_insert_threads`.\r\n\r\nBut still where is no ability and plans for ability to control insert rate in MB/s or rows/s.\r\n\r\n\r\nAs workaround you can use Linux utility pv set the rate:\r\n\r\n```\r\nclickhouse-client -q 'select * from T format Native' |pv --rate-limit=1M | clickhouse-client -q 'insert into T1 format Native'\r\n```"
      },
      {
        "user": "byx313",
        "created_at": "2020-03-01T15:29:16Z",
        "body": "> No.\r\n> \r\n> Currently `INSERT INTO SELECT` works in one thread (insert part).\r\n> In the next stable release (20.2) will be implemented **Parallel INSERT in INSERT SELECT query** #8166 and a setting `max_insert_threads`.\r\n> \r\n> But still where is no ability and plans for ability to control insert rate in MB/s or rows/s.\r\n> \r\n> As workaround you can use Linux utility pv set the rate:\r\n> \r\n> ```\r\n> clickhouse-client -q 'select * from T format Native' |pv --rate-limit=1M | clickhouse-client -q 'insert into T1 format Native'\r\n> ```\r\n\r\nthank you for the reply guy"
      }
    ]
  },
  {
    "number": 9115,
    "title": "How to update using Join with 2 join condition",
    "created_at": "2020-02-14T10:03:02Z",
    "closed_at": "2020-02-15T09:03:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9115",
    "body": "I want to make below query as clickhouse query.\r\n```sql\r\nupdate table1 \r\nset table1.col4 = table2.col4\r\nfrom table2 \r\nwhere table1.col1 = table2.col1 and table1.col2 = table2.col2 and table2.col3='2020-01-02';\r\n```\r\n\r\nI made a query like below, But, I got error and don`t know how to make 2 join condition.\r\n\r\n```sql\r\n\r\nCREATE TABLE test1\r\n(\r\n    `col1` Int8, \r\n    `col2` String, \r\n    `col3` Date, \r\n    `col4` UInt16\r\n)\r\nENGINE = Log\r\n\r\nINSERT INTO test1 VALUES(1,'001','2020-01-01', 1)(1,'002','2020-01-01', 1)(2,'001','2020-01-01', 2)(2,'002','2020-01-02', 3)(2,'003','2020-01-04', 5);\r\n\r\n-- create join engine\r\nCREATE TABLE test_join AS test1\r\nENGINE = Join(ANY, LEFT, col1, col2)\r\n\r\nOk.\r\n\r\nINSERT INTO test_join SELECT *\r\nFROM test1\r\nWHERE col3 = '2020-01-02'\r\n\r\n-- update\r\n:) ALTER TABLE test1 UPDATE col4 = joinGet('test_join', 'col4', col1, col2);\r\n\r\nSyntax error: failed at position 73 (end of query):\r\n\r\nALTER TABLE test1 UPDATE col4 = joinGet('test_join', 'col4', col1, col2);\r\n\r\nExpected one of: AND, OR, token, WHERE, NOT, BETWEEN, LIKE, IS, NOT LIKE, IN, NOT IN, GLOBAL IN, GLOBAL NOT IN, Comma, QuestionMark\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9115/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "4ertus2",
        "created_at": "2020-02-14T12:33:46Z",
        "body": "You've made something very strange.\r\n\r\n1. ALTER UPDATE is special operation. Do not use it in general ETL logic.\r\n2. Engine JOIN is a kind of optimisation of JOINs with joinGet extension. Do not use it for JOINs if general JOIN doesn't work. Do not use it in dictionary-like scenario if dictGet doesn't work.\r\n3. It's not clear in docs but engine JOIN do not support complex keys yet.\r\n\r\nYou're trying to combine several special extensions in totally unexpected dangerous way. Nobody helps you if something goes wrong.\r\nCreate intermediate table and update via it."
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-15T00:52:26Z",
        "body": "ext.dictionaries\r\n\r\n```\r\nCREATE TABLE test1\r\n(\r\n    `col1` Int8, \r\n    `col2` String, \r\n    `col3` Date, \r\n    `col4` UInt16\r\n)\r\nENGINE = MergeTree order by tuple();\r\n\r\nINSERT INTO test1 VALUES(1,'001','2020-01-01', 1)(1,'002','2020-01-01', 1)\r\n(2,'001','2020-01-01', 2)(2,'002','2020-01-02', 3)(2,'003','2020-01-04', 5);\r\n\r\nCREATE TABLE test_join AS test1 ENGINE = MergeTree order by tuple();\r\n\r\nINSERT INTO test_join SELECT * FROM test1 WHERE col3 = '2020-01-02';\r\n\r\nCREATE DICTIONARY test_join_dict (`col1` Int8, `col2` String, `col3` Date, `col4` UInt16)\r\nPRIMARY KEY col1,col2 \r\nSOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE test_join DB 'default' USER 'default')) \r\nLIFETIME(MIN 0 MAX 0) LAYOUT(COMPLEX_KEY_HASHED());\r\n\r\nALTER TABLE test1 UPDATE col4 = dictGet('default.test_join_dict', 'col4', tuple(col1, col2))\r\nwhere dictHas('default.test_join_dict', tuple(col1, col2))\r\n\r\n```\r\n"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-15T09:03:01Z",
        "body": "Thank for help. Thanks."
      }
    ]
  },
  {
    "number": 9083,
    "title": "multiple update queries at the same time",
    "created_at": "2020-02-12T01:42:16Z",
    "closed_at": "2020-02-12T04:17:50Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9083",
    "body": "If I send more than one update query at the same time and the queries update same data, How does the ClickHouse handle data??\r\n\r\nDoes the ClickHouse handle query sequentially? If so, afterward update query is waiting for forward update query? \r\n\r\nIf the queries update data at the same time, How does the ClickHouse keep data integrity?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9083/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-12T04:08:20Z",
        "body": "Sequentially. Though several mutations can be combined into one. But still data will be updated as updates applied sequentially.\r\n\r\n>How does the ClickHouse keep data integrity\r\n\r\nData will be consistent after all updates ended (eventually). But during mutations your selects will see partially updated data because mutations are not atomic."
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-12T04:17:50Z",
        "body": "Thank you."
      }
    ]
  },
  {
    "number": 9058,
    "title": "When import file, String type contains \"BACKSLASH TAB\".",
    "created_at": "2020-02-10T01:00:15Z",
    "closed_at": "2020-02-11T00:59:54Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9058",
    "body": "When I import tsv file and there are 64 columns, one String column(column 57) contains \"BACKSLASH and TAB\" so, it can`t import tsv file.\r\n\r\nversion - 20.1.2.4 \r\n\r\n```sql\r\n# cat /home/test/data/group_06/test_data.tsv | clickhouse-client --password=123456 --database=bts --query=\"INSERT INTO t_order_mergetree FORMAT TabSeparatedWithNames\" --input_format_tsv_empty_as_default=1;\r\nCode: 27. DB::Exception: Cannot parse input: expected \\t before: \\nabc\\t20190528-007178\\t2019-05-28 11:28:38\\t1\\t\"\"\\tKRW\\t26345\\t26345\\t0.00\\t\\t2019-05-28 11:25:00\\tcard\\t\\tF\\t\\t0\\tT\\t1.000000\\t34000.00\\t2500.00\\tF\\tF\\tT\\t\"\"\\tNCHECKOUT\\t\"\"\\t\\t0.00\\t: (at row 965285)\r\n\r\nRow 965285:\r\nColumn 0,   name: m_id,                      type: String,                   parsed text: \"abc\"\r\n...\r\nColumn 56,  name: input_channel_detail_type,    type: Nullable(FixedString(4)), parsed text: <EMPTY>\r\nColumn 57,  name: inflow_path,                  type: Nullable(String),         parsed text: \"criteo<BACKSLASH><TAB>\"\r\nColumn 58,  name: app_order_discount_amount,    type: Nullable(Float64),        parsed text: <EMPTY>\r\nColumn 59,  name: app_product_discount_amount,  type: Nullable(Float64),        parsed text: <EMPTY>\r\nColumn 60,  name: deferred_payment_commission,  type: Nullable(Float64),        parsed text: \"0\"\r\nColumn 61,  name: seperate_delivery_count,      type: Nullable(Float64),        parsed text: \"0.00\"\r\nColumn 62,  name: balanced_price,               type: Nullable(Float64),        parsed text: \"0\"\r\nColumn 63,  name: add_paid_amount,              type: Nullable(Float64),        parsed text: \"0\"\r\nERROR: Line feed found where tab is expected. It's like your file has less columns than expected.\r\nAnd if your file have right number of columns, maybe it have unescaped backslash in value before tab, which cause tab has escaped.\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9058/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-10T16:14:43Z",
        "body": "\\ -- (char(92)) is a special character for TSV and must be escaped with `slash (\\)`\r\n\r\n```\r\nSELECT char(92)\r\nFORMAT TSV\r\n\r\n\\\\\r\n```"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-10T23:38:27Z",
        "body": "Can`t I use DOUBLE QUOTE in tsv also?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-11T00:59:06Z",
        "body": "you can.\r\nDOUBLE QUOTE in tsv is a usual character, not special.\r\n\r\n```\r\nSELECT\r\n    '\"aaaa',\r\n    111\r\nFORMAT TSV\r\n\r\n\"aaaa\t111\r\n```"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-11T00:59:54Z",
        "body": "Thanks for your kindly answer"
      }
    ]
  },
  {
    "number": 9045,
    "title": "drop partition for all shard in one DDL execution",
    "created_at": "2020-02-07T09:03:53Z",
    "closed_at": "2020-02-11T03:02:34Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9045",
    "body": "version:19.11.3\r\nbackground:\r\nthree servers:A,B,C ares running clickhouse cluster.\r\n2 shard 1 replicas config looks like below:\r\n``` \r\n<cluster_test>\r\n        <shard>\r\n            <replica>\r\n                <host>B</host>\r\n                <port>9000</port>\r\n            </replica>\r\n        </shard>\r\n        <shard>\r\n            <replica>\r\n                <host>C</host>\r\n                <port>9000</port>\r\n            </replica>\r\n        </shard>\r\n</cluster_test>  \r\n```\r\n\r\nIn server A,there is a test.hist_all table created by below script:\r\n**CREATE TABLE hits_all AS tutorial.hits_local_all\r\nENGINE = Distributed(cluster_test, test, hits_local, rand());**\r\n\r\nIn server B&C,there are concrete table test.hist to store data created by below script:\r\n**CREATE TABLE test.hits_local (...) ENGINE = MergeTree PARTITION BY dayno ....**\r\n\r\nQuestion:\r\nCan I drop patition data with SQL below **in just one execution**.\r\n#ALTER TABLE hist on cluster cluster_test DROP PARTITION 20200114;#",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9045/comments",
    "author": "karlchan-cn",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-07T15:05:43Z",
        "body": "Not sure about 19.11. In latest (supported) 19.16+ versions you can.\r\n\r\n\r\n>in just one execution.\r\n\r\nAnyway, even `on cluster` is not atomic. "
      },
      {
        "user": "filimonov",
        "created_at": "2020-02-10T12:11:50Z",
        "body": "@den-crane drop partition on Distributed?\r\n\r\nDistributed doesn't have any partitions, why should you drop partitions from it? \r\n\r\n@karlchan-cn, try\r\n```sql\r\nALTER TABLE hits_local on cluster cluster_test DROP PARTITION 20200114;\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-10T18:43:46Z",
        "body": "My understanding that `test.hits` is MergeTree not Distributed"
      },
      {
        "user": "filimonov",
        "created_at": "2020-02-11T03:02:34Z",
        "body": "> My understanding that `test.hits` is MergeTree not Distributed\r\n\r\nYou probably right: 'test.hist to store data created by below script: CREATE TABLE test.hits_local'. Misleading. :confused: \r\n"
      },
      {
        "user": "karlchan-cn",
        "created_at": "2020-02-12T05:30:35Z",
        "body": "Sorry for some wrong description.\r\nThe problems is I want to delete data in server B&C on table hits_local  in just one SQL execution with script like below\r\n#ALTER TABLE hits_local on cluster cluster_test DROP PARTITION 20200114;#.\r\n\r\n@den-crane Thanks for your kindly answering ,i will try the new version and check the result.\r\n\r\n> Not sure about 19.11. In latest (supported) 19.16+ versions you can.\r\n> \r\n> > in just one execution.\r\n> \r\n> Anyway, even `on cluster` is not atomic.\r\n\r\nRight now , my solutions is to execute DDL script on both B&C server,so the script has to be executed twice.@filimonov thanks for your concern, right now i did as you mentioned.\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 9044,
    "title": "Import tsv exception: Cannot parse input: expected \\t ",
    "created_at": "2020-02-07T08:49:13Z",
    "closed_at": "2020-02-07T15:05:31Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9044",
    "body": "I imported tsv file to CH, but, I got an error.\r\nHow can I fix this?\r\n\r\ntsv file for import\r\n```tsv\r\n2010-01-01\tabc\t1\t\t\t2020-02-07\r\n2020-01-02\t\t2\t\t\t\r\n2020-01-03\taaa\t\t\t\t2020-02-04\r\n```\r\n\r\ntable\r\n```sql\r\nCREATE TABLE default.test3\r\n(\r\n    `EventDate` Date, \r\n    `CounterID` Nullable(String), \r\n    `UserID` Nullable(UInt32), \r\n    `day1` Nullable(Date), \r\n    `day2` Nullable(Date), \r\n    `day3` Nullable(Date)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY EventDate\r\n```\r\n\r\nError\r\n```\r\n# cat test.tsv | clickhouse-client --query=\"INSERT INTO test3 FORMAT TSV\"\r\nCode: 27, e.displayText() = DB::Exception: Cannot parse input: expected \\t before: -07\\n2020-01-02\\t\\t2\\t\\t\\t\\n2020-01-03\\taaa\\t\\t\\t\\t2020-02-04\\n: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: EventDate, type: Date,             parsed text: \"2010-01-01\"\r\nColumn 1,   name: CounterID, type: Nullable(String), parsed text: \"abc\"\r\nColumn 2,   name: UserID,    type: Nullable(UInt32), parsed text: \"1\"\r\nColumn 3,   name: day1,      type: Nullable(Date),   parsed text: \"<TAB><TAB>2020-02\"\r\nERROR: garbage after Nullable(Date): \"-07<LINE FEED>2020-0\"\r\n\r\n, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. 0xbc31d9c Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n1. 0x4f6ccd9 DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n2. 0x496bab9 ?  in /usr/bin/clickhouse\r\n3. 0x92ed647 DB::TabSeparatedRowInputFormat::readRow(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, DB::RowReadExtension&)  in /usr/bin/clickhouse\r\n4. 0x97e5f69 DB::IRowInputFormat::generate()  in /usr/bin/clickhouse\r\n5. 0x91a4c27 DB::ISource::work()  in /usr/bin/clickhouse\r\n6. 0x9169435 DB::InputStreamFromInputFormat::readImpl()  in /usr/bin/clickhouse\r\n7. 0x8a6d32f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n8. 0x94eb632 DB::ParallelParsingBlockInputStream::parserThreadFunction(unsigned long)  in /usr/bin/clickhouse\r\n9. 0x4fa4657 ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>)  in /usr/bin/clickhouse\r\n10. 0x4fa4c84 ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n11. 0x4fa3b77 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n12. 0x4fa212f ?  in /usr/bin/clickhouse\r\n13. 0x7e25 start_thread  in /usr/lib64/libpthread-2.17.so\r\n14. 0xfebad clone  in /usr/lib64/libc-2.17.so\r\n (version 20.1.2.4 (official build))\r\nCode: 27. DB::Exception: Cannot parse input: expected \\t before: -07\\n2020-01-02\\t\\t2\\t\\t\\t\\n2020-01-03\\taaa\\t\\t\\t\\t2020-02-04\\n: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: EventDate, type: Date,             parsed text: \"2010-01-01\"\r\nColumn 1,   name: CounterID, type: Nullable(String), parsed text: \"abc\"\r\nColumn 2,   name: UserID,    type: Nullable(UInt32), parsed text: \"1\"\r\nColumn 3,   name: day1,      type: Nullable(Date),   parsed text: \"<TAB><TAB>2020-02\"\r\nERROR: garbage after Nullable(Date): \"-07<LINE FEED>2020-0\"\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9044/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-07T14:59:43Z",
        "body": "By default CH expects in TSV Nulls encoded as \\N\r\n\r\nYou can use `--input_format_tsv_empty_as_default arg   Treat empty fields in TSV input as default values.`\r\n\r\n```\r\ncat test.tsv | clickhouse-client --input_format_tsv_empty_as_default=1 --query=\"INSERT INTO default.test3 FORMAT TSV\"\r\n\r\nSELECT *\r\nFROM default.test3\r\n\r\n\u250c\u2500\u2500EventDate\u2500\u252c\u2500CounterID\u2500\u252c\u2500UserID\u2500\u252c\u2500day1\u2500\u252c\u2500day2\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500day3\u2500\u2510\r\n\u2502 2010-01-01 \u2502 abc       \u2502      1 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 2020-02-07 \u2502\r\n\u2502 2020-01-02 \u2502 \u1d3a\u1d41\u1d38\u1d38      \u2502      2 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502       \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2502 2020-01-03 \u2502 aaa       \u2502   \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 2020-02-04 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-07T15:05:29Z",
        "body": "Thanks a lot. It worked"
      }
    ]
  },
  {
    "number": 8999,
    "title": "Create MATERIALIZED VIEW against ReplicatedMergeTree and Distributed tables",
    "created_at": "2020-02-04T17:48:51Z",
    "closed_at": "2020-02-11T17:38:12Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8999",
    "body": "I am trying to create VM for my cluster. before getting to the point here is the details:\r\n\r\nI have 2 shards and 2 replicas in each.\r\n\r\nDetails:\r\n\r\ncluster name: _clicks_cluster_\r\n\r\nI have a replicated table:\r\n```\r\nCREATE TABLE default.clicks_replicated\r\n(\r\n    ...\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated', '{replica}')\r\nPARTITION BY (...)\r\nORDER BY (...)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n\r\nThen I created the distributed from it:\r\n```\r\nCREATE TABLE IF NOT EXISTS default.clicks_distributed AS default.clicks_replicated\r\nENGINE = Distributed(clicks_cluster, default, clicks_replicated, cityHash64(my_column));\r\n```\r\n\r\nNow I want to create a VM.:\r\n\r\nBut I found out I don't get the new data if I create it against `clicks_distributed`  with `ENGINE = SummingMergeTree` \r\n\r\nAlso creating it against `clicks_replicated` will lead to incomplete data per replica. \r\n\r\nwhat would be the query for creating VM in this case?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8999/comments",
    "author": "hatrena",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-04T18:55:37Z",
        "body": "Materialized View is an insert trigger. It gets data from INSERT. It never reads/selects a source table.\r\n\r\nThe most used schema is to create at **all nodes** the **same set of tables / MVs**. MV's also **replicated** Engine.\r\n\r\n\r\nCREATE TABLE default.clicks_replicated\r\n(\r\n)\r\nENGINE = ReplicatedMergeTree\r\n\r\nCREATE MATERIALIZED VIEW default.clicks_replicatedMV \r\nEngine= **ReplicatedSummingMergeTree**\r\nas select .... **from default.clicks_replicated**\r\n\r\n....\r\nCREATE TABLE IF NOT EXISTS default.clicks_distributed AS default.clicks_replicated\r\nENGINE = Distributed(clicks_cluster, default, clicks_sharded, cityHash64(my_column));"
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-04T20:32:30Z",
        "body": "ReplicatedSummingMergeTree requires 2 to 3 parameters. \r\n\r\nAnd if I do: \r\n\r\n`Engine = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated', '{replica}')` \r\n\r\n\r\nI get this error: \r\n\r\n`Existing table metadata in ZooKeeper differs in mode of merge operation. Stored in ZooKeeper: 0, local: 2`"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-04T20:50:40Z",
        "body": "SummingMT completely another table. ZK Path should be different\r\n\r\nReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated_sumXXXMyFirstMV', '{replica}')"
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-04T21:04:23Z",
        "body": "Damn, I totally missed that part. my bad. Now it works perfectly fine. Thanks a lot for your quick response. \ud83d\udc4d "
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-10T17:53:12Z",
        "body": "I tested the MV in our staging where we have 1 shard and 2 replicas. everything is fine. \r\n\r\nThen I have tested it in our production where we have 2 shards and 2 replicas in each. we also have 4 Kubernetes pods. \r\n\r\nI have checked in all 4 pods and the MV does exist in all of them, but the result of the same query is different on each pod. data is increasing, but only in a very specific pod. \r\n\r\n```\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS default.vm_click_line_chart\r\nENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/vm_click_line_chart', '{replica}')\r\nORDER BY date POPULATE AS\r\nSELECT\r\n    count() AS clicks,\r\n    toDate(request_time) AS date,\r\n    organization_id\r\nFROM default.clicks_sharded\r\nWHERE (today() - toDate(request_time)) <= 180\r\nGROUP BY\r\n    date,\r\n    organization_id;\r\n```\r\n\r\nAny clue of such behavior?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-10T18:48:12Z",
        "body": "> but the result of the same query is different on each pod\r\n\r\nWhat query?"
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-10T19:44:19Z",
        "body": "a simple query for checking stats like\r\n`SELECT SUM(clicks), date FROM default.vm_click_line_chart WHERE date > '2020-02-01' AND organization_id = 'XXX' group by date`\r\n\r\nThe result of that query differs in every pod."
      },
      {
        "user": "mirajgodha",
        "created_at": "2021-01-08T05:59:52Z",
        "body": "@hatrena  Were you able to get the same results on all the pods, if yes how?"
      },
      {
        "user": "hatrena",
        "created_at": "2021-01-08T11:48:59Z",
        "body": "@mirajgodha , as @den-crane said\r\n> The most used schema is to create at all nodes the same set of tables / MVs. MV's also replicated Engine."
      },
      {
        "user": "mazensibai",
        "created_at": "2022-11-14T06:22:56Z",
        "body": "> I tested the MV in our staging where we have 1 shard and 2 replicas. everything is fine.\r\n> \r\n> Then I have tested it in our production where we have 2 shards and 2 replicas in each. we also have 4 Kubernetes pods.\r\n> \r\n> I have checked in all 4 pods and the MV does exist in all of them, but the result of the same query is different on each pod. data is increasing, but only in a very specific pod.\r\n> \r\n> ```\r\n> CREATE MATERIALIZED VIEW IF NOT EXISTS default.vm_click_line_chart\r\n> ENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/vm_click_line_chart', '{replica}')\r\n> ORDER BY date POPULATE AS\r\n> SELECT\r\n>     count() AS clicks,\r\n>     toDate(request_time) AS date,\r\n>     organization_id\r\n> FROM default.clicks_sharded\r\n> WHERE (today() - toDate(request_time)) <= 180\r\n> GROUP BY\r\n>     date,\r\n>     organization_id;\r\n> ```\r\n> \r\n> Any clue of such behavior?\r\n\r\nthe mistake that you did is that you have created the MV as sharded .. that is why you are getting different result ..."
      }
    ]
  },
  {
    "number": 8686,
    "title": "New installation on Ubuntu VM: Password required for user default. ",
    "created_at": "2020-01-16T15:05:24Z",
    "closed_at": "2020-01-16T15:42:11Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8686",
    "body": "I can't get the initial setup to work on my newly created Ubuntu 18.04.3 LTS virtual machine.\r\nI followed the instructions on the website by executing the following terminal commands:\r\n\r\n```\r\nsudo apt-get install dirmngr    # optional\r\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4    # optional\r\nsudo apt-get update\r\nsudo apt-get install clickhouse-client clickhouse-server\r\n```\r\n\r\nThis should suffice, right? I then start the server, try to start the client and this is what happens:\r\n```\r\ntaxel@taxel-VirtualBox:~$ sudo service clickhouse-server start\r\ntaxel@taxel-VirtualBox:~$ clickhouse-client\r\nClickHouse client version 19.17.6.36 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 194. DB::Exception: Received from localhost:9000. DB::Exception: Password required for user default. \r\n```\r\nI also tried changing the default password from `<password></password>` to `<password>123</password>` and logging in via `clickhouse-client --password=123` but it outputs that the password is wrong (and yes, I have ensured the xml file is saved and the server is restarted)\r\n\r\nAny help would be much appreciated.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8686/comments",
    "author": "Taxel",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-16T15:34:21Z",
        "body": "On install CH asked for a default password for default user and placed it to /etc/clickhouse-server/users.d/default-password.xml \r\nYou can change this password or remove this file to empty password."
      },
      {
        "user": "Taxel",
        "created_at": "2020-01-16T15:42:11Z",
        "body": "Thanks, that fixed it!"
      }
    ]
  },
  {
    "number": 8592,
    "title": "Memory limit (for query) exceeded on SELECT",
    "created_at": "2020-01-09T13:29:06Z",
    "closed_at": "2020-01-11T17:52:42Z",
    "labels": [
      "question",
      "memory"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8592",
    "body": "Hi,\r\n\r\nI am running a quite complex SELECT query on a clickhouse 19.16.5.15, and I have the following error :\r\n`DB::Exception: Memory limit (for query) exceeded: would use 723.47 MiB (attempt to allocate chunk of 5201580 bytes), maximum: 720.00 MiB.`\r\n\r\nPreviously, playing with max_bytes_before_external_sort and max_bytes_before_external_group_by (setting them to half of the max_memory_usage), allow me to run such queries, but it is no more be the case. \r\nMy current configuration is :\r\n```\r\n<max_memory_usage>754974720</max_memory_usage>\r\n<max_bytes_before_external_sort>377487360</max_bytes_before_external_sort>\r\n<max_bytes_before_external_group_by>377487360</max_bytes_before_external_group_by>\r\n<max_memory_usage_for_all_queries>1509949440</max_memory_usage_for_all_queries>\r\n```\r\n\r\nIf I activated debug log, I could see that the query seems to go on disk (what I expect), since I get several:\r\n`2020.01.08 14:22:46.881201 [ 46 ] {767e6850-f1b4-49ae-af81-a62e8e24573c} <Debug> Aggregator: Writing part of aggregation data into temporary file /data/tmp/tmp30010qaaaaa.`\r\n\r\nNevertheless, I finally got this stacktrace:\r\n```\r\n0. 0x3582798 StackTrace::StackTrace() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n1. 0x358b1df DB::Exception::Exception(std::string const&, int) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n2. 0x5bf6a99 DB::IBlockInputStream::checkTimeLimit() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n3. 0x5bfb2d0 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n4. 0x6240336 DB::FilterBlockInputStream::readImpl() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n5. 0x5bfb2f5 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n6. 0x62373a8 DB::ExpressionBlockInputStream::readImpl() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n7. 0x5bfb2f5 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n8. 0x626c7a9 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n9. 0x626ce6b ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n10. 0x35bf902 ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n11. 0x722562f execute_native_thread_routine /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n12. 0x7fe8c1fdfdd5 start_thread /usr/lib64/libpthread-2.17.so\r\n13. 0x7fe8c1b04ead clone /usr/lib64/libc-2.17.so\r\n```\r\n\r\nI also try to decrease max_insert_block_size and max_block_size, without any improvement.\r\n\r\nDo you know other settings I could play for allowing this query to be executed ? (even if it is slow).\r\n\r\nThe exact query is the following:\r\n```\r\nselect\r\n\t\t/* @TOPN_SELECT_PART@ */\r\n\t\t\r\n\t\tany(ifNull(if(c1 = 0, '(UNDEFINED)',dictGetString('agents', 'name', toUInt64(c1))),'(UNDEFINED)')) as `co1`, \r\n\t\tany(ifNull(if(c2 = 0, '(UNDEFINED)',dictGetString('applications', 'name', toUInt64(c2))),'(UNDEFINED)')) as `co2`, \r\n\t\tany(ifNull(if(c3 = 0, '(UNDEFINED)',dictGetString('netflow_sources', 'name', toUInt64(c3))),'(UNDEFINED)')) as `co3`, \r\n\t\tany(ifNull(if(c4 = 0, '(UNDEFINED)',dictGetString('wakb_ip_protocols', 'name', toUInt64(c4))),'(UNDEFINED)')) as `co4`, \r\n\t\tany(ifNull(if(c5 = 0, '(UNDEFINED)',dictGetString('offices', 'name', toUInt64(c5))),'(UNDEFINED)')) as `co5`, \r\n\t\tany(ifNull(if(c6 = 0, '(UNDEFINED)',dictGetString('offices', 'name', toUInt64(c6))),'(UNDEFINED)')) as `co6`, \r\n\t\tany(ifNull(if(c7 = 0, '(UNDEFINED)',dictGetString('netflow_interfaces', 'name', toUInt64(c7))),'(UNDEFINED)')) as `co7`, \r\n\t\tany(ifNull(if(c8 = 0, '(UNDEFINED)',dictGetString('netflow_interfaces', 'name', toUInt64(c8))),'(UNDEFINED)')) as `co8`, \r\n\t\tany(ifNull(t.`ClientIp`,'(UNDEFINED)')) as `co9`, \r\n\t\tany(ifNull(t.`ServerIp`,'(UNDEFINED)')) as `co10`, \r\n\t\tany(ifNull(if(c11 = 0, '(UNDEFINED)',dictGetString('classes_of_service', 'name', toUInt64(c11))),'(UNDEFINED)')) as `co11`, \r\n\t\tany(ifNull(if(c12 = 0, '(UNDEFINED)',dictGetString('classes_of_service', 'name', toUInt64(c12))),'(UNDEFINED)')) as `co12`, \r\n\t\tany(ifNull(t.`Port`, 0)) as `co13`, \r\n\t\tany(ifNull(t.`DomainName`,'(UNDEFINED)')) as `co14`, \r\n\t\tany(ifNull(t.`Flags`, 0)) as `co15`, \r\n\t\tmax(t.`ClientNetworkTimeMax`) as `co16`, \r\n\t\tmin(t.`ClientNetworkTimeMin`) as `co17`, \r\n\t\tsum(t.`ClientNetworkTimeSum`) as `co18`, \r\n\t\tsum(t.`ClientPackets`) as `co19`, \r\n\t\tsum(t.`ClientBytes`) as `co20`, \r\n\t\tsum(t.`ClientDataBytes`) as `co21`, \r\n\t\tsum(t.`ClientDataPackets`) as `co22`, \r\n\t\tsum(t.`ServerResponseTimeSum`) as `co23`, \r\n\t\tmin(t.`ServerResponseTimeMin`) as `co24`, \r\n\t\tmax(t.`ServerResponseTimeMax`) as `co25`, \r\n\t\tsum(t.`ServerPackets`) as `co26`, \r\n\t\tsum(t.`ServerBytes`) as `co27`, \r\n\t\tsum(t.`1000msResponsesNb`) as `co28`, \r\n\t\tsum(t.`100msResponsesNb`) as `co29`, \r\n\t\tsum(t.`10msResponsesNb`) as `co30`, \r\n\t\tsum(t.`2msResponsesNb`) as `co31`, \r\n\t\tsum(t.`500msResponsesNb`) as `co32`, \r\n\t\tsum(t.`50msResponsesNb`) as `co33`, \r\n\t\tsum(t.`5msResponsesNb`) as `co34`, \r\n\t\tsum(t.`LateResponsesNb`) as `co35`, \r\n\t\tsum(t.`NewConnectionsNb`) as `co36`, \r\n\t\tmax(t.`ResponseTimeMax`) as `co37`, \r\n\t\tmin(t.`ResponseTimeMin`) as `co38`, \r\n\t\tsum(t.`ResponseTimeSum`) as `co39`, \r\n\t\tsum(t.`ResponsesNb`) as `co40`, \r\n\t\tsum(t.`RetransmissionsNb`) as `co41`, \r\n\t\tsum(t.`ServerDataBytes`) as `co42`, \r\n\t\tsum(t.`ServerDataPackets`) as `co43`, \r\n\t\tmax(t.`ServerNetworkTimeMax`) as `co44`, \r\n\t\tmin(t.`ServerNetworkTimeMin`) as `co45`, \r\n\t\tsum(t.`ServerNetworkTimeSum`) as `co46`, \r\n\t\tmax(t.`TotalNetworkTimeMax`) as `co47`, \r\n\t\tmin(t.`TotalNetworkTimeMin`) as `co48`, \r\n\t\tsum(t.`TotalNetworkTimeSum`) as `co49`, \r\n\t\tmax(t.`TotalResponseTimeMax`) as `co50`, \r\n\t\tmin(t.`TotalResponseTimeMin`) as `co51`, \r\n\t\tsum(t.`TotalResponseTimeSum`) as `co52`, \r\n\t\tmax(t.`TotalTransactionTimeMax`) as `co53`, \r\n\t\tmin(t.`TotalTransactionTimeMin`) as `co54`, \r\n\t\tsum(t.`TotalTransactionTimeSum`) as `co55`, \r\n\t\tsum(t.`TransactionsNb`) as `co56`, \r\n\t\tsum(t.`WaasDreInput`) as `co57`, \r\n\t\tsum(t.`WaasDreOutput`) as `co58`, \r\n\t\tsum(t.`WaasInputBytes`) as `co59`, \r\n\t\tsum(t.`WaasLzInput`) as `co60`, \r\n\t\tsum(t.`WaasLzOutput`) as `co61`, \r\n\t\tsum(t.`WaasOutputBytes`) as `co62`, \r\n\t\tany(t.`c1`) as `co63`, \r\n\t\tany(t.`c2`) as `co64`, \r\n\t\tany(t.`c3`) as `co65`, \r\n\t\tany(t.`c4`) as `co66`, \r\n\t\tany(t.`c5`) as `co67`, \r\n\t\tany(t.`c6`) as `co68`, \r\n\t\tany(t.`c7`) as `co69`, \r\n\t\tany(t.`c8`) as `co70`, \r\n\t\tany(t.`c11`) as `co71`, \r\n\t\tany(t.`c12`) as `co72` ,\r\n\t\tany(ranking_row) as final_ranking\r\nfrom (\r\n\tselect\r\n\t\t\t/* @OUTER_SELECT_PART@ */\r\n\t\t\tc9 as `ClientIp`,\r\n\t\t\tc10 as `ServerIp`,\r\n\t\t\tc13 as `Port`,\r\n\t\t\tc14 as `DomainName`,\r\n\t\t\tc15 as `Flags`,\r\n\t\t\tc16 as `ClientNetworkTimeMax`,\r\n\t\t\tc17 as `ClientNetworkTimeMin`,\r\n\t\t\tc18 as `ClientNetworkTimeSum`,\r\n\t\t\tc19 as `ClientPackets`,\r\n\t\t\tc20 as `ClientBytes`,\r\n\t\t\tc21 as `ClientDataBytes`,\r\n\t\t\tc22 as `ClientDataPackets`,\r\n\t\t\tc23 as `ServerResponseTimeSum`,\r\n\t\t\tc24 as `ServerResponseTimeMin`,\r\n\t\t\tc25 as `ServerResponseTimeMax`,\r\n\t\t\tc26 as `ServerPackets`,\r\n\t\t\tc27 as `ServerBytes`,\r\n\t\t\tc28 as `1000msResponsesNb`,\r\n\t\t\tc29 as `100msResponsesNb`,\r\n\t\t\tc30 as `10msResponsesNb`,\r\n\t\t\tc31 as `2msResponsesNb`,\r\n\t\t\tc32 as `500msResponsesNb`,\r\n\t\t\tc33 as `50msResponsesNb`,\r\n\t\t\tc34 as `5msResponsesNb`,\r\n\t\t\tc35 as `LateResponsesNb`,\r\n\t\t\tc36 as `NewConnectionsNb`,\r\n\t\t\tc37 as `ResponseTimeMax`,\r\n\t\t\tc38 as `ResponseTimeMin`,\r\n\t\t\tc39 as `ResponseTimeSum`,\r\n\t\t\tc40 as `ResponsesNb`,\r\n\t\t\tc41 as `RetransmissionsNb`,\r\n\t\t\tc42 as `ServerDataBytes`,\r\n\t\t\tc43 as `ServerDataPackets`,\r\n\t\t\tc44 as `ServerNetworkTimeMax`,\r\n\t\t\tc45 as `ServerNetworkTimeMin`,\r\n\t\t\tc46 as `ServerNetworkTimeSum`,\r\n\t\t\tc47 as `TotalNetworkTimeMax`,\r\n\t\t\tc48 as `TotalNetworkTimeMin`,\r\n\t\t\tc49 as `TotalNetworkTimeSum`,\r\n\t\t\tc50 as `TotalResponseTimeMax`,\r\n\t\t\tc51 as `TotalResponseTimeMin`,\r\n\t\t\tc52 as `TotalResponseTimeSum`,\r\n\t\t\tc53 as `TotalTransactionTimeMax`,\r\n\t\t\tc54 as `TotalTransactionTimeMin`,\r\n\t\t\tc55 as `TotalTransactionTimeSum`,\r\n\t\t\tc56 as `TransactionsNb`,\r\n\t\t\tc57 as `WaasDreInput`,\r\n\t\t\tc58 as `WaasDreOutput`,\r\n\t\t\tc59 as `WaasInputBytes`,\r\n\t\t\tc60 as `WaasLzInput`,\r\n\t\t\tc61 as `WaasLzOutput`,\r\n\t\t\tc62 as `WaasOutputBytes`,\r\n\t\t\tc1 as `c1`,\r\n\t\t\tc2 as `c2`,\r\n\t\t\tc3 as `c3`,\r\n\t\t\tc4 as `c4`,\r\n\t\t\tc5 as `c5`,\r\n\t\t\tc6 as `c6`,\r\n\t\t\tc7 as `c7`,\r\n\t\t\tc8 as `c8`,\r\n\t\t\tc11 as `c11`,\r\n\t\t\tc12 as `c12`,\r\n\t\t\trowNumberInAllBlocks() as ranking_row\r\n\tfrom\r\n\t\t(select \r\n\t\t\t\t/* @AGGR_OUT_PART@ */\r\n\t\t\t\taggr_in.`Agent` as c1,\r\n\t\t\t\taggr_in.`Application` as c2,\r\n\t\t\t\taggr_in.`Source` as c3,\r\n\t\t\t\taggr_in.`Protocol` as c4,\r\n\t\t\t\taggr_in.`ClientOffice` as c5,\r\n\t\t\t\taggr_in.`ServerOffice` as c6,\r\n\t\t\t\taggr_in.`ClientInterface` as c7,\r\n\t\t\t\taggr_in.`ServerInterface` as c8,\r\n\t\t\t\taggr_in.`ClientIp` as c9,\r\n\t\t\t\taggr_in.`ServerIp` as c10,\r\n\t\t\t\taggr_in.`ClientCos` as c11,\r\n\t\t\t\taggr_in.`ServerCos` as c12,\r\n\t\t\t\taggr_in.`Port` as c13,\r\n\t\t\t\taggr_in.`DomainName` as c14,\r\n\t\t\t\taggr_in.`Flags` as c15,\r\n\t\t\t\tmax(aggr_in.`ClientNetworkTimeMax`) as c16,\r\n\t\t\t\tmin(aggr_in.`ClientNetworkTimeMin`) as c17,\r\n\t\t\t\tsum(aggr_in.`ClientNetworkTimeSum`) as c18,\r\n\t\t\t\tsum(aggr_in.`ClientPackets`) as c19,\r\n\t\t\t\tsum(aggr_in.`ClientBytes`) as c20,\r\n\t\t\t\tsum(aggr_in.`ClientDataBytes`) as c21,\r\n\t\t\t\tsum(aggr_in.`ClientDataPackets`) as c22,\r\n\t\t\t\tsum(aggr_in.`ServerResponseTimeSum`) as c23,\r\n\t\t\t\tmin(aggr_in.`ServerResponseTimeMin`) as c24,\r\n\t\t\t\tmax(aggr_in.`ServerResponseTimeMax`) as c25,\r\n\t\t\t\tsum(aggr_in.`ServerPackets`) as c26,\r\n\t\t\t\tsum(aggr_in.`ServerBytes`) as c27,\r\n\t\t\t\tsum(aggr_in.`1000msResponsesNb`) as c28,\r\n\t\t\t\tsum(aggr_in.`100msResponsesNb`) as c29,\r\n\t\t\t\tsum(aggr_in.`10msResponsesNb`) as c30,\r\n\t\t\t\tsum(aggr_in.`2msResponsesNb`) as c31,\r\n\t\t\t\tsum(aggr_in.`500msResponsesNb`) as c32,\r\n\t\t\t\tsum(aggr_in.`50msResponsesNb`) as c33,\r\n\t\t\t\tsum(aggr_in.`5msResponsesNb`) as c34,\r\n\t\t\t\tsum(aggr_in.`LateResponsesNb`) as c35,\r\n\t\t\t\tsum(aggr_in.`NewConnectionsNb`) as c36,\r\n\t\t\t\tmax(aggr_in.`ResponseTimeMax`) as c37,\r\n\t\t\t\tmin(aggr_in.`ResponseTimeMin`) as c38,\r\n\t\t\t\tsum(aggr_in.`ResponseTimeSum`) as c39,\r\n\t\t\t\tsum(aggr_in.`ResponsesNb`) as c40,\r\n\t\t\t\tsum(aggr_in.`RetransmissionsNb`) as c41,\r\n\t\t\t\tsum(aggr_in.`ServerDataBytes`) as c42,\r\n\t\t\t\tsum(aggr_in.`ServerDataPackets`) as c43,\r\n\t\t\t\tmax(aggr_in.`ServerNetworkTimeMax`) as c44,\r\n\t\t\t\tmin(aggr_in.`ServerNetworkTimeMin`) as c45,\r\n\t\t\t\tsum(aggr_in.`ServerNetworkTimeSum`) as c46,\r\n\t\t\t\tmax(aggr_in.`TotalNetworkTimeMax`) as c47,\r\n\t\t\t\tmin(aggr_in.`TotalNetworkTimeMin`) as c48,\r\n\t\t\t\tsum(aggr_in.`TotalNetworkTimeSum`) as c49,\r\n\t\t\t\tmax(aggr_in.`TotalResponseTimeMax`) as c50,\r\n\t\t\t\tmin(aggr_in.`TotalResponseTimeMin`) as c51,\r\n\t\t\t\tsum(aggr_in.`TotalResponseTimeSum`) as c52,\r\n\t\t\t\tmax(aggr_in.`TotalTransactionTimeMax`) as c53,\r\n\t\t\t\tmin(aggr_in.`TotalTransactionTimeMin`) as c54,\r\n\t\t\t\tsum(aggr_in.`TotalTransactionTimeSum`) as c55,\r\n\t\t\t\tsum(aggr_in.`TransactionsNb`) as c56,\r\n\t\t\t\tsum(aggr_in.`WaasDreInput`) as c57,\r\n\t\t\t\tsum(aggr_in.`WaasDreOutput`) as c58,\r\n\t\t\t\tsum(aggr_in.`WaasInputBytes`) as c59,\r\n\t\t\t\tsum(aggr_in.`WaasLzInput`) as c60,\r\n\t\t\t\tsum(aggr_in.`WaasLzOutput`) as c61,\r\n\t\t\t\tsum(aggr_in.`WaasOutputBytes`) as c62\r\n\t\tfrom (\r\n\t\t\tselect\r\n\t\t\t\t\t/* @AGGR_IN_PART@ */\r\n\t\t\t\t\ttoStartOfMinute(data_table.`Timestamp`, 'Europe/Paris') as Timestamp,\r\n\t\t\t\t\tmax(data_table.`ClientNetworkTimeMax`) as `ClientNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ClientNetworkTimeMin`) as `ClientNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ClientNetworkTimeSum`) as `ClientNetworkTimeSum`,\r\n\t\t\t\t\tsum(data_table.`ClientPackets`) as `ClientPackets`,\r\n\t\t\t\t\tsum(data_table.`ClientBytes`) as `ClientBytes`,\r\n\t\t\t\t\tsum(data_table.`ClientDataBytes`) as `ClientDataBytes`,\r\n\t\t\t\t\tsum(data_table.`ClientDataPackets`) as `ClientDataPackets`,\r\n\t\t\t\t\tsum(data_table.`ServerResponseTimeSum`) as `ServerResponseTimeSum`,\r\n\t\t\t\t\tmin(data_table.`ServerResponseTimeMin`) as `ServerResponseTimeMin`,\r\n\t\t\t\t\tmax(data_table.`ServerResponseTimeMax`) as `ServerResponseTimeMax`,\r\n\t\t\t\t\tsum(data_table.`ServerPackets`) as `ServerPackets`,\r\n\t\t\t\t\tsum(data_table.`ServerBytes`) as `ServerBytes`,\r\n\t\t\t\t\tsum(data_table.`1000msResponsesNb`) as `1000msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`100msResponsesNb`) as `100msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`10msResponsesNb`) as `10msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`2msResponsesNb`) as `2msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`500msResponsesNb`) as `500msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`50msResponsesNb`) as `50msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`5msResponsesNb`) as `5msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`LateResponsesNb`) as `LateResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`NewConnectionsNb`) as `NewConnectionsNb`,\r\n\t\t\t\t\tmax(data_table.`ResponseTimeMax`) as `ResponseTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ResponseTimeMin`) as `ResponseTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ResponseTimeSum`) as `ResponseTimeSum`,\r\n\t\t\t\t\tsum(data_table.`ResponsesNb`) as `ResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`RetransmissionsNb`) as `RetransmissionsNb`,\r\n\t\t\t\t\tsum(data_table.`ServerDataBytes`) as `ServerDataBytes`,\r\n\t\t\t\t\tsum(data_table.`ServerDataPackets`) as `ServerDataPackets`,\r\n\t\t\t\t\tmax(data_table.`ServerNetworkTimeMax`) as `ServerNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ServerNetworkTimeMin`) as `ServerNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ServerNetworkTimeSum`) as `ServerNetworkTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalNetworkTimeMax`) as `TotalNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalNetworkTimeMin`) as `TotalNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalNetworkTimeSum`) as `TotalNetworkTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalResponseTimeMax`) as `TotalResponseTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalResponseTimeMin`) as `TotalResponseTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalResponseTimeSum`) as `TotalResponseTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalTransactionTimeMax`) as `TotalTransactionTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalTransactionTimeMin`) as `TotalTransactionTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalTransactionTimeSum`) as `TotalTransactionTimeSum`,\r\n\t\t\t\t\tsum(data_table.`TransactionsNb`) as `TransactionsNb`,\r\n\t\t\t\t\tsum(data_table.`WaasDreInput`) as `WaasDreInput`,\r\n\t\t\t\t\tsum(data_table.`WaasDreOutput`) as `WaasDreOutput`,\r\n\t\t\t\t\tsum(data_table.`WaasInputBytes`) as `WaasInputBytes`,\r\n\t\t\t\t\tsum(data_table.`WaasLzInput`) as `WaasLzInput`,\r\n\t\t\t\t\tsum(data_table.`WaasLzOutput`) as `WaasLzOutput`,\r\n\t\t\t\t\tsum(data_table.`WaasOutputBytes`) as `WaasOutputBytes`,\r\n\t\t\t\t\tdata_table.`Agent` as `Agent`,\r\n\t\t\t\t\tdata_table.`Application` as `Application`,\r\n\t\t\t\t\tdata_table.`Source` as `Source`,\r\n\t\t\t\t\tdata_table.`Protocol` as `Protocol`,\r\n\t\t\t\t\tdata_table.`ClientOffice` as `ClientOffice`,\r\n\t\t\t\t\tdata_table.`ServerOffice` as `ServerOffice`,\r\n\t\t\t\t\tdata_table.`ClientInterface` as `ClientInterface`,\r\n\t\t\t\t\tdata_table.`ServerInterface` as `ServerInterface`,\r\n\t\t\t\t\tdata_table.`ClientIp` as `ClientIp`,\r\n\t\t\t\t\tdata_table.`ServerIp` as `ServerIp`,\r\n\t\t\t\t\tdata_table.`ClientCos` as `ClientCos`,\r\n\t\t\t\t\tdata_table.`ServerCos` as `ServerCos`,\r\n\t\t\t\t\tdata_table.`Port` as `Port`,\r\n\t\t\t\t\tdata_table.`DomainName` as `DomainName`,\r\n\t\t\t\t\tdata_table.`Flags` as `Flags`\r\n\t\t\tfrom\r\n\t\t\t\t\t/* @DATA_TABLE@ */\r\n\t\t\t\t\tavc.topconversationdetails as data_table\r\n\t\t\t\t\t/* @INNER_JOIN_PART@ */\r\n\t\t\t\t\t\r\n\t\t\twhere\r\n\t\t\t\t\t/* @WHERE_PART@ */\r\n\t\t\t\t\tdata_table.`Timestamp` >= toStartOfMinute(toDateTime('2020-01-01 15:54:00','Europe/Paris'), 'Europe/Paris')\r\n\t\t\t\t\tand data_table.`Timestamp` < toStartOfMinute(toDateTime('2020-01-08 15:54:00','Europe/Paris'), 'Europe/Paris')\r\n\t\t\t\t\tand ((toNullable(data_table.`Customer`)= (select a from (select `id` as a from sdm.customers where sdm.customers.`vmId` = 150005) as b)))\r\n\t\t\t\t\tand ((toNullable(data_table.`Service`)= (select a from (select `id` as a from sdm.services where sdm.services.`vmId` = 150006) as b)))\r\n\t\t\tgroup by \r\n\t\t\t\t\t/* @GROUPBY_IN_PART@ */\r\n\t\t\t\t\tTimestamp,\r\n\t\t\t\t\tdata_table.`Agent`,\r\n\t\t\t\t\tdata_table.`Protocol`,\r\n\t\t\t\t\t`ServerIp`,\r\n\t\t\t\t\tdata_table.`ServerCos`,\r\n\t\t\t\t\tdata_table.`ClientOffice`,\r\n\t\t\t\t\tdata_table.`ClientCos`,\r\n\t\t\t\t\t`Flags`,\r\n\t\t\t\t\tdata_table.`ServerInterface`,\r\n\t\t\t\t\tdata_table.`ClientInterface`,\r\n\t\t\t\t\tdata_table.`Application`,\r\n\t\t\t\t\tdata_table.`Source`,\r\n\t\t\t\t\tdata_table.`ServerOffice`,\r\n\t\t\t\t\t`Port`,\r\n\t\t\t\t\t`DomainName`,\r\n\t\t\t\t\t`ClientIp`\r\n\t\t) as aggr_in\r\n\t\t\t/* @MIDDLE_JOIN_PART@ */\r\n\t\t\t\r\n\t\tgroup by\r\n\t\t\t\t/* @GROUPBY_OUT_PART@ */\r\n\t\t\t\taggr_in.`Agent`,\r\n\t\t\t\taggr_in.`Protocol`,\r\n\t\t\t\tc10,\r\n\t\t\t\taggr_in.`ServerCos`,\r\n\t\t\t\taggr_in.`ClientOffice`,\r\n\t\t\t\taggr_in.`ClientCos`,\r\n\t\t\t\tc15,\r\n\t\t\t\taggr_in.`ServerInterface`,\r\n\t\t\t\taggr_in.`ClientInterface`,\r\n\t\t\t\taggr_in.`Application`,\r\n\t\t\t\taggr_in.`Source`,\r\n\t\t\t\taggr_in.`ServerOffice`,\r\n\t\t\t\tc13,\r\n\t\t\t\tc14,\r\n\t\t\t\tc9\r\n\t\torder by\r\n\t\t\t\t/* @ORDERBY_PART@ */\r\n\t\t\t\t( empty(c10) OR c10 = '(UNDEFINED)' OR c10 IS NULL) ASC,\r\n\t\t\t\tc15 IS NULL ASC,\r\n\t\t\t\tc13 IS NULL ASC,\r\n\t\t\t\tc14 IS NULL ASC,\r\n\t\t\t\t( empty(c9) OR c9 = '(UNDEFINED)' OR c9 IS NULL) ASC,\r\n\t\t\t\tc16 desc\r\n\t\t) as aggr_out\r\n\t) as t\r\n\t/* @OUTER_JOIN_PART@ */\r\n\t\r\ngroup by if(ranking_row<100, ranking_row, 101)\r\norder by final_ranking asc\r\n```\r\n\r\nWhere avc.topconversationdetails is a merge tree table, and the other tables are dictionnaries tables.\r\nIt is mostly a top N with others query, with 2 level of aggregation.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8592/comments",
    "author": "edonin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-09T18:27:08Z",
        "body": "723.47 MiB ridiculously unrealistic target. CH was designed to use 10GB as a base point.\r\n\r\ntry `set max_threads=1, max_read_buffer_size=100000, max_compress_block_size=100000, min_compress_block_size=100000`"
      },
      {
        "user": "edonin",
        "created_at": "2020-01-15T09:51:58Z",
        "body": "Thanks for the answer. For me, changing the max_thread to 1 is working. I am able to run my big query on a very small machine."
      }
    ]
  },
  {
    "number": 8531,
    "title": "About deleting new values every day affects performance",
    "created_at": "2020-01-05T09:02:19Z",
    "closed_at": "2020-01-06T01:46:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8531",
    "body": "\r\nA multi-million data table needs to delete a part of the data and add a new part every day. Will this affect the performance of ck query?\r\n\r\n ENGINE = MergeTree() ORDER BA_MONTH",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8531/comments",
    "author": "samz406",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-05T16:27:51Z",
        "body": "It's unclear how are you going to `delete a part`. \r\n`alter table ... drop partition`  does not affect performance."
      },
      {
        "user": "samz406",
        "created_at": "2020-01-06T01:19:42Z",
        "body": "no partition, first use ALTER TABLE table DELETE WHERE BA_MONTH='xxx', and insert data,Will this affect the performance  query?\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-01-06T01:38:45Z",
        "body": "`ALTER TABLE table DELETE` is very heavy operation in comparison with `drop partition` .\r\n`ALTER TABLE table DELETE` causes a huge I/O and CPU usage. After it finishes it does not affect performance.\r\n\r\nConsider to use `alter table ... drop partition` and table with monthly / daily partitioning. \r\n` drop partition` much more faster and more reliable operation. "
      },
      {
        "user": "samz406",
        "created_at": "2020-01-06T01:46:11Z",
        "body": "ok. thank you "
      }
    ]
  },
  {
    "number": 8506,
    "title": "MemoryTracker exception despite unlimited memory setting",
    "created_at": "2020-01-02T21:19:41Z",
    "closed_at": "2020-01-02T22:44:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8506",
    "body": "Received the following exception multiple times during writes to a node (stack trace at end).\r\n\r\n`B::Exception: Memory limit (total) exceeded: would use 74.51 GiB (attempt to allocate chunk of 4217732 bytes), maximum: 74.51 GiB (version 19.17.4.11) `\r\n\r\nI can't figure out what memory limit is being exceeded.  (These errors are happening during writes).  The 74.51GiB value is not configured anywhere, and the box itself has 792G of total memory, of which we are only using a small fraction.  The only configured limit on the default profile is per query of 100GiB `max_memory_usage_per_query = 107374182400`.\r\n\r\nThese errors seem to correspond to large merges; when the merge finally completed the errors cleared up.  Is ClickHouse possibly misreading the total available system memory? \r\n\r\n\r\n> 0. 0x3512b60 StackTrace::StackTrace() /usr/bin/clickhouse\r\n> 1. 0x351195e MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 2. 0x3510d39 MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 3. 0x3510d39 MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 4. 0x3510d39 MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 5. 0x3561138 DB::WriteBufferFromFileBase::WriteBufferFromFileBase(unsigned long, char*, unsigned long) /usr/bin/clickhouse\r\n> 6. 0x35443fb DB::WriteBufferFromFileDescriptor::WriteBufferFromFileDescriptor(int, unsigned long, char*, unsigned long) /usr/bin/clickhouse\r\n> 7. 0x6bf42bc DB::WriteBufferFromFile::WriteBufferFromFile(std::string const&, unsigned long, int, unsigned int, char*, unsigned long) /usr/bin/clickhouse\r\n> 8. 0x6c062d6 DB::createWriteBufferFromFileBase(std::string const&, unsigned long, unsigned long, unsigned long, int, unsigned int, char*, unsigned long) /usr/bin/clickhouse\r\n> 9. 0x696c006 DB::IMergedBlockOutputStream::ColumnStream::ColumnStream(std::string const&, std::string const&, std::string const&, std::string const&, std::string const&, std::shared_ptr<DB::ICompressionCodec> const&, unsigned long, unsigned long, unsigned long) /usr/bin/clickhouse\r\n> 10. 0x696c2e1 ? /usr/bin/clickhouse\r\n> 11. 0x696a3c3 DB::IMergedBlockOutputStream::addStreams(std::string const&, std::string const&, DB::IDataType const&, std::shared_ptr<DB::ICompressionCodec> const&, unsigned long, bool) /usr/bin/clickhouse\r\n> 12. 0x650fc0c DB::MergedBlockOutputStream::MergedBlockOutputStream(DB::MergeTreeData&, std::string const&, DB::NamesAndTypesList const&, std::shared_ptr<DB::ICompressionCodec>, bool) /usr/bin/clickhouse\r\n> 13. 0x64dde7f DB::MergeTreeDataWriter::writeTempPart(DB::BlockWithPartition&) /usr/bin/clickhouse\r\n> 14. 0x651b7c4 DB::ReplicatedMergeTreeBlockOutputStream::write(DB::Block const&) /usr/bin/clickhouse\r\n> 15. 0x67a8726 DB::PushingToViewsBlockOutputStream::write(DB::Block const&) /usr/bin/clickhouse\r\n> 16. 0x67b3f01 DB::SquashingBlockOutputStream::finalize() /usr/bin/clickhouse\r\n> 17. 0x67b41d1 DB::SquashingBlockOutputStream::writeSuffix() /usr/bin/clickhouse\r\n> 18. 0x609d2a5 DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::atomic<bool>*) /usr/bin/clickhouse\r\n> 19. 0x62d73b1 DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::function<void (std::string const&)>, std::function<void (std::string const&)>) /usr/bin/clickhouse\r\n> 20. 0x359e471 DB::HTTPHandler::processQuery(Poco::Net::HTTPServerRequest&, HTMLForm&, Poco::Net::HTTPServerResponse&, DB::HTTPHandler::Output&) /usr/bin/clickhouse\r\n> 21. 0x35a14b1 DB::HTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&) /usr/bin/clickhouse\r\n> 22. 0x6dbcc59 Poco::Net::HTTPServerConnection::run() /usr/bin/clickhouse\r\n> 23. 0x6db98bf Poco::Net::TCPServerConnection::start() /usr/bin/clickhouse\r\n> 24. 0x6db9fb5 Poco::Net::TCPServerDispatcher::run() /usr/bin/clickhouse\r\n> 25. 0x723f481 Poco::PooledThread::run() /usr/bin/clickhouse\r\n> 26. 0x723b208 Poco::ThreadImpl::runnableEntry(void*) /usr/bin/clickhouse\r\n> 27. 0x791d69f ? /usr/bin/clickhouse\r\n> 28. 0x7f5f8e934dd5 start_thread /usr/lib64/libpthread-2.17.so\r\n> 29. 0x7f5f8e459ead __clone /usr/lib64/libc-2.17.so\r\n> ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8506/comments",
    "author": "genzgd",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-02T21:41:33Z",
        "body": "There is no such parameter `max_memory_usage_per_query`\r\n\r\nCheck your settings\r\n```\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE name LIKE 'max%mem%'\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_memory_usage                 \u2502 50659012608 \u2502\r\n\u2502 max_memory_usage_for_user        \u2502 0           \u2502\r\n\u2502 max_memory_usage_for_all_queries \u2502 50659012608 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "genzgd",
        "created_at": "2020-01-02T22:08:09Z",
        "body": "Sorry, I was thinking of it as max_memory_usage \"per_query\" since that's how it's referenced in the documentation and in error logs:\r\n\r\n```\r\nSELECT \r\n    name, \r\n    value\r\nFROM system.settings\r\nWHERE name LIKE 'max%mem%'\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_memory_usage                 \u2502 107374182400 \u2502\r\n\u2502 max_memory_usage_for_user        \u2502 0            \u2502\r\n\u2502 max_memory_usage_for_all_queries \u2502 0            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 0.002 sec. ```\r\n"
      },
      {
        "user": "genzgd",
        "created_at": "2020-01-02T22:22:31Z",
        "body": "To be clear, I assumed that settings was not being referenced, since error messages for that setting look like `Memory limit (for query) exceeded`, not `Memory limit (total) exceeded`.  Examples of the \"total\" memory limit being exceeded are very rare based on my Google search."
      },
      {
        "user": "den-crane",
        "created_at": "2020-01-02T22:22:48Z",
        "body": ">B::Exception: Memory limit (total) exceeded: would use 74.51 GiB (attempt to allocate chunk of >4217732 bytes), maximum: 74.51 GiB (version 19.17.4.11) \r\n\r\n**(total) exceeded**\r\nIt could be a problem from other sessions which set max_memory_usage_for_all_queries=74GB because CH has an issue with `max_memory_usage_for_all_queries`"
      },
      {
        "user": "genzgd",
        "created_at": "2020-01-02T22:30:51Z",
        "body": "That actually looks like the problem, we do have a different profile which just happens to have a 74.51 GB value for that `max_memory_usage_for_all_queries` setting.  Do you have a link to that issue you mentioned?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-01-02T22:38:28Z",
        "body": "I don't remember the issue number.\r\n\r\nSTR:\r\n\r\n```\r\nfor i in `seq 1 5000`; do echo -n \" Result: \"; clickhouse-client --max_memory_usage_for_all_queries=100000 -q \"select sleep(1)\"; done\r\n\r\nclickhouse-client --max_memory_usage_for_all_queries=0 --max_memory_usage=0 --max_memory_usage_for_user=0 -q \"create table X Engine=Memory as select * from numbers(10000000000);\"\r\nReceived exception from server (version 19.17.5):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (total) exceeded: would use 4.00 MiB (attempt to allocate chunk of 4195072 bytes), maximum: 97.66 KiB.\r\n\r\n```\r\n\r\nmax_memory_usage_for_all_queries -- Maximum memory usage for processing all concurrently running queries on the server.\r\n\r\nTry `max_memory_usage_for_user` instead of `max_memory_usage_for_all_queries`"
      },
      {
        "user": "genzgd",
        "created_at": "2020-01-02T22:44:10Z",
        "body": "Will do,thanks so much!"
      }
    ]
  },
  {
    "number": 8228,
    "title": "mysql connection in clickhouse",
    "created_at": "2019-12-16T07:32:32Z",
    "closed_at": "2019-12-23T18:47:54Z",
    "labels": [
      "question",
      "comp-foreign-db"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8228",
    "body": "I'm using clickhouse for a while now. I have inserted 1 million records so far and I intend to add to it to about 100 billion. It's blazing fast, and I like how it compresses data. \r\n\r\nThe problem is that it keeps throwing an error every now and then, for example when I just login to clickhouse client:\r\n\r\nCannot load data for command line suggestions: Code: 1000, e.displayText() = DB::Exception: Received from localhost:9000. DB::Exception: mysqlxx::ConnectionFailed: Unknown MySQL server host 'host' (-2) ((nullptr):0). (version 19.17.5.18 (official build))\r\n\r\nFor doing ordinary tasks it seems to not affect the performance, but the main problem is that when I want to get partitions using command:\r\n\r\n`SELECT partition FROM system.parts WHERE table='bars'`\r\n\r\nagain it throws the same exception. I went through the documentation, but I couldn't find a solution.\r\n\r\nAny help would be appreciated...\r\n\r\nPS: I used: Engine = MergeTree() Partition by isin Order by time primary key time",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8228/comments",
    "author": "ashkank66",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-12-16T13:39:58Z",
        "body": "It probably means that you have table with `MySQL` engine which can't connect to MySQL.\r\nIt also strange that we have `nullptr` in error message. May be a misconfiguration. \r\n\r\nCan you please check that all you MySQL configurations are correct?\r\nAnd also find full stacktrace after this error in logs?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-16T19:17:50Z",
        "body": "> Unknown MySQL server host 'host' (-2) ((nullptr):0)\r\n\r\nProbably you have erroneously specified `host` as hostname for MySQL server, like this:\r\n`<host>host</host>`\r\n\r\nThe `(nullptr):0` part is Ok - it's what we have as the error message from the library."
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-17T05:43:54Z",
        "body": "I actually haven't configured MySQL on my clickhouse, and to be honest, I have to admit I tried to find a configuration for MySQL but I couldn't.\r\nCould you tell me where should I configure it?"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-17T07:17:18Z",
        "body": "2019.12.17 10:46:30.000314 [ 44 ] {} <Information> Application: MYSQL: Connecting to database@host:0 as user user\r\n2019.12.17 10:46:30.001630 [ 44 ] {} <Error> Application: mysqlxx::ConnectionFailed\r\n2019.12.17 10:46:30.001943 [ 44 ] {} <Error> void DB::AsynchronousMetrics::run(): Poco::Exception. Code: 1000, e.code() = 2005, e.displayText() = mysqlxx::ConnectionFailed: Unknown MySQL server host 'host' (-2) ((nullptr):0) (version 19.17.5.18 (official build)\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-17T16:52:08Z",
        "body": "It looks like you have configured MySQL table actually.\r\n\r\n```\r\ngrep -r -i mysql /etc/clickhouse-server/\r\ngrep -i mysql /etc/metrika.xml\r\ngrep -r -i mysql /var/lib/clickhouse/\r\n```"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-18T09:58:43Z",
        "body": "This is all the responses:\r\n\r\nroot@ashkanPC:/home/ashkan# grep -r -i mysql /etc/clickhouse-server/\r\n/etc/clickhouse-server/users.xml:                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).\r\nroot@ashkanPC:/home/ashkan# grep -i mysql /etc/metrika.xml\r\ngrep: /etc/metrika.xml: No such file or directory\r\nroot@ashkanPC:/home/ashkan# grep -r -i mysql /var/lib/clickhouse/\r\n/var/lib/clickhouse/preprocessed_configs/users.xml:                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:       /etc/clickhouse-server/mysql_dictionary.xml      -->\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:    <comment>This dictionary is set to connect clickhouse to mysql</comment>\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:\t  <mysql>\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:\t  </mysql>\r\n/var/lib/clickhouse/metadata/db_name.sql:ENGINE = MySQL('host:port', 'database', 'user', 'password')\r\n"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-18T10:00:29Z",
        "body": "I created a file mysql_dictionary in hope of getting rid of the error, but no proper result, so I deleted it later"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2019-12-18T10:51:56Z",
        "body": "> /var/lib/clickhouse/metadata/db_name.sql:ENGINE = MySQL('host:port', 'database', 'user', 'password')\r\n\r\nThat means that you have `MySQL` database with name `db_name`, which has incorrect configuration (instead of `'host:port', 'database', 'user', 'password'` must be real values). And this database can't connect to MySql server.\r\n\r\nYou can just run `DROP DATABASE db_name` to remove it.\r\n"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-21T05:25:32Z",
        "body": "That's right, thank you."
      }
    ]
  },
  {
    "number": 8122,
    "title": "OPTIMIZE FINAL makes skip index no longer work",
    "created_at": "2019-12-10T13:33:11Z",
    "closed_at": "2019-12-11T04:09:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8122",
    "body": "**Describe the bug or unexpected behaviour**\r\noptimize final makes skip index no longer work\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n19.17.4\r\n* Steps to reproduce\r\n```sql\r\nset allow_experimental_data_skipping_indices=1;\r\ncreate table test(I Int64, S String, INDEX s_index (S) TYPE bloom_filter() GRANULARITY 8192) Engine=MergeTree order by I;\r\ninsert into test select number, toString(rand()) from numbers(10000000);\r\ninsert into test values(45645645, '666');\r\nSET send_logs_level = 'trace';\r\nselect * from test where S = '666';\r\n```\r\n\r\nThis is the correct behavior before `optimize final`: 1 marks to read from 1 ranges, Read 1 rows\r\n```\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393157 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> executeQuery: (from 127.0.0.1:36838) SELECT * FROM test WHERE S = '666'\r\n\u2192 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) [bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393593 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"S = '666'\" moved to PREWHERE\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393803 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Key condition: unknown\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.401436 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.402200 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.402954 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.403693 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404496 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404563 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 0 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404598 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Selected 6 parts by date, 1 parts by key, 1 marks to read from 1 ranges\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404671 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Trace> default.test (SelectExecutor): Reading approx. 8192 rows with 1 streams\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404745 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404813 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Expression\r\n  MergeTreeThread\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.405284 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Information> executeQuery: Read 1 rows, 20.00 B in 0.012 sec., 82 rows/sec., 1.62 KiB/sec.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.405305 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> MemoryTracker: Peak memory usage (for query): 10.04 MiB.\r\n\r\n1 rows in set. Elapsed: 0.013 sec.\r\n```\r\n\r\nAfter `optimize table test final`\r\n```sql\r\nselect * from test where S = '666';\r\n```\r\n\r\nthis behavior is unexpected: 1221 marks to read from 1 ranges, Read 10000001 rows\r\n```\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389243 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> executeQuery: (from 127.0.0.1:36838) SELECT * FROM test WHERE S = '666'\r\n\u2197 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) [bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389696 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"S = '666'\" moved to PREWHERE\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389902 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Key condition: unknown\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398603 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 0 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398652 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1221 marks to read from 1 ranges\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398716 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> default.test (SelectExecutor): Reading approx. 10002432 rows with 24 streams\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398974 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.402274 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> executeQuery: Query pipeline:\r\nUnion\r\n Expression \u00d7 24\r\n  Expression\r\n   MergeTreeThread\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418648 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418689 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> UnionBlockInputStream: Waited for threads to finish\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418751 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Information> executeQuery: Read 10000001 rows, 177.89 MiB in 0.029 sec., 339693250 rows/sec., 5.90 GiB/sec.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418778 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> MemoryTracker: Peak memory usage (for query): 12.83 MiB.\r\n\r\n1 rows in set. Elapsed: 0.030 sec. Processed 10.00 million rows, 186.54 MB (329.23 million rows/s., 6.14 GB/s.)\r\n```\r\n\r\n**Expected behavior**\r\nOnly 1 mark to read, but 1221 marks to read\r\nOnly 1 Row should be read, but 10000001 rows were read",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8122/comments",
    "author": "kaijianding",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-12-10T16:53:40Z",
        "body": "First of all -- GRANULARITY 8192 is a nonsense. Your index granula will contain 8192*8192 rows.\r\nTry GRANULARITY 2.\r\n\r\nSecond. Before optimize 666 is stored in a separate part [insert into test values(45645645, '666');]. Only this Skip index's granula contains this value 666. Other parts don't. After optimize this 666 will be in a huge granula which points to 8192*8192 = 67108864 rows.\r\n"
      },
      {
        "user": "amosbird",
        "created_at": "2019-12-11T02:52:20Z",
        "body": "I always find the term \"granularity\" to be overly used. We have `index_granularity` meaning the max row number of a granule, and we have index granularity meaning the granules one index unit covers. "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-11T03:36:10Z",
        "body": "@amosbird Possible solutions:\r\n- rename GRANULARITY to GRANULARITY FACTOR and support old syntax only for ATTACH queries;\r\n- remove GRANULARITY from documentation example (so it will be 1 by default) and only briefly mention it;"
      },
      {
        "user": "kaijianding",
        "created_at": "2019-12-11T04:09:58Z",
        "body": "@den-crane thanks, it works after change GRANULARITY 8192 to GRANULARITY 1. \r\nclose this issue"
      }
    ]
  },
  {
    "number": 8121,
    "title": "\"Too many open files\" while loading data into table",
    "created_at": "2019-12-10T13:19:59Z",
    "closed_at": "2020-05-17T15:54:31Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8121",
    "body": "Am getting the below error while loading data and only half of the data is being loaded into the table \r\n\r\nDB::Exception: Cannot open file /t-3tb-data/clickhouse/data/database/table/tmp_insert_0c87b3bf0c31a7766299a14d202c8da9_648_648_0/TI_verification_status.mrk, errno: 24, strerror: Too many open files.\r\n\r\nCan someone help me quickly here.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8121/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "byx313",
        "created_at": "2019-12-10T14:25:32Z",
        "body": "> Am getting the below error while loading data and only half of the data is being loaded into the table\r\n> \r\n> DB::Exception: Cannot open file /t-3tb-data/clickhouse/data/database/table/tmp_insert_0c87b3bf0c31a7766299a14d202c8da9_648_648_0/TI_verification_status.mrk, errno: 24, strerror: Too many open files.\r\n> \r\n> Can someone help me quickly here.\r\n\r\nYou got too many files in OS.\r\nMethod 1,increase open files limit\r\ncheck open files \r\n> ulimit -a\r\n\r\nincrease open files \r\n> ulimit -n 65536\r\n\r\nMethod 2,increase messege count in one batch/one insert operation.\r\n"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T14:51:36Z",
        "body": "@byx313 \r\nI tried the first option but the problem is still same :(\r\nMethod2: You mean to say single insert will do than multiple inserts ?"
      },
      {
        "user": "byx313",
        "created_at": "2019-12-10T14:55:41Z",
        "body": "> @byx313\r\n> I tried the first option but the problem is still same :(\r\n> Method2: You mean to say single insert will do than multiple inserts ?\r\n\r\nDo 'ulimit -a' again to check whether the operation work.\r\n\r\n> Method2: You mean to say single insert will do than multiple inserts ?\r\n\r\nYes.10w message a batch a insert is better than 1w message * 10 concurrent insert"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T14:58:12Z",
        "body": "@byx313\r\nyes, I did ulimit -a to check and yes the change is reflected."
      },
      {
        "user": "byx313",
        "created_at": "2019-12-10T15:00:03Z",
        "body": "> @byx313\r\n> yes, I did ulimit -a to check and yes the change is reflected.\r\n\r\nmay be you should try to change you insert frequency.What's the frequency now?"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T15:06:40Z",
        "body": "@byx313\r\nam loading one file after the other , once the first file is loaded starting with the next one."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-10T20:09:16Z",
        "body": "This happens because you are using too granular partition key in a table.\r\nSolution: do not use `PARTITION BY`."
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-11T09:40:06Z",
        "body": "@alexey-milovidov  I have partitioned the table on state code which has some 60 values \r\n\r\nSo if I don't use the PARTITION BY  it doesn't have impact on queries??"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-11T10:03:48Z",
        "body": "@alexey-milovidov Yes, I removed the PARTITION BY and without any error I could load the data :)\r\nMy worry is query returning time. \r\nThank you."
      },
      {
        "user": "filimonov",
        "created_at": "2019-12-12T00:23:19Z",
        "body": "> @byx313\r\n> yes, I did ulimit -a to check and yes the change is reflected.\r\n\r\nAlso for clickhouse user? What is your OS? How did you install/run clickhouse?\r\n\r\nI'm asking because official packages should extend that limit during installation, and 9fficial docker readme mentions how to increase max number of opened files for clickhouse. "
      }
    ]
  },
  {
    "number": 8017,
    "title": "What it depends on that ClickHouse will trigger DELETE/UPDATE syntaxs",
    "created_at": "2019-12-03T23:03:28Z",
    "closed_at": "2019-12-05T23:13:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8017",
    "body": "When I used the \r\n\r\n> ALTER TABLE [db.]table DELETE WHERE filter_expr\r\n\r\nI found out that it took so many hours to delete all data.\r\nI know the delete/update process is running in background,so I would like to know the mechanism that what it depends on that ClickHouse will trigger the operation?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8017/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-12-04T04:07:00Z",
        "body": "CH starts to execute`Alter delete`  in background  right after you called it (if it is not blocked by other running delete or a running merge). It takes so long because it rewrites all data.\r\n\r\nTry `truncate table` or `drop partition` if you need to remove all rows."
      },
      {
        "user": "byx313",
        "created_at": "2019-12-04T05:15:00Z",
        "body": "> CH starts to execute`Alter delete` in background right after you called it (if it is not blocked by other running delete or a running merge). It takes so long because it rewrites all data.\r\n> \r\n> Try `truncate table` or `drop partition` if you need to remove all rows.\r\n\r\nDo you mean that \uff081\uff09CH get all data \uff082\uff09 check data whether meet the delete expression \uff083\uff09delete the old data file & rewrite the left data to the disk?"
      },
      {
        "user": "den-crane",
        "created_at": "2019-12-04T16:38:36Z",
        "body": "Yes. CH evaluates which parts should be mutated (using where conditions) and do a special merge for those parts (CH writes each parts' columns to a new file excluding removed rows -> replaces old parts with new)."
      },
      {
        "user": "byx313",
        "created_at": "2019-12-05T23:13:17Z",
        "body": "thx you so much!"
      }
    ]
  },
  {
    "number": 7926,
    "title": "readonly setting - help",
    "created_at": "2019-11-26T10:57:56Z",
    "closed_at": "2019-11-27T13:28:45Z",
    "labels": [
      "question",
      "operations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7926",
    "body": "in users.xml, \r\n1) I created a new profile with readonly as \r\n```xml\r\n<read>\r\n    <readonly>\r\n            <readonly>1</readonly>\r\n    </readonly>\r\n</read>\r\n```\r\n2) created a new user assigning readonly profile\r\n```xml\r\n<dbread>\r\n    <password>password</password>\r\n    <profile>read</profile>\r\n    <quota>default</quota>\r\n    <networks incl=\"networks\" replace=\"replace\">\r\n       <ip>::/0</ip> \r\n    </networks>\r\n    <readonly>\r\n            <readonly>1</readonly>\r\n    </readonly>\r\n</dbread>\r\n```\r\nlogged in as same user(dbread/password), but I can create and drop table. I am not sure whether I have missed anything.\r\n\r\nalso help me do set `allow_ddl=0`, so that user can not generate DDLs.\r\n\r\nThanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7926/comments",
    "author": "viputh6",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-26T14:38:41Z",
        "body": "```\r\n<profiles>\r\n        <read>\r\n            <readonly>1</readonly>\r\n        </read>\r\n....\r\n</profiles>\r\n\r\n<users>\r\n   <dbread>\r\n                 <profile>read</profile>\r\n            ....\r\n    </dbread>\r\n...\r\n<users>\r\n\r\n```"
      },
      {
        "user": "viputh6",
        "created_at": "2019-11-26T15:57:24Z",
        "body": "thanks a lot. readonly is working now.\r\n\r\ncan you help me to set allow_ddl=0? how to disable DDL generation for a user?"
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-26T16:16:51Z",
        "body": "```\r\n<profiles>\r\n        <read>\r\n            <readonly>1</readonly>\r\n          <allow_ddl>0</allow_ddl>\r\n        </read>\r\n....\r\n</profiles>\r\n\r\n```"
      },
      {
        "user": "viputh6",
        "created_at": "2019-11-27T05:29:04Z",
        "body": "Thanks denis."
      }
    ]
  },
  {
    "number": 7917,
    "title": "Confusion about compression",
    "created_at": "2019-11-25T23:55:52Z",
    "closed_at": "2019-11-26T01:35:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7917",
    "body": "When I go deep into ClickHouse compression,I got some questions.Hope to get the reply\r\n(1)Q1\r\n`<compression incl=\"clickhouse_compression\">\r\n    <case>\r\n        <min_part_size>10000000000</min_part_size>\r\n        <min_part_size_ratio>0.01</min_part_size_ratio>\r\n        <method>zstd</method>\r\n    </case>\r\n</compression>\r\n`\r\n\r\n> ClickHouse checks min_part_size and min_part_size_ratio and processes the case blocks that match these conditions. If none of the <case> matches, ClickHouse applies the lz4 compression algorithm\r\n\r\nIs it right that when I do a INSERT query and the data size is greater than 100MB(10000000000 *  0.01),the zstd compression algorithm will be used?What should I do if I would like to change the default algorithm to zstd not lz4?\r\n(2)Q2\r\n`value Float32 CODEC(Delta, ZSTD)`\r\nThe pipeline codec really confuse me.The second parameter is the compression algorithm,in my opinion,the first parameter is used to show the column data type clearly and help to get a better compression ratio and speed?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7917/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-26T00:42:35Z",
        "body": ">What should I do if I would like to change the default algorithm to zstd not lz4?\r\n\r\n```\r\n<compression>\r\n<case>\r\n<min_part_size>0</min_part_size>         \r\n<min_part_size_ratio>0</min_part_size_ratio>    \r\n<method>zstd</method> \r\n</case>\r\n</compression> \r\n```\r\n-------------------\r\n\r\n>Is it right that when I do a INSERT query and the data size is greater than 100MB(10000000000 * 0.01),\r\n\r\nRight. But also merge process able to compress two small (Lz4) parts with size less 100 to a new part with Zstd (because of this case rules).\r\n\r\n-------------------\r\n\r\n>value Float32 CODEC(Delta, ZSTD)\r\n>The pipeline codec really confuse me.The second parameter is the compression algorithm,in my >opinion,the first parameter is used to show the column data type clearly and help to get a better >compression ratio and speed?\r\n\r\ncorrect. Also try Gorilla CODEC for Float32\r\n"
      },
      {
        "user": "byx313",
        "created_at": "2019-11-26T01:35:17Z",
        "body": "> > What should I do if I would like to change the default algorithm to zstd not lz4?\r\n> \r\n> ```\r\n> <compression>\r\n> <case>\r\n> <min_part_size>0</min_part_size>         \r\n> <min_part_size_ratio>0</min_part_size_ratio>    \r\n> <method>zstd</method> \r\n> </case>\r\n> </compression> \r\n> ```\r\n> \r\n> > Is it right that when I do a INSERT query and the data size is greater than 100MB(10000000000 * 0.01),\r\n> \r\n> Right. But also merge process able to compress two small (Lz4) parts with size less 100 to a new part with Zstd (because of this case rules).\r\n> \r\n> > value Float32 CODEC(Delta, ZSTD)\r\n> > The pipeline codec really confuse me.The second parameter is the compression algorithm,in my >opinion,the first parameter is used to show the column data type clearly and help to get a better >compression ratio and speed?\r\n> \r\n> correct. Also try Gorilla CODEC for Float32\r\n\r\nthx!"
      }
    ]
  },
  {
    "number": 7888,
    "title": "some users have query_log and some don't",
    "created_at": "2019-11-22T07:32:32Z",
    "closed_at": "2019-11-25T13:06:34Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7888",
    "body": "I can get query log from `system.query_log` by users who execute query from `tabix` .\r\nbut users who execute query by `official jdbc`, I can't find their query_log.\r\n\r\nIs there anything wrong?\r\nI am so confused.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7888/comments",
    "author": "Tasselmi",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2019-11-22T13:20:45Z",
        "body": "There is setting `log_queries`, which enables query logging (disabled by default) and it may have different values for different users and profiles."
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-11-23T02:26:58Z",
        "body": "> There is setting `log_queries`, which enables query logging (disabled by default) and it may have different values for different users and profiles.\r\n\r\nI've setted `log_queries`  in `config.xml`."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-23T16:16:43Z",
        "body": "@Tasselmi it's a user or query level setting, so it must be set in users.xml for a user profile."
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-11-25T13:04:30Z",
        "body": "> @Tasselmi it's a user or query level setting, so it must be set in users.xml for a user profile.\r\n\r\nOK.\r\nIt works fine now. Thanks."
      }
    ]
  },
  {
    "number": 7872,
    "title": "Access outer fields in subquery",
    "created_at": "2019-11-21T08:15:48Z",
    "closed_at": "2019-11-25T02:42:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7872",
    "body": "Hi, how can I query something like this (like in MySQL) with 2 MergeTree tables\r\n`SELECT\r\n  TABLE_A.col1_from_a,\r\n  (SELECT col1_from_b FROM TABLE_B WHERE col2_from_b = TABLE_A.col2_from_a LIMIT 1) AS some_alias\r\nFROM TABLE_A\r\nWHERE <some filter from TABLE_A>`\r\n\r\nIt queries a field from TABLE_B using a field from TABLE_A.\r\nI tried using JOIN like this\r\n`SELECT\r\n    TABLE_A.col1_from_a,\r\n    TABLE_B.col1_from_b\r\nFROM TABLE_A\r\nANY LEFT JOIN TABLE_B ON TABLE_A.col2_from_a = TABLE_B.col2_from_b\r\nWHERE <some filter from TABLE_A>`\r\nbut it showed processing all the rows from TABLE_B (tens of millions of rows).\r\n`col2_from_a` is the sorting key of `TABLE_A`, `col2_from_b` is the sorting key of `TABLE_B`.\r\nAny suggestion would be appreciated.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7872/comments",
    "author": "qza1800",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-21T13:44:45Z",
        "body": ">Hi, how can I query something like this\r\n\r\nCH does not support correlated subqueries.\r\n\r\n>I tried using JOIN like this\r\n>but it showed processing all the rows from TABLE_B (tens of millions of rows).\r\n\r\nIt's actually the best way (HashJoin) to solve this on BigData.\r\n\r\n>col2_from_a is the sorting key of TABLE_A, col2_from_b\r\n\r\nHashJoin does not use sorting keys\r\n\r\nTry\r\n```\r\nSELECT TABLE_A.col1_from_a, TABLE_B.col1_from_b \r\n   FROM TABLE_A \r\n   ANY LEFT JOIN (select col2_from_b TABLE_B where col2_from_b in \r\n      (select col2_from_a from TABLE_A where <some filter from TABLE_A>))\r\n   ON TABLE_A.col2_from_a = TABLE_B.col2_from_b \r\nWHERE <some filter from TABLE_A>\r\n```\r\n\r\nOr you can use external dictionary with source=TABLE_B. \r\nThough such dictionary (tens of millions of rows) could use 5-100GB RAM.\r\n\r\n"
      },
      {
        "user": "filimonov",
        "created_at": "2019-11-21T21:08:16Z",
        "body": "#6697 "
      },
      {
        "user": "qza1800",
        "created_at": "2019-11-25T02:40:52Z",
        "body": "@den-crane Thank you so much for your useful information.\r\nActually I thought about the query you suggested, but I'm trying to create a `View` which `<some filter from TABLE_A>` is coming later."
      },
      {
        "user": "qza1800",
        "created_at": "2019-11-25T02:42:13Z",
        "body": "> #6697\r\n\r\nSo I think I should close this issue here. Thank you."
      }
    ]
  },
  {
    "number": 7865,
    "title": "Escape double quote sign in CSV",
    "created_at": "2019-11-20T16:39:48Z",
    "closed_at": "2019-11-21T13:36:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7865",
    "body": "I have CSV file with comma as separator sign, but without quotes for column entries.\r\n\r\nIssue I have is with double quote sign existing in string value for column,\r\nwhen I try to escape it with another double quote ( \"\" ) for command line clickhouse-client\r\nI got error and insert fails.\r\n\r\nWhen I escape one double quote with backslash ( \\\" ) then it works but got backslash and quote in entry in column.\r\n\r\nWhat is the proper way to escape double quote in CSV structure like I have ?\r\n\r\nClickhouse server version is 19.11.12\r\n\r\nhere is example for structures and data:\r\n```\r\n# testdata.csv\r\n749a2c8c-3682-4745-aefe-c21b3164bade,name,\"\"MY COMPANY\"\" COM\r\n749a2c8c-3682-4745-aefe-c21b3164bade,hash,67FF87AF9E9E4BA9E4C03FAC4A23F21C\r\n\r\n# table structure\r\nCREATE TABLE temp.events (`event_id` String, `property_name` String, `property_value` String) ENGINE = MergeTree() PARTITION BY tuple() ORDER BY event_id SETTINGS index_granularity = 8192\r\n\r\n#shell script to insert data\r\ncat testdata.csv | clickhouse-client --host=localhost --query='INSERT INTO temp.events (event_id, property_name, property_value) FORMAT CSV'\r\n\r\nCode: 117. DB::Exception: Expected end of line: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: event_id,       type: String, parsed text: \"749a2c8c-3682-4745-aefe-c21b3164bade\"\r\nColumn 1,   name: property_name,  type: String, parsed text: \"name\"\r\nColumn 2,   name: property_value, type: String, parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nERROR: There is no line feed. \"M\" found instead.\r\n It's like your file has more columns than expected.\r\nAnd if your file have right number of columns, maybe it have unquoted string value with comma.\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7865/comments",
    "author": "goranc",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-20T18:03:20Z",
        "body": "--format_csv_allow_double_quotes=0\r\n```\r\ncat testdata.csv | clickhouse-client --format_csv_allow_double_quotes=0 --host=localhost --query='INSERT INTO events (event_id, property_name, property_value) FORMAT CSV'\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-20T18:31:08Z",
        "body": "Ah, double quote escaping works only inside quoted string.\r\n```\r\n749a2c8c-3682-4745-aefe-c21b3164bade,name,\"\"\"Y COMPANY\"\" COM\"\r\n749a2c8c-3682-4745-aefe-c21b3164bade,hash,67FF87AF9E9E4BA9E4C03FAC4A23F21C\r\n```\r\n\r\n-----------------------\r\nSo yeah CH supports only double quote escaping by double quote.\r\n\r\nThis string `[,\"MY COMPANY\" COM]` needs escaping because it starts with \"\r\nThis string `[,\u0445\u0430\u0445\u0430 \"MY COMPANY\" COM]` does not need escaping.\r\nThis string `[,\\\"MY COMPANY\\\" COM]` does not need escaping and \\ -- is not escaping, but usual symbol and it works because the string starts with \\ not with \".\r\n\r\n\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-20T21:31:15Z",
        "body": "@den-crane backslash escaping is for TSV and escaping by doubling quotes is for CSV."
      },
      {
        "user": "goranc",
        "created_at": "2019-11-21T10:01:14Z",
        "body": "Thanks for info, sometimes is hard to find proper parameter.\r\n\r\nParameter \"format_csv_allow_double_quotes\" resolve the problem, and there is no need to escape double quote at all.\r\nOther special characters should be escaped as usual.\r\n"
      },
      {
        "user": "inkrement",
        "created_at": "2022-10-05T06:45:21Z",
        "body": "I have a related question: How would you handle a mixture between CSV & TSV (i.e., CSV with escaping instead of quotes)? TSV does not allow changing the delimiter, CSV hates the escapes, and I was unable to get CustomSeparated to work (although I set the comma as separator it detects the full row as single column)."
      }
    ]
  },
  {
    "number": 7849,
    "title": "Avoid `Too many partitions for single INSERT block` in Kafka Engine",
    "created_at": "2019-11-19T17:58:02Z",
    "closed_at": "2019-11-19T18:30:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7849",
    "body": "Hi! I'm using Kafka engine and getting this error: ```DB::Exception: Too many partitions for single INSERT block (more than 100). The limit is controlled by 'max_partitions_per_insert_block' setting. ...```\r\nI understand why this error can occur in small inserts by hand (using clickhouse-client or http interface), but in case with Kafka engine documentation says: \r\n```\r\nTo improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn't formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block.\r\n```\r\nSo if I understand correctly Kafka engine should merge blocks before insert to increase performance and avoid this error. Also I can suspect the root of that error in my case is that in every kafka message is one `row` of data, so I suppose that blocks are merged but this does not reduce parts count. Is there a way to overcome this? Is it a bug? I'm hoping to avoid writing middleware pre-batching service, since Kafka engine does almost all needed things\r\n\r\nI'm using version 19.15.5.18. Thanks in advance!\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7849/comments",
    "author": "LizardWizzard",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-19T18:10:48Z",
        "body": ">I understand why this error can occur in small inserts by hand\r\n\r\nNo. It's not about this.\r\nIt's about partitioning key in your MV. One insert into kafka (into MV) tries to create more than 100 partitions.\r\n\r\nFor example \r\n```\r\n\r\ncreate table A( D Date) Engine=MergeTree partition by D order by D;\r\ninsert into A select number from numbers(365);\r\nCode: 252. DB::Exception: Received from localhost:9000. DB::Exception: Too many partitions for single INSERT block (more than 100).\r\n```\r\n\r\nThe error is because this insert tries to create 365 partitions\r\n\r\n```\r\ncreate table A( D Date) Engine=MergeTree partition by toYYYYMM(D) order by D;\r\ninsert into A select number from numbers(365);\r\nOK.\r\n```\r\nNo error. Because this insert creates only 12 partitions."
      },
      {
        "user": "LizardWizzard",
        "created_at": "2019-11-19T18:30:26Z",
        "body": "Thank you so much for explanation! Got it :) Changed partition expression and error is gone"
      }
    ]
  },
  {
    "number": 7794,
    "title": "distributed_ddl_task_timeout",
    "created_at": "2019-11-15T16:03:49Z",
    "closed_at": "2019-11-16T16:36:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7794",
    "body": "when I `CREATE MATERIALIZED VIEW v  ON CLUSTER xxx populate ` I have a problem.\r\nhow to solve it ?\r\n\r\n```\r\n/clickhouse/task_queue/ddl/query-0000000131 is executing longer than distributed_ddl_task_timeout (=180)\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7794/comments",
    "author": "Tasselmi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-16T15:03:56Z",
        "body": "Just change timeout.\r\n\r\nset distributed_ddl_task_timeout = 9000;"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-11-16T15:06:18Z",
        "body": "> Just change timeout.\r\n> \r\n> set distributed_ddl_task_timeout = 9000;\r\n\r\nTHANKS. PLEASE HELP ME IN  #7803"
      },
      {
        "user": "Nurlan199206",
        "created_at": "2021-07-22T04:31:21Z",
        "body": "@den-crane where i should add this line? config.xml or in database table?"
      },
      {
        "user": "ravibhure",
        "created_at": "2022-01-24T14:36:28Z",
        "body": "@Nurlan199206 \r\n`<distributed_ddl_task_timeout>900</distributed_ddl_task_timeout>`\r\n\r\n```\r\n$ cat configs/users.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n    <!-- Profiles of settings. -->\r\n    <profiles>\r\n        <!-- Default settings. -->\r\n        <default>\r\n            <!-- Maximum memory usage for processing single query, in bytes. -->\r\n            <!-- Use cache of uncompressed blocks of data. Meaningful only for processing many of very short queries. -->\r\n            <use_uncompressed_cache>0</use_uncompressed_cache>\r\n\r\n            <!-- How to choose between replicas during distributed query processing.\r\n                                  random - choose random replica from set of replicas with minimum number of errors\r\n                 nearest_hostname - from set of replicas with minimum number of errors, choose replica\r\n                  with minimum number of different symbols between replica's hostname and local hostname\r\n                  (Hamming distance).\r\n                 in_order - first live replica is chosen in specified order.\r\n                 first_or_random - if first replica one has higher number of errors, pick a random one from replicas with minimum number of errors.\r\n            -->\r\n            <load_balancing>random</load_balancing>\r\n\t    <distributed_ddl_task_timeout>900</distributed_ddl_task_timeout>\r\n        </default>\r\n\r\n        <!-- Profile that allows only read queries. -->\r\n        <readonly>\r\n            <readonly>1</readonly>\r\n        </readonly>\r\n    </profiles>\r\n\r\n```"
      },
      {
        "user": "palmtree100",
        "created_at": "2024-04-17T17:50:38Z",
        "body": "> Just change timeout.\r\n> \r\n> set distributed_ddl_task_timeout = 9000;\r\n\r\nHow to set this parameter at query level in dbeaver? and how to set in scripts, eg inside subprocess.check_output()? Thanks "
      }
    ]
  },
  {
    "number": 7765,
    "title": "Drop in writes on high number of selects.",
    "created_at": "2019-11-13T23:16:06Z",
    "closed_at": "2019-11-14T06:04:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7765",
    "body": "Version: ClickHouse 19.13.1.11\r\nServer has 400GB RAM with 48 CPU cores. and 3.2 TB of HDD.\r\nInsert batch size is 20K records (~40MB). Each insert is taking ~1.5secs.\r\nServer has 12.5B records.\r\nWe are running parallel select queries of ~500. Selects include some count queries, some aggregations and group bys.\r\nCPU and RAM are at their minimals (like 10% of CPU and 10GB of RAM used).\r\nDuring this time, inserts are dropped. Noticed \"TimeoutExceptions\" on client side (Using Http Client for inserts)\r\nGet below exception at high rate. (very few select queries went through)\r\n\r\nCode: 202, e.displayText() = DB::Exception: Too many simultaneous queries. Maximum: 100 (version 19.13.1.11 (official build))\r\n\r\nTried to increase max simultaneous queries in config.xml (to 1000).\r\n <max_concurrent_queries>1000</max_concurrent_queries>\r\n\r\nEven after increasing, exception still says Maximun: 100\r\nCode: 202, e.displayText() = DB::Exception: Too many simultaneous queries. Maximum: 100 (version 19.13.1.11 (official build))\r\n\r\nSeems my change is not taking effect.\r\n\r\nHow can I make sure that writes are not impacted due to reads?\r\nWhy cant I increase simultaneous queries in spite of having RAM and MEMORY available?\r\nHow can we support more selects? (we need little higher selects to build dashboards, aggregations, and for AD purposes)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7765/comments",
    "author": "SreekanthMannari",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-13T23:22:46Z",
        "body": "Did you restart CH after setting max_concurrent_queries ?"
      },
      {
        "user": "SreekanthMannari",
        "created_at": "2019-11-13T23:27:03Z",
        "body": "No. I haven't restarted.\r\nBut config.xml in \"preprocessed_configs\" folder shows my change."
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-13T23:28:59Z",
        "body": ">But config.xml in \"preprocessed_configs\" folder shows my change.\r\n\r\nIt does not matter. All parameters from config.xml (except cluster & dictionaries configurations) require CH reboot to apply. "
      },
      {
        "user": "SreekanthMannari",
        "created_at": "2019-11-14T01:00:32Z",
        "body": "Thanks. Setting worked after CH reboot. \r\nDoes reboot needed even for User.xml changes like max memory settings?"
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-14T01:27:11Z",
        "body": ">Does reboot needed even for User.xml changes like max memory settings?\r\n\r\nNo, changes in user.xml does not need reboot."
      },
      {
        "user": "haiertashu",
        "created_at": "2021-06-03T06:25:08Z",
        "body": "how to make sure the server settings modified such as 'max_concurrent_queries' taking effect \uff1f where/which system table can show the latest changes\uff1f or any other way"
      }
    ]
  },
  {
    "number": 7711,
    "title": "cannot parse CSV with '\\x7F' as delmiter. \\x7F is ascii code.",
    "created_at": "2019-11-11T09:27:56Z",
    "closed_at": "2019-11-11T10:45:54Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7711",
    "body": "$clickhouse-client --host dbt20 --port 9000 --format_csv_delimiter='\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt \r\nCode: 19, e.displayText() = DB::Exception: A setting's value string has to be an exactly one character long, Stack trace:\r\n\r\n0. 0x563fac3bd7b0 StackTrace::StackTrace() /usr/bin/clickhouse\r\n1. 0x563fac3bd585 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse\r\n2. 0x563fac0ba051 ? /usr/bin/clickhouse\r\n3. 0x563fb03d2c5d boost::program_options::variables_map::notify() /usr/bin/clickhouse\r\n4. 0x563fac48413c DB::Client::init(int, char**) /usr/bin/clickhouse\r\n5. 0x563fac46feef mainEntryClickHouseClient(int, char**) /usr/bin/clickhouse\r\n6. 0x563fac2f9fed main /usr/bin/clickhouse\r\n7. 0x7f1c99a2e3d5 __libc_start_main /usr/lib64/libc-2.17.so\r\n8. 0x563fac3632ea _start /usr/bin/clickhouse\r\n (version 19.16.2.2 (official build))\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7711/comments",
    "author": "xuxudede",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-11T09:59:50Z",
        "body": "You have specified four characters: \\\\, x, 7, F, as a delimiter, because escape sequence is not interpreted by your shell interpreter.\r\n\r\nWrite\r\n```\r\nclickhouse-client --host dbt20 --port 9000 --format_csv_delimiter=$'\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt\r\n```\r\ninstead (if you use `bash` as a shell interpreter).\r\nNote the dollar sign before quote."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-11T10:01:00Z",
        "body": "`man bash`, `QUOTING` section."
      },
      {
        "user": "xuxudede",
        "created_at": "2019-11-11T10:45:04Z",
        "body": "> You have specified four characters: \\, x, 7, F, as a delimiter, because escape sequence is not interpreted by your shell interpreter.\r\n> \r\n> Write\r\n> \r\n> ```\r\n> clickhouse-client --host dbt20 --port 9000 --format_csv_delimiter=$'\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt\r\n> ```\r\n> \r\n> instead (if you use `bash` as a shell interpreter).\r\n> Note the dollar sign before quote.\r\n\r\nTKS"
      }
    ]
  },
  {
    "number": 7647,
    "title": "Change bitmapBuild result type from default UInt8",
    "created_at": "2019-11-06T10:03:10Z",
    "closed_at": "2019-11-06T13:00:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7647",
    "body": "Hi there! Can i somehow change result type of bitmapBuild([1,2,3,4]) not to ```AggregateFunction(groupBitmap, UInt8)``` which i assume selects type by selecting max integer in set, but to ```AggregateFunction(groupBitmap, UInt32)``` without using hacks like bitmapBuild([1,2,3,4, 4294967295])",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7647/comments",
    "author": "mrAndersen",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-06T10:20:06Z",
        "body": "CC @yuzhichang "
      },
      {
        "user": "yuzhichang",
        "created_at": "2019-11-06T12:58:59Z",
        "body": "@mrAndersen You can cast array to UInt32 explicitly, for example `bitmapBuild(cast([1,2,3,4] as Array(UInt32)))`."
      },
      {
        "user": "mrAndersen",
        "created_at": "2019-11-06T13:00:26Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 7642,
    "title": "When Join Clause in Select query, The query use index of join column?",
    "created_at": "2019-11-06T01:48:38Z",
    "closed_at": "2019-11-06T04:03:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7642",
    "body": "orders - pk: order_id\r\norder_product - pk: (order_id, product_no)\r\n\r\n```sql\r\nselect o.o_zipcode, op.opt_id \r\nfrom orders o \r\n    inner join order_product op \r\n        on o.order_id = op.order_id;\r\n```\r\n\r\nTwo tables has `order_id` as primary key. When I join two tables in select query, Does the query use index?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7642/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-06T02:08:29Z",
        "body": "Join does not use indexes. Because currently CH supports only HashJoin. And HashJoin puts a right table into memory into HashTable with joinKey as a lookup key. "
      },
      {
        "user": "chu1070y",
        "created_at": "2019-11-06T04:03:07Z",
        "body": ":)"
      }
    ]
  },
  {
    "number": 7503,
    "title": "How to shutdown clickhouse instance ?",
    "created_at": "2019-10-28T08:26:21Z",
    "closed_at": "2019-10-28T09:45:22Z",
    "labels": [
      "question",
      "operations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7503",
    "body": "First time to use clickhouse, how can i close or shutdown clickhouse ? Can only use \"kill -9 pid\" ?\r\nThx.  ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7503/comments",
    "author": "Myshiner",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-10-28T09:39:33Z",
        "body": "Sending SIGKILL (9) with `kill` command will work but recommended way is \r\n```\r\nsystemctl stop clickhouse-server\r\n```\r\n\r\nor  \r\n```\r\n/etc/init.d/clickhouse-server stop\r\n``` "
      },
      {
        "user": "Myshiner",
        "created_at": "2019-10-28T09:45:22Z",
        "body": "Ok, thanks for your quick reply."
      }
    ]
  },
  {
    "number": 7502,
    "title": "Threads control",
    "created_at": "2019-10-28T07:41:44Z",
    "closed_at": "2019-10-29T09:25:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7502",
    "body": "When I use different index, The Clickhouse use different number of parallel threads(streams).\r\n\r\nI can refer to the max_threads settings in clickhouse documents.\r\n\r\nbut, It is only for max_threads.\r\n\r\nIs there any other ways to control thread number by different index?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7502/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-10-28T09:41:52Z",
        "body": "ClickHouse  can decrease a number of working threads below `max_threads` automatically if selected datasize  is small enough. "
      },
      {
        "user": "chu1070y",
        "created_at": "2019-10-28T23:42:17Z",
        "body": "Is there any way to increase a number of working threads?"
      },
      {
        "user": "filimonov",
        "created_at": "2019-10-29T08:37:13Z",
        "body": "```\r\nset max_threads = 20;\r\n```\r\nCan also be adjusted for particular user profile, of globally in default profile in users.xml"
      },
      {
        "user": "chu1070y",
        "created_at": "2019-10-29T09:25:56Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 7499,
    "title": "How does the Clickhouse find other column data?",
    "created_at": "2019-10-28T05:31:51Z",
    "closed_at": "2019-10-28T07:24:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7499",
    "body": "When I run a query with where statement like `select * from tbl1 where col1 = 'CH' `, Clickhouse find other columns which correspond with `col1 = 'CH'`.\r\n\r\nUnlike row-base DBMS, ClickHouse is column-base DBMS that saves data by column. Then How CH find other column data with same row?\r\n\r\nCould you explain a process step by step that the ClickHouse finds other columns data?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7499/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "hczhcz",
        "created_at": "2019-10-28T06:30:46Z",
        "body": "The expression `col1 = 'CH'` will yield a \"condition column\" of boolean values. Physically, it is one or more memory blocks containing UInt8 values.\r\nThen, we can perform a for-loop within the range of the row numbers. Let the row number to be \"i\", the i-th element in the condition column will indicate whether the i-th row is selected. If so, we will visit each column, grab the i-th element, and append it to the corresponding result column."
      },
      {
        "user": "chu1070y",
        "created_at": "2019-10-28T07:24:56Z",
        "body": "It`s very helpful. Thanks."
      }
    ]
  },
  {
    "number": 7489,
    "title": "Strange Null literal handling behavior?",
    "created_at": "2019-10-25T13:00:26Z",
    "closed_at": "2019-10-25T13:21:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7489",
    "body": "I am observing this counterintuitive behavior with special `Null` value:\r\n```\r\nSELECT isNull(CAST('Null', 'Nullable(String)'))\r\n```\r\nreturns `0`\r\n\r\nWhereas:\r\n```\r\nSELECT isNull(CAST(Null, 'Nullable(String)'))\r\n```\r\nreturns `1`\r\n\r\nand\r\n```\r\nSELECT isNull(CAST('Null', 'Nullable(Int32)'))\r\n```\r\nreturns `1`\r\n\r\nN.B.: single quotes and their absence around `Null`.\r\n\r\nIs this a bug or a feature?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7489/comments",
    "author": "traceon",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-10-25T13:16:11Z",
        "body": "CAST('Null', 'Nullable(String)') -- it's a string with text Null.\r\n\r\n```\r\n\r\nSELECT\r\n    CAST('Null', 'Nullable(String)'),\r\n    NULL\r\n\r\n\u250c\u2500CAST('Null', 'Nullable(String)')\u2500\u252c\u2500NULL\u2500\u2510\r\n\u2502 Null                             \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nit's how cast works from strings to Int32\r\n```\r\nSELECT CAST('aaaaaaa', 'Nullable(Int32)')\r\n\r\n\u250c\u2500CAST('aaaaaaa', 'Nullable(Int32)')\u2500\u2510\r\n\u2502                               \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "traceon",
        "created_at": "2019-10-25T13:21:13Z",
        "body": "I see, that makes sense."
      }
    ]
  },
  {
    "number": 7443,
    "title": "Is there any way to predaggregate uniqState?",
    "created_at": "2019-10-23T02:12:23Z",
    "closed_at": "2019-10-23T13:38:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7443",
    "body": "There is no function RunningDifference for uniqState.\r\n\r\nBut two uniqStates can be summed by +\r\n\r\nSo RunningDifference can be emulated like this\r\n```\r\ncreate table z(d Date, z String, u String)\r\nEngine=MergeTree partition by tuple() order by tuple();\r\n\r\nCREATE MATERIALIZED VIEW mvz\r\nENGINE = AggregatingMergeTree order by (z,d) settings index_granularity = 8 \r\nas select d, z,uniqState(u) as us from z group by z,d;\r\n\r\ninsert into z select today()+1, 'g1' , toString(number) from numbers(1000);\r\ninsert into z select today()+2, 'g1' , toString(number+100) from numbers(1000);\r\ninsert into z select today()+3, 'g1' , toString(number+200) from numbers(1000);\r\ninsert into z select today()+4, 'g1' , toString(number+200) from numbers(1000);\r\ninsert into z select today()+5, 'g1' , toString(number+300) from numbers(1000);\r\n\r\nselect m1, m2 from (\r\nSELECT\r\n        groupArray(d) AS gd,\r\n        arrayMap(x -> toString(gd[x+1])||' - '||toString(gd[x+2]), range(toUInt64(length(gd)-1))) m1,\r\n        groupArray(us) AS gus,\r\n        arrayMap(x -> (arrayReduce('uniqMerge', [gus[x+1]+gus[x+2]]) - arrayReduce('uniqMerge', [gus[x+2]])) , range(toUInt64(length(gd)-1))) m2\r\n          from (select d, us FROM mvz  order by d ) )\r\n    Array Join m1, m2\r\n\r\n\u250c\u2500m1\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500m2\u2500\u2510\r\n\u2502 2019-10-24 - 2019-10-25 \u2502 100 \u2502\r\n\u2502 2019-10-25 - 2019-10-26 \u2502 100 \u2502\r\n\u2502 2019-10-26 - 2019-10-27 \u2502   0 \u2502\r\n\u2502 2019-10-27 - 2019-10-28 \u2502 100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThere is only problem that it's working only if state fully aggregated by date.\r\nAnd there is no function partialUniqMerge to merge by date but leave states.\r\n`select d, partialUniqMerge(us) FROM mvz group by d order by d`\r\n\r\nBut probably such function exists internally because Distributed gets such data from shards.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7443/comments",
    "author": "den-crane",
    "comments": [
      {
        "user": "vpanfilov",
        "created_at": "2019-10-23T08:03:25Z",
        "body": "Have you tried `-MergeState` modifier?\r\n\r\n```\r\nselect d, uniqMergeState(us) FROM mvz group by d order by d\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2019-10-23T13:38:38Z",
        "body": "@vpanfilov ha, that what I asked for. \r\nuniqMergeState works. \r\n\r\nGreat. Thank you."
      }
    ]
  },
  {
    "number": 7421,
    "title": "I wonder how storage policy do on disk failure.",
    "created_at": "2019-10-22T08:19:00Z",
    "closed_at": "2019-11-04T06:11:26Z",
    "labels": [
      "question",
      "comp-multidisk"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7421",
    "body": "My ClickHouse configurations are like below.\r\n\r\n* Node1\r\n  - two disks \r\n    * /data1\r\n    * /data2\r\n\r\n* Node2\r\n  - two disks\r\n    * /data1\r\n    * /data2\r\n\r\nData is replicated between Node1 and Node2 by ReplicatedMergeTree. \r\nAnd each ReplicatedMergeTree uses the below storage_policy.\r\nSo ReplicatedMergeTree splits data into two disks; /data1 and /data2.\r\n\r\n**Settings**\r\n```XML\r\n<storage_configuration>                                                          \r\n    <disks>                                                                      \r\n        <jbod1>                                                                  \r\n            <path>./jbod1/</path>                                                \r\n            <keep_free_space_bytes>1024</keep_free_space_bytes>                  \r\n        </jbod1>                                                                 \r\n        <jbod2>                                                                  \r\n            <path>./jbod2/</path>                                                \r\n            <keep_free_space_bytes>1024</keep_free_space_bytes>                  \r\n        </jbod2>                                                                 \r\n    </disks>                                                                     \r\n                                                                                 \r\n    <policies>                                                                   \r\n        <jbods>                                                                  \r\n            <volumes>                                                            \r\n                <main>                                                           \r\n                    <disk>jbod1</disk>                                           \r\n                    <disk>jbod2</disk>                                           \r\n                </main>                                                          \r\n            </volumes>                                                           \r\n        </jbods>                                                                 \r\n    </policies>                                                                  \r\n</storage_configuration>                                                         \r\n```\r\n\r\nWhat happen If `/data1` on Node1 is broken?\r\nDoes Node1 lose the data on `/data1`?\r\nOr Does Node1 copy data from Node2?\r\n\r\nThank you in advance.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7421/comments",
    "author": "achimbab",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-03T21:58:38Z",
        "body": "If data1 is broken, you should replace the disk (create filesystem and mount at the same location) and restart clickhouse-server. ClickHouse will download missing parts from replica."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-03T22:00:51Z",
        "body": "You can also remove disk1 from volumes configuration and restart clickhouse-server. ClickHouse will download missing parts from replica and place them on disk2."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-03T22:01:44Z",
        "body": "BTW, different replicas can have different volumes and different placement of data parts (but the set of data parts as a whole will be synchronized)."
      },
      {
        "user": "achimbab",
        "created_at": "2019-11-04T06:11:24Z",
        "body": "@alexey-milovidov \r\nThank you for your explanation."
      }
    ]
  },
  {
    "number": 7391,
    "title": "DDL background thread is not initialized",
    "created_at": "2019-10-19T14:28:01Z",
    "closed_at": "2020-04-01T18:05:15Z",
    "labels": [
      "question",
      "comp-dddl"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7391",
    "body": "hey , guys.\r\n\r\nTO support clickhouse , I bought 11 machines each with 64 cores/256GB RAM/2TB SSD. they cost me about  230K us dollar.\r\n\r\nRecently I am doing some testing. But `DDL background thread is not initialized` made me confused.\r\n\r\n```\r\nclickhouse version:  ClickHouse server version 19.15.3 revision 54426\r\nos: centos 7.4\r\nshards: 11 (yes lack of money, so 1 replica)\r\ninternal_replication = false\r\n```\r\n\r\nI don't know why:\r\n```\r\nCREATE TABLE IF NOT EXISTS test.user_log ON CLUSTER cluster_1st\r\n(\r\n    `dt` Date,\r\n    `app_id` Int8,\r\n    `app_version_name` String,\r\n    `event_id` String\r\n)\r\nENGINE = MergeTree('/clickhouse/tables/{layer}-{shard}/user_log', '{replica}')\r\nPARTITION BY dt\r\nORDER BY (app_id, app_version_name)\r\nSETTINGS index_granularity = 8192\r\n\r\nReceived exception from server (version 19.15.3):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: DDL background thread is not initialized..\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n```\r\n\r\nSOS~~~\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7391/comments",
    "author": "Tasselmi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-10-19T16:13:59Z",
        "body": "ON CLUSTER uses Zookeeper. "
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-19T17:42:36Z",
        "body": "If I make 5 shards each with 2 replica , and 1 shard with 1 replica. Is that ok? I have odd number of machine(11)\n\n\n| |\n\u6881\u51e1\n\u90ae\u7bb1\uff1atasselmi@yeah.net\n|\n\n\u7b7e\u540d\u7531 \u7f51\u6613\u90ae\u7bb1\u5927\u5e08 \u5b9a\u5236\n\nOn 10/20/2019 00:14, Denis Zhuravlev wrote:\n\nON CLUSTER uses Zookeeper.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or unsubscribe."
      },
      {
        "user": "den-crane",
        "created_at": "2019-10-19T21:19:08Z",
        "body": "ON CLUSTER  is not related to replication. \r\nYou can use 11 shards with MergeTree tables (without replication) but CH needs at least one small standalone Zookeeper for ON CLUSTER."
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-20T10:26:32Z",
        "body": "> ON CLUSTER is not related to replication.\r\n> You can use 11 shards with MergeTree tables (without replication) but CH needs at least one small standalone Zookeeper for ON CLUSTER.\r\n\r\nTHanks for your advice.\r\n"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-20T17:25:54Z",
        "body": "> ON CLUSTER is not related to replication.\r\n> You can use 11 shards with MergeTree tables (without replication) but CH needs at least one small standalone Zookeeper for ON CLUSTER.\r\n\r\nhi\uff0cI want to make it clear:\r\nif I use 11 shards each with 1 replica, to use `on cluster` I have to deploy a standalone zookeeper and set `internal_replication = true`, right?   ` internal_replication = true` is necessary\uff1for I only need a standalone zookeeper and set `internal_replication = false` also be fine?"
      },
      {
        "user": "hodgesrm",
        "created_at": "2019-10-20T18:15:58Z",
        "body": "Shards and replicas are not connected.  If you have sharded tables there is no replication between shards--the data are disjoint.  You don't need zookeeper in this case unless you plan to add replicas later. \r\n\r\nNote that if you don't use replication and a host drops out for some reason, any data on that host will become unavailable.  With replicas you would still have at least one copy of the data.  Also, if you plan to add replicas later you may want to start with ReplicatedMergeTree engine on replicated tables.  This avoids a migration at a future date and gets you used to managing zookeeper from the start. "
      },
      {
        "user": "den-crane",
        "created_at": "2019-10-20T22:29:52Z",
        "body": "> hi\uff0cI want to make it clear:\r\n> if I use 11 shards each with 1 replica, to use `on cluster` I have to deploy a standalone zookeeper and set `internal_replication = true`, right? ` internal_replication = true` is necessary\uff1for I only need a standalone zookeeper and set `internal_replication = false` also be fine?\r\n\r\nZookeeper server and internal_replication = false. \r\n\r\nReplicated tables need internal_replication = true but you use MergeTree tables so your cluster needs internal_replication = false. \r\n"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-21T02:01:07Z",
        "body": "> Shards and replicas are not connected. If you have sharded tables there is no replication between shards--the data are disjoint. You don't need zookeeper in this case unless you plan to add replicas later.\r\n> \r\n> Note that if you don't use replication and a host drops out for some reason, any data on that host will become unavailable. With replicas you would still have at least one copy of the data. Also, if you plan to add replicas later you may want to start with ReplicatedMergeTree engine on replicated tables. This avoids a migration at a future date and gets you used to managing zookeeper from the start.\r\n\r\nLack of money to deploy replicas. Machine is so fuck expensive and our data is huge amount...\r\nThanks for your advice. All you said is right, I can not agree more. Maybe I will deploy 2 replicas next year (buy more machines) and do a migration.\r\n\r\n"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-21T02:01:45Z",
        "body": "> > hi\uff0cI want to make it clear:\r\n> > if I use 11 shards each with 1 replica, to use `on cluster` I have to deploy a standalone zookeeper and set `internal_replication = true`, right? ` internal_replication = true` is necessary\uff1for I only need a standalone zookeeper and set `internal_replication = false` also be fine?\r\n> \r\n> Zookeeper server and internal_replication = false.\r\n> \r\n> Replicated tables need internal_replication = true but you use MergeTree tables so your cluster needs internal_replication = false.\r\n\r\nok, thanks."
      },
      {
        "user": "blinkov",
        "created_at": "2020-04-01T16:54:14Z",
        "body": "@Tasselmi, do you have any further questions?"
      },
      {
        "user": "Tasselmi",
        "created_at": "2020-04-01T17:33:46Z",
        "body": "No thanks\n\n\n\n\n| |\n\u6881\u51e1\n\u90ae\u7bb1\uff1atasselmi@yeah.net\n|\n\n\u7b7e\u540d\u7531 \u7f51\u6613\u90ae\u7bb1\u5927\u5e08 \u5b9a\u5236\n\nOn 04/02/2020 00:54, Ivan Blinkov wrote:\n\n@Tasselmi, do you have any further questions?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe."
      }
    ]
  },
  {
    "number": 7312,
    "title": "What is \"active\" znode means in Zookeeper?",
    "created_at": "2019-10-14T12:13:18Z",
    "closed_at": "2019-11-03T22:36:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7312",
    "body": "When I check my zk, I found a 'actived' znode in the clickhouse path.I check the document but nothing found out.\r\nso what's this znode means?\r\nbyw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7312/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-03T22:36:02Z",
        "body": "> so what's this znode means?\r\n\r\nReplica has a session with ZooKeeper and can serve INSERT queries.\r\n\r\n> byw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk?\r\n\r\nNo, you don't need to do anything manually with ZK nodes."
      },
      {
        "user": "byx313",
        "created_at": "2019-11-05T06:29:58Z",
        "body": "> > so what's this znode means?\r\n> \r\n> Replica has a session with ZooKeeper and can serve INSERT queries.\r\n> \r\n> > byw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk?\r\n> \r\n> No, you don't need to do anything manually with ZK nodes.\r\n\r\nthx\uff01"
      }
    ]
  },
  {
    "number": 7151,
    "title": "Log file not recreated if deleted",
    "created_at": "2019-09-30T09:42:09Z",
    "closed_at": "2019-09-30T12:02:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7151",
    "body": "Hi,\r\n\r\nMy ClickHouse is integrated with a system that regularly take the log files, backup them, and delete them locally. It is a common system to lots of services.\r\n\r\nNevertheless, I got an issue with clickhouse. If you delete one ClickHouse log file when ClickHouse is up and running, the log file will not be recreated, and all new clickhouse logs will be lost.\r\n\r\nForcing the reload of the config file or flushing the log  (SYSTEM RELOAD CONFIG or SYSTEM FLUSH LOGS) does not seems to fix the issue.\r\n\r\nSo is there a way to force clickhouse to recreate the log file ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7151/comments",
    "author": "edonin",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-09-30T09:46:07Z",
        "body": "You can send SIGHUP to clickhouse-server to create next log file."
      },
      {
        "user": "edonin",
        "created_at": "2019-09-30T12:02:44Z",
        "body": "Super, it is working.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 7147,
    "title": "I think sth is wrong with arrayDifference",
    "created_at": "2019-09-30T07:58:53Z",
    "closed_at": "2019-10-06T23:07:15Z",
    "labels": [
      "question",
      "comp-arrays"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7147",
    "body": "THIS IS FINE \r\n\r\n```\r\nSELECT arrayDifference([1, 2, 3, 4])\r\n\r\n\u250c\u2500arrayDifference([1, 2, 3, 4])\u2500\u2510\r\n\u2502 [0,1,1,1]                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nBUT what `-1` mean ?\r\n```\r\nSELECT arrayDifference([1, 2, 2, 3, 3, 2])\r\n\r\n\u250c\u2500arrayDifference([1, 2, 2, 3, 3, 2])\u2500\u2510\r\n\u2502 [0,1,0,1,0,-1]                      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd this is not reasonable:\r\n```\r\nSELECT arrayDifference(['a', 'b'])\r\n\r\nReceived exception from server (version 19.5.3):\r\nCode: 43. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: arrayDifference cannot process values of type String.\r\n```\r\narray(T) takes T type, but a function can not support String type......",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7147/comments",
    "author": "Tasselmi",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-09-30T08:36:44Z",
        "body": "> BUT what -1 mean ?\r\n\r\n`-1` means that last element of array (`2`) minus previous one (`3`) equals `2 - 3 = -1`\r\n\r\nIsn't it the expected behaviour?\r\n\r\n> array(T) takes T type, but a function can not support String type......\r\n\r\nDoes strings support substraction? What is the result of 'Hello' minus 'world!' ?"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-09-30T08:39:56Z",
        "body": "> > BUT what -1 mean ?\r\n> \r\n> `-1` means that last element of array (`2`) minus previous one (`3`) equals `2 - 3 = -1`\r\n> \r\n> Isn't it the expected behaviour?\r\n> \r\n> > array(T) takes T type, but a function can not support String type......\r\n> \r\n> Does strings support substraction? What is the result of 'Hello' minus 'world!' ?\r\n\r\nI thought arrayDifference means if they are not equal........ 1 means true and 0 means false\r\n......\r\nSo I misunderstood.."
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-09-30T08:42:31Z",
        "body": "> > BUT what -1 mean ?\r\n> \r\n> `-1` means that last element of array (`2`) minus previous one (`3`) equals `2 - 3 = -1`\r\n> \r\n> Isn't it the expected behaviour?\r\n> \r\n> > array(T) takes T type, but a function can not support String type......\r\n> \r\n> Does strings support substraction? What is the result of 'Hello' minus 'world!' ?\r\n\r\nI want to make [1, 2, 2, 3, 3, 2, 2]  -> [1, 2, 3, 2]\r\nI am doing a program computing user-behaviour-path ~~~"
      },
      {
        "user": "den-crane",
        "created_at": "2019-09-30T14:05:52Z",
        "body": "select arrayConcat([arr[1]], arrayFilter(x,y -> x = y, arraySlice(arr, 2), arraySlice(arr, 1, -1))) from (  select [1, 2, 2, 3, 3, 2, 2] arr )\r\n\r\nSELECT arrayConcat([arr[1]], arrayFilter(x,y -> y!=0, arr ,arrayDifference(arr))) from (select [1, 2, 2, 3, 3, 2] arr)\r\n\r\n\r\n"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-07T06:23:20Z",
        "body": "> select arrayConcat([arr[1]], arrayFilter(x,y -> x = y, arraySlice(arr, 2), arraySlice(arr, 1, -1))) from ( select [1, 2, 2, 3, 3, 2, 2] arr )\r\n> \r\n> SELECT arrayConcat([arr[1]], arrayFilter(x,y -> y!=0, arr ,arrayDifference(arr))) from (select [1, 2, 2, 3, 3, 2] arr)\r\n\r\nThanks for your help ~"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-07T11:48:48Z",
        "body": "> select arrayConcat([arr[1]], arrayFilter(x,y -> x = y, arraySlice(arr, 2), arraySlice(arr, 1, -1))) from ( select [1, 2, 2, 3, 3, 2, 2] arr )\r\n> \r\n> SELECT arrayConcat([arr[1]], arrayFilter(x,y -> y!=0, arr ,arrayDifference(arr))) from (select [1, 2, 2, 3, 3, 2] arr)\r\n\r\n\r\n\r\nThe first method is more universal.  It takes parameters of any type, but arrayDifference can only take numeric type.\r\n\r\n```\r\nSELECT arrayConcat(array(event_list[1]), arrayFilter((x, y) -> (x != y), arraySlice(event_list, 2), arraySlice(event_list, 1, -1)))\r\nFROM ( SELECT [1, 2, 2, 3, 2, 2] AS event_list ) m\r\n```"
      }
    ]
  },
  {
    "number": 6973,
    "title": "Data written because of max_bytes_before_external_*",
    "created_at": "2019-09-18T11:33:22Z",
    "closed_at": "2019-10-24T22:50:42Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6973",
    "body": "Hello.\r\n\r\nIs it possible to see how much data was written to disk during query because of max_bytes_before_external_* ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6973/comments",
    "author": "Tri0L",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-09-18T15:45:14Z",
        "body": "Hi. This information you can find in `system.query_log` table (you need to enable query_log in config and set `log_queries=1`). Then if you know query id, run query like\r\n```\r\nSELECT \r\n    ProfileEvents.Names, \r\n    ProfileEvents.Values\r\nFROM system.query_log\r\nARRAY JOIN ProfileEvents\r\nWHERE query_id = '<query_id>' and ProfileEvents.Names = '<event_name>'\r\n```\r\n\r\nFor external aggregation `<event_name>` is `ExternalAggregationCompressedBytes` or `ExternalAggregationUncompressedBytes`.\r\nFor external sorting there is no special event, but you can use `WriteBufferFromFileDescriptorWriteBytes` to get the number of written bytes in total for query."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T09:26:02Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "blinkov",
        "created_at": "2019-10-20T10:00:44Z",
        "body": "@Tri0L do you have any further questions?"
      },
      {
        "user": "Tri0L",
        "created_at": "2019-10-24T22:50:42Z",
        "body": "Nope, thanks a lot!"
      }
    ]
  },
  {
    "number": 6671,
    "title": "Collapsing Merge Tree Using FINAL in the SQL",
    "created_at": "2019-08-26T17:21:09Z",
    "closed_at": "2019-08-28T07:02:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6671",
    "body": "Hello,\r\n\r\nI decided to use CollapsingMergeTree for a test table. This table basically keeps the football players name and their teams. For example ;\r\n\r\n```\r\nCREATE TABLE players\r\n    (\r\n     `team_id` UInt32,\r\n     `player_name` String,\r\n      `pid` UInt32,\r\n      sign Int8 DEFAULT 1\r\n    ) ENGINE = CollapsingMergeTree(sign) ORDER BY (player_name, pid) SETTINGS index_granularity = 8192\r\n```\r\nSo I understood the usage of sign for the querying with 'having' but what I want to ask is when I want to get the player count by grouping by team_id, having sum(sign) > 0 ' won't work properly since it will aggregate all the player's sign values. \r\n\r\nSo what I would like to ask is if I use FINAL in my SQL all the time, will it decrease performance dramatically? (There could be around 50M-100M rows at maximum!)\r\n\r\nThanks in the advance!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6671/comments",
    "author": "MeteHanC",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-08-27T13:55:53Z",
        "body": "Yes, FINAL affects performance dramatically."
      },
      {
        "user": "MeteHanC",
        "created_at": "2019-08-28T07:02:49Z",
        "body": "> Yes, FINAL affects performance dramatically.\r\n\r\nThank you! And this issue could be helpful as well : #1661"
      }
    ]
  },
  {
    "number": 6484,
    "title": " DB::Exception: Bad get: has UInt64, requested String.",
    "created_at": "2019-08-14T05:44:51Z",
    "closed_at": "2019-08-19T11:18:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6484",
    "body": "When I set up the distributed tables, there was such a mistake.\r\n`node01 :) create table dummy (p Date, k UInt64, d String) ENGINE = MergeTree(p, k, 8192)\r\n\r\nCREATE TABLE dummy\r\n(\r\n    `p` Date, \r\n    `k` UInt64, \r\n    `d` String\r\n)\r\nENGINE = MergeTree(p, k, 8192)\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.020 sec. \r\nnode01 :) create table distributed (p Date, k UInt64, d String) ENGINE = Distributed(cluster-1, 'default', 'dummy')\r\n\r\nCREATE TABLE distributed\r\n(\r\n    `p` Date, \r\n    `k` UInt64, \r\n    `d` String\r\n)\r\nENGINE = Distributed(cluster - 1, 'default', 'dummy')\r\n\r\nReceived exception from server (version 19.9.5):\r\nCode: 170. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Bad get: has UInt64, requested String. \r\n\r\n0 rows in set. Elapsed: 0.001 sec. \r\n`\r\n I don't know why. I tried many times.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6484/comments",
    "author": "TomatoBoy90",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-08-14T16:57:31Z",
        "body": ">Distributed(cluster - 1\r\n\r\n-1 ? It should be a cluster name. \r\nFor example `test_shard_localhost` : `ENGINE = Distributed(test_shard_localhost, default, dummy)`\r\n\r\ncheck for available clusters `select distinct cluster from system.clusters`\r\n\r\n\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-08-19T11:36:14Z",
        "body": "@JonLeeCSDN \r\n`cluster-1` is interpreted as arithmetic expression. You have to put in in backticks: \r\n```\r\n`cluster-1`\r\n```\r\n or use different cluster name."
      },
      {
        "user": "TomatoBoy90",
        "created_at": "2019-08-21T07:38:06Z",
        "body": "> @JonLeeCSDN\r\n> `cluster-1` is interpreted as arithmetic expression. You have to put in in backticks:\r\n> \r\n> ```\r\n> `cluster-1`\r\n> ```\r\n> \r\n> or use different cluster name.\r\n\r\nthank you ,in fact ,code as follow will be true:\r\n`node03 :) CREATE TABLE ontime_all AS ontime_local_2 ENGINE = Distributed('cluster-1', 'h2', 'ontime_local_2', 100);\r\n\r\nCREATE TABLE ontime_all AS ontime_local_2\r\nENGINE = Distributed('cluster-1', 'h2', 'ontime_local_2', 100)\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.005 sec"
      }
    ]
  },
  {
    "number": 6353,
    "title": "Using runningDifference() and quantilesExactWeighted() correctly on time series data",
    "created_at": "2019-08-05T20:47:57Z",
    "closed_at": "2019-08-06T07:31:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6353",
    "body": "We need to calculate exact value quantiles on large non-uniformly sampled time series data. The schema is (String sensor_path, DateTime timestamp, Float64 value). \r\n\r\nIt would be perfect if we could use something like \u201eSELECT quantilesExactWeighted(...)(value, delta)...\u201c,\r\nwhere \u201edelta\u201c is the result of \u201erunningDifference(timestamp)\u201c. \r\nUnfortunately, this does not work, because for each row, delta is the time difference between the previous and the current row instead of the time difference between the current and the next row, as needed for the weights parameter of quantilesExactWeighted(). \r\n\r\nSelf-joining the time series to shift delta forward one row will probably not work when the time series does not fit into memory, right?\r\n\r\nIs there maybe another, more efficient solution?\r\n\r\nI\u2019d be glad to try to submit a patch with a new variant of runningDifference() if there is no other solution. \r\n\r\nMany thanks for your excellent work on ClickHouse!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6353/comments",
    "author": "oflasch",
    "comments": [
      {
        "user": "zombiemonkey",
        "created_at": "2019-08-06T05:06:20Z",
        "body": "You can use arrays to get a series per key, sort by timestamp then calculate pretty much anything on the series. Performance on array vectors is certainly not the same as column vectors however if you consider the fact you have to sort to get things in order you end up with performance that roughly matches the new optimize order by primary key functionality. GROUP BY can also spill to disk which can help the memory issue.\r\n\r\nExample assuming positive values only - generate 1000 individual series and some metrics\r\n\r\n```\r\nDROP TABLE IF EXISTS timeseries;\r\n\r\nCREATE TABLE timeseries\r\nENGINE = MergeTree\r\nPARTITION BY toStartOfDay(ts)\r\nORDER BY (key, ts) AS\r\nSELECT \r\n    concat('device-', toString(number % 1000)) AS key, \r\n    toDateTime('2019-01-01 00:00:00') + toIntervalSecond(rand(number) % 1000) AS ts, \r\n    any(rand(number + 100000)) AS value\r\nFROM numbers(1000000)\r\nGROUP BY key, ts;\r\n```\r\n\r\nCalculate the delta and convert back to column\r\n\r\n```\r\nselect\r\n    key,\r\n    quantilesExactWeighted(0.5)(_value, _delta) AS q\r\nfrom (\r\n    with\r\n        arrayMap(i -> (_s[i].1, _s[i].2, abs(_s[i+1].2 - _s[i].2)), arrayEnumerate(arraySort(x -> x.1, groupArray((ts, value))) AS _s)) AS _d\r\n    select\r\n        key,\r\n        (arrayJoin(_d) AS series).1 AS _ts,\r\n        series.2 AS _value,\r\n        series.3 AS _delta\r\n    from \r\n        timeseries\r\n    group by key\r\n) group by key\r\n```\r\n\r\nCalculate the delta and use -Array combiner on the array without conversion.\r\n\r\n```\r\nselect\r\n    key,\r\n    quantilesExactWeightedArray(0.5)(_d.2, _d.3) AS q\r\nfrom (\r\n    select\r\n        key,\r\n        arrayMap(i -> (_s[i].1, _s[i].2, abs(_s[i+1].2 - _s[i].2)), arrayEnumerate(arraySort(x -> x.1, groupArray((ts, value))) AS _s)) AS _d\r\n    from timeseries\r\n    group by key\r\n) group by key\r\n```"
      },
      {
        "user": "oflasch",
        "created_at": "2019-08-06T07:28:27Z",
        "body": "Many thanks for the detailed solution, looks very good. I did not know about the effect of GROUP BY on memory usage. Neat optimization!\r\n\r\nYour observation that the series must be sorted anyway brought another possible solution to mind that I wanted to document for others perhaps facing the same task:\r\nWe can simply sort the series by timestamp in descending order before calculating runningDifference(), then apply abs() to the resulting negative deltas, which should give the desired \u201cforward_delta\u201d:\r\n\r\n```\r\nSELECT \r\n    *, \r\n    abs(runningDifference(q)) AS forward_delta\r\nFROM \r\n(\r\n    SELECT \r\n        number AS n, \r\n        n * n AS q\r\n    FROM numbers(15)\r\n    ORDER BY n DESC\r\n)\r\nORDER BY n ASC\r\n\r\n\u250c\u2500\u2500n\u2500\u252c\u2500\u2500\u2500q\u2500\u252c\u2500forward_delta\u2500\u2510\r\n\u2502  0 \u2502   0 \u2502             1 \u2502\r\n\u2502  1 \u2502   1 \u2502             3 \u2502\r\n\u2502  2 \u2502   4 \u2502             5 \u2502\r\n\u2502  3 \u2502   9 \u2502             7 \u2502\r\n\u2502  4 \u2502  16 \u2502             9 \u2502\r\n\u2502  5 \u2502  25 \u2502            11 \u2502\r\n\u2502  6 \u2502  36 \u2502            13 \u2502\r\n\u2502  7 \u2502  49 \u2502            15 \u2502\r\n\u2502  8 \u2502  64 \u2502            17 \u2502\r\n\u2502  9 \u2502  81 \u2502            19 \u2502\r\n\u2502 10 \u2502 100 \u2502            21 \u2502\r\n\u2502 11 \u2502 121 \u2502            23 \u2502\r\n\u2502 12 \u2502 144 \u2502            25 \u2502\r\n\u2502 13 \u2502 169 \u2502            27 \u2502\r\n\u2502 14 \u2502 196 \u2502             0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      }
    ]
  },
  {
    "number": 6064,
    "title": "Clickhouse count is not working",
    "created_at": "2019-07-19T07:31:10Z",
    "closed_at": "2019-07-19T10:22:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6064",
    "body": "Hi,\r\n\r\nI have some issues when trying to count the number of rows of a table using a simple query like:\r\n`SELECT count()\r\nFROM mop3 \r\nWHERE (key = category) AND (value = lips)`\r\n\r\nThe table is\r\n`CREATE TABLE mop3\r\n(\r\n    customer_id Int32,\r\n    order_id Int64,\r\n    order_date_created DateTime,\r\n    key String,\r\n    value String,\r\n    quantity Int32,\r\n    unit_amount Decimal32(4),\r\n    total_amount Decimal32(4)\r\n) ENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(order_date_created)\r\nORDER BY (key, value)\r\n`\r\n\r\nThank you very much\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6064/comments",
    "author": "masosky",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2019-07-19T07:35:13Z",
        "body": "What's the issue, and what version do you use?"
      },
      {
        "user": "masosky",
        "created_at": "2019-07-19T08:09:11Z",
        "body": "I am using last version released\r\n`ClickHouse client version 19.9.2.4.\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 19.9.2 revision 54421.`\r\n\r\nError:\r\n`Received exception from server (version 19.9.2):\r\nCode: 47. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Missing columns: 'category' 'lips' while processing query: 'SELECT count() FROM mop3  WHERE (key = category) AND (value = lips)', required columns: 'key' 'category' 'value' 'lips', source columns: 'quantity' 'unit_amount' 'value' 'customer_id' 'order_id' 'total_amount' 'order_date_created' 'key'.`"
      },
      {
        "user": "vasyaabr",
        "created_at": "2019-07-19T08:13:59Z",
        "body": "Use\r\n`SELECT count() FROM mop3 WHERE (key = 'category') AND (value = 'lips')`"
      },
      {
        "user": "masosky",
        "created_at": "2019-07-19T08:16:19Z",
        "body": "Oops, I forgot the single quotes.\r\nBut I do not understand I tried that previously and I didn't work.\r\nBut now it is working!\r\n\r\nSorry for the inconvenience and thanks for the fast answer."
      }
    ]
  },
  {
    "number": 6063,
    "title": "How to drop database based on MySQL Engine",
    "created_at": "2019-07-19T06:18:38Z",
    "closed_at": "2020-08-05T03:27:42Z",
    "labels": [
      "question",
      "unfinished code"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6063",
    "body": "This problem occurs when I delete the database based on the MySQL storage engine.\r\n\r\nThis is My Create database stament:\r\n`SHOW CREATE DATABASE mydb;\r\nCREATE DATABASE mydb ENGINE = MySQL('localhost:3306', 'docker', 'docker', 'docker')`\r\n\r\nThis is Exception when I try to drop database:\r\n`Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: MySQL database engine does not support remove table..`\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6063/comments",
    "author": "rangez",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-19T11:11:25Z",
        "body": "CC @zhang2014 "
      },
      {
        "user": "zhang2014",
        "created_at": "2019-07-22T02:51:18Z",
        "body": "I'll try to fix it. you can currently drop the database using the following command:\r\n```\r\nclickhouse :) DETACH DATABASE {need drop database name}\r\nclickhouse :) exit\r\n~ cd {clickhouse data path}\r\n~ rm -rf metadata/{need drop database name}\r\n```\r\n\r\n"
      },
      {
        "user": "rangez",
        "created_at": "2019-07-30T06:50:30Z",
        "body": "thank you very much @zhang2014"
      },
      {
        "user": "tonal",
        "created_at": "2019-09-10T04:53:24Z",
        "body": "Also mysql db don`t drop if not connect to it:\r\n```\r\nhost2 :) CREATE DATABASE mysql_db ENGINE = MySQL('yandex.ru:3306', 'test_db', 'yandex_admin', '1234');\r\n\r\nCREATE DATABASE mysql_db\r\nENGINE = MySQL('yandex.ru:3306', 'test_db', 'yandex_admin', '1234')\r\n\r\nOk\r\n0 rows in set. Elapsed: 0.064 sec. \r\n\r\nhost2 :) show databases;\r\n\r\nSHOW DATABASES\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 default  \u2502\r\n\u2502 mysql_db \u2502\r\n\u2502 system   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n4 rows in set. Elapsed: 0.001 sec. \r\n\r\nhost2 :) drop database mysql_db;\r\n\r\nDROP DATABASE mysql_db\r\n\r\nReceived exception from server (version 19.13.3):\r\nCode: 48. DB::Exception: Received from localhost:9000. DB::Exception: MySQL database engine does not support remove table.. \r\n\r\n0 rows in set. Elapsed: 0.313 sec. \r\n```"
      },
      {
        "user": "zhang2014",
        "created_at": "2019-09-10T12:36:41Z",
        "body": "This is a feature that hasn't been implemented for some reason, and I will implement them as soon as possible : )"
      },
      {
        "user": "jigetage",
        "created_at": "2019-10-17T01:46:49Z",
        "body": "detach database test-db"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-08-04T20:43:31Z",
        "body": "I have to mention that silly automatic comments from \"robot-clickhouse\" and from \"blinkov\" is just a personal idea of one of our friends and we strongly discourage this idea."
      },
      {
        "user": "zhang2014",
        "created_at": "2020-08-05T03:23:31Z",
        "body": "maybe we can close this issue ?"
      },
      {
        "user": "gempir",
        "created_at": "2024-10-01T15:03:38Z",
        "body": "`DETACH DATABASE my_db` just loads forever for me, same with `DROP`\r\n\r\n```\r\nTimeout exceeded while receiving data from server. Waited for 300 seconds, timeout is 300 seconds.\r\nCancelling query.\r\n```\r\n\r\nThe database in question was firewalled and I do not get a response, I think ClickHouse is trying to wait for a response from that server. \r\n\r\nIs there a way to remove the db without having to remove some magic files?"
      }
    ]
  },
  {
    "number": 6050,
    "title": "19.11-*-stable dictionaries loading failed",
    "created_at": "2019-07-18T12:47:34Z",
    "closed_at": "2019-07-18T19:54:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6050",
    "body": "After upgrading to 19.11 dictionaries loading fail with error:\r\n\r\n```\r\n2019.07.18 13:46:47.501892 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'interserver_http_host', expected 'dictionary'\r\n2019.07.18 13:46:47.502034 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host', expected 'dictionary'\r\n2019.07.18 13:46:47.502177 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host[1]', expected 'dictionary'\r\n2019.07.18 13:46:47.502256 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host[2]', expected 'dictionary'\r\n2019.07.18 13:46:47.502324 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'logger', expected 'dictionary'\r\n2019.07.18 13:46:47.502388 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'profiles', expected 'dictionary'\r\n2019.07.18 13:46:47.502472 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'max_concurrent_queries', expected 'dictionary'\r\n2019.07.18 13:46:47.502539 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'zookeeper-servers', expected 'dictionary'\r\n\r\n```\r\n\r\n cat /etc/clickhouse-server/dnl_dictionary.xml\r\n\r\n```\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n  <dictionary>\r\n    <name>hosts</name>\r\n    <source>\r\n      <odbc>\r\n        <connection_string>DSN=PostgreSQLCHglobal</connection_string>\r\n        <table>hosts</table>\r\n      </odbc>\r\n......\r\n......\r\n......\r\n</yandex>\r\n\r\n```\r\nWorks on 19.9.4.34 and older.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6050/comments",
    "author": "mikeeremin",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-18T13:45:06Z",
        "body": "Dictionaries will load successfully regardless to this warning.\r\n\r\nThis warning present, because some config elements get substituted into `/etc/clickhouse-server/dnl_dictionary.xml`. You can check it in `/etc/clickhouse-server/preprocessed/...` directory.\r\n\r\nExtraneous config elements are substituted from `conf.d` directory.\r\nYou can override configuration with files from `conf.d` or _config_name_.d directory, for example `config.d`, `users.d` or `dnl_dictionary.d`. Overrides from `conf.d` directory will be substituted into every configuration file while overrides from _config_name_.d directories will be substituted only to the corresponding configuration file."
      },
      {
        "user": "mikeeremin",
        "created_at": "2019-07-18T14:00:11Z",
        "body": "```\r\nclickhouse-test-node1 :) system reload dictionaries;\r\n\r\nSYSTEM RELOAD DICTIONARIES\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.005 sec.\r\n\r\nclickhouse-test-node1 :) select * from system.dictionaries\\G\r\n\r\nSELECT *\r\nFROM system.dictionaries\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname:               programreleasefiles\r\nstatus:             NOT_LOADED\r\norigin:             /etc/clickhouse-server/dnl_dictionary.xml\r\ntype:\r\nkey:\r\nattribute.names:    []\r\nattribute.types:    []\r\nbytes_allocated:    0\r\nquery_count:        0\r\nhit_rate:           0\r\nelement_count:      0\r\nload_factor:        0\r\nsource:\r\nloading_start_time: 0000-00-00 00:00:00\r\nloading_duration:   0\r\nlast_exception:\r\n\r\n\r\n```"
      },
      {
        "user": "vitlibar",
        "created_at": "2019-07-18T15:43:06Z",
        "body": "`SYSTEM RELOAD DICTIONARIES` reloads only those dictionaries which have been loaded before.\r\nClickhouse has not loaded `programreleasefiles` before so the command `SYSTEM RELOAD DICTIONARIES` doesn't reload it.\r\n\r\nIf you want `programreleasefiles` to be loaded use\r\n\r\n```\r\nSYSTEM RELOAD DICTIONARY programreleasefiles\r\n```\r\n\r\nor just start using this dictionary\r\n```\r\nSELECT dictGetUInt64('programreleasefiles', 'a', 1)\r\n```\r\n"
      },
      {
        "user": "mikeeremin",
        "created_at": "2019-07-18T19:54:16Z",
        "body": "Worked, thanks.\r\n"
      },
      {
        "user": "fessmage",
        "created_at": "2019-09-05T18:56:39Z",
        "body": "I got NOT_LOADED state for every dictionary after update of clickhouse-server from 18.* to 19.* and only thing that worked for me - advice from @vitlibar. `system reload dictionary <dictionary_name>` for every dictionary - fixes problem."
      },
      {
        "user": "filimonov",
        "created_at": "2019-09-06T00:26:55Z",
        "body": "> I got NOT_LOADED state for every dictionary after update of clickhouse-server from 18.* to 19.* and only thing that worked for me - advice from @vitlibar. `system reload dictionary <dictionary_name>` for every dictionary - fixes problem.\r\n\r\nTry disabling lazy loading of dictionaries if you need them preinitialized. "
      },
      {
        "user": "fessmage",
        "created_at": "2019-09-06T06:45:08Z",
        "body": "Oh, thanks, i understood reason now. On server start i have problem with `clickhouse-odbc-bridge` - forgot to update it with rest packages. And with setting `dictionaries_lazy_load` defaults true - because of that error with odbc dictionaries, it resulted in all dictionaries not loaded."
      },
      {
        "user": "Naveen071110",
        "created_at": "2020-04-27T14:20:22Z",
        "body": "when i type system reload dictionary dictionary_name\r\nThen it shows that connection to all replicas failed!\r\nCan you explain why?\r\n"
      }
    ]
  },
  {
    "number": 6030,
    "title": "order by with formatReadableSize orders alphabetically instead of numerically",
    "created_at": "2019-07-17T01:37:49Z",
    "closed_at": "2019-07-17T10:50:04Z",
    "labels": [
      "question",
      "usability"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6030",
    "body": "This is probably due to the `formatReadableSize` being applied before the results get displayed, but it would be super nice if order by still ordered by the original values.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6030/comments",
    "author": "abraithwaite",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-07-17T02:04:29Z",
        "body": "this is contradicts SQL.\r\n\r\nyou can use \r\nselect formatReadableSize(size)\r\n...\r\norder by size"
      },
      {
        "user": "filimonov",
        "created_at": "2019-07-17T10:50:04Z",
        "body": "It is not a bug. `formatReadableSize` return strings, so they are ordered as strings.\r\n\r\nUse the approach shown in @den-crane 's answer,  "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-17T10:51:14Z",
        "body": "@abraithwaite \r\n\r\nConsider more simple example:\r\n\r\n```\r\nSELECT toString(number) AS x FROM numbers(100) ORDER BY x\r\n```\r\n\r\nIt's obvious that this query sort data alphabetically (by String value). And it is the only way it should work.\r\n\r\n`formatReadableSize` is similar. This function returns `String` data type, not some `String but sort numerically` magic.\r\n"
      },
      {
        "user": "abraithwaite",
        "created_at": "2019-07-17T16:15:54Z",
        "body": "Thanks!  I was aware that it was being converted to a string but didn't realize I could order by the original field.  Works great!"
      }
    ]
  },
  {
    "number": 6013,
    "title": "Table info  isn't show on system.parts ",
    "created_at": "2019-07-15T15:57:24Z",
    "closed_at": "2019-07-16T02:15:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6013",
    "body": "I created table (for example XX), queried successfully. \r\nHow ever I can't see any info of table XX when I query info from system.parts. \r\nWhen I run `Select table from system.parts` that didn't display XX. It displayed 4 names: cpu, cpu, cpu, tags.\r\n\r\nCould I find the information of a table I created in database (such name, size,..)?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6013/comments",
    "author": "ngoanpv",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-07-15T19:08:25Z",
        "body": "system.parts has information about parts of MergeTree tables.\r\nIf XX is empty it has no parts, and the size of the X is 0.\r\n\r\nFor engines *Log you can use OS utilities only [du -sh /var/lib/clickhouse/data/db/XX]\r\nFor engine Memory no way at all."
      },
      {
        "user": "ngoanpv",
        "created_at": "2019-07-16T02:15:18Z",
        "body": "Very clear, thank you. @den-crane "
      }
    ]
  },
  {
    "number": 5853,
    "title": "How to pretty print a generated RowBinary file?",
    "created_at": "2019-07-03T14:20:07Z",
    "closed_at": "2019-07-05T14:07:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5853",
    "body": "Hello all,\r\n\r\nGiven a valid ClickHouse RowBinary file, how I can prettyprint the row data within the file?\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5853/comments",
    "author": "Jack012a",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-03T14:30:36Z",
        "body": "Sure! It's very easy to do with `clickhouse-local`:\r\n\r\n```\r\nclickhouse-local --structure \"x UInt8, y String, ...\" --query \"SELECT * FROM table\" --input-format RowBinary --output-format TSV < file\r\n```\r\n\r\nYou have to know correct data structure (`--structure` parameter).\r\n\r\nYou can use `clickhouse-local` tool for converting between various formats and for data processing without a server."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-03T14:31:31Z",
        "body": "If you want **pretty**print, use `--output-format Pretty`."
      },
      {
        "user": "Jack012a",
        "created_at": "2019-07-05T13:38:50Z",
        "body": "It works for me.\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 5677,
    "title": "How much RAM is required for ZooKeeper?",
    "created_at": "2019-06-19T01:48:52Z",
    "closed_at": "2019-06-19T07:27:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5677",
    "body": "Is there a guide for memory usage of ZooKeeper for ClickHouse?\r\n\r\nMy system uses the ReplicatedMergeTree, creates 3000 parts for one day and stores it about one year.\r\n\r\nI set 8GB memory for ZooKeeper. I don't know whether is it enough or not.\r\n\r\nI hope your help.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5677/comments",
    "author": "achimbab",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-06-19T02:10:41Z",
        "body": "ZK is in-memory database. \r\n\r\nSo lets check ZK database size\r\n\r\necho mntr | nc localhost 2181|grep zk_approximate_data_size\r\nzk_approximate_data_size\t68723698\r\n\r\n~68MB\r\n\r\nThe same size as snapshot size\r\ndu -sh /var/lib/zookeeper/version-2/snapshot.4350f30c19\r\n77M\t/var/lib/zookeeper/version-2/snapshot.4350f30c19\r\n\r\nSo in my case ZK needs 77MB+~200MB JVM needs.\r\nI would say 500MB is enough.\r\n\r\n\r\nCheck a MergeTree setting **use_minimalistic_part_header_in_zookeeper**. If enabled, Replicated tables will store compact part metadata in a single part znode. This can dramatically reduce ZooKeeper snapshot size (especially if the tables have a lot of columns). Note that after enabling this setting you will not be able to downgrade to a version that doesn't support it."
      },
      {
        "user": "achimbab",
        "created_at": "2019-06-19T04:53:18Z",
        "body": "@den-crane \r\nThe use_minimalistic_checksums_in_zookeeper is enabled.\r\n```\r\n:) select * from system.merge_tree_settings\r\n\r\n... \r\nuse_minimalistic_checksums_in_zookeeper                    \u2502 1            \u2502       0 \u2502\r\n...\r\n```\r\nMy ZooKeeper's zk_approximate_data_size is ```1219607498.```\r\n\r\nI will monitor how this value grows.\r\n\r\nIt's very helpful, thank you very much."
      }
    ]
  },
  {
    "number": 5592,
    "title": "ALTER column datatype timeouts. What's the best way?",
    "created_at": "2019-06-12T09:11:26Z",
    "closed_at": "2019-06-12T14:44:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5592",
    "body": "When trying to change column's datatype\r\n```\r\n:) alter table sflow modify column column_1 LowCardinality(UInt8);\r\n\r\nALTER TABLE t1\r\n    MODIFY COLUMN\r\n    `column_1` LowCardinality(UInt8)\r\n```\r\nGetting response\r\n```\r\nTimeout exceeded while receiving data from server. Waited for 300 seconds, timeout is 300 seconds.\r\nCancelling query.\r\n```\r\n\r\nIs the only way to increase timeout / recreate the table?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5592/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-06-12T14:34:41Z",
        "body": ">Timeout exceeded while receiving data from server. \r\n\r\nThis client's timeout because of zero network activity. You can tune receive_timeout in client config.\r\n\r\nAlter was not canceled. It worked until the end. \r\n\r\n>Is the only way to increase timeout / recreate the table?\r\n\r\nyou can do nothing because nothing bad happened. You just got several false errors. \r\nAnd you can increase receive_timeout / send_timeout in arguments of client or /etc/clickhouse-client/config.xml"
      },
      {
        "user": "den-crane",
        "created_at": "2019-06-12T14:41:22Z",
        "body": "LowCardinality(UInt8) -- is nonsense. It is ALWAYS worse than UInt8.\r\nLowCardinality makes sense only for String type."
      },
      {
        "user": "simPod",
        "created_at": "2019-06-12T14:44:04Z",
        "body": "_Yea, sorry, using it with strings always._\r\n\r\nThanks for clarifying."
      }
    ]
  },
  {
    "number": 5512,
    "title": "Is there accumulate for array?",
    "created_at": "2019-06-02T09:10:53Z",
    "closed_at": "2019-06-05T13:45:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5512",
    "body": "data : [1, 4, 6, 8]\r\n\r\nresult : 1 * 4 * 6 * 8 = 192",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5512/comments",
    "author": "lzy305",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-06-04T15:49:53Z",
        "body": "```\r\n\r\nSELECT exp2(arraySum(x -> log2(x), [1, 4, 6, 8]))\r\n\r\n\u250c\u2500exp2(arraySum(lambda(tuple(x), log2(x)), [1, 4, 6, 8]))\u2500\u2510\r\n\u2502                                                     192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "lzy305",
        "created_at": "2019-06-05T13:45:22Z",
        "body": "Good idea, Thanks"
      }
    ]
  },
  {
    "number": 5351,
    "title": "Clickhouse Add date column to new table with specific date",
    "created_at": "2019-05-20T18:07:04Z",
    "closed_at": "2019-05-20T18:34:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5351",
    "body": "table 1----- without date column\r\ntable 2----- want to insert date column with a specific date\r\n\r\nQuery--- insert into table 2\r\n               select a,b,c,d,NOW() from table 1 \r\n\r\nNOW() will insert current date but I want to insert a specific date  into table 2\r\n\r\nEX: 2019-12-09\r\n\r\nPlease help me with the query as early as possible\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5351/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-05-20T18:22:01Z",
        "body": "```\r\ninsert into table2\r\nselect a,b,c,d,toDate('2019-12-09') from table1\r\n```"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-05-20T18:22:50Z",
        "body": "i tried this and its not working :("
      },
      {
        "user": "den-crane",
        "created_at": "2019-05-20T18:26:43Z",
        "body": "> i tried this and its not working :(\r\n\r\nwith what error ???"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-05-20T18:34:21Z",
        "body": "Hi sorry i was using todate instead of toDate('YYYY-MM-DD')\r\n\r\nIts working and thanks for the quick reply :)"
      }
    ]
  },
  {
    "number": 5205,
    "title": "Error when try get data from Dictionary by ip as Nullabel(String)",
    "created_at": "2019-05-06T14:08:43Z",
    "closed_at": "2019-05-06T14:34:38Z",
    "labels": [
      "question",
      "comp-dictionary"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5205",
    "body": "I am using Maxmind GeoIp database as Dictionary\r\n\r\n```\r\n<dictionary>\r\n        <name>geoip_city_blocks_ipv4</name>\r\n        <source>\r\n            <file>\r\n                <path>/etc/clickhouse-server/geoip/City/GeoLite2-City-Blocks-IPv4.csv</path>\r\n                <format>CSVWithNames</format>\r\n            </file>\r\n        </source>\r\n        <lifetime>300</lifetime>\r\n        <layout>\r\n            <ip_trie/>\r\n        </layout>\r\n        <structure>\r\n            <key>\r\n                <attribute>\r\n                    <name>prefix</name>\r\n                    <type>String</type>\r\n                </attribute>\r\n            </key>\r\n            <attribute>\r\n                <name>geoname_id</name>\r\n                <type>UInt32</type>\r\n                <null_value>0</null_value>\r\n            </attribute>\r\n...\r\n```\r\n\r\nDictionary have data like:\r\n\r\n```\r\n\u2500prefix\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500geoname_id\u2500\u2510\r\n\u2502 100::/24      \u2502    2070830 \u2502\r\n\u2502 100:100::/24  \u2502    1811017 \u2502\r\n\u2502 100:200::/23  \u2502    1811017 \u2502\r\n\u2502 100:400::/22  \u2502    2077456 \u2502\r\n\u2502 100:800::/21  \u2502    1809935 \u2502\r\n\u2502 100:1000::/20 \u2502    1861060 \u2502\r\n\u2502 100:2000::/19 \u2502    1809935 \u2502\r\n\u2502 100:4000::/23 \u2502    1862415 \u2502\r\n\u2502 100:4200::/23 \u2502    1850147 \u2502\r\n\u2502 100:4400::/22 \u2502    1863018 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd I have table with ips\r\n\r\n```\r\nCREATE TABLE mautic.test (\r\n  `id` Int32,\r\n  `ip` Nullable(String)\r\n) ENGINE = MergeTree() ORDER BY (id);\r\n\r\nINSERT INTO mautic.test VALUES\r\n    (1, '174.105.199.64'),\r\n    (2, '40.107.219.92'),\r\n    (3, '40.107.219.59'),\r\n    (4, '65.246.27.210'),\r\n    (5, '50.98.35.219'),\r\n    (6, '70.67.156.137');\r\n```\r\n\r\nI am trying get data from dictionary\r\n\r\n```\r\nSELECT\r\n    ip,\r\n    dictGet('geoip_city_blocks_ipv4', 'geoname_id', tuple(IPv4StringToNum(ip))) AS geoname_id\r\nFROM\r\n    test_ip\r\n```\r\n\r\nBut getting error \r\n\r\n```\r\nCode: 53, e.displayText() = DB::Exception: Key does not match, expected either UInt32 or FixedString(16)\r\n```\r\n\r\nBut if change `ip` field type to just `String`, the same request work without any errors.\r\n\r\nI don't know is it bug or I make something wrong.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5205/comments",
    "author": "bookin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-05-06T14:15:39Z",
        "body": "Ext. Dictionaries do not support Nulls.\r\nJust use assumeNotNull(ip) .\r\n\r\n"
      },
      {
        "user": "bookin",
        "created_at": "2019-05-06T14:34:38Z",
        "body": "@den-crane, I understood, thank for help"
      }
    ]
  },
  {
    "number": 5105,
    "title": "How to reduce memory usage with arrayEnumerate?",
    "created_at": "2019-04-24T22:00:30Z",
    "closed_at": "2019-04-25T00:03:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5105",
    "body": "I have to use arrayEnumerate because of nested ( ['a','a'] k, [1,2] v ) k='a' -> [1,2]\r\nAnd I noticed that arrayEnumerate eats memory for long arrays (for example 1000 elements)\r\n\r\ncreate table X(A Array(String)) engine = Memory;\r\ninsert into X select arrayMap(x->toString (x) , range(1000)) from numbers(10);\r\n\r\nselect arrayFilter(x->x='777', A) from X format Null;\r\nPeak memory usage (for query): 1.21 MiB\r\n\r\nselect arrayFilter(x->A[x]='777', arrayEnumerate(A)) from X format Null;\r\nPeak memory usage (for query): **257.33 MiB**\r\n\r\n\r\ninsert into X select arrayMap(x->toString (x) , range(1000)) from numbers(1000);\r\n\r\nselect arrayFilter(x->x='777', A) from X format Null;\r\nPeak memory usage (for query): 14.01 MiB.\r\n\r\nselect arrayFilter(x->A[x]='777', arrayEnumerate(A)) from X format Null;\r\nPeak memory usage (for query): **16.02 GiB**\r\n\r\nselect arrayFilter(x->A[x]='777', range(length(A))) from X format Null;\r\nPeak memory usage (for query): **16.03 GiB.**\r\n\r\nHow I can reduce memory usage in this case?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5105/comments",
    "author": "den-crane",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-04-24T22:42:21Z",
        "body": "Consider the following expression:\r\n`arrayMap(x -> f(x, b), a)`\r\n\r\nTo calculate this expression, temporary array join by `a` is performed:\r\n\r\n```\r\na[1], b\r\na[2], b\r\na[3], b\r\n...\r\n```\r\n\r\nand expression is evaluated:\r\n\r\n```\r\na[1], b, f(a[1], b)\r\na[2], b, f(a[2], b)\r\na[3], b, f(a[3], b)\r\n...\r\n```\r\n\r\nthen the result is collected.\r\n\r\nIf `b` is also an array, it will be multiplied by the size of `a` in memory:\r\n\r\n```\r\na[1], ['x', 'y', 'z']\r\na[2], ['x', 'y', 'z']\r\na[3], ['x', 'y', 'z']\r\n```\r\n\r\nif `a` has size 3 and `b` has size 3, we need 3 * 3 = 9 elements to keep in memory for calculations."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-04-24T22:42:41Z",
        "body": "Possible workarounds:\r\n1. Lower `max_block_size`."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-04-24T22:45:52Z",
        "body": "Possible solutions:\r\n1. Automatically use `LowCardinality` data type for array joining. But LowCardinality arrays are not supported."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-04-24T22:51:27Z",
        "body": "create table X(A Array(String)) engine = **TinyLog**;\r\ninsert into X select arrayMap(x->toString (x) , range(1000)) from numbers(1000);\r\nSET send_logs_level = 'trace', **max_block_size = 1**;\r\nselect arrayFilter(x->A[x]='777', arrayEnumerate(A)) from X format Null;\r\n\r\nPeak memory usage (for query): 18.08 MiB."
      },
      {
        "user": "den-crane",
        "created_at": "2019-04-25T00:02:36Z",
        "body": "Thank you.\r\n\r\nIt seems I always used arrayEnumerate to iterate & filter through nested (K,V) by mistake. \r\narrayFilter is able to do pure magic.\r\n\r\n```\r\nselect arrayFilter((v, k) -> k = 'a', values, keys) from\r\n(select ['a','a','b','a'] keys, [1,2,3,4] values)\r\n--\r\n[1,2,4]\r\n\r\n```\r\nselect arrayFilter((v, k)-> k = '777', A, A) from X format Null;\r\nPeak memory usage (for query): 14.20 MiB.\r\n"
      }
    ]
  },
  {
    "number": 4893,
    "title": "ngrambf_v1 index not accessible",
    "created_at": "2019-04-03T12:32:25Z",
    "closed_at": "2019-04-04T20:28:13Z",
    "labels": [
      "question",
      "comp-skipidx"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4893",
    "body": "Hello \r\n\r\n\r\nI'm trying to create or alter a table to index of type ngrambf_v1.  But the server refuse the creation of the index  with the following error message : Index type 'ngrambf_v1'. Available index types: set, minmax\r\n\r\n**How to reproduce**\r\njust create of alter a table to add an index of type : ngrambf or tokenbf\r\n\r\n* Which ClickHouse server version to use\r\n\r\n19.4.3.11\r\n\r\n* Which interface to use, if matters\r\nHTTP Interface thru TABIX\r\n\r\n* Non-default settings, if any\r\nallow_experimental_data_skipping_indices=1\r\n\r\n**Expected behavior**\r\nIndex of the specified type created.\r\n\r\n**Error message and/or stacktrace**\r\n2019.04.03 12:26:29.478866 [ 28 ] {5371c57b-1508-4038-ad3d-c95cbd8983c0} <Error> executeQuery: Code: 80, e.displayText() = DB::Exception: Unknown Index type 'ngrambf_v1'. Available index types: set, minmax (from [::ffff:10.1.30.136]:62140) (in query:  /*TABIX_QUERY_ID_lG85IYRl*/    alter table IPANEMA.FLOW add INDEX src_site_idx (src_site,lower(src_site)) TYPE ngrambf_v1(3, 256, 4, 0) GRANULARITY 4), Stack trace:\r\n\r\n0. /usr/bin/clickhouse-server(StackTrace::StackTrace()+0x16) [0x6f73536]\r\n1. /usr/bin/clickhouse-server(DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)+0x22) [0x3712ab2]\r\n2. /usr/bin/clickhouse-server(DB::MergeTreeIndexFactory::get(DB::NamesAndTypesList const&, std::shared_ptr<DB::ASTIndexDeclaration>, DB::Context const&) const+0x485) [0x6735bd5]\r\n3. /usr/bin/clickhouse-server(DB::MergeTreeData::setPrimaryKeyIndicesAndColumns(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::IAST>, DB::ColumnsDescription const&, DB::IndicesDescription const&, bool)+0xb0b) [0x66ebb5b]\r\n4. /usr/bin/clickhouse-server(DB::MergeTreeData::checkAlter(DB::AlterCommands const&, DB::Context const&)+0x845) [0x66efc95]\r\n5. /usr/bin/clickhouse-server(DB::StorageMergeTree::alter(DB::AlterCommands const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, DB::Context const&)+0xc4) [0x664ab54]\r\n6. /usr/bin/clickhouse-server(DB::InterpreterAlterQuery::execute()+0x647) [0x6a0c8e7]\r\n7. /usr/bin/clickhouse-server() [0x655cb1f]\r\n8. /usr/bin/clickhouse-server(DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::function<void (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>, std::function<void (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>)+0xfa) [0x655e8aa]\r\n9. /usr/bin/clickhouse-server(DB::HTTPHandler::processQuery(Poco::Net::HTTPServerRequest&, HTMLForm&, Poco::Net::HTTPServerResponse&, DB::HTTPHandler::Output&)+0x35f6) [0x372e1b6]\r\n10. /usr/bin/clickhouse-server(DB::HTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&)+0x3f1) [0x3730181]\r\n11. /usr/bin/clickhouse-server(Poco::Net::HTTPServerConnection::run()+0x32c) [0x707ceec]\r\n12. /usr/bin/clickhouse-server(Poco::Net::TCPServerConnection::start()+0xf) [0x7076eef]\r\n13. /usr/bin/clickhouse-server(Poco::Net::TCPServerDispatcher::run()+0xe9) [0x7077629]\r\n14. /usr/bin/clickhouse-server(Poco::PooledThread::run()+0x81) [0x712a761]\r\n15. /usr/bin/clickhouse-server(Poco::ThreadImpl::runnableEntry(void*)+0x38) [0x7126928]\r\n16. /usr/bin/clickhouse-server() [0xadef39f]\r\n17. /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7ff1ad3a16ba]\r\n18. /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7ff1ac9c241d]\r\n\r\n\r\nThanks in advance \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4893/comments",
    "author": "TH-HA",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-04-04T20:27:58Z",
        "body": "The minimum version is `v19.5.1.246` that is available in testing."
      },
      {
        "user": "TH-HA",
        "created_at": "2019-04-05T16:55:13Z",
        "body": "Thanks a lot for your answer. \r\nA small comment on this part , it may be usefull to avoid such kind of question to put on the actual documentation the clickhouse version on which the documentation apply. I was naively and wrongly  thinking that it was always on the latest stable version. \r\nRegards  "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-04-05T17:18:32Z",
        "body": "Yes, this is a frequent complaint. Today, the docs are not in sync with any ClickHouse release.\r\n(For most features, the docs lag behind, but there are some features where docs are prepared in advance.)\r\n\r\nWe're only going to prepare a better process of docs publishing. I cannot guarantee when it will happen."
      }
    ]
  },
  {
    "number": 4762,
    "title": "ch complains \"Password required for user default., e.what() = DB::Exception.\" when drop a partition .But actually it has the user. And when try many times it will succeed",
    "created_at": "2019-03-22T08:58:55Z",
    "closed_at": "2019-03-22T10:12:13Z",
    "labels": [
      "question",
      "comp-dddl"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4762",
    "body": "**Version of ch:**\r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) select version()\r\n\r\nSELECT version()\r\n\r\n\u250c\u2500version()\u2500\u2510\r\n\u2502 18.14.19  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n**Cluster Config:**\r\n\r\n> SELECT * FROM system.clusters \r\n\r\n\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500shard_num\u2500\u252c\u2500shard_weight\u2500\u252c\u2500replica_num\u2500\u252c\u2500host_name\u2500\u2500\u2500\u2500\u2500\u252c\u2500host_address\u2500\u2500\u252c\u2500port\u2500\u252c\u2500is_local\u2500\u252c\u2500user\u2500\u2500\u2500\u2500\u252c\u2500default_database\u2500\u2510\r\n\u2502 ads_model_ck_cluster \u2502         1 \u2502            1 \u2502           1 \u2502 xx.xx.30.65  \u2502 xx.xx.30.65  \u2502 9000 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         1 \u2502            1 \u2502           2 \u2502 xx.xx.40.123 \u2502 xx.xx.40.123 \u2502 9000 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         2 \u2502            1 \u2502           1 \u2502 xx.xx.30.64  \u2502 xx.xx.30.64  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         2 \u2502            1 \u2502           2 \u2502 xx.xx.30.69  \u2502 xx.xx.30.69  \u2502 9000 \u2502              0 \u2502 default \u2502                  \u2502\r\n....\r\n\u2502 ads_model_ck_cluster \u2502        15 \u2502            1 \u2502           1 \u2502 xx.xx.30.86  \u2502 xx.xx.30.86  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502        15 \u2502            1 \u2502           2 \u2502 xx.xx.30.83  \u2502 xx.xx.30.83  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n**Then Create test ReplicatedMergeTree loal table and Distribute table** \r\n\r\n> CREATE TABLE default.test1 on cluster ads_model_ck_cluster ( id UInt64,  name String, d Date) ENGINE =ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/default/test1', '{replica}') PARTITION BY toMonday(d) ORDER BY (id, d) SETTINGS index_granularity = 8192;\r\n\r\n> CREATE TABLE default.test1_all on cluster ads_model_ck_cluster (id UInt64,  name String, d Date) ENGINE = Distributed('ads_model_ck_cluster', 'test', 'test1', rand());\r\n\r\nINSERT some volume data into the table \r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :)  select count() from test1_all;\r\n\r\nSELECT count()\r\nFROM test1_all \r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502 4390912 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n**Get Detailed info of the partitions:**\r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) select database,table,partition_id,partition,name,active,rows,path,modification_time from system.parts where table='test1';\r\n\r\nSELECT \r\n    database, \r\n    table, \r\n    partition_id, \r\n    partition, \r\n    name, \r\n    active, \r\n    rows, \r\n    path, \r\n    modification_time\r\nFROM system.parts \r\nWHERE table = 'test1'\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500partition_id\u2500\u252c\u2500partition\u2500\u2500\u2500\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500active\u2500\u252c\u2500\u2500\u2500rows\u2500\u252c\u2500path\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500modification_time\u2500\u2510\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_25_0 \u2502      0 \u2502      1 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_25_0/ \u2502 2019-03-22 16:05:07 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_30_1 \u2502      0 \u2502    123 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_30_1/ \u2502 2019-03-22 16:05:14 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_35_2 \u2502      0 \u2502   4642 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_35_2/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_40_3 \u2502      0 \u2502 146165 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_40_3/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_43_4 \u2502      1 \u2502 290307 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_43_4/ \u2502 2019-03-22 16:07:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_26_26_0 \u2502      0 \u2502      8 \u2502 /export/data/clickhouse/data/default/test1/20190318_26_26_0/ \u2502 2019-03-22 16:05:11 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_27_27_0 \u2502      0 \u2502     11 \u2502 /export/data/clickhouse/data/default/test1/20190318_27_27_0/ \u2502 2019-03-22 16:05:11 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_28_28_0 \u2502      0 \u2502     17 \u2502 /export/data/clickhouse/data/default/test1/20190318_28_28_0/ \u2502 2019-03-22 16:05:12 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_29_29_0 \u2502      0 \u2502     26 \u2502 /export/data/clickhouse/data/default/test1/20190318_29_29_0/ \u2502 2019-03-22 16:05:13 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_30_30_0 \u2502      0 \u2502     60 \u2502 /export/data/clickhouse/data/default/test1/20190318_30_30_0/ \u2502 2019-03-22 16:05:13 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_31_31_0 \u2502      0 \u2502    142 \u2502 /export/data/clickhouse/data/default/test1/20190318_31_31_0/ \u2502 2019-03-22 16:05:14 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_32_32_0 \u2502      0 \u2502    305 \u2502 /export/data/clickhouse/data/default/test1/20190318_32_32_0/ \u2502 2019-03-22 16:05:15 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_33_33_0 \u2502      0 \u2502    553 \u2502 /export/data/clickhouse/data/default/test1/20190318_33_33_0/ \u2502 2019-03-22 16:05:15 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_34_34_0 \u2502      0 \u2502   1182 \u2502 /export/data/clickhouse/data/default/test1/20190318_34_34_0/ \u2502 2019-03-22 16:05:16 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_35_35_0 \u2502      0 \u2502   2337 \u2502 /export/data/clickhouse/data/default/test1/20190318_35_35_0/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_36_36_0 \u2502      0 \u2502   4585 \u2502 /export/data/clickhouse/data/default/test1/20190318_36_36_0/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_37_37_0 \u2502      0 \u2502   9213 \u2502 /export/data/clickhouse/data/default/test1/20190318_37_37_0/ \u2502 2019-03-22 16:05:34 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_38_38_0 \u2502      0 \u2502  18316 \u2502 /export/data/clickhouse/data/default/test1/20190318_38_38_0/ \u2502 2019-03-22 16:05:55 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_39_39_0 \u2502      0 \u2502  36600 \u2502 /export/data/clickhouse/data/default/test1/20190318_39_39_0/ \u2502 2019-03-22 16:05:58 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_40_40_0 \u2502      0 \u2502  72809 \u2502 /export/data/clickhouse/data/default/test1/20190318_40_40_0/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_41_41_0 \u2502      0 \u2502    501 \u2502 /export/data/clickhouse/data/default/test1/20190318_41_41_0/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_42_42_0 \u2502      0 \u2502  72817 \u2502 /export/data/clickhouse/data/default/test1/20190318_42_42_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_43_43_0 \u2502      0 \u2502  70824 \u2502 /export/data/clickhouse/data/default/test1/20190318_43_43_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_44_44_0 \u2502      1 \u2502   3065 \u2502 /export/data/clickhouse/data/default/test1/20190318_44_44_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n**Try to delete one partition using alter table** \r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) alter table default.test1 on cluster ads_model_ck_cluster drop partition '2019-03-18';\r\n\r\nALTER TABLE default.test1 ON CLUSTER ads_model_ck_cluster\r\n    DROP PARTITION '2019-03-18'\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.66  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  29 \u2502               18 \u2502\r\n\u2502 xx.xx.30.69  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  28 \u2502               18 \u2502\r\n\u2502 xx.xx.30.70  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  27 \u2502               18 \u2502\r\n\u2502 xx.xx.30.76  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  26 \u2502               18 \u2502\r\n\u2502 xx.xx.30.71  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  25 \u2502               18 \u2502\r\n\u2502 xx.xx.30.79  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.82:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  24 \u2502               18 \u2502\r\n\u2502 xx.xx.217.46 \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  23 \u2502               18 \u2502\r\n\u2502 xx.xx.30.86  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  22 \u2502               18 \u2502\r\n\u2502 xx.xx.30.65  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  21 \u2502               18 \u2502\r\n\u2502 xx.xx.40.123 \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  20 \u2502               18 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.72  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  19 \u2502               12 \u2502\r\n\u2502 xx.xx.30.67  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  18 \u2502               12 \u2502\r\n\u2502 xx.xx.217.49 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  17 \u2502               12 \u2502\r\n\u2502 xx.xx.30.81  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.217.52:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  16 \u2502               12 \u2502\r\n\u2502 xx.xx.217.47 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  15 \u2502               12 \u2502\r\n\u2502 xx.xx.30.74  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.78:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  14 \u2502               12 \u2502\r\n\u2502 xx.xx.30.85  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  13 \u2502               12 \u2502\r\n\u2502 xx.xx.30.77  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  12 \u2502               12 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.78  \u2502 9000 \u2502      0 \u2502       \u2502                  11 \u2502                9 \u2502\r\n\u2502 xx.xx.30.73  \u2502 9000 \u2502      0 \u2502       \u2502                  10 \u2502                9 \u2502\r\n\u2502 xx.xx.217.45 \u2502 9000 \u2502      0 \u2502       \u2502                   9 \u2502                9 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.50 \u2502 9000 \u2502      0 \u2502       \u2502                   8 \u2502                8 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.52 \u2502 9000 \u2502      0 \u2502       \u2502                   7 \u2502                7 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.82 \u2502 9000 \u2502      0 \u2502       \u2502                   6 \u2502                6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.68  \u2502 9000 \u2502      0 \u2502       \u2502                   5 \u2502                3 \u2502\r\n\u2502 xx.xx.40.126 \u2502 9000 \u2502      0 \u2502       \u2502                   4 \u2502                3 \u2502\r\n\u2502 xx.xx.30.64  \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.48 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                0 \u2502\r\n\u2502 xx.xx.30.80  \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 xx.xx.30.83  \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nReceived exception from server (version 18.14.19):\r\nCode: 194. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: There was an error on [xx.xx.30.79:9000]: Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.82:9000. DB::Exception: Password required for user default., e.what() = DB::Exception. \r\n\r\n30 rows in set. Elapsed: 1.072 sec. \r\n\r\n**Still some node has data** \r\nxx.xx.30.68 \r\n291660\r\nxx.xx.30.67 \r\n291660\r\nxx.xx.30.78 \r\n291577\r\nxx.xx.30.74 \r\n291577\r\nxx.xx.30.79 \r\n293352\r\nxx.xx.30.82 \r\n293352\r\nxx.xx.30.81 \r\n293279\r\nxx.xx.217.52 \r\n293279\r\n\r\n**Then I try it for the third Time** \r\nckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) alter table default.test1 on cluster ads_model_ck_cluster drop partition '2019-03-18';\r\n\r\nALTER TABLE default.test1 ON CLUSTER ads_model_ck_cluster\r\n    DROP PARTITION '2019-03-18'\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.78  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  29 \u2502               15 \u2502\r\n\u2502 xx.xx.30.67  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  28 \u2502               15 \u2502\r\n\u2502 xx.xx.30.72  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  27 \u2502               15 \u2502\r\n\u2502 xx.xx.30.81  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.217.52:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  26 \u2502               15 \u2502\r\n\u2502 xx.xx.30.65  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  25 \u2502               15 \u2502\r\n\u2502 xx.xx.30.86  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  24 \u2502               15 \u2502\r\n\u2502 xx.xx.217.47 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  23 \u2502               15 \u2502\r\n\u2502 xx.xx.30.66  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  22 \u2502               15 \u2502\r\n\u2502 xx.xx.30.69  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  21 \u2502               15 \u2502\r\n\u2502 xx.xx.30.70  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  20 \u2502               15 \u2502\r\n\u2502 xx.xx.30.76  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  19 \u2502               15 \u2502\r\n\u2502 xx.xx.30.71  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  18 \u2502               15 \u2502\r\n\u2502 xx.xx.30.85  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  17 \u2502               15 \u2502\r\n\u2502 xx.xx.30.82  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  16 \u2502               15 \u2502\r\n\u2502 xx.xx.30.77  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  15 \u2502               15 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.45 \u2502 9000 \u2502      0 \u2502       \u2502                  14 \u2502               12 \u2502\r\n\u2502 xx.xx.217.46 \u2502 9000 \u2502      0 \u2502       \u2502                  13 \u2502               12 \u2502\r\n\u2502 xx.xx.40.123 \u2502 9000 \u2502      0 \u2502       \u2502                  12 \u2502               12 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.52 \u2502 9000 \u2502      0 \u2502       \u2502                  11 \u2502               11 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.50 \u2502 9000 \u2502      0 \u2502       \u2502                  10 \u2502                6 \u2502\r\n\u2502 xx.xx.30.73  \u2502 9000 \u2502      0 \u2502       \u2502                   9 \u2502                6 \u2502\r\n\u2502 xx.xx.40.126 \u2502 9000 \u2502      0 \u2502       \u2502                   8 \u2502                6 \u2502\r\n\u2502 xx.xx.217.48 \u2502 9000 \u2502      0 \u2502       \u2502                   7 \u2502                6 \u2502\r\n\u2502 xx.xx.30.79  \u2502 9000 \u2502      0 \u2502       \u2502                   6 \u2502                6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.68  \u2502 9000 \u2502      0 \u2502       \u2502                   5 \u2502                4 \u2502\r\n\u2502 xx.xx.217.49 \u2502 9000 \u2502      0 \u2502       \u2502                   4 \u2502                4 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.64 \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                2 \u2502\r\n\u2502 xx.xx.30.74 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.80 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 xx.xx.30.83 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nReceived exception from server (version 18.14.19):\r\nCode: 194. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: There was an error on [xx.xx.30.67:9000]: Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception. \r\n\r\n30 rows in set. Elapsed: 1.071 sec. \r\n\r\n**Although there are errors warning, the data is deleted successfully.**\r\nThis is very strange. Can anyone give a hint on this?  \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4762/comments",
    "author": "inolddays",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-03-22T09:49:05Z",
        "body": "Duplicate of #1861 \r\n\r\nPR #3598 fixes that (should work properly in versions > 19.1)\r\n\r\nGenerally, when you do some DDL on non-leader replica it forwards the request to a leader, and during that forwarding \"default\" passwordless user was used. "
      },
      {
        "user": "inolddays",
        "created_at": "2019-03-22T10:08:49Z",
        "body": "Many Thanks! Alexey "
      }
    ]
  },
  {
    "number": 4737,
    "title": "Confused about server binaries size (deb)",
    "created_at": "2019-03-20T14:20:05Z",
    "closed_at": "2019-03-21T08:25:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4737",
    "body": "Hello. \r\nI'm a little confused about clickhouse binaries size. I was going to upgrade version from 18 to 19 and noticed deb packets size: \r\n\r\n    clickhouse-server-base_18.16.1_amd64.deb                339M\r\n    clickhouse-server-base_19.4.1.3_amd64.deb               734M\r\n\r\nTwo questions:\r\n1) Is it correct that size doubles for a major version?\r\n2) What is included in a distribution that takes ~1.8G for a server binary (/usr/bin/clickhouse unarchived) in 19.4.1.3 version? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4737/comments",
    "author": "nitso",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-03-20T15:19:56Z",
        "body": "you need only 3 debs\r\n\r\n```\r\n 103M Mar 18 12:57 clickhouse-common-static_19.5.1.122_amd64.deb\r\n 12K Mar 18 12:57 clickhouse-client_19.5.1.122_all.deb\r\n 24K Mar 18 12:57 clickhouse-server_19.5.1.122_all.deb\r\n```\r\n\r\n242M  /usr/bin/clickhouse"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-03-20T21:46:48Z",
        "body": "`\u0441lickhouse-server-base` is an obsolete package that has embedded debug info.\r\nMost of its size is debug information. There is a lot of debug info due to large number of C++ template instantiations. You can obtain debug info separately from `clickhouse-common-static-dbg` package.\r\n\r\nThe size of `clickhouse-common-static` package (the main package with `clickhouse` binary) has increased from 69M (for 18.16) to 102M (for 19.4). This is Ok and caused by addition of new features (mostly for HDFS and Parquet integration)."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-03-20T21:49:20Z",
        "body": "PS. JFYI, number of commits since version 18.16 was increased by 14% and number of LOC in `dbms` directory - by 9%."
      },
      {
        "user": "nitso",
        "created_at": "2019-03-21T08:25:25Z",
        "body": "Thanks a lot for explanation. "
      }
    ]
  },
  {
    "number": 4716,
    "title": "Question around set indexes",
    "created_at": "2019-03-17T22:20:10Z",
    "closed_at": "2019-03-18T09:48:30Z",
    "labels": [
      "question",
      "comp-skipidx"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4716",
    "body": "Hi there,\r\n\r\nI'm a bit confused about set indexes and where they kick in. I've tried the examples in the tests folder and they work as expected however if I try to apply them in my case granules are never dropped. I've tried with larger values and more partitions (use toStartOfHour(ts) below) but set indexes never kick in. Am I mistaken in their use/purpose and have to wait for bloom indices or am I using them incorrectly?\r\n\r\nThanks!\r\n\r\n```\r\nSET allow_experimental_data_skipping_indices = 1;\r\n\r\nDROP TABLE IF EXISTS test.idx_test;\r\n\r\nCREATE TABLE test.idx_test (\r\n                s_key UInt64,\r\n                id UInt32,\r\n                ts DateTime,\r\n                value UInt64,\r\n                INDEX s_key_idx (s_key) TYPE set(0) GRANULARITY 1000\r\n) ENGINE = MergeTree\r\nORDER BY (id, ts)\r\nSETTINGS index_granularity = 32;\r\n\r\nINSERT INTO test.idx_test\r\nSELECT\r\n                cityHash64(id) AS s_key,\r\n                number AS id,\r\n                toDateTime(1551398400 + rand(1)%86400) AS ts,\r\n                rand(2) AS value\r\nFROM system.numbers LIMIT 100000;\r\n\r\nSET send_logs_level = 'debug';\r\n\r\nselect * from test.idx_test where id = 3000 format PrettySpace;\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.564572 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> executeQuery: (from 127.0.0.1:34334) select * from test.idx_test where id = 3000 format PrettySpace;\r\n[clickhouse-demo] 2019.03.17 22:01:10.566056 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> test.idx_test (SelectExecutor): Key condition: (column 0 in [3000, 3000])\r\n[clickhouse-demo] 2019.03.17 22:01:10.566108 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> test.idx_test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1 marks to read from 1 ranges\r\n[clickhouse-demo] 2019.03.17 22:01:10.566366 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> executeQuery: Query pipeline:\r\n\r\nExpression\r\nExpression\r\n  Filter\r\n   MergeTreeThread\r\n\r\n               s_key     id                    ts        value\r\n\r\n16286406272394286119   3000   2019-03-01 13:11:22   3080386888\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.568391 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Information> executeQuery: Read 32 rows, 768.00 B in 0.004 sec., 8682 rows/sec., 203.51 KiB/sec.\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.568446 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> MemoryTracker: Peak memory usage (for query): 1.44 MiB.\r\n\r\n1 rows in set. Elapsed: 0.004 sec.\r\n\r\nselect * from test.idx_test where s_key = 16286406272394286119 format PrettySpace;\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:35.857723 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> executeQuery: (from 127.0.0.1:34334) select * from test.idx_test where s_key = 16286406272394286119 format PrettySpace;\r\n[clickhouse-demo] 2019.03.17 22:01:35.858430 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"s_key = 16286406272394286119\" moved to PREWHERE\r\n[clickhouse-demo] 2019.03.17 22:01:35.858879 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Key condition: unknown\r\n[clickhouse-demo] 2019.03.17 22:01:35.859056 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Index `s_key_idx` has dropped 0 granules.\r\n[clickhouse-demo] 2019.03.17 22:01:35.859096 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 3125 marks to read from 1 ranges\r\n[clickhouse-demo] 2019.03.17 22:01:35.859269 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> executeQuery: Query pipeline:\r\nExpression\r\nExpression\r\n  MergeTreeThread\r\n\r\n               s_key     id                    ts        value\r\n16286406272394286119   3000   2019-03-01 13:11:22   3080386888\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:35.864492 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Information> executeQuery: Read 100000 rows, 781.27 KiB in 0.007 sec., 14970210 rows/sec., 114.22 MiB/sec.\r\n[clickhouse-demo] 2019.03.17 22:01:35.864554 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> MemoryTracker: Peak memory usage (for query): 3.99 MiB.\r\n\r\n1 rows in set. Elapsed: 0.008 sec. Processed 100.00 thousand rows, 800.02 KB (12.81 million rows/s., 102.50 MB/s.)\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4716/comments",
    "author": "zombiemonkey",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-03-18T08:52:41Z",
        "body": "I think the reason is that `set(0)` is an empty set, so empty index was created. Try to use, for example, `set(100)`. 100 here is the max set size which is created per granule (so, if granule has more than 100 distinct values, set won't be created, and granule will never be dropped)."
      },
      {
        "user": "zombiemonkey",
        "created_at": "2019-03-18T09:48:30Z",
        "body": "Ah ok - thanks Nicolai. Have it working now. The docs say that set(0) means no limit but that must be incorrect. Cheers!"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2019-03-18T10:42:52Z",
        "body": "Well, it seems that set(0) should mean no limit, and it is fixed in #4640. Docs was correct."
      },
      {
        "user": "den-crane",
        "created_at": "2019-03-18T16:04:40Z",
        "body": "IMHO skip `INDEX s_key_idx (s_key) TYPE set(0) GRANULARITY 1000` just has no sense.\r\nIt makes a query slower and doubles the column stored data.\r\n\r\n\r\n```\r\ncreate table BX(\r\nI Int64, \r\nS String,\r\nINDEX Sx S TYPE set(0) GRANULARITY 1000\r\n) Engine=MergeTree order by I;\r\n\r\ninsert into BX select number, toString(rand()) from numbers(10000000);\r\ninsert into BX values(45645645, '666');\r\nselect * from BX where S = '666'\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.235 sec.\r\n```\r\n\r\n```\r\nalter table BX drop index Sx;\r\nselect * from BX where S = '666'\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.070 sec. Processed 10.00 million rows, 267.41 MB (143.10 million rows/s., 3.83 GB/s.)\r\n```\r\nAnd this is an expected behaviour."
      },
      {
        "user": "zombiemonkey",
        "created_at": "2019-03-30T09:23:39Z",
        "body": "@den-crane - I know :) I think you misunderstood that this was seeking clarity as to why set(0) was not working as documented. As KochetovNicolai pointed out - there was a bug fixed in #4640. 1000 was simply one of the values that was used for testing from a range of values and not the problem/issue WRT the behavior of set(0)."
      }
    ]
  },
  {
    "number": 4628,
    "title": "Top N elements group by time",
    "created_at": "2019-03-08T16:14:16Z",
    "closed_at": "2020-04-05T15:54:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4628",
    "body": "Following is a sample schema\r\n\r\n```\r\n{\r\n    name String,\r\n    dateTime String,\r\n    ....\r\n    value UInt32\r\n}\r\n```\r\n\r\nWe would like to group them by timeseries. However, we are interested only in the top 5 elements with the highest value sum. \r\n\r\n```\r\nSELECT any(name),\r\n       sum(value)\r\nFROM TABLE\r\nWHERE name IN\r\n    (SELECT name\r\n     FROM TABLE\r\n     GROUP BY name\r\n     ORDER BY sum(value) DESC\r\n     LIMIT 5)\r\nGROUP BY name,\r\n         toRelativeMinuteNum(dateTime)\r\n```\r\n\r\nThe above gives the top 5 elements in the `IN` call and which are then used to again be grouped by name with the time bucket.\r\n\r\nHowever, we would also like to get the calculated internal `sum(value)` also separately. So\r\n\r\n```\r\nSELECT any(name),\r\n       sum(value),\r\n  (SELECT name\r\n   FROM TABLE\r\n   GROUP BY name\r\n   ORDER BY sum(value) DESC\r\n   LIMIT 5) AS topSumNames\r\nFROM TABLE\r\nWHERE name IN topSumNames\r\nGROUP BY name,\r\n         toRelativeMinuteNum(dateTime)\r\n```\r\n\r\nThis above fails with the error \r\n```\r\nDB::Exception: Scalar subquery returned more than one row\r\n```\r\n\r\nQuestions:\r\n\r\n1. How to get the internal sub query value also in the select output\r\n2. Is there a better way to perform this top 5 elements\r\n\r\nThe `LIMIT BY clause` is not working, when we put in the `dateTime` in the grouping.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4628/comments",
    "author": "sundarv85",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-03-08T16:37:18Z",
        "body": "bad\r\n```\r\nSELECT name, toRelativeMinuteNum(dateTime) r,  ss, sum(value)\r\nFROM TABLE inner join (SELECT name, sum(value) ss FROM TABLE GROUP BY name ORDER BY ss DESC LIMIT 5) using name\r\nGROUP BY name, ss, r\r\n```\r\n\r\nbetter\r\n\r\n```\r\ncreate temporary table TX as \r\nSELECT name, sum(value) ss FROM TABLE GROUP BY name ORDER BY ss DESC LIMIT 5;\r\n\r\nSELECT name, toRelativeMinuteNum(dateTime) r,  ss, sum(value)\r\nFROM TABLE inner join (SELECT name, sum(value) ss FROM TX) using name\r\nWHERE name in (select name from TX)\r\nGROUP BY name,ss, r;\r\n\r\n```"
      },
      {
        "user": "sundarv85",
        "created_at": "2019-03-08T20:28:50Z",
        "body": "Thanks @den-crane. The INNER JOIN did the trick.\r\n\r\nCould you explain more the difference between having a direct `SELECT` in the inner join, vs the `temporary table`.  Why the former is `bad` while the other one is `better`\r\n\r\nThe issue is - this `sum(value)` and order changes based on the time range (1d or 1M) that is being queried. So we could not create a table that could cover various time ranges. With the `temporary table`, do you suggest to create the temporary table everytime a query is about to be executed? Also, how to make this temporary table work for parallel queries for different time ranges.."
      },
      {
        "user": "den-crane",
        "created_at": "2019-03-08T22:04:29Z",
        "body": "> Could you explain more the difference between having a direct `SELECT` in the inner join, vs the `temporary table`. Why the former is `bad` while the other one is `better`\r\n\r\nI had an idea that a filter `WHERE name in (select name from TX)` makes the query faster but now I am not so sure, because perhaps `using name` do the same in the *bad* query.\r\nThough if you have `index` by name column it can be much faster with `WHERE name in `.\r\nCheck the speed and processed rows statistics on real data. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T20:25:04Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "blinkov",
        "created_at": "2020-04-01T16:46:57Z",
        "body": "@sundarv85, do you have any further questions?"
      },
      {
        "user": "sundarv85",
        "created_at": "2020-04-05T15:54:15Z",
        "body": "Thanks @blinkov. No questions anymore. It worked. "
      }
    ]
  },
  {
    "number": 4544,
    "title": "Should Select be optimized for MATERIALIZED VIEW?",
    "created_at": "2019-03-01T10:10:30Z",
    "closed_at": "2019-03-02T15:57:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4544",
    "body": "When using materialized view as a trigger, that said used for inserting data to table A when data is inserted to table B:\r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS tableBToTableA TO tableA\r\nAS\r\nSELECT\r\n    some_fields\r\nFROM tableB\r\nGROUP BY some_fields\r\n```\r\n\r\nIs it also required to optimize the MV select so it uses partitioning key and sorting keys of `tableB`? Or it doesn't matter?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4544/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-03-02T15:54:24Z",
        "body": "Does not matter.\r\nMV does not read real tableB (except at a populate stage).\r\nMV gets inserted buffer (from insert query) and evaluates select over this buffer."
      },
      {
        "user": "simPod",
        "created_at": "2019-03-02T15:57:49Z",
        "body": "Thanks for explanation!"
      }
    ]
  },
  {
    "number": 4512,
    "title": "[Question] Too many parts error Monitoring",
    "created_at": "2019-02-26T12:54:53Z",
    "closed_at": "2019-02-27T20:28:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4512",
    "body": "Hello. \r\nAccording to our last experience with the `Too many parts` errors we would like to start monitor this metric.\r\nHow I can do this ?\r\nFor example I think about such kind of query:\r\n`select count(*) from system.merges`\r\nor maybe\r\n`select count(*) from system.merges` + `select count(*) from system.replication_queue`\r\n?\r\nOr maybe there is some already built-in metric ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4512/comments",
    "author": "igor-sh8",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-02-26T13:20:22Z",
        "body": "select value from system.asynchronous_metrics where metric = 'MaxPartCountForPartition'\r\n\r\nselect value from system.metrics where metric = 'ReadonlyReplica'"
      },
      {
        "user": "igor-sh8",
        "created_at": "2019-02-27T20:28:20Z",
        "body": "Great. Thanks. \r\nWe will try it."
      }
    ]
  },
  {
    "number": 4455,
    "title": "GROUP BY Enum attribute WITH ROLLUP ",
    "created_at": "2019-02-20T09:26:06Z",
    "closed_at": "2019-02-25T08:01:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4455",
    "body": "This question may become a bug, that's why I created issue.\r\n\r\nHere is test table with three versions of  the same Enum attribute.\r\n\r\n```\r\nCREATE table tmp_test_rollup\r\n(\r\n  UserGroupZeroBased Enum8('new' = 0, 'certain' = 1, 'test' = 2),\r\n  UserGroupOneBased  Enum8('new' = 1, 'certain' = 2, 'test' = 3),\r\n  UserGroupWithAll   Enum8('all' = 0, 'new' = 1, 'certain' = 2, 'test' = 3),\r\n  Period             UInt64,\r\n  Metric             Int64\r\n)\r\n  engine = TinyLog;\r\n\r\nINSERT INTO tmp_test_rollup (UserGroupZeroBased, UserGroupOneBased, UserGroupWithAll, Period, Metric)\r\nVALUES ('new', 'new', 'new', 1, 1000),\r\n       ('new', 'new', 'new', 2, 2000),\r\n       ('certain', 'certain', 'certain', 1, 500),\r\n       ('certain', 'certain', 'certain', 1, 100);\r\n```\r\nIf I want group by UserGroup with rollup, I can do three versions of query.\r\n\r\n1. In results of this query I'll get value `new` in rows where I expect get empty value. \r\n```\r\nSELECT Period, UserGroupZeroBased, sum(Metric)\r\nFROM tmp_test_rollup\r\nGROUP BY ROLLUP (Period, UserGroupZeroBased);\r\n``` \r\n\r\n2. In results of this query I'll get exception `Unexpected value 0 for type Enum8('new' = 1, 'certain' = 2, 'test' = 3)`\r\n```\r\nSELECT Period, UserGroupOneBased, sum(Metric)\r\nFROM tmp_test_rollup\r\nGROUP BY ROLLUP (Period, UserGroupOneBased);\r\n```\r\n3. In this query I'll get the expected correct result, but through special expression of `UserGroupWithAll` which was written for this case.\r\n```\r\nSELECT Period, UserGroupWithAll, sum(Metric)\r\nFROM tmp_test_rollup\r\nGROUP BY ROLLUP (Period, UserGroupWithAll);\r\n```\r\n\r\nResults in the first and second case correspond to expected results?\r\nSolution of this problem by extension of Enum list (as in the third example) is supposed?\r\n\r\n\r\nCH  version: 19.1.6",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4455/comments",
    "author": "levw",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-02-25T00:09:20Z",
        "body": "Clickhouse types are NOT NULLs by default, so to represent somehow the default/empty/nonexisting values it need to use some value which can be stored in that NOT NULL type.  For all numerical types (and also Enum type) the default value is 0, which is quite logical.\r\n\r\nWhen doing group by with rollup - default value for a column in rollup rows is used.\r\nThat's how that zero appears there. So i would say that this behaviour is expected. \r\n\r\nBest practice here: just always add 'empty' value to your 'not null' Enums at 0 position. Like that: \r\n```  UserGroupWithEmpty   Enum8('' = 0, 'new' = 1, 'certain' = 2, 'test' = 3),```\r\n\r\nIf you will use Nullable types NULL will be used as a default value, and you will not face taht issue, try like that:\r\n```  UserGroupWithNull   Nullable(Enum8('new' = 1, 'certain' = 2, 'test' = 3)),```\r\n(But remember that Nullable type have some performance/storage overhead) \r\n"
      },
      {
        "user": "levw",
        "created_at": "2019-02-25T08:01:35Z",
        "body": "Thanks for answer! "
      }
    ]
  },
  {
    "number": 4414,
    "title": "What is a \"granule\"?",
    "created_at": "2019-02-15T21:31:37Z",
    "closed_at": "2019-11-01T18:20:35Z",
    "labels": [
      "question",
      "comp-documentation",
      "comp-skipidx"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4414",
    "body": "The documentation on data skipping indexes states:\r\n\r\n> These indices aggregate some information about the specified expression on blocks, which consist of granularity_value granules, then these aggregates are used in SELECT queries for reducing the amount of data to read from the disk by skipping big blocks of data where where query cannot be satisfied.\r\n\r\nWhat exactly is a granule? Is it a row?\r\n\r\nAs a related question: are there plans for an index type similar to btree/hash secondary indexes of traditional RDBMS so a WHERE could efficiently look up rows without needing to be part of a prefix of the primary key or scanning all rows for the given column?\r\nAs I understand it, the current data skipping indexes basically allow only to answer the question \"does this block of rows contain the value that I am looking for?\" instead of \"which rows in this block contain the value that I am looking for\".",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4414/comments",
    "author": "arctica",
    "comments": [
      {
        "user": "alesapin",
        "created_at": "2019-02-18T16:32:26Z",
        "body": "> What exactly is a granule? Is it a row?\r\n\r\nGranule is a batch of rows of fixed size which addresses with primary key. Term make sense only for MergeTree* engine family. It can be set with setting `index_granularity=N`, default value is 8192 rows per batch. So if you use default value, you will have index per each 8192 row.\r\n\r\n> As I understand it, the current data skipping indexes basically allow only to answer the question \"does this block of rows contain the value that I am looking for?\" instead of \"which rows in this block contain the value that I am looking for\".\r\n\r\nYes, you understood correctly. This way (sparse index) of indexing is very efficient. Index is very small so it can be placed in memory. Sequential processing of group of small granules is also very fast. \r\nYou can set `index_granularity=1` (primary key per each row) and also set `GRANULARITY=1` if you want to get index per each row, but this will require a lot of memory."
      },
      {
        "user": "arctica",
        "created_at": "2019-02-19T10:43:11Z",
        "body": "Thank you for the explanation. Maybe a small piece of text could be added to the documentation like \"(a granule is one block of primary key containing `index_granularity` rows)?\r\n\r\nI see now how this index can be properly used. It only makes sense when the value being filtered for is very sparse or one needs very fine grained primary keys.\r\n\r\nAs I now understand it, the data skipping index is tied to the primary key. E.g. If I have index_granularity=8192 and GRANULARITY=1, then each 8192 rows, the index contains say the minmax for the Nth primary key.\r\n\r\nIs there an advantage to tieing the data skipping index to the primary key or would it make sense to make it its own stand-alone index which could have its own granularity defined by rows? If I had a data skipping index with GRANULARITY=4096rows then one could easily compute which primary key the current data skipping index batch belongs to since the number of rows is always fixed. That way one could have a finer grained data skipping index if filtering just by that column. It would also make for easier understanding of the index.\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-02-19T16:51:33Z",
        "body": "> As I now understand it, the data skipping index is tied to the primary key. E.g. If I have index_granularity=8192 and GRANULARITY=1, then each 8192 rows, the index contains say the minmax for the Nth primary key.\r\n\r\nCorrect.\r\n\r\n> Is there an advantage to tieing the data skipping index to the primary key or would it make sense to make it its own stand-alone index which could have its own granularity defined by rows?\r\n\r\nEvery column has the .mrk file along with .bin (data) file. These files store \"marks\" - offsets in data file, that allow to read or skip data for specific granules. These marks have primary key index granularity.\r\n\r\nIf you have different granularity for secondary keys, you either:\r\n- cannot skip data efficiently (you'll have to read and throw off data instead of seek);\r\n- have to store secondary .mrk files for every column."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T18:25:23Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4350,
    "title": "Table functions are forbidden in readonly mode...",
    "created_at": "2019-02-11T18:32:59Z",
    "closed_at": "2022-07-17T23:08:08Z",
    "labels": [
      "enhancement",
      "question",
      "not planned"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4350",
    "body": "i've configured user readonly to be used to only request data...\r\ni've created un view on clickhouse to external mysql database.\r\nwhen try to request (only read only request!) to clickhouse to this view with readonly user i receive this error:\r\n\"Table functions are forbidden in readonly mode\"\r\n\r\nthere a way to use view to external db with readonly user ?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4350/comments",
    "author": "mcarbonneaux",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-02-11T18:54:26Z",
        "body": "Yes. You can use `MySQL` table engine (`CREATE TABLE ... ENGINE = MySQL(...)`) instead of `mysql` table function.\r\n\r\nTable function is just a way to dynamically create a table for single query."
      },
      {
        "user": "mcarbonneaux",
        "created_at": "2019-02-11T20:12:28Z",
        "body": "ok fine ! \r\ni've created table with MySQL engine and created view on it...\r\n\r\nthere no way to do direct view on mysql function ?\r\n\r\nlike that:\r\n```\r\nCREATE VIEW clikhdb.clickhview\r\nAS\r\nSELECT\r\n   mysqlcolumn,\r\nFROM  mysql('<mysqlhost>:<mysqlport>','mymysqldbs', 'mymysqltable', 'mysqluser', 'mysqlpass')\r\nGROUP BY\r\n   mysqlcolumn\r\nORDER BY\r\n   mysqlcolumn\r\n```\r\n\r\nthey are readonly why not authorised to select from this view ?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-02-11T20:55:16Z",
        "body": "Ok, I understand - the user should be allowed to do a query if a table function is used indirectly via view."
      },
      {
        "user": "arctica",
        "created_at": "2019-02-15T20:12:53Z",
        "body": "I am also running into this problem because I am implementing replication and sharding outside of ClickHouse as we don't want to rely on Zookeeper. To read data, we need to use the remote() table function in order to aggregate data from several shards. For security purposes, it would be great if the user could run only read queries but currently usage of remote() is prohibited. Please consider allowing remote() for read only users."
      },
      {
        "user": "ztlpn",
        "created_at": "2019-02-21T13:09:51Z",
        "body": "BTW you don't need ZooKeeper to use builtin ClickHouse sharding (Distributed tables etc.)"
      },
      {
        "user": "arctica",
        "created_at": "2019-02-22T10:50:06Z",
        "body": "@ztlpn interesting, thanks for raising that point. But I can't change the sharding e.g. add new servers without adjusting the config file on each server and restart ClickHouse right? That might be not ideal in a quite dynamic setting. It would be cool if the cluster settings could be stored in a ClickHouse table which can be dynamically adjusted."
      },
      {
        "user": "arctica",
        "created_at": "2019-02-22T14:27:59Z",
        "body": "Actually I see there is already a system.clusters table but doesn't allow writes to it."
      },
      {
        "user": "ztlpn",
        "created_at": "2019-02-25T13:06:52Z",
        "body": "@arctica Yes, you need to update the config files, but you don't need to restart servers because cluster configuration is updated on the fly."
      },
      {
        "user": "arctica",
        "created_at": "2019-03-01T15:28:17Z",
        "body": "@ztlpn Thanks for that information. That's a situation that albeit sub-optimal, I can make it work in our use-case."
      },
      {
        "user": "filimonov",
        "created_at": "2020-11-09T09:22:16Z",
        "body": "BTW - it's quite silly that we can't also use `numbers(...), numbers_mt(...), zeros(...)` etc. in readonly mode... "
      }
    ]
  },
  {
    "number": 3978,
    "title": "Question:How to kill last executed select query which cost more time than expected through http interface?",
    "created_at": "2019-01-03T02:10:56Z",
    "closed_at": "2019-03-11T07:17:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3978",
    "body": "When querying clickhouse through http interface, if the query time beyond the expectation, we want to kill this query and save the server's resources. Is there any way for this purpose?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3978/comments",
    "author": "AlexanderJLiu",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-01-03T11:04:38Z",
        "body": "Use `max_execution_time` setting or increase timeouts for http connection on your client.\r\n( Related: #1403 )"
      },
      {
        "user": "AlexanderJLiu",
        "created_at": "2019-01-03T12:23:46Z",
        "body": "@filimonov Yes, it works, thanks. \ud83d\udc4d \r\nI use this setting via passing HTTP CGI parameters: `URL?max_execution_time=1`, not in config file. By this way, querying from the clickhouse console client with the same user can avoid timeout limit.\r\n\r\n---\r\n**Another question:** If querying a distributed table with `max_execution_time` setting, the query in remote server will stop as well according to my test.  But the query error says `Code: 159, Message: Timeout exceeded: elapsed 4.386062641 seconds, maximum: 1`, elapsed not 1 but 4, how to explain?"
      }
    ]
  },
  {
    "number": 3971,
    "title": "ALL JOIN inflating numbers",
    "created_at": "2018-12-30T13:55:37Z",
    "closed_at": "2018-12-31T17:13:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3971",
    "body": "Hi there,\r\nimagine a simple query with a join like the following. When using an ANY join I get the same result as without a join but as soon as I use ALL, the result from the local fields (impressions, value..) is inflated. Usually this inflation is just 1-4% but in some cases it is 10-100 times. What would be the best course of action here?\r\n\r\n```\r\nSELECT\r\n    category,\r\n    count() AS impressions,\r\n    uniq(sessionId) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM impressions ALL LEFT JOIN\r\n(\r\n    SELECT\r\n        CounterID,\r\n        engaged,\r\n        visible\r\n    FROM visits\r\n    GROUP BY CounterID\r\n) USING CounterID\r\nGROUP BY category\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3971/comments",
    "author": "Slind14",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-12-30T19:58:38Z",
        "body": "If I understand your question correctly, this is how ALL JOIN is expected to behave.\r\n(And ALL JOIN is the default behaviour of JOIN in other relational DBMSs)\r\nIf there are multiple rows in the right table with corresponding CounterID, it will create multiple rows in the result."
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-30T20:52:52Z",
        "body": ">Usually this inflation is just 1-4% but in some cases it is 10-100 times. \r\n\r\nCH always gives exact numbers. If you experience some inflation, even 0.00001% it means something wrong with design or it's some unknown CH's bug.\r\n\r\n`GROUP BY CounterID ) USING CounterID`\r\nmeans that the right table has only one row with each CounterID, so it should not be any difference with ALL vs ANY.\r\n\r\nCan you show exact table DDLs and exact SQL, because your example is too vague and wrong for the right table _engaged_ and _visible_ are not a SUMS and they are not in GROUP BY section."
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T05:08:58Z",
        "body": "Here is a real query and the result:\r\n\r\nBoth tables are a simple MergeTree.\r\n\r\n```\r\nSELECT\r\n    date,\r\n    count() AS impressions,\r\n    uniq(sessionUUID) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM `impressions` ANY LEFT JOIN\r\n(\r\n    SELECT\r\n        impressionUUID,\r\n        visible,\r\n        engaged\r\n    FROM `meta`\r\n    WHERE date = yesterday()\r\n) USING impressionUUID\r\nWHERE date = yesterday()\r\nGROUP BY date;\r\n```\r\n\r\n## NO JOIN\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500impressions\u2500\u252c\u2500sessions\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500visible\u2500\u252c\u2500engaged\u2500\u2510\r\n\u2502 2018-12-30 \u2502     4353169 \u2502   123935 \u2502 5636888.545389 \u2502         \u2502         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n## ANY LEFT JOIN\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500impressions\u2500\u252c\u2500sessions\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500visible\u2500\u252c\u2500engaged\u2500\u2510\r\n\u2502 2018-12-30 \u2502     4353169 \u2502   123935 \u2502 5636888.545389 \u2502 2662372 \u2502    2274 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n## ALL LEFT JOIN\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500impressions\u2500\u252c\u2500sessions\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500\u2500visible\u2500\u252c\u2500engaged\u2500\u2510\r\n\u2502 2018-12-30 \u2502    41745815 \u2502   123935 \u2502 17641794.240708 \u2502 39908334 \u2502  148958 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThe Meta table can contain an impressionUUID twice because visible and engaged both have their own entry/row). Here is the total count vs uniq impressions and the amount of \"duplicated\" impressionUUIDs:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500\u2500\u2500count\u2500\u252c\u2500impressions\u2500\u252c\u2500duplicates\u2500\u2510\r\n\u2502 2018-12-30 \u2502 2772833 \u2502     2646950 \u2502     125883 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T13:55:53Z",
        "body": "So probably meta has duplicates for some reason\r\n\r\nSELECT      impressionUUID, count() cnt\r\n    FROM `meta` WHERE date = yesterday() group by impressionUUID having cnt > 1\r\n\r\nWhat table engine is used by meta ? Replacing ? "
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T13:59:42Z",
        "body": "> Both tables are a simple MergeTree.\r\n\r\nYour query returns: `13 rows in set. Elapsed: 0.485 sec.` all with a cnt of 2\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T14:15:15Z",
        "body": "OK. Is it valid to have duplicates?\r\nThe observed difference is because these duplicates multiply select's result rows with ALL and does not with ANY."
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T14:21:57Z",
        "body": "Yes, these are not really duplicates, visible and engaged are treated separately. So there is one row for engaged and one for visible. \r\nWhy would this inflate them so much. I'm don't think this ever happened with mysql."
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T15:03:51Z",
        "body": ">Yes, these are not really duplicates, visible and engaged are treated separately. \r\n\r\nso are they flags?\r\n\r\nvisible 0  engaged  1\r\nvisible 1  engaged  1\r\n\r\nor numbers?\r\n\r\nWhat CH version do you use?\r\n"
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T15:06:24Z",
        "body": "They are boolean (numbers which can only be 0 and 1).\r\nIt is not possible that one row has visible and engaged set. It is always\r\nimpressionA visible 1 engaged 0\r\nimpressionA visible 0 engaged 1\r\n\r\n`18.16.1 revision 54412`"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T15:18:32Z",
        "body": "OK. So is any difference in result \r\n```\r\n\r\nSELECT\r\n    date,\r\n    count() AS impressions,\r\n    uniq(sessionUUID) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM `impressions` ANY LEFT JOIN\r\n(\r\n    SELECT\r\n        impressionUUID,\r\n        sum(visible) visible,\r\n        sum(engaged) engaged\r\n    FROM `meta`\r\n    WHERE date = yesterday()\r\n    GROUP BY impressionUUID\r\n) USING impressionUUID\r\nWHERE date = yesterday()\r\nGROUP BY date;\r\n\r\n```\r\n\r\nthe same with ALL LEFT JOIN"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T15:22:41Z",
        "body": ">18.16.1 revision 54412\r\n\r\nand check any difference if you execute \r\nset compile_expressions = 0;\r\nbefore query"
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T15:56:17Z",
        "body": "You are right, it works with the group by. My mistake, sorry. \r\n\r\nWhat I still don't get is why this usually results in an inflation of 1-4% and only on this one database of 10 times. The type of data is the same and the duplicates are not really different in those other databases.\r\n\r\nBtw. is there any way to tell `ANY` to pick the latest record? (go through it staring with the latest)"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T17:10:54Z",
        "body": "It's in your data. Just check what is going on for one of these (of 13) impressionUUID.\r\nThere is no mystery or magic or inflation.\r\n\r\n>Btw. is there any way to tell ANY to pick the latest record? (go through it staring with the latest)\r\n\r\nNo.\r\nIt will be something like that\r\n```\r\n\r\nselect mx.1 impressionUUID, sum(mx.2) visible, sum(mx.3) engaged from\r\n( select argMax((impressionUUID,visible,engaged),date) mx \r\n  from `meta`\r\n  group by impressionUUID, visible, engaged)\r\ngroup by impressionUUID\r\n```"
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T17:13:58Z",
        "body": "I see. Thank you."
      }
    ]
  },
  {
    "number": 3784,
    "title": "Question: How to load fast big flat files ?",
    "created_at": "2018-12-07T13:47:32Z",
    "closed_at": "2018-12-08T06:25:40Z",
    "labels": [
      "question",
      "performance"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3784",
    "body": "The best method I found is to use the table function `file(path, format, structure)`. It takes as an input parameter the relative path to the file from the setting `user_files_path`. One can change this setting in `/etc/clickhouse-server/config.xml` \r\n\r\n**Question:** Is it possible to change `user_files_path` in a clickhouse-client session with an `sql` command ?\r\n\r\nI suppose an alternative method instead of copying/placing the flat-file under `user_files_path` is to pipe the flat-file to command line client (`clickhouse-client`) but that requires access to the file system and the command has to be invoked from my python application.\r\n\r\nIs there another method to load fast big flat-files (millions of rows) ?\r\n\r\n**Clarification:** I want to load the data from flat-files to a temporary clickhouse table engine e.g. merge tree, log, memory, so that I can read and process column data fast and use these as an input to my TRIADB clickhouse table engines.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3784/comments",
    "author": "healiseu",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-12-07T19:57:02Z",
        "body": "> Question: Is it possible to change user_files_path in a clickhouse-client session with an sql command ?\r\n\r\nNo, because the setting is security limit: to allow to read only restricted subset of files from server's filesystem. But you can specify `user_files_path` as `/` (and use path relative to filesystem root) if you really don't care.\r\n\r\n> I suppose an alternative method instead of copying/placing the flat-file under user_files_path is to pipe the flat-file to command line client (clickhouse-client) but that requires access to the file system and the command has to be invoked from my python application.\r\n\r\nIf your files are on client side and you need to transfer it over the network, better to use `clickhouse-client` (than to transfer by other tools). If your files are already on server's filesystem, better to use `file` table function.\r\n\r\nNote: sometimes using clickhouse-client may be faster, because it use \"double buffering\". It will parse next chunk of data while waiting for server to insert previous chunk of data. But this should not be very significant.\r\n\r\nIf you have multiple files, you can load them in parallel.\r\n\r\n> Is there another method to load fast big flat-files (millions of rows) ?\r\n\r\nThere are also some advanced solutions, like to prepare a partition for MergeTree table, then move it to the server and attach. It should not be faster unless you can do data preparation on separate cluster in parallel.\r\n\r\n"
      },
      {
        "user": "healiseu",
        "created_at": "2018-12-08T06:25:32Z",
        "body": "Hi @alexey-milovidov, thank you for your answers, you covered me sufficiently for the moment."
      }
    ]
  },
  {
    "number": 3659,
    "title": "Pushing WHERE conditions from the view to underlying table ",
    "created_at": "2018-11-25T01:00:09Z",
    "closed_at": "2020-01-14T20:27:10Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3659",
    "body": "Can you add parameters to the view\uff0c If there are no parameters, then every request must be queried before filtering, resulting in unnecessary waste of computing resources. In addition, JDBC query avoids transferring a large amount of SQL code.\r\nLook forward to your reply\r\nthanks ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3659/comments",
    "author": "754154377",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2018-11-26T09:58:41Z",
        "body": "Do you mean that you what to push down WHERE predicate from main query to VIEW? There is a setting `enable_optimize_predicate_expression`. Try to enable it and check if it works for your case."
      },
      {
        "user": "754154377",
        "created_at": "2018-11-26T13:51:54Z",
        "body": "> Do you mean that you what to push down WHERE predicate from main query to VIEW? There is a setting `enable_optimize_predicate_expression`. Try to enable it and check if it works for your case.\r\n\r\nFor Example: To express my thoughts, I have fabricated the following functions, which do not actually exist.\r\ncreate table shop_sale (event_date Date, shop_id String, goods_id String, sale_amt Float32) ENGINE = MergeTree(event_date , (shop_id), 8192);  \r\n\r\ninsert into shop_sale VALUES('2000-01-01', 'AB01', 'A', 11201), ('2000-01-01', 'AB02', 'B', 11301), ('2000-01-01', 'AB02', 'C'. 12301);\r\n\r\ncreate view view_shop_sale (event_date Date, shop_id String, sale_amt Float32)  as select event_date, shop_id, sale_amt from shop_sale where event_date = ::eventDate:: and shop_id = ::shopId::\r\n;\r\n\r\nselect *\r\nfrom view_shop_sale \r\nwhere eventDate= '2000-01-01' and shopId= 'AB01'"
      },
      {
        "user": "den-crane",
        "created_at": "2018-11-26T20:12:30Z",
        "body": "You don't need parameters, it works out the box\r\n\r\nIf you create view like \r\ncreate view view_shop_sale  as **select event_date, shop_id, sale_amt from shop_sale**\r\n\r\n**enable_optimize_predicate_expression = 0**\r\nselect * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\nwill be executed as \r\nselect * from (**select event_date, shop_id, sale_amt from shop_sale**) where eventDate= '2000-01-01' and shopId= 'AB01'\r\n\r\nenable_optimize_predicate_expression = 1\r\nselect * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\nwill be re-written and executed as \r\nselect * from **shop_sale** where eventDate= '2000-01-01' and shopId= 'AB01'\r\n"
      },
      {
        "user": "754154377",
        "created_at": "2018-11-27T01:36:30Z",
        "body": "> You don't need parameters, it works out the box\r\n> \r\n> If you create view like\r\n> create view view_shop_sale as **select event_date, shop_id, sale_amt from shop_sale**\r\n> \r\n> **enable_optimize_predicate_expression = 0**\r\n> select * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\n> will be executed as\r\n> select * from (**select event_date, shop_id, sale_amt from shop_sale**) where eventDate= '2000-01-01' and shopId= 'AB01'\r\n> \r\n> enable_optimize_predicate_expression = 1\r\n> select * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\n> will be re-written and executed as\r\n> select * from **shop_sale** where eventDate= '2000-01-01' and shopId= 'AB01'\r\n\r\nthanks \uff01 \r\nIn addition, can replicated tables support views?\r\nFor example:\r\ncreate view view_shop_sale ON CLUSTER xxx_3replicas as select event_date, shop_id, sale_amt from shop_sale ?"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2018-11-27T10:04:56Z",
        "body": "Yes, you can create view on any replica (or on cluster itself).\r\nIf you need several shards you can also create distributed table over views."
      },
      {
        "user": "blinkov",
        "created_at": "2019-03-26T11:10:02Z",
        "body": "@754154377 do you have any further questions?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T19:25:26Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3434,
    "title": "Kafka EOF reached for partition ...",
    "created_at": "2018-10-21T09:09:39Z",
    "closed_at": "2018-10-22T20:14:22Z",
    "labels": [
      "question",
      "comp-kafka"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3434",
    "body": "```\r\nClickHouse client version 18.14.9.\r\nConnecting to database monitoring at localhost:9000 as user broker.\r\nConnected to ClickHouse server version 18.14.9 revision 54409.\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u252c\u2500changed\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_insert_block_size    \u2502 1048576 \u2502       0 \u2502 The maximum block size for insertion, if we control the creation of blocks for insertion. \u2502\r\n\u2502 stream_flush_interval_ms \u2502 7500    \u2502       0 \u2502 Timeout for flushing data from streaming storages.                                        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\nI just upgrade from 1.1.54385 to 18.14.9 and I'm receiving \"EOF reached for partition...\" when any of my materialized views are running. Queues with little traffic don't show this message so much but a queue that gets 100+ messages per second is constantly getting this message and some queue messages aren't written to the MergeTree table. Any advice would be greatly appreciated.\r\n\r\n```\r\n2018.10.21 08:57:38.093341 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146709\r\n2018.10.21 08:57:38.620841 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146723\r\n2018.10.21 08:57:38.767349 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146724\r\n2018.10.21 08:57:38.973400 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146727\r\n2018.10.21 08:57:39.254782 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146730\r\n2018.10.21 08:57:39.706890 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146739\r\n2018.10.21 08:57:40.088991 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146745\r\n2018.10.21 08:57:40.242327 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146746\r\n2018.10.21 08:57:40.766095 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146749\r\n2018.10.21 08:57:41.090802 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146751\r\n2018.10.21 08:57:41.216236 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146752\r\n2018.10.21 08:57:41.587863 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146756\r\n2018.10.21 08:57:41.705616 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146759\r\n2018.10.21 08:57:41.933883 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146763\r\n2018.10.21 08:57:42.111605 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146765\r\n2018.10.21 08:57:42.585086 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146773\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3434/comments",
    "author": "daledude",
    "comments": [
      {
        "user": "vavrusa",
        "created_at": "2018-10-22T19:26:39Z",
        "body": "This just means that Kafka consumer reached EOF for given partition. Background consumer uses this as a signal for closing current batch, instead of waiting for timeout. It's useful for tracing/debugging (which is why it's emitted at trace level)."
      },
      {
        "user": "daledude",
        "created_at": "2018-10-22T20:14:22Z",
        "body": "Thank you @vavrusa! My msg loss is unrelated then."
      }
    ]
  },
  {
    "number": 2897,
    "title": "[Question] ALTER DELETE in Materialized Views",
    "created_at": "2018-08-20T15:28:41Z",
    "closed_at": "2018-08-20T16:32:06Z",
    "labels": [
      "question",
      "comp-matview",
      "comp-mutations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2897",
    "body": "Hello, will alter delete work for materialized views as well? Thanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2897/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T16:26:44Z",
        "body": "You have to apply it to `.inner.` table (that you can find in `system.tables`) and mutations will work if the underlying storage is of MergeTree family."
      },
      {
        "user": "simPod",
        "created_at": "2018-08-20T16:32:06Z",
        "body": "Thank's for the reply! I also see you implememented query forwarding to the underlying table \ud83d\udc4d "
      }
    ]
  },
  {
    "number": 2895,
    "title": "Is there a way to check mutation is in progress?",
    "created_at": "2018-08-20T12:57:39Z",
    "closed_at": "2018-08-20T19:35:14Z",
    "labels": [
      "question",
      "comp-documentation",
      "usability",
      "comp-mutations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2895",
    "body": "I recently triggered a DELETE mutation on relatively small table. Is there a way I can see it is in progress or approximatelly determine when it will finish? \r\nIn `mutations` table I can only see `parts_to_do=1` but nothing else.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2895/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T19:27:38Z",
        "body": "> In mutations table I can only see parts_to_do=1 but nothing else.\r\n\r\nThere is nothing else to track mutations: only system.mutations table."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T19:28:05Z",
        "body": "For Replicated tables, you can also look at `system.replication_queue`."
      },
      {
        "user": "simPod",
        "created_at": "2018-08-20T19:35:14Z",
        "body": "Thank you! \r\n\r\nFor those wondering, also check server error log."
      }
    ]
  },
  {
    "number": 2892,
    "title": "Ways to include per user configuration from external files",
    "created_at": "2018-08-19T11:34:32Z",
    "closed_at": "2018-08-20T09:12:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2892",
    "body": "Hello!\r\n\r\nI'm working on some ways to simplify user management for Clickhouse.\r\n\r\nGlobal user configuration file \"/etc/clickhouse-server/users.xml\" does not work for me very well. Because it requires complicated logic when we add/remove users.\r\n\r\nI have two options to maintain it properly:\r\n- Regenerate this user.xml file each time when we add new user external source (JSON/YAML). But it requires external code to generate it and increases complexity.\r\n- Read content of existing file, add new section, write changes. But it also involves pretty tricky XML processing and can break something.\r\n\r\nI'm interested in extracting this information to separate files:\r\n```\r\n<admin>\r\n  <password>new_password</password>\r\n  <networks incl=\"networks\" replace=\"replace\">\r\n    <ip>::/0</ip>\r\n  </networks>\r\n  <profile>default</profile>\r\n  <quota>default</quota>\r\n</admin>\r\n```\r\n\r\nIs there is any way to extract configuration for each use in separate file? \r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2892/comments",
    "author": "pavel-odintsov",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T03:36:55Z",
        "body": "Sure. You can add files for each user inside `/etc/clickhouse-server/users.d` directory.\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T03:38:58Z",
        "body": "```\r\n$ cat /etc/clickhouse-server/users.d/alice.xml\r\n<yandex>\r\n    <users>\r\n      <alice>\r\n          <profile>analytics</profile>\r\n            <networks>\r\n                  <ip>::/0</ip>\r\n            </networks>\r\n          <password_sha256_hex>...</password_sha256_hex>\r\n          <quota>analytics</quota>\r\n      </alice>\r\n    </users>\r\n</yandex>\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T03:39:59Z",
        "body": "The list of external files is tracked and updated on the fly."
      },
      {
        "user": "pavel-odintsov",
        "created_at": "2018-08-20T09:12:01Z",
        "body": "Hello!\r\n\r\nWow! That's awesome! It works for me! Thank you!"
      }
    ]
  },
  {
    "number": 2746,
    "title": "What is the difference between version v18.x.x and v1.1.xxxx?",
    "created_at": "2018-07-28T03:41:02Z",
    "closed_at": "2018-07-28T04:03:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2746",
    "body": "I notice there is a naming pattern change on ClickHouse version code.\r\nI wonder whether both are compatible, and I am safe to upgrade from v1.1.xxx to v18.xx? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2746/comments",
    "author": "vnnw",
    "comments": [
      {
        "user": "vnnw",
        "created_at": "2018-07-28T04:03:10Z",
        "body": "Sorry. The CHANGELOG.md already explains my question."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2018-07-28T15:09:49Z",
        "body": "TLDR: \r\nVersions 1 and 18 are totally compatible."
      }
    ]
  },
  {
    "number": 2203,
    "title": "[Question] Parallel execution of subqueries",
    "created_at": "2018-04-10T15:15:14Z",
    "closed_at": "2020-10-29T19:58:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2203",
    "body": "If we have a query Q like:\r\n\r\n```sql\r\n\r\nselect \r\n  c1, \r\n  c2,\r\n  c3 / c4 \r\nfrom \r\n  ( select  c1,\r\n                c2,\r\n                count() as c3\r\n    from T1\r\n    where ...\r\n    group by c1, c2\r\n  )\r\n  ALL LEFT JOIN\r\n  ( select c1, \r\n               c2,\r\n               count() as c4\r\n    from T2\r\n    where ...\r\n    group by c1, c2\r\n  ) \r\n  USING (c1, c2)\r\n```\r\nT1 and T2 are distributed tables with merge tree engine, will Clickhouse execute those two sub queries   in parallel and then join the  subquery results?  In our test settings, they seems execute sequentially. just want to check if there are any settings to control this behavior.  \r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2203/comments",
    "author": "zhoudehui",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-10-29T19:58:10Z",
        "body": "No.\r\n\r\nThe reason is that the \"left\" subquery is not buffered in memory. It is processed in a streaming fashion with lookups in the right table. That's why we have to first make a lookup-optimized data structure for the \"right\" subquery and then proceed with streaming reading and joining of the \"left\" subquery. It allows to save memory when the \"left\" subquery is going to produce huge resultset."
      },
      {
        "user": "zhoudehui",
        "created_at": "2020-12-01T05:58:16Z",
        "body": "> No.\r\n> \r\n> The reason is that the \"left\" subquery is not buffered in memory. It is processed in a streaming fashion with lookups in the right table. That's why we have to first make a lookup-optimized data structure for the \"right\" subquery and then proceed with streaming reading and joining of the \"left\" subquery. It allows to save memory when the \"left\" subquery is going to produce huge resultset.\r\n\r\nThanks Alex."
      }
    ]
  },
  {
    "number": 972,
    "title": "Data duplication",
    "created_at": "2017-07-12T08:43:22Z",
    "closed_at": "2019-10-28T21:46:28Z",
    "labels": [
      "question",
      "comp-distributed"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/972",
    "body": "We have a Distributed table with two-node cluster with ReplicatedMergeTree tables. Once in 3 secs we make an insert to the Distributed table and see that some of the data are duplicated. Why and how we can avoid this?\r\n\r\nClickHouse server version 1.1.54236",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/972/comments",
    "author": "alniks",
    "comments": [
      {
        "user": "ipolevoy",
        "created_at": "2017-07-24T04:10:24Z",
        "body": "bump, hey good people from Clickhouse, any advice? "
      },
      {
        "user": "ipolevoy",
        "created_at": "2017-08-07T03:27:14Z",
        "body": "hello?"
      },
      {
        "user": "alex-zaitsev",
        "created_at": "2017-08-07T08:59:44Z",
        "body": "Please share your cluster configuration. You seem to have one shard and two replicas. The data has to be replicated, so it looks like your Distributed table is not properly configured and thinks there are two shards."
      },
      {
        "user": "SlyderBY",
        "created_at": "2017-08-07T14:25:03Z",
        "body": "This is how our cluster configuration looks:\r\n```\r\n    <remote_servers>\r\n        <test>\r\n            <shard>\r\n                <replica>\r\n                    <host>10.1.1.10</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n                <replica>\r\n                    <host>10.1.1.11</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n        </test>\r\n    </remote_servers>\r\n\r\n```"
      },
      {
        "user": "alex-zaitsev",
        "created_at": "2017-08-07T14:57:02Z",
        "body": "Please also provide an output of 'show create table' for shard tables and distributed table.\r\n\r\nBut I guess the problem is that you do not have internal_replication for your shard tables:\r\n\r\n   <internal_replication>true</internal_replication>"
      },
      {
        "user": "SlyderBY",
        "created_at": "2017-08-07T15:15:58Z",
        "body": " 'show create table' output for one of the tables:\r\n`CREATE TABLE test.campaign_events ( campaign_id UInt64,  contact_id UInt64,  type UInt8,  email String,  user_id UInt64,  user_agent String,  url String,  latitude Float32,  longitude Float32,  ip String,  created_at DateTime,  event_date Date,  merged_url String) ENGINE = Distributed(test, \\'test\\', \\'campaign_events_local\\')`\r\n\r\nAdded '<internal_replication>true</internal_replication>' to the server configuration, will see if it helps."
      },
      {
        "user": "vas-and-tor",
        "created_at": "2017-08-08T16:28:26Z",
        "body": "We have similar problem.\r\n\r\nHere is our cluster:\r\n\r\n```\r\nSELECT *\r\nFROM system.clusters\r\nWHERE cluster = 'logs'\r\n\r\n\u250c\u2500cluster\u2500\u252c\u2500shard_num\u2500\u252c\u2500shard_weight\u2500\u252c\u2500replica_num\u2500\u252c\u2500host_name\u2500\u2500\u2500\u252c\u2500host_address\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500is_local\u2500\u252c\u2500user\u2500\u2500\u2500\u2500\u252c\u2500default_database\u2500\u2510\r\n\u2502 logs    \u2502         1 \u2502            1 \u2502           1 \u2502 clickhouse2 \u2502 127.0.1.1       \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         1 \u2502            1 \u2502           2 \u2502 clickhouse9 \u2502 192.168.231.101 \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         2 \u2502            1 \u2502           1 \u2502 clickhouse3 \u2502 192.168.231.107 \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         2 \u2502            1 \u2502           2 \u2502 clickhouse8 \u2502 192.168.231.102 \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         3 \u2502            1 \u2502           1 \u2502 clickhouse4 \u2502 192.168.231.105 \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         3 \u2502            1 \u2502           2 \u2502 clickhouse7 \u2502 192.168.231.106 \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         4 \u2502            1 \u2502           1 \u2502 clickhouse5 \u2502 192.168.231.6   \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 logs    \u2502         4 \u2502            1 \u2502           2 \u2502 clickhouse6 \u2502 192.168.231.104 \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nOur tables:\r\n\r\n```\r\nSHOW CREATE TABLE metrika.tmp_api_logs\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE metrika.tmp_api_logs ( Timestamp DateTime,  HttpMethod String,  HostName String,  UriPath String,  UriParams String,  HttpStatus Int32,  RequestTime Float64,  RequestLength Int64,  BytesSent Int64,  UpstreamResponseTime Float64,  PortalUserId String,  TraceId String,  EventDate Date MATERIALIZED toDate(Timestamp)) ENGINE = MergeTree(EventDate, intHash32(Timestamp), (Timestamp, intHash32(Timestamp)), 8192) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```\r\nSHOW CREATE TABLE metrika_new.api_logs_local\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE metrika_new.api_logs_local ( Timestamp DateTime,  HttpMethod String,  HostName String,  UriPath String,  UriParams String,  HttpStatus Int32,  RequestTime Float64,  RequestLength Int64,  BytesSent Int64,  UpstreamResponseTime Float64,  PortalUserId String,  TraceId String,  EventDate Date MATERIALIZED toDate(Timestamp)) ENGINE = ReplicatedMergeTree(\\'/clickhouse/tables/{logsshard}/metrika/api_logs\\', \\'{replica}\\', EventDate, intHash32(Timestamp), (Timestamp, intHash32(Timestamp)), 8192) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```\r\nSHOW CREATE TABLE metrika_new.api_logs_insert\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE metrika_new.api_logs_insert ( Timestamp DateTime,  HttpMethod String,  HostName String,  UriPath String,  UriParams String,  HttpStatus Int32,  RequestTime Float64,  RequestLength Int64,  BytesSent Int64,  UpstreamResponseTime Float64,  PortalUserId String,  TraceId String) ENGINE = Distributed(logs, \\'metrika_new\\', \\'api_logs_local\\', rand()) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```\r\nSELECT count()\r\nFROM metrika.tmp_api_logs\r\nWHERE (EventDate > '2017-01-10') AND (EventDate <= '2017-02-10')\r\n\r\n\u250c\u2500\u2500\u2500\u2500count()\u2500\u2510\r\n\u2502 1327505338 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAfter I did\r\n\r\n```\r\nINSERT INTO metrika_new.api_logs_insert SELECT\r\n    Timestamp,\r\n    HttpMethod,\r\n    HostName,\r\n    UriPath,\r\n    UriParams,\r\n    HttpStatus,\r\n    RequestTime,\r\n    RequestLength,\r\n    BytesSent,\r\n    UpstreamResponseTime,\r\n    PortalUserId,\r\n    TraceId\r\nFROM metrika.tmp_api_logs\r\nWHERE (EventDate > '2017-01-10') AND (EventDate <= '2017-02-10')\r\n```\r\n\r\nI expect count() from `metrika_new.api_logs_insert` to be `1327505338`, but I have\r\n\r\n```\r\nSELECT count()\r\nFROM metrika_new.api_logs_insert\r\n\r\n\u250c\u2500\u2500\u2500\u2500count()\u2500\u2510\r\n\u2502 1709437703 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```\r\n$ for i in `seq 2 9`; do echo -n \"clickhouse$i: \" && curl clickhouse$i:8123 -d \"select count() from metrika_new.api_logs_local\" ; done\r\nclickhouse2: 431643023\r\nclickhouse3: 426589622\r\nclickhouse4: 427823619\r\nclickhouse5: 423381439\r\nclickhouse6: 423381439\r\nclickhouse7: 427823619\r\nclickhouse8: 426589622\r\nclickhouse9: 431643023\r\n```\r\n\r\nWe have version 1.1.54245"
      },
      {
        "user": "vas-and-tor",
        "created_at": "2017-08-10T06:47:57Z",
        "body": "I reproduced this problem even with direct inserts to a shard, like this:\r\n\r\n```\r\nINSERT INTO metrika_new.api_logs_local SELECT\r\n    Timestamp,\r\n    HttpMethod,\r\n    HostName,\r\n    UriPath,\r\n    UriParams,\r\n    HttpStatus,\r\n    RequestTime,\r\n    RequestLength,\r\n    BytesSent,\r\n    UpstreamResponseTime,\r\n    PortalUserId,\r\n    TraceId\r\nFROM remote('clickhouse2', metrika.tmp_api_logs)\r\nWHERE (cityHash64(*) % 4) = <index of a shard> -- 0, 1, 2 or 3\r\n```\r\n\r\nBut after I changed `MATERIALIZED` expression for `EventDate` to `DEFAULT` everything seems to be fine. I have same `count()` after insert."
      },
      {
        "user": "alniks",
        "created_at": "2018-08-01T05:09:13Z",
        "body": "<internal_replication>true</internal_replication> helped us"
      },
      {
        "user": "imvs",
        "created_at": "2019-01-25T08:32:36Z",
        "body": "I have duplication to.\r\n\r\nHere is configuration:\r\n<pre>\r\n<code>\r\nSELECT *\r\nFROM system.clusters\r\n\r\n\u250c\u2500cluster\u2500\u252c\u2500shard_num\u2500\u252c\u2500shard_weight\u2500\u252c\u2500replica_num\u2500\u252c\u2500host_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500host_address\u2500\u252c\u2500port\u2500\u252c\u2500is_local\u2500\u252c\u2500user\u2500\u2500\u2500\u2500\u252c\u2500default_database\u2500\u2510\r\n\u2502 test    \u2502         1 \u2502            1 \u2502           1 \u2502 ch-test02.unix.eklmn.ru \u2502 10.13.52.134 \u2502 9001 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 test    \u2502         1 \u2502            1 \u2502           2 \u2502 ch-test04.unix.eklmn.ru \u2502 10.13.52.136 \u2502 9001 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 test    \u2502         2 \u2502            1 \u2502           1 \u2502 ch-test03.unix.eklmn.ru \u2502 10.13.52.135 \u2502 9001 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 test    \u2502         2 \u2502            1 \u2502           2 \u2502 ch-test05.unix.eklmn.ru \u2502 10.13.52.137 \u2502 9001 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n</code>\r\n</pre>\r\n\r\n   `<remote_servers>`\r\n    `<test>`\r\n        `<shard>`\r\n            `<weight>1</weight>`\r\n            `<internal_replication>true</internal_replication>`\r\n            `<replica>`\r\n                `<host>ch-test02.unix.eklmn.ru</host>`\r\n                `<port>9001</port>`\r\n            `</replica>`\r\n            `<replica>`\r\n                `<host>ch-test04.unix.eklmn.ru</host>`\r\n                `<port>9001</port>`\r\n            `</replica>`\r\n        `</shard>`\r\n        `<shard>`\r\n            `<weight>1</weight>`\r\n            `<internal_replication>true</internal_replication>`\r\n            `<replica>`\r\n                `<host>ch-test03.unix.eklmn.ru</host>`\r\n                `<port>9001</port>`\r\n            `</replica>`\r\n            `<replica>`\r\n                `<host>ch-test05.unix.eklmn.ru</host>`\r\n                `<port>9001</port>`\r\n            `</replica>`\r\n        `</shard>`\r\n    `</test>`\r\n    `</remote_servers>`\r\n\r\n\r\n<pre>\r\n<code>\r\nCREATE TABLE hits ON CLUSTER test\r\n(\r\n    EventDate DateTime,\r\n    CounterID UInt32,\r\n    UserID UInt32\r\n) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/hits', '{replica}')\r\nPARTITION BY toYYYYMM(EventDate)\r\nORDER BY (CounterID, EventDate)\r\n</code>\r\n</pre>\r\n<pre>\r\n<code>\r\nCREATE TABLE hits_all AS hits\r\n    ENGINE = Distributed(test, default, hits, rand());\r\n</code>\r\n</pre>\r\nReplacements on each server is:\r\n`    <macros>`\r\n`        <shard>01</shard>`\r\n`        <replica>ch-test02.unix.eklmn.ru</replica>`\r\n`    </macros>`\r\n`        <macros>`\r\n`        <shard>02</shard>`\r\n`        <replica>ch-test03.unix.eklmn.ru</replica>`\r\n`    </macros>`\r\n`        <macros>`\r\n`        <shard>01</shard>`\r\n`        <replica>ch-test04.unix.eklmn.ru</replica>`\r\n`    </macros>`\r\n`        <macros>`\r\n`        <shard>02</shard>`\r\n`        <replica>ch-test05.unix.eklmn.ru</replica>`\r\n`    </macros>`\r\n\r\nNow i execute repeatedly insertion, for example:\r\n\r\n<pre>\r\n<code>\r\nINSERT INTO hits_all SELECT toDateTime('2019-01-25 23:00:00'),4,1\r\n</code>\r\n</pre>\r\n\r\nand got duplications:\r\n<pre>\r\n<code>\r\nSELECT *\r\nFROM hits_all\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500EventDate\u2500\u252c\u2500CounterID\u2500\u252c\u2500UserID\u2500\u2510\r\n\u2502 2019-01-25 23:00:00 \u2502         1 \u2502      1 \u2502\r\n\u2502 2019-01-25 23:00:00 \u2502         2 \u2502      1 \u2502\r\n\u2502 2019-01-25 23:00:00 \u2502         3 \u2502      1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500EventDate\u2500\u252c\u2500CounterID\u2500\u252c\u2500UserID\u2500\u2510\r\n\u2502 2019-01-25 23:00:00 \u2502         4 \u2502      1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500EventDate\u2500\u252c\u2500CounterID\u2500\u252c\u2500UserID\u2500\u2510\r\n\u2502 2019-01-25 23:00:00 \u2502         1 \u2502      1 \u2502\r\n\u2502 2019-01-25 23:00:00 \u2502         3 \u2502      1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500EventDate\u2500\u252c\u2500CounterID\u2500\u252c\u2500UserID\u2500\u2510\r\n\u2502 2019-01-25 23:00:00 \u2502         4 \u2502      1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n</code>\r\n</pre>"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T19:25:23Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "filimonov",
        "created_at": "2019-10-28T21:46:28Z",
        "body": "@vas-and-tor your issue sounds quite different from original one. Please open new issue if it is still actual.\r\n\r\n@imvs please open new issue if you still need some help with that.\r\n\r\nClosing"
      },
      {
        "user": "lyfzwrthlvng",
        "created_at": "2020-06-22T10:35:38Z",
        "body": "@imvs did you open a separate issue? Did you figure out why you were seeing duplicate data? Some days baack we also saw similar issue, unfortunately we restarted and issue was gone, so can't reproduct it since then. Wanted to get better understanding of the issue."
      },
      {
        "user": "PalaceK999",
        "created_at": "2021-09-07T07:32:06Z",
        "body": "use  FINAL"
      }
    ]
  },
  {
    "number": 467,
    "title": "How to use `Distributed` with `MaterializedView`",
    "created_at": "2017-02-08T10:12:51Z",
    "closed_at": "2017-02-28T12:35:26Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/467",
    "body": "```\r\nCREATE TABLE IF NOT EXISTS insert_view(...) ENGINE = Null;\r\n\r\nCREATE MATERIALIZED VIEW data_local ENGINE = AggregatingMergeTree(..., sumState(num1) as num1,sumState(num2) as num2,sumState(num3) as num3,minState(num4) as num4,maxState(num5) as num5,sumState(num6) as num6 FROM insert_view GROUP BY xxxx;\r\n\r\nCREATE TABLE data as data_local ENGINE = Distributed(perftest_2shards_1replicas, default, data_local, rand());\r\n```\r\n\r\nBut all record insert in a shard?\r\nso, how to use `Distributed` with `MaterializedView`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/467/comments",
    "author": "VictoryWangCN",
    "comments": [
      {
        "user": "ludv1x",
        "created_at": "2017-02-13T13:50:47Z",
        "body": "1) Suppose that you want to store pairs of (`user`, `user_visit_time`) in your distributed db\r\n2) You have many `local` (or replicated) tables on each server with such data\r\n3) The data is updated periodically, you insert several thousands pairs each `n` minutes\r\n4) In addition to your data stored in many `local` tables, you want to store some metadata (or statistics) for each portion of data inserted in `local` tables. For example, you want to store number of uniq users for each minute.\r\n5) To do so, on each server you create `local_stat` tables that are MaterializedView tables: (approx.) `CREATE MATERIALIZED VIEW local_stat (createDate DateTime, uniq_users AggregateFunction(uniq)) AS SELECT now() AS createDate, uniqState(user) AS uniq_users FROM local GROUP BY toRelativeMinuteNum(user_visit_time)`\r\n6) Now on each server you have `local` tables with main data and `local_stat` MaterializedView tables with auxiliary statistics\r\n7) Each `INSERT` into `local` makes corresponding INSERT SELECT into `local_stat`\r\n8) Now you want to wrap many local tables (`local` and `local_stat`) into convenient `Distributed` tables: `local_all` and `local_stat_all` (and create such wrapper on each node).\r\n9) Now each INSERT into `local_all` is transformed in many local INSERTs into `local` tables. Each local INSERT activate INSERT SELECT for each \"attached\" MaterializedView table (i.e. for `local_stat`).\r\n10) After these chain of INSERTs are finished, you could SELECT results via Distrubuted tables `local` and `local_stat`\r\n\r\nSo, in your case you should create addition Distributed table for `insert_view` and send INSERTs into it."
      },
      {
        "user": "VictoryWangCN",
        "created_at": "2017-02-21T08:09:23Z",
        "body": "@ludv1x  but it's not work..."
      },
      {
        "user": "ludv1x",
        "created_at": "2017-02-21T10:11:34Z",
        "body": "What doesn't precisely work?\r\nCould you provide your configuration and queries?"
      },
      {
        "user": "VictoryWangCN",
        "created_at": "2017-02-27T06:56:55Z",
        "body": "config\r\n```\r\n<yandex>\r\n <clickhouse_remote_servers>\r\n   <perftest_2shards_1replicas>\r\n        <shard>\r\n            <replica>\r\n                <host>localtest.clickhouse.shard1</host>\r\n                <port>9000</port>\r\n            </replica>\r\n        </shard>\r\n        <shard>\r\n            <replica>\r\n                <host>localtest.clickhouse.shard2</host>\r\n                <port>9000</port>\r\n            </replica>\r\n        </shard>\r\n    </perftest_2shards_1replicas>\r\n </clickhouse_remote_servers>\r\n\r\n <zookeeper-servers>\r\n   <node>\r\n     <host>10.1.1.153</host>\r\n     <port>2181</port>\r\n   </node>\r\n </zookeeper-servers>\r\n\r\n <macros>\r\n   <replica>10.1.1.154</replica>\r\n   <shard>01</shard>\r\n </macros>\r\n</yandex>\r\n```\r\nand sql \r\n```\r\n\r\nCREATE TABLE IF NOT EXISTS insert_view_local(metricId Int64, applicationId Int64, agentRunId Int64, num1 Float64, num2 Float64, tc_startDate Date, tc_startTime UInt64) ENGINE = Null;\r\n\r\nCREATE TABLE insert_view as insert_view_local ENGINE = Distributed(perftest_2shards_1replicas, default, insert_view_local, rand());\r\n\r\nCREATE MATERIALIZED VIEW metric_data_entity_pt1h ENGINE = AggregatingMergeTree(tc_startDate,(tc_startTime, applicationId, metricId, agentRunId), 8192) AS SELECT tc_startDate, tc_startTime, applicationId,  metricId, agentRunId, sumState(num1) as num1,sumState(num2) as num2 FROM insert_view GROUP BY tc_startDate,tc_startTime,applicationId, metricId, agentRunId;\r\n\r\n```\r\ni use `insert into insert_view values(1, 10, 0, 0.4, 0.7, toDate('2017-02-27'), 1488178550000)`\r\n\r\nall the data is on the machine where the insertion statement is executed..."
      },
      {
        "user": "ludv1x",
        "created_at": "2017-02-28T11:39:42Z",
        "body": "Materialized View over Distributed table don't distribute insertions among the cluster.\r\nOnly insertions into `default.insert_view_local` will be distributed.\r\n\r\nYou need create Materialized View over `insert_view_local` (not over `insert_view`) on each server."
      },
      {
        "user": "VictoryWangCN",
        "created_at": "2017-02-28T12:33:39Z",
        "body": "so, The final sql statement is as follows:\r\n```\r\nCREATE TABLE IF NOT EXISTS insert_view_local(metricId Int64, applicationId Int64, agentRunId Int64, num1 Float64, num2 Float64, tc_startDate Date, tc_startTime UInt64) ENGINE = Null;\r\n\r\nCREATE TABLE insert_view as insert_view_local ENGINE = Distributed(perftest_2shards_1replicas, default, insert_view_local, rand());\r\n\r\nCREATE MATERIALIZED VIEW metric_data_entity_pt1h_local ENGINE = AggregatingMergeTree(tc_startDate,(tc_startTime, applicationId, metricId, agentRunId), 8192) AS SELECT tc_startDate, tc_startTime, applicationId,  metricId, agentRunId, sumState(num1) as num1,sumState(num2) as num2 FROM insert_view_local GROUP BY tc_startDate,tc_startTime,applicationId, metricId, agentRunId;\r\n\r\nCREATE TABLE metric_data_entity_pt1h as metric_data_entity_pt1h_local ENGINE = Distributed(perftest_2shards_1replicas, default, metric_data_entity_pt1h_local, rand());\r\n```\r\n\r\ninsert into `insert_view` and query `select some_column from metric_data_entity_pt1h` ?"
      },
      {
        "user": "VictoryWangCN",
        "created_at": "2017-02-28T12:34:33Z",
        "body": "it works.\r\nI highly appreciate your help, thanks."
      },
      {
        "user": "PangKuo",
        "created_at": "2019-03-01T11:05:28Z",
        "body": "What if many replicas in one shard? Can I change the ENGINE of materialized view to ReplicatedMergeTree?\r\n"
      },
      {
        "user": "Prakash9944",
        "created_at": "2019-09-09T13:40:50Z",
        "body": "asdad"
      },
      {
        "user": "qianjiangchao1992",
        "created_at": "2023-12-28T09:00:43Z",
        "body": "> \u6240\u4ee5\uff0c\u6700\u7ec8\u7684sql\u8bed\u53e5\u5982\u4e0b\uff1a\r\n> \r\n> ```\r\n> CREATE TABLE IF NOT EXISTS insert_view_local(metricId Int64, applicationId Int64, agentRunId Int64, num1 Float64, num2 Float64, tc_startDate Date, tc_startTime UInt64) ENGINE = Null;\r\n> \r\n> CREATE TABLE insert_view as insert_view_local ENGINE = Distributed(perftest_2shards_1replicas, default, insert_view_local, rand());\r\n> \r\n> CREATE MATERIALIZED VIEW metric_data_entity_pt1h_local ENGINE = AggregatingMergeTree(tc_startDate,(tc_startTime, applicationId, metricId, agentRunId), 8192) AS SELECT tc_startDate, tc_startTime, applicationId,  metricId, agentRunId, sumState(num1) as num1,sumState(num2) as num2 FROM insert_view_local GROUP BY tc_startDate,tc_startTime,applicationId, metricId, agentRunId;\r\n> \r\n> CREATE TABLE metric_data_entity_pt1h as metric_data_entity_pt1h_local ENGINE = Distributed(perftest_2shards_1replicas, default, metric_data_entity_pt1h_local, rand());\r\n> ```\r\n> \r\n> \u63d2\u5165`insert_view`\u5e76\u67e5\u8be2`select some_column from metric_data_entity_pt1h`?\r\nHello, sorry to bother you. I've encountered an issue where multiple queries on a distributed materialized view table yield inconsistent results, even though I have already stopped writing data to the source local table.\r\n"
      }
    ]
  },
  {
    "number": 9032,
    "title": "result set is encoded through the MySQL client ",
    "created_at": "2020-02-06T10:45:56Z",
    "closed_at": "2020-06-30T18:08:28Z",
    "labels": [
      "help wanted",
      "unexpected behaviour"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9032",
    "body": "Hi All,\r\n\r\n\r\nI am trying to connect the Clickhouse with the MySQL client . But, the result set is coming something encoded .\r\n\r\nAnything I missed ?\r\n\r\n\r\nmysql -h127.0.0.1 -P9001 -udefault -pSakthi@321\r\n\r\nmysql> show databases;\r\n+------------------+\r\n| name             |\r\n+------------------+\r\n| 0x64656661756C74 |\r\n| 0x6A65737573     |\r\n| 0x73797374656D   |\r\n+------------------+\r\n3 rows in set (0.00 sec)\r\nRead 3 rows, 354.00 B in 0.000 sec., 6478 rows/sec., 746.55 KiB/sec.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9032/comments",
    "author": "sakthi7",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-07T15:11:34Z",
        "body": "CH version?\r\n\r\ncc @yurriy"
      },
      {
        "user": "sakthi7",
        "created_at": "2020-02-08T11:19:07Z",
        "body": "Hi @den-crane ,\r\n\r\n\r\nThanks for the revert .\r\n\r\n[root@ip-172-31-8-156 ~]# clickhouse-server --version\r\nClickHouse server version 19.17.4.11.\r\n[root@ip-172-31-8-156 ~]# clickhouse-client --version\r\nClickHouse client version 19.17.4.11.\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "sakthi7",
        "created_at": "2020-02-08T11:37:55Z",
        "body": "Hi @den-crane ,\r\n\r\n\r\nThanks for the hint . I have tried with the MySQL 5.7 client . It is working fine for me .\r\n\r\n**Logs :**\r\n\r\n[root@ip-172-31-8-156 bin]# ./mysql --version\r\n./mysql  Ver 14.14 Distrib 5.7.28, for linux-glibc2.12 (x86_64) using  EditLine wrapper\r\n[root@ip-172-31-8-156 bin]# \r\n[root@ip-172-31-8-156 bin]# \r\n[root@ip-172-31-8-156 bin]# ./mysql -h127.0.0.1 -P9001 -udefault -pxxxxxxxxx\r\nmysql: [Warning] Using a password on the command line interface can be insecure.\r\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\r\nYour MySQL connection id is 16\r\nServer version: 19.17.4.11-ClickHouse \r\n\r\nCopyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.\r\n\r\nOracle is a registered trademark of Oracle Corporation and/or its\r\naffiliates. Other names may be trademarks of their respective\r\nowners.\r\n\r\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\r\n\r\nmysql> show databases;\r\n+---------+\r\n| name    |\r\n+---------+\r\n| default |\r\n| jesus   |\r\n| system  |\r\n+---------+\r\n3 rows in set (0.00 sec)\r\nRead 3 rows, 354.00 B in 0.000 sec., 6166 rows/sec., 710.56 KiB/sec.\r\n\r\n\r\n\r\nNow, why it is not working with MySQL 8 ? It is because of the character set ? Because the default charset is UTF8MB4 on MySQL 8 .\r\n\r\n \r\nThanks !!!\r\n"
      },
      {
        "user": "yurriy",
        "created_at": "2020-02-11T08:46:41Z",
        "body": "> Now, why it is not working with MySQL 8 ? It is because of the character set ? Because the default charset is UTF8MB4 on MySQL 8 .\r\n\r\nI will look into that."
      },
      {
        "user": "sakthi7",
        "created_at": "2020-02-17T19:57:27Z",
        "body": "Thank you !!!"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-18T22:53:53Z",
        "body": "@yurriy is not working on this issue right now, we have to reassign."
      },
      {
        "user": "yurriy",
        "created_at": "2020-06-30T17:00:52Z",
        "body": "> @yurriy is not working on this issue right now, we have to reassign.\r\n\r\nIt was fixed in #9079. Probably @den-crane reopened it because it wasn't in a stable version at that moment."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-06-30T18:08:39Z",
        "body": "Ok, thank you!"
      }
    ]
  },
  {
    "number": 503,
    "title": "How to copy big data  to ClickHouse",
    "created_at": "2017-02-17T05:56:57Z",
    "closed_at": "2017-02-17T18:22:10Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/503",
    "body": "I used tpch make 20G data in my OS.\r\nI know used command\r\n ```\r\n  time clickhouse-client --query=\"INSERT INTO NATION FORMAT CSV\" < some.csv\r\n```\r\ntpch make   data format is |  ,I read document can't find customer define  format .\r\n\r\nhow to copy this data to ClickHouse?\r\n\r\nThanks....",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/503/comments",
    "author": "sangli00",
    "comments": [
      {
        "user": "ludv1x",
        "created_at": "2017-02-17T11:12:51Z",
        "body": "Could you provide example of data (or its format description) which you try to load?"
      },
      {
        "user": "sangli00",
        "created_at": "2017-02-17T12:47:44Z",
        "body": "1  copy this data to postgres database \r\n2 copy postgres database from to directory\r\n3 use ClickHouse-client copy to ClickHouse \r\nIs very trouble\r\n\r\n![Uploading 5784F93A-C638-4F3A-A79B-D653845DAFD6.png\u2026]()\r\n"
      },
      {
        "user": "ludv1x",
        "created_at": "2017-02-17T14:27:06Z",
        "body": "I downloaded `TPCH_Tools_v2.17.1.zip` and loaded data from `customer.tbl.150000` into ClickHouse.\r\n\r\nYou just need to remove last `|` in each line and replace `|` to `\\t`.\r\nAfter you can import data into ClickHouse using TabSeparated FORMAT.\r\n\r\n```\r\nsed 's/|$//g' customer.tbl.150000 | tr \"|\" \"\\t\" | clickhouse-client -q \"INSERT INTO tpc FORMAT TSV\"\r\n```\r\n"
      },
      {
        "user": "sangli00",
        "created_at": "2017-02-17T14:29:16Z",
        "body": "yes, I remove last ```|```\r\nbut I can't replace ```|``` to ```\\t```\r\n\r\nused TabSeparated FORMAT is OK.\r\nThanks.\r\n"
      }
    ]
  }
]