[
  {
    "number": 1778,
    "title": "`Parameter transformer.layers.N.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method`",
    "created_at": "2024-06-13T13:06:39Z",
    "closed_at": "2024-06-13T16:07:01Z",
    "labels": [
      "bug",
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1778",
    "body": "### System Info\r\n\r\nWhile trying to debug poor quality of outputs from TRT LLM for Llama3 70b tp=4 (compared to vLLM and HF), I ran into the following message when building bfloat16 engine.\r\n\r\n```\r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network\r\n```\r\n\r\n(repeated for each layer)\r\n\r\nIs this message harmless?\r\n\r\nThe commands I run:\r\n\r\n```sh\r\npython convert_checkpoint.py \\\r\n--model_dir /workspace/llama3-70b \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--dtype bfloat16 \\\r\n--tp_size 4\r\n\r\ntrtllm-build \\\r\n--checkpoint_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4-engine \\\r\n--gpt_attention_plugin bfloat16 \\\r\n--gemm_plugin bfloat16 \\\r\n--use_custom_all_reduce disable \\\r\n--max_num_tokens 32768 \\\r\n--max_batch_size 48 \\\r\n--max_input_len 8192 \\\r\n--max_output_len 4096\r\n```\r\n\r\nThe full logs:\r\n\r\n```\r\n[TensorRT-LLM] TensorRT-LLM version: 0.11.0.dev2024060400                                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set bert_attention_plugin to auto.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set nccl_plugin to auto.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lookup_plugin to None.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lora_plugin to None.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set moe_plugin to auto.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.                                                                                                                                                                                                                                   \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha to True.                                                                                                                                                                                                                                          \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_kv_cache to True.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set remove_input_padding to True.                                                                                                                                                                                                                                  \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multi_block_mode to False.                                                                                                                                                                                                                                     \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set enable_xqa to True.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set tokens_per_block to 64.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_paged_context_fmha to False.                                                                                                                                                                                                                               \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_fp8_context_fmha to False.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multiple_profiles to False.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_state to True.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set streamingllm to False.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Specifying a `max_num_tokens` larger than 16384 is usually not recommended, we do not expect perf gain with that and too large `max_num_tokens` could possibly exceed the TensorRT tensor volume, causing runtime errors. Got `max_num_tokens` = 32768             \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.mup_width_multiplier = 1.0                                                                                                                                                                                                          \r\n[06/13/2024-13:01:15] [TRT-LLM] [I] Set dtype to bfloat16.                                                                                                                                                                                                                                             \r\n[06/13/2024-13:01:15] [TRT] [I] [MemUsageChange] Init CUDA: CPU +17, GPU +0, now: CPU 160, GPU 528 (MiB)                                                                                                                                                                                               \r\n[06/13/2024-13:01:19] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 4607, GPU 1678 (MiB)                                                                                                                                                                      \r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.\r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                              [138/782]\r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.                                                                                                                                   \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set nccl_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                         \r\n```\r\n\r\n### Who can help?\r\n\r\n@byshiue \r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n4xH100 SXM\r\n\r\n### Expected behavior\r\n\r\nUnsure, maybe the message is harmless\r\n\r\n### actual behavior\r\n\r\nN/A\r\n\r\n### additional notes\r\n\r\nN/A",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1778/comments",
    "author": "DreamGenX",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-13T14:11:46Z",
        "body": "This is a known issue introduced by new feature weightless engine, it's just a warning message and harmless. Please ignore it and we'll fix it in the coming release."
      },
      {
        "user": "DreamGenX",
        "created_at": "2024-06-13T15:32:50Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 1748,
    "title": "\u2018cudaStream_t\u2019 has not been declared when building tensorrt_llm v0.10.0",
    "created_at": "2024-06-06T06:09:39Z",
    "closed_at": "2024-06-06T06:29:02Z",
    "labels": [
      "question",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1748",
    "body": "### System Info\n\nCPU: INTEL\r\nGPU Name: A100-SXM4-80GB\r\nTensorRT-LLM: tag v0.10.0\r\nContainer Used: No\r\nDriver Version: 535.54.03\r\nCUDA Version: 12.1\r\nOS: Ubuntu 22.04\r\nOthers: tensorrt==10.0.1.16, pytorch==2.2.0+cu121, python==3.10.12\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\npython scripts/build_wheel.py --trt_root=/usr/local/tensorrt --clean --install --cuda_architectures=\"80-real\"\n\n### Expected behavior\n\nWorks fine\n\n### actual behavior\n\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:48:37: error: \u2018cudaStream_t\u2019 has not been declared\r\n   48 | void invokeRGLRU(lruParams& params, cudaStream_t stream);\r\n      |                                     ^~~~~~~~~~~~\r\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:51:43: error: \u2018cudaStream_t\u2019 has not been declared\r\n   51 | void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);\r\n      |                                           ^~~~~~~~~~~~\n\n### additional notes\n\nNo",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1748/comments",
    "author": "zhangts20",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:12:26Z",
        "body": "Env issue\r\nTry this container: nvcr.io/nvidia/tritonserver:24.05-trtllm-python-py3"
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:22:10Z",
        "body": "@hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks"
      },
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:29:35Z",
        "body": "> @hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks\r\n\r\nYou could build the container that come FROM this container\r\nOr use pip install tensorrt_llm==xxx in your container."
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:46:14Z",
        "body": "@hijkzzz thanks! I have installed it successfully using pip install tensorrt_llm==xxx"
      },
      {
        "user": "yorickvP",
        "created_at": "2024-06-12T13:19:32Z",
        "body": "In case anyone else wants to build from source, add `#include <cuda_runtime.h>` to `cpp/tensorrt_llm/kernels/lruKernel.h`"
      }
    ]
  },
  {
    "number": 1523,
    "title": "can trtllm-build process on cpu? ",
    "created_at": "2024-04-29T09:02:41Z",
    "closed_at": "2024-04-30T03:43:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1523",
    "body": "### System Info\n\nNVIDIA A800 40G\n\n### Who can help?\n\n@byshiue \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\ncan trtllm-build process on cpu? like parameter load_model_on_cpu in convert_checkpoint.py\n\n### Expected behavior\n\nnone\n\n### actual behavior\n\nnone\n\n### additional notes\n\nnone",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1523/comments",
    "author": "thend-wk",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-04-29T14:04:27Z",
        "body": "trtllm-build requires NVIDIA GPUs to timing the kernel perf."
      },
      {
        "user": "thend-wk",
        "created_at": "2024-04-30T01:27:55Z",
        "body": "> trtllm-build requires NVIDIA GPUs to timing the kernel perf.\r\n\r\ni get it, thanks"
      }
    ]
  },
  {
    "number": 1410,
    "title": "What is the meaning for the benchmark output `tokens_per_sec` and `generation_tokens_per_second`? ",
    "created_at": "2024-04-07T07:43:18Z",
    "closed_at": "2024-04-10T09:11:21Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1410",
    "body": "I run benchmark like this:\r\n```\r\nmpirun -n 2 --allow-run-as-root python benchmark.py \\\r\n    -m llama_13b \\\r\n    --mode plugin \\\r\n    --batch_size \"1;8;16\" \\\r\n    --input_output_len \"710,190\" \\\r\n    --max_input_len 750 --max_output_len 200\r\n```\r\nI got this:\r\n```\r\n[BENCHMARK] model_name llama_13b world_size 2 num_heads 40 num_kv_heads 40 num_layers 40 hidden_size 5120 vocab_size 32000 precision float16 batch_size 1 input_length 710 output_length 190 gpu_peak_mem(gb) 0.0 build_time(s) 116.39 tokens_per_sec 43.09 percentile95(ms) 5120.208 percentile99(ms) 5120.208 latency(ms) 4409.816 compute_cap sm80 quantization QuantMode.0 generation_time(ms) 3751.546 total_generated_tokens 189.0 generation_tokens_per_second 50.379\r\n```\r\n\r\nI see there are two token per sec numbers, which is correct? and what is the meaning for each of them?\r\n\r\nI can't find any documentation mentioning that.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1410/comments",
    "author": "sleepwalker2017",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-09T07:57:54Z",
        "body": "`tokens_per_sec` means the throughput of end to end inference. It is computed by `generated_tokens / total_latency`. In your case, it is computed by `190 / 4409.816 * 1000 = 43.09`.\r\n\r\n`generation_tokens_per_second` only consider the generation. It means the thorughput during generation and computed by `generated_tokens / generation_time`. In your case, it is computed by  `189 / 3751.546 * 1000 = 50.379`. "
      },
      {
        "user": "sleepwalker2017",
        "created_at": "2024-04-09T08:02:32Z",
        "body": "> `tokens_per_sec` means the throughput of end to end inference. It is computed by `generated_tokens / total_latency`. In your case, it is computed by `190 / 4409.816 * 1000 = 43.09`.\r\n> \r\n> `generation_tokens_per_second` only consider the generation. It means the thorughput during generation and computed by `generated_tokens / generation_time`. In your case, it is computed by `189 / 3751.546 * 1000 = 50.379`.\r\n\r\nGot it, the lower number includes the prefill stage."
      },
      {
        "user": "YiandLi",
        "created_at": "2024-04-26T07:55:08Z",
        "body": "what about  `gpu_peak_mem` mean ? It is 0 in my case.\r\n"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-09T07:05:06Z",
        "body": "Could you take a try on latest main branch? "
      }
    ]
  }
]