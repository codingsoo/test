[
  {
    "number": 37677,
    "title": "[<Ray component: Cluster>] KeyError: 'CPU' error in Linux",
    "created_at": "2023-07-22T10:49:28Z",
    "closed_at": "2023-07-24T21:18:06Z",
    "labels": [
      "question",
      "triage",
      "core"
    ],
    "url": "https://github.com/ray-project/ray/issues/37677",
    "body": "### What happened + What you expected to happen\r\n\r\n**What I will do:**\r\nI tried to get the total number of cpus provided by the cluster;\r\n\r\n**What I got wrong:**\r\nThe specific error information is as follows:\r\n{cluster_resources()['CPU']} CPU resources in total;\r\nKeyError: 'CPU'\r\n\r\n**Update:**\r\n_I seem to have found the reason, when there is no available cpu in the cluster, the 'CPU' key is no longer in the returned dict; This leads to errors;_\r\n\r\n### Versions / Dependencies\r\n\r\nray: 2.3.1\r\nos: debian 11\r\npython: 3.9.2\r\n\r\n### Reproduction script\r\n\r\nfrom ray import init, cluster_resources\r\ninit()\r\nprint(f\"{cluster_resources()['CPU']}\")\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/37677/comments",
    "author": "stevenhubhub",
    "comments": [
      {
        "user": "jjyao",
        "created_at": "2023-07-24T21:18:06Z",
        "body": "Yea, try to do `cluster_resources().get(\"CPU\", 0)`"
      },
      {
        "user": "stevenhubhub",
        "created_at": "2023-07-27T08:45:26Z",
        "body": "> Yea, try to do `cluster_resources().get(\"CPU\", 0)`\r\n\r\nThanks!"
      },
      {
        "user": "davide-russo-tfs",
        "created_at": "2024-09-30T10:51:06Z",
        "body": "Good morning, I have the same issue while trying to use Ray on Databricks cluster (with autoscaling). The runtime used is 15.1ML.\r\nI imported the following libraries:\r\n```\r\nfrom ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\r\nfrom ray.util.multiprocessing import Pool\r\nimport ray\r\n```\r\nThis is how I set up the environment:\r\n```\r\nsetup_ray_cluster(\r\n        num_worker_nodes  = 4,\r\n        num_cpus_per_node = 4,\r\n        autoscale          = True\r\n    )\r\nray.init(ignore_reinit_error = True)\r\n```\r\nthen I decorated a function to be run in parallel by using `@ray.remote` and tried to create a pool of processes this way:\r\n```\r\nwith Pool(processes = 8) as pool:\r\n        pool.starmap(foo, inputs)\r\n```\r\n\r\nHow can I solve this problem? Thank you for your help."
      }
    ]
  },
  {
    "number": 31151,
    "title": "ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...>] ",
    "created_at": "2022-12-16T06:43:57Z",
    "closed_at": "2022-12-16T12:58:13Z",
    "labels": [
      "question",
      "docs"
    ],
    "url": "https://github.com/ray-project/ray/issues/31151",
    "body": "### Description\n\nHellow,i'm sorry to bother you.I want to use ray 2.0.0.dev0.But I don't know where I can find it.There is only version ray 3.0.0.dev0 in the documentation.Can you tell me where i can get it,thanks!\n\n### Link\n\n_No response_",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/31151/comments",
    "author": "aa-oo",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2022-12-16T12:56:58Z",
        "body": "Hey @aa-oo , thanks for filing this issue.\r\n* All versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n* To get a stable version of Ray, you can simply try `pip install ray==2.2` (brand new one) or some older versions like `pip install ray==2.1` or `pip install ray==2.0`."
      },
      {
        "user": "aa-oo",
        "created_at": "2022-12-16T13:52:05Z",
        "body": "Ok,thank you!\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: \"Sven ***@***.***&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2022\u5e7412\u670816\u65e5(\u661f\u671f\u4e94) \u665a\u4e0a8:57\r\n\u6536\u4ef6\u4eba: ***@***.***&gt;; \r\n\u6284\u9001: ***@***.***&gt;; ***@***.***&gt;; \r\n\u4e3b\u9898: Re: [ray-project/ray] ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...&gt;]  (Issue #31151)\r\n\r\n\r\n\r\n\r\n\r\n \r\nHey @aa-oo , thanks for filing this issue.\r\n  \r\nAll versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n \r\nTo get a stable version of Ray, you can simply try pip install ray==2.2 (brand new one) or some older versions like pip install ray==2.1 or pip install ray==2.0.\r\n  \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;"
      }
    ]
  },
  {
    "number": 30662,
    "title": "[tune] How to use an imported parameter via argparse in trainable function",
    "created_at": "2022-11-25T17:32:05Z",
    "closed_at": "2022-11-29T20:04:15Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/30662",
    "body": "### What happened + What you expected to happen\n\nI have a tuning task using an imported parameter via argparse in trainable function. The task crashes complaining the argument is not provided. It works fine If I use it outside the trainable function. \n\n### Versions / Dependencies\n\nRay 2.1.0\n\n### Reproduction script\n\nThe script being imported called \u201cinput_param.py\u201d:\r\n\r\n    import sys, argparse\r\n\r\n    parser = argparse.ArgumentParser(description='')\r\n    parser.add_argument('--ttt', type=int, required=True, help='anything > 1')\r\n    args = parser.parse_args()\r\n\r\n    ttt = args.ttt\r\n\r\nThe tuning task code is named as \u2018example.py\u2019:\r\n\r\n    import os\r\n    from ray import tune, air\r\n    from hyperopt import hp\r\n    from ray.tune.search.hyperopt import HyperOptSearch\r\n    import input_param as input_param\r\n\r\n    def trainable(config):\r\n        #print('!! ttt = ', input_param.ttt)\r\n        score = config[\"a\"] ** 2 + config[\"b\"]\r\n        tune.report(SCORE=score)\r\n\r\n\r\n    search_space = {\r\n        \"a\": hp.uniform(\"a\", 0, 1),\r\n        \"b\": hp.uniform(\"b\", 0, 1)\r\n        }\r\n\r\n    raw_log_dir = \"./ray_log\"\r\n    raw_log_name = \"example\"\r\n\r\n    algorithm = HyperOptSearch(search_space, metric=\"SCORE\", mode=\"max\", n_initial_points=1)\r\n\r\n\r\n    tuner = tune.Tuner(trainable,\r\n            tune_config = tune.TuneConfig(\r\n                num_samples = 10,\r\n                search_alg=algorithm,\r\n                ),\r\n            param_space=search_space,\r\n            run_config = air.RunConfig(local_dir = raw_log_dir, name = raw_log_name) #\r\n            )\r\n\r\n    print('!! ttt = ', input_param.ttt)\r\n    results = tuner.fit()\r\n    print(results.get_best_result(metric=\"SCORE\", mode=\"max\").config)\r\n\r\nI run the task via the following command:\r\n\r\n    py example.py --ttt 99\r\n\r\nThe following is part of the error:\r\n\r\n    (pid=19560) default_worker.py: error: the following arguments are required: --ttt\r\n    (pid=19560) 2022-11-23 20:45:01,769     ERROR worker.py:763 -- Worker exits with an exit code 2.\r\n\r\n\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/30662/comments",
    "author": "wxie2013",
    "comments": [
      {
        "user": "justinvyu",
        "created_at": "2022-11-28T17:56:06Z",
        "body": "Is it possible to work around this by passing the arguments into the config? Is there a specific reason why the arguments need to be stored and accessed in the trainable as a separate python module?\r\n\r\n```python\r\nsearch_space = {\r\n    # ...\r\n    \"ttt\": input_param.ttt,\r\n}\r\n```"
      },
      {
        "user": "wxie2013",
        "created_at": "2022-11-28T20:19:02Z",
        "body": "Thanks for the follow-up.  It is possible to implement a walkaround.  It would be nice to understand the reason why above example code doesn't work so that I won't stumble into similar problems in the future. "
      },
      {
        "user": "Yard1",
        "created_at": "2022-11-29T18:45:27Z",
        "body": "Hey @wxie2013, as I mentioned in the discuss thread, this is because the trainable function is ran in a separate process on each Tune worker in parallel. Therefore, argparse will expect arguments that are simply not provided when Ray spawns those processes."
      },
      {
        "user": "wxie2013",
        "created_at": "2022-11-29T20:04:15Z",
        "body": "Hi @Yard1, got it. Thanks for the help"
      }
    ]
  },
  {
    "number": 13576,
    "title": "[tune] Errors when using points_to_evaluate argument for a few search algos",
    "created_at": "2021-01-20T05:28:07Z",
    "closed_at": "2021-01-21T02:23:26Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/13576",
    "body": "I am trying out a few search algos (run from scratch) with Ray version 1.1.0, and is running into a few issues when using `points_to_evaluate` argument:\r\n\r\nSearch Space Setup:\r\n```\r\neven_int_model_dim = [x for x in range(1, 12+1) if x % 2 == 0]\r\neven_int_batch_size = [x for x in range(1, 16+1) if x % 2 == 0]\r\nint_sequence = [x for x in range(20, 100+1)]\r\nint_local_context_len = [x for x in range(3, 15+1)]\r\nint_num_heads = [x for x in range(2, 6+1)]\r\n\r\nconfig={'seed': 0, 'train_start_date': 'None', 'valid_start_date': '2017-01-01', 'test_start_date': '2018-01-01',\r\n\t'data_path': 'path/to/data', 'num_epoch': 100, 'loss_fn': loss_fn, 'device': 'cuda:0', \r\n\t'sequence': tune.choice(int_sequence),\r\n\t'local_context_len': tune.choice(int_local_context_len), 'batch_size': tune.choice(even_int_batch_size), 'num_heads': tune.choice(int_num_heads),\r\n\t'model_dim': tune.choice(even_int_model_dim), 'num_layers': tune.choice([1, 2]), 'dropout': tune.uniform(0, 0.7),\r\n\t'allocator': 'numark', 'max_weight': 0.1, 'stochasticity': tune.choice([True, False]),\r\n\t'resample': False, 'n_draws': 100, 'n_portfolios': 5, 'feature_dims': 0, \r\n\t'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, \r\n\t'eps': 0.00000001, 'weight_decay': 0, 'amsgrad': True}\r\n\r\ncurrent_best = [{'sequence': 100, 'local_context_len': 5, 'batch_size': 16, \r\n\t\t 'num_heads': 6, 'model_dim': 12, 'num_layers': 1, 'dropout': 0.01, 'stochasticity': False}]\r\n```\r\n\r\nWhen using `HyperOpt`:\r\n`algo = HyperOptSearch(points_to_evaluate=current_best)`\r\nError:\r\n> File \"/home/user/anaconda3/envs/user/lib/python3.7/site-packages/hyperopt/pyll/base.py\", line 874, in rec_eval\r\n>     rval_var = node.pos_args[int(switch_i) + 1]\r\n> IndexError: list index out of range\r\n\r\nWhen using `Optuna`:\r\n`algo = OptunaSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'\r\n\r\nWhen using `Ax`:\r\n`algo = AxSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13576/comments",
    "author": "turmeric-blend",
    "comments": [
      {
        "user": "krfricke",
        "created_at": "2021-01-20T08:35:22Z",
        "body": "Hi @turmeric-blend, the `points_to_evaluate` arguments for most algorithms are currently only available in the nightly wheels / current master and not in the latest release.\r\nYou can try `ray install-nightly` to install the nightly wheels. "
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-21T02:23:26Z",
        "body": "ok thanks"
      }
    ]
  },
  {
    "number": 13517,
    "title": "[tune] how to enforce even integer number in the search space",
    "created_at": "2021-01-18T08:18:39Z",
    "closed_at": "2021-01-19T09:11:44Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/13517",
    "body": "I require my `batch_size` in the search space to be an **even number**, so I tried `tune.qrandint(4, 64, 2)`. However I am also using `HyperOpt` which gave this warning:\r\n\r\n> HyperOpt does not support quantization for integer values. Reverting back to 'randint'.\r\n\r\nMaking certain trials contain odd `batch_size` which produces error in my model. Is there another way to enforce even integer number in the search space?\r\n\r\nEDIT:\r\n\r\nI also tried `tune.sample_from(lambda spec: np.random.randint(2, 64) * 2)`, and HyperOpt gave error:\r\n\r\n> HyperOpt does not support parameters of type `Function` with samplers of type `NoneType`",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13517/comments",
    "author": "turmeric-blend",
    "comments": [
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-18T08:56:32Z",
        "body": "solved by directly using `np.random.randint(2, 128)*2` in config search space instead of `tune.`"
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-19T00:41:36Z",
        "body": "reopening as it actually just returned a constant instead of random even integer between 2 and 128. Issue remains how to  enforce even integer number in the search space?"
      },
      {
        "user": "richardliaw",
        "created_at": "2021-01-19T04:31:36Z",
        "body": "Can you try doing a tune.randint, and then in your training function, multiply it by 2?"
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-19T06:32:27Z",
        "body": "I assume you mean\r\n\r\n```\r\nconfig={..., 'batch_size': tune.randint(1, 64), ...}\r\n\r\n\r\ndef train_function(config):\r\n      batch_size = config['batch_size']*2\r\n```\r\n\r\nI guess it works but then I have to always be aware that my range is half the max range in `randint`.\r\n\r\nAnyway, I did something like this instead, `tune.choice([x for x in range(1, 17) if x % 2 == 0])` which works well."
      },
      {
        "user": "krfricke",
        "created_at": "2021-01-19T09:11:44Z",
        "body": "Please note that this has been improved upon in the latest master and the ray nightly wheels. The next release will support quantized integers in hyperopt out of the box (e.g. using `tune.qrandint()`).\r\n\r\nPlease re-open if you have any more questions."
      }
    ]
  },
  {
    "number": 11971,
    "title": "[rllib] PPO ICM learning rate",
    "created_at": "2020-11-12T13:05:46Z",
    "closed_at": "2020-11-14T11:21:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/11971",
    "body": "Hello, I know the default ppo learning rate is 5e-5, default curiosity learning rate is 0.001. \r\nI just want to know whether the two learning rate are same?   \r\n\r\nIf I use curiosity in ppotrainer, how do I set it?\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11971/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-11-13T08:10:26Z",
        "body": "Hey @zzchuman , no they are not the same. The curiosity module has its own optimizer and lr.\r\nYou can set the curiosity lr inside the exploration_config key, the same way as it's done in rllib/utils/explorations/tests/test_curiosity.py:\r\n```\r\n            config[\"exploration_config\"] = {\r\n                \"type\": \"Curiosity\",\r\n                \"eta\": 0.2,\r\n                \"lr\": 0.001,  # <- HERE\r\n                \"feature_dim\": 128,\r\n                \"feature_net_config\": {\r\n                    \"fcnet_hiddens\": [],\r\n                    \"fcnet_activation\": \"relu\",\r\n                },\r\n                \"sub_exploration\": {\r\n                    \"type\": \"StochasticSampling\",\r\n                }\r\n            }\r\n```"
      },
      {
        "user": "zzchuman",
        "created_at": "2020-11-13T08:14:21Z",
        "body": "Thank you! got it! @sven1977 ,  I have a try! Thank you! "
      }
    ]
  },
  {
    "number": 10416,
    "title": "Always getting .nan reward while training with PPO or DQN",
    "created_at": "2020-08-29T04:38:03Z",
    "closed_at": "2020-08-30T02:54:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10416",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Can anyone please give me hints why I am always getting the following while training with PPO or DQN?\r\nepisode_len_mean: .nan\r\nepisode_reward_max: .nan\r\nepisode_reward_mean: .nan\r\nepisode_reward_min: .nan\r\nepisodes_this_iter: 0\r\nepisodes_total: 0\r\n\r\nRay Version: 0.8.7\r\nOS: macOS\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10416/comments",
    "author": "ashutosh1906",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-08-29T07:15:57Z",
        "body": "`episodes_total: 0`. This is the reason. Until an episode has finished, we can't calculate any rewards. Does your env eventually return done=True at some point?"
      },
      {
        "user": "ashutosh1906",
        "created_at": "2020-08-30T02:54:07Z",
        "body": "Thank you. After putting \"done = True\" at some points, episodes_total becomes non-zero and does not return any .nan."
      }
    ]
  },
  {
    "number": 10312,
    "title": "[ray] How to startup workers more than number of cores",
    "created_at": "2020-08-25T14:55:28Z",
    "closed_at": "2020-08-26T05:16:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10312",
    "body": "How to set ray startup arguments to let 150 workers running on a 96 cores machine? I notice ray will auto-scale on the local machine, but how to set while running a cluster?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10312/comments",
    "author": "Seraphli",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-26T01:24:12Z",
        "body": "Just set --num-cpus=150! "
      },
      {
        "user": "Seraphli",
        "created_at": "2020-08-26T05:16:19Z",
        "body": "I tried this and it works. Thank you."
      }
    ]
  },
  {
    "number": 9863,
    "title": "How to init Ray with a specified GPU id to run all trials of Tune?",
    "created_at": "2020-08-02T12:46:26Z",
    "closed_at": "2020-08-03T02:01:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9863",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\nSay, I have 4 GPUs with ids=[0, 1, 2, 3] and I only want to run all trials for Tune on id=2 and id=3 only. That means I can only maximize the use of the third and fourth GPU without touching the first two GPUs. How can I achieve this? \r\n\r\n```ray.init(num_cpus=num_cpus, num_gpus=num_gpus, temp_dir=ray_log)```\r\n\r\nThe attribute ```num_gpus``` is the number of GPUs ray can use. When setting ```num_gpus=1```, all the trials run on the first device (GPU id=0).  When increasing ```num_gpus```, all the trials will ordinally use GPUs from id=0 to id=3... I want to know how to specify the exact GPU ids, e.g., all trials run on id=2 and id=3.\r\n\r\nI've tried specifying GPU id in the training functions, but raised ```RuntimeError: CUDA error: invalid device ordinal```. \r\n\r\nI'm still new to this great project. Appreciate your warm help!\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nOS: Linux\r\nPython: 3.7.4\r\nRay: 0.8.6",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9863/comments",
    "author": "guoxuxu",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-08-02T17:24:52Z",
        "body": "Try setting the CUDA_VISIBLE_DEVICES before running the ray script?"
      },
      {
        "user": "guoxuxu",
        "created_at": "2020-08-03T02:01:24Z",
        "body": "Soga. It works now. Thanks very much!"
      },
      {
        "user": "ndvbd",
        "created_at": "2023-05-16T18:22:39Z",
        "body": "But is there a smarter way, to automatically choose the free gpus from the cluster?"
      }
    ]
  },
  {
    "number": 9309,
    "title": "[rllib] Cannot detect pybullet environments.",
    "created_at": "2020-07-06T00:56:25Z",
    "closed_at": "2020-07-06T03:43:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9309",
    "body": "### Pybullet Environments Cannot Be Detected By Ray/rllib\r\n\r\nHi, I'm trying to use rllib to train pybullet games. My environment is ray 0.8.4, ubuntu 16.04, Pytorch  1.2.0. It seems that ray cannot detect these games and said the game was not registered. But I can make the gym environment outside ray within the same script. I attached a simple code to show what's wrong. Could someone help with this? Thanks!!\r\n\r\n```\r\nimport ray\r\nfrom ray.rllib.agents.ppo import PPOTrainer\r\nfrom ray.tune.registry import register_env\r\nimport gym\r\nimport pybullet_envs\r\n\r\nenv = gym.make('HumanoidBulletEnv-v0')\r\nprint(\"Made Successfully\")\r\n\r\nclass MyEnv(gym.Env):\r\n    def __init__(self, env_config):\r\n        self.env = gym.make('HumanoidBulletEnv-v0')\r\n        self.action_space = self.env.action_space\r\n        self.observation_space = self.env.observation_space\r\n\r\n    def reset(self):\r\n        obs = self.env.reset()\r\n        return obs\r\n\r\n    def step(self, action):\r\n        action = self.action_space.high * action\r\n        obs, reward, done, info = self.env.step(action)\r\n        return obs, reward, done, info\r\n\r\nregister_env(\"myenv\", lambda config: MyEnv(config))\r\n\r\n\r\ndef main():\r\n    ray.init()    \r\n    trainer = PPOTrainer(env=\"myenv\", config={\r\n        \"use_pytorch\": True,\r\n        })\r\n\r\n    for i in range(100):\r\n        trainer.train()\r\n\r\n    trainer.stop()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\nWhen I run the code, the environment outside ray could be made successfully and 'Made Successfully' was printed. But then I get the error that \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"pybullet_train.py\", line 44, in <module>\r\n    main()\r\n  File \"pybullet_train.py\", line 39, in main\r\n    trainer.train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 502, in train\r\n    raise e\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 491, in train\r\n    result = Trainable.train(self)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 150, in _train\r\n    fetches = self.optimizer.step()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py\", line 59, in step\r\n    for e in self.workers.remote_workers()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/memory.py\", line 29, in ray_get_and_free\r\n    result = ray.get(object_ids)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1513, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(UnregisteredEnv): ray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 118, in spec\r\n    return self.env_specs[id]\r\nKeyError: 'HumanoidBulletEnv-v0'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 287, in __init__\r\n    self.env = _validate_env(env_creator(env_context))\r\n  File \"pybullet_train.py\", line 27, in <lambda>\r\n    register_env(\"myenv\", lambda config: MyEnv(config))\r\n  File \"pybullet_train.py\", line 14, in __init__\r\n    self.env = gym.make('HumanoidBulletEnv-v0')\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 142, in make\r\n    return registry.make(id, **kwargs)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 86, in make\r\n    spec = self.spec(path)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 128, in spec\r\n    raise error.UnregisteredEnv('No registered env with id: {}'.format(id))\r\ngym.error.UnregisteredEnv: No registered env with id: HumanoidBulletEnv-v0\r\n```\r\n\r\nI know 'import pybullet_envs' will register the environments in gym. It looked like rollout workers didn't detect these environments. Could someone tell me how to solve this? Thank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9309/comments",
    "author": "KarlXing",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-07-06T02:59:30Z",
        "body": "Can you try moving the import into the constructor for your class? The problem is the import only applies locally and not on the Ray workers."
      },
      {
        "user": "KarlXing",
        "created_at": "2020-07-06T03:43:13Z",
        "body": "Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply."
      },
      {
        "user": "Glaucus-2G",
        "created_at": "2020-07-14T08:27:50Z",
        "body": "> Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply.\r\n\r\n\r\nCould you show me your codes? I am learning how to use it! Thank you!"
      },
      {
        "user": "Glaucus-2G",
        "created_at": "2020-07-15T02:25:16Z",
        "body": "> > Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply.\r\n\r\n> Could you show me your codes? I am learning how to use it! Thank you!\r\n\r\nI think I have got it.  Thank you!"
      }
    ]
  },
  {
    "number": 8622,
    "title": "Restoring from checkpoint on different machine",
    "created_at": "2020-05-26T17:23:53Z",
    "closed_at": "2020-05-26T19:35:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8622",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI trained a model on one machine and I am able to load and restore properly from a saved checkpoint on that machine. When I copied over the checkpoint and try to restore and execute on a different machine, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_rllib_model.py\", line 77, in <module>\r\n    test_agent.restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/tune/trainable.py\", line 417, in restore\r\n    self._restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 639, in _restore\r\n    self.__setstate__(extra_data)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 192, in __setstate__\r\n    Trainer.__setstate__(self, state)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 1070, in __setstate__\r\n    self.optimizer.restore(state[\"optimizer\"])\r\nAttributeError: 'NoneType' object has no attribute 'restore'\r\n```\r\nAm I missing some files or something? I've copied over all the files in the checkpoint directory and also the `params.pkl` and `params.json` file. \r\nI'm creating a trainer instance and restoring from the checkpoint. \r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8622/comments",
    "author": "jangkj09",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-05-26T18:14:29Z",
        "body": "Maybe the ray versions are different on the two machines? Could you post your specs?"
      },
      {
        "user": "jangkj09",
        "created_at": "2020-05-26T19:35:30Z",
        "body": "Yes, that solved the problem. On one machine I had the stable 0.8.5 and on the other I had 0.9.0.dev0\r\nRe-installing with 0.8.5 resolved the problem. \r\n\r\nI don't know if this exists somewhere in the docs, but it would be helpful to indicate this difference more explicitly. I spent over an hour trying to debug what was going on. Thanks!"
      }
    ]
  },
  {
    "number": 8545,
    "title": "[ray] Is it bad practice to use sockets (pyzmq) to communicate between ray remote functions?",
    "created_at": "2020-05-22T06:17:38Z",
    "closed_at": "2020-05-27T15:03:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8545",
    "body": "I have a `send()` function that generates random numpy arrays at every time step, and a `recv()` function that receives and prints those generated arrays. I am using `zmq` for sending/receiving the numpy arrays across the processes, and `pyarrow` to serialize and deserialize arrays. I wasn't able to find any examples using ray and zmq together, so I would like to know whether this is bad practice. If so, is there a recommended way to have the distributed-ly running processes communicate with each other using ray?\r\n\r\nThank you so much! \r\n\r\nPasted below is minimal working code (on Ubuntu 18.0.4, python=3.6.9, pyzmq=19.0.1, ray=0.8.5, pyarrow=0.17.1):\r\n\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport ray\r\nimport zmq\r\nray.init()\r\n\r\n\r\n@ray.remote\r\ndef send():\r\n    port = 5556\r\n    context = zmq.Context()\r\n    send_socket = context.socket(zmq.PUSH)\r\n    send_socket.bind(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        msg = np.random.rand(1, 3) # this could be larger, e.g. numpy-ed torch neural network weights\r\n        object_id = pa.serialize(msg).to_buffer()\r\n        send_socket.send(object_id)\r\n\r\n@ray.remote\r\ndef recv():        \r\n    port = 5556\r\n    context = zmq.Context()\r\n    recv_socket = context.socket(zmq.PULL)\r\n    recv_socket.connect(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        object_id = recv_socket.recv()\r\n        msg = pa.deserialize(object_id)\r\n        print(msg)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.wait([send.remote(), recv.remote()])\r\n```\r\n## Note:\r\nI had to use pyarrow for serialization since ray object id's (obtained via `ray.put()`) could not be passed through zmq sockets; doing so gives the error below:  \r\n```\r\nObjectID(45b95b1c8bd3a9c4ffffffff0100008801000000) does not provide a buffer interface.\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8545/comments",
    "author": "cyoon1729",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T06:33:20Z",
        "body": "Ray already handles inter-process communication as well as serialization using apache arrow. You can just do.\r\n\r\n```python3\r\nimport ray\r\nray.init()\r\n\r\n@ray.remote\r\nclass ReceiveServer:\r\n    def recv(self, msg):\r\n        print(msg)\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\nrecv_server_handle = ReceiveServer.remote()\r\nray.wait(send.remote(recv_server_handle))\r\n```\r\nThis should do the same thing."
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-22T07:35:22Z",
        "body": "@rkooo567 Thank you so much for your response and the example above. I would like to ask another question:\r\n \r\nSay, for instance, I have the `ReceiveServer` above to store the `msg` in an internal storage `self.storage (deque)` when `recv()` is called in `send()`, while continuously (as in a `while: True` loop) sampling data from `self.storage` and processing it in another member function `process()`.\r\n\r\nIf I were to run `process.remote()` asynchronously with respect to `send()`, would a mutual exclusion of `ReceiveSercer.storage` be enforced? Is this legal? \r\n\r\nThe code below implements what I tried to describe, but does not print anything:\r\n```python\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    def process(self):\r\n        while True:\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```\r\nIf it is indeed acceptable to use ray, pyarrow, and zmq together as in the first example, I would like to proceed with that. Are there any glaring issues with doing so? In particular, ray will be used purely as an alternative to python multiprocessing. \r\n\r\nThank you so much again for your time.\r\n"
      },
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T21:17:35Z",
        "body": "It doesn't print anything because Actor (class with @ray.remote) is running in a single process, and `recv` will never run because `process` is occupying the process (because it is running a while loop). \r\n\r\nmutual exclusion of ReceiveSercer.storage be enforced? Is this legal?: Yes. Ray handles this issue and you never need to worry about locking. \r\n\r\nThere's nothing wrong with using zmq and pyarrow if you have the right reason. It is just not efficient because what you try to achieve using zmq and pyarrow is what Ray exists for. Ray is a distributed computing framework that abstracts inter-process communication problems (and many others).    \r\n\r\nYou can make this work in this way.  \r\n```python3\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\nimport asyncio\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    async def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    async def process(self):\r\n        while True:\r\n            await asyncio.sleep(0.0)\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```"
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-27T15:03:37Z",
        "body": "Thanks @rkooo567! This was very helpful. "
      },
      {
        "user": "uchiiii",
        "created_at": "2023-08-07T15:14:51Z",
        "body": "I am very new to ray-project and have a question regarding this.\r\n\r\nRay supports inter-process communication as suggested above. What kind of protocol is used under the hood, `zmq` or anything else? Or it shares data using object storage like Plasma? \r\n\r\nThank you for you reply in advance! "
      }
    ]
  },
  {
    "number": 8413,
    "title": "[sgd] Can TorchTrainer print out something every one or several iterations?",
    "created_at": "2020-05-12T08:36:16Z",
    "closed_at": "2020-06-11T20:23:53Z",
    "labels": [
      "question",
      "sgd"
    ],
    "url": "https://github.com/ray-project/ray/issues/8413",
    "body": "Seems by default TorchTrainer only returns stats after train() finishes? During the training, is there a way I get some information (for example loss values, or just something to indicate the training is happening in the background?) for each iteration or every several iterations?\r\nOtherwise if one epoch training takes a lot of time, then I probably don't know what's going on. I may doubt whether the program crashes indeed or the training is just long.\r\n\r\n@richardliaw \r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8413/comments",
    "author": "hkvision",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-05-14T08:36:26Z",
        "body": "You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use."
      },
      {
        "user": "hkvision",
        "created_at": "2020-06-09T12:35:33Z",
        "body": "> You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use.\r\n\r\nThank you so much! @richardliaw Sorry for the late reply. `use_tqdm` works great!\r\nIf I specify `num_steps`, then every call for `train` only trains several batches, and would it be the case that some data won't get trained?"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-06-11T20:23:53Z",
        "body": "Ah yes; there's a workaround but we should push this. I'll make a new PR."
      }
    ]
  },
  {
    "number": 7912,
    "title": "Details about the hyperparameter in PPO Algorithm?",
    "created_at": "2020-04-06T14:27:08Z",
    "closed_at": "2020-04-06T14:37:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7912",
    "body": "Hi, so I want to tune my hyperparameter for the PPO Algorithm but I've found difficulties when reading the docs about the configs, so I guess I want to ask you guys in here about:\r\n1. What is the value of `lr_schedule` in the PPO Algorithm? Suppose that my starting learning_rate is `'lr': 1e-4` and I want to decay its value to 0 when I train.\r\n2. Is it possible to set the hidden layer size in the PPO algorithm? If yes, what is the corresponding config as I didn't find it in the documentation (I found this kind of config in the SAC algorithm documentation but not in PPO).\r\n\r\nThank you very much guys! I really appreciate your help \ud83d\ude04 ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7912/comments",
    "author": "Nicholaz99",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-06T14:36:29Z",
        "body": "Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n1) You are basically configuring a PiecewiseSchedule.\r\nSo lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n2) You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\")."
      },
      {
        "user": "Nicholaz99",
        "created_at": "2020-04-06T14:47:52Z",
        "body": "Thank you so\r\n\r\n> Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n> \r\n> 1. You are basically configuring a PiecewiseSchedule.\r\n>    So lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n> 2. You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\").\r\n\r\nThank you so much for your help!!! It helps me a lot for my project \ud83d\ude04 "
      }
    ]
  },
  {
    "number": 7849,
    "title": "[rllib] Unable to configure exploration parameters in PPO: Unknown config parameter `explore`",
    "created_at": "2020-04-01T09:25:26Z",
    "closed_at": "2020-05-01T08:08:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7849",
    "body": "Hello,\r\n\r\nI am unable to configure exploration parameters when trying to create a PPO trainer. Dictionary entries \"explore\" and \"exploration_config\" is said to be unknown. Below are the relevant trainer definition and the traceback.\r\n\r\n`trainer = PPOTrainer(\r\n                      env=env_title,\r\n                      config={\r\n                          \r\n                        \"explore\": True,\r\n                        \"exploration_config\": {\r\n                          \"type\": \"EpsilonGreedy\",\r\n                          # Parameters for the Exploration class' constructor:\r\n                          # \"initial_epsilon\"=1.0,  # default is 1.0\r\n                          # \"final_epsilon\"=0.05,  # default is 0.05\r\n                          \"epsilon_timesteps\": max_steps,  # Timesteps over which to anneal epsilon, defult is int(1e5).\r\n                        },\r\n\r\n\r\n                        \"num_workers\": 5,\r\n                        \"num_gpus\": 2,\r\n                        \"model\": nw_model,\r\n                        \"multiagent\": {\r\n                          \"policy_graphs\": policy_graphs,\r\n                          \"policy_mapping_fn\": policy_mapping_fn,\r\n                          \"policies_to_train\": [\"ppo_policy{}\".format(i) for i in range(n_agents)],\r\n                        },\r\n                        \"callbacks\": {\r\n                          \"on_episode_start\": tune.function(on_episode_start),\r\n                          \"on_episode_step\": tune.function(on_episode_step),\r\n                          \"on_episode_end\": tune.function(on_episode_end),\r\n                        },\r\n                        \"log_level\": \"ERROR\",\r\n                      })`\r\n\r\n\r\nFull traceback:\r\n\r\n`Exception                                 Traceback (most recent call last)\r\n<ipython-input-9-252111d46b85> in <module>()\r\n    121                           \"on_episode_end\": tune.function(on_episode_end),\r\n    122                         },\r\n--> 123                         \"log_level\": \"ERROR\",\r\n    124                       })\r\n    125 \r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py in __init__(self, config, env, logger_creator)\r\n     88 \r\n     89         def __init__(self, config=None, env=None, logger_creator=None):\r\n---> 90             Trainer.__init__(self, config, env, logger_creator)\r\n     91 \r\n     92         def _init(self, config, env_creator):\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in __init__(self, config, env, logger_creator)\r\n    370             logger_creator = default_logger_creator\r\n    371 \r\n--> 372         Trainable.__init__(self, config, logger_creator)\r\n    373 \r\n    374     @classmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py in __init__(self, config, logger_creator)\r\n     94         self._restored = False\r\n     95         start_time = time.time()\r\n---> 96         self._setup(copy.deepcopy(self.config))\r\n     97         setup_time = time.time() - start_time\r\n     98         if setup_time > SETUP_TIME_THRESHOLD:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in _setup(self, config)\r\n    476         merged_config = deep_update(merged_config, config,\r\n    477                                     self._allow_unknown_configs,\r\n--> 478                                     self._allow_unknown_subkeys)\r\n    479         self.raw_user_config = config\r\n    480         self.config = merged_config\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/util.py in deep_update(original, new_dict, new_keys_allowed, whitelist)\r\n    158         if k not in original:\r\n    159             if not new_keys_allowed:\r\n--> 160                 raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n    161         if isinstance(original.get(k), dict):\r\n    162             if k in whitelist:\r\n\r\nException: Unknown config parameter `explore` `\r\n\r\n\r\n\r\nI am using Google Colab and Tensorflow 2.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7849/comments",
    "author": "ZekiDorukErden",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:39:56Z",
        "body": "Hi, you are probably on an older version of ray? What's your version number?\r\nFor now, try to remove these two keys (`exploration_config `and `explore`) altogether. You probably should not run PPO with EpsilonGreedy anyways."
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T09:52:13Z",
        "body": "Thanks for the reply! Apparently I am using version 0.8.0.dev5 (I copied the code block for dependencies in Ray with Google Colab tutorial without changing)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:59:33Z",
        "body": "Ok, cool. So it's working now?"
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T10:03:00Z",
        "body": "Yes, when I run without the exploration settings, it worked. Thanks for the help!"
      }
    ]
  },
  {
    "number": 7737,
    "title": "Why actor methods cannot be called directly?",
    "created_at": "2020-03-25T03:57:27Z",
    "closed_at": "2020-03-31T15:54:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7737",
    "body": "When calling a actor method, that is, call the `__call__` method of an `ActorMethod` object. And this method is implemented as raise an `Exception` directly\r\n```\r\nException: Actor methods cannot be called directly. Instead of running 'object.get()', try 'object.get.remote()'\r\n```\r\n\r\nBut why is it necessary? Why it can't be\r\n\r\n```python\r\nclass ActorMethod:\r\n    ...\r\n    def __call__(self, *args, **kwargs):\r\n        return ray.get(self._remote(args, kwargs))\r\n    ...\r\n```\r\n\r\nThen in some case, If do the following:\r\n```python\r\nclass Foo(object):\r\n    def foo(self):\r\n        return \"foo\"\r\n\r\nclass Bar(object):\r\n    def bar(self, foo_obj):\r\n        return foo_obj.foo()\r\n \r\nRayFoo = ray.remote(Foo)\r\nRayBar = ray.remote(Bar)\r\n\r\nif __name__ == \"__main__\":\r\n    f = Foo()\r\n    b = Bar()\r\n    print(b.bar(f))\r\n\r\n    ray.init(log_to_driver=False)\r\n    rf = RayFoo.remote()\r\n    rb = RayBar.remote()\r\n    print(rb.bar(rf))\r\n```\r\nwith the original `__call__` implementation, this is not possible, but with the proposed one, this works perfectly.\r\n\r\nIs there any design consideration?\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7737/comments",
    "author": "cloudhan",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-25T05:33:16Z",
        "body": "This is a design decision we made a couple years ago. The reason is to remain consistent across the API - tasks, methods, and class invocations.\r\n\r\nThe high level goal is to safeguard against user errors. I should note that commonly, new users often complain about the verbosity of this decision :) "
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T06:03:29Z",
        "body": "Are there any design pattern to walkaround the issue I mentioned, that is, what if I want to support both local and Ray decorated types. How to avoid implementing those types twice?"
      },
      {
        "user": "ericl",
        "created_at": "2020-03-25T07:59:50Z",
        "body": "You can do that with a wrapper class that automatically invokes .remote() under the hood, e.g., `h = Wrapper(handle)`."
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T11:16:55Z",
        "body": "Tried to hack a new decorator that replace the object constructor with a wrapper and then which replace the actor_method_obj.__call__ method with a new wrapper that return ray.get(actor_method_obj.<method_name>.remote()), too convoluted, will use @ericl 's wrapper.\r\n\r\nBTW, it is viable to add an option to allow this type of behavior, e.g.\r\n```python\r\n@ray.remote(allow_non_remote_calls=True)\r\nclass Foo(object): ...\r\n``` "
      }
    ]
  },
  {
    "number": 7490,
    "title": "[rllib] why the RolloutWorker uses the default config  everytime",
    "created_at": "2020-03-06T17:57:35Z",
    "closed_at": "2020-03-07T03:58:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7490",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay: 0.8.2\r\nPython: 3.6\r\nTF: 2.0\r\nOS: macOS Catalina\r\n\r\nI  create some RolloutWorker instances in our customized training flow, you can run the code.\r\n\r\n```python\r\nimport argparse\r\n\r\nimport ray\r\nimport gym\r\nimport copy\r\nimport random\r\nimport numpy as np\r\n\r\nfrom ray import tune\r\nfrom ray.rllib.utils import try_import_tf\r\n\r\nfrom ray.rllib.models import ModelCatalog\r\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\r\n\r\nfrom ray.rllib.models.tf.tf_action_dist import Categorical\r\nfrom ray.rllib.agents.pg.pg import PGTFPolicy\r\n\r\nfrom ray.rllib.evaluation import RolloutWorker\r\nfrom ray.rllib.evaluation.metrics import collect_metrics\r\nfrom ray.rllib.policy.sample_batch import SampleBatch\r\nfrom ray.rllib.policy.tests.test_policy import TestPolicy\r\nfrom ray.rllib.policy.tf_policy import TFPolicy\r\n\r\nfrom ray.rllib.offline import NoopOutput, IOContext, OutputWriter, InputReader\r\n\r\nfrom ray.rllib.agents.trainer import with_common_config\r\n\r\nfrom ray.rllib.evaluation.postprocessing import Postprocessing, compute_advantages\r\nfrom ray.rllib.policy.tf_policy_template import build_tf_policy\r\n\r\n\r\nfrom ray.rllib.models.tf.misc import normc_initializer, get_activation_fn\r\n\r\n\r\ntf = try_import_tf()\r\n\r\n\r\nclass CustomCategorical(Categorical):\r\n    def __init__(self, inputs, model=None, temperature=1.0):\r\n        \"\"\" The inputs are action logits \"\"\"\r\n        super().__init__(inputs, model, temperature)\r\n        self.softmax = tf.nn.softmax(inputs)\r\n\r\n    def prob(self, actions):\r\n        _prob_given_action = tf.one_hot(actions, depth=self.softmax.get_shape().as_list()[-1]) * self.softmax\r\n        return tf.reduce_sum(_prob_given_action, axis=-1)\r\n\r\n\r\nclass DemoModel(TFModelV2):\r\n    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\r\n        super(DemoModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\r\n\r\n        self.loss_inputs = [\r\n            ('taken_actions', tf.placeholder(tf.int32, (None,))),\r\n            ('returns', tf.placeholder(tf.float32, (None,)))\r\n        ]\r\n        self.ph_obs_input = tf.placeholder(tf.float32, (None,)+obs_space.shape)\r\n\r\n        self.ph_all_inputs = {k: v for _, (k, v) in enumerate(self.loss_inputs + [(\"obs\", self.ph_obs_input)])}\r\n        self.ph_all_inputs['prev_actions'] = tf.placeholder(tf.int32, (None,))\r\n        self.ph_all_inputs['prev_rewards'] = tf.placeholder(tf.float32, (None,))\r\n\r\n        inputs = tf.keras.layers.Input(\r\n            shape=(np.product(obs_space.shape), ))\r\n\r\n        _layer_out = tf.keras.layers.Dense(\r\n            128,\r\n            activation=tf.nn.tanh,\r\n            kernel_initializer=normc_initializer(1.0))(inputs)\r\n\r\n        self._layer_out = tf.keras.layers.Dense(\r\n            num_outputs,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(1.0))(_layer_out)\r\n\r\n        self._value_out = tf.keras.layers.Dense(\r\n            1,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(0.01))(_layer_out)\r\n\r\n        self.action_dist = CustomCategorical(self._layer_out, None)\r\n        self.predicted_actions = self.action_dist.sample()\r\n\r\n        # self.model and self.base_model are different\r\n        self.model = tf.keras.Model(inputs, [self._layer_out, self._value_out])\r\n        self.register_variables(self.model.variables)\r\n\r\n    def forward(self, input_dict, state, seq_lens):\r\n        model_out, self._value_out = self.model(input_dict[\"obs_flat\"])\r\n        return model_out, state\r\n\r\n    def custom_loss(self, policy_loss, loss_inputs):\r\n        return policy_loss\r\n\r\n    def from_batch(self, train_batch, is_training=True):\r\n        \"\"\"Convenience function that calls this model with a tensor batch.\r\n\r\n        All this does is unpack the tensor batch to call this model with the\r\n        right input dict, state, and seq len arguments.\r\n        \"\"\"\r\n\r\n        input_dict = {\r\n            \"obs\": train_batch[SampleBatch.CUR_OBS],\r\n            \"is_training\": is_training,\r\n        }\r\n        if SampleBatch.PREV_ACTIONS in train_batch:\r\n            input_dict[\"prev_actions\"] = train_batch[SampleBatch.PREV_ACTIONS]\r\n        if SampleBatch.PREV_REWARDS in train_batch:\r\n            input_dict[\"prev_rewards\"] = train_batch[SampleBatch.PREV_REWARDS]\r\n        states = []\r\n        i = 0\r\n        while \"state_in_{}\".format(i) in train_batch:\r\n            states.append(train_batch[\"state_in_{}\".format(i)])\r\n            i += 1\r\n        return self.__call__(input_dict, states, train_batch.get(\"seq_lens\"))\r\n\r\n    def value_function(self):\r\n        return tf.reshape(self._value_out, [-1])\r\n\r\n    def metrics(self):\r\n        \"\"\"Override to return custom metrics from your model.\r\n\r\n        The stats will be reported as part of the learner stats, i.e.,\r\n            info:\r\n                learner:\r\n                    model:\r\n                        key1: metric1\r\n                        key2: metric2\r\n\r\n        Returns:\r\n            Dict of string keys to scalar tensors.\r\n        \"\"\"\r\n        return {}\r\n\r\n\r\ndef pg_tf_loss(policy, model, dist_class, train_batch):\r\n    \"\"\"The basic policy gradients loss.\"\"\"\r\n    logits, _ = model.from_batch(train_batch)\r\n    action_dist = dist_class(logits, model)\r\n    return -tf.reduce_mean(\r\n        action_dist.logp(train_batch[SampleBatch.ACTIONS]) * train_batch[SampleBatch.REWARDS])\r\n\r\n\r\n# def post_process_advantages(policy,\r\n#                             sample_batch,\r\n#                             other_agent_batches=None,\r\n#                             episode=None):\r\n#     \"\"\"This adds the \"advantages\" column to the sample train_batch.\"\"\"\r\n#     return compute_advantages(\r\n#         sample_batch,\r\n#         0.0,\r\n#         policy.config[\"gamma\"],\r\n#         use_gae=False,\r\n#         use_critic=False)\r\n\r\n\r\nDEFAULT_CONFIG = with_common_config({\r\n    # # No remote workers by default.\r\n    # \"num_workers\": 0,\r\n    # # Learning rate.\r\n    # \"lr\": 0.0004,\r\n})\r\n\r\n\r\nCustomPolicy = build_tf_policy(\r\n    name=\"CustomPolicy\",\r\n    get_default_config=lambda: DEFAULT_CONFIG,\r\n    # postprocess_fn=post_process_advantages,\r\n    loss_fn=pg_tf_loss)\r\n\r\n\r\ndef set_variables(policy: CustomPolicy):\r\n    policy._variables = ray.experimental.tf_utils.TensorFlowVariables(\r\n        [], policy._sess, policy.variables())\r\n\r\n\r\ndef training_workflow(config, reporter):\r\n    sess = tf.Session()\r\n    env = gym.make(\"CartPole-v0\")\r\n\r\n    # policy = CustomPolicy(observation_space=env.observation_space, action_space=env.action_space, config=config,\r\n    #                       sess=sess, model=model, loss_inputs=model.loss_inputs, loss='Not None',\r\n    #                       action_sampler=model.predicted_actions, obs_input=model.ph_obs_input)\r\n\r\n    conf = {'config': config, 'sess': sess, 'model': model, 'loss_inputs': model.loss_inputs, 'loss': 'Not None',\r\n            'action_sampler': model.predicted_actions, 'obs_input': model.ph_obs_input}\r\n\r\n    policy = CustomPolicy(env.observation_space, env.action_space,\r\n                          config=config)  # , existing_inputs=model.ph_all_inputs)#, existing_model=model)\r\n    set_variables(policy)\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n\r\n    for i in range(config[\"num_iters\"]):\r\n        # Broadcast weights to the policy evaluation workers\r\n        weights = ray.put({\"default_policy\": policy.get_weights()})\r\n        for w in workers:\r\n            w.set_weights.remote(weights)\r\n\r\n        # Gather a batch of samples\r\n        T1 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Update the remote policy replicas and gather another batch of samples\r\n        # new_value = policy.get_weights()\r\n        # for w in workers:\r\n        #     w.for_policy.remote(lambda p: p.update_some_value(new_value))\r\n\r\n        # Gather another batch of samples\r\n        T2 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Improve the policy using the T1 batch\r\n        policy.learn_on_batch(T1)\r\n\r\n        # Do some arbitrary updates based on the T2 batch\r\n        # print(f'iter: {i}, sum_rewards: {(sum(T2[\"rewards\"])):.2f}')\r\n\r\n        reporter(**collect_metrics(remote_workers=workers))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--gpu\", action=\"store_true\")\r\n    parser.add_argument(\"--num-iters\", type=int, default=3)\r\n    parser.add_argument(\"--num-workers\", type=int, default=1)\r\n    parser.add_argument(\"--num-cpus\", type=int, default=0)\r\n\r\n    args = parser.parse_args()\r\n    ray.init(num_cpus=args.num_cpus or None)\r\n    ModelCatalog.register_custom_model(\"demo_model\", DemoModel)\r\n\r\n    tune.run(\r\n        training_workflow,\r\n        # resources_per_trial={\r\n        #     \"gpu\": 1 if args.gpu else 0,\r\n        #     \"cpu\": 1,\r\n        #     \"extra_cpu\": args.num_workers,\r\n        # },\r\n        config={\r\n            \"num_workers\": args.num_workers,\r\n            \"num_iters\": args.num_iters,\r\n            \"lr\": 1e-3,\r\n            \"model\": {\r\n                \"custom_model\": \"demo_model\",\r\n                \"max_seq_len\": 20,\r\n                \"custom_options\": {\r\n                    \"activation\": tf.nn.tanh,\r\n                }\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nIn\r\n```\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n```\r\nThe RolloutWorker creates a instance by using the default config not the config passed into the workflow. \r\n\r\nAre there any methods to fix it?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7490/comments",
    "author": "GoingMyWay",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-06T21:56:48Z",
        "body": "You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers."
      },
      {
        "user": "GoingMyWay",
        "created_at": "2020-03-07T02:34:41Z",
        "body": "> You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers.\r\n\r\nExactly. Thanks."
      }
    ]
  },
  {
    "number": 7467,
    "title": "[tune][rllib] _InactiveRpcError Deadline Exceeded",
    "created_at": "2020-03-05T16:32:58Z",
    "closed_at": "2020-04-22T17:22:40Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7467",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have VirtualBox running on Centos 7 and I am having trouble initializing Ray. After I run ray.init(), I get an _InactiveRpcError due to a Deadline Exceeded exception. What info should I provide in order to troubleshoot this error?\r\n\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray 0.8.2\r\nredis 3.4.1\r\nPython 3.6\r\nCentos 7 on VirtualBox",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7467/comments",
    "author": "Leonolovich",
    "comments": [
      {
        "user": "Leonolovich",
        "created_at": "2020-03-05T20:46:30Z",
        "body": "Running ray.init(local_mode=True) allows me to continue without errors to my tune.run() step, but I havent been able to resolve the Inactive Rcp Error. This is not ideal as I can only get one worker to perform training when using local_mode=True."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-04-22T02:36:49Z",
        "body": "Can you please try again on the latest Ray version?"
      },
      {
        "user": "Leonolovich",
        "created_at": "2020-04-22T17:22:39Z",
        "body": "That appears to have made the issue go away. For documentation, I was having the issue on version 0.8.2 or Ray and I no longer have the issue on version 0.8.4.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 7424,
    "title": "Actor method arguments",
    "created_at": "2020-03-03T19:27:19Z",
    "closed_at": "2020-03-03T21:06:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7424",
    "body": "Why do actor methods do not support passing arguments? There is an assertion that fails if the actor method function arguments are larger than 0.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7424/comments",
    "author": "commanderka",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-03T19:31:13Z",
        "body": "Can you please provide more context? i.e., a script and stack trace for reproducing this issue?"
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T20:20:46Z",
        "body": "I can only provide a code snippet, the problem is that there are too many dependencies. But I think its a more conceptual thing anyway.\r\n\r\n```\r\n@ray.remote(num_gpus=1)\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = bla\r\n        self.landmarkDetector = bla\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Improved)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    @ray.method\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/25/image_0409.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(currentActor,imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\ngives me \r\n\r\n> File \"testActors.py\", line 10, in <module>\r\n>     class PreprocessorActor(object):\r\n>   File \"testActors.py\", line 18, in PreprocessorActor\r\n>     @ray.method\r\n>   File \"/usr/local/lib/python3.6/dist-packages/ray/actor.py\", line 40, in method\r\n>     assert len(args) == 0\r\n> AssertionError\r\n\r\nThe idea is to have remote workers that are constantly fed with images to preprocess and to collect the preprocessed images. The problem is that the initialization of the preprocessing takes time, so I used the concept of Actors. Perhaps I have some conceptually wrong understanding, I dont know.\r\nRay version is 0.8.2"
      },
      {
        "user": "simon-mo",
        "created_at": "2020-03-03T20:26:05Z",
        "body": "`@ray.method` decorator is only there if you want to pass special parameters for a remote method, for example, [specifying the number of return values](@ray.method(num_return_vals=2)). By default, all methods for a `@ray.remote` actor can be called. \r\n\r\nYou can just remote the `@ray.method` decorator. "
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T21:06:27Z",
        "body": "Works like this now. I think the error was just misleading. I will close the issue. Nevertheless I would encourage to update the doku with some practical samples, perhaps also concerning the ActorPool class.\r\n\r\n```\r\n@ray.remote\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = something\r\n        self.landmarkDetector = something\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Relative)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\n\r\n\r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/20/image_0308.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 7394,
    "title": "Does DQN \"rollout.py\" have exploration turned off?",
    "created_at": "2020-03-02T03:57:53Z",
    "closed_at": "2020-03-02T10:34:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7394",
    "body": "When I call \"rollout.py\" I am not sure if exploration is turned off or not. I've looked over the file and can't seem to find `explore=False` anywhere.\r\n\r\nSo, when we evaluate trained policy (e.g. DQN) with rollout script - does it actually turn off random actions or not?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7394/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-02T04:08:14Z",
        "body": "I don't think it's actually turned off by default right now."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:01:53Z",
        "body": "The default config for DQN for evaluation is `exploration=False` (greedy action selection)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:03:53Z",
        "body": "However, in rollout.py, we do not use the evaluation_config, which is something, we should probably change."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:09:22Z",
        "body": "Then again, rollout.py picks up an already trained DQN, so its timesteps should already be past the epsilon exploration period, which then means it's (almost) not exploring anymore (if `final_epsilon` is 0.0, it won't explore at all). So for your specific DQN case, it should be fine (as in: not picking random actions anymore). What's your `exploration_config`?"
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T10:34:15Z",
        "body": "The above PR makes sure that rollout.py always uses the evaluation_config (which for DQN, is explore=False).\r\nIn the meantime, you can add `--config '{\"explore\": false}'` to your rollout.py command line to make sure, your algo picks only greedy acitons."
      },
      {
        "user": "drozzy",
        "created_at": "2020-03-02T13:07:44Z",
        "body": "Awesome."
      }
    ]
  },
  {
    "number": 7194,
    "title": "[rllib]PPO with action branching?",
    "created_at": "2020-02-17T13:42:11Z",
    "closed_at": "2020-02-18T05:29:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7194",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### PPO with action branching?\r\nRay version: 0.8.0\r\nTensorflow Version: 1.14.0\r\nOS: Ubuntu 18.04\r\n\r\nI'm currently working on training action branching agents with PPO. What else do I need to do besides set the action space to something like `gym.spaces.Tuple([gym.spaces.Discrete(3), gym.spaces.Discrete(5)])`, or I need to write a custom loss function? I was wondering if the gradients would be correct. ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7194/comments",
    "author": "jinbo-huang",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-17T18:57:55Z",
        "body": "Yeah that's all you need for PPO. The action will be automatically computed for the space."
      },
      {
        "user": "jinbo-huang",
        "created_at": "2020-02-18T05:29:02Z",
        "body": "Thank you for your answer. It helped a lot."
      }
    ]
  },
  {
    "number": 6986,
    "title": "[Question][rllib] Stochastic Game tensorboard separate rewards",
    "created_at": "2020-01-31T07:44:54Z",
    "closed_at": "2020-02-06T16:32:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6986",
    "body": "### What is your question?\r\n\r\nI am designing a simple stochastic game wherein I have two agents. The first agent (the good guy) is rewarded according to some task. The second agent (the adversary) is rewarded negative proportional to the first. This is to encourage the adversary to screw up the good guy.\r\n\r\nAs a first pass, I just set the reward of the adversary equal to negative the reward of the good guy. This seems to cause some issue with tensorboard, however, because it looks like the rewards are summed together, which results is a reward of 0 for each iteration.\r\n\r\nIt would be nice to be able to visualize the rewards of each agent individually. I imagine that this would be very useful for other MARL scenarios, not just SG. Is this something that is possible?\r\n\r\nThank you!\r\n\r\npython3.7\r\ntensorflow2.1\r\nray0.8.1\r\nmac10.14\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6986/comments",
    "author": "rusu24edward",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-01T01:43:53Z",
        "body": "Are you using separate policies for each agent? You can view the individual policy scores under the `policy_X_reward_mean` etc keys."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-03T21:58:22Z",
        "body": "I am using separate policies for each agent. I'm not sure what you mean by `policy_x_reward_mean` key. Is that something in the tensorboard interface?"
      },
      {
        "user": "ericl",
        "created_at": "2020-02-03T22:00:29Z",
        "body": "Yep, you should be able to find those in tensorboard, `result.json`, or printed to stdout if you use the `-v` flag."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-06T16:32:01Z",
        "body": "Nice! I found them. For me, they are a few pages in stored as `policy_reward_mean/<policy_name>`\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 4505,
    "title": "Config Does Not Accept Custom Parameters",
    "created_at": "2019-03-29T02:31:36Z",
    "closed_at": "2019-03-29T08:21:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/4505",
    "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.5\r\n- **Python version**: 3.7\r\n- **Exact command to reproduce**: \r\n`run_experiments({\r\n        \"test\": {\r\n            \"run\": my_trainable_func,\r\n            \"env\": multienv_name,\r\n            \"config\": {\r\n                \"multiagent\": {\r\n                    \"policy_graphs\": policy_graphs,\r\n                    \"policy_mapping_fn\": tune.function(lambda agent_id: f'agent_{agent_id}'),\r\n                },\r\n                \"num_iters\": 5\r\n            },\r\n            \"resources_per_trial\": {\r\n                \"cpu\": 2,\r\n                \"gpu\": 1,\r\n            },\r\n        }\r\n    })`\r\n\r\n### Describe the problem\r\nI am trying to include custom config parameters which my_trainable_func uses, but seem unable to add anything because I get an unknown config parameter error. As per Issue #3160, @ericl has mentioned that many config parameters have been deprecated, but I'm curious to know what the intended way of adding algorithm-specific hyperparameters into the config is.\r\n\r\n### Source code / logs\r\n`Traceback (most recent call last):\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 90, in run\r\n(pid=426)     self._entrypoint()\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 141, in entrypoint\r\n(pid=426)     return self._trainable_func(config, self._status_reporter)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 249, in _trainable_func\r\n(pid=426)     output = train_func(config, reporter)\r\n(pid=426)   File \"<ipython-input-13-0b4744367540>\", line 103, in fed_train\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py\", line 276, in __init__\r\n(pid=426)     Trainable.__init__(self, config, logger_creator)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\", line 88, in __init__\r\n(pid=426)     self._setup(copy.deepcopy(self.config))\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py\", line 364, in _setup\r\n(pid=426)     self._allow_unknown_subkeys)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/util.py\", line 89, in deep_update\r\n(pid=426)     raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n(pid=426) Exception: Unknown config parameter `num_iters` `\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/4505/comments",
    "author": "kiddyboots216",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-03-29T05:29:00Z",
        "body": "This is probably since you are subclassing agent? Agent checks that no unknown config keys are present, to avoid typos in your experiment config.\r\n\r\nIf you want to add a custom key, you can add it to the default config of your custom agent. You can check out any of the existing agent classes for an example of the config."
      },
      {
        "user": "kiddyboots216",
        "created_at": "2019-03-29T08:04:41Z",
        "body": "Ah, to be clear, I was wondering whether there was a way to do this without subclassing Agent ('my_trainable_func' I am trying to Tune a function and not a class). "
      },
      {
        "user": "ericl",
        "created_at": "2019-03-29T08:18:40Z",
        "body": "The exception you posted is originating from agent, so you must be calling agent code somehow."
      },
      {
        "user": "kiddyboots216",
        "created_at": "2019-03-29T08:21:01Z",
        "body": "Thanks Eric! Resolved; I was passing in the config (with custom parameters that were specific to my training function) into the Agent without removing the parameters beforehand. Now I know that I need to remove any parameters the Agent can't identify."
      }
    ]
  },
  {
    "number": 3785,
    "title": "Tune doesn't work with multi agent env",
    "created_at": "2019-01-15T19:50:26Z",
    "closed_at": "2019-01-16T17:55:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3785",
    "body": "<!--\r\nGeneral questions should be asked on the mailing list ray-dev@googlegroups.com.\r\n\r\nBefore submitting an issue, please fill out the following form.\r\n-->\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.1\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.1\r\n- **Python version**: 3.6.7\r\n- **Exact command to reproduce**:\r\n\r\n\r\n<!--\r\nYou can obtain the Ray version with\r\n\r\npython -c \"import ray; print(ray.__version__)\"\r\n-->\r\n\r\n### Describe the problem\r\nI am trying to use Tune in combination with RLlib to train with a custom multi agent environment. It works, when I am just using RLlib. But when I try to train using Tune i get `RecursionError: maximum recursion depth exceeded`. Does Tune currently support multi agent environments? Please find my code and the full stack trace here:\r\n\r\n### Source code / logs\r\n```python3\r\nimport ray\r\nimport ray.rllib.agents.ppo as ppo\r\nimport ray.tune as tune\r\nimport ray.tune.schedulers\r\nfrom ray.tune.logger import pretty_print\r\nfrom ray.tune.registry import register_env\r\nimport beer_distribution_game\r\n\r\ndef env_creator(env_config):\r\n    import gym\r\n    import beer_distribution_game\r\n    return beer_distribution_game.BeerDistributionGameV0()\r\n\r\ndef policy_mapper(agent_id):\r\n    return agent_id\r\n\r\nray.init(redis_address='localhost:6379')\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nspace_env = beer_distribution_game.BeerDistributionGameV0()\r\nspaces = space_env.get_spaces()\r\nsingle_config = {\r\n            'model' : {\r\n                'conv_filters' : None,\r\n                'fcnet_activation' : 'relu',\r\n                'fcnet_hiddens': [50, 100, 100]\r\n            },\r\n    'gamma': 0.7\r\n}\r\n\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nconfig = {\r\n    'beer-game-tune': {\r\n        'run': 'PPO',\r\n        'env': 'SimpleBeerGame',\r\n        'stop': {'episode_reward_mean' : -2000},\r\n        'config': {\r\n            'multiagent': {\r\n                'policy_mapping_fn': policy_mapper,\r\n                'policy_graphs': {\r\n                    'manufactorer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['manufactorer']['observation_space'], spaces['manufactorer']['action_space'], single_config),\r\n                    'distributor':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['distributor']['observation_space'], spaces['distributor']['action_space'], single_config),\r\n                    'supplier':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['supplier']['observation_space'], spaces['supplier']['action_space'], single_config),\r\n                    'retailer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['retailer']['observation_space'], spaces['retailer']['action_space'], single_config)\r\n                    },\r\n                'policies_to_train': [\r\n                    'manufactorer', 'distributor', 'supplier', 'retailer']\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nscheduler = ray.tune.schedulers.AsyncHyperBandScheduler(time_attr='training_iteration', reward_attr='episode_reward_mean', max_t=100)\r\n\r\ntrials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n```\r\n\r\n\r\nThis is the stack trace:\r\n```\r\n== Status ==\r\nUsing AsyncHyperBand: num_stopped=0\r\nBracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None\r\nBracket: Iter 90.000: None | Iter 30.000: None\r\nBracket: Iter 90.000: None\r\nResources requested: 0/16 CPUs, 0/0 GPUs\r\nMemory usage on this node: 1.7/16.3 GB\r\n\r\nDeprecation warning: Function values are ambiguous in Tune configuations. Either wrap the function with `tune.function(func)` to specify a function literal, or `tune.sample_from(func)` to tell Tune to sample values from the function during variant generation: <function policy_mapper at 0x7fa09014a8c8>\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-6-6efdb03a13a1> in <module>\r\n----> 1 trials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run_experiments(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\r\n    106     last_debug = 0\r\n    107     while not runner.is_finished():\r\n--> 108         runner.step()\r\n    109         if time.time() - last_debug > DEBUG_PRINT_INTERVAL:\r\n    110             logger.info(runner.debug_string())\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in step(self)\r\n    112             raise TuneError(\"Called step when all trials finished?\")\r\n    113         self.trial_executor.on_step_begin()\r\n--> 114         next_trial = self._get_next_trial()\r\n    115         if next_trial is not None:\r\n    116             self.trial_executor.start_trial(next_trial)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _get_next_trial(self)\r\n    252         trials_done = all(trial.is_finished() for trial in self._trials)\r\n    253         wait_for_trial = trials_done and not self._search_alg.is_finished()\r\n--> 254         self._update_trial_queue(blocking=wait_for_trial)\r\n    255         trial = self._scheduler_alg.choose_trial_to_run(self)\r\n    256         return trial\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _update_trial_queue(self, blocking, timeout)\r\n    362             timeout (int): Seconds before blocking times out.\r\n    363         \"\"\"\r\n--> 364         trials = self._search_alg.next_trials()\r\n    365         if blocking and not trials:\r\n    366             start = time.time()\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in next_trials(self)\r\n     48             trials (list): Returns a list of trials.\r\n     49         \"\"\"\r\n---> 50         trials = list(self._trial_generator)\r\n     51         self._finished = True\r\n     52         return trials\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in _generate_trials(self, unresolved_spec, output_path)\r\n     67             raise TuneError(\"Must specify `run` in {}\".format(unresolved_spec))\r\n     68         for _ in range(unresolved_spec.get(\"num_samples\", 1)):\r\n---> 69             for resolved_vars, spec in generate_variants(unresolved_spec):\r\n     70                 experiment_tag = str(self._counter)\r\n     71                 if resolved_vars:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in generate_variants(unresolved_spec)\r\n     40         \"cpu\": {\"eval\": \"spec.config.num_workers\"}\r\n     41     \"\"\"\r\n---> 42     for resolved_vars, spec in _generate_variants(unresolved_spec):\r\n     43         assert not _unresolved_values(spec)\r\n     44         yield format_vars(resolved_vars), spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    138     for resolved_spec in grid_search:\r\n    139         resolved_vars = _resolve_lambda_vars(resolved_spec, lambda_vars)\r\n--> 140         for resolved, spec in _generate_variants(resolved_spec):\r\n    141             for path, value in grid_vars:\r\n    142                 resolved_vars[path] = _get_value(spec, path)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    121 def _generate_variants(spec):\r\n    122     spec = copy.deepcopy(spec)\r\n--> 123     unresolved = _unresolved_values(spec)\r\n    124     if not unresolved:\r\n    125         yield {}, spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\n... last 1 frames repeated, from the frame below ...\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nThis is the whole config:\r\n```python\r\n{'beer-game-tune': {'config': {'multiagent': {'policies_to_train': ['manufactorer',\r\n                                                                    'distributor',\r\n                                                                    'supplier',\r\n                                                                    'retailer'],\r\n                                              'policy_graphs': {'distributor': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                Box(4,),\r\n                                                                                Discrete(20),\r\n                                                                                {'gamma': 0.7,\r\n                                                                                 'model': {'conv_filters': None,\r\n                                                                                           'fcnet_activation': 'relu',\r\n                                                                                           'fcnet_hiddens': [50,\r\n                                                                                                             100,\r\n                                                                                                             100]}}),\r\n                                                                'manufactorer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                 Box(4,),\r\n                                                                                 Discrete(20),\r\n                                                                                 {'gamma': 0.7,\r\n                                                                                  'model': {'conv_filters': None,\r\n                                                                                            'fcnet_activation': 'relu',\r\n                                                                                            'fcnet_hiddens': [50,\r\n                                                                                                              100,\r\n                                                                                                              100]}}),\r\n                                                                'retailer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}}),\r\n                                                                'supplier': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}})},\r\n                                              'policy_mapping_fn': <function policy_mapper at 0x7f804434a1e0>}},\r\n                    'env': 'SimpleBeerGame',\r\n                    'run': 'PPO',\r\n                    'stop': {'episode_reward_mean': -2000}}}\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3785/comments",
    "author": "MariusDanner",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-01-16T02:06:59Z",
        "body": "The issue is that tune is trying to expand lambda functions to generate trial variants. To fix that, you can 'escape' the policy mapper function with tune.function(func).\r\n\r\nThis is an unfortunate gotcha of the tune API, we should eventually raise an error on raw functions passed in the config."
      },
      {
        "user": "MariusDanner",
        "created_at": "2019-01-16T17:55:14Z",
        "body": "Thank you! Now it works"
      }
    ]
  },
  {
    "number": 3173,
    "title": "Issue running train.py can not locate lz4 or GPU",
    "created_at": "2018-10-31T19:32:10Z",
    "closed_at": "2018-11-01T06:06:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3173",
    "body": "When running the following (it does run succesfully but I get errors in setup on the gpu and lz4)\r\nRay does not find lz4 or my gpu\r\n.\r\n\r\n```\r\n===============================================================================================================================\r\n**********  stats  ************************\r\n\r\n\r\nUbuntu 16.04\r\n\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\n\r\n\r\n>>import ray; print(ray.__version__)\r\n0.5.3\r\n\r\n\r\n>>pip install lz4\r\nRequirement already satisfied: lz4 in /usr/lib/python2.7/dist-packages (0.7.0)\r\n\r\n\r\n>>lrwxrwxrwx  1 root root    10 Oct  1 22:50 cuda -> ./cuda-9.2\r\n\r\n>>nvidia-smi\r\n\r\nWed Oct 31 15:13:34 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1060    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   57C    P0    28W /  N/A |    365MiB /  6069MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1056      G   /usr/lib/xorg/Xorg                           262MiB |\r\n|    0      2508      G   compiz                                         7MiB |\r\n|    0      6924      G   ...uest-channel-token=14148272352303891226    92MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n=============================================================================================================\r\n\r\n\r\n\r\nsudo python /home/rjn/.local/lib/python2.7/site-packages/ray/rllib/train.py --env=Pong-ram-v4 --run=IMPALA \r\n\r\n\r\nProcess STDOUT and STDERR is being redirected to /tmp/raylogs/.\r\nWaiting for redis server at 127.0.0.1:27991 to respond...\r\nWaiting for redis server at 127.0.0.1:19620 to respond...\r\nStarting the Plasma object store with 6.00 GB memory.\r\nStarting local scheduler with the following resources: {'GPU': 1, 'CPU': 8}.\r\nFailed to start the UI, you may need to run 'pip install jupyter'.\r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\n\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nCreated LogSyncer for /home/rjn/ray_results/default/IMPALA_Pong-ram-v4_0_2018-10-31_15-27-354nWfrf -> \r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\nResources requested: 3/8 CPUs, 1/1 GPUs\r\nResult logdir: /home/rjn/ray_results/default\r\nRUNNING trials:\r\n - IMPALA_Pong-ram-v4_0:\tRUNNING\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:36.476907: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:36.551185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-31 15:27:36.551570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.50GiB\r\n2018-10-31 15:27:36.551584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-10-31 15:27:36.750449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-31 15:27:36.750495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-10-31 15:27:36.750501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-10-31 15:27:36.750697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5263 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:38.726462: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.728976: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.729023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729030: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.729080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.729086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\n2018-10-31 15:27:38.747833: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.750367: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.750429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750454: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.750584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.750590: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sampler.RolloutMetrics'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nResult for IMPALA_Pong-ram-v4_0:\r\n  date: 2018-10-31_15-27-47\r\n  done: false\r\n  episode_len_mean: 1290.9\r\n  episode_reward_max: -19.0\r\n  episode_reward_mean: -20.2\r\n  episode_reward_min: -21.0\r\n  episodes: 10\r\n  experiment_id: 16a7956cbdbf4bc78ef6f280ddc93142\r\n  hostname: rjn-Oryx-Pro\r\n  info:\r\n    learner:\r\n      cur_lr: 0.0005000000237487257\r\n      entropy: 862.072021484375\r\n      grad_gnorm: 40.0\r\n      policy_loss: -53.67061996459961\r\n      var_gnorm: 22.6600399017334\r\n      vf_explained_var: 0.07421219348907471\r\n      vf_loss: 30.673662185668945\r\n    num_steps_sampled: 14750\r\n    num_steps_trained: 14500\r\n    num_weight_syncs: 295\r\n    sample_throughput: 2048.296\r\n    train_throughput: 4096.592\r\n  iterations_since_restore: 1\r\n  node_ip: 192.168.1.100\r\n  pid: 7915\r\n  policy_reward_mean: {}\r\n  time_since_restore: 10.099857091903687\r\n  time_this_iter_s: 10.099857091903687\r\n  time_total_s: 10.099857091903687\r\n  timestamp: 1541014067\r\n  timesteps_since_restore: 14750\r\n  timesteps_this_iter: 14750\r\n  timesteps_total: 14750\r\n  training_iteration: 1\r\n\r\n\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3173/comments",
    "author": "rnunziata",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-11-01T00:09:15Z",
        "body": "> sudo python\r\n\r\nThis is almost certainly going to cause a different python environment to be used. Why not drop the sudo?"
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T01:03:36Z",
        "body": "removing the sudo did make a difference....same errors. Any thoughts on this. Maybe I will try to compile form source."
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T03:52:28Z",
        "body": "I can't think of a reason why lz4 can't be found if you can load it from a python interpreter.\r\n\r\nThough, not having that is probably fine on a single machine since no network transfers are happening."
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T04:38:08Z",
        "body": "ok...so you are saying I can at least ignore it.\r\nwhat about  it not findng cuda device.  Its a nivda GTX1060  and seem to be ok. Or am I not reading those errors correctly since it does run if I use Pong-ram-v0. "
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T05:05:06Z",
        "body": "Hm, the error might be expected on the workers since we force those to use CPUs. If you see GPU utilization then that should be fine."
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T05:50:44Z",
        "body": "I think that is it...there are two workers here and only two errors  I can not tell by the error line what task they belong to but I think it is probably good guess. I think the learner grabs the entire GPU memory since I  do not set any kind of growth or percentage variable. Thank you for your help."
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T06:06:29Z",
        "body": "Great!"
      }
    ]
  }
]