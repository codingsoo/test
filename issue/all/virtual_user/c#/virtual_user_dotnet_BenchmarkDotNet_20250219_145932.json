[
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/1891",
    "source": {
      "issue_number": 1891
    },
    "initial_question": {
      "title": "Reading result data via C# code",
      "body": "Hello there\r\n\r\nI've set up BenchmarkDotNet, but I would like to use it not via the default exporters and statistics, but more in the means of an integration-test. Unfortunately, I didn't see a way to get the actual results as just plain values via the provided API.\r\nIs this in fact not possible or even a good idea? The statistics and the printing are neat, but I wouldn't want to check the results manually.\r\nI've also tried to toy around with a  custom-exporter and basically make this one the test, but this seems also very complicated and doesn't look to be your intention."
    },
    "satisfaction_conditions": [
      "Benchmark results must be programmatically accessible",
      "Results must contain individual measurement data",
      "Data must be retrievable in a structured format",
      "Results must be obtainable directly after benchmark execution"
    ],
    "created_at": "2022-01-13T11:41:50Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/1346",
    "source": {
      "issue_number": 1346
    },
    "initial_question": {
      "title": "Benchmark Crashing on AssemblyCache in .NET Core 3.0.100",
      "body": "I've setup my environment before in .NET Framework and my benchmarks worked really great in the past! Kudos for this great lib!\r\n\r\nMy current issue now is that I'm trying to run benchmarks in a new project, this time, in .NET Core 3.0. After writing my last benchmark and executing them, I stumbled upon this error: \r\n\r\n```\r\nC:\\Program Files\\dotnet\\sdk\\3.0.100\\Microsoft.Common.CurrentVersion.targets(4563,5): error MSB3030: Could not copy the file \"C:\\Users\\Kevin.Avignon\\Documents\\Dev\\GitHub\\R.RC\\Robotmaster.CollectionRecommendation\\Robotmaster.CollectionRecommendation.Benchmarks\\bin\\Release\\netcoreapp3.0\\f66f5594-78e5-4bd4-a083-3d1539baf8a9\\obj\\Release\\netcoreapp3.0\\f66f5594-78e5-4bd4-a083-3d1539baf8a9.exe\" because it was not found. [C:\\Users\\Kevin.Avignon\\Documents\\Dev\\GitHub\\R.RC\\Robotmaster.CollectionRecommendation\\Robotmaster.CollectionRecommendation.Benchmarks\\bin\\Release\\netcoreapp3.0\\f66f5594-78e5-4bd4-a083-3d1539baf8a9\\BenchmarkDotNet.Autogenerated.csproj]\r\nC:\\Program Files\\dotnet\\sdk\\3.0.100\\Microsoft.Common.CurrentVersion.targets(2106,5): warning MSB3101: Could not write state file \"obj\\Release\\netcoreapp3.0\\BenchmarkDotNet.Autogenerated.csprojAssemblyReference.cache\". The file 'C:\\Users\\KavignonUserDir\\Documents\\Dev\\GitHub\\R.RC\\Project\\Project.Benchmarks\\bin\\Release\\netcoreapp3.0\\f66f5594-78e5-4bd4-a083-3d1539baf8a9\\obj\\Release\\netcoreapp3.0\\BenchmarkDotNet.Autogenerated.csprojAssemblyReference.cache' already exists. [C:\\Users\\KavignonUserDir\\Documents\\Dev\\GitHub\\R.RC\\Project\\Project.Benchmarks\\bin\\Release\\netcoreapp3.0\\f66f5594-78e5-4bd4-a083-3d1539baf8a9\\BenchmarkDotNet.Autogenerated.csproj]\r\n```\r\n\r\nI made sure of the following before running my benchmarks: \r\n- Release mode\r\n- Passing some arguments to the library: -m --allStats -f *\r\n- Unchecked the option \"Prefer 32 bit.\"\r\n- My benchmarks functions are public\r\n\r\nWhat's the catch here?"
    },
    "satisfaction_conditions": [
      "Benchmark execution environment must be properly configured"
    ],
    "created_at": "2020-01-08T00:01:09Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/1330",
    "source": {
      "issue_number": 1330
    },
    "initial_question": {
      "title": "How to join the results from multiple types?",
      "body": "Hi, \r\nI want merge benchmark results. \r\nI use \r\nBenchmarkDotNet version=\"0.12.0\" \r\ntargetFramework=\"net461\"\r\n\r\n` \r\n        \r\n            BenchmarkRunner.Run<MyBenchmark>(ManualConfig\r\n                    .Create(DefaultConfig.Instance)\r\n                    .With(ConfigOptions.JoinSummary)\r\n                    .With(ConfigOptions.DisableLogFile));\r\n\r\n            BenchmarkRunner.Run<MyBenchmark1>(ManualConfig\r\n                    .Create(DefaultConfig.Instance)\r\n                    .With(ConfigOptions.JoinSummary)\r\n                    .With(ConfigOptions.DisableLogFile));\r\n//-------------------------------------------------------------------------\r\n\r\n    public class MyBenchmark\r\n    {\r\n        [Benchmark]\r\n        public void Fn1() => Thread.Sleep(10);\r\n\r\n        [Benchmark]\r\n        public void Fn2() => Thread.Sleep(10);\r\n    }\r\n\r\n    public class MyBenchmark1\r\n    {\r\n        [Benchmark]\r\n        public void Fn__1() => Thread.Sleep(10);\r\n\r\n        [Benchmark]\r\n        public void Fn__2() => Thread.Sleep(10);\r\n    }\r\n`\r\n\r\nbut did'nt obtain merged results in  BenchmarkDotNet.Artifacts . That is correct way to merge benchmarks results?"
    },
    "satisfaction_conditions": [
      "Multiple benchmark results must appear in a single combined output",
      "All specified benchmark classes must be executed in a single BenchmarkRunner operation",
      "Configuration settings must be consistently applied across all benchmarks",
      "Results must be properly stored in BenchmarkDotNet.Artifacts"
    ],
    "created_at": "2019-12-13T21:50:28Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/1272",
    "source": {
      "issue_number": 1272
    },
    "initial_question": {
      "title": "ArgumentsSource for large complex type",
      "body": "Hi,\r\n\r\nI want to generate a large amount of test input data to benchmark my query.\r\nBasically I will combine predefined params into a large amount of test input:\r\n\r\n` public IEnumerable<TestParam> Data() `\r\n`{`\r\n            `  var dimensions = new string[] { \"A\", \"B\" };`\r\n           `   var kpis = new string[] { \"C\" };`\r\n            `  parameters = CombineArray.generateParams(dimensions, kpis);`\r\n           `   return parameters.AsEnumerable();`\r\n`}`\r\n\r\nThen use it as argumentSource of my benchmark ` [ArgumentsSource(nameof(Data))]`\r\nThe problem is eveytime benchmark method run, it again executes Data method, which can be slow if I try to put large amount of dimensions and kpis, it can generate million of test input.\r\n\r\nI think I can put it on `GlobalSetup`, but don't know how to setup benchmark method to run all of the test input there.\r\nPlease help,\r\nThank you!!!"
    },
    "satisfaction_conditions": [
      "Test input data generation occurs only once per benchmark configuration",
      "Generated test data persists across benchmark method invocations",
      "Maintains correct parameter combinations for benchmarking",
      "Compatible with cross-process benchmark execution"
    ],
    "created_at": "2019-10-04T01:42:16Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/1140",
    "source": {
      "issue_number": 1140
    },
    "initial_question": {
      "title": "Targeting .NET 4.6.2 but executing as 4.7.2",
      "body": "I have a small benchmark project that is targeting netcoreapp2 and net462.\r\n\r\n```\r\n<Project Sdk=\"Microsoft.NET.Sdk\">\r\n\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFrameworks>netcoreapp2;net462</TargetFrameworks>\r\n  </PropertyGroup>\r\n\r\n  <ItemGroup>\r\n    <PackageReference Include=\"BenchmarkDotNet\" Version=\"0.11.5\" />\r\n  </ItemGroup>\r\n\r\n</Project>\r\n```\r\n\r\n\r\nHowever when running this benchmark it only seems to respect the netcoreapp target and not the full framework one. It states that the CLR job was run targeting net472. Could I be doing this wrong or is this an issue? I have targeted only two jobs [CoreJob, ClrJob].\r\n\r\n>   [Host] : .NET Core 2.0.9 (CoreCLR 4.6.26614.01, CoreFX 4.6.26614.01), 64bit RyuJIT\r\n>   Clr    : .NET Framework 4.7.2 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.3394.0\r\n>   Core   : .NET Core 2.0.9 (CoreCLR 4.6.26614.01, CoreFX 4.6.26614.01), 64bit RyuJIT\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Application executes using the highest installed .NET Framework runtime version",
      "Benchmark results accurately report the actual runtime version being used",
      "Target framework specification in project file remains respected for compilation"
    ],
    "created_at": "2019-04-28T17:57:36Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/804",
    "source": {
      "issue_number": 804
    },
    "initial_question": {
      "title": "What is the point of BuildScriptFilePath ?",
      "body": "I'm in the process of attempting to implement #290 and noticed that the Generators create a `.bat` build script which seems quite handy, but then the Builders don't actually use the build script generated.\r\n\r\nFor example the `DotNetCliBuilder` runs it's own `restore` and `build` commands and doesn't use the generated build script to do the building.  The Roslyn Builder outputs a log entry for the `BuildScriptFilePath` but then doesn't use it at all.\r\n\r\nThis might be intentional, so sorry if I'm being naive (the solution is quite a learning curve :) \r\n\r\n"
    },
    "satisfaction_conditions": [
      "Explanation clarifies the current purpose of BuildScriptFilePath",
      "Historical context is provided for the feature's evolution",
      "Current practical use case is identified",
      "Clear guidance for implementers is provided"
    ],
    "created_at": "2018-06-25T05:23:10Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/775",
    "source": {
      "issue_number": 775
    },
    "initial_question": {
      "title": "Few questions related to understanding of results",
      "body": "### 1. What is a difference between DryCore and Core?\r\nResult in Core:   0.2182 ns\r\nResult in DryCore: 647,800.0000 ns\r\n\r\n### 2. What is the meaning of histograms?\r\n\r\n[-0.002 ns ; 0.074 ns) | @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n[ 0.074 ns ; 0.178 ns) | @@@@@@@@@@@@@@@@@@\r\n[ 0.178 ns ; 0.280 ns) | @@@@@@@@\r\n[ 0.280 ns ; 0.355 ns) | @@@@@@@@@@@@@@@@@\r\n[ 0.355 ns ; 0.444 ns) | @@@@@@@@\r\n[ 0.444 ns ; 0.528 ns) | @@@\r\n[ 0.528 ns ; 0.604 ns) | @@@@@\r\n[ 0.604 ns ; 0.651 ns) |\r\n[ 0.651 ns ; 0.727 ns) | @@\r\n[ 0.727 ns ; 0.797 ns) |\r\n[ 0.797 ns ; 0.873 ns) | @\r\n"
    },
    "satisfaction_conditions": [
      "Explanation clearly distinguishes between DryCore and Core benchmark execution behaviors",
      "Explains the histogram visualization format",
      "Provides interpretation of histogram data points",
      "Explains the quantitative meaning of the @ symbols"
    ],
    "created_at": "2018-06-04T20:05:03Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/708",
    "source": {
      "issue_number": 708
    },
    "initial_question": {
      "title": "Error of DisassemblyDiagnoser(printAsm: true, printSource: true) in MultiTarget project",
      "body": "I have a multitarget project with windows diagnoser enabler for NET47:\r\n\r\n```\r\n<Project Sdk=\"Microsoft.NET.Sdk\">\r\n\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFrameworks>netcoreapp2.0;net47</TargetFrameworks>\r\n\t<LangVersion>latest</LangVersion>\r\n  </PropertyGroup>\r\n\r\n  <ItemGroup>\r\n    <PackageReference Include=\"BenchmarkDotNet\" Version=\"0.10.13\" />\r\n    \r\n    <PackageReference Include=\"DynamicExpresso.Core\" Version=\"2.0.0\" />\r\n    <PackageReference Include=\"Newtonsoft.Json\" Version=\"11.0.2\" />\r\n  </ItemGroup>\r\n  \r\n    <ItemGroup Condition=\"'$(TargetFramework)' == 'net47'\">\r\n\t\t<PackageReference Include=\"BenchmarkDotNet.Diagnostics.Windows\" Version=\"0.10.13\" />\r\n\t</ItemGroup>\r\n\r\n  <ItemGroup>\r\n    <ProjectReference Include=\"..\\..\\Routines\\Routines.csproj\" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nDisassembler configuration enabled on conditional compiling:\r\n\r\n```\r\n#if !NETCOREAPP2_0\r\n    [DisassemblyDiagnoser(printAsm: true, printSource: true)]\r\n#endif\r\n    public class BenchmarkConverAll\r\n    { \r\n\r\n```\r\nInlining diagnoser enabled that way works, when dissasembly diagnoser returns an error:\r\n\r\nTest executed as \r\n& dotnet run -c Release -f net47 -p \"$BenchmarkProjectPath\"\r\n\r\n\r\n```\r\nPS D:\\cot\\DashboardCode\\Routines> D:\\cot\\DashboardCode\\Routines\\BenchmarkWindows.ps1\r\nMicrosoft (R) Build Engine version 15.6.82.30579 for .NET Core\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Restore completed in 64.03 ms for D:\\cot\\DashboardCode\\Routines\\Routines\\Routines.csproj.\r\n  Restore completed in 64.46 ms for D:\\cot\\DashboardCode\\Routines\\Tests\\Benchmark\\Benchmark.csproj.\r\n  Routines -> D:\\cot\\DashboardCode\\Routines\\Routines\\bin\\Release\\netstandard2.0\\DashboardCode.Routines.dll\r\n  Benchmark -> D:\\cot\\DashboardCode\\Routines\\Tests\\Benchmark\\bin\\Release\\netcoreapp2.0\\Benchmark.dll\r\n  Benchmark -> D:\\cot\\DashboardCode\\Routines\\Tests\\Benchmark\\bin\\Release\\net47\\Benchmark.exe\r\n\r\nBuild succeeded.\r\n    0 Warning(s)\r\n    0 Error(s)\r\n\r\nTime Elapsed 00:00:05.85\r\nMicrosoft (R) Build Engine version 15.6.82.30579 for .NET Core\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Restore completed in 104.64 ms for D:\\cot\\DashboardCode\\Routines\\Routines\\Routines.csproj.\r\n  Restore completed in 121.11 ms for D:\\cot\\DashboardCode\\Routines\\Tests\\Benchmark\\Benchmark.csproj.\r\n// ***** BenchmarkRunner: Start   *****\r\n// Found benchmarks:\r\n//   BenchmarkConverAll.TestConverAll: RyuJitX64(Jit=RyuJit, Platform=X64)\r\n\r\n// Validating benchmarks:\r\n// ***** Building 1 benchmark(s) in Parallel: Start   *****\r\n// ***** Done, took 00:00:05 (5.29 sec)   *****\r\n// **************************\r\n// Benchmark: BenchmarkConverAll.TestConverAll: RyuJitX64(Jit=RyuJit, Platform=X64)\r\n// *** Execute ***\r\n// Launch: 1 / 1\r\n// Execute: D:\\cot\\DashboardCode\\Routines\\Tests\\Benchmark\\bin\\Release\\net47\\74d4fd8a-270d-46ab-bde8-907d0dfaa814.exe diagnoserAttached\r\n// BeforeAnythingElse\r\n\r\n// Benchmark Process Environment Information:\r\n// Runtime=.NET Framework 4.7 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.2633.0\r\n// GC=Concurrent Workstation\r\n// Job: RyuJitX64(Jit=RyuJit, Platform=X64)\r\n\r\nPilot  1: 16 op, 3459366.51 ns, 216.2104 us/op\r\n\r\n..............................\r\n\r\n// AfterAll\r\n\\ ---------------------------\r\nFailed to read source code location!\r\nPlease make sure that the project, which defines benchmarks contains following settings:\r\n\t <DebugType>pdbonly</DebugType>\r\n\t <DebugSymbols>true</DebugSymbols>\r\n\\ ---------------------------\r\ndotnet.exe : \r\nAt D:\\cot\\DashboardCode\\Routines\\BenchmarkWindows.ps1:17 char:1\r\n+ & dotnet run -c Release -f net47 -p \"$BenchmarkProjectPath\"\r\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    + CategoryInfo          : NotSpecified: (:String) [], RemoteException\r\n    + FullyQualifiedErrorId : NativeCommandError\r\n \r\nUnhandled Exception: \r\nSystem.InvalidOperationException: There is an error in XML document (0, 0). ---> System.Xml.XmlException: Root element is missing.\r\n   at System.Xml.XmlTextReaderImpl.Throw(Exception e)\r\n   at System.Xml.XmlTextReaderImpl.ParseDocumentContent()\r\n   at System.Xml.XmlReader.MoveToContent()\r\n   at Microsoft.Xml.Serialization.GeneratedAssembly.XmlSerializationReaderDisassemblyResult.Read9_DisassemblyResult()\r\n   --- End of inner exception stack trace ---\r\n   at System.Xml.Serialization.XmlSerializer.Deserialize(XmlReader xmlReader, String encodingStyle, XmlDeserializationEvents events)\r\n   at BenchmarkDotNet.Diagnosers.WindowsDisassembler.Dissasemble(DiagnoserActionParameters parameters)\r\n   at BenchmarkDotNet.Diagnosers.DisassemblyDiagnoser.Handle(HostSignal signal, DiagnoserActionParameters parameters)\r\n   at BenchmarkDotNet.Extensions.CommonExtensions.ForEach[T](IList`1 source, Action`1 command)\r\n   at BenchmarkDotNet.Loggers.SynchronousProcessOutputLoggerWithDiagnoser.ProcessInput()\r\n   at BenchmarkDotNet.Toolchains.Executor.Execute(Process process, Benchmark benchmark, SynchronousProcessOutputLoggerWithDiagnoser loggerWithDiagnoser, ILogger logger)\r\n   at BenchmarkDotNet.Toolchains.Executor.Execute(Benchmark benchmark, ILogger logger, String exePath, String workingDirectory, String args, IDiagnoser diagnoser, IResolver \r\nresolver, IConfig config)\r\n   at BenchmarkDotNet.Toolchains.Executor.Execute(ExecuteParameters executeParameters)\r\n   at BenchmarkDotNet.Running.BenchmarkRunnerCore.Execute(ILogger logger, Benchmark benchmark, IToolchain toolchain, BuildResult buildResult, IConfig config, IResolver \r\nresolver)\r\n   at BenchmarkDotNet.Running.BenchmarkRunnerCore.RunCore(Benchmark benchmark, ILogger logger, ReadOnlyConfig config, String rootArtifactsFolderPath, Func`2 \r\ntoolchainProvider, IResolver resolver, BuildResult buildResult)\r\n   at BenchmarkDotNet.Running.BenchmarkRunnerCore.Run(BenchmarkRunInfo benchmarkRunInfo, ILogger logger, String title, String rootArtifactsFolderPath, Func`2 \r\ntoolchainProvider, IResolver resolver, List`1 artifactsToCleanup)\r\n   at BenchmarkDotNet.Running.BenchmarkRunnerCore.Run(BenchmarkRunInfo benchmarkRunInfo, Func`2 toolchainProvider)\r\n   at Benchmark.Program.Main(String[] args)\r\n```"
    },
    "satisfaction_conditions": [
      "Configuration must be compatible with multi-target project structure"
    ],
    "created_at": "2018-03-31T21:56:58Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/668",
    "source": {
      "issue_number": 668
    },
    "initial_question": {
      "title": "Can ParamsSource be evaluated after GlobalSetup?",
      "body": "I have to initialize several objects that are generating parameters. Therefore my idea was to initialize these objects in the GlobalSetup area. \r\n\r\n        private IProjectInfo _generatedProject1;\r\n        private IProjectInfo _generatedProject2;\r\n\r\n        [GlobalSetup]\r\n        public void BenchmarkSetup()\r\n        {\r\n            var loader = new ProjectLoader();\r\n            _generatedProject1 = loader.GetSample();\r\n            _generatedProject2 = loader.GetBigSample();\r\n        }\r\n\r\n        [ParamsSource(nameof(Projects))]\r\n        public IProjectInfo Project;\r\n\r\n        public IEnumerable<IParam> Projects()\r\n        {\r\n            yield return new ProjectParameter { Project = _generatedProject1};\r\n            yield return new ProjectParameter { Project = _generatedProject2};\r\n        }\r\n\r\n        [Benchmark]\r\n        public IProject OpenProject()\r\n        {\r\n            var project = OpenProject(Project);\r\n            return project;\r\n        }\r\n\r\n        public class ProjectParameter : IParam\r\n        {\r\n            public IProjectInfo Project { get; set; }\r\n            public string ToSourceCode() => \"\";\r\n            public object Value => Project;\r\n            public string DisplayText => Project.Name;\r\n        }\r\n\r\nBut unfortunately the parameters are built before, so I get an exception. Is there any workaround?"
    },
    "satisfaction_conditions": [
      "Project data must be accessible during benchmark execution",
      "Parameter values must remain consistent across process boundaries",
      "Parameter generation must not depend on data created in GlobalSetup"
    ],
    "created_at": "2018-02-27T14:28:15Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/656",
    "source": {
      "issue_number": 656
    },
    "initial_question": {
      "title": "Question: is it possible to access method, mean and scale programmatically once the benchmarks have ran?",
      "body": "Is it possible to access method, mean and scale programmatically once the benchmarks have ran?"
    },
    "satisfaction_conditions": [
      "Benchmark results must be programmatically accessible after execution",
      "Statistical metrics (mean, scale) must be retrievable from the benchmark output",
      "Results must be accessible within the same program execution context as the benchmark run",
      "Retrieved data must be suitable for automated pass/fail determination"
    ],
    "created_at": "2018-02-22T16:43:55Z"
  },
  {
    "id": "https://github.com/dotnet/BenchmarkDotNet/issues/323",
    "source": {
      "issue_number": 323
    },
    "initial_question": {
      "title": "Dead code elimination - is this safe?",
      "body": "I have a benchmark like so:\r\n\r\n```cs\r\n[Params(1, 10, 100, 500, 1000, 10000)]\r\npublic int NumberOfMatches { get; set; }\r\n\r\n[Benchmark]\r\npublic List<bool> DotNetGlobIsMatch()\r\n{\r\n    // we collect all results in a list \r\n    // and return it to prevent dead code elimination (optimisation)\r\n    var results = new List<bool>(NumberOfMatches);\r\n    for (int i = 0; i < NumberOfMatches; i++)\r\n    {\r\n        var testString = _testData[i];\r\n        var result = _dotnetGlob.IsMatch(testString);\r\n        results.Add(result);\r\n    }\r\n    return results;\r\n}\r\n```\r\n\r\nI would use `[OperationsPerInvoke]` except it's not dynamic, so I use  `[Params(1, 10, 100, 500, 1000, 10000)]` instead.\r\n\r\nMy question is, I don't really like having to construct and return a list in my Benchmark method, however I need to ensure that something is done with each result in the iteration so that dead code elimination doesn't remove it.\r\n\r\nIs this safe to do instead?\r\n\r\n```cs\r\n[Benchmark]\r\npublic IEnumerable<bool> DotNetGlobIsMatch()\r\n{\r\n    for (int i = 0; i < NumberOfMatches; i++)\r\n    {\r\n        var testString = _testData[i];\r\n        var result = _dotnetGlob.IsMatch(testString);\r\n        yield return result;\r\n    }\r\n}\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Results of each iteration must be preserved and not optimized away by the compiler",
      "Minimal overhead from result handling",
      "Supports variable number of iterations defined by NumberOfMatches parameter",
      "Maintains deterministic behavior across multiple benchmark runs"
    ],
    "created_at": "2016-12-10T17:12:36Z"
  }
]