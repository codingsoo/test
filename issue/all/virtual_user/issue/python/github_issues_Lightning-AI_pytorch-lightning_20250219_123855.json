[
  {
    "number": 20086,
    "title": "module statistics has no attribute mean",
    "created_at": "2024-07-15T07:18:01Z",
    "closed_at": "2024-07-15T11:51:42Z",
    "labels": [
      "question",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20086",
    "body": "### Bug description\r\n\r\nWhen updating to the new PyTorch-lightning version 2.3.3 and using the MlFlowLogger as logger arg in the trainer I\u2018m getting the error trace (see error section)\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nmaster\r\n\r\n### How to reproduce the bug\r\n\r\n_No response_\r\n\r\n### Error messages and logs\r\n\r\n```\r\ntrain.py 9 <module>\r\nimport mlflow.pytorch\r\n \r\n__init__.py 1190 <module>\r\nfrom mlflow.pytorch._lightning_autolog import MlflowModelCheckpointCallback  # noqa: F401\r\n \r\n_lightning_autolog.py 24 <module>\r\nimport pytorch_lightning as pl\r\n \r\n__init__.py 27 <module>\r\nfrom pytorch_lightning.callbacks import Callback  # noqa: E402\r\n \r\n__init__.py 29 <module>\r\nfrom pytorch_lightning.callbacks.pruning import ModelPruning\r\n \r\npruning.py 32 <module>\r\nfrom pytorch_lightning.core.module import LightningModule\r\n \r\n__init__.py 16 <module>\r\nfrom pytorch_lightning.core.module import LightningModule\r\n \r\nmodule.py 62 <module>\r\nfrom pytorch_lightning.loggers import Logger\r\n \r\n__init__.py 14 <module>\r\nfrom pytorch_lightning.loggers.comet import CometLogger\r\n \r\ncomet.py 30 <module>\r\nfrom pytorch_lightning.loggers.logger import Logger, rank_zero_experiment\r\n \r\nlogger.py 103 <module>\r\ndefault_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n \r\nAttributeError:\r\nmodule 'statistics' has no attribute 'mean'\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.3.3\r\n#- PyTorch Version (e.g., 2.0): 2.3.1\r\n#- Python version (e.g., 3.9): 3.11\r\n#- OS (e.g., Linux): MacOS Sonoma 14.5\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20086/comments",
    "author": "FabianKuon",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-07-15T10:28:22Z",
        "body": "Hey @FabianKuon \r\nThe statistics module is a standard module in Python and it definitely has a mean function. Could you please check that you don't have a different statistics module in the python path? For example, if you have a `statistics.py` module in your code, it would cause a collision with the standard library package. In this case, please rename or delete it. "
      },
      {
        "user": "FabianKuon",
        "created_at": "2024-07-15T11:51:42Z",
        "body": "@awaelchli thank you very much for your quick response. You\u2018re right I do have a statistics.py file in my repo. Renaming that solved the issue. \r\n\r\nOnce again thank you very much "
      }
    ]
  },
  {
    "number": 20020,
    "title": "Teardown trying to copy \"meta\" tensors",
    "created_at": "2024-06-27T08:19:33Z",
    "closed_at": "2024-06-27T21:48:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20020",
    "body": "### Bug description\n\nI've got a model template that I'm using with torch.vmap, and to use it I need to store a meta model in my lightning module. However lightning keeps trying to copy the meta model to devices / during teardown etc... This results in an error for the meta tensor, since it does not have a copy method. Any good way to work around this?\r\n\r\nThis code generates my MLP's template, and during any lightning method that copies things, it dies when ```self.base_model.copy()``` or ```self.base_model.to(\"device\")``` is called.\r\n\r\n```python\r\nself.base_model = copy.deepcopy(self.mlp)\r\nself.base_model.to('meta')\r\n```\n\n### What version are you seeing the problem on?\n\nv2.2\n\n### How to reproduce the bug\n\n```python\nRun any lightning model with a meta model as a lightning property\r\n\r\n\r\nself.base_model = copy.deepcopy(self.mlp)\r\nself.base_model.to('meta')\n```\n\n\n### Error messages and logs\n\n```\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.2\r\n#- Lightning App Version (e.g., 0.5.2): \r\n#- PyTorch Version (e.g., 2.0): 2.2\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): Windows\r\n#- CUDA/cuDNN version: 12.1\r\n#- GPU models and configuration: 3090\r\n#- How you installed Lightning(`conda`, `pip`, source): conda\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20020/comments",
    "author": "kvndhrty",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-06-27T18:35:26Z",
        "body": "Hey @kvndhrty \r\nI think a pretty easy way to work around this is to not register your meta-template model as a submodule. You can easily do that by packing it into a list:\r\n```py\r\ndef __init__(self):\r\n    super().__init__()\r\n    with torch.device(\"meta\"):\r\n        self._template_model = [TemplateModel()]\r\n        \r\n    # then access it like so in your other code: \r\n    self._template_model[0]\r\n    \r\n    # ... or write a getter to return you the template model without indexing\r\n```\r\n\r\nI think that the assumption Lightning makes about your model not being on the meta device after training is a reasonable one. Even so before training, since eventually Lightning moves the model to GPU before training. I think it would become quite complex if we had to add logic to ignore such submodules on the meta-device. More so, it would be error-prone, because meta-device initialization is needed for large model training.\r\nSo I would like to suggest we don't treat this as a bug.\r\n\r\nOne other thing you could do is ask yourself whether it is even necessary to have your template model as an attribute at all. Since the creation on meta-device is basically free, you could also just do that on-the-fly whenever you need that. Get the properties you need and store them somewhere. Then you don't need to keep that template model around.\r\n    "
      }
    ]
  },
  {
    "number": 18975,
    "title": "Training a simple XOR network yields incorrect, undeterministic behaviour",
    "created_at": "2023-11-09T10:17:57Z",
    "closed_at": "2023-11-10T12:10:01Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18975",
    "body": "### Bug description\n\nHi, I am trying to train a simple DNN to solve the XOR problem. This can be trivially solved with a pure torch implementation. I cannot replicate the same simple model in lightning. Instead the trained model oscillates between different states, never managing to correctly produce XOR.\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\n# import libraries\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass XOR(nn.Module):\r\n    def __init__(self):\r\n        super(XOR, self).__init__()\r\n        self.linear_sigmoid_stack = nn.Sequential(\r\n            nn.Linear(2, 2),\r\n            nn.Sigmoid(),\r\n            nn.Linear(2, 1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        return self.linear_sigmoid_stack(x)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # create data\r\n    Xs = torch.Tensor([[0., 0.],\r\n                       [0., 1.],\r\n                       [1., 0.],\r\n                       [1., 1.]])\r\n\r\n    y = torch.Tensor([0., 1., 1., 0.]).reshape(Xs.shape[0], 1)\r\n\r\n    xor_network = XOR()\r\n\r\n    epochs = 1000\r\n    mseloss = nn.MSELoss()\r\n    optimizer = torch.optim.Adam(xor_network.parameters(), lr=0.03)\r\n    all_losses = []\r\n    current_loss = 0\r\n    plot_every = 50\r\n\r\n    for epoch in range(epochs):\r\n\r\n        # input training example and return the prediction\r\n        yhat = xor_network.forward(Xs)\r\n\r\n        # calculate MSE loss\r\n        loss = mseloss(yhat, y)\r\n\r\n        # backpropogate through the loss gradiants\r\n        loss.backward()\r\n\r\n        # update model weights\r\n        optimizer.step()\r\n\r\n        # remove current gradients for next iteration\r\n        optimizer.zero_grad()\r\n\r\n        # append to loss\r\n        current_loss += loss\r\n        if epoch % plot_every == 0:\r\n            all_losses.append(current_loss / plot_every)\r\n            current_loss = 0\r\n\r\n        # print progress\r\n        if epoch % 500 == 0:\r\n            print(f'Epoch: {epoch} completed')\r\n```\r\n\r\nI tried to use Lightning to simplify away the boilerplate code like so:\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nimport lightning as L\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\n\r\n\r\nclass XORNetwork(L.LightningModule):\r\n    def __init__(self):\r\n        super(XORNetwork, self).__init__()\r\n        self.linear_sigmoid_stack = nn.Sequential(\r\n            nn.Linear(2, 2),\r\n            nn.Sigmoid(),\r\n            nn.Linear(2, 1)\r\n        )\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        x, y = batch\r\n        yhat = self.forward(x)\r\n        loss = F.mse_loss(yhat, y)\r\n        return loss\r\n\r\n    def forward(self, x):\r\n        return self.linear_sigmoid_stack(x)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    X = torch.Tensor([[0., 0.], [0., 1.], [1., 0], [1., 1]])\r\n    labels = torch.Tensor([0., 1., 1., 0])\r\n    dataset = TensorDataset(X, labels)\r\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\r\n\r\n    xor_network = XORNetwork()\r\n\r\n    # train model\r\n    trainer = L.Trainer(max_epochs=500, accelerator=\"cpu\")\r\n    trainer.fit(model=xor_network, train_dataloaders=dataloader)\r\n\r\n    xor_network.eval()\r\n    with torch.no_grad():\r\n        test_output = xor_network(X)\r\n        print(test_output.round())\r\n```\n```\n\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:               None\r\n\t- available:         False\r\n\t- version:           None\r\n* Lightning:\r\n\t- lightning:         2.1.1\r\n\t- lightning-utilities: 0.9.0\r\n\t- pytorch-lightning: 2.1.1\r\n\t- torch:             2.1.0\r\n\t- torchmetrics:      1.2.0\r\n* Packages:\r\n\t- aiohttp:           3.8.6\r\n\t- aiosignal:         1.3.1\r\n\t- async-timeout:     4.0.3\r\n\t- attrs:             23.1.0\r\n\t- certifi:           2023.7.22\r\n\t- charset-normalizer: 3.3.2\r\n\t- filelock:          3.13.1\r\n\t- frozenlist:        1.4.0\r\n\t- fsspec:            2023.10.0\r\n\t- idna:              3.4\r\n\t- jinja2:            3.1.2\r\n\t- lightning:         2.1.1\r\n\t- lightning-utilities: 0.9.0\r\n\t- markupsafe:        2.1.3\r\n\t- mpmath:            1.3.0\r\n\t- multidict:         6.0.4\r\n\t- networkx:          3.2.1\r\n\t- numpy:             1.26.1\r\n\t- packaging:         23.2\r\n\t- pip:               22.3.1\r\n\t- pytorch-lightning: 2.1.1\r\n\t- pyyaml:            6.0.1\r\n\t- requests:          2.31.0\r\n\t- setuptools:        65.5.1\r\n\t- sympy:             1.12\r\n\t- torch:             2.1.0\r\n\t- torchmetrics:      1.2.0\r\n\t- tqdm:              4.66.1\r\n\t- typing-extensions: 4.8.0\r\n\t- urllib3:           2.0.7\r\n\t- wheel:             0.38.4\r\n\t- yarl:              1.9.2\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         arm\r\n\t- python:            3.10.13\r\n\t- release:           23.0.0\r\n\t- version:           Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:34 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T8103\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18975/comments",
    "author": "Fohlen",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-11-09T13:59:33Z",
        "body": "@Fohlen In your Lightning code,\r\n\r\n1. You didn't choose the same learning rate. Make it 0.03 in both cases.\r\n2. You didn't run for the same number of epochs. Make it 1000 in both cases.\r\n\r\nAnd in the raw PyTorch code you are missing the test code:\r\n\r\n```py\r\n    xor_network.eval()\r\n    with torch.no_grad():\r\n        test_output = xor_network(Xs)\r\n        print(test_output.round())\r\n```\r\n\r\nTo make both of them the same, the hyperparameters need to be the same of course. Can you try again? I get the correct predictions (i.e. 0 1 1 0) after these fixes. \r\n\r\nIn addition, to make it fully deterministic you can set the seed \r\n\r\n```py\r\nL.seed_everything(0)\r\n```"
      },
      {
        "user": "Fohlen",
        "created_at": "2023-11-10T12:10:01Z",
        "body": "Hi @awaelchli, this indeed produces the correct result. I can get the code to converge correctly within 100 epochs or less with pure Torch, any idea why that wouldn't be the case with lightning?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-11-10T12:32:37Z",
        "body": "> I can get the code to converge correctly within 100 epochs or less with pure Torch\r\n\r\nThe code that you posted can't actually converge in 100 epochs. Please share what you changed to make that possible. "
      },
      {
        "user": "Fohlen",
        "created_at": "2023-11-10T17:07:55Z",
        "body": "Sorry for the imprecise wording. After some experimentation with epochs I could produce the correct result at `epoch=250` (not convergence). However, this appears to be extremely sensitive to the seed one uses when training. I find this interesting. According to the Deep Learning book, the correct weights should be learned with a single pass of this network. However, this behaviour is not lightning-specific. Thanks for your help, I will keep on digging in torch to find out the reason for this behaviour \ud83d\udc4d "
      }
    ]
  },
  {
    "number": 18890,
    "title": "ModuleNotFoundError: No module named 'lightning' in lightning container image",
    "created_at": "2023-10-30T07:58:12Z",
    "closed_at": "2023-11-04T14:29:38Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18890",
    "body": "### Bug description\n\n`lightning.pytorch` module is installed in lightning container images as pytorch_lightning, thus it is not compatible with the documentation. \r\n\r\nIn order for import to work in a container image, the import should be of the following form:\r\n\r\n```python\r\nfrom lightning.pytorch.loggers import CSVLogger\r\n```\r\n\r\nWhile the documentation states:\r\n\r\n```python\r\nfrom pytorch_lightning.loggers import CSVLogger\r\n```\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\ndocker run -it --rm pytorchlightning/pytorch_lightning:latest-py3.10-torch2.0-cuda12.0.1\r\npython -c \"from lightning.pytorch.loggers import CSVLogger\"\n```\n\n\n### Error messages and logs\n\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'lightning'\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:               None\r\n        - available:         False\r\n        - version:           11.7\r\n* Lightning:\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.1.0rc0\r\n        - torch:             2.0.1\r\n        - torchmetrics:      1.0.3\r\n* Packages:\r\n        - absl-py:           2.0.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.7.1\r\n        - async-timeout:     4.0.3\r\n        - attrs:             23.1.0\r\n        - cachetools:        5.3.1\r\n        - certifi:           2019.11.28\r\n        - chardet:           3.0.4\r\n        - charset-normalizer: 3.3.0\r\n        - click:             8.1.7\r\n        - cloudpickle:       2.2.1\r\n        - cmake:             3.27.6\r\n        - coloredlogs:       15.0.1\r\n        - contourpy:         1.1.1\r\n        - coverage:          7.3.1\r\n        - cycler:            0.12.0\r\n        - dbus-python:       1.2.16\r\n        - deepspeed:         0.9.3\r\n        - docstring-parser:  0.15\r\n        - exceptiongroup:    1.1.3\r\n        - fastapi:           0.103.2\r\n        - filelock:          3.12.4\r\n        - flatbuffers:       23.5.26\r\n        - fonttools:         4.43.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.9.2\r\n        - google-auth:       2.23.2\r\n        - google-auth-oauthlib: 1.0.0\r\n        - grpcio:            1.59.0\r\n        - h11:               0.14.0\r\n        - hjson:             3.1.0\r\n        - humanfriendly:     10.0\r\n        - hydra-core:        1.3.2\r\n        - idna:              2.8\r\n        - importlib-resources: 6.1.0\r\n        - iniconfig:         2.0.0\r\n        - jinja2:            3.1.2\r\n        - joblib:            1.3.2\r\n        - jsonargparse:      4.25.0\r\n        - kiwisolver:        1.4.5\r\n        - lightning-utilities: 0.9.0\r\n        - lit:               17.0.2\r\n        - markdown:          3.4.4\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.3\r\n        - matplotlib:        3.7.3\r\n        - mdurl:             0.1.2\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - networkx:          3.1\r\n        - ninja:             1.11.1\r\n        - numpy:             1.26.0\r\n        - nvidia-cublas-cu11: 11.10.3.66\r\n        - nvidia-cuda-cupti-cu11: 11.7.101\r\n        - nvidia-cuda-nvrtc-cu11: 11.7.99\r\n        - nvidia-cuda-runtime-cu11: 11.7.99\r\n        - nvidia-cudnn-cu11: 8.5.0.96\r\n        - nvidia-cufft-cu11: 10.9.0.58\r\n        - nvidia-curand-cu11: 10.2.10.91\r\n        - nvidia-cusolver-cu11: 11.4.0.1\r\n        - nvidia-cusparse-cu11: 11.7.4.91\r\n        - nvidia-nccl-cu11:  2.14.3\r\n        - nvidia-nvtx-cu11:  11.7.91\r\n        - oauthlib:          3.2.2\r\n        - omegaconf:         2.3.0\r\n        - onnx:              1.14.1\r\n        - onnxruntime:       1.16.0\r\n        - packaging:         23.1\r\n        - pandas:            2.1.1\r\n        - pillow:            10.0.1\r\n        - pip:               23.2.1\r\n        - pluggy:            1.3.0\r\n        - protobuf:          4.24.3\r\n        - psutil:            5.9.5\r\n        - py-cpuinfo:        9.0.0\r\n        - pyasn1:            0.5.0\r\n        - pyasn1-modules:    0.3.0\r\n        - pydantic:          1.10.13\r\n        - pygments:          2.16.1\r\n        - pygobject:         3.36.0\r\n        - pyparsing:         3.1.1\r\n        - pytest:            7.4.0\r\n        - pytest-cov:        4.1.0\r\n        - pytest-random-order: 1.1.0\r\n        - pytest-rerunfailures: 12.0\r\n        - pytest-timeout:    2.1.0\r\n        - python-apt:        2.0.1+ubuntu0.20.4.1\r\n        - python-dateutil:   2.8.2\r\n        - pytorch-lightning: 2.1.0rc0\r\n        - pytz:              2023.3.post1\r\n        - pyyaml:            6.0.1\r\n        - requests:          2.22.0\r\n        - requests-oauthlib: 1.3.1\r\n        - requests-unixsocket: 0.2.0\r\n        - rich:              13.5.3\r\n        - rsa:               4.9\r\n        - scikit-learn:      1.3.1\r\n        - scipy:             1.11.3\r\n        - setuptools:        59.5.0\r\n        - six:               1.14.0\r\n        - sniffio:           1.3.0\r\n        - starlette:         0.27.0\r\n        - sympy:             1.12\r\n        - tensorboard:       2.14.1\r\n        - tensorboard-data-server: 0.7.1\r\n        - tensorboardx:      2.6.2.2\r\n        - threadpoolctl:     3.2.0\r\n        - tomli:             2.0.1\r\n        - torch:             2.0.1\r\n        - torchmetrics:      1.0.3\r\n        - tqdm:              4.66.1\r\n        - triton:            2.0.0\r\n        - typeshed-client:   2.4.0\r\n        - typing-extensions: 4.7.1\r\n        - tzdata:            2023.3\r\n        - urllib3:           1.25.8\r\n        - uvicorn:           0.23.2\r\n        - werkzeug:          3.0.0\r\n        - wget:              3.2\r\n        - wheel:             0.41.2\r\n        - yarl:              1.9.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.10.13\r\n        - release:           5.3.0-28-generic\r\n        - version:           #30~18.04.1-Ubuntu SMP Fri Jan 17 06:14:09 UTC 2020\r\n\r\n</details>\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18890/comments",
    "author": "GuyPozner",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-10-30T13:56:30Z",
        "body": "Hey @GuyPozner \r\n\r\nTo use the `import lightning.x.y` import, you need to install the lightning package: `pip install lightning`\r\nIf you want to use the `import pytorch_lightning.x.y` style, you need to install `pip install pytorch-lightning`\r\n\r\nYou can't mix and match them. Our documentation exclusively uses the new package imports with lightning. That's the new and recommended way. "
      },
      {
        "user": "GuyPozner",
        "created_at": "2023-10-30T14:19:38Z",
        "body": "Hi @awaelchli,\r\nThanks for the fast response, I understood that. What I am thinking, is that if this is the recommended way to import, it should work as expected with the containers images that gets released. what do you think? have you reproduced the issue?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-10-30T14:20:31Z",
        "body": "Sorry, which container images are you talking about?"
      },
      {
        "user": "GuyPozner",
        "created_at": "2023-10-31T07:05:18Z",
        "body": "this container image: pytorchlightning/pytorch_lightning:latest-py3.10-torch2.0-cuda12.0.1\r\n\r\nBut I think that this is true for every container image.\r\n\r\nThanks for the fix :)"
      }
    ]
  },
  {
    "number": 18378,
    "title": "Fabric cannot launch with specified gpu indices",
    "created_at": "2023-08-24T00:21:36Z",
    "closed_at": "2023-08-24T12:48:57Z",
    "labels": [
      "question",
      "3rd party",
      "fabric",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18378",
    "body": "### Bug description\n\nHi,\r\n\r\nI launch fabric with specified gpu indices, and get RuntineError messges as follows.\r\nHow can I solve this issue? Thanks.\r\n\r\nSoftware version:\r\n```\r\ndeepspeed 0.10.0\r\nlightning 2.0.7\r\npytorch 2.0.1\r\npython 3.10.9\r\n```\r\n\r\nThe code looks like:\r\n```python\r\nimport lightning as L\r\nfabric = L.Fabric(accelerator=\"cuda\", devices=\"0,1,4,5\", strategy='deepspeed')\r\nfabric.launch()\r\n```\r\n\r\nThe error messges and logs is:\r\n```\r\n[2023-08-24 07:56:20,780] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/test_fab.py\", line 45, in <module>\r\n    fabric.launch()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 687, in launch\r\n    return self._strategy.launcher.launch(function, *args, **kwargs)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/launchers/subprocess_script.py\", line 92, in launch\r\n    return function(*args, **kwargs)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 776, in _run_with_setup\r\n    self._strategy.setup_environment()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/ddp.py\", line 113, in setup_environment\r\n    self._setup_distributed()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/deepspeed.py\", line 579, in _setup_distributed\r\n    _validate_device_index_selection(self.parallel_devices)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/deepspeed.py\", line 821, in _validate_device_index_selection\r\n    raise RuntimeError(\r\nRuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n```\r\n\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9): \r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @carmocca @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18378/comments",
    "author": "seraphzl",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-08-24T01:28:38Z",
        "body": "@seraphzl This is expected, and is not supported by DeepSpeed. The error comes directly from Fabric and is informing you about the limitation. In addition, it suggests how you can select the devices via the environment variable. If you read the error message carefully:\r\n\r\n```\r\nRuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n```\r\n\r\nSo basically you remove the setting for `devices=[0, 1, 4, 5]` and launch with `CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py`. Please let me know if that works.\r\n"
      },
      {
        "user": "seraphzl",
        "created_at": "2023-08-24T13:55:21Z",
        "body": "> @seraphzl This is expected, and is not supported by DeepSpeed. The error comes directly from Fabric and is informing you about the limitation. In addition, it suggests how you can select the devices via the environment variable. If you read the error message carefully:\r\n> \r\n> ```\r\n> RuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n> ```\r\n> \r\n> So basically you remove the setting for `devices=[0, 1, 4, 5]` and launch with `CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py`. Please let me know if that works.\r\n\r\n@awaelchli Using the CUDA_VISIBLE_DEVICES environment setting to launch the training works. Thank you for reply."
      }
    ]
  },
  {
    "number": 18170,
    "title": "Basic ProgressBar does not work",
    "created_at": "2023-07-26T21:40:27Z",
    "closed_at": "2023-07-27T11:20:36Z",
    "labels": [
      "question",
      "progress bar: tqdm",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18170",
    "body": "### Bug description\r\n\r\nVersion of `pytorch_lightning==2.0.6`, `tqdm==4.65.0`\r\nI want to display the training progress of my models and the basic ProgressBar from pytorch_lightning.callbacks does not work (nothing shows up).\r\nHowever, when I switched to RichProgressBar, the rich progress bar shows up.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\r\n### How to reproduce the bug\r\n\r\n`python\r\npytorch_lightning.callbacks import ProgressBar` does not show up anything.\r\n\r\n`from pytorch_lightning.callbacks import RichProgressBar` can show the training progress.\r\n\r\n\r\n### Error messages and logs\r\n\r\n\r\nNothing shows up for ```ProgressBar```.\r\n\r\n### Environment\r\n`pytorch_lightning==2.0.6`\r\n`tqdm==4.65.0`\r\n\r\n### More info\r\nRunning things in a Linux environment, with an A40 GPU.\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18170/comments",
    "author": "dnaihao",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-26T22:22:25Z",
        "body": "Hi @dnaihao \r\n\r\nYou don't need to import the progress bar if you want to use tqdm. It gets enabled by default if you just run the Trainer :)\r\nThe reason why you don't see anything is because you accidentally imported the base class, but you probably wanted to import `pytorch_lightning.callbacks import TQDMProgressBar`. (But again, it is the default, so it's not necessary technically). \r\n\r\nLet me know if that resolves your problem :)"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-27T11:20:36Z",
        "body": "Yes, you are right, in previous versions prior to 2.0, ProgressBar was the tqdm-version. Later, the name of the base class was changed from ProgressBarBase to just ProgressBar #17058. Apologies if this caused confusion!"
      }
    ]
  },
  {
    "number": 18135,
    "title": "_PrefetchDataFetcher ignores prefetch_factor",
    "created_at": "2023-07-21T21:34:48Z",
    "closed_at": "2023-07-24T15:11:19Z",
    "labels": [
      "question",
      "data handling",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18135",
    "body": "### Bug description\r\n\r\nNo matter what prefetch_factor is set for the `DataLoader` in a `LightningDataModule` wrapper, when the `_PrefetchDataFetcher` is initialized, the value is always reset to 1.\r\n\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18135/comments",
    "author": "botcs",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-23T18:56:39Z",
        "body": "@botcs The prefetching done in the trainer is independent of the prefetching in the DataLoader. The trainer prefetches 1 batch just to know in advance whether the data loader is exhausted or not, that's all. But of course, you can set any value for `DataLoader(prefetch_factor=N)` and this will be handled by PyTorch. Let me know if you have any questions."
      }
    ]
  },
  {
    "number": 16102,
    "title": "Torch sees GPU but does not use it",
    "created_at": "2022-12-17T17:36:44Z",
    "closed_at": "2022-12-18T20:54:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16102",
    "body": "### Bug description\n\nWhen I use ``torch.cuda.device_count()``\r\nit returns 1, which is correct\r\nBut then when using Lightning, it shows this in terminal\r\n``LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n``\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 1.10):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16102/comments",
    "author": "ThatGuyCalledJesse",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-12-18T20:47:01Z",
        "body": "@ThatGuyCalledJesse That's correct. If you have one GPU, then Lightning can only use one and that's device with index 0. \r\nIf you had multiple GPUs, it would show:\r\n\r\n`LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0, 1]`\r\n\r\netc.\r\n"
      },
      {
        "user": "yotamcons",
        "created_at": "2023-11-20T11:09:29Z",
        "body": "I made the same mistake, thank you for asking.\r\nWhat bothered me was the \"local rank\" which i couldn't figure out. This calls for better logging, especially in the uncomfortable new filed of gpu computations."
      }
    ]
  },
  {
    "number": 15973,
    "title": "access to the last training epoch that triggered early stopping",
    "created_at": "2022-12-09T01:07:08Z",
    "closed_at": "2022-12-14T02:50:04Z",
    "labels": [
      "question",
      "callback: early stopping"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15973",
    "body": "### Description & Motivation\r\n\r\nI wish there was a method to access when the last training epoch number was. The max_training_epoch differs from the last training epoch resulting from the early stopping.\n\ncc @borda @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15973/comments",
    "author": "jaeho3690",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-12-13T04:38:37Z",
        "body": "Hi @jaeho3690 \r\nIs this what you are looking for?\r\n\r\n```py\r\nprint(trainer.early_stopping_callback.stopped_epoch)\r\n\r\n```"
      }
    ]
  },
  {
    "number": 14199,
    "title": "on_save_checkpoint runs multiple times on DDP",
    "created_at": "2022-08-14T13:42:46Z",
    "closed_at": "2022-08-14T18:23:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/14199",
    "body": "## \ud83d\udc1b Bug\r\n\r\non_save_checkpoint handler of the LightningModule runs once per GPU when running on multiple GPUs with `strategy=dpp`. This can have unwanted side effects when saving additional checkpoint information.\r\n\r\nIs there a way to make this run only the master? \r\n\r\nIs this the expected behavior? I would think the former would be the more common use case.\r\n\r\n### To Reproduce\r\n\r\n```py\r\nfrom transformers import AutoModel\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self,  model_name, save_path):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.model = AutoModel.from_pretained(self.hparams.model_name)\r\n\r\n    def forward(self, pixel_values):\r\n        outputs = self.model(pixel_values=pixel_values)\r\n        return outputs\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n    def on_save_checkpoint(self, checkpoint):\r\n        print(\"Saving model latest checkpoint in HF format..\")\r\n        self.model.save_pretrained(self.hparams.save_path)\r\n```\r\n\r\n- Lightning Component (e.g. Trainer, LightningModule):\r\n- PyTorch Lightning Version  1.6.4\r\n- OS (e.g., Linux): Linux\r\n- CUDA/cuDNN version: 11.3\r\n- GPU models and configuration:  8xT4\r\n- Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/14199/comments",
    "author": "jordanparker6",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-08-14T18:20:49Z",
        "body": "> Is this the expected behavior?\r\n\r\nYes, it definitely is. \r\n\r\n> This can have unwanted side effects when saving additional checkpoint information.\r\n\r\nWhat for example? Can you give an example?\r\n`on_save_checkpoint`'s primary use is to modify the dictionary checkpoint before it gets saved. Whether all ranks do something with this dictionary or not is up to the implementation in the strategy. Example: Sharded models may want to save a portion of the weights on each rank. \r\n\r\n> Is there a way to make this run only the master?\r\n\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n        if self.global_rank == 0:\r\n            # run only on rank 0\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-08-14T18:23:13Z",
        "body": "Closing the issue as behavior is expected. In general, all hooks need to run on all ranks. \r\n\r\nIf you are looking to save only on one rank, this can be accounted for by:\r\n\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n        if self.global_rank == 0:\r\n            print(\"Saving model latest checkpoint in HF format..\")\r\n            self.model.save_pretrained(self.hparams.save_path)\r\n```"
      }
    ]
  },
  {
    "number": 13360,
    "title": "Using Dataloader in pytorch_lightning when using DDP training?",
    "created_at": "2022-06-22T06:51:21Z",
    "closed_at": "2022-08-01T14:32:41Z",
    "labels": [
      "question",
      "won't fix",
      "distributed",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13360",
    "body": "My computer has 2 gpus and I have some problems:\r\n1. First, I used Pytorch\r\nI have 10 images, I created distributed dataloader (using sampler) follow Pytorch instruction, batchsize = 4, gpu=2.\r\n=> So with each gpu, length of batch0 is 4 and length of batch1 is 4. Sampler added 2 to batch1 to make batchsize = 4.\r\nI trained with ddp_spawn with pytorch code and everything is ok.\r\n\r\n2. Next, I used Pytorch Lightning\r\nI also use 10 images, I created dataloader (Normal Dataloader) follow Pytorch Instruction, batchsize =4, gpu=2\r\n=> so with each gpu, length of batch0 is 4 and length of batch1 is 1.\r\n\r\nNow, I want to use pytorch lightning but I want batchsize=4 same distributed sampler when working with pytorchlightning. How should I do?\r\n\r\nThanks\n\ncc @awaelchli @rohitgr7 @akihironitta @justusschock @ninginthecloud @otaj",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13360/comments",
    "author": "NguyenDuyDuc1491995",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-06-22T10:22:49Z",
        "body": "> So with each gpu, length of batch0 is 4 and length of batch1 is 4. Sampler added 2 to batch1 to make batchsize = 4.\r\n\r\nthat makes total batches to be processed = 16, which seems incorrect.\r\nPL uses distributed sampler internally so it should be `batch0=4` and `batch1=1` on each GPU."
      },
      {
        "user": "NguyenDuyDuc1491995",
        "created_at": "2022-06-22T16:50:56Z",
        "body": "The problem is when I train semantic segmentation DeepLabV3 by pytorchlightning\r\nBatch0 =4 is ok, but batch1=1 is error. Because the batch is 1 so there is a problem with batchnorm.\r\n\r\nBut I used distributed dataloader with sampler in pytorch and I saw it will create batch0=4 and batch1=4 ( it will take 3 more images) so I wonder if I can create a distributed dataloader as same as above in pytorch lightning?"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-06-22T17:21:49Z",
        "body": "The distributed sampler has nothing to do with batching. It only ensures that each GPU gets the same amount of samples and since your dataset of 10 images is divisible by 2, that's never a problem. \r\n\r\nTo avoid uneven batch sizes, just set `drop_last=True` in the dataloader and then you are guaranteed to get the batch size 4 for each batch. This is the same in PyTorch and PL, there should be no difference. "
      }
    ]
  },
  {
    "number": 12943,
    "title": "Distribution not moved to correct device",
    "created_at": "2022-05-01T04:06:14Z",
    "closed_at": "2022-06-23T04:17:41Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12943",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI'm attempting to port a variational auto-encoder to use `torch.distributions`. I noticed that when I define a `torch.distributions.Distribution` as a module parameter - for example the `prior` in the example below - it doesn't get moved to the correct device. I'm fairly certain this is because you call `module.apply()` with a lambda in order to recursively move modules to the specific device - unfortunately a Distribution is not a module but it does hold tensors so this strategy fails in this edge case.\r\n\r\n### To Reproduce\r\n\r\n```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nfrom torch.distributions import MultivariateNormal\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n        latent_dim=2\r\n        self.prior = MultivariateNormal(\r\n\t\t\ttorch.zeros(latent_dim), \r\n\t\t\tscale_tril=torch.diag(torch.ones(latent_dim)))\r\n\r\n    def forward(self, x):\r\n        assert self.prior.loc.device == x.device, \"incorrect device\"\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        gpus=1,\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3090\r\n                - NVIDIA GeForce RTX 3090\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.22.3\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.1+cu111\r\n        - pytorch-lightning: 1.6.1\r\n        - tqdm:              4.64.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #121-Ubuntu SMP Thu Mar 24 16:04:27 UTC 2022",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12943/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-05-02T18:59:20Z",
        "body": "anything that is not `nn.Module` or registered under buffer will not be moved to the device. Here the `LightningModule` follow the steps of `nn.Module` so even if you do this:\r\n```py\r\nclass BoringModel(nn.Module):  # <- using `nn.Module` here\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n        latent_dim=2\r\n        self.prior = MultivariateNormal(\r\n\t\t\ttorch.zeros(latent_dim), \r\n\t\t\tscale_tril=torch.diag(torch.ones(latent_dim)))\r\n\r\nmodel = BoringModel()\r\nmodel.cuda()\r\n```\r\n`model.prior` will stay on the CPU.\r\n\r\nI'd suggest doing\r\n```py\r\nclass LitModel(LightningModule):\r\n    def on_fit_start(self):\r\n        self.prior = MultivariateNormal(\r\n            torch.zeros(latent_dim, device=self.device), \r\n            scale_tril=torch.diag(torch.ones(latent_dim, device=self.device))\r\n        )\r\n\r\n```\r\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2022-05-03T09:14:05Z",
        "body": "then you need to define it under `on_test_start/on_validation_start` as well. To make sure they are saved in the checkpoint, maybe try registering them as a buffer so that they are moved to the correct devices as well as saved to the checkpoint."
      }
    ]
  },
  {
    "number": 8559,
    "title": "Wandblogger not logging train loss after every step",
    "created_at": "2021-07-26T13:30:21Z",
    "closed_at": "2021-07-26T14:30:23Z",
    "labels": [
      "help wanted",
      "question",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8559",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI am using wandb with Pytorch Lightning. I am logging train/loss, val/loss, train/metric, val/metric. Everything is logged properly to wandb dashboard except the **train/loss** (after every step).\r\n\r\nHere's the main lightning module:\r\n\r\n`class ImageClassification(pl.LightningModule):\r\n    def __init__(self, model):\r\n        super().__init__()\r\n        self.model = model\r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        self.lr = CFG['lr']\r\n    \r\n    def forward(self, x):\r\n        output = self.model(x)\r\n        return output\r\n    \r\n    def configure_optimizers(self):\r\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=CFG['lr'])\r\n        return self.optimizer\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        images, targets = batch['image'], batch['target']\r\n        output = self.model(images)\r\n        loss = self.criterion(output.view(-1), targets)\r\n        \r\n        self.log('train/loss', loss, logger=True)  # the thing that is not being logged\r\n\r\n        try:\r\n            auc = roc_auc_score(targets.detach().cpu(), output.sigmoid().detach().cpu())\r\n            self.log(\"train/auc\", auc, prog_bar=True, logger=True)\r\n        except:\r\n            pass\r\n        \r\n        return {\r\n            \"loss\": loss,\r\n            \"predictions\": output,\r\n            \"targets\" : targets\r\n        }\r\n    \r\n    def training_epoch_end(self, outputs):\r\n\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets)\r\n        preds = torch.stack(preds)\r\n\r\n        train_auc = roc_auc_score(targets.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log(\"train/auc_epoch\", train_auc,logger=True)\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        images, targets = batch['image'], batch['target']\r\n        output = self.model(images)\r\n        loss = self.criterion(output.view(-1), targets)\r\n\r\n        self.log('val/loss', loss,prog_bar=True, logger=True)\r\n\r\n        return {\r\n            \"predictions\": output,\r\n            \"targets\": targets\r\n        }\r\n    \r\n    def validation_epoch_end(self, outputs):\r\n\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets)\r\n        preds = torch.stack(preds)\r\n    \r\n        val_auc = roc_auc_score(targets.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log(\"val/auc_epoch\", val_auc,prog_bar=True,logger=True)\r\n    \r\n    def test_step(self, batch, batch_idx):\r\n        images = batch['image']\r\n        output = self.model(images)\r\n        return output`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8559/comments",
    "author": "Gladiator07",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T13:41:38Z",
        "body": "is the logging interval small enough? I.e., `Trainer(log_every_n_step=n)` where n must be smaller than `len(dataloader)`. "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T14:23:47Z",
        "body": "```python\r\ntry:\r\n        auc = roc_auc_score(targets.detach().cpu(), output.sigmoid().detach().cpu())\r\n        self.log(\"train/auc\", auc, prog_bar=True, logger=True)\r\n    except:\r\n        pass\r\n```\r\n\r\nIf an exception raises here, it will be silently ignored and nothing gets logged. Please try to remove the try-except block here and see if it raises an exception."
      }
    ]
  },
  {
    "number": 6104,
    "title": "Early stopping on custom metric without validation_step",
    "created_at": "2021-02-20T16:19:44Z",
    "closed_at": "2021-02-23T18:18:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6104",
    "body": "#### What is your question?\r\n\r\nI have a metric that I can only define using every predictions on the validation split, so I cannot use `validation_step` since it only operates on batches of data.\r\nI have a callback that computes and log this metric in `on_train_epoch_end`. \r\nI am not executing the validation loop because it's useless in my case.\r\nMy question is: How can I properly use the EarlyStopping callback ? (Same question for ModelCheckpoint)\r\n\r\n#### What have you tried?\r\nI have tried manually calling `pl_module.on_validation_epoch_end()` in my callback but it doesn't seem to work because EarlyStopping never stops the model even though the patience should have dropped to 0.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Kubuntu 20.04\r\n - Packaging: pip\r\n - Version: 1.1.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6104/comments",
    "author": "Inspirateur",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-21T00:37:08Z",
        "body": "If I understood you correctly, you just need to make sure to create your instance as:\r\n\r\n`EarlyStopping(monitor=\"your_metric\")`\r\n\r\nAnd then, in your LightningModule's `on_validation_epoch_end` do `self.log(\"your_metric\", value)`"
      },
      {
        "user": "Inspirateur",
        "created_at": "2021-02-23T18:18:41Z",
        "body": "Seems like that works, thank you.\r\nI just had to define an empty `validation_step` method in my lightning module so the fake validation would be quick."
      }
    ]
  },
  {
    "number": 5705,
    "title": "Why do some metrics require `num_classes=1` for binary classification?",
    "created_at": "2021-01-29T13:16:35Z",
    "closed_at": "2021-02-04T23:34:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5705",
    "body": "## \u2753 Why do some metrics require `num_classes=1` for binary classification?\r\n\r\n#### What is your question?\r\n\r\nWhy do some metrics require the argument `num_classes=1` for binary classification (and some don't) to give the correct results?\r\n\r\nI find it rather unintuitively to calculate Recall/Precision/F1 with the argument `num_classes=1` for a binary classification, whereas e.g. ConfusionMatrix requires `num_classes=2` in the same situation.\r\n\r\nFurthermore, using Recall/Precision/F1 with `num_classes=2` for a binary classification gives wrong results - so this also might be considered a bug-report.\r\n\r\nIt took me quite some time to figure out, why calculated metrics are different from what I calculated by hand from the confusion matrix.\r\n\r\n#### Code\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning import metrics\r\n\r\n# example data\r\npreds = [0] * 200 + [1] * 30 + [0] * 10 + [1] * 20\r\ntargets = [0] * 200 + [1] * 30 + [1] * 10 + [0] * 20\r\n\r\npreds = torch.tensor(preds)\r\ntargets = torch.tensor(targets)\r\n\r\n# define method for printing metrics\r\n\r\n\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(num_classes=num_classes)\r\n    recall = metrics.classification.Recall(num_classes=num_classes)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=1)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n\r\n_print_some_metrics(preds, targets, num_classes=1)\r\n_print_some_metrics(preds, targets, num_classes=2)\r\n```\r\n\r\nResults in\r\n\r\n> $ _print_some_metrics(preds, targets, num_classes=1)\r\n> Precision:\r\n> 0.6000000238418579\r\n> \r\n> Recall:\r\n> 0.75\r\n> \r\n> F1:\r\n> 0.6666666865348816\r\n> \r\n> AVG Precision:\r\n> 0.48846155405044556\r\n> \r\n> Accuracy:\r\n> 0.8846153616905212\r\n> \r\n> ConfMat:\r\n> tensor([[200.,  20.],\r\n>         [ 10.,  30.]])\r\n> \r\n> \r\n> $ _print_some_metrics(preds, targets, num_classes=2)\r\n> Precision:\r\n> 0.8846153616905212\r\n> \r\n> Recall:\r\n> 0.8846153616905212\r\n> \r\n> F1:\r\n> 0.8846153616905212\r\n> \r\n> AVG Precision:\r\n> 0.48846155405044556\r\n> \r\n> Accuracy:\r\n> 0.8846153616905212\r\n> \r\n> ConfMat:\r\n> tensor([[200.,  20.],\r\n>         [ 10.,  30.]])\r\n\r\nAs one can see, Precision/Recall/F1 give different (wrong) results when setting `num_classes=2` in a binary classification.\r\nAveragePrecision doesn't even work with the binary usecase when setting `num_classes=2` whereas ConfusionMatrix doesn't work when setting `num_classes=1`.\r\n\r\nI wonder if there is a specific reason why one would set `num_classes=1` in a binary classification (where actually 2 classes exist).\r\n\r\nWouldn't it be more straightforward to set `num_classes=2` for binary classification for all metrics?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5705/comments",
    "author": "kapsner",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-01-30T18:18:55Z",
        "body": "So I can try to clarify this a bit:\r\n* we are aware of this, and many of these metrics have already been changed on the `release/1.2-dev` branch. For example will your example with `precision = metrics.classification.Precision(num_classes=2)` give an error because input tensors are clearly binary but the class was initialized with `num_classes=2`.\r\n* confusion matrix needs `num_classes=2` for binary problems, because we to know the size of the tensor that we should allocate \r\n* the difference between specifying `num_classes=1` or `num_classes=2` really comes down to if you want to calculate the score on only the positive class (this is probably what you want) or both classes (which really does not make sense for binary problems, because many of the scores reduce to the same then).\r\n"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-02-01T13:33:42Z",
        "body": "so the usecase could be that the user wants set `average=None` to get the score for each class. Again in that case we kind of need to now the size of the tensor that we should allocate (the metric states)."
      }
    ]
  },
  {
    "number": 5636,
    "title": "Understanding accumulate_grad_batches  parameter? ",
    "created_at": "2021-01-24T10:10:44Z",
    "closed_at": "2021-01-24T18:19:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5636",
    "body": "I am very new to PL. As far as I understand **accumulate_grad_batches** works similar to  **'gradient_accumulation_steps'** , where the main task is to increase the effective batch size.  But I do not see any change in training epoch step count when increasing the  **accumulate_grad_batches** parameters.\r\n\r\nLet's say, I have a dataset of 1000 examples and my batch_size is one and I only use a single GPU. So in this case, if I use the value 2  for the **accumulate_grad_batches**,  the number of steps for an epoch should be shown as 500 (logger). But I still see 1000.\r\n\r\nIs it a bug or PL doesn't divide the number of steps when showing in the log?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5636/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-24T13:16:39Z",
        "body": "total step count will remain the same since it refers to total batches, but internally `optimizer/scheduler.step` is updated accordingly. You can check `self.global_step` with and without accumulation."
      }
    ]
  },
  {
    "number": 5552,
    "title": "How to iterate over training set AGAIN on training epoch end?",
    "created_at": "2021-01-18T07:57:20Z",
    "closed_at": "2021-01-19T11:09:43Z",
    "labels": [
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5552",
    "body": "## \u2753 Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nHow to iterate over training set AGAIN on training epoch end or on validation epoch start?\r\nI have a model that works as expected on MNIST, but for my unique data, val_loss<train_loss for all samples.\r\nI have no idea what causes this, and it is too suspicious to allow me to go on.\r\n\r\n*I want to do a validation step on training data, in eval mode*\r\n\r\nI hope it will ease my mind.\r\n\r\n#### Code\r\nWell, if I had code for how to correctly do this I wouldn't ask :)\r\n\r\n#### What have you tried?\r\nThis doesn't sound like a standard use case, not even sure that's supported.\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win] Win\r\n - Packaging [e.g. pip, conda] Pip\r\n - Version [e.g. 0.5.2.1] 1.1.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5552/comments",
    "author": "noamzilo",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-18T08:33:00Z",
        "body": "Hey @noamzilo,\r\n\r\nIf you have a `val_dataloader`, the entire dataset of validation will be used once the latest batch of your train_dataset would be reached.\r\n\r\nTherefore, you can compare your metrics at this point in epoch_end hooks.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-18T08:37:18Z",
        "body": "It is not an accident that PyTorch is in the name of PyTorchLightning :)) \r\nYou can just do it as you would in PyTorch:\r\n\r\n```python\r\nself.eval()\r\nfor idx, batch in enumerate(self.train_dataloader()):\r\n    # do what you have to do\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "number": 5550,
    "title": "on_train_epoch_end vs training_epoch_end",
    "created_at": "2021-01-17T21:37:32Z",
    "closed_at": "2021-01-18T16:55:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5550",
    "body": "## \u2753 Questions and Help\r\n\r\nWhat is the difference between on_train_epoch_end and training_epoch_end? For what applications should we use each?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5550/comments",
    "author": "Bajo1994",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-18T08:45:53Z",
        "body": "Hey @Bajo1994,\r\n\r\n* `training_epoch_end` will be used for the user to aggregate the outputs from training_step at the end of an epoch.\r\n\r\nExample.\r\n```\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n```\r\n\r\n* on_train_epoch_end is a hook. It would be used to add extra logic to control the behaviour of the model.\r\nBut it is left to the user to choice how he wants to use it :)\r\n\r\nI hope it helps !\r\n\r\nBest,\r\nT.C \r\n\r\n"
      }
    ]
  },
  {
    "number": 5454,
    "title": "Validation step is ignored when using DataModule",
    "created_at": "2021-01-11T00:49:13Z",
    "closed_at": "2021-01-12T01:34:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5454",
    "body": "#### What is your question?\r\nHi, guys!\r\nI created my own DataModule and loading it to the trainer. However, it appears that the \"fit\" is skipping the validation step.\r\nHow can I ensure that the code runs through the validation step too?\r\n\r\n#### Code\r\n```\r\nclass DataModule(pl.LightningDataModule):\r\n    def __init__(self, batch_size=25, seed=0):\r\n    # def __init__(self, dataset, batch_size=25, seed=0):\r\n        super().__init__()\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.seed = seed\r\n        self.split = [801, 100, 100]\r\n        # self.transform = torchvision.transforms.ToTensor()\r\n\r\n    def setup(self, stage=None):\r\n        # train/valid/test split\r\n        # and assign to use in dataloaders via self\r\n        train_set, valid_set, test_set = torch.utils.data.random_split(self.dataset, self.split, generator=torch.Generator().manual_seed(self.seed))\r\n\r\n        if stage == 'fit' or stage is None:\r\n\r\n            self.train_set = train_set\r\n            self.valid_set = valid_set\r\n\r\n        if stage == 'test' or stage is None:\r\n            self.test_set = test_set\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\r\n\r\n    def valid_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False)\r\n\r\n    def test_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False)\r\n\r\nclass LitReg(pl.LightningModule):\r\n    def __init__(self, in_dims, out_dims, lr=2e-4, max_dict={}):\r\n        super().__init__()\r\n        self.in_size = in_dims\r\n        self.out_size = out_dims\r\n        self.lr = lr\r\n        self.max_dict = max_dict\r\n\r\n        # model\r\n        self.model = nn.Sequential(\r\n            nn.Linear(self.in_size, self.in_size),\r\n\r\n            nn.LeakyReLU(0.02),\r\n\r\n            nn.Linear(self.in_size, self.out_size)\r\n        )\r\n\r\n        self.model.apply(self.weights_init)\r\n\r\n    def forward(self, data):\r\n        out = self.model(data)\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y, l_rate = batch\r\n        pred_y = self.model(x)\r\n        train_loss = F.mse_loss(pred_y, y) \r\n\r\n        self.log('train_loss', train_loss, prog_bar=True)\r\n        return train_loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._shared_eval(batch, batch_idx, 'val')\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        self._shared_eval(batch, batch_idx, 'test')\r\n\r\n    def _shared_eval(self, batch, batch_idx, prefix):\r\n        x, y, l_rate = batch\r\n        pred_y = self.model(x)\r\n\r\n        loss = F.mse_loss(pred_y, y) \r\n        self.log(f'{prefix}_loss', loss, prog_bar=True)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), self.lr)\r\n        return optimizer\r\n\r\n    def weights_init(self, m):\r\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n        if isinstance(m, nn.Linear):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n```\r\n#### What have you tried?\r\nPlacing breakpoints to debug in VSCode, but all in vain.\r\nAlso accessed both valid and test datasets and loaders. All looks set.\r\nWhat is working? If I load the data the following way.\r\n```\r\n    **train_loader = DataLoader(X_train, batch_size=args.batch_size)\r\n    val_loader = DataLoader(X_val, batch_size=args.batch_size)\r\n    test_loader = DataLoader(X_test, batch_size=args.batch_size)\r\n\r\n    trainer.fit(model, train_loader, val_loader)**\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Win\r\n - Packaging pip\r\n - Version 1.1.3\r\n\r\nThank you for your attention!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5454/comments",
    "author": "ncuxomun",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2021-01-11T03:35:03Z",
        "body": "```\r\n    def validation_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'val')\r\n```\r\nYou forgot return!"
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-01-11T17:20:23Z",
        "body": "> ```\r\n>     def validation_step(self, batch, batch_idx):\r\n>         return self._shared_eval(batch, batch_idx, 'val')\r\n> ```\r\n> \r\n> You forgot return!\r\n\r\nThanks for the remark! I corrected the mistake. But I still get the same output, i.e. the validation step is skipped. Is there anything else that I am missing?"
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-01-12T00:33:25Z",
        "body": "Thanks for re-opening it. I was looking your your earlier comment))\r\nYes, it is connected via the shared function here. I also tried without it, that is enabling both val and test function, and disabling the the shared one.\r\nNo, the script does not not run with the validation progress bar. It displays \"loss\" and \"train_loss\" only. Another observation is that what I did include \"return\" and \"fit by trainer.fit(model, datamodule=dm), the training took more epochs, ignoring the early stopping callback (which is honored when I don't work with datamodule)."
      },
      {
        "user": "s-rog",
        "created_at": "2021-01-12T00:45:23Z",
        "body": "Could you try initializing the dataset in datamodules setup instead of passing it in as an arg?\r\n\r\nAlso could you check the length of the datasets before and after splitting? I suspect validation is not getting any data."
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-01-12T01:20:05Z",
        "body": "Cool, thanks for suggestions. Here's what I did:\r\n1. Dataset was moved into prepare_data, where it was initialized.\r\n2. Then I placed a breakpoint into the dataset to check its dimensions. This prompted the following warning, which I did not see before: \r\n_\"UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop warnings.warn(*args, *kwargs)_\"\r\n3. Then, I renamed \"_valid_dataloader_\" into \"**val_dataloader**\", re-ran then case and all progress bar values were displayed as required. \r\nIt was my bad, I should have called that function precisely as the said in the docs. One thing that is surprising is that this \"skipping\" part was not a big deal when I was loading the train_dataloader and val_dataloader individually instead of datamodule. Do you know what could be the issue?\r\nPlease feel free to close the issue if you think so.\r\nOnce again, thanks a lot for responding and giving suggestions that helped resolve the issue."
      }
    ]
  },
  {
    "number": 5129,
    "title": "How to forbid save optimizer's state \uff1f",
    "created_at": "2020-12-14T15:12:33Z",
    "closed_at": "2020-12-15T02:59:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5129",
    "body": "Sometime's it's time-consuming.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5129/comments",
    "author": "Maybewuss",
    "comments": [
      {
        "user": "DuinoDu",
        "created_at": "2020-12-14T15:47:55Z",
        "body": "Set `save_weights_only=True` in ModelCheckpoint."
      }
    ]
  },
  {
    "number": 4947,
    "title": "How to log more than one metrics in logger?",
    "created_at": "2020-12-02T15:31:25Z",
    "closed_at": "2020-12-02T16:01:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4947",
    "body": "I want to log two metircs.What should i do?\r\nself.log('my_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) \r\nThis can only log one metrics.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4947/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-12-02T15:33:51Z",
        "body": "@7zhanghao  You can just write another log.\r\n```py\r\n self.log('another_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n```"
      },
      {
        "user": "zhutmost",
        "created_at": "2020-12-02T15:50:54Z",
        "body": "@7zhanghao You can use `self.log_dict`"
      }
    ]
  },
  {
    "number": 4874,
    "title": "Metric Reset",
    "created_at": "2020-11-26T17:37:30Z",
    "closed_at": "2020-11-28T22:40:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4874",
    "body": "How can I manually reset a metric? \r\nOr metric states are reset to default values after calling the `compute()` method?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4874/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-11-27T06:12:39Z",
        "body": "`metric.compute()` resets the state. This is usually done after an epoch."
      }
    ]
  },
  {
    "number": 4711,
    "title": "How to monitor more than one quantity?",
    "created_at": "2020-11-17T12:41:15Z",
    "closed_at": "2020-11-18T00:18:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4711",
    "body": "What i do if i want to monitor more than one quantity?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4711/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-11-17T13:59:23Z",
        "body": "You can pass multiple `ModelCheckpoint` callbacks to the trainer callback list\r\n\r\n```python\r\nTrainer(callbacks=[ModelCheckpoint(monitor=\"a\"), ModelCheckpoint(monitor=\"b\")])\r\n```\r\n\r\nhowever, this is not fully supported and the saved checkpoints will contain the state for only one of the `ModelCheckpoint`s\r\n\r\nDuplicate of #2908"
      }
    ]
  },
  {
    "number": 4646,
    "title": "Loading samples to RAM with DDP.",
    "created_at": "2020-11-12T21:06:02Z",
    "closed_at": "2020-11-13T10:41:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4646",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'm facing an IO bottleneck that can be fixed with a custom `torch.utils.data.Dataset` that loads each sample to RAM. Then training goes fast as I don't need to read my samples (images) from disk (slow). Everything works well with when I'm using 1 GPU, but I'm a bit lost when I switch to multiple GPUs with DDP.\r\n\r\nDDP divides the samples to each GPU and I'm wondering when/where I should load my samples to RAM so that each process only loads its own partition of the samples?\r\n\r\n#### Code\r\n\r\n```\r\nclass RAMDataset(data.Dataset):\r\n    def __init__(self, paths,labels,transform):\r\n        \"\"\"Dataset that loads all samples to RAM.\"\"\"\r\n        self.paths = paths\r\n        self.labels = labels\r\n        self.transform = transform\r\n\r\n    def __len__(self):\r\n        return len(self.samples)\r\n\r\n    def load_to_RAM(self):\r\n        self.images = []\r\n        for path in self.paths:\r\n            with open(path, \"rb\") as f:\r\n                str_encode = f.read()\r\n                nparr = np.frombuffer(str_encode, np.uint8)\r\n                self.images.append(cv2.imdecode(nparr, cv2.IMREAD_COLOR))\r\n\r\n    def __getitem__(self, index):\r\n        # Run self.load_to_RAM() first!\r\n        image = self.transform(self.images[index])\r\n        label = self.labels[index]\r\n        return image, label\r\n```\r\n\r\n#### What have you tried?\r\n\r\nWith 1 GPU `self.load_to_RAM()` can be excecuted as soon as the Dataset has been created.\r\n\r\n```\r\ndataset = RAMDataset(paths,labels)\r\ndataset.load_to_RAM()\r\nloader = DataLoader(dataset,...)\r\ntrainer.fit(model,loader)\r\n```\r\n\r\nBut obviously this would load the samples `num_gpus` times to the RAM of the node.\r\n\r\nI quickly tried to call `self.train_dataloader.dataset.load_to_RAM()`on the hook `setup()` but got the following error...\r\n```\r\nAttributeError: '_PatchDataLoader' object has no attribute 'dataset'\r\n```\r\n..and I'm 99% this solution would also load all of the samples to RAM.\r\n\r\n#### Possible solution?\r\n\r\n1. Find out which process (which GPU and which node) is currently running.\r\n2. Get the allocated slice of the samples for this process.\r\n3. Load only that slice of the `self.paths` to RAM.\r\n\r\nTried to go through the source code but couldn't find out how I could implement this.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4646/comments",
    "author": "jopo666",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2020-11-13T00:27:32Z",
        "body": "The OS should automatically cache the dataset after the first epoch (assuming you have enough ram) so you shouldn't need to do this.\r\n\r\nI'm actually not sure if distributed sampler shards the same data/item on the same GPU through different epochs... but that's more of a pytorch question than a lightning one."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T07:55:29Z",
        "body": "I do have enough RAM if I manage to divide my data to multiple nodes. If the `BasicDataset()` below is cached after the first epoch wouldn't that mean only the labels and paths are cached? What I would want is that the loaded images are cached. I'll look into PyTorch and see if the same data/items are kept for each epoch.\r\n\r\n```\r\nclass BasicDataset(data.Dataset):\r\n    def __init__(self, paths,labels,transform):\r\n        self.paths = paths\r\n        self.labels = labels\r\n\r\n    def __getitem__(self, index):\r\n        image = load(self.paths[index]) # This is my bottleneck.\r\n        label = self.labels[index]\r\n        return image, label\r\n```"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-13T08:01:31Z",
        "body": "Ah you're doing cross node DDP, I was referring to single node above... I'm afraid I don't have much experience with multi node DDP.\r\n\r\nBut if each node gets the same subset of data every epoch, the OS should cache the files all the same (without needing `load_to_RAM`) after the first epoch since you have enough RAM."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T08:06:21Z",
        "body": "Would the actual loaded images be cached even though they are loaded inside `__getitem__()` and not just the whole `Dataset` object with only the `paths` to these files?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-13T08:29:08Z",
        "body": "Yes, this is not stored in your program/script memory but instead it's simply file caching at the system level.\r\n\r\nI use Ubuntu but this should be the same for other linux ditros unless some memory/caching settings have been changed/limited. You can look at the cache part of your ram usage in htop as your data is loaded into memory during the first epoch."
      }
    ]
  },
  {
    "number": 4607,
    "title": "How to change the Datamodule during training with a callback?",
    "created_at": "2020-11-10T15:59:21Z",
    "closed_at": "2020-12-13T17:55:10Z",
    "labels": [
      "question",
      "won't fix",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4607",
    "body": "#### What is your question?\r\nHow to change the Datamodule during training with a callback?\r\nMore details:\r\nI am looking for a way to reinitialized my Datamodule with different parameter, I am currently sending the height of my images as argument to my datamodule and I want to change this height at some point during training, the simple way is to call trainer.fit multiple times with different datamodules, but I am wondering is there a way to do this on callback, in the same way as you do when you change the optimizer or lr_scheduler?\r\n\r\n\r\nSomething similar to this:\r\n```\r\ndef on_train_epoch_start(self, trainer, pl_module):\r\n            sch = optim.lr_scheduler.StepLR(optimizer, 1, 0.96)\r\n            scheduler = {\r\n                'scheduler': sch,\r\n                'interval': interval,  # or 'step'\r\n                'monitor': 'train_loss',\r\n                'reduce_on_plateau': False,\r\n                'frequency': 1,\r\n            }\r\n            trainer.lr_schedulers = trainer.configure_schedulers([scheduler])\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4607/comments",
    "author": "MohammedAljahdali",
    "comments": [
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-12-13T17:55:10Z",
        "body": "Answer from @teddykoker:\r\n \r\n\r\n> I have done this using a callback:\r\n> ```\r\n> class Scheduler(pl.Callback):\r\n>     def _prepare_epoch(self, trainer, model, epoch):\r\n>         phase = ... \r\n>         trainer.datamodule.set_phase(phase)\r\n> \r\n>     def on_epoch_end(self, trainer, model):\r\n>         self._prepare_epoch(trainer, model, trainer.current_epoch + 1)\r\n> \r\n> class Data(pl.LightningDataModule):\r\n>     def set_phase(self, phase: dict):\r\n>         self.size = phase.get(\"size\", self.size)\r\n>         train_transforms = T.Compose(\r\n>             [\r\n>                 T.RandomResizedCrop(self.size, scale=(self.min_scale, 1.0)),\r\n>                 T.RandomHorizontalFlip(),\r\n>                 T.ToTensor(),\r\n>                 normalize,\r\n>             ]\r\n>         )\r\n>         self.train_ds = ImageFolder(self.train_dir, transform=train_transforms)\r\n> \r\n>        \r\n>     def train_dataloader(self):\r\n>         train_dl = DataLoader(\r\n>             self.train_ds,\r\n>             batch_size=self.batch_size,\r\n>             shuffle=True,\r\n>             num_workers=self.num_workers,\r\n>             pin_memory=True,\r\n>         )\r\n>         return train_dl\r\n> ```\r\n> Its important to note:\r\n> \r\n> 1. You can access your datamodule from a callback using trainer.datamodule\r\n> 2. In order to have train_dataloader(), val_dataloader() called every epoch, you must set reload_dataloaders_every_epoch=True in your trainer.\r\n\r\nThank you @teddykoker for the help. "
      }
    ]
  },
  {
    "number": 4604,
    "title": "How to load to from a checkpoint to same device when pretrained encoder was used",
    "created_at": "2020-11-10T14:35:57Z",
    "closed_at": "2020-11-12T12:17:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4604",
    "body": "## \u2753 Questions and Help \r\n\r\nI implemented a `ClassificationNet` (see below) that's using a pretrained encoder. After training, I'm trying to load it to CPU using `ClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\")`, but since `map_location` in `get_encoder` is `None`, the encoder tries to load to GPU. How can I inform `get_encoder` to load to the same `map_location`? \r\nSince I just started using Lightning, I guess there's a much smarter way to circumvent this situation altogether -- I look forward to your suggestions :) Thanks!\r\n\r\n#### Code\r\n``` python\r\nclass ClassificationNet(LightningModule):\r\n    ...\r\n    self.encoder = get_encoder(pretrained=True)\r\n\r\nget_encoder(pretrained=False, map_location=None):\r\n    model = FancyModel()\r\n    if pretrained:\r\n        ckpt_data = torch.utils.model_zoo.load_url(url, map_location=map_location)\r\n    ....\r\n```\r\n\r\n - OS: Manjaro Linux\r\n - Version 1.0.5",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4604/comments",
    "author": "hakanyi",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-11-11T20:50:04Z",
        "body": "one solution can be having an additional argument for it.\r\n```python\r\nclass ClassificationNet(LightningModule):\r\n    def __init__(self, encoder_map_location, ...):\r\n        self.encoder = get_encoder(pretrained=True, map_location=encoder_map_location)\r\n```\r\n```python\r\nClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\"), encoder_map_location=torch.device(\"cpu\"))\r\n```\r\n\r\nor maybe without an additional argument. Not sure about this one though. But should work.\r\n```python\r\nclass ClassificationNet(LightningModule):\r\n    def __init__(self, ...):\r\n        ....\r\n    \r\n    def setup(self, stage):\r\n        self.encoder = get_encoder(pretrained=True, map_location=self.device)\r\n```\r\n```python\r\nClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\"))\r\n```"
      }
    ]
  },
  {
    "number": 4465,
    "title": "How to save the latest and best checkpoint?",
    "created_at": "2020-11-01T10:25:10Z",
    "closed_at": "2020-11-01T12:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4465",
    "body": "I can set a checkpoing callback to save best model, but I also want it save the latest model, so that i can `resume_from_checkpoint` from latest checkpoint. how to do this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4465/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-01T11:56:07Z",
        "body": "@xiadingZ  There is `save_last` parameter in ModelCheckpoint that saves the last epoch"
      }
    ]
  },
  {
    "number": 4135,
    "title": "Accuracy metric number of classes warning question",
    "created_at": "2020-10-14T04:26:41Z",
    "closed_at": "2020-10-14T16:15:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4135",
    "body": "For context, I'm running a heavy multi-label classification model with small batches (~10 elements) and a large number of classes (~50 classes). While training I'm getting this warning on **accuracy** metric calculation:\r\n\r\n> You have set 32 number of classes which is different from predicted (6) and target (28) number of classes\r\n\r\nShould I be worried? I'm not sure why this should be a warning. \r\nI keep wondering why is it bad having fewer classes in predicted output compared with the expected number of classes.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4135/comments",
    "author": "Vichoko",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-14T11:19:44Z",
        "body": "Which version of lightning are you using?\r\nWe did a revamp of metrics lately and the class based `Accuracy` metric does not throw this warning anymore (functional still does).\r\nIt was originally put in place to warn users that we estimated a different number of labels (from pred and target) than what the user provided, and it could mean that your output was biased and not estimating values within the hole label space. That said, it does not necessarily means that there is anything wrong with your model."
      }
    ]
  },
  {
    "number": 3803,
    "title": "Access metrics in custom callbacks",
    "created_at": "2020-10-02T19:05:49Z",
    "closed_at": "2020-10-04T02:43:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3803",
    "body": "## \u2753 Questions and Help\r\n\r\nI have found it useful/helpful to sometimes access metrics in custom callbacks. In v0.9.0 this works using something like this:\r\n\r\n```\r\ndef training_step(self, batch, batch_idx):\r\n    return {\"loss\": self._step(batch)}\r\n\r\ndef validation_step(self, batch, batch_idx):\r\n    return {\"val_loss\": self._step(batch)}\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ...\r\n    return {\"interesting_key_train\": interesting_value}\r\n\r\ndef validation_epoch_end(self, outputs):\r\n    # ...\r\n    return {\"interesting_key_val\": interesting_value}\r\n```\r\n\r\nThe setup allows for the values returned in the `_epoch_end` methods to be accessed via `trainer.callback_metrics`. As such, a callback could use these values, e.g.\r\n\r\n```\r\nclass CustomCallback(Callback):\r\n\r\n    def on_validation_end(self, trainer, pl_module):\r\n        metrics = trainer.callback_metrics\r\n        interesting_value = metrics[\"interesting_key_train\"]\r\n```\r\n\r\nWhen using the current master branch, the above approach is possible for values returned in `validation_epoch_end` but no longer possible for `training_epoch_end` as setting a return value in `training_epoch_end` raises the exception,\r\n\r\n```\r\nMisconfigurationException: training_epoch_end expects a return of None. HINT: remove the return statement in training_epoch_end\r\n```\r\n\r\nAdditionally the values stored in `trainer.callback_metrics` have changed. Using the example above, in v0.9.0, it is `{\"loss\": ..., \"interesting_key_train\": ..., \"interesting_key_val\": ...}` and on master it is simply `{\"interesting_key_val\": ...}`.\r\n\r\nWhat is the intended way to access metrics (in particular from the training loop) in callbacks?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3803/comments",
    "author": "pbmstrk",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2020-10-03T22:32:34Z",
        "body": "> When using the current master branch, the above approach is possible for values returned in validation_epoch_end but no longer possible for training_epoch_end as setting a return value in training_epoch_end raises the exception,\r\n\r\nCan you use `self.log(\"interesting_key_train\", interesting_value)`? \r\n\r\nThough there does seem to be an issue with accessing metrics on epoch end on master @williamFalcon "
      },
      {
        "user": "pbmstrk",
        "created_at": "2020-10-03T23:32:48Z",
        "body": "`self.log` works on master, I had tried it earlier today and run into issues but these seem to be resolved now."
      }
    ]
  },
  {
    "number": 3698,
    "title": "How to keep some LightningModule's parameters on cpu when using CUDA devices for training",
    "created_at": "2020-09-28T11:46:05Z",
    "closed_at": "2020-10-18T08:56:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3698",
    "body": "## \u2753 Questions and Help\r\n\r\n\r\n#### What is your question?\r\nI tried to transform my code into Lightning yesterday, but the CUDA OOM error occurred. My model has a very large parameter ```nn.Embedding(24000000, 128)``` (more than 22GB), which obviously exceeds the memory of my CUDA device. I implemented two classes to sovle this problem in my torch_version code, the pseudo code is as follows:\r\n\r\n#### PyTorch Code\r\n```python\r\nclass Emb(nn.Module):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.emb = nn.Emebdding(24000000, 128)\r\n        def forward(self, idx):\r\n                return self.emb(idx)\r\n\r\nclass MyModule(nn.Module):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n        def forward(self, input):\r\n                out = self.calculation(input)\r\n                return out\r\n\r\n# train part:\r\nget_emb = Emb()\r\nmodel = MyModule()\r\nmodel = model.cuda()\r\noptimizer = some_optimizer([{\"params\": e.parameters}, {\"params\": model.parametersba}], lr=1e-3)\r\nloss_metric = some_loss()\r\nfor epo in epoch:\r\n        for x, y in dataloader:\r\n                embs = get_emb(x.cpu()).cuda()\r\n                out = model(embs)\r\n                loss = loss_metric(out, y)\r\n                optimizer.zero_grad()\r\n                loss.backward()\r\n                optimizer.step()\r\n```\r\nThe torch_version code above keeps the nn.Embedding on cpu and ensures that the optimization of training is completed on CUDA devices. But I don't know how to achieve this via pytorch_lightning, because the entire 'training' part is encapsulated in training_step. The PL code  is as follows:\r\n\r\n#### PL Code\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = nn.Embedding(24000000, 128)\r\n                self.loss_metric = some_loss()\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb(x)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n# train part\r\nmodel = MyModule()\r\ntrainer = pl.Trainer(gpus=-1)\r\ntrainer.fit(model, dataloader)\r\n```\r\nSo, is there any recommended way to keep a part of the LightningModule's parameters on cpu when using CUDA devices for training? \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 16.04.6 LTS\r\n - CUDA: version 10.2, 2080Ti\r\n - Version 0.9.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3698/comments",
    "author": "David-AJ",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-28T20:35:35Z",
        "body": "if you do this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = [nn.Embedding(24000000, 128)]\r\n                self.loss_metric = some_loss()\r\n        def forward(self, input):\r\n                x, y = input\r\n                embs = self.emb[0](x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n```\r\nit should work I guess. Can't think of a better solution than this :sweat_smile: "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-09-29T03:39:41Z",
        "body": "> if you do this:\r\n> \r\n> ```python\r\n> class MyModule(pl.LightningModule):\r\n>         def __init__(self):\r\n>                 xxxxxx # some init operations\r\n>                 self.calculation = some_calculation()\r\n>                 self.emb = [nn.Embedding(24000000, 128)]\r\n>                 self.loss_metric = some_loss()\r\n>         def forward(self, input):\r\n>                 x, y = input\r\n>                 embs = self.emb[0](x.cpu()).to(self.device)\r\n>                 out = self.calculation(embs)\r\n>                 return {\"loss\": self.loss_metric(out, y)}\r\n> ```\r\n> \r\n> it should work I guess. Can't think of a better solution than this \ud83d\ude05\r\n\r\n@rohitgr7 Really?! In this case, will the self.emb be saved in ckpt along with other parameters of ```MyModule```? Sorry, I just noticed that I had a typo in the PL Code: ```forward(self, input) -> training_step(self, batch, batch_idx)```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-29T07:48:51Z",
        "body": "Yeah, won't save. Didnt think of that. If any module in the lightning has a `.to` method then it will be moved to device. Somehow need to think of a way to override this `.to` method for embeddings."
      },
      {
        "user": "David-AJ",
        "created_at": "2020-09-30T03:23:40Z",
        "body": "@rohitgr7 I tried to use ```self.emb = nn.Embedding(24000000, 128).cpu()``` in lightning code, but it failed. Actually, it is very common in recommendation system to use this kind of large-scale embedding as the trainable weight of the model. For example, the sparse features of User Id (more than 24000000) can be represented by a dense embedding matrix. So is there any possible to implement this operation in Pytorch-Lightning? "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T20:35:33Z",
        "body": "Looked at PyTorch source code. Found something. Can you try this?  @David-AJ\r\n```python\r\n\r\nclass SpecialEmbedding(nn.Module):\r\n        def __init__(self, fin, fout):\r\n                self.emb = nn.Embedding(fin, fout)\r\n\r\n        def _apply(self, fn):\r\n                return self\r\n\r\n        def forward(self, x):\r\n                return self.emb(x)\r\n\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = SpecialEmbedding(24000000, 128)\r\n                self.loss_metric = some_loss()\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb(x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n# train part\r\nmodel = MyModule()\r\ntrainer = pl.Trainer(gpus=-1)\r\ntrainer.fit(model, dataloader)\r\n```"
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-09T06:50:50Z",
        "body": "@rohitgr7 Thanks for your kindly help! But this ```SpecialEmbedding ```code failed again \ud83d\ude05\r\nthe error message is as follows:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"debug.py\", line 742, in <module>\r\n    trainer.fit(model)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1064, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/accelerators/dp_backend.py\", line 97, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 844, in run_training_batch\r\n    self.hiddens\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1015, in optimizer_closure\r\n    hiddens)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1197, in training_forward\r\n    output = self.model(*args)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 70, in forward\r\n    \"them on device: {}\".format(self.src_device_obj, t.device))\r\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu\r\n```\r\nIt seems that pytorch_lightning forces the parameters of a module to be set on the same device?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-09T19:58:46Z",
        "body": "not 100% sure why is this `RuntimeError` should be raised. @awaelchli any suggestions on how to make this work/?\r\n\r\nActually now I also want to know if this is the right way or not or there is another way around since it seems super useful.\r\n\r\n@David-AJ is it working on a single GPU device with no distributed backend?"
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-10T03:35:22Z",
        "body": "> not 100% sure why is this `RuntimeError` should be raised. @awaelchli any suggestions on how to make this work/?\r\n> \r\n> Actually now I also want to know if this is the right way or not or there is another way around since it seems super useful.\r\n> \r\n> @David-AJ is it working on a single GPU device with no distributed backend?\r\n\r\nHi @rohitgr7, I tried to run this code on a single GPU and the ```RuntimeError``` was raised again."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-11T16:15:24Z",
        "body": "do you want to train the embedding layer or is it pretrained?\r\nIf you want to train it, I'm afraid you can't have it on cpu while also using DP. The error you got above is because DataParallel detects that. "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-12T02:58:01Z",
        "body": "@awaelchli Yes, I want to train it, how about using DDP or any other distributed backend? Actually @rohitgr7\u2018s first solution ```self.emb = [nn.Embedding(24000000, 128)]``` could make ```self.emb``` on CPU while training with DP, but in that case the ```self.emb``` won't be saved in the ckpt, nor can be loaded using load_from_checkpoint. Could this problem be solved by overriding the ```on_save_checkpoint``` and ```on_load_checkpoint```?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-12T03:01:40Z",
        "body": "> `self.emb = [nn.Embedding(24000000, 128)]`\r\n\r\n\ud83e\udd23  this is a funny trick. Very creative. Yeah, this makes torch unaware of this module, and keeps it on the cpu. \r\n\r\n> Could this problem be solved by overriding the on_save_checkpoint and on_load_checkpoint?\r\n\r\nYes, I think that would do the trick!\r\nBut will this this embedding layer not be a huge bottleneck? You will need to transfer all outputs to the GPU and this blocks execution."
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-12T03:18:03Z",
        "body": "@awaelchli \r\n\r\n> But will this this embedding layer not be a huge bottleneck? You will need to transfer all outputs to the GPU and this blocks execution.\r\n\r\nLooks like I have no choice\ud83e\udd23 otherwise I have to train all the module on the cpu, don't know which one could be faster. The application scenario is in the recommendation system, and in fact the number of users and items far exceeds 24 million, all these ID sparse features should be represented by the embedding layer and trained in the module. Do you have any other suggestions on how to make it faster?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-17T14:22:54Z",
        "body": "> Could this problem be solved by overriding the on_save_checkpoint and on_load_checkpoint?\r\n\r\nyes. I would do it this way. grab the state dict of the embedding layer and add it to the checkpoint dict. when loading, you do the opposite and read the state dict. \r\n\r\n> Do you have any other suggestions on how to make it faster?\r\n\r\nsorry, nothing comes to my mind :("
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-18T08:56:00Z",
        "body": "I modified my code like this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = [nn.Embedding(24000000, 128)]\r\n                self.loss_metric = some_loss()\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb[0](x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n        def on_save_checkpoint(self, checkpoint):\r\n                checkpoint[\"emb\"] = self.emb\r\n\r\n        def on_load_checkpoint(self, checkpoint):\r\n                self.emb = checkpoint[\"emb\"]\r\n```\r\nIt works! Thank you @rohitgr7 and @awaelchli!"
      }
    ]
  },
  {
    "number": 3696,
    "title": "How to set default EarlyStopping patience?",
    "created_at": "2020-09-28T06:21:06Z",
    "closed_at": "2020-09-28T20:03:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3696",
    "body": "Is it possible to set the default EarlyStopping patience without creating a custom early stopping callback? \r\n\r\nInstead of writing:\r\n```\r\ntrainer = pl.Trainer(early_stop_callback=EarlyStopping(patience=XXX))\r\n```\r\n\r\nI'd like to overwrite the default patience directly and then use EvalResult(early_stop_on=...). ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3696/comments",
    "author": "chrismaliszewski",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-28T19:45:19Z",
        "body": "No. Any problem with using a custom callback??"
      }
    ]
  },
  {
    "number": 3473,
    "title": "Correct way of implementing early stopping",
    "created_at": "2020-09-12T10:24:28Z",
    "closed_at": "2020-09-12T13:57:42Z",
    "labels": [
      "question",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3473",
    "body": "I am trying to implement early stopping on my LSTM classifier.Running the script on colab GPU environment. Here's the code\r\n\r\n```python\r\n!pip install pytorch-lightning torchtext \r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import random_split\r\nfrom torchtext import data, datasets\r\nfrom torch.nn.utils.rnn import pack_padded_sequence\r\nfrom pytorch_lightning.metrics import functional as FM\r\nfrom pytorch_lightning.callbacks import EarlyStopping\r\n\r\n#### set up fields\r\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\r\nLABEL = data.Field(sequential=False)\r\n\r\n##### make splits for data\r\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\r\n\r\n##### build the vocabulary\r\nTEXT.build_vocab(train)\r\nLABEL.build_vocab(train)\r\n\r\n#### make iterator for splits\r\ntrain_iter, test_iter = data.BucketIterator.splits(\r\n    (train, test), batch_size=100, device=0)\r\n\r\nclass LightningLSTM(pl.LightningModule):\r\n  def __init__(self, embedd_dim, hidden_size, output_dim, vocab_size, **kwargs):\r\n    super().__init__()\r\n    self.embedd_dim = embedd_dim\r\n    self.hidden_size = hidden_size\r\n    self.output_dim = output_dim\r\n    self.vocab_size = vocab_size\r\n    self.embedding = nn.Embedding(self.vocab_size, self.embedd_dim)\r\n    self.lstm = nn.LSTM(self.embedd_dim, self.hidden_size, batch_first=True, **kwargs)\r\n    self.linear = nn.Linear(self.hidden_size, self.output_dim)\r\n    self.softmax = nn.Softmax(1)\r\n\r\n  def forward(self, x, lengths):\r\n    output = self.embedding(x)\r\n    packed_output = pack_padded_sequence(output, lengths.cpu().numpy(), batch_first=True, enforce_sorted=False)\r\n    output, (ht, ct) = self.lstm(packed_output)\r\n    output = self.linear(ht).squeeze(0)\r\n    return output\r\n\r\n  def configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n    return optimizer\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    x, l, y = *batch.text, batch.label\r\n    y-=1\r\n    y_hat = self(x, l)\r\n    loss = F.cross_entropy(y_hat, y)\r\n    y_pred = torch.argmax(self.softmax(y_hat),1)\r\n    result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)\r\n    acc = FM.accuracy(y_pred, y, num_classes=self.output_dim)\r\n    result.log('val_loss', loss, prog_bar=True, on_step=True)\r\n    result.log('val_acc', acc, prog_bar=True, on_step=True)\r\n    return result\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    x, l, y = *batch.text, batch.label\r\n    y-=1\r\n    y_hat = self(x, l)\r\n    loss = F.cross_entropy(y_hat, y)\r\n    y_pred = torch.argmax(self.softmax(y_hat),1)\r\n    result = pl.EvalResult(checkpoint_on=loss)\r\n    acc = FM.accuracy(y_pred, y, num_classes=self.output_dim)\r\n    result = pl.TrainResult(minimize=loss)\r\n    result.log('loss', loss)\r\n    result.log('train_acc', acc)\r\n    return result\r\n\r\nEMBEDDING_DIM = 200\r\nVOCAB_SIZE = len(TEXT.vocab)\r\nOUTPUT_DIM = 2 #two labels - positive and negative\r\nHIDDEN_SIZE = 1024\r\nmodel = LightningLSTM(EMBEDDING_DIM, HIDDEN_SIZE, OUTPUT_DIM, VOCAB_SIZE)\r\n\r\nearly_stopping = EarlyStopping('val_loss', patience=3, mode='min')\r\ntrainer = pl.Trainer(gpus=1, max_epochs=10, early_stop_callback=early_stopping)\r\ntrainer.fit(model, train_iter, test_iter) `\r\n```\r\nThis is the output + warning I get when I start the training - \r\n\r\n\r\n``` \r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0]\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\r\n  warnings.warn(*args, **kwargs)\r\n\r\n  | Name      | Type      | Params\r\n----------------------------------------\r\n0 | embedding | Embedding | 50 M  \r\n1 | lstm      | LSTM      | 5 M   \r\n2 | linear    | Linear    | 2 K   \r\n3 | softmax   | Softmax   | 0     \r\nEpoch 5: 100%\r\n500/500 [02:07<00:00, 3.91it/s, loss=0.105, v_num=3, step_val_loss=1.06, step_val_acc=0.71, epoch_val_loss=0.748, epoch_val_acc=0.763]\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: \r\n                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the\r\n                    'monitor' key of EarlyStopping has no effect.\r\n                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')\r\n                \r\n  warnings.warn(*args, **kwargs)\r\nSaving latest checkpoint..\r\n\r\n1\r\n ```\r\n\r\nIf I don't specifically mention `early_stop_on = loss` in the EvalResult initialization (in the validation_step method), the trainer keeps training it for the max number of epochs specified. Also, I do not get the warning when I remove the `early_stop_on` parameter. Early stopping works fine when I include the parameter.\r\n\r\nI am confused about what is the right way to implement early stopping.  `early_stopping = EarlyStopping('val_loss', patience=3, mode='min')` this line seems to implement early stopping as well. But doesn't work unless I explicitly mention in the EvalResult object.\r\n\r\nCan anyone point out if I am missing something?\r\n\r\nThanks!\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3473/comments",
    "author": "DhruvilKarani",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-12T13:32:17Z",
        "body": "Yes, currently you need to do it this way. The result object was meant to make early stopping etc. easier, but currently if you use a custom callback, you need to add that key manually. We will iterate on this and are currently discussing ways to make configuration of these callbacks easier and more intuitive. part of discussion is here #3286 "
      }
    ]
  },
  {
    "number": 3009,
    "title": "how to show estimated total training time in progress bar?",
    "created_at": "2020-08-16T17:57:11Z",
    "closed_at": "2020-10-25T06:02:05Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3009",
    "body": "how to show estimated total training time in progress bar?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3009/comments",
    "author": "aiyolo",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-16T18:45:18Z",
        "body": "I once made a progress bar for myself which shows the total epochs and the estimated time. I'll share it here. \r\nYou can simply pass it to the Trainer like so:\r\n`Trainer(callbacks=[GlobalProgressBar()])`\r\nHowever, this will not show the old progress bar and we currently do not support multiple progress bar callbacks, but I will think of a solution for this.\r\nIn the meantime, I hope this is of any help. \r\n\r\n\r\n```python\r\nimport importlib.util\r\nimport sys\r\n\r\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\r\n\r\n# check if ipywidgets is installed before importing tqdm.auto\r\n# to ensure it won't fail and a progress bar is displayed\r\nif importlib.util.find_spec('ipywidgets') is not None:\r\n    from tqdm.auto import tqdm\r\nelse:\r\n    from tqdm import tqdm\r\n\r\n\r\nclass GlobalProgressBar(ProgressBarBase):\r\n\r\n    def __init__(self, process_position: int = 0):\r\n        super().__init__()\r\n        self._process_position = process_position\r\n        self._enabled = True\r\n        self.main_progress_bar = None\r\n\r\n    def __getstate__(self):\r\n        # can't pickle the tqdm objects\r\n        state = self.__dict__.copy()\r\n        state['main_progress_bar'] = None\r\n        return state\r\n\r\n    @property\r\n    def process_position(self) -> int:\r\n        return self._process_position\r\n\r\n    def disable(self) -> None:\r\n        self._enabled = False\r\n\r\n    def enable(self) -> None:\r\n        self._enabled = True\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n        super().on_train_start(trainer, pl_module)\r\n        self.main_progress_bar = tqdm(\r\n            desc='Total Epochs',\r\n            initial=trainer.current_epoch,\r\n            total=trainer.max_epochs,\r\n            position=(2 * self.process_position),\r\n            disable=False,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n        )\r\n\r\n    def on_train_end(self, trainer, pl_module):\r\n        self.main_progress_bar.close()\r\n\r\n    def on_epoch_end(self, trainer, pl_module):\r\n        self.main_progress_bar.update(1)\r\n\r\n```"
      },
      {
        "user": "zxvix",
        "created_at": "2022-09-06T10:08:13Z",
        "body": "I'll share my solution in case anyone finds it useful. It simply adds a total remaining time field in the current progress bar. Use it by replacing `RichProgressBar` with `BetterProgressBar`.\r\n```python\r\nimport re\r\nfrom datetime import timedelta\r\nfrom typing import Union\r\n\r\nfrom pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\r\nfrom rich.progress import ProgressColumn\r\nfrom rich.style import Style\r\nfrom rich.text import Text\r\n\r\nclass RemainingTimeColumn(ProgressColumn):\r\n    \"\"\"Show total remaining time in training\"\"\"\r\n\r\n    max_refresh = 1.0\r\n\r\n    def __init__(self, style: Union[str, Style]) -> None:\r\n        self.style = style\r\n        self.estimated_time_per_epoch = None\r\n        super().__init__()\r\n\r\n    def render(self, task) -> Text:\r\n        if 'Epoch' in task.description:\r\n            # fetch current epoch number from task description\r\n            m = re.search(r'Epoch (\\d+)/(\\d+)', task.description)\r\n            current_epoch, total_epoch = int(m.group(1)), int(m.group(2))\r\n\r\n            elapsed = task.finished_time if task.finished else task.elapsed\r\n            remaining = task.time_remaining\r\n            \r\n            if remaining:\r\n                time_per_epoch = elapsed + remaining\r\n                if self.estimated_time_per_epoch is None:\r\n                    self.estimated_time_per_epoch = time_per_epoch\r\n                else:\r\n                    # smooth the time_per_epoch estimation\r\n                    self.estimated_time_per_epoch = 0.99 * self.estimated_time_per_epoch + 0.01 * time_per_epoch\r\n\r\n                remaining_total = self.estimated_time_per_epoch * (total_epoch - current_epoch - 1) + remaining\r\n\r\n                return Text(f\"{timedelta(seconds=int(remaining_total))}\", style=self.style)\r\n            \r\n        else:\r\n            return Text(\"\")\r\n\r\n\r\nclass BetterProgressBar(RichProgressBar):\r\n    def configure_columns(self, trainer) -> list:\r\n        columns = super().configure_columns(trainer)\r\n        columns.insert(4, RemainingTimeColumn(style=self.theme.time))\r\n        return columns\r\n```"
      }
    ]
  },
  {
    "number": 2984,
    "title": "CrossEntropyLoss with weights",
    "created_at": "2020-08-15T02:46:24Z",
    "closed_at": "2020-08-15T09:49:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2984",
    "body": "I need weights in CrossEntropyLoss (actually multiple, but the same issue).  The documentation talks about tensors copied from other tensors, but there is no tensor to copy from in the init.  So I'm stuck.\r\nTo make the weights unquestionably simple, I use ones.\r\n\r\n```\r\nclass JJG_Transformer(pl.LightningModule):\r\n\r\n    def __init__(self, alphanet_plus_2, letter_weights_per_position):\r\n        super(JJG_Transformer, self).__init__()\r\n        self.criterions = []\r\n        for weight in self.letter_weights_per_position:\r\n            weight = torch.ones((94))\r\n            self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n    def validation_step(self, batch, batch_idx):\r\n        batch_im, batch_true_value_NT, batch_letter_transformer_input = batch\r\n        out_NTA = self(batch_im, batch_letter_transformer_input)\r\n        loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n        loss1 = self.criterions[1](out_NTA[:,1,:], batch_true_value_NT[:,1])\r\n        loss = loss0 + loss1\r\n        tensorboard_logs = {'val_loss': loss, 'val_loss0': loss0, 'val_loss1':loss1}\r\n        return {'val_loss': loss, 'log': tensorboard_logs}\r\n\r\n```\r\n\r\n  ```\r\nFile \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py\", line 254, in <module>\r\n    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1017, in fit\r\n    self.accelerator_backend.train(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 56, in train\r\n    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 219, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1196, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1229, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 325, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 609, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 160, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py\", line 128, in validation_step\r\n    loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\", line 948, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward\r\nTraceback (most recent call last):\r\n  File \"kiss_transformer.py\", line 254, in <module>\r\n    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1030, in fit\r\n    results = self.accelerator_backend.spawn_ddp_children(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 118, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, mp_queue=None, model=model, is_master=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 219, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1196, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1229, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 325, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 609, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 160, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"kiss_transformer.py\", line 128, in validation_step\r\n    loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\", line 948, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward\r\n```\r\n\r\n```\r\ntrainer = pl.Trainer( gpus=[0, 1],  \r\n                accumulate_grad_batches=16, \r\n                max_epochs=500, \r\n                check_val_every_n_epoch=1, \r\n                distributed_backend='ddp', \r\n```\r\n\r\npl__version__ 0.9.0rc12\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2984/comments",
    "author": "johngrabner",
    "comments": [
      {
        "user": "sykrn",
        "created_at": "2020-08-15T06:57:00Z",
        "body": "I think you just need to use `.cuda()` for the weight or criterion."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-15T09:49:13Z",
        "body": "@sykrn is on the right track. \r\nThe problem is that your criterion is in a list. nn.Module does not recognize submodules in lists. \r\nYou solve your problem by using ModuleList: \r\n\r\n```python \r\n\r\nself.criterions = nn.ModuleList()  # this is the fix\r\n\r\nfor weight in self.letter_weights_per_position:\r\n    weight = torch.ones((94))\r\n    self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n```\r\nnow your criterions (and tensors within it) will be automatically moved to the right device!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-15T22:31:32Z",
        "body": "It means:\r\n\r\n```python\r\nclass Wrong(nn.Module):\r\n\r\n    def __init__(self)\r\n        super().init()\r\n        self.list = [CrossEntropyLoss()]\r\n\r\nw = Wrong()\r\nw.children() # no children\r\n```\r\nvs.\r\n```python\r\nclass Right(nn.Module):\r\n\r\n    def __init__(self)\r\n        super().init()\r\n        self.list = nn.ModuleList([CrossEntropyLoss()])\r\n\r\nr = Right()\r\nr.children() # correctly returns the cross entropy module\r\n```\r\n\r\nOperations like model.to('cuda') will also recursively operate on all submodules (children and their children etc.). But a Python list for example is not considered a submodule, because it's not an instance of nn.Module"
      }
    ]
  },
  {
    "number": 2928,
    "title": "is limit_train_batches shuffle or random",
    "created_at": "2020-08-12T08:13:07Z",
    "closed_at": "2020-08-13T10:30:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2928",
    "body": "hi, I am using limit_train_batches . If it is set, is it means a subdataset of whole train dataset ? similar with torch.utils.data.random_split",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2928/comments",
    "author": "qmpzzpmq",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-08-12T09:09:10Z",
        "body": "Yes, it is a subset of the train dataset\r\nBut, it doesn't similar with `random_split`"
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2020-08-12T10:08:17Z",
        "body": "@ydcjeff I mean, is it random?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-08-12T10:30:59Z",
        "body": "I think it is not random. It is the first `limit_train_batches` of the train dataset."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-12T15:20:47Z",
        "body": "Yes exactly, @ydcjeff is right. It will fetch batches from the dataloader until it reaches that amount, so your dataset and dataloader settings regarding shuffling will be respected. "
      },
      {
        "user": "adosar",
        "created_at": "2024-03-24T20:10:31Z",
        "body": "> @awaelchli @ydcjeff thx\r\n\r\nWhat if the dataloader uses `shuffle == True`?"
      }
    ]
  },
  {
    "number": 2888,
    "title": "Understanding the Progress Bar",
    "created_at": "2020-08-08T14:01:20Z",
    "closed_at": "2020-08-08T14:42:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2888",
    "body": "I train on MNIST with data loaders defined below (full train / test sets with `batch_size=128`).  \r\n`'val_check_interval': 0.1`, so per training epoch, I have 10 validation runs.  \r\n\r\nNow:\r\n- 10000 (test) images / 128 (batch_size) = 78.125, so steps such as 54/79 do make sense.  \r\n- 60000 (train) images / 128 (batch_size) = 468.75, so I'd expect something like 120/469.  \r\n\r\nWhat is the \"1259\" representing in the progress bar? I can observe in tensorboard, that the epoch number goes up at exactly 459.\r\n```\r\nValidating:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 54/79 [00:08<00:03,  6.57it/s]\r\nEpoch 4:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 976/1259 [04:01<01:09,  4.05it/s, loss=19279.273, v_num=0]\r\n```\r\n\r\n#### Code\r\n##### Data Loaders\r\n```python\r\n    def train_dataloader(self) -> DataLoader:\r\n        \"\"\"Pytorch-lightning function.\"\"\"\r\n        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\r\n                                                    torchvision.transforms.ToTensor()])\r\n        train_set = torchvision.datasets.MNIST(root=DATA_DIR_PATH / 'mnist_data',\r\n                                               train=True,\r\n                                               download=True,\r\n                                               transform=transform)\r\n        return DataLoader(train_set,\r\n                          batch_size=128,\r\n                          shuffle=True,\r\n                          num_workers=0)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        \"\"\"Pytorch-lightning function.\"\"\"\r\n        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\r\n                                                    torchvision.transforms.ToTensor()])\r\n        val_set = torchvision.datasets.MNIST(root=DATA_DIR_PATH / 'mnist_data',\r\n                                             train=False,\r\n                                             download=True,\r\n                                             transform=transform)\r\n        return DataLoader(val_set,\r\n                          batch_size=128,\r\n                          shuffle=False,\r\n                          num_workers=0)\r\n```\r\n#### What's your environment?\r\n - OS: Ubuntu 20.04\r\n - Packaging: pipenv\r\n - Lightning Version: 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2888/comments",
    "author": "matthaeusheer",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-08-08T14:16:28Z",
        "body": "It's 79*10 (10 times validation per epoch) + 469(train batches) = 1259. The progress bar contains both train and validation steps."
      }
    ]
  },
  {
    "number": 2666,
    "title": "Plotting learning rate from a lr_scheduler via a Callback",
    "created_at": "2020-07-21T23:00:50Z",
    "closed_at": "2020-08-05T07:35:41Z",
    "labels": [
      "feature",
      "good first issue",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2666",
    "body": "I think the title explains a lot. But let me elaborate, I have a LightningModule which has a configure_optimizers method returns an optimizer and a scheduler. Later in a Callback I have a `on_batch_end` function in which I try to log the learning rate.\r\n\r\nOf course if the scheduler was accessible as a class member, we could `self.scheduler.get_lr()` on it and use the value to plot. Since this is not how it has been implemented, I am wondering how to do this?\r\n\r\nWould appreciate any pointers.\r\nPytorchLightning - 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2666/comments",
    "author": "bhashithe",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2020-07-22T00:29:35Z",
        "body": "If you have the same lr throughout the network (single param group) you can get it from:\r\n`self.trainer.optimizers[0].param_groups[0]['lr']`\r\nchange the indexing based on your optimizer and param configuration."
      },
      {
        "user": "bhashithe",
        "created_at": "2020-07-22T01:04:56Z",
        "body": "That worked, even if i have multiple groups does it work the same if I do something like this?\r\n\r\n`{f'lr_group{i}': param['lr'] for i, param in enumerate(self.trainer.optimizers[0].param_groups}`"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-22T04:49:59Z",
        "body": "There is a `LearningRateLogger` callback in lightning."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-08-05T05:58:02Z",
        "body": "As @rohitgr7 mention, the `LearningRateLogger` which can be imported as `from pytorch_lightning.callbacks import LearningRateLogger` should be able to do what you ask for."
      }
    ]
  },
  {
    "number": 2404,
    "title": "Will load_from_checkpoint load Huggingface models as well? ",
    "created_at": "2020-06-28T20:12:31Z",
    "closed_at": "2020-07-01T09:13:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2404",
    "body": "#### What is your question?\r\n\r\nJust wanted to know will using the `load_from_checkpoint` for a `LightningModule` load the state_dict for the **HuggingFace** models as well?\r\n\r\nEg: for the given example in the docs, will state_dict be loaded for `BertModel.from_pretrained` thing as well? \r\nIdeally, `load_from_checkpoint` should load state_dict for Bert as well like `BertModel.from_pretrained(same_checkpoint)` would do.\r\n\r\n#### Code\r\n```\r\nclass BertMNLIFinetuner(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)\r\n        self.W = nn.Linear(bert.config.hidden_size, 3)\r\n        self.num_classes = 3\r\n\r\n\r\n    def forward(self, input_ids, attention_mask, token_type_ids):\r\n\r\n        h, _, attn = self.bert(input_ids=input_ids,\r\n                         attention_mask=attention_mask,\r\n                         token_type_ids=token_type_ids)\r\n\r\n        h_cls = h[:, 0]\r\n        logits = self.W(h_cls)\r\n        return logits, attn\r\n``` ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2404/comments",
    "author": "vibhavagarwal5",
    "comments": [
      {
        "user": "dscarmo",
        "created_at": "2020-06-28T21:26:32Z",
        "body": "When working with the Hugging Face library i just store the model string (in this case best-base-cased) as an hparam."
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-28T22:00:54Z",
        "body": "Sure, that is fine. I think you misunderstood my question. Will the state_dict from the checkpoint (using `load_from_checkpoint`) load the bert model as well or not? Or will I have to load it explicitly using `model.from_pretrained(checkpoint_path)` ?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-28T22:17:10Z",
        "body": "I don't know this Bert stuff , but if it helps:\r\n`Model.load_from_checkpoint()` will init your model with the args and kwargs from the checkpoint and then call `model.load_state_dict` to load the model weights as you would do in pure PyTorch.\r\nSo, if your` self.bert` is a `nn.Module` , then that will have the parameters loaded as well. "
      },
      {
        "user": "dscarmo",
        "created_at": "2020-06-28T22:20:43Z",
        "body": "Oh now i understand. The updated weights of your whole trained lightning module (including bert since it is a nn.Module) will be loaded. Now I am just wondering if init's `from_pretrained`  will overwrite your trained weights? \r\n\r\nI hope PL will only load the trained weights after init runs."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-28T22:33:56Z",
        "body": "yes, so the from_pretrained will run first and load your pretrained weights (since it's in the init). and then the load_state dict will overwrite them again with the weights from the checkpoint. \r\n\r\nI encourage you to make a sanity check and not blindly believe me :), for example, you could load your checkpoint manually and replace all weights with ones and then save it again. Then load the checkpoint again using pytorch lightning and print the weights of the loaded model."
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T04:52:30Z",
        "body": "My guess is the same that after from_pretrained weights are loaded, checkpoints weights override them.  Which makes sense. "
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T04:56:17Z",
        "body": "Another doubt, do the checkpoints save all sorts of callbacks as well? Coz I don't see it in the dict keys when I do torch.load(chckpoint). "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-29T05:04:13Z",
        "body": "we do that for early stopping and model checkpoint (see v0.8.2), but not for your custom callbacks. We recently discussed how to do that, here is an open discussion #2401 . "
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T05:17:51Z",
        "body": "v.0.8.2 is not released yet right? "
      }
    ]
  },
  {
    "number": 2370,
    "title": "Access the logging directory through LightningModule or Trainer",
    "created_at": "2020-06-26T09:25:06Z",
    "closed_at": "2020-06-27T12:26:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2370",
    "body": "Is there a way to access the current logging directory (e.g., lightning_logs/version_x)? I've searched the documentation and the source code but haven't found a solution yet.\r\n\r\nI want to save some intermediate raw tensors to that directory.\r\n\r\nThanks,\r\nDavid",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2370/comments",
    "author": "DavidRuhe",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-06-26T20:35:57Z",
        "body": "For tensorboard I think you can use `self.logger.log_dir`, not sure about others. I think this property should be present for all the available loggers if possible."
      }
    ]
  },
  {
    "number": 2310,
    "title": "how to train a network that doesn't require any training data",
    "created_at": "2020-06-21T22:46:58Z",
    "closed_at": "2020-06-23T18:46:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2310",
    "body": "The Wake-Sleep algorithm doesn't require any data during the sleep phase (effectively it generates it's own data). pytorch-lightning, however, appears to require a `train_dataloader()` method.\r\n\r\nThe only way I have to make pytorch-lightning run at all (for this admitted unusual case) is to specify some dummy dataset in `train_dataloader`, and then to ignore the data that gets passed to `training_step`. But I don't like that cycles are spent iterating through irrelevant data then. Is there a more elegant way?\r\n\r\nI considered defining my own custom `DataLoader` that returns the simulated data that the sleep phase uses, but this started seeming like even more of a hack than the previous solution. After all, my \"dataloader\" doesn't load any data; it effectively generates new data every \"epoch\". It's seems unnatural to split the sleep phase updates in this way.\r\n\r\nIs there a more straightforward way in lightning to train a network that doesn't require any data? Thanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2310/comments",
    "author": "jeff-regier",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-23T17:00:36Z",
        "body": "Why is it not possible for you to write a Dataset class that generates the data? You mention you already have code for generating the data, so why not wrap it into a Dataset class?"
      },
      {
        "user": "jeff-regier",
        "created_at": "2020-06-23T18:46:56Z",
        "body": "I had been concerned that the Dataset class would shuffle the data unnecessarily, thus slowing down training. But an `IterableDataset` seems to work: the `__iter__` method is overridden to return an iterator that yields a whole batch at once. Maybe this is the best way."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-23T18:53:43Z",
        "body": "No, that is simply not true. The Dataloader is doing the batching and shuffling, not the dataset. Besides, shuffling does not slow training down. If your dataset has an undefined or infinite length, use the IterableDataset, otherwise use the regular Dataset class. Once you have that, just pass it to the Dataloader. All of this is regular PyTorch :)"
      }
    ]
  },
  {
    "number": 2308,
    "title": "How to make inference right",
    "created_at": "2020-06-21T19:30:23Z",
    "closed_at": "2020-06-24T22:31:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2308",
    "body": "Hello everyone. I'm new to pytorch-lightning, but already excited with this framework. It's very convenient to train my models using lightning. Now my usecase is: I have trained my model and want to do inference on my test data and get results (for example, in csv format). I'd like to do my inference pytorch-lightning-way. What is the best practice to do it?   \r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n#### What's your environment?\r\n\r\n - OS: [Linux]\r\n - Packaging [pip]\r\n - Version [0.8.1]\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2308/comments",
    "author": "Podidiving",
    "comments": [
      {
        "user": "versatran01",
        "created_at": "2020-06-24T16:10:14Z",
        "body": "My understanding is that you just load the module and call freeze() on it and use it as any nn.Module"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T19:50:09Z",
        "body": "You don't need to freeze(). Just run with `torch.no_grad()` or set `torch.set_grad_enabled(False)`."
      },
      {
        "user": "Podidiving",
        "created_at": "2020-06-24T20:01:59Z",
        "body": "I think, one of the coolest features of lightning is that your don\u2019t need to specify your device (devices)\nIf you using LightningModule as plain nn.Module you should transfer your model and batches on devices manually, am I right? \nSo, I have my trained model. I\u2019d like to make inference on test data. I can define test_step and aggregate results in on_test_epoch_end, but I cannot run Trainer without train stage. Can I get around this somehow?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T20:08:49Z",
        "body": "`test_step` and `test_epoch_end` are called with `trainer.test()`. AFAIK, these methods are used just to evaluate your test dataset and not return any logits. If you want to do just this evaluation, you don't have to do the transfer of model or batches. But I think if you want to get the logits from the model you need to do it manually just like a vanilla PyTorch model."
      },
      {
        "user": "Podidiving",
        "created_at": "2020-06-24T20:16:58Z",
        "body": "Ok, thanks, didn\u2019t know about `test` method! I think, if you want to make some kind of submission data, you can return logits from `test_step` method in dict object, and then aggregate and save them in `test_epoch_end`  "
      }
    ]
  },
  {
    "number": 2263,
    "title": "Full batch training",
    "created_at": "2020-06-19T06:19:52Z",
    "closed_at": "2020-06-19T12:27:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2263",
    "body": "## \u2753 Questions and Help\r\n\r\nFor smaller datasets, it makes sense to do full batch training, not minibatch. How do you implement fullbatch training in pytorch lightning, given that train and validation might be different sizes?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2263/comments",
    "author": "turian",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T06:28:02Z",
        "body": "what do you mean by full batch training?\r\n\r\nThis is not lightning specific though... this is pytorch\r\n\r\n```\r\nDataloader(..., batch_size=1)\r\nDataloader(..., batch_size=32)\r\nDataloader(..., batch_size=len(dataset))\r\n```"
      }
    ]
  },
  {
    "number": 2179,
    "title": "train_percent_check as a method for reducing train data size.",
    "created_at": "2020-06-14T09:54:51Z",
    "closed_at": "2020-06-15T17:27:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2179",
    "body": "## \u2753 Question\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nWhen we use `train_percent_check=x` while training does it uses the same x per cent of data in every epoch?\r\nCan it be used as a method for making total data size short during training? \r\n\r\n#### Code\r\n\r\n`Trainer(gpus=4,max_epochs=20,train_percent_check=.2,log_gpu_memory=True,weights_summary=None)`\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2179/comments",
    "author": "Nilanshrajput",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-15T05:34:43Z",
        "body": "> When we use train_percent_check=x while training does it uses the same x per cent of data in every epoch?\r\n\r\nYes, if you set shuffle=False in your dataloader, otherwise it will sample a different subset of data every epoch.\r\n\r\n> Can it be used as a method for making total data size short during training?\r\n\r\nYes, that's what it is meant for. You can for example use it to sanity check that your model can overfit to the data.\r\n\r\n"
      },
      {
        "user": "Nilanshrajput",
        "created_at": "2020-06-15T10:39:28Z",
        "body": "What I meant was, even id don't put shuffle=False, defining a train_percent should shuffle into that first x per cent of training data every epoch. So essentially it's like our training data is cut off to x per cent, and everything else remain the same."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-15T10:45:30Z",
        "body": "Yes exactly, `train_percent_check=0.5` means use 50% of the dataset in the train_dataloader, for example. And this will be 50% every epoch."
      }
    ]
  },
  {
    "number": 2102,
    "title": "Validation step metrics not logged",
    "created_at": "2020-06-07T05:28:55Z",
    "closed_at": "2020-08-16T18:30:40Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2102",
    "body": "## \u2753 Questions and Help\r\n\r\nIt seems like data output in the `validation_step` does not get logged to tensorboard, it needs to be aggregated first in the `validation_epoch_end`, which is not the case for `training_step`.\r\n\r\nThe below would only show `val_loss` and only aggregated but all of `mae`, `mape` etc from every iteration.\r\n\r\nAs a workaround I could explicitly log, but how do I get the current iteration in the callbacks I only see how to get the current epoch.\r\n\r\n```\r\n    def step(self, y_hat, y, mode='train'):\r\n        loss = F.mse_loss(y_hat, y)\r\n        mae = F.l1_loss(y_hat, y)\r\n        mape = median_absolute_percentage_error(y_hat, y)\r\n        r2 = r2_score(y_hat, y)\r\n        out = {'loss': loss, 'mae': mae, 'mape': mape, 'R2': r2}\r\n        if mode=='train':\r\n            out['log'] = out.copy()\r\n            return out\r\n        elif mode =='val':\r\n            out = {f'{mode}_{k}': v for k,v in out.items()}\r\n            out['log'] = out.copy()\r\n            return out\r\n        else:\r\n            raise Exception('Unsupported mode')\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        return self.step(y_hat, y, 'val')\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        return {'val_loss': avg_loss, 'log': tensorboard_logs}\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2102/comments",
    "author": "feribg",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T18:28:38Z",
        "body": "you can get the current step with `self.global_step` or self.trainer.global_step\r\n\r\nyes, it's true you can manually log it yourself, but there is a reason why we don't log each step of the validation, because the loggers (all as far as I know) use a global step for logging, and this means if your training epoch has n bathces, and your validation has m batches, after the first epoch you will log\r\nn + m steps for training + validation and then your training loss will contingue with step n+m+1 instead of n+1 for epoch 2. You will see a  big jump in the visualization.\r\n\r\nTensorBoard is too limited, you cannot set the abscissa to anything else than the step (as far as I know). Therefore logging validation step makes no sense."
      },
      {
        "user": "feribg",
        "created_at": "2020-06-07T22:34:09Z",
        "body": "To your point re the step # being inconsistent, I'm not sure why that would be the case when the `val` metrics are always prefixed with `val_` so you will get `m` number of `val_...` metric for each validatio epoch, right? I'm struggling to understand where the mixup between train and valid comes into play when they are namespaced differently  and stored as different metrics?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T22:53:09Z",
        "body": "epoch 1 starts\r\nbatch 1: train_loss = ... gloabal_step = 1\r\nbatch 2: train_loss = ... gloabal_step = 2\r\nbatch 3: train_loss = ... gloabal_step = 3\r\nvalidation starts\r\nbatch 1: val_loss = ... gloabal_step = 4\r\nbatch 2: val_loss = ... gloabal_step = 5\r\nepoch 2 starts\r\nbatch 1: train_loss = ... **gloabal_step =6**\r\nbatch 2: train_loss = ... **gloabal_step = 7**\r\nbatch 3: train_loss = ... **gloabal_step = 8**\r\n\r\ntry it yourself :)\r\n\r\nthis is a limitation of the loggers, which cannot log to the past history, only at the current global step."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T23:08:38Z",
        "body": "why do you need to log every validation step anyway? it's not going to tell you much about your validation results"
      }
    ]
  },
  {
    "number": 2045,
    "title": "2 optimizers : skip updates for the second optim",
    "created_at": "2020-06-01T22:16:04Z",
    "closed_at": "2020-06-02T19:00:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2045",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI have a model with 2 optimizers : one for the backbone weight and second for \"backbone support\" weights. However, 2nd optimizer should only accumulate grads through the whole epoch and perform one update at the epoch end. \r\n\r\nLightning keeps asking for an output for the 2nd optimizer, but there is nothing to output in addition to the first optimizer results. How can I bypass this ?\r\n\r\n#### Code\r\nHere are defined the training_step and optimizer_step functions to illustrate my issue.\r\n```\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            images, target = batch\r\n            output = self(images)\r\n            loss_val = F.cross_entropy(output, target)\r\n            acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\r\n            weight_cons, act_cons, weight_p_c_cons = self.normalized_consumption()\r\n\r\n            tqdm_dict = {'Loss/train_loss': loss_val, \r\n                        'Acc/acc1': acc1,\r\n                        'Acc/acc5': acc5,}\r\n            output = OrderedDict({\r\n                'loss': loss_val,\r\n                'Loss/loss': loss_val,\r\n                'Acc/acc1': acc1,\r\n                'Acc/acc5': acc5,\r\n                'progress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n     if optimizer_idx == 1:\r\n         # Do nothing ?\r\n\r\ndef optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\r\n        if optimizer_i == 0:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n        # update 2nd optimizer at the end of the epoch\r\n        if optimizer_i == 1 and self.__nbbatch -1 <= batch_nb:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI tried to pass an empty output dict in training_step if the optimizer_idx == 1. However, Lightning complains that the dict is empty (`trainer/logging.py:106` -> `Nonetype has no attribute 'items'`)\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging pip\r\n - Version 0.7.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2045/comments",
    "author": "sebastienwood",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-01T22:18:20Z",
        "body": "override the backward pass. \r\ncheck the current batch idx and don\u2019t apply the backward step until the end of the epoch"
      },
      {
        "user": "sebastienwood",
        "created_at": "2020-06-02T19:00:01Z",
        "body": "I managed to make it works thanks :) \r\nI had to also return a dummy `loss` for the 2nd optimizer that lightning really wanted to have (as usual in an ordered dict)\r\n\r\nAs a side note, a strange issue (that has probably nothing to do with Lightning) is that a custom function stopped receiving the backward call when using two optimizers. "
      }
    ]
  },
  {
    "number": 1988,
    "title": "Stopping the code along with a graceful shutdown.",
    "created_at": "2020-05-28T16:12:51Z",
    "closed_at": "2020-05-29T07:27:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1988",
    "body": "##\r\nIs there a way to stop the training in the model when some criteria are satisfied. Something along the lines:\r\n```\r\nclass myCallback(Callback):\r\n    def __init__(self):\r\n        ...\r\n    def on_epoch_end(self, trainer, pl_module):\r\n        if criteria:\r\n            model.stop_training = True # stops the training; need help here\r\n```\r\nNote that I also want to have the early stopping feature where the 'val_loss' is monitored but want to stop running the code if some other criteria is satisfied. Also, is my method of having this feature in the callback module correct or should I inherit the early stopping criteria?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1988/comments",
    "author": "nsidn98",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:44:58Z",
        "body": "You could raise KeyboardInterrupt, this will lead to a graceful shutdown. There is some work in progress in this PR:\r\n#1631 \r\nBut it should already work if you raise it from within your code"
      }
    ]
  },
  {
    "number": 1982,
    "title": "Is there a way to make the Trainer skip loading optimizer from a checkpoint?",
    "created_at": "2020-05-28T12:01:42Z",
    "closed_at": "2020-05-29T10:41:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1982",
    "body": "Use case is if I want to load a model's weights but reset the the optimizer state. Is there a flag I can pass that skips loading the optimizer?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1982/comments",
    "author": "nihalsid",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:50:59Z",
        "body": "There is no flag but you could implement the model hook on_load_checkpoint and reset the optimizer yourself."
      },
      {
        "user": "colllin",
        "created_at": "2021-10-19T01:20:30Z",
        "body": "Can you clarify how to accomplish this?  I'd need to save a reference to the optimizer within `configure_optimizers`, e.g. `self.optim = whatever; return self.optim`, right?  Then in `on_load_checkpoint`, I'd... need to manually clear the optimizer state?  Is that easy to do?  If I'm understanding correctly, I can't simply re-init a fresh optimizer, since I already returned it from `configure_optimizers` and a reference was saved somewhere."
      },
      {
        "user": "colllin",
        "created_at": "2021-10-19T01:29:17Z",
        "body": "I think you can accomplish it this way without needing to know any of the optimizer's implementation details:\r\n```\r\n    # in your lightning module...\r\n    def on_load_checkpoint(self, checkpoint):\r\n        fresh_optimizer = Adam(...)\r\n        self.my_optimizer.load_state_dict(fresh_optimizer.state_dict())\r\n```"
      },
      {
        "user": "colllin",
        "created_at": "2021-10-19T01:52:47Z",
        "body": "I think I was confused about the difference between `Trainer(resume_from_checkpoint=\"...\")` vs. `LitModule.load_from_checkpoint(\"...\")`, and the docs could probably be improved here.\r\n\r\nIt appears that `LightningModule.load_from_checkpoint` doesn't even instantiate your optimizers, because only the `Trainer` does that.  Therefore, I believe:\r\n- you MUST use the Trainer's `resume_from_checkpoint` arg if you want to re-load the optimizer state (and other training state), and \r\n- you NEED NOT WORRY about accidentally loading other training state when calling `LightningModule.load_from_checkpoint`, because the lightning module isn't responsible for training state in the first place.\r\n\r\nPlease help me out if I didn't arrive at the correct conclusion."
      },
      {
        "user": "YJ-20",
        "created_at": "2024-03-19T01:30:29Z",
        "body": "If you want to reset the optimizer state in `lightning`, you can use `on_load_checkpoint` as like this.\r\n```python\r\ndef on_load_checkpoint(self, checkpoint):\r\n    checkpoint[\"optimizer_states\"] = []\r\n```"
      },
      {
        "user": "marz869",
        "created_at": "2024-03-22T17:08:12Z",
        "body": "> I think I was confused about the difference between `Trainer(resume_from_checkpoint=\"...\")` vs. `LitModule.load_from_checkpoint(\"...\")`, and the docs could probably be improved here.\r\n> \r\n> It appears that `LightningModule.load_from_checkpoint` doesn't even instantiate your optimizers, because only the `Trainer` does that. Therefore, I believe:\r\n> \r\n>     * you MUST use the Trainer's `resume_from_checkpoint` arg if you want to re-load the optimizer state (and other training state), and\r\n> \r\n>     * you NEED NOT WORRY about accidentally loading other training state when calling `LightningModule.load_from_checkpoint`, because the lightning module isn't responsible for training state in the first place.\r\n> \r\n> \r\n> Please help me out if I didn't arrive at the correct conclusion.\r\n\r\n@colllin LightningModule.load_from_checkpoint re_load optimizer, but resume_from_checkpoint, reload epoch and global step...and reset the optimizer!! So if I have a checkpoint with all states(lr scheduler, epoch, global step, ...) if I do LightningModule.load_from_checkpoint, it re_loads optimizer, lr scheduler and optimizer works as I expect, but epoch and global step is set to 0. And if in the next line I load checkpoints again in trainer.fit, then I can continue training the model from last epoch, but optimizer is reset, and lr_scheduler does not work at all!!"
      }
    ]
  },
  {
    "number": 1980,
    "title": "Collect all  losses into a list?",
    "created_at": "2020-05-28T10:09:45Z",
    "closed_at": "2020-05-29T23:08:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1980",
    "body": "What's the easiest way to return a list with all my losses in it? Note: I don't want to log it, but I want to have a list that I can plot in altair after training is done.\r\n\r\nCurrently, I'm maintaining a list outside the lightning module:\r\n\r\n```\r\nlosses = [] # <-- this one\r\n...\r\n\r\nclass MyModule(LightningModule):\r\n    def validation_epoch_end(self, outputs):\r\n            avg_loss = torch.stack([ x['val_loss'] for x in outputs ]).mean()\r\n            correct = sum([ x['correct'] for x in outputs])\r\n            accuracy = float(correct) / len(outputs)\r\n            losses.append(avg_loss)   # <--- append to outside var here\r\n            \r\n            return {'avg_loss' : avg_loss, 'accuracy': accuracy, 'log' : {'val_loss': avg_loss, 'accuracy': accuracy}}\r\n```\r\n\r\nIs there a \"lightning\" way of doing this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1980/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:53:41Z",
        "body": "Why not save it into an attribute self.losses and then have it accessible via model.losses from outside?"
      }
    ]
  },
  {
    "number": 1979,
    "title": "Dynamically change optimizer frequency",
    "created_at": "2020-05-28T08:42:13Z",
    "closed_at": "2020-05-28T12:09:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1979",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI have a WGAN and the ratio between iterations on the discriminator and on the generator is fixed at 5:1. I accomplished this by passing the frequency parameter in the `configure_optimizers` method\r\n```\r\nres_1 = {\r\n            'optimizer': optimizer_d,\r\n            'frequency': 5,\r\n            'lr_scheduler': scheduler_d\r\n        }\r\n```\r\nsame for generator\r\n```\r\nres_2 = {\r\n            'optimizer': optimizer_g,\r\n            'frequency': 1, \r\n            'lr_scheduler': scheduler_g\r\n        }\r\n```\r\n\r\nHow can I dynamically change the `frequency` parameter, such that for the first `n` iterations I have a frequency `x` and after I have a frequency `y`.\r\n\r\n#### Code\r\n\r\nDon't know how to do it.\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - OS: OS independent\r\n - Packaging: pip\r\n - Version: 0.7.6\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1979/comments",
    "author": "lucadiliello",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-28T12:04:45Z",
        "body": "There is no official way of doing this. However, this can be achieved using a `Callback` to change the `Trainer` attribute `optimizer_frequencies`. So if you define callback like this:\r\n```\r\nclass DynamicOptimizerFrequency(Callback):\r\n    def on_epoch_end(self, trainer, model):\r\n           if trainer.current_epoch >10:\r\n               trainer.optimizer_frequency = [3,1]\r\n           if trainer.current_epoch > 20:\r\n               trainer.optimizer_frequency = [2,1]\r\n```\r\nwhere you change the code to however dynamically you want to change the frequencies. "
      }
    ]
  },
  {
    "number": 1930,
    "title": "How to access training and validation losses from callbacks? ",
    "created_at": "2020-05-23T08:45:53Z",
    "closed_at": "2020-05-23T18:52:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1930",
    "body": "For example, if my validation_epoch_end in the trainer returns {'avg_loss':loss, 'log':logs}, how to get the loss value from a callback method like:def on_validation_end(trainer, pl_module)?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1930/comments",
    "author": "NagarajSMurthy",
    "comments": [
      {
        "user": "MaharshiYeluri01",
        "created_at": "2020-05-23T16:00:43Z",
        "body": "you can access the current epoch variables from trainer.callback_metrics\r\n/looks something like below\r\n{'epoch': 4,\r\n 'loss': tensor(0.4924, device='cuda:0'),\r\n 'train_loss': tensor(0.4924, device='cuda:0'),\r\n 'val_auc': tensor(0.7359, dtype=torch.float64),\r\n 'val_loss': tensor(0.7714, device='cuda:0')}"
      }
    ]
  },
  {
    "number": 877,
    "title": "How do I test before any training?",
    "created_at": "2020-02-17T06:34:46Z",
    "closed_at": "2020-02-17T09:40:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/877",
    "body": "## \u2753 Questions and Help\r\n\r\nI am now migrating some of my previous works into lightning. I wish to see if it is able to reproduce my previous results or not. But the doc implies that all the testing has to be performed after training or after loading the previous lightning training state, which I do not have either.\r\n\r\nSo How can I test before training?\r\n\r\n#### Code\r\n\r\n```python\r\n\r\n    trainer = Trainer(logger=logger, max_epochs=5, gpus=[3], distributed_backend=None)\r\n    hparams = HParams(fold=fold, model=model_name, batch_size=8, num_workers=16)\r\n    system = MySYS(hparams, trainer)\r\n\r\n    system.model.load_state_dict(torch.load(state_dict))\r\n    trainer.test()\r\n```\r\nIt cannot work since the trainer does not initialize at all.\r\n\r\n#### What have you tried?\r\nI found inside the code for testing:\r\n```python\r\n\r\n    def test(self, model=None):\r\n        r\"\"\"\r\n\r\n        Separates from fit to make sure you never run on your test set until you want to.\r\n\r\n        Args:\r\n            model (LightningModule): The model to test.\r\n\r\n        Example::\r\n\r\n            # Option 1\r\n            # run test after fitting\r\n            trainer = Trainer()\r\n            model = LightningModule()\r\n\r\n            trainer.fit()\r\n            trainer.test()\r\n\r\n            # Option 2\r\n            # run test from a loaded model\r\n            model = LightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n            trainer = Trainer()\r\n            trainer.test(model)\r\n        \"\"\"\r\n        self.testing = True\r\n        if model is not None:\r\n            self.fit(model)\r\n        else:\r\n            self.run_evaluation(test=True)\r\n```\r\nWhich requires to fit the model that I do not understand at all. Why a fitting is required inside training code? If for the purpose of initialization, can't we just put some init code here?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/877/comments",
    "author": "shijianjian",
    "comments": [
      {
        "user": "festeh",
        "created_at": "2020-02-17T07:18:03Z",
        "body": "If you dig a bit deeper into the sources, you'll find that `fit` actually calls `evaluate` and do not fit a model if `testing` flag is True. So you can run `trainer.test(system)`, it should probably work."
      }
    ]
  },
  {
    "number": 690,
    "title": "How to make test_end() return metrics ",
    "created_at": "2020-01-15T17:47:23Z",
    "closed_at": "2020-01-21T12:31:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/690",
    "body": "I have searched through the docs / Google as well as looked through the source code.\r\n\r\nIt seems like test_end() returns nothing (it has no `return` in the function). I was wondering if I was missing something really obvious. \r\n\r\nI would simply like to return the metrics of the test end.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/690/comments",
    "author": "Laksh1997",
    "comments": [
      {
        "user": "matthew-z",
        "created_at": "2020-01-19T14:09:06Z",
        "body": "You may try this:\r\n\r\n```py\r\nclass MyModel(pl.LightningModule):\r\n  def __init__(self, ...):\r\n    self.test_result = None\r\n  def test_end(self, outputs):\r\n    self.test_result = get_eval_metrics(outputs)\r\n\r\nmodel = MyModel()\r\ntrainer.test(model)\r\nprint(model.test_result)\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "number": 477,
    "title": "About the Weight Initialization in PL",
    "created_at": "2019-11-08T02:28:25Z",
    "closed_at": "2019-11-08T13:53:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/477",
    "body": "Hi,\r\n\r\nI am tring to use BERT for a project. The pretrained BERT model is part of my model. I am wondering how will PL initialize the model weights. Will it overwrite the pretrained BERT weights?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/477/comments",
    "author": "magic282",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-11-08T02:33:44Z",
        "body": "lightning doesn\u2019t do any magic like this under the hood. you control all the weights and what gets initiated "
      },
      {
        "user": "magic282",
        "created_at": "2019-11-08T04:36:13Z",
        "body": "I see. So where should I do the weight initialization step if I want to follow the PL design idea? In the `__init__` of `pl.LightningModule`?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-08T13:53:42Z",
        "body": "This is up to you and you should follow standard PyTorch guidelines.\r\nNormally it's done in ```__init__```"
      }
    ]
  },
  {
    "number": 10285,
    "title": "UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop",
    "created_at": "2021-11-01T05:06:39Z",
    "closed_at": "2021-12-08T11:31:38Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10285",
    "body": "## \ud83d\udc1b Bug\r\n\r\nUsing pytorch-lightning 1.5.0rc1, I will get UserWarning:\r\n```\r\npytorch_lightning/trainer/configuration_validator.py:156: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\r\n  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\r\n```\r\nBut with pytorch-lightning 1.4.9, there is no such warning.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom collections import OrderedDict\r\nimport pytorch_lightning as pl\r\n\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\n\r\nclass TestLrModule(pl.LightningModule):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(TestLrModule, self).__init__()\r\n        self._fc = OrderedDict([\r\n            ('fc0', nn.Linear(input_size, hidden_size)),\r\n            ('tan0', nn.ReLU()),\r\n            ('fc1', nn.Linear(hidden_size, 1)),\r\n        ])\r\n        self.fc = nn.Sequential(self._fc)\r\n        self._loss_fn = nn.MSELoss()\r\n\r\n    def forward(self, x):\r\n        y = self.fc(x)\r\n        return y.squeeze(dim=1)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return self._loss_fn(y_hat, y)\r\n\r\n    def training_epoch_end(self, outputs):\r\n        loss = torch.mean(torch.stack([x['loss'] for x in outputs]))\r\n        self.log('train_loss', loss, on_epoch=True)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return self._loss_fn(y_hat, y)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        loss = torch.mean(torch.stack(outputs))\r\n        self.log('val_loss', loss, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=2e-3, weight_decay=1e-4)\r\n\r\n\r\nsample, feature = 4000, 10\r\nrx, ry = torch.rand(sample, feature), torch.rand(sample)\r\ntest_sample = int(sample * 0.2)\r\ntest_rx, test_ry = torch.rand(test_sample, feature), torch.rand(test_sample)\r\n\r\ntrain_data = DataLoader(TensorDataset(rx, ry), batch_size=32, num_workers=2)\r\nvalid_data = DataLoader(TensorDataset(test_rx, test_ry), batch_size=32, num_workers=2)\r\n\r\nm = TestLrModule(rx.shape[1], 16)\r\ntrainer = pl.Trainer(max_epochs=20)\r\ntrainer.fit(m, train_data, valid_data)\r\n```\r\n\r\n### Environment\r\n\r\n- PyTorch Version  1.8.0\r\n- Python version: 3.8.5\r\n- OS (e.g., Linux): linux\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10285/comments",
    "author": "7starsea",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-11-01T09:14:50Z",
        "body": "it was fixed recently. Can you try master?"
      },
      {
        "user": "7starsea",
        "created_at": "2021-11-01T13:33:53Z",
        "body": "Thanks. With the master version, the ```UserWarning: you defined a validation_step but have no val_dataloader``` disappears. \r\n\r\nBy the way, I am not sure should I take care of the following ```UserWarning```:\r\n```configuration_validator.py:102: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).```\r\n\r\nThe sample code is like\r\n```\r\nclass TestLrModule(pl.LightningModule):\r\n    # standard training/validation_step here\r\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\r\n        optimizer.zero_grad(set_to_none=True)\r\n\r\ntrainer = pl.Trainer(max_epochs=max_epochs, callbacks=[early_stop_callback],\r\n                          check_val_every_n_epoch=4, accumulate_grad_batches=6)\r\n```\r\n\r\nThanks."
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-12-08T11:31:38Z",
        "body": "@7starsea apologies for the late reply. While accumulating, optimization doesn't happen at every step thus optimizer_step won't be called right after every training batch but only when the accumulation interval is complete. This is just a warning for the user to make sure they write their own custom logic within the `optimizer_step` taking the accumulation flag, set inside Trainer, into consideration."
      }
    ]
  },
  {
    "number": 9697,
    "title": "IsADirectoryError: [Errno 21] Is a directory: '/home/pc/SR/dC/1-Data_Preparation'",
    "created_at": "2021-09-25T04:04:35Z",
    "closed_at": "2021-09-27T07:14:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9697",
    "body": "I am using **Jupyter Lab Server**. It has pre-installed tf2.3_py3.6 kernel installed in it. It has 2 GPUS in it.\r\n\r\nPyTorch Lightning Version (e.g., 1.3.0): '1.4.6'\r\nPyTorch Version (e.g., 1.8): '1.6.0+cu101'\r\nPython version: 3.6\r\nOS (e.g., Linux): system='Linux'\r\nCUDA/cuDNN version: 11.2\r\nGPU models and configuration: Mentioned below\r\nHow you installed PyTorch (conda, pip, source): pip\r\n\r\n\r\nNVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:62:00.0 Off |                    0 |\r\n| N/A   36C    P0    57W / 300W |   2842MiB / 32510MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-SXM2...  Off  | 00000000:89:00.0 Off |                    0 |\r\n| N/A   32C    P0    43W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                              \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\nI have trained a model, and saved the best model.\r\n\r\n```\r\nclass SRTagger(pl.LightningModule):\r\n\r\n  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\r\n    super().__init__()\r\n    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\r\n    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n    self.n_training_steps = n_training_steps\r\n    self.n_warmup_steps = n_warmup_steps\r\n    self.criterion = nn.BCELoss()\r\n\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\r\n    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def test_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def training_epoch_end(self, outputs):\r\n    \r\n    labels = []\r\n    predictions = []\r\n    for output in outputs:\r\n      for out_labels in output[\"labels\"].detach().cpu():\r\n        labels.append(out_labels)\r\n      for out_predictions in output[\"predictions\"].detach().cpu():\r\n        predictions.append(out_predictions)\r\n\r\n    labels = torch.stack(labels).int()\r\n    predictions = torch.stack(predictions)\r\n\r\n    for i, name in enumerate(LABEL_COLUMNS):\r\n      class_roc_auc = auroc(predictions[:, i], labels[:, i])\r\n      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\r\n\r\n\r\n  def configure_optimizers(self):\r\n\r\n    optimizer = optim.RAdam(self.parameters(), lr=2e-5)\r\n\r\n    scheduler = get_linear_schedule_with_warmup(\r\n      optimizer,\r\n      num_warmup_steps=self.n_warmup_steps,\r\n      num_training_steps=self.n_training_steps\r\n    )\r\n\r\n    return dict(\r\n      optimizer=optimizer,\r\n      lr_scheduler=dict(\r\n        scheduler=scheduler,\r\n        interval='step'\r\n      )\r\n    )\r\n\r\n```\r\n**After training, I want to load my best model without training it again**\r\n\r\n\r\n```\r\ncheckpoint_callback = ModelCheckpoint(\r\n  dirpath=\"checkpoints\",\r\n  filename=\"best-checkpoint\",\r\n  save_top_k=1,\r\n  verbose=True,\r\n  monitor=\"val_loss\",\r\n  mode=\"min\"\r\n)\r\n\r\nlogger = TensorBoardLogger(\"lightning_logs\", name=\"SReply\")\r\n\r\n# And early stopping triggers when the loss hasn't improved for the last \r\n# 2 epochs (you might want to remove/reconsider this when training on real-world projects):\r\n\r\n\r\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\r\n# We can start the training process:\r\n# checkpoint_callback supports only a bool value. If set to True, it will create a model checkpoint\r\n# instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n\r\ntrainer = pl.Trainer(\r\n  logger=logger,\r\n  callbacks=[early_stopping_callback,checkpoint_callback],\r\n  max_epochs=N_EPOCHS,\r\n  gpus=1,\r\n  progress_bar_refresh_rate=50,\r\n  amp_level='O3'\r\n  )\r\n\r\ntrained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\ntrained_model.eval()\r\ntrained_model.freeze()\r\n```\r\n\r\n\r\n**Error**\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nIsADirectoryError                         Traceback (most recent call last)\r\n/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py in <module>\r\n----> 1 trained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\n      2 trained_model.eval()\r\n      3 trained_model.freeze()\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\r\n    129             checkpoint = pl_load(checkpoint_path, map_location=map_location)\r\n    130         else:\r\n--> 131             checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)\r\n    132 \r\n    133         if hparams_file is not None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/cloud_io.py in load(path_or_url, map_location)\r\n     30         return torch.hub.load_state_dict_from_url(str(path_or_url), map_location=map_location)\r\n     31     fs = get_filesystem(path_or_url)\r\n---> 32     with fs.open(path_or_url, \"rb\") as f:\r\n     33         return torch.load(f, map_location=map_location)\r\n     34 \r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/spec.py in open(self, path, mode, block_size, cache_options, **kwargs)\r\n    980                 autocommit=ac,\r\n    981                 cache_options=cache_options,\r\n--> 982                 **kwargs,\r\n    983             )\r\n    984             if not ac and \"r\" not in mode:\r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/implementations/local.py in _open(self, path, mode, block_size, **kwargs)\r\n    143         if self.auto_mkdir and \"w\" in mode:\r\n    144             self.makedirs(self._parent(path), exist_ok=True)\r\n--> 145         return LocalFileOpener(path, mode, fs=self, **kwargs)\r\n    146 \r\n    147     def touch(self, path, **kwargs):\r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/implementations/local.py in __init__(self, path, mode, autocommit, fs, compression, **kwargs)\r\n    234         self.compression = get_compression(path, compression)\r\n    235         self.blocksize = io.DEFAULT_BUFFER_SIZE\r\n--> 236         self._open()\r\n    237 \r\n    238     def _open(self):\r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/implementations/local.py in _open(self)\r\n    239         if self.f is None or self.f.closed:\r\n    240             if self.autocommit or \"w\" not in self.mode:\r\n--> 241                 self.f = open(self.path, mode=self.mode)\r\n    242                 if self.compression:\r\n    243                     compress = compr[self.compression]\r\n\r\nIsADirectoryError: [Errno 21] Is a directory: '/home/pc/SR/dC/1-Data_Preparation'\r\n```\r\n\r\n**This error. comes when I try to load my model second time after closing and reopening the jupyter notebook. I run the code except training it.**\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9697/comments",
    "author": "pratikchhapolika",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-09-25T04:27:46Z",
        "body": "> This error. comes when I try to load my model second time after closing and reopening the jupyter notebook. I run the code except training it.\r\n\r\nDid you confirm that the directory `'/home/pc/SR/dC/1-Data_Preparation'` exists? "
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-25T04:34:33Z",
        "body": "> > This error. comes when I try to load my model second time after closing and reopening the jupyter notebook. I run the code except training it.\r\n> \r\n> Did you confirm that the directory `'/home/pc/SR/dC/1-Data_Preparation'` exists?\r\n\r\nYes I am inside this directory : `'/home/pc/SR/dC/1-Data_Preparation'` and when I do\"\r\n\r\n`!ls '/home/pc/SR/dC/1-Data_Preparation/checkpoints`\r\n\r\nI get :  **best-checkpoint.ckpt**"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-09-27T06:04:15Z",
        "body": "```py\r\ntrainer = pl.Trainer(\r\n  logger=logger,\r\n  callbacks=[early_stopping_callback,checkpoint_callback],\r\n  max_epochs=N_EPOCHS,\r\n  gpus=1,\r\n  progress_bar_refresh_rate=50,\r\n  amp_level='O3'\r\n  )\r\n\r\ntrained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\ntrained_model.eval()\r\n```\r\nyou have created a new trainer instance and using a new checkpoint instance.. so `best_model_path` doesn't exist at this point."
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-27T06:41:53Z",
        "body": "> ```python\r\n> trainer = pl.Trainer(\r\n>   logger=logger,\r\n>   callbacks=[early_stopping_callback,checkpoint_callback],\r\n>   max_epochs=N_EPOCHS,\r\n>   gpus=1,\r\n>   progress_bar_refresh_rate=50,\r\n>   amp_level='O3'\r\n>   )\r\n> \r\n> trained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\n> trained_model.eval()\r\n> ```\r\n> \r\n> you have created a new trainer instance and using a new checkpoint instance.. so `best_model_path` doesn't exist at this point.\r\n\r\nSo what solution do you suggest?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-09-27T07:01:21Z",
        "body": "you can just pass the path of the best checkpoint as a string or maybe extract it from one of your saved checkpoints to reload it correctly."
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-27T07:14:10Z",
        "body": "> you can just pass the path of the best checkpoint as a string or maybe extract it from one of your saved checkpoints to reload it correctly.\r\n\r\nSRTagger.load_from_checkpoint(\"path_to_cpkt_file\",n_classes=len(LABEL_COLUMNS))\r\ntrained_model.eval()\r\n\r\nClosing it."
      }
    ]
  },
  {
    "number": 9488,
    "title": "Getting error with Pytorch lightning when passing model checkpoint",
    "created_at": "2021-09-13T14:43:28Z",
    "closed_at": "2021-09-13T17:52:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9488",
    "body": "I am training a multi-label classification problem using Hugging face models.\r\n\r\nI am using `Pytorch lightning` to train the model.\r\n \r\n\r\nHere is the code:\r\n\r\nAnd early stopping triggers when the loss hasn't improved for the last \r\n\r\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\r\n\r\nWe can start the training process:\r\n\r\n\r\n    checkpoint_callback = ModelCheckpoint(\r\n      dirpath=\"checkpoints\",\r\n      filename=\"best-checkpoint\",\r\n      save_top_k=1,\r\n      verbose=True,\r\n      monitor=\"val_loss\",\r\n      mode=\"min\"\r\n    )\r\n\r\n\r\n    trainer = pl.Trainer(\r\n      logger=logger,\r\n      callbacks=[early_stopping_callback],\r\n      max_epochs=N_EPOCHS,\r\n     checkpoint_callback=checkpoint_callback,\r\n      gpus=1,\r\n      progress_bar_refresh_rate=30\r\n    )\r\n    # checkpoint_callback=checkpoint_callback,\r\n\r\nAs soon as I run this, I get error:\r\n\r\n\r\n    ~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py in _configure_checkpoint_callbacks(self, checkpoint_callback)\r\n         75             if isinstance(checkpoint_callback, Callback):\r\n         76                 error_msg += \" Pass callback instances to the `callbacks` argument in the Trainer constructor instead.\"\r\n    ---> 77             raise MisconfigurationException(error_msg)\r\n         78         if self._trainer_has_checkpoint_callbacks() and checkpoint_callback is False:\r\n         79             raise MisconfigurationException(\r\n    \r\n    MisconfigurationException: Invalid type provided for checkpoint_callback: Expected bool but received <class 'pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint'>. Pass callback instances to the `callbacks` argument in the Trainer constructor instead.\r\n\r\n\r\n**How can I fix this issue?**",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9488/comments",
    "author": "pratikchhapolika",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-09-13T14:53:17Z",
        "body": "`checkpoint_callback` supports only a bool value. If set to True, it will create a model checkpoint instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n```py\r\ntrainer = Trainer(callbacks=[checkpoint_callback, early_stopping_callback], ...)\r\n```"
      }
    ]
  },
  {
    "number": 9176,
    "title": "on_save_checkoint never called",
    "created_at": "2021-08-28T15:11:45Z",
    "closed_at": "2021-08-29T00:01:00Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9176",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI wrote a `Callback` class and found `on_save_checkpoint` had never been called\r\n\r\n### To Reproduce\r\n\r\nMy callback class:\r\n```\r\nfrom pytorch_lightning.callbacks import Callback\r\nfrom os.path import join\r\nimport torch\r\nimport os\r\nimport pytorch_lightning as pl\r\nfrom typing import Dict, Any, Optional\r\n\r\n\r\nclass JitSave(Callback):\r\n\r\n    def __init__(self):\r\n        self.outputs = None\r\n        self.n_dataloaders = None\r\n\r\n    def on_save_checkpoint(\r\n        self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', checkpoint: Dict[\r\n                str, Any]\r\n    ) -> dict:\r\n        # Torch.jit.save\r\n        jit_model_dir = join(\r\n            join(os.getcwd(), \"checkpoints\"), f\"jit_{self.logger[0].version}\" + \"{}.pt\"\r\n        )\r\n        torch.jit.save(self.model.cpu().to_torchscript(), jit_model_dir.format(\"cpu\"))\r\n        torch.jit.save(self.model.to_torchscript(), jit_model_dir.format(\"gpu\"))\r\n        print(f\"torch.jit.save path :\\n{jit_model_dir}\")\r\n        # return {\"jitsave_path\": jit_model_dir}\r\n        return checkpoint\r\n\r\n    def setup(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', stage: Optional[str] = None) -> None:\r\n        self.n_dataloaders = len(pl_module.val_dataloader())\r\n\r\n    def _reset(self):\r\n        self.outputs = [[] for _ in range(self.n_dataloaders)]\r\n\r\n    def on_epoch_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\r\n        self._reset()\r\n\r\n    def on_validation_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\r\n        for dataloader_idx, output in enumerate(self.outputs):\r\n            pass\r\n```\r\n`on_validation_epoch_end` works but on_save_checkpoint not.\r\n\r\nThis is my `ModelCheckpoint`:\r\n\r\n```\r\nmodel_checkpoint:\r\n  _target_: pytorch_lightning.callbacks.ModelCheckpoint\r\n  monitor: \"val/f1\" # name of the logged metric which determines when model isimproving\r\n  mode: \"max\" # can be \"max\" or \"min\"\r\n  save_top_k: 1 # save k best models (determined by above metric)\r\n  save_last: False # additionaly always save model from last epoch\r\n  verbose: False\r\n  dirpath: \"checkpoints/\"\r\n  filename: \"epoch_{epoch:03d}\"\r\n  auto_insert_metric_name: False\r\n  save_weights_only: True\r\n```\r\n\r\nCallbacks are passed to the trainer:\r\n\r\n```\r\ncallbacks: List[Callback] = []\r\n    if \"callbacks\" in config:\r\n        for _, cb_conf in config.callbacks.items():\r\n            if \"_target_\" in cb_conf:\r\n                log.info(f\"Instantiating callback <{cb_conf._target_}>\")\r\n                callbacks.append(hydra.utils.instantiate(cb_conf))\r\n```\r\n\r\n```\r\ntrainer: Trainer = hydra.utils.instantiate(\r\n        config.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\"\r\n    )\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n`on_save_checkpoint` should be called.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.5\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.0+cu102\r\n        - pytorch-lightning: 1.4.2\r\n        - tqdm:              4.62.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #60~20.04.1-Ubuntu SMP Thu May 6 09:52:46 UTC 2021",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9176/comments",
    "author": "zhiyuanpeng",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-08-28T17:14:14Z",
        "body": "you're specifying `save_weights_only=True` so no callback states are added to the checkpoint. this means the callback's `on_save_checkpoint` is never called"
      }
    ]
  },
  {
    "number": 9155,
    "title": "AttributeError: Can't pickle local object when attempting multi-GPU training",
    "created_at": "2021-08-27T00:24:54Z",
    "closed_at": "2021-08-27T18:42:58Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9155",
    "body": "## \ud83d\udc1b Bug\r\n\r\nRunning the provided script with multiple GPUs causes the following error:\r\n```\r\n$ python pickle_test.py\r\n.../lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:746: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  rank_zero_warn(\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n.../lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:99: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping val loop\r\n  rank_zero_warn(f\"you passed in a {loader_name} but have no {step_name}. Skipping {stage} loop\")\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\nTraceback (most recent call last):\r\n  File \"pickle_test.py\", line 81, in <module>\r\n    test_x(tmpdir)\r\n  File \"pickle_test.py\", line 77, in test_x\r\n    trainer.fit(model=model, datamodule=dm)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    self._run(model)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._dispatch()\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 158, in start_training\r\n    mp.spawn(self.new_process, **self.mp_spawn_kwargs)\r\n  File \".../lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \".../lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \".../lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \".../lib/python3.8/multiprocessing/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \".../lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'LightningDataModule.from_datasets.<locals>.train_dataloader'\r\n```\r\n\r\n### To Reproduce\r\n\r\nThe following script causes the bug:\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import LightningDataModule\r\nfrom torch.nn import functional as F\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import MNIST\r\n\r\ntmpdir = '../../data'\r\n\r\n\r\ndef mnist(root: str, normalize: bool = False):\r\n    tlist = [transforms.ToTensor()]\r\n\r\n    if normalize:\r\n        tlist.append(transforms.Normalize((0.5,), (0.5,)))\r\n\r\n    transform = transforms.Compose(tlist)\r\n\r\n    trainset = MNIST(root=root, train=True, download=True, transform=transform)\r\n    testset = MNIST(root=root, train=False, download=True, transform=transform)\r\n    return trainset, testset\r\n\r\n\r\ndef mnist_datamodule(data_path: str, batch_size: int, num_workers: int):\r\n    train, val = mnist(data_path, normalize=True)\r\n    return LightningDataModule.from_datasets(train, val, None, batch_size=batch_size, num_workers=num_workers)\r\n\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc1 = torch.nn.Linear(28 * 28, 32)\r\n        self.fc2 = torch.nn.Linear(32, 10)\r\n\r\n    def forward(self, x):\r\n        x = torch.flatten(x, 1)\r\n        x = F.sigmoid(self.fc1(x))\r\n        x = F.softmax(self.fc2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_x(tmpdir):\r\n    # init model\r\n    model = BoringModel()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        max_epochs=1,\r\n        progress_bar_refresh_rate=20,\r\n        gpus=2\r\n    )\r\n\r\n    dm = mnist_datamodule(tmpdir, 16, 1)\r\n\r\n    # Train the model \u26a1\r\n    trainer.fit(model=model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_x(tmpdir)\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\nRunning on a SLURM cluster\r\n- PyTorch Lightning Version (e.g., 1.3.0): 1.4.1\r\n- PyTorch Version (e.g., 1.8): 1.9.0\r\n- Python version: 3.8.0\r\n- OS (e.g., Linux): Linux HPCC\r\n- CUDA/cuDNN version: 10.1\r\n- GPU models and configuration: 2x 2080\r\n- How you installed PyTorch (`conda`, `pip`, source): conda\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9155/comments",
    "author": "import-antigravity",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-08-27T11:26:21Z",
        "body": "@import-antigravity this is because `LightningModule.from_datasets` patches out the data loader methods. \r\nYou will have to select `accelerator=\"ddp\"` as a workaround."
      }
    ]
  },
  {
    "number": 8823,
    "title": "Allow `--gpus=None` to be specified on the CLI.",
    "created_at": "2021-08-10T03:30:58Z",
    "closed_at": "2021-08-10T16:32:13Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8823",
    "body": "Currently specifying `--gpus=None` breaks the utils.argparse logic. I want to allow the string None to be a valid option. In other words, I want the user to be able to explicitly specify the default value for this argument, which is currently not possible.\r\n\r\nWhy? In my workflow I customized my argparser. This caused the `--gpus` argument to default to something non-None. Then, for whatever reason, my nvidia drivers stopped working (as they sometimes do), so I wanted to fallback on the CPU. When I tried to set `--gpus=None` argparse balked at me because it was not a valid gpu option. But removing the `--gpus` option no longer defaulted to None. Therefore, there was no way for the user to overwrite my short-sighted defaults and simply get the CPU.\r\n\r\nIf the only way to set the value of an argument is by removing the specification, then that can cause issues like the one I had. Regardless, I think it is good design such that you can always _change_ the value of an argument to achieve a particular funtionality. \r\n\r\nAs an example say I have a script:\r\n\r\n```bash\r\npython fit.py \\\r\n    --gpus=1 \\\r\n    --num_workers=2 \r\n```\r\n\r\nthe diff from the above to the CPU version where it removes the line:\r\n\r\n```bash\r\npython fit.py \\\r\n    --num_workers=2 \r\n```\r\n\r\nno longer gives a reader any indication that a --gpus option was ever there, or is something that could be specified and changed, whereas \r\n\r\n```bash\r\npython fit.py \\\r\n    --gpus=None \\\r\n    --num_workers=2 \r\n```\r\n\r\nPreserves the original `--gpus` arg as something important that this script might vary. (It also makes it much eaiser to futher parametarize that argument in bash itself). I believe that having the option to simply change the value rather than being forced to remove the entire line is desirable. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8823/comments",
    "author": "Erotemic",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-08-10T07:01:35Z",
        "body": "@Erotemic in your use case what is the advantage of None over just setting gpus=0?\r\n\r\nNote we don't have parsing for \"None\" in other trainer arguments."
      },
      {
        "user": "Erotemic",
        "created_at": "2021-08-10T16:32:10Z",
        "body": "Well, now I feel silly.\r\n\r\nI'm so used to setting the specific device I want to use, I didn't even consider that. It still feels a little weird that the user can't set the argument explicitly to the default, but as long as 0 is effectively the default, then I'm happy enough to close the issue and PR. No need to add complexity when its not needed."
      }
    ]
  },
  {
    "number": 8678,
    "title": "multigpu ddp: Code after fit executed many times",
    "created_at": "2021-08-02T13:28:33Z",
    "closed_at": "2021-08-03T08:37:52Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8678",
    "body": "## \ud83d\udc1b Bug\r\n\r\nAfter training model with the Trainer.fit on 4-gpu machine with the accelerator=\"ddp\", my code which goes after that executed 3 (?) times. \r\nI receive 2 exceptions \"FileNotFoundError\" and then printing of successful weights saving.\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```py\r\n....\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    precision=16 if train_opt.get(\"fp16\", False) else 32,\r\n    accelerator=\"ddp\",\r\n    accumulate_grad_batches=train_opt.get(\"grad_accum\", 1),\r\n    max_epochs=train_opt.get(\"epochs\", 20),\r\n    default_root_dir=train_opt.get(\"root_dir\", None),\r\n    callbacks=callbacks,\r\n    logger=logger,\r\n    log_every_n_steps=1,\r\n)\r\n....\r\ntrainer.fit(model, dataloaders[0], dataloaders[1])\r\nif trainer.state.status != TrainerStatus.FINISHED:\r\n    raise InterruptedError()\r\n\r\npath = checkpoint_callback.best_model_path\r\n\r\nos.makedirs(os.path.dirname(target_path), exist_ok=True)\r\nmodel.load_state_dict(torch.load(str(path))[\"state_dict\"])\r\ntorch.save(model.model.state_dict(), target_path)\r\n```\r\n\r\n### Expected behavior\r\n\r\nA single execution of the code after trainer.fit\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 1.4.0rc0\r\n\t- tqdm:              4.61.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.7\r\n\t- version:           #1 SMP Tue May 11 20:50:07 UTC 2021\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8678/comments",
    "author": "johngull",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-03T08:36:48Z",
        "body": "Dear @johngull,\r\n\r\nThis is an expected behaviour.\r\n\r\nUsing accelerator `ddp, this will create multiple independent processes and you script will be run `world_size` times.\r\n\r\n```py\r\n....\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    precision=16 if train_opt.get(\"fp16\", False) else 32,\r\n    accelerator=\"ddp\",\r\n    accumulate_grad_batches=train_opt.get(\"grad_accum\", 1),\r\n    max_epochs=train_opt.get(\"epochs\", 20),\r\n    default_root_dir=train_opt.get(\"root_dir\", None),\r\n    callbacks=callbacks,\r\n    logger=logger,\r\n    log_every_n_steps=1,\r\n)\r\n....\r\ntrainer.fit(model, dataloaders[0], dataloaders[1])\r\n\r\n#\u00a0You should manipulate checkpoints only on rank 0 :)\r\nif trainer.is_global_zero:\r\n    path = checkpoint_callback.best_model_path\r\n    os.makedirs(os.path.dirname(target_path), exist_ok=True)\r\n    model.load_state_dict(torch.load(str(path))[\"state_dict\"])\r\n    torch.save(model.model.state_dict(), target_path)\r\n```\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "johngull",
        "created_at": "2021-08-03T08:45:12Z",
        "body": "Hello @tchaton,\r\n\r\nThank you a lot for the clarification and the tip on how to fix it.\r\nI have several questions here.\r\n\r\n- Shall I wrap everything else before trainer.fit also?\r\n- Is there another acceleration method that is faster than data-parallel but doesn't have such behavior?\r\n\r\nThanks.\r\n"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-03T13:57:50Z",
        "body": "Hey @tchaton,\r\n\r\nMy pleasure :)\r\n\r\n`Shall I wrap everything else before trainer.fit also?`\r\n\r\nThe processes are being created on `trainer.fit` call, therefore the trainer isn't aware of its rank before. Alternatively, you could use `ddp_spawn`.\r\n\r\nYes, `ddp_spawn`.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 8351,
    "title": "_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError",
    "created_at": "2021-07-09T11:40:19Z",
    "closed_at": "2021-08-29T18:20:37Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8351",
    "body": "## \ud83d\udc1b Bug\r\n\r\nEncountering the following issue:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError\r\nTraceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError\r\n```\r\n\r\n* This happens when using just 1 GPU without multi-gpu\r\n* I have manually pickled my model, data loader and all modules and have fixed any issues\r\n\r\n - PyTorch Lightning Version 1.3.8\r\n - PyTorch Version 1.9\r\n - Python version: 3.8.2\r\n - OS: Linux\r\n - CUDA/cuDNN version: cuda/10.2-cudnn7.5.1\r\n - GPU models and configuration:\r\n - How you installed PyTorch: pip\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8351/comments",
    "author": "cyrusvahidi",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-07-09T14:54:31Z",
        "body": "Add \r\n\r\n```\r\nif __name__ == \"__main__\" \r\n```\r\n\r\nwhere your entry point to the script is. \r\nChances are high you get this because you have num_workers > 0 in your DataLoader."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-09T17:56:28Z",
        "body": "@rastegah sorry for the standard answer but this was my best guess. \r\nIt looks like you are not posting the full error stack trace so we don't know where this is originating from. And without a code sample it will also be hard to know what's going on. "
      },
      {
        "user": "cyrusvahidi",
        "created_at": "2021-07-14T10:17:34Z",
        "body": "Here is the entry point to the script: \r\n```\r\nimport fire\r\nimport os\r\nimport gin\r\n\r\nfrom dptm.model import lightning_run\r\nfrom dptm.utils import gin_register_and_parse\r\n\r\n@gin.configurable\r\ndef run_train(gin_file: str = \"gin/dptm.gin\"):\r\n    gin_config_path = os.path.join(os.getcwd(), gin_file)\r\n    gin_register_and_parse(gin_config_path)\r\n\r\n    lightning_run(gin_config_path)\r\n\r\ndef main():\r\n  fire.Fire(run_train)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n```\r\n@gin.configurable\r\ndef lightning_run(gin_config: str = None,\r\n                  n_epochs: float = 50, \r\n                  batch_size: float = 1, \r\n                  patience: int = 5,\r\n                  log: bool = False,\r\n                  data_module: Callable = LitDataModule):\r\n    dataset = LitDataModule()\r\n\r\n    model = LitModule()\r\n\r\n    # Initialize a trainer\r\n    logger = init_logger(gin_config) if log else None\r\n    trainer = pl.Trainer(gpus=1,\r\n                                   max_epochs=n_epochs,\r\n                                   progress_bar_refresh_rate=20, \r\n                                   logger=logger)\r\n\r\n    # Train the model \u26a1\r\n    trainer.fit(model, dataset)\r\n    trainer.test(model)\r\n```\r\n\r\nAnd the stack trace occurs during validation sanity check:\r\n\r\n`Validation sanity check: 0it [00:00, ?it/s]Traceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError\r\nTraceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError`\r\n\r\nI've spent hours trying to locate the issue. Manually pickled all modules and data modules. I'm usually an analogous environment and setup to other projects that do not encounter this error."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-14T10:23:22Z",
        "body": "Can you set `num_workers=0` just to make sure this is not due to the multiprocessing in dataloaders?\r\nAnd are you 100% sure this is the full stack trace?\r\nThere is nothing more above ` Validation sanity check: 0it [00:00, ?it/s]Traceback (most recent call last): File`?"
      },
      {
        "user": "cyrusvahidi",
        "created_at": "2021-07-14T11:44:53Z",
        "body": "Setting `num_workers=0` does work now! Any idea how to resolve this? "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-14T15:03:09Z",
        "body": "Yes, that means you have somewhere a non-picklable object that gets accessed or imported in the code that runs in the worker processes. That's usually everything inside your dataset code and everything that gets imported there. PyTorch / Python will pickle all these objects to recreate the state in spawned processes. You would run into this issue even without Lightning I'm pretty sure. \r\nTo test it, simply do this: \r\n\r\n```python\r\ndef main():\r\n\r\n    dataset = MyDaset(...)\r\n    dataloader = DataLoader(..., num_workers=2)\r\n    data = next(iter(dataloader))\r\n    \r\n    # comment out all Lightning code\r\n    # trainer.fit()\r\n```\r\n    \r\nand if you run into a pickle error we know for sure what's the problem."
      }
    ]
  },
  {
    "number": 7544,
    "title": "Training fails at the end of the epoch when returning None in the training step",
    "created_at": "2021-05-14T09:17:48Z",
    "closed_at": "2021-05-14T13:32:46Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7544",
    "body": "## \ud83d\udc1b Bug\r\n\r\nSometimes my training loss in a batch is nan. Hence, I return None as loss so that the model will not backpropagate through it as suggested here: #4956. It works fine during the epoch; however, the code fails at the end of the epoch in the function reduce_across_time (line 532).\r\n\r\n```python\r\n           if isinstance(value, list):\r\n                value = torch.tensor(value)\r\n```\r\n\r\nIn case of None, value will be equal to [None] and torch cannot create a proper tensor out of it (*** RuntimeError: Could not infer dtype of NoneType)\r\n\r\nIs it me doing something wrong, or is it a bug in Lightning? Is there any workaround?\r\n\r\nPytorch Version \r\npytorch-lightning-1.3.1\r\ntorch 1.8.1+cu11\r\npython 3.7.9",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7544/comments",
    "author": "TommasoBendinelli",
    "comments": [
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T10:41:40Z",
        "body": "Sure, this reproduce the bug\r\n```python\r\nimport os\r\nimport random\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        if batch_idx == 2:\r\n            loss = None\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=5,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=10,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-05-14T10:53:36Z",
        "body": "I think its because of this\r\n```python\r\nif batch_idx == 2:\r\n    loss = None\r\nself.log(\"train_loss\", loss)\r\n```\r\n`None` values are being logged and stored here which are then accumulated at epoch end which is then throwing this error.\r\nThis should work\r\n```python\r\nif batch_idx == 2:\r\n    loss = None\r\nelse:\r\n    self.log(\"train_loss\", loss)\r\n```\r\nor lightning should handle this internally?"
      },
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T11:01:04Z",
        "body": "Ahh, I see, it makes sense.  When averaging the loss across multiple batches, how does lightning handles the fact that a batch was skipped due to the loss being None? Does it simply not include it in the average? "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-14T11:08:32Z",
        "body": "Sorry, had to delete my answer and double check but yes, it averages only over the metrics logged, not over all training_steps. "
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-05-14T11:13:16Z",
        "body": "to be specific it does weighted average by default using batch_size. In your case, it hasn't reached up till that point because this error is thrown while converting the logs list to PyTorch tensor and since it contains NaN values, it is throwing the error. Ideally, if a batch is skipped then it shouldn't contribute while aggregating the results so you can have an else statement there which will just work fine."
      }
    ]
  },
  {
    "number": 6778,
    "title": "No TPU devices were found.",
    "created_at": "2021-04-01T05:38:18Z",
    "closed_at": "2021-04-02T05:16:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6778",
    "body": "Thanks for great framework.  \r\nI tried to train with tpu (Google Cloud Platform Environment). I encounter error like this:\r\n```\r\nkaki_ai@kaki-ins:~/kopite-bot$ python3 train_blender.py\r\n16:14:31 | Overriding opt[\"no_cuda\"] to True (previously: False)\r\n16:14:31 | Loading model with `--beam-block-full-context false`\r\n16:14:31 | loading dictionary from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model.dict\r\n16:14:31 | num words = 54944\r\n16:14:32 | DEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\r\n16:14:33 | Total parameters: 87,508,992 (87,508,992 trainable)\r\n16:14:33 | Loading existing model params from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model\r\nTraceback (most recent call last):\r\n  File \"train_blender.py\", line 47, in <module>\r\n    val_dataloader=test_loader,\r\n  File \"/home/kaki_ai/kopite-bot/training/lightning_base.py\", line 135, in fit\r\n    accumulate_grad_batches=self.accumulate_grad_batches,\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 39, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 321, in __init__\r\n    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 91, in __init__\r\n    self.tpu_cores = device_parser.parse_tpu_cores(tpu_cores)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/device_parser.py\", line 113, in parse_tpu_cores\r\n    raise MisconfigurationException('No TPU devices were found.')\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No TPU devices were found.\r\n```\r\n\r\nIf you have any doubts, please help me. Thank you!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6778/comments",
    "author": "sooftware",
    "comments": [
      {
        "user": "kaushikb11",
        "created_at": "2021-04-01T06:50:27Z",
        "body": "Hi @sooftware,\r\nDid you export the env variables required to connect to TPU? TPU_IP_ADDRESS & XRT_TPU_CONFIG.\r\n\r\nIf yes, could you try the master and update us on it? We pushed an update last night #6767. Thanks!"
      },
      {
        "user": "sooftware",
        "created_at": "2021-04-02T00:22:40Z",
        "body": "I set `export XRT_TPU_CONFIG=\"tpu_worker;0;$TPU_IP_ADDRESS:xxxx\"` and It works. Thank you!"
      }
    ]
  },
  {
    "number": 6421,
    "title": "trainer.test is breaking when a model is not passed",
    "created_at": "2021-03-08T21:56:10Z",
    "closed_at": "2021-03-25T16:23:02Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6421",
    "body": "From the docs:\r\n\r\n```\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test()\r\n```\r\n\r\nTrainer.test should use the best checkpoint when a model isn't provided, and currently, that doesn't work.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6421/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-03-08T23:27:30Z",
        "body": "Here is an example that shows that it works:\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning import Trainer\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('test_loss', loss)\r\n\r\n    def on_test_start(self):\r\n        checkpoint = torch.load(self.trainer.checkpoint_callback.best_model_path)\r\n        assert torch.allclose(checkpoint[\"state_dict\"][\"l1.weight\"], self.l1.weight)\r\n        assert torch.abs(self.l1.weight).sum().item() > 0\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args)\r\n\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n    trainer = Trainer(\r\n        max_epochs=2,\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n    )\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n    # erase model weight\r\n    torch.fill_(model.l1.weight.data, 0)\r\n    assert torch.abs(model.l1.weight).sum().item() == 0\r\n    trainer.test()\r\n    assert torch.abs(model.l1.weight).sum().item() > 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\nIf you look at the assertion there in on_test_start, the weights are correctly loaded.\r\nPlease let me know under what circumstances it doesn't work. A reproducible example would be very much appreciated. Feel free to take my code and modify it."
      }
    ]
  },
  {
    "number": 5897,
    "title": "Auto_scale_batch_size fails for to bigger batch sizes, cuDNN failure",
    "created_at": "2021-02-10T08:51:55Z",
    "closed_at": "2021-02-15T15:44:41Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5897",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm using a pre-trained ResNet50 on 224x224 images with 16-bit precision, I wanted to test the auto_scale_batch_size functionality.\r\n\r\nThe output in the terminal is the following:\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nUsing native 16bit precision.\r\nBatch size 2 succeeded, trying batch size 4\r\nBatch size 4 succeeded, trying batch size 8\r\nBatch size 8 succeeded, trying batch size 16\r\nBatch size 16 succeeded, trying batch size 32\r\nBatch size 32 succeeded, trying batch size 64\r\nBatch size 64 succeeded, trying batch size 128\r\nBatch size 128 succeeded, trying batch size 256\r\n\r\nAll good till then. On batch size 256 the GPU's memory certainly is not sufficient and it fails on a 2d convolution.\r\nThe auto_scaling should be aborted at this point and the batch_size fixed to 128.\r\nInstead the script fails with the message \"RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\".\r\n\r\n### To Reproduce\r\nI'm not doing anything exceptional:\r\n1. Parsing arguments with \"--auto_scale_batch_size\", \"true\"\r\n2. Initiating model, datamodule and trainer using the parsed arguments\r\n3. trainer.tune(model, dm)\r\n\r\n### Question\r\nIs this a problem on the Pytorch Lightning side not capturing the exception or is this anyhow linked to Cuda and installing a different version could be enough?\r\n\r\n\r\n### Environment\r\n1x GeForce RTX 2080Ti\r\nUbuntu 20.10\r\nPython 3.8.5\r\nPytorch Lightning 1.1.8\r\nPytorch 1.7.1\r\nCuda 11.2\r\nEnv created with miniconda, packages installed with pip\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5897/comments",
    "author": "FlorianMF",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-02-10T08:56:45Z",
        "body": "Hi @FlorianMF,\r\n\r\nHave you tried to run that network with batchsize 256 without the auto_scaling? Because we don't do anything special on the convolutional side and this seems to be a bug in PyTorch.\r\n\r\nAlso could you try with `torch.backends.cudnn.benchmark = True` and/or `torch.backends.cudnn.enabled = False`? "
      },
      {
        "user": "FlorianMF",
        "created_at": "2021-02-10T09:22:45Z",
        "body": "Hi @justusschock, \r\nI get the same error without auto_scaling for a batch_size=256, as well when adding 'torch.backends.cudnn.benchmark = True' at the top of the script.\r\nWhen adding \"torch.backends.cudnn.enabled = False\" the error message \"RuntimeError: CUDA out of memory\" confirms my assumption.\r\n\r\nSo, am I 'forced' to use 'auto_scale_batch_size' once to get the maximum batch size and then rerun the script without the flag? I think your goal proposing this functionality is that this is not needed though.\r\n"
      },
      {
        "user": "justusschock",
        "created_at": "2021-02-10T10:03:05Z",
        "body": "No, usually you don't have to rerun it. But with torch.backends.cudnn.enabled = False, PyTorch will use some other algorithm for convolutions, which may be more memory demanding. So for some reason there seems to be a bug within PyTorch for the cudnn convolution. Our tuner only listens to the `Runtime: CUDA out of memory`-Error as everything else could also be a user-error.\r\n\r\nUnfortunately there is nothing else we can do on that"
      },
      {
        "user": "FlorianMF",
        "created_at": "2021-02-10T10:24:26Z",
        "body": "You're right. The error is captured when 'torch.backends.cudnn.enabled = False' is added at the top and the script continues."
      }
    ]
  },
  {
    "number": 5572,
    "title": "When overwriting the setup() function in the LightningDataModule, it is forced to add an unnecessary parameter.",
    "created_at": "2021-01-19T14:10:32Z",
    "closed_at": "2021-01-19T14:38:56Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5572",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## When overwriting the setup() function in the LightningDataModule, it is forced to add an unnecessary parameter.\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0):1.7\r\n - OS (e.g., Linux):Windows\r\n - How you installed PyTorch (`conda`, `pip`, source):pip\r\n - Python version:3.6.12\r\n - CUDA/cuDNN version:11.0\r\n - GPU models and configuration: \r\n - Any other relevant information: def setup(self)\r\n\r\n### Additional context\r\n\r\nI have to add another argument to setup(self) for it to work, such as setup(self,a), which I won't actually use at all.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5572/comments",
    "author": "Toyhom",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-19T14:36:44Z",
        "body": "the other parameter refers to `stage` which can be 'fit'(called with `trainer.fit`) or 'test'(called with `trainer.test`). Using this you can avoid loading both train & val and test data all at once.\r\nsomething like:\r\n```python\r\ndef setup(self, stage):\r\n    if stage == 'fit':  # will be 'fit' when calling trainer.fit()\r\n        # load train & val data only\r\n    elif stage == 'test':  # will be 'test' when calling trainer.test()\r\n        # load test data only\r\n```"
      }
    ]
  },
  {
    "number": 5027,
    "title": "On \"import pytorch-lightning\": AttributeError: python: undefined symbol: THCudaHalfTensor_normall",
    "created_at": "2020-12-08T19:40:09Z",
    "closed_at": "2020-12-09T14:56:05Z",
    "labels": [
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5027",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nSimply by importing pytorch-lightning, I receive the following error: `AttributeError: python: undefined symbol: THCudaHalfTensor_normall`\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 1, in <module>\r\n    import pytorch_lightning\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/__init__.py\", line 56, in <module>\r\n    from pytorch_lightning import metrics\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/__init__.py\", line 14, in <module>\r\n    from pytorch_lightning.metrics.metric import Metric\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/metric.py\", line 26, in <module>\r\n    from pytorch_lightning.utilities.apply_func import apply_to_collection\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/__init__.py\", line 25, in <module>\r\n    from apex import amp\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg/apex/__init__.py\", line 12, in <module>\r\n    from . import optimizers\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg/apex/optimizers/__init__.py\", line 2, in <module>\r\n    from .fp16_optimizer import FP16_Optimizer\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg/apex/optimizers/fp16_optimizer.py\", line 8, in <module>\r\n    lib.THCudaHalfTensor_normall.argtypes=[ctypes.c_void_p, ctypes.c_void_p]\r\n  File \"/data/nv419/anaconda3/lib/python3.7/ctypes/__init__.py\", line 377, in __getattr__\r\n    func = self.__getitem__(name)\r\n  File \"/data/nv419/anaconda3/lib/python3.7/ctypes/__init__.py\", line 382, in __getitem__\r\n    func = self._FuncPtr((name_or_ordinal, self))\r\nAttributeError: python: undefined symbol: THCudaHalfTensor_normall\r\n```\r\n\r\n### To Reproduce\r\n`import pytorch-lightning`\r\n\r\n### Expected behavior\r\nFor pytorch-lightning to import and be used correctly\r\n\r\n### Environment\r\n* CUDA:\r\n        - GPU:\r\n                - Tesla V100-PCIE-32GB\r\n                - Tesla V100-PCIE-32GB\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.17.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0+cu101\r\n        - tqdm:              4.54.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.7.4\r\n        - version:           #59~18.04.1-Ubuntu SMP Wed Oct 21 12:14:56 UTC 2020\r\n\r\n### Additional context\r\npytorch-lightning version is 1.0.8 (couldn't import it in obviously...)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5027/comments",
    "author": "nihirv",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-09T03:04:09Z",
        "body": "From your error message it looks like you have apex installed. \r\nTry to import apex: \r\n`from apex import amp`\r\nand you will probably see the same error. Check your apex installation, maybe it is incompatible with your pytorch version or it wasn't correctly compiled. "
      },
      {
        "user": "nihirv",
        "created_at": "2020-12-09T14:56:05Z",
        "body": "> From your error message it looks like you have apex installed.\r\n> Try to import apex:\r\n> `from apex import amp`\r\n> and you will probably see the same error. Check your apex installation, maybe it is incompatible with your pytorch version or it wasn't correctly compiled.\r\n\r\nThis was the issue! Fixed it by uninstalling apex (`pip uninstall apex`). Thanks!\r\n\r\nClosing the issue\r\n"
      }
    ]
  },
  {
    "number": 4653,
    "title": "accumulate_grad_batches ignores last batches in epoch if number of steps is not divisible by accumulate_grad_batches?",
    "created_at": "2020-11-13T09:39:10Z",
    "closed_at": "2020-11-13T11:00:50Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4653",
    "body": "Suppose I have accumulate_grad_batches=256 and number of steps in my epoch is 260. Loss is updated only on step number 256 every epoch. I suppose it means that the last 4 batches grads are ignored. Is that correct?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4653/comments",
    "author": "Vozf",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:31:27Z",
        "body": "I suppose we do not ignore for last batches. Can you share a minimal example if it's not working?"
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T10:32:54Z",
        "body": "So what is done with last 6 batches? Is it aggreagated over 6 batches instead of asked 256?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:36:18Z",
        "body": "We call `.backward` and `optimizer.step` `optimizer.zero_grad()` for the last 4 batches."
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T10:44:52Z",
        "body": "So first you accumulate 256 batches and call backward and then you accumulate 4 batches and call backward, correct?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:59:08Z",
        "body": "Yep we accumulate 256 if possible and accumulate the rest of batches if it's not divisible by 256"
      }
    ]
  },
  {
    "number": 3752,
    "title": "Default reduction always applied by `Metric`, even when requesting `'none'` reduction",
    "created_at": "2020-09-30T19:46:37Z",
    "closed_at": "2020-10-05T14:13:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3752",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nMetric reduction doesn't behave the same between the functional and class API when using `reduction='none'`. The functional API applies no reduction as expected, but the class API seems to apply the default reduction regardless.\r\n\r\nI haven't investigated the code yet to find the specific cause of the bug, so I'm not sure how widespread this bug is, but I've encountered it using both the `DiceCoefficient` and my own implementation of the differentiable dice, inheriting from `TensorMetric`.\r\n\r\n### To Reproduce\r\n\r\nGiven a pair of `pred` and `target`, I get the following behavior with 3 class + background segmentation data:\r\n```python\r\n>>> from pytorch_lightning.metrics import DiceCoefficient\r\n>>> from pytorch_lightning.metrics.functional import dice_score\r\n>>> DiceCoefficient(reduction=\"none\")(pred, target)\r\ntensor(0.0800)\r\n>>> dice_score(pred, target, reduction=\"none\")\r\ntensor([0.0876, 0.0937, 0.0586], device='cuda:0')\r\n```\r\nwhere I would have expected both version to give the same result.\r\n\r\nThe class API seems to apply the default reduction of `'elementwise_mean'` even though I requested `'none'`, since:\r\n```python\r\n>>> dice_score(x_hat, x, reduction=\"none\").mean()\r\ntensor(0.0800, device='cuda:0')\r\n```\r\n\r\n### Expected behavior\r\nReduction behavior should be consistent between class and functional API, and to behave like the current functional API.\r\n\r\n### Environment\r\nI just now installed Lightning from Git to ensure that it's not a bug that's already been solved since the last release.\r\n\r\n* CUDA:\r\n        - GPU: TITAN Xp\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 0.9.1rc4\r\n        - tqdm:              4.49.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture: 64bit, ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3752/comments",
    "author": "nathanpainchaud",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-05T13:44:51Z",
        "body": "Hi @nathanpainchaud, running your code example on master produces the correct result (your issue was probably solved by PR #3517). Could you please try upgrading?"
      }
    ]
  },
  {
    "number": 3738,
    "title": "RuntimeError: Input and hidden tensors are not at the same device, found",
    "created_at": "2020-09-30T08:05:07Z",
    "closed_at": "2020-09-30T12:34:03Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3738",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI train LSTM for character level  text generation. At first I initialize hidden and cell with zeros using `torch.zeros`. Unfortunately this tensors are defaultly assigned to the cpu so I get the following error while training\r\n\r\n```python\r\nRuntimeError: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu\r\n```\r\n\r\n### To Reproduce\r\n\r\n#### Model\r\n\r\n```python\r\nclass RNN(pl.LightningModule):\r\n    lr = 0.0005\r\n\r\n    def __init__(self, input_size, hidden_size, embeding_size, n_categories, n_layers, output_size, p):\r\n        super().__init__()\r\n\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        \r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        \r\n        \r\n        self.embeding = nn.Embedding(input_size+n_categories, embeding_size)\r\n        self.lstm = nn.LSTM(embeding_size+n_categories, hidden_size, n_layers, dropout=p)\r\n        self.out_fc = nn.Linear(hidden_size, output_size)\r\n        \r\n        self.dropout = nn.Dropout(p)\r\n        \r\n\r\n    def forward(self, batch_of_category, batch_of_letter, hidden, cell):\r\n        ## letter level operations\r\n        \r\n        embeding = self.dropout(self.embeding(batch_of_letter))\r\n        category_plus_letter = torch.cat((batch_of_category, embeding), 1)\r\n\r\n        #sequence_length = 1\r\n        category_plus_letter = category_plus_letter.unsqueeze(1)\r\n        \r\n        out, (hidden, cell) = self.lstm(category_plus_letter, (hidden, cell))\r\n        out = self.out_fc(out)\r\n        out = out.squeeze(1)\r\n        \r\n        return out, (hidden, cell)\r\n        \r\n\r\n    def configure_optimizers(self):\r\n        optimizer = Adam(self.parameters(), self.lr)\r\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\r\n\r\n        return [optimizer], [scheduler]\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        item_dict = batch\r\n        loss = 0\r\n        batch_of_category = item_dict[\"category_tensors\"]\r\n\r\n        #we loop over letters, single batch at the time \r\n        \r\n        hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\r\n        cell = torcAh.zeros(self.n_layers, 1, self.hidden_size).cuda()\r\n        \r\n        for t in range(item_dict[\"input_tensors\"].size(1)):\r\n            batch_of_letter = item_dict[\"input_tensors\"][:, t]\r\n            \r\n            output, (hidden, cell) = self(batch_of_category, batch_of_letter, hidden, cell)\r\n            \r\n            loss += criterion(output, item_dict[\"target_tensors\"][:, t])\r\n\r\n        loss = loss/(t+1)\r\n\r\n\r\n        tensorboard_logs = {'train_loss': loss}\r\n\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n    \r\n    \r\n    def init_hidden(self, batch_size):\r\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\r\n        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size)\r\n        \r\n        return hidden, cell\r\n```\r\n\r\n#### Batch\r\n\r\n```\r\n(['Russian', 'English', 'Russian', 'English'],\r\n ['Piskarenkov', 'Clarkson', 'Pochkaev', 'Woods'],\r\n tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\r\n tensor([[42,  9, 19, 11,  1, 18,  5, 14, 11, 15, 22],\r\n         [29, 12,  1, 18, 11, 19, 15, 14,  0,  0,  0],\r\n         [42, 15,  3,  8, 11,  1,  5, 22,  0,  0,  0],\r\n         [49, 15, 15,  4, 19,  0,  0,  0,  0,  0,  0]]),\r\n tensor([[ 9, 19, 11,  1, 18,  5, 14, 11, 15, 22, 59],\r\n         [12,  1, 18, 11, 19, 15, 14, 59,  0,  0,  0],\r\n         [15,  3,  8, 11,  1,  5, 22, 59,  0,  0,  0],\r\n         [15, 15,  4, 19, 59,  0,  0,  0,  0,  0,  0]]))\r\n```\r\n\r\n#### Trainer \r\n\r\n```python\r\ndm = NamesDatamodule(1)\r\n\r\nrnn_model = RNN(input_size=ds.n_tokens,\r\n            hidden_size=256,\r\n            embeding_size = 128, \r\n            n_layers=2,    \r\n            n_categories=ds.n_categories,\r\n            output_size=ds.n_tokens,\r\n            p=0.3)\r\n\r\n\r\ntrainer = Trainer(max_epochs=3, \r\n                  logger=None,\r\n                  gpus=1,\r\n                  early_stop_callback=False,\r\n                  checkpoint_callback=False,\r\n                  )\r\n\r\ntrainer.fit(rnn_model, dm)\r\n```\r\n\r\n### Expected behavior\r\n\r\nHidden values should automatically be assigned to the `device`\r\n\r\n### Environment\r\n\r\nGoogle Colab\r\n\r\n - Pytroch 1.6.0+cu101\r\n - Lightning 0.9.1rc3\r\n - Python version:\r\n - GPU models and configuration: single colab GPU\r\n\r\n### Additional context\r\n\r\nProblem can be solved by adding `.cuda()` to the variables but it is not a solution that I think should be necessary \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3738/comments",
    "author": "tugot17",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T10:00:04Z",
        "body": "Try changing to\r\n```python\r\nhidden = torch.zeros(self.n_layers, 1, self.hidden_size)..to(self.device)\r\ncell = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\r\n```"
      },
      {
        "user": "tugot17",
        "created_at": "2020-09-30T11:55:25Z",
        "body": "@rohitgr7 Yeah, this fixes the problem, however I'm not entirely sure it will also work in case if I used more then a single machine to train the model "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T12:02:18Z",
        "body": "`self.device` will always give you the device for the current process(ddp) or current batch(dp) being executed."
      }
    ]
  },
  {
    "number": 2939,
    "title": "mlflow checkpoints in the wrong location ",
    "created_at": "2020-08-12T22:58:48Z",
    "closed_at": "2020-08-15T10:54:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2939",
    "body": "I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2939/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-13T06:30:27Z",
        "body": "@david-waterworth mind try the latest 0.9rc12?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-14T06:15:19Z",
        "body": "It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n"
      }
    ]
  },
  {
    "number": 2679,
    "title": "Default checkpoint location problematic when using docker ",
    "created_at": "2020-07-23T17:53:41Z",
    "closed_at": "2020-08-11T14:11:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2679",
    "body": "The default behavior of `ModelCheckpoint` is to use `os.getcwd()`. Outside my docker container, this ended up being the same directory where my tensorboard logs were saved (e.g. `/my/dir/tb_logs/default/version_0/checkpoints/`).  But inside the docker container, it saved to the internal working directory (e.g. `/home/default/version_0/checkpoints/`). Since this location disappeared along with the container, the checkpoint was gone, and there was no warning raised to explain why.\r\n\r\nRequiring a checkpoint directory isn't desirable, but I'd like to help others avoid this grief in the future. Is there a better way to infer a default location than `os.getcwd()`? Something as simple as a print statement with the checkpoint location would have saved me a lot of time troubleshooting.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2679/comments",
    "author": "drStacky",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-08T05:21:33Z",
        "body": "You can set the `default_root_dir` arg in the Trainer. Is that what you want? Otherwise there is an option verbose in the ModelCheckpoint callback which, when turned on, should print the file path everytime it saves."
      },
      {
        "user": "drStacky",
        "created_at": "2020-08-11T14:11:44Z",
        "body": "Somehow I misread the explanation of `default_root_dir`. I thought it only changed the name of the `default` directory, not the whole path. This is exactly what I needed. Thanks!"
      }
    ]
  },
  {
    "number": 2670,
    "title": "bug in pytorch_lightning.metrics.functional.auroc",
    "created_at": "2020-07-22T09:40:31Z",
    "closed_at": "2020-08-03T22:29:51Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2670",
    "body": "the code:\r\n```\r\ndef validation_epoch_end(self, outputs):\r\n        .........\r\n        print(total_y_hat.device)\r\n        print(total_y_true.device)\r\n        print(total_y_hat)\r\n        print(total_y_true)\r\n        print(total_y_hat.shape)\r\n        print(total_y_true.shape)\r\n        auc_score = auroc(total_y_hat, total_y_true)\r\n```\r\nthe output is:\r\n```\r\nGet data done!\r\nValidation sanity check:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00,  1.06it/s]\r\n\r\ncuda:0\r\ncuda:0\r\ntensor([0.5084, 0.5084, 0.5084,  ..., 0.5084, 0.5084, 0.5084], device='cuda:0')\r\ntensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\r\ntorch.Size([16384])\r\ntorch.Size([16384])\r\nTraceback (most recent call last):\r\n  File \"lighting_sales.py\", line 443, in <module>\r\n    main(hparams)\r\n  File \"lighting_sales.py\", line 392, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in fit\r\n    self.single_gpu_train(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 176, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1076, in run_pretrain_routine\r\n    False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 330, in _evaluate\r\n    eval_results = model.validation_epoch_end(outputs)\r\n  File \"lighting_sales.py\", line 252, in validation_epoch_end\r\n    auc_score = auroc(total_y_hat, total_y_true)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 817, in auroc\r\n    return _auroc(pred=pred, target=target, sample_weight=sample_weight, pos_label=pos_label)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 766, in new_func\r\n    x, y = func_to_decorate(*args, **kwargs)[:2]\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 815, in _auroc\r\n    return roc(pred, target, sample_weight, pos_label)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 553, in roc\r\n    pos_label=pos_label)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 504, in _binary_clf_curve\r\n    torch.tensor([target.size(0) - 1])])\r\nRuntimeError: All input tensors must be on the same device. Received cuda:0 and cpu\r\n```\r\n          \r\n           ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2670/comments",
    "author": "BeHappyForMe",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-07-22T11:19:04Z",
        "body": "That bug is fixed on master. See #2657 "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-22T20:31:02Z",
        "body": "Does it work with master branch? If not, do you use the functional or module interface for the metric?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-08-03T22:29:51Z",
        "body": "@BeHappyForMe I'm closing this but please open if still experiencing this with master."
      }
    ]
  },
  {
    "number": 2538,
    "title": "tensor_metric decorator does not let you return Tuple or List ouputs",
    "created_at": "2020-07-07T10:51:10Z",
    "closed_at": "2020-07-07T13:17:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2538",
    "body": "## \ud83d\udc1b Bug\r\n\r\nWhen creating a metric function that returns multiple outputs in the form of a Tuple or List the metric class complains that it can't convert a Tuple or List to a tensor, even though the contents of the Tuple/List are tensors.\r\n\r\n### To Reproduce\r\n\r\nAn example of this would be a function  to return the topk accuracy \r\n\r\n```\r\n\r\n    @tensor_metric()\r\n    def accuracy(output, target, topk=(1,)):\r\n        \"\"\"Computes the precision@k for the specified values of k\"\"\"\r\n\r\n        maxk = max(topk)\r\n        batch_size = target.size(0)\r\n    \r\n        _, pred = output.topk(maxk, 1, True, True)\r\n        pred = pred.t()\r\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n    \r\n        res = []\r\n        for k in topk:\r\n            correct_k = correct[:k].view(-1).float().sum(0)\r\n            res.append(correct_k.mul_(100.0 / batch_size))\r\n        return res\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        acc = self.accuracy(y_hat, y, topk=(1, 5))\r\n\r\n```\r\n\r\n### Error Output\r\n\r\n```\r\n\r\nEpoch 1:   0%|          | 0/1876 [00:00<?, ?it/s] Traceback (most recent call last):\r\n  File \"/home/local/CORP/dbyrne/Documents/Projects/RL/pytorch-lightning-bolts/pl_bolts/models/mnist_module.py\", line 138, in <module>\r\n    trainer.fit(model)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 912, in fit\r\n    self.dp_train(model)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 258, in dp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1093, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 375, in train\r\n    self.run_training_epoch()\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 458, in run_training_epoch\r\n    _outputs = self.run_training_batch(batch, batch_idx)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 634, in run_training_batch\r\n    loss, batch_output = optimizer_closure()\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 598, in optimizer_closure\r\n    output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 754, in training_forward\r\n    output = self.model(*args)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 65, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 69, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 209, in parallel_apply\r\n    raise output\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 166, in _worker\r\n    output = module.training_step(*input, **kwargs)\r\n  File \"/home/local/CORP/dbyrne/Documents/Projects/RL/pytorch-lightning-bolts/pl_bolts/models/mnist_module.py\", line 52, in training_step\r\n    acc = self.accuracy(y_hat, y, topk=(1, 5))\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py\", line 58, in new_func\r\n    result = function_to_decorate(*args, **kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py\", line 59, in new_func\r\n    return func_to_apply(result, *dec_args, **dec_kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py\", line 84, in _convert_to_tensor\r\n    raise TypeError(f\"The given type ('{type(data).__name__}') cannot be converted to a tensor!\")\r\nTypeError: The given type ('list') cannot be converted to a tensor!\r\nException ignored in: <function tqdm.__del__ at 0x7f8c82724ae8>\r\nTraceback (most recent call last):\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1086, in __del__\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1293, in close\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1471, in display\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1089, in __repr__\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1433, in format_dict\r\nTypeError: cannot unpack non-iterable NoneType object\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n### Environment\r\n\r\n - PyTorch Version: 1.4\r\n - OS: Linux\r\n - How you installed PyTorch: Conda\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: RTX 2080",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2538/comments",
    "author": "djbyrne",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-07-07T12:54:59Z",
        "body": "Could you try to use the decorator `tensor_collection_metric`? The intention with `tensor_metric` is that should decorate metric functions that returns a single tensor, whereas `tensor_collection_metric` is meant to be used on a collection of metrics.\r\n\r\nOtherwise you could just stack your `res` list into a tensor: `res=torch.cat(res)`."
      }
    ]
  },
  {
    "number": 2400,
    "title": "CrossEntropyLoss fails to run with GPU",
    "created_at": "2020-06-28T15:04:30Z",
    "closed_at": "2020-06-29T01:47:41Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2400",
    "body": "## \ud83d\udc1b Bug\r\n\r\nUsing the following `training_step` method which uses `nn.CrossEntropyLoss()` loss function:\r\n\r\n```python\r\n    def training_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        predict = self(x1, x2)\r\n        target = torch.arange(x1.size()[0])\r\n        loss = self.loss_fn(predict, target)\r\n        return {'loss': loss}\r\n```\r\nfails to run with GPU throwing the following error:\r\n\r\n```python\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'target' in call to _thnn_nll_loss_forward\r\n```\r\nThe function `self.loss_fn` is shown below:\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning import LightningModule\r\nfrom torch import nn\r\n\r\n\r\nclass NPairsLoss(LightningModule):\r\n    \"\"\"\r\n    The N-Pairs Loss.\r\n    It measures the loss given predicted tensors x1, x2 both with shape [batch_size, hidden_size],\r\n    and target tensor y which is the identity matrix with shape  [batch_size, batch_size].\r\n    \"\"\"\r\n\r\n    def __init__(self, alpha=100):\r\n        super(NPairsLoss, self).__init__()\r\n        self.ce = nn.CrossEntropyLoss()\r\n        self.alpha = alpha\r\n\r\n    def similarities(self, x1, x2):\r\n        \"\"\"\r\n        Calculates the cosine similarity matrix for every pair (i, j),\r\n        where i is an embedding from x1 and j is another embedding from x2.\r\n\r\n        :param x1: a tensors with shape [batch_size, hidden_size].\r\n        :param x2: a tensors with shape [batch_size, hidden_size].\r\n        :return: the cosine similarity matrix with shape [batch_size, batch_size].\r\n        \"\"\"\r\n        x1 = x1 / torch.norm(x1, dim=1, keepdim=True)\r\n        x2 = x2 / torch.norm(x2, p=2, dim=1, keepdim=True)\r\n        return self.alpha * torch.matmul(x1, x2.t())\r\n\r\n    def forward(self, predict, target):\r\n        \"\"\"\r\n        Computes the N-Pairs Loss between the target and predictions.\r\n        :param predict: the prediction of the model,\r\n        Contains the batches x1 (image embeddings) and x2 (description embeddings).\r\n        :param target: the identity matrix with shape  [batch_size, batch_size].\r\n        :return: N-Pairs Loss value.\r\n        \"\"\"\r\n        x1, x2 = predict\r\n        predict = self.similarities(x1, x2)\r\n        # by construction the probability distribution must be concentrated on the diagonal of the similarities matrix.\r\n        # so, Cross Entropy can be used to measure the loss.\r\n        return self.ce(predict, target)\r\n```\r\nIs `target = torch.arange(x1.size()[0])` not being created in the GPU?\r\n\r\n### Expected behavior\r\n\r\nThat  target tensor (`target = torch.arange(x1.size()[0])`) is created on the GPU. \r\n\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce RTX 2080\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.0\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.1\r\n\t- pytorch-lightning: 0.8.1\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.7.3\r\n\t- version:           #41-Ubuntu SMP Tue Dec 3 00:27:35 UTC 2019\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2400/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-06-28T17:14:21Z",
        "body": "No, you have to move `target = torch.arange(x1.size()[0])` to the GPU(or any other device you want) because it's not present in the batch from the dataloader.\r\nYou can use `target = torch.arange(x1.size()[0]).to(x.get_device())`."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-29T01:48:09Z",
        "body": "You can also use:\r\n```\r\narget = torch.arange(x1.size()[0]).to(self.device)\r\n```\r\n\r\nthe PL module knows what device it is on."
      },
      {
        "user": "taylorchu",
        "created_at": "2020-07-04T07:56:32Z",
        "body": "@williamFalcon is there a reason why this is not managed by lightning?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-04T11:02:20Z",
        "body": "@taylorchu If you pass that from the DataLoader(or Dataset) itself it will be handled automatically, but if a tensor is created in between the procedure by the user itself, one has to move it to the device manually the PyTorch way."
      }
    ]
  },
  {
    "number": 2238,
    "title": "when no checkpoints are saved, test fails",
    "created_at": "2020-06-18T15:02:14Z",
    "closed_at": "2020-07-10T01:20:47Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2238",
    "body": "Build a model that doesn't save a checkpoint and this crashes.\r\nIt should use the last model instead.\r\n```\r\nmodel = ...\r\ntrainer = Trainer(fast_dev_run)\r\ntrainer.fit(model)\r\ntrainer.test()\r\n```\r\n\r\n@yukw777 \r\n```\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <torch.serialization._open_file object at 0x7f443d4d88d0>, name = ''\r\nmode = 'rb'\r\n\r\n    def __init__(self, name, mode):\r\n>       super(_open_file, self).__init__(open(name, mode))\r\nE       FileNotFoundError: [Errno 2] No such file or directory: ''\r\n```\r\n\r\n\r\nThe test for this should be:\r\n\r\n```\r\ndef test_no_ckpt_test(tmpdir):\r\n    model = EvaluationModel()\r\n    trainer = Trainer(fast_dev_run)\r\n    trainer.fit(model)\r\n    trainer.test()\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2238/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-24T18:24:33Z",
        "body": "```    trainer.test(model)```"
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-24T18:40:40Z",
        "body": "@nischal-sanil could you help me understand the issue better by explaining how you don\u2019t have any models saved?"
      },
      {
        "user": "nischal-sanil",
        "created_at": "2020-06-25T05:39:15Z",
        "body": "I was running the Trainer with `fast_dev_run` set to True, hence there were no checkpoints. Changing  `trainer.test()` to `trainer.test(model)` has resolved the issue.\r\nThanks,"
      },
      {
        "user": "nischal-sanil",
        "created_at": "2020-06-26T01:17:51Z",
        "body": "Hi again,\r\n\r\nConsider the following code snippet\r\n\r\n```\r\nclass Net(LightningModule):\r\n    ...\r\n\r\nresnet18 = models.resnet18(pretrained=True)\r\nmodel = Net(resnet18)\r\n\r\n# parameters before training\r\nold_params = model.parameters()\r\n\r\ntrainer = Trainer(max_steps=10) \r\ntrainer.fit(model)  \r\n\r\n# parameters after training \r\nnew_params = model.parameters()\r\n```\r\n\r\nWhere I store the parameters of the model before and after training in `old_params` and `new_params`, a check on their equality returns `True`, suggesting there're no changes to the parameters.\r\n\r\n```\r\ndef check_params(old,new):\r\n    return all([torch.equal(o,n) for o,n in zip(old,new)])\r\n\r\ncheck_params(old_params,new_params)\r\n## True\r\n```\r\n Therefore, while running `trainer.test(model)` the trained model is not used for testing. I had assumed that the `Trainer` would have updated the parameters to the `model`, but from the above snippet that does not seem to be the case.  And I am not able to figure out if this is an expected behavior of the `model` or there is a problem with my class definition. If this is the expected behavior of the `model`, then are there any work around for calling `trainer.test()` without using a checkpoint because in my case loading from a checkpoint results in an error similar to this issue #2359.\r\n"
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-29T18:52:09Z",
        "body": "@williamFalcon is it true that if `fast_dev_run` is `True`, there is no checkpoints saved? I just tried to manually check this in `test_test_checkpoint_path`, but `checkpoint_callback` of the trainer still had a valid `best_model_path` even when `fast_dev_run` is `True`.\r\n\r\n@nischal-sanil your code comparing the old parameters to the new parameters is not quite correct. Since the tensors returned by `model.parameters()` are \"references\" to the underlying data, both `old_params` and `new_params` would point to the same \"trained\" tensors, hence `check_params()` would return `True`. You'd have to do something like `old_params = [p.clone() for p in model.parameters()]` to get around that.\r\n\r\n"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-29T18:59:52Z",
        "body": "fast_dev_run should not save checkpoints (nor write a logs file). it's like a \"compiler\""
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-29T19:55:15Z",
        "body": "@williamFalcon hmm maybe there's a bug? I see this:\r\n\r\n```\r\n\u276f python pl_examples/basic_examples/cpu_template.py --fast_dev_run True\r\n/Users/peteryu/.pyenv/versions/pl/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\r\nRunning in fast_dev_run mode: will run a full train, val and test loop using a single batch\r\nGPU available: False, used: False\r\nTPU available: False, using: 0 TPU cores\r\n\r\n  | Name      | Type        | Params | In sizes   | Out sizes \r\n--------------------------------------------------------------------\r\n0 | c_d1      | Linear      | 39 M   | [2, 784]   | [2, 50000]\r\n1 | c_d1_bn   | BatchNorm1d | 100 K  | [2, 50000] | [2, 50000]\r\n2 | c_d1_drop | Dropout     | 0      | [2, 50000] | [2, 50000]\r\n3 | c_d2      | Linear      | 500 K  | [2, 50000] | [2, 10]   \r\nEpoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.18it/s, loss=2.523, v_num=0]\r\n\u276f ls lightning_logs/version_0/checkpoints/epoch=0.ckpt \r\nlightning_logs/version_0/checkpoints/epoch=0.ckpt\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-10T01:20:47Z",
        "body": "added a warning and return {}"
      }
    ]
  },
  {
    "number": 2081,
    "title": "RuntimeError: Address already in use on 'ddp' mode pl 0.8.0",
    "created_at": "2020-06-05T10:58:42Z",
    "closed_at": "2020-06-05T14:33:40Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2081",
    "body": " Trainer configuration:\r\n```\r\n    trainer = pl.Trainer(\r\n        logger= CometLogger( api_key=\"ID\"),\r\n        auto_select_gpus=True,\r\n        gpus=3,\r\n        distributed_backend=\"ddp\",\r\n   )\r\n```\r\nThe error:\r\n```\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2]\r\nCometLogger will be initialized in online mode\r\nCometLogger will be initialized in online mode\r\ninitializing ddp: LOCAL_RANK: 0/2 WORLD_SIZE:3\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 156, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"train.py\", line 148, in main_train\r\n    trainer.fit(model)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 884, in fit\r\n    self.spawn_ddp_children(model)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 395, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 425, in ddp_train\r\n    model.init_ddp_connection(self.proc_rank, self.world_size, self.is_slurm_managing_tasks)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 962, in init_ddp_connection\r\n    torch_distrib.init_process_group(torch_backend, rank=proc_rank, world_size=world_size)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 393, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 172, in _env_rendezvous_handler\r\n    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\nRuntimeError: Address already in use\r\n```\r\nEnv\r\n```\r\n* CUDA:\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.8.0-dev\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.46.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.7.7\r\n        - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2081/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-05T11:09:11Z",
        "body": "check ps -elf | grep python. maybe a previous run is occupying that port.  "
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-10T17:11:04Z",
        "body": "Does this mean I can't launch multiple DDP jobs on the same node with many GPUs because the port will conflict? (e.g. 2 independent DDP jobs requiring 4 GPU each in an 8 GPU machine)\r\n\r\nEDIT: it seems that I can set `MASTER_PORT` env var to avoid this issue, correct? If so it'd be nice if lightning can detect this and use a new port automatically :)"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-10T19:36:16Z",
        "body": "you can... just the the MASTER_PORT env var.\r\n\r\nLightning does in fact pick a random port... but you set the seed, so it's always the same haha. We need to disable the seed for the port choosing or continue trying ports if the ports are taken.\r\n\r\nThis would be a great PR!"
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-10T20:35:49Z",
        "body": "@williamFalcon PR submitted at #2140"
      },
      {
        "user": "ShanakaRG",
        "created_at": "2023-12-19T01:55:53Z",
        "body": "` kill -9 $(ps aux | grep main.py | grep -v grep | awk '{print $2}')` \r\n\r\nThis solved my problem. However, if I did not use `kill -9` it does not work for me."
      },
      {
        "user": "chenfengshijie",
        "created_at": "2024-06-24T09:35:21Z",
        "body": "Use torchrun instead of python solve this problem?Since torchrun can choose a free port to launch."
      }
    ]
  },
  {
    "number": 1155,
    "title": "No validation checks when overfit_pct is set",
    "created_at": "2020-03-15T13:43:17Z",
    "closed_at": "2020-05-03T23:15:57Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1155",
    "body": "## \ud83d\udc1b Bug\r\n\r\nWhen setting the `overfit_pct` to any value between 0 and 1 (exclusive) in trainer, the validation checks are disabled.\r\n\r\n### To Reproduce\r\n\r\nI have worked on a minimal example to reproduce the bug:\r\n\r\n```python3\r\nimport pytorch_lightning as pl\r\nimport torch\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Dataset, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n    def __getitem__(self, idx):\r\n        X = torch.rand(1, self.input_dim)\r\n        y = torch.randint(0, self.output_dim, (1,))\r\n        return X, y\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(input_dim, output_dim)\r\n        self.dataset = Dataset(input_dim, output_dim)\r\n\r\n    def forward(self, x, y):\r\n        yhat = torch.softmax(self.layer(x), -1)\r\n        return F.nll_loss(logits, y)\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'loss': loss, 'log': {'loss': loss}}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Model(100, 10)\r\n    trainer = pl.Trainer(overfit_pct=.01)\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nValidation checks occur normally\r\n\r\n### Environment\r\n```bash\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Manjaro Linux\r\nGCC version: (GCC) 8.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.7.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] mkl                       2020.0                      166  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.7.1                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1155/comments",
    "author": "qmeeus",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T04:01:07Z",
        "body": "~~Yes there is a bug here~~, but I had to fix @qmeeus's code sample to make it visible. \r\nThe sanity validation checks run, but the validation at the end of the epoch doesn't.\r\nWhen setting `overfit_pct=1`, validation checks work as expected.\r\nHere is the fixed minimal code sample:\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Dataset, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n    def __getitem__(self, idx):\r\n        X = torch.rand(self.input_dim)\r\n        y = torch.randint(0, self.output_dim, (1,))\r\n        return X, y\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(input_dim, output_dim)\r\n        self.dataset = Dataset(input_dim, output_dim)\r\n\r\n    def forward(self, x, y):\r\n        logits = torch.softmax(self.layer(x), -1)\r\n        return F.nll_loss(logits, y.flatten(0))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'loss': loss, 'log': {'loss': loss}}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        print('see that validation runs only in sanity check')\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n    def validation_end(self, outputs):\r\n        loss = torch.stack([output['val_loss'] for output in outputs]).mean()\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Model(100, 10)\r\n    trainer = pl.Trainer(overfit_pct=0.1, max_epochs=10)\r\n    trainer.fit(model)\r\n```\r\nFor the record, @qmeeus your code had these issues:\r\n- No val_dataloader defined\r\n- Wrong shapes returned in dataloader\r\n- Wrong shape for nll_loss labels"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T04:14:27Z",
        "body": "Actually `overfit_pct `argument is not documented in the Trainer class. We should fix that and say that setting `overfit_pct `is the same as setting `train_percent_check`, `val_percent_check `and `test_percent_check`."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T06:26:11Z",
        "body": "**False alarm!** Turns out it is simply because you chose a too small value for `overfit_pct`. \r\nYour dataset has size 1000, and dataloader has batch_size 64. \r\n1000 / 64 ~= 15 batches\r\nWhen you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch. \r\n\r\n@qmeeus Please let me know if it isn't clear. I think the behaviour of `overfit_pct `is correct."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T06:30:40Z",
        "body": "@williamFalcon Should we make it so that `overfit_pct `does not round to 0 batches?\r\n"
      }
    ]
  }
]