[
  {
    "number": 37677,
    "title": "[<Ray component: Cluster>] KeyError: 'CPU' error in Linux",
    "created_at": "2023-07-22T10:49:28Z",
    "closed_at": "2023-07-24T21:18:06Z",
    "labels": [
      "question",
      "triage",
      "core"
    ],
    "url": "https://github.com/ray-project/ray/issues/37677",
    "body": "### What happened + What you expected to happen\r\n\r\n**What I will do:**\r\nI tried to get the total number of cpus provided by the cluster;\r\n\r\n**What I got wrong:**\r\nThe specific error information is as follows:\r\n{cluster_resources()['CPU']} CPU resources in total;\r\nKeyError: 'CPU'\r\n\r\n**Update:**\r\n_I seem to have found the reason, when there is no available cpu in the cluster, the 'CPU' key is no longer in the returned dict; This leads to errors;_\r\n\r\n### Versions / Dependencies\r\n\r\nray: 2.3.1\r\nos: debian 11\r\npython: 3.9.2\r\n\r\n### Reproduction script\r\n\r\nfrom ray import init, cluster_resources\r\ninit()\r\nprint(f\"{cluster_resources()['CPU']}\")\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/37677/comments",
    "author": "stevenhubhub",
    "comments": [
      {
        "user": "jjyao",
        "created_at": "2023-07-24T21:18:06Z",
        "body": "Yea, try to do `cluster_resources().get(\"CPU\", 0)`"
      },
      {
        "user": "davide-russo-tfs",
        "created_at": "2024-09-30T10:51:06Z",
        "body": "Good morning, I have the same issue while trying to use Ray on Databricks cluster (with autoscaling). The runtime used is 15.1ML.\r\nI imported the following libraries:\r\n```\r\nfrom ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\r\nfrom ray.util.multiprocessing import Pool\r\nimport ray\r\n```\r\nThis is how I set up the environment:\r\n```\r\nsetup_ray_cluster(\r\n        num_worker_nodes  = 4,\r\n        num_cpus_per_node = 4,\r\n        autoscale          = True\r\n    )\r\nray.init(ignore_reinit_error = True)\r\n```\r\nthen I decorated a function to be run in parallel by using `@ray.remote` and tried to create a pool of processes this way:\r\n```\r\nwith Pool(processes = 8) as pool:\r\n        pool.starmap(foo, inputs)\r\n```\r\n\r\nHow can I solve this problem? Thank you for your help."
      }
    ]
  },
  {
    "number": 31151,
    "title": "ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...>] ",
    "created_at": "2022-12-16T06:43:57Z",
    "closed_at": "2022-12-16T12:58:13Z",
    "labels": [
      "question",
      "docs"
    ],
    "url": "https://github.com/ray-project/ray/issues/31151",
    "body": "### Description\n\nHellow,i'm sorry to bother you.I want to use ray 2.0.0.dev0.But I don't know where I can find it.There is only version ray 3.0.0.dev0 in the documentation.Can you tell me where i can get it,thanks!\n\n### Link\n\n_No response_",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/31151/comments",
    "author": "aa-oo",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2022-12-16T12:56:58Z",
        "body": "Hey @aa-oo , thanks for filing this issue.\r\n* All versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n* To get a stable version of Ray, you can simply try `pip install ray==2.2` (brand new one) or some older versions like `pip install ray==2.1` or `pip install ray==2.0`."
      }
    ]
  },
  {
    "number": 13576,
    "title": "[tune] Errors when using points_to_evaluate argument for a few search algos",
    "created_at": "2021-01-20T05:28:07Z",
    "closed_at": "2021-01-21T02:23:26Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/13576",
    "body": "I am trying out a few search algos (run from scratch) with Ray version 1.1.0, and is running into a few issues when using `points_to_evaluate` argument:\r\n\r\nSearch Space Setup:\r\n```\r\neven_int_model_dim = [x for x in range(1, 12+1) if x % 2 == 0]\r\neven_int_batch_size = [x for x in range(1, 16+1) if x % 2 == 0]\r\nint_sequence = [x for x in range(20, 100+1)]\r\nint_local_context_len = [x for x in range(3, 15+1)]\r\nint_num_heads = [x for x in range(2, 6+1)]\r\n\r\nconfig={'seed': 0, 'train_start_date': 'None', 'valid_start_date': '2017-01-01', 'test_start_date': '2018-01-01',\r\n\t'data_path': 'path/to/data', 'num_epoch': 100, 'loss_fn': loss_fn, 'device': 'cuda:0', \r\n\t'sequence': tune.choice(int_sequence),\r\n\t'local_context_len': tune.choice(int_local_context_len), 'batch_size': tune.choice(even_int_batch_size), 'num_heads': tune.choice(int_num_heads),\r\n\t'model_dim': tune.choice(even_int_model_dim), 'num_layers': tune.choice([1, 2]), 'dropout': tune.uniform(0, 0.7),\r\n\t'allocator': 'numark', 'max_weight': 0.1, 'stochasticity': tune.choice([True, False]),\r\n\t'resample': False, 'n_draws': 100, 'n_portfolios': 5, 'feature_dims': 0, \r\n\t'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, \r\n\t'eps': 0.00000001, 'weight_decay': 0, 'amsgrad': True}\r\n\r\ncurrent_best = [{'sequence': 100, 'local_context_len': 5, 'batch_size': 16, \r\n\t\t 'num_heads': 6, 'model_dim': 12, 'num_layers': 1, 'dropout': 0.01, 'stochasticity': False}]\r\n```\r\n\r\nWhen using `HyperOpt`:\r\n`algo = HyperOptSearch(points_to_evaluate=current_best)`\r\nError:\r\n> File \"/home/user/anaconda3/envs/user/lib/python3.7/site-packages/hyperopt/pyll/base.py\", line 874, in rec_eval\r\n>     rval_var = node.pos_args[int(switch_i) + 1]\r\n> IndexError: list index out of range\r\n\r\nWhen using `Optuna`:\r\n`algo = OptunaSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'\r\n\r\nWhen using `Ax`:\r\n`algo = AxSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13576/comments",
    "author": "turmeric-blend",
    "comments": [
      {
        "user": "krfricke",
        "created_at": "2021-01-20T08:35:22Z",
        "body": "Hi @turmeric-blend, the `points_to_evaluate` arguments for most algorithms are currently only available in the nightly wheels / current master and not in the latest release.\r\nYou can try `ray install-nightly` to install the nightly wheels. "
      }
    ]
  },
  {
    "number": 13517,
    "title": "[tune] how to enforce even integer number in the search space",
    "created_at": "2021-01-18T08:18:39Z",
    "closed_at": "2021-01-19T09:11:44Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/13517",
    "body": "I require my `batch_size` in the search space to be an **even number**, so I tried `tune.qrandint(4, 64, 2)`. However I am also using `HyperOpt` which gave this warning:\r\n\r\n> HyperOpt does not support quantization for integer values. Reverting back to 'randint'.\r\n\r\nMaking certain trials contain odd `batch_size` which produces error in my model. Is there another way to enforce even integer number in the search space?\r\n\r\nEDIT:\r\n\r\nI also tried `tune.sample_from(lambda spec: np.random.randint(2, 64) * 2)`, and HyperOpt gave error:\r\n\r\n> HyperOpt does not support parameters of type `Function` with samplers of type `NoneType`",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/13517/comments",
    "author": "turmeric-blend",
    "comments": [
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-18T08:56:32Z",
        "body": "solved by directly using `np.random.randint(2, 128)*2` in config search space instead of `tune.`"
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-19T00:41:36Z",
        "body": "reopening as it actually just returned a constant instead of random even integer between 2 and 128. Issue remains how to  enforce even integer number in the search space?"
      },
      {
        "user": "richardliaw",
        "created_at": "2021-01-19T04:31:36Z",
        "body": "Can you try doing a tune.randint, and then in your training function, multiply it by 2?"
      },
      {
        "user": "turmeric-blend",
        "created_at": "2021-01-19T06:32:27Z",
        "body": "I assume you mean\r\n\r\n```\r\nconfig={..., 'batch_size': tune.randint(1, 64), ...}\r\n\r\n\r\ndef train_function(config):\r\n      batch_size = config['batch_size']*2\r\n```\r\n\r\nI guess it works but then I have to always be aware that my range is half the max range in `randint`.\r\n\r\nAnyway, I did something like this instead, `tune.choice([x for x in range(1, 17) if x % 2 == 0])` which works well."
      },
      {
        "user": "krfricke",
        "created_at": "2021-01-19T09:11:44Z",
        "body": "Please note that this has been improved upon in the latest master and the ray nightly wheels. The next release will support quantized integers in hyperopt out of the box (e.g. using `tune.qrandint()`).\r\n\r\nPlease re-open if you have any more questions."
      }
    ]
  },
  {
    "number": 11971,
    "title": "[rllib] PPO ICM learning rate",
    "created_at": "2020-11-12T13:05:46Z",
    "closed_at": "2020-11-14T11:21:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/11971",
    "body": "Hello, I know the default ppo learning rate is 5e-5, default curiosity learning rate is 0.001. \r\nI just want to know whether the two learning rate are same?   \r\n\r\nIf I use curiosity in ppotrainer, how do I set it?\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11971/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-11-13T08:10:26Z",
        "body": "Hey @zzchuman , no they are not the same. The curiosity module has its own optimizer and lr.\r\nYou can set the curiosity lr inside the exploration_config key, the same way as it's done in rllib/utils/explorations/tests/test_curiosity.py:\r\n```\r\n            config[\"exploration_config\"] = {\r\n                \"type\": \"Curiosity\",\r\n                \"eta\": 0.2,\r\n                \"lr\": 0.001,  # <- HERE\r\n                \"feature_dim\": 128,\r\n                \"feature_net_config\": {\r\n                    \"fcnet_hiddens\": [],\r\n                    \"fcnet_activation\": \"relu\",\r\n                },\r\n                \"sub_exploration\": {\r\n                    \"type\": \"StochasticSampling\",\r\n                }\r\n            }\r\n```"
      }
    ]
  },
  {
    "number": 10312,
    "title": "[ray] How to startup workers more than number of cores",
    "created_at": "2020-08-25T14:55:28Z",
    "closed_at": "2020-08-26T05:16:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10312",
    "body": "How to set ray startup arguments to let 150 workers running on a 96 cores machine? I notice ray will auto-scale on the local machine, but how to set while running a cluster?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10312/comments",
    "author": "Seraphli",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-26T01:24:12Z",
        "body": "Just set --num-cpus=150! "
      }
    ]
  },
  {
    "number": 8622,
    "title": "Restoring from checkpoint on different machine",
    "created_at": "2020-05-26T17:23:53Z",
    "closed_at": "2020-05-26T19:35:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8622",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI trained a model on one machine and I am able to load and restore properly from a saved checkpoint on that machine. When I copied over the checkpoint and try to restore and execute on a different machine, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_rllib_model.py\", line 77, in <module>\r\n    test_agent.restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/tune/trainable.py\", line 417, in restore\r\n    self._restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 639, in _restore\r\n    self.__setstate__(extra_data)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 192, in __setstate__\r\n    Trainer.__setstate__(self, state)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 1070, in __setstate__\r\n    self.optimizer.restore(state[\"optimizer\"])\r\nAttributeError: 'NoneType' object has no attribute 'restore'\r\n```\r\nAm I missing some files or something? I've copied over all the files in the checkpoint directory and also the `params.pkl` and `params.json` file. \r\nI'm creating a trainer instance and restoring from the checkpoint. \r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8622/comments",
    "author": "jangkj09",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-05-26T18:14:29Z",
        "body": "Maybe the ray versions are different on the two machines? Could you post your specs?"
      },
      {
        "user": "jangkj09",
        "created_at": "2020-05-26T19:35:30Z",
        "body": "Yes, that solved the problem. On one machine I had the stable 0.8.5 and on the other I had 0.9.0.dev0\r\nRe-installing with 0.8.5 resolved the problem. \r\n\r\nI don't know if this exists somewhere in the docs, but it would be helpful to indicate this difference more explicitly. I spent over an hour trying to debug what was going on. Thanks!"
      }
    ]
  },
  {
    "number": 8545,
    "title": "[ray] Is it bad practice to use sockets (pyzmq) to communicate between ray remote functions?",
    "created_at": "2020-05-22T06:17:38Z",
    "closed_at": "2020-05-27T15:03:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8545",
    "body": "I have a `send()` function that generates random numpy arrays at every time step, and a `recv()` function that receives and prints those generated arrays. I am using `zmq` for sending/receiving the numpy arrays across the processes, and `pyarrow` to serialize and deserialize arrays. I wasn't able to find any examples using ray and zmq together, so I would like to know whether this is bad practice. If so, is there a recommended way to have the distributed-ly running processes communicate with each other using ray?\r\n\r\nThank you so much! \r\n\r\nPasted below is minimal working code (on Ubuntu 18.0.4, python=3.6.9, pyzmq=19.0.1, ray=0.8.5, pyarrow=0.17.1):\r\n\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport ray\r\nimport zmq\r\nray.init()\r\n\r\n\r\n@ray.remote\r\ndef send():\r\n    port = 5556\r\n    context = zmq.Context()\r\n    send_socket = context.socket(zmq.PUSH)\r\n    send_socket.bind(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        msg = np.random.rand(1, 3) # this could be larger, e.g. numpy-ed torch neural network weights\r\n        object_id = pa.serialize(msg).to_buffer()\r\n        send_socket.send(object_id)\r\n\r\n@ray.remote\r\ndef recv():        \r\n    port = 5556\r\n    context = zmq.Context()\r\n    recv_socket = context.socket(zmq.PULL)\r\n    recv_socket.connect(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        object_id = recv_socket.recv()\r\n        msg = pa.deserialize(object_id)\r\n        print(msg)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.wait([send.remote(), recv.remote()])\r\n```\r\n## Note:\r\nI had to use pyarrow for serialization since ray object id's (obtained via `ray.put()`) could not be passed through zmq sockets; doing so gives the error below:  \r\n```\r\nObjectID(45b95b1c8bd3a9c4ffffffff0100008801000000) does not provide a buffer interface.\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8545/comments",
    "author": "cyoon1729",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T06:33:20Z",
        "body": "Ray already handles inter-process communication as well as serialization using apache arrow. You can just do.\r\n\r\n```python3\r\nimport ray\r\nray.init()\r\n\r\n@ray.remote\r\nclass ReceiveServer:\r\n    def recv(self, msg):\r\n        print(msg)\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\nrecv_server_handle = ReceiveServer.remote()\r\nray.wait(send.remote(recv_server_handle))\r\n```\r\nThis should do the same thing."
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-22T07:35:22Z",
        "body": "@rkooo567 Thank you so much for your response and the example above. I would like to ask another question:\r\n \r\nSay, for instance, I have the `ReceiveServer` above to store the `msg` in an internal storage `self.storage (deque)` when `recv()` is called in `send()`, while continuously (as in a `while: True` loop) sampling data from `self.storage` and processing it in another member function `process()`.\r\n\r\nIf I were to run `process.remote()` asynchronously with respect to `send()`, would a mutual exclusion of `ReceiveSercer.storage` be enforced? Is this legal? \r\n\r\nThe code below implements what I tried to describe, but does not print anything:\r\n```python\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    def process(self):\r\n        while True:\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```\r\nIf it is indeed acceptable to use ray, pyarrow, and zmq together as in the first example, I would like to proceed with that. Are there any glaring issues with doing so? In particular, ray will be used purely as an alternative to python multiprocessing. \r\n\r\nThank you so much again for your time.\r\n"
      },
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T21:17:35Z",
        "body": "It doesn't print anything because Actor (class with @ray.remote) is running in a single process, and `recv` will never run because `process` is occupying the process (because it is running a while loop). \r\n\r\nmutual exclusion of ReceiveSercer.storage be enforced? Is this legal?: Yes. Ray handles this issue and you never need to worry about locking. \r\n\r\nThere's nothing wrong with using zmq and pyarrow if you have the right reason. It is just not efficient because what you try to achieve using zmq and pyarrow is what Ray exists for. Ray is a distributed computing framework that abstracts inter-process communication problems (and many others).    \r\n\r\nYou can make this work in this way.  \r\n```python3\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\nimport asyncio\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    async def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    async def process(self):\r\n        while True:\r\n            await asyncio.sleep(0.0)\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```"
      },
      {
        "user": "uchiiii",
        "created_at": "2023-08-07T15:14:51Z",
        "body": "I am very new to ray-project and have a question regarding this.\r\n\r\nRay supports inter-process communication as suggested above. What kind of protocol is used under the hood, `zmq` or anything else? Or it shares data using object storage like Plasma? \r\n\r\nThank you for you reply in advance! "
      }
    ]
  },
  {
    "number": 8413,
    "title": "[sgd] Can TorchTrainer print out something every one or several iterations?",
    "created_at": "2020-05-12T08:36:16Z",
    "closed_at": "2020-06-11T20:23:53Z",
    "labels": [
      "question",
      "sgd"
    ],
    "url": "https://github.com/ray-project/ray/issues/8413",
    "body": "Seems by default TorchTrainer only returns stats after train() finishes? During the training, is there a way I get some information (for example loss values, or just something to indicate the training is happening in the background?) for each iteration or every several iterations?\r\nOtherwise if one epoch training takes a lot of time, then I probably don't know what's going on. I may doubt whether the program crashes indeed or the training is just long.\r\n\r\n@richardliaw \r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8413/comments",
    "author": "hkvision",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-05-14T08:36:26Z",
        "body": "You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use."
      },
      {
        "user": "hkvision",
        "created_at": "2020-06-09T12:35:33Z",
        "body": "> You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use.\r\n\r\nThank you so much! @richardliaw Sorry for the late reply. `use_tqdm` works great!\r\nIf I specify `num_steps`, then every call for `train` only trains several batches, and would it be the case that some data won't get trained?"
      }
    ]
  },
  {
    "number": 7912,
    "title": "Details about the hyperparameter in PPO Algorithm?",
    "created_at": "2020-04-06T14:27:08Z",
    "closed_at": "2020-04-06T14:37:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7912",
    "body": "Hi, so I want to tune my hyperparameter for the PPO Algorithm but I've found difficulties when reading the docs about the configs, so I guess I want to ask you guys in here about:\r\n1. What is the value of `lr_schedule` in the PPO Algorithm? Suppose that my starting learning_rate is `'lr': 1e-4` and I want to decay its value to 0 when I train.\r\n2. Is it possible to set the hidden layer size in the PPO algorithm? If yes, what is the corresponding config as I didn't find it in the documentation (I found this kind of config in the SAC algorithm documentation but not in PPO).\r\n\r\nThank you very much guys! I really appreciate your help \ud83d\ude04 ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7912/comments",
    "author": "Nicholaz99",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-06T14:36:29Z",
        "body": "Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n1) You are basically configuring a PiecewiseSchedule.\r\nSo lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n2) You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\")."
      }
    ]
  },
  {
    "number": 7737,
    "title": "Why actor methods cannot be called directly?",
    "created_at": "2020-03-25T03:57:27Z",
    "closed_at": "2020-03-31T15:54:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7737",
    "body": "When calling a actor method, that is, call the `__call__` method of an `ActorMethod` object. And this method is implemented as raise an `Exception` directly\r\n```\r\nException: Actor methods cannot be called directly. Instead of running 'object.get()', try 'object.get.remote()'\r\n```\r\n\r\nBut why is it necessary? Why it can't be\r\n\r\n```python\r\nclass ActorMethod:\r\n    ...\r\n    def __call__(self, *args, **kwargs):\r\n        return ray.get(self._remote(args, kwargs))\r\n    ...\r\n```\r\n\r\nThen in some case, If do the following:\r\n```python\r\nclass Foo(object):\r\n    def foo(self):\r\n        return \"foo\"\r\n\r\nclass Bar(object):\r\n    def bar(self, foo_obj):\r\n        return foo_obj.foo()\r\n \r\nRayFoo = ray.remote(Foo)\r\nRayBar = ray.remote(Bar)\r\n\r\nif __name__ == \"__main__\":\r\n    f = Foo()\r\n    b = Bar()\r\n    print(b.bar(f))\r\n\r\n    ray.init(log_to_driver=False)\r\n    rf = RayFoo.remote()\r\n    rb = RayBar.remote()\r\n    print(rb.bar(rf))\r\n```\r\nwith the original `__call__` implementation, this is not possible, but with the proposed one, this works perfectly.\r\n\r\nIs there any design consideration?\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7737/comments",
    "author": "cloudhan",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-25T05:33:16Z",
        "body": "This is a design decision we made a couple years ago. The reason is to remain consistent across the API - tasks, methods, and class invocations.\r\n\r\nThe high level goal is to safeguard against user errors. I should note that commonly, new users often complain about the verbosity of this decision :) "
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T06:03:29Z",
        "body": "Are there any design pattern to walkaround the issue I mentioned, that is, what if I want to support both local and Ray decorated types. How to avoid implementing those types twice?"
      },
      {
        "user": "ericl",
        "created_at": "2020-03-25T07:59:50Z",
        "body": "You can do that with a wrapper class that automatically invokes .remote() under the hood, e.g., `h = Wrapper(handle)`."
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T11:16:55Z",
        "body": "Tried to hack a new decorator that replace the object constructor with a wrapper and then which replace the actor_method_obj.__call__ method with a new wrapper that return ray.get(actor_method_obj.<method_name>.remote()), too convoluted, will use @ericl 's wrapper.\r\n\r\nBTW, it is viable to add an option to allow this type of behavior, e.g.\r\n```python\r\n@ray.remote(allow_non_remote_calls=True)\r\nclass Foo(object): ...\r\n``` "
      }
    ]
  },
  {
    "number": 7467,
    "title": "[tune][rllib] _InactiveRpcError Deadline Exceeded",
    "created_at": "2020-03-05T16:32:58Z",
    "closed_at": "2020-04-22T17:22:40Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7467",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have VirtualBox running on Centos 7 and I am having trouble initializing Ray. After I run ray.init(), I get an _InactiveRpcError due to a Deadline Exceeded exception. What info should I provide in order to troubleshoot this error?\r\n\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray 0.8.2\r\nredis 3.4.1\r\nPython 3.6\r\nCentos 7 on VirtualBox",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7467/comments",
    "author": "Leonolovich",
    "comments": [
      {
        "user": "Leonolovich",
        "created_at": "2020-03-05T20:46:30Z",
        "body": "Running ray.init(local_mode=True) allows me to continue without errors to my tune.run() step, but I havent been able to resolve the Inactive Rcp Error. This is not ideal as I can only get one worker to perform training when using local_mode=True."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-04-22T02:36:49Z",
        "body": "Can you please try again on the latest Ray version?"
      },
      {
        "user": "Leonolovich",
        "created_at": "2020-04-22T17:22:39Z",
        "body": "That appears to have made the issue go away. For documentation, I was having the issue on version 0.8.2 or Ray and I no longer have the issue on version 0.8.4.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 7424,
    "title": "Actor method arguments",
    "created_at": "2020-03-03T19:27:19Z",
    "closed_at": "2020-03-03T21:06:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7424",
    "body": "Why do actor methods do not support passing arguments? There is an assertion that fails if the actor method function arguments are larger than 0.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7424/comments",
    "author": "commanderka",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-03T19:31:13Z",
        "body": "Can you please provide more context? i.e., a script and stack trace for reproducing this issue?"
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T20:20:46Z",
        "body": "I can only provide a code snippet, the problem is that there are too many dependencies. But I think its a more conceptual thing anyway.\r\n\r\n```\r\n@ray.remote(num_gpus=1)\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = bla\r\n        self.landmarkDetector = bla\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Improved)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    @ray.method\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/25/image_0409.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(currentActor,imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\ngives me \r\n\r\n> File \"testActors.py\", line 10, in <module>\r\n>     class PreprocessorActor(object):\r\n>   File \"testActors.py\", line 18, in PreprocessorActor\r\n>     @ray.method\r\n>   File \"/usr/local/lib/python3.6/dist-packages/ray/actor.py\", line 40, in method\r\n>     assert len(args) == 0\r\n> AssertionError\r\n\r\nThe idea is to have remote workers that are constantly fed with images to preprocess and to collect the preprocessed images. The problem is that the initialization of the preprocessing takes time, so I used the concept of Actors. Perhaps I have some conceptually wrong understanding, I dont know.\r\nRay version is 0.8.2"
      },
      {
        "user": "simon-mo",
        "created_at": "2020-03-03T20:26:05Z",
        "body": "`@ray.method` decorator is only there if you want to pass special parameters for a remote method, for example, [specifying the number of return values](@ray.method(num_return_vals=2)). By default, all methods for a `@ray.remote` actor can be called. \r\n\r\nYou can just remote the `@ray.method` decorator. "
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T21:06:27Z",
        "body": "Works like this now. I think the error was just misleading. I will close the issue. Nevertheless I would encourage to update the doku with some practical samples, perhaps also concerning the ActorPool class.\r\n\r\n```\r\n@ray.remote\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = something\r\n        self.landmarkDetector = something\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Relative)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\n\r\n\r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/20/image_0308.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 7394,
    "title": "Does DQN \"rollout.py\" have exploration turned off?",
    "created_at": "2020-03-02T03:57:53Z",
    "closed_at": "2020-03-02T10:34:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7394",
    "body": "When I call \"rollout.py\" I am not sure if exploration is turned off or not. I've looked over the file and can't seem to find `explore=False` anywhere.\r\n\r\nSo, when we evaluate trained policy (e.g. DQN) with rollout script - does it actually turn off random actions or not?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7394/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:01:53Z",
        "body": "The default config for DQN for evaluation is `exploration=False` (greedy action selection)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:03:53Z",
        "body": "However, in rollout.py, we do not use the evaluation_config, which is something, we should probably change."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T07:09:22Z",
        "body": "Then again, rollout.py picks up an already trained DQN, so its timesteps should already be past the epsilon exploration period, which then means it's (almost) not exploring anymore (if `final_epsilon` is 0.0, it won't explore at all). So for your specific DQN case, it should be fine (as in: not picking random actions anymore). What's your `exploration_config`?"
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T10:34:15Z",
        "body": "The above PR makes sure that rollout.py always uses the evaluation_config (which for DQN, is explore=False).\r\nIn the meantime, you can add `--config '{\"explore\": false}'` to your rollout.py command line to make sure, your algo picks only greedy acitons."
      }
    ]
  },
  {
    "number": 7194,
    "title": "[rllib]PPO with action branching?",
    "created_at": "2020-02-17T13:42:11Z",
    "closed_at": "2020-02-18T05:29:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7194",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### PPO with action branching?\r\nRay version: 0.8.0\r\nTensorflow Version: 1.14.0\r\nOS: Ubuntu 18.04\r\n\r\nI'm currently working on training action branching agents with PPO. What else do I need to do besides set the action space to something like `gym.spaces.Tuple([gym.spaces.Discrete(3), gym.spaces.Discrete(5)])`, or I need to write a custom loss function? I was wondering if the gradients would be correct. ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7194/comments",
    "author": "jinbo-huang",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-17T18:57:55Z",
        "body": "Yeah that's all you need for PPO. The action will be automatically computed for the space."
      }
    ]
  },
  {
    "number": 6986,
    "title": "[Question][rllib] Stochastic Game tensorboard separate rewards",
    "created_at": "2020-01-31T07:44:54Z",
    "closed_at": "2020-02-06T16:32:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6986",
    "body": "### What is your question?\r\n\r\nI am designing a simple stochastic game wherein I have two agents. The first agent (the good guy) is rewarded according to some task. The second agent (the adversary) is rewarded negative proportional to the first. This is to encourage the adversary to screw up the good guy.\r\n\r\nAs a first pass, I just set the reward of the adversary equal to negative the reward of the good guy. This seems to cause some issue with tensorboard, however, because it looks like the rewards are summed together, which results is a reward of 0 for each iteration.\r\n\r\nIt would be nice to be able to visualize the rewards of each agent individually. I imagine that this would be very useful for other MARL scenarios, not just SG. Is this something that is possible?\r\n\r\nThank you!\r\n\r\npython3.7\r\ntensorflow2.1\r\nray0.8.1\r\nmac10.14\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6986/comments",
    "author": "rusu24edward",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-01T01:43:53Z",
        "body": "Are you using separate policies for each agent? You can view the individual policy scores under the `policy_X_reward_mean` etc keys."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-03T21:58:22Z",
        "body": "I am using separate policies for each agent. I'm not sure what you mean by `policy_x_reward_mean` key. Is that something in the tensorboard interface?"
      },
      {
        "user": "ericl",
        "created_at": "2020-02-03T22:00:29Z",
        "body": "Yep, you should be able to find those in tensorboard, `result.json`, or printed to stdout if you use the `-v` flag."
      }
    ]
  },
  {
    "number": 3173,
    "title": "Issue running train.py can not locate lz4 or GPU",
    "created_at": "2018-10-31T19:32:10Z",
    "closed_at": "2018-11-01T06:06:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3173",
    "body": "When running the following (it does run succesfully but I get errors in setup on the gpu and lz4)\r\nRay does not find lz4 or my gpu\r\n.\r\n\r\n```\r\n===============================================================================================================================\r\n**********  stats  ************************\r\n\r\n\r\nUbuntu 16.04\r\n\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\n\r\n\r\n>>import ray; print(ray.__version__)\r\n0.5.3\r\n\r\n\r\n>>pip install lz4\r\nRequirement already satisfied: lz4 in /usr/lib/python2.7/dist-packages (0.7.0)\r\n\r\n\r\n>>lrwxrwxrwx  1 root root    10 Oct  1 22:50 cuda -> ./cuda-9.2\r\n\r\n>>nvidia-smi\r\n\r\nWed Oct 31 15:13:34 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1060    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   57C    P0    28W /  N/A |    365MiB /  6069MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1056      G   /usr/lib/xorg/Xorg                           262MiB |\r\n|    0      2508      G   compiz                                         7MiB |\r\n|    0      6924      G   ...uest-channel-token=14148272352303891226    92MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n=============================================================================================================\r\n\r\n\r\n\r\nsudo python /home/rjn/.local/lib/python2.7/site-packages/ray/rllib/train.py --env=Pong-ram-v4 --run=IMPALA \r\n\r\n\r\nProcess STDOUT and STDERR is being redirected to /tmp/raylogs/.\r\nWaiting for redis server at 127.0.0.1:27991 to respond...\r\nWaiting for redis server at 127.0.0.1:19620 to respond...\r\nStarting the Plasma object store with 6.00 GB memory.\r\nStarting local scheduler with the following resources: {'GPU': 1, 'CPU': 8}.\r\nFailed to start the UI, you may need to run 'pip install jupyter'.\r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\n\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nCreated LogSyncer for /home/rjn/ray_results/default/IMPALA_Pong-ram-v4_0_2018-10-31_15-27-354nWfrf -> \r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\nResources requested: 3/8 CPUs, 1/1 GPUs\r\nResult logdir: /home/rjn/ray_results/default\r\nRUNNING trials:\r\n - IMPALA_Pong-ram-v4_0:\tRUNNING\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:36.476907: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:36.551185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-31 15:27:36.551570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.50GiB\r\n2018-10-31 15:27:36.551584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-10-31 15:27:36.750449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-31 15:27:36.750495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-10-31 15:27:36.750501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-10-31 15:27:36.750697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5263 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:38.726462: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.728976: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.729023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729030: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.729080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.729086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\n2018-10-31 15:27:38.747833: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.750367: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.750429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750454: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.750584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.750590: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sampler.RolloutMetrics'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nResult for IMPALA_Pong-ram-v4_0:\r\n  date: 2018-10-31_15-27-47\r\n  done: false\r\n  episode_len_mean: 1290.9\r\n  episode_reward_max: -19.0\r\n  episode_reward_mean: -20.2\r\n  episode_reward_min: -21.0\r\n  episodes: 10\r\n  experiment_id: 16a7956cbdbf4bc78ef6f280ddc93142\r\n  hostname: rjn-Oryx-Pro\r\n  info:\r\n    learner:\r\n      cur_lr: 0.0005000000237487257\r\n      entropy: 862.072021484375\r\n      grad_gnorm: 40.0\r\n      policy_loss: -53.67061996459961\r\n      var_gnorm: 22.6600399017334\r\n      vf_explained_var: 0.07421219348907471\r\n      vf_loss: 30.673662185668945\r\n    num_steps_sampled: 14750\r\n    num_steps_trained: 14500\r\n    num_weight_syncs: 295\r\n    sample_throughput: 2048.296\r\n    train_throughput: 4096.592\r\n  iterations_since_restore: 1\r\n  node_ip: 192.168.1.100\r\n  pid: 7915\r\n  policy_reward_mean: {}\r\n  time_since_restore: 10.099857091903687\r\n  time_this_iter_s: 10.099857091903687\r\n  time_total_s: 10.099857091903687\r\n  timestamp: 1541014067\r\n  timesteps_since_restore: 14750\r\n  timesteps_this_iter: 14750\r\n  timesteps_total: 14750\r\n  training_iteration: 1\r\n\r\n\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3173/comments",
    "author": "rnunziata",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-11-01T00:09:15Z",
        "body": "> sudo python\r\n\r\nThis is almost certainly going to cause a different python environment to be used. Why not drop the sudo?"
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T01:03:36Z",
        "body": "removing the sudo did make a difference....same errors. Any thoughts on this. Maybe I will try to compile form source."
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T03:52:28Z",
        "body": "I can't think of a reason why lz4 can't be found if you can load it from a python interpreter.\r\n\r\nThough, not having that is probably fine on a single machine since no network transfers are happening."
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T04:38:08Z",
        "body": "ok...so you are saying I can at least ignore it.\r\nwhat about  it not findng cuda device.  Its a nivda GTX1060  and seem to be ok. Or am I not reading those errors correctly since it does run if I use Pong-ram-v0. "
      },
      {
        "user": "ericl",
        "created_at": "2018-11-01T05:05:06Z",
        "body": "Hm, the error might be expected on the workers since we force those to use CPUs. If you see GPU utilization then that should be fine."
      },
      {
        "user": "rnunziata",
        "created_at": "2018-11-01T05:50:44Z",
        "body": "I think that is it...there are two workers here and only two errors  I can not tell by the error line what task they belong to but I think it is probably good guess. I think the learner grabs the entire GPU memory since I  do not set any kind of growth or percentage variable. Thank you for your help."
      }
    ]
  }
]