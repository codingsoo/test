[
  {
    "number": 19252,
    "title": "lighting.pytorch IS NOT pytorch_lightning",
    "created_at": "2024-01-09T11:29:48Z",
    "closed_at": "2024-01-11T11:26:39Z",
    "labels": [
      "question",
      "ver: 2.0.x",
      "package"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19252",
    "body": "### Bug description\n\nDocumentation mentions that `pytorch_lightning` is now `lightning`, and that the old API can be found at `lightning_pytorch`.\r\n\r\nAnd yet, if you do:\r\n\r\n```python\r\nimport lightning.pytorch as pl\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass MyModel(LightningModule):\r\n    pass\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\r\n```\r\nthis still won't work, you will get:\r\n\r\n```\r\nTypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `LightningModule`\r\n```\r\n\r\nbecause the `Trainer` was imported from `lightning.pytorch`, while the model is a `LightningModule` from `pytorch_lightning`. Are they the same or not?\r\n\r\nWhy can't we \"mix\" them?\r\n\r\nThis is sometimes necessary when working with other packages where some rely on `lightning` or `lightning.pytorch`, and other rely on `pytorch_lightning`.\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n```python\nimport lightning.pytorch as pl\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass MyModel(LightningModule):\r\n    pass\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19252/comments",
    "author": "svnv-svsv-jm",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-01-09T12:54:51Z",
        "body": "Hi @svnv-svsv-jm \r\n\r\n> because the Trainer was imported from lightning.pytorch, while the model is a LightningModule from pytorch_lightning. Are they the same or not?\r\n> \r\n> Why can't we \"mix\" them?\r\n\r\nThey are two different packages, with the same source code and the only difference is the imports. But Python doesn't know that, so you can't mix them together in the same source code. If you import LightningModule from one package, but the Trainer from the other one, it won't work. They are not meant to be interchangeable. \r\n\r\n> This is sometimes necessary when working with other packages where some rely on lightning or lightning.pytorch, and other rely on pytorch_lightning.\r\n\r\nIt's not possible to mix them. You will have to rewrite the code so that all of it is only from one package.\r\n\r\n\r\n\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-10T10:41:16Z",
        "body": "We don't maintain two copies of the code. We maintain lightning (see the content in GitHub here) and generate the pytorch_lightning package automatically. \r\n\r\nThat there are two packages is a consequence of the decision to rename the package from pytorch-lightning to lightning. "
      }
    ]
  },
  {
    "number": 19119,
    "title": "StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'",
    "created_at": "2023-12-06T10:50:14Z",
    "closed_at": "2023-12-10T00:26:53Z",
    "labels": [
      "question",
      "waiting on author",
      "callback: finetuning",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19119",
    "body": "### Bug description\r\n\r\nFinetunning with a pytorch_lightning Trainer does not work now.\r\nThe call to `self.finetune_function()` should pass the `opt_idx` as the last parameter.\r\nLine 313 in `pytorch_lightning/callbacks/finetuning.py`.\r\nAfter that fix, finetuning works again.\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.1\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nRunning a baseline training run.\r\nI am using a `StagedFinetuning` finetuning object, not sure if that is related.\r\nUsing training with a simple data loader: num_workers=1, batch_size=1\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nMain error message:\r\n```StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'```\r\n\r\nDetailed trace:\r\n```\r\n  File \"/home/rob/projects/model-training/model_training/training/train.py\", line 83, in main\r\n    trainer.fit(model, dl_train, val_dataloaders=[dl_val])\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 989, in _run\r\n    results = self._run_stage()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1035, in _run_stage\r\n    self.fit_loop.run()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 201, in run\r\n    self.on_advance_start()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 341, in on_advance_start\r\n    call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 208, in _call_callback_hooks\r\n    fn(trainer, trainer.lightning_module, *args, **kwargs)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/finetuning.py\", line 313, in on_train_epoch_start\r\n    self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\nTypeError: StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'\r\n```\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\nOnly installed the \"pytorch-lightning\" pip package, not \"lightning\".\r\n\r\n```\r\n#- Lightning Component:  Trainer\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.1.2\r\n#- Lightning App Version (e.g., 0.5.2): NA\r\n#- PyTorch Version (e.g., 2.0): 2.1.1\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): Linux, Ubuntu 22.04, kernel 6.2.0-37-generic #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n#- CUDA/cuDNN version: 12.2\r\n#- GPU models and configuration: Nvidia GTX 1050 Ti 4 GiB\r\n#- How you installed Lightning(`conda`, `pip`, source): poetry (pip install)\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19119/comments",
    "author": "robwijnhoven",
    "comments": [
      {
        "user": "robwijnhoven",
        "created_at": "2023-12-06T11:02:57Z",
        "body": "Downgraded pytorch-lightning to 2.1.1 to be in sync with the pytorch version, but no effect there.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-06T15:05:55Z",
        "body": "@robwijnhoven You can remove `opt_idx` from your `finetune_function` definition. If this doesn't work, please share your entire `StagedFinetuning` implementation. Thanks"
      }
    ]
  },
  {
    "number": 18589,
    "title": "Can't seem to change distributed backend to gloo on Windows",
    "created_at": "2023-09-19T14:08:12Z",
    "closed_at": "2023-09-19T18:28:28Z",
    "labels": [
      "question",
      "strategy: ddp",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18589",
    "body": "### Bug description\r\n\r\nI am trying to run a training module with CUDA using PyTorch Lightning, but Lightning keeps trying to use NCCL. I have tried every solution I have found online, from specifying it in the code to prepending `PL_TORCH_DISTRIBUTED_BACKEND=gloo` to the laucnh command in the terminal, but Lightning still seems to try to use NCCL. I have verified that gloo is available for use in my system. Any help would be greatly appreciated.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nmaster\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nos.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\r\nmy_data = MyDataModule(args...)\r\nmy_model = MyModel(args...)\r\ntrainer = Trainer()\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\n# also pops up when running PL_TORCH_DISTRIBUTED_BACKEND=gloo python train.py\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n$ PL_TORCH_DISTRIBUTED_BACKEND=gloo python train.py\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\r\n  warnings.warn(\"No audio backend is available.\")\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarnin\r\ng: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the\r\nML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip\r\n install lightning[extra]` or one of them to enable TensorBoard support by default\r\n  warning_cache.warn(\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set.\r\nSetting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\r\n  rank_zero_warn(\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:69: UserWarning: You passed in a `v\r\nal_dataloader` but have no `validation_step`. Skipping val loop.\r\n  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\r\n  warnings.warn(\"No audio backend is available.\")\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntrain.py 62 <module>\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\ntrainer.py 532 fit\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntrain.py 62 <module>\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\ntrainer.py 532 fit\r\ncall._call_and_handle_interrupt(\r\n\r\ncall.py 42 _call_and_handle_interrupt\r\ncall._call_and_handle_interrupt(\r\n\r\ncall.py 42 _call_and_handle_interrupt\r\nreturn trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n\r\nsubprocess_script.py 93 launch\r\nreturn trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n\r\nreturn function(*args, **kwargs)\r\nsubprocess_script.py 93 launch\r\n\r\ntrainer.py 571 _fit_impl\r\nself._run(model, ckpt_path=ckpt_path)\r\n\r\ntrainer.py 938 _run\r\nself.strategy.setup_environment()\r\n\r\nddp.py 143 setup_environment\r\nreturn function(*args, **kwargs)\r\n\r\ntrainer.py 571 _fit_impl\r\nself._run(model, ckpt_path=ckpt_path)\r\n\r\ntrainer.py 938 _run\r\nself.strategy.setup_environment()\r\nself.setup_distributed()\r\n\r\n\r\nddp.py 143 setup_environment\r\nddp.py 191 setup_distributed\r\n_init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\r\n\r\ndistributed.py 258 _init_dist_connection\r\nself.setup_distributed()\r\n\r\ntorch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\nddp.py 191 setup_distributed\r\n\r\ndistributed_c10d.py 907 init_process_group\r\n_init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\r\n\r\ndistributed.py 258 _init_dist_connection\r\ntorch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\ndistributed_c10d.py 907 init_process_group\r\ndefault_pg = _new_process_group_helper(\r\n\r\ndistributed_c10d.py 1013 _new_process_group_helper\r\nraise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\r\n\r\nRuntimeError:\r\nDistributed package doesn't have NCCL built in\r\ndefault_pg = _new_process_group_helper(\r\n\r\ndistributed_c10d.py 1013 _new_process_group_helper\r\nraise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\r\n\r\nRuntimeError:\r\nDistributed package doesn't have NCCL built in\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA TITAN X (Pascal)\r\n                - NVIDIA GeForce GTX 970\r\n        - available:         True\r\n        - version:           11.8\r\n* Lightning:\r\n        - lightning:         2.0.8\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.0.8\r\n        - pytorchvideo:      0.1.5\r\n        - torch:             2.0.1\r\n        - torchaudio:        2.0.2\r\n        - torchmetrics:      1.1.1\r\n        - torchvision:       0.15.2\r\n* Packages:\r\n        - aiofiles:          22.1.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - aiosqlite:         0.18.0\r\n        - annotated-types:   0.5.0\r\n        - ansicon:           1.89.0\r\n        - anyio:             3.5.0\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arrow:             1.2.3\r\n        - asttokens:         2.0.5\r\n        - async-timeout:     4.0.3\r\n        - attrs:             22.1.0\r\n        - av:                10.0.0\r\n        - babel:             2.11.0\r\n        - backcall:          0.2.0\r\n        - backoff:           2.2.1\r\n        - beautifulsoup4:    4.12.2\r\n        - bleach:            4.1.0\r\n        - blessed:           1.20.0\r\n        - boto3:             1.28.42\r\n        - botocore:          1.31.42\r\n        - bottleneck:        1.3.5\r\n        - brotlipy:          0.7.0\r\n        - certifi:           2023.7.22\r\n        - cffi:              1.15.1\r\n        - charset-normalizer: 2.0.4\r\n        - click:             8.1.7\r\n        - colorama:          0.4.6\r\n        - comm:              0.1.2\r\n        - contourpy:         1.0.5\r\n        - croniter:          1.4.1\r\n        - cryptography:      41.0.2\r\n        - cycler:            0.11.0\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.6.7\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.4.1\r\n        - deeplake:          3.6.23\r\n        - defusedxml:        0.7.1\r\n        - dill:              0.3.7\r\n        - einops:            0.6.1\r\n        - entrypoints:       0.4\r\n        - executing:         0.8.3\r\n        - fastapi:           0.103.1\r\n        - fastjsonschema:    2.16.2\r\n        - filelock:          3.12.3\r\n        - fonttools:         4.25.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.9.0\r\n        - fvcore:            0.1.5.post20221221\r\n        - h11:               0.14.0\r\n        - huggingface-hub:   0.17.1\r\n        - humbug:            0.3.2\r\n        - idna:              3.4\r\n        - inquirer:          3.1.3\r\n        - iopath:            0.1.10\r\n        - ipykernel:         6.25.0\r\n        - ipython:           8.12.2\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        8.0.4\r\n        - itsdangerous:      2.1.2\r\n        - jedi:              0.18.1\r\n        - jinja2:            3.1.2\r\n        - jinxed:            1.2.0\r\n        - jmespath:          1.0.1\r\n        - joblib:            1.2.0\r\n        - json5:             0.9.6\r\n        - jsonschema:        4.17.3\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.4.9\r\n        - jupyter-console:   6.6.3\r\n        - jupyter-core:      5.3.0\r\n        - jupyter-events:    0.6.3\r\n        - jupyter-server:    1.23.4\r\n        - jupyter-server-fileid: 0.9.0\r\n        - jupyter-server-ydoc: 0.8.0\r\n        - jupyter-ydoc:      0.2.4\r\n        - jupyterlab:        3.6.3\r\n        - jupyterlab-pygments: 0.1.2\r\n        - jupyterlab-server: 2.22.0\r\n        - jupyterlab-widgets: 3.0.5\r\n        - kiwisolver:        1.4.4\r\n        - lightning:         2.0.8\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - llvmlite:          0.40.0\r\n        - lxml:              4.9.2\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.1\r\n        - matplotlib:        3.7.2\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - mistune:           0.8.4\r\n        - mkl-fft:           1.3.6\r\n        - mkl-random:        1.2.2\r\n        - mkl-service:       2.4.0\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - multiprocess:      0.70.15\r\n        - munkres:           1.1.4\r\n        - nbclassic:         0.5.5\r\n        - nbclient:          0.5.13\r\n        - nbconvert:         6.5.4\r\n        - nbformat:          5.7.0\r\n        - nest-asyncio:      1.5.6\r\n        - networkx:          3.1\r\n        - notebook:          6.5.4\r\n        - notebook-shim:     0.2.2\r\n        - numba:             0.57.0\r\n        - numcodecs:         0.11.0\r\n        - numexpr:           2.8.4\r\n        - numpy:             1.24.3\r\n        - ordered-set:       4.1.0\r\n        - packaging:         23.1\r\n        - pandas:            2.0.3\r\n        - pandocfilters:     1.5.0\r\n        - parameterized:     0.9.0\r\n        - parso:             0.8.3\r\n        - pathos:            0.3.1\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.4.0\r\n        - pip:               23.2.1\r\n        - platformdirs:      3.10.0\r\n        - ply:               3.11\r\n        - portalocker:       2.7.0\r\n        - pox:               0.3.3\r\n        - ppft:              1.7.6.7\r\n        - pretty-errors:     1.2.25\r\n        - prometheus-client: 0.14.1\r\n        - prompt-toolkit:    3.0.36\r\n        - psutil:            5.9.0\r\n        - pure-eval:         0.2.2\r\n        - pycparser:         2.21\r\n        - pydantic:          2.1.1\r\n        - pydantic-core:     2.4.0\r\n        - pygments:          2.15.1\r\n        - pyjwt:             2.8.0\r\n        - pyopenssl:         23.2.0\r\n        - pyparsing:         3.0.9\r\n        - pyqt5:             5.15.7\r\n        - pyqt5-sip:         12.11.0\r\n        - pyrsistent:        0.18.0\r\n        - pysocks:           1.7.1\r\n        - python-dateutil:   2.8.2\r\n        - python-editor:     1.0.4\r\n        - python-json-logger: 2.0.7\r\n        - python-multipart:  0.0.6\r\n        - pytorch-lightning: 2.0.8\r\n        - pytorchvideo:      0.1.5\r\n        - pytz:              2022.7\r\n        - pywin32:           305.1\r\n        - pywinpty:          2.0.10\r\n        - pyyaml:            6.0\r\n        - pyzmq:             23.2.0\r\n        - qtconsole:         5.4.2\r\n        - qtpy:              2.2.0\r\n        - readchar:          4.0.5\r\n        - regex:             2023.8.8\r\n        - requests:          2.31.0\r\n        - rfc3339-validator: 0.1.4\r\n        - rfc3986-validator: 0.1.1\r\n        - rich:              13.5.2\r\n        - s3transfer:        0.6.2\r\n        - safetensors:       0.3.3\r\n        - scikit-learn:      1.2.2\r\n        - scipy:             1.11.1\r\n        - send2trash:        1.8.0\r\n        - setuptools:        68.0.0\r\n        - sip:               6.6.2\r\n        - six:               1.16.0\r\n        - sniffio:           1.2.0\r\n        - soupsieve:         2.4\r\n        - stack-data:        0.2.0\r\n        - starlette:         0.27.0\r\n        - starsessions:      1.3.0\r\n        - sympy:             1.11.1\r\n        - tabulate:          0.9.0\r\n        - termcolor:         2.3.0\r\n        - terminado:         0.17.1\r\n        - threadpoolctl:     2.2.0\r\n        - tinycss2:          1.2.1\r\n        - tokenizers:        0.13.3\r\n        - toml:              0.10.2\r\n        - torch:             2.0.1\r\n        - torchaudio:        2.0.2\r\n        - torchmetrics:      1.1.1\r\n        - torchvision:       0.15.2\r\n        - tornado:           6.3.2\r\n        - tqdm:              4.65.0\r\n        - traitlets:         5.7.1\r\n        - transformers:      4.33.1\r\n        - typing-extensions: 4.7.1\r\n        - tzdata:            2023.3\r\n        - urllib3:           1.26.16\r\n        - uvicorn:           0.23.2\r\n        - wcwidth:           0.2.5\r\n        - webencodings:      0.5.1\r\n        - websocket-client:  0.58.0\r\n        - websockets:        11.0.3\r\n        - wheel:             0.38.4\r\n        - widgetsnbextension: 4.0.5\r\n        - win-inet-pton:     1.1.0\r\n        - y-py:              0.5.9\r\n        - yacs:              0.1.8\r\n        - yarl:              1.9.2\r\n        - ypy-websocket:     0.8.2\r\n* System:\r\n        - OS:                Windows\r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         Intel64 Family 6 Model 85 Stepping 4, GenuineIntel\r\n        - python:            3.11.4\r\n        - release:           10\r\n        - version:           10.0.19041\r\n\r\n</details>\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18589/comments",
    "author": "amansingh427",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-09-19T17:27:00Z",
        "body": "Hey @amansingh427 \r\nIn the latest Lightning versions, the backend can no longer be set through the environment variable `PL_TORCH_DISTRIBUTED_BACKEND`. You can set it like so:\r\n\r\n```py\r\nfrom lightning.pytorch.strategies import DDPStrategy\r\n\r\ntrainer = Trainer(strategy=DDPStrategy(process_group_backend=\"gloo\"), ...)\r\n```"
      }
    ]
  },
  {
    "number": 16970,
    "title": "Start training using CLI on Slurm cluster",
    "created_at": "2023-03-06T16:12:59Z",
    "closed_at": "2023-04-14T08:48:10Z",
    "labels": [
      "bug",
      "question",
      "won't fix",
      "waiting on author",
      "lightningcli"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16970",
    "body": "### Bug description\r\n\r\nHi,\r\nIm trying to run a simple pytorch lightning model training on mnist data using the pytorch CLI (with yaml config) as a slurm job.\r\n\r\n### How to reproduce the bug\r\nIm starting the slurm job using: `sbatch train_submit.sh`\r\ntrain_submit.sh:\r\n``` bash\r\n#!/bin/bash -l\r\n\r\n# SLURM SUBMIT SCRIPT\r\n#SBATCH --nodes=1             # This needs to match Trainer(num_nodes=...)\r\n#SBATCH --ntasks-per-node=1   # This needs to match Trainer(devices=...)\r\n#SBATCH --cpus-per-task=5\r\n#SBATCH --mem-per-cpu=5240\r\n#SBATCH --gpus=1\r\n#SBATCH --time=01:00:00\r\n#SBATCH --mail-type=BEGIN,END\r\n\r\n# activate conda env\r\n# source activate $1\r\n\r\n# debugging flags (optional)\r\n# export NCCL_DEBUG=INFO\r\n# export PYTHONFAULTHANDLER=1\r\n\r\n# on your cluster you might need these:\r\n# set the network interface\r\n# export NCCL_SOCKET_IFNAME=^docker0,lo\r\n\r\n# might need the latest CUDA\r\n# module load NCCL/2.4.7-1-cuda.10.0\r\n\r\n# run script from above\r\nsrun python3 cli_test.py fit --config config.yaml\r\n```\r\nconfig.yaml file:\r\n```\r\nseed_everything_default: null\r\ntrainer:\r\n  accelerator: gpu\r\n  limit_train_batches: 100\r\n  max_epochs: 500\r\n  devices: 1\r\n  logger: true\r\n  callbacks:\r\n    - class_path: pytorch_lightning.callbacks.ModelCheckpoint\r\n      init_args:\r\n        save_top_k: 1\r\n        monitor: 'val_loss'\r\n        mode: min\r\n        filename: 'vit-best'\r\n    - class_path: pytorch_lightning.callbacks.ModelCheckpoint\r\n      init_args:\r\n        save_last: true\r\n        filename: 'vit-last'\r\nckpt_path: null\r\nlog_dir: /cluster/dir/to/log\r\n```\r\n\r\ncli_test.py:\r\n```\r\n# main.py\r\nfrom pytorch_lightning.cli import LightningCLI\r\n\r\nimport os\r\nfrom torch import optim, nn, utils, Tensor\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision.transforms import ToTensor\r\nimport pytorch_lightning as pl\r\n\r\n# define any number of nn.Modules (or use your current ones)\r\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\r\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\r\n\r\n\r\n# define the LightningModule\r\nclass LitAutoEncoder(pl.LightningModule):\r\n    def __init__(self, encoder, decoder):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        # it is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = nn.functional.mse_loss(x_hat, x)\r\n        # Logging to TensorBoard (if installed) by default\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str = os.getcwd(), batch_size: int = 32):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: str):\r\n        self.mnist_test = MNIST(self.data_dir, train=False)\r\n        self.mnist_predict = MNIST(self.data_dir, train=False)\r\n        mnist_full = MNIST(self.data_dir, train=True)\r\n        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\r\n\r\n\r\ndef cli_main():\r\n    cli = LightningCLI(LitAutoEncoder, MNISTDataModule)\r\n    # note: don't call fit!!\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli_main()\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nslurm-9842342.out (File where std:output is printed)\r\n```\r\n2023-03-06 17:02:07.694344: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nusage: cli_test.py [-h] [-c CONFIG] [--print_config ^H[=flags]]\r\n                   {fit,validate,test,predict,tune} ...\r\ncli_test.py: error: 'Configuration check failed :: No action for destination key \"seed_everything_default\" to check its value.'\r\nsrun: error: eu-g2-16: task 0: Exited with exit code 2\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce GTX 1080 Ti\r\n        - available:         True\r\n        - version:           11.3\r\n* Lightning:\r\n        - lightning-utilities: 0.7.1\r\n        - pytorch-ignite:    0.4.10\r\n        - pytorch-lightning: 1.9.4\r\n        - pytorch3dunet:     1.3.3\r\n        - torch:             1.11.0+cu113\r\n        - torch-cluster:     1.6.0\r\n        - torch-fidelity:    0.3.0\r\n        - torch-geometric:   2.0.4\r\n        - torch-scatter:     2.0.9\r\n        - torch-sparse:      0.6.13\r\n        - torch-spline-conv: 1.2.1\r\n        - torchaudio:        0.11.0+cu113\r\n        - torchmetrics:      0.11.3\r\n        - torchvision:       0.12.0+cu113\r\n* Packages:\r\n        - absl-py:           1.0.0\r\n        - accesscontrol:     5.3.1\r\n        - acquisition:       4.10\r\n        - affine:            2.3.1\r\n        - aiohttp:           3.8.1\r\n        - aiohttp-cors:      0.7.0\r\n        - aioredis:          2.0.1\r\n        - aiosignal:         1.2.0\r\n        - alabaster:         0.7.12\r\n        - alembic:           1.8.1\r\n        - amply:             0.1.5\r\n        - aniso8601:         9.0.1\r\n        - anndata:           0.8.0\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.6.1\r\n        - app-model:         0.1.1\r\n        - appdirs:           1.4.4\r\n        - apptools:          5.1.0\r\n        - argcomplete:       2.0.0\r\n        - argh:              0.26.2\r\n        - argon2:            0.1.10\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arviz:             0.12.1\r\n        - ase:               3.22.1\r\n        - asn1crypto:        1.5.1\r\n        - astor:             0.8.1\r\n        - asttokens:         2.0.5\r\n        - astunparse:        1.6.3\r\n        - async-generator:   1.10\r\n        - async-timeout:     4.0.2\r\n        - atomicwrites:      1.4.0\r\n        - attrs:             21.4.0\r\n        - audioread:         2.1.9\r\n        - authencoding:      4.3\r\n        - autopage:          0.5.1\r\n        - autopep8:          1.6.0\r\n        - aws-requests-auth: 0.4.3\r\n        - babel:             2.10.1\r\n        - backcall:          0.2.0\r\n        - beautifulsoup4:    4.11.1\r\n        - bidict:            0.22.0\r\n        - bids-validator:    1.9.3\r\n        - biopython:         1.79\r\n        - bitstring:         3.1.9\r\n        - black:             22.3.0\r\n        - bleach:            5.0.0\r\n        - blessings:         1.7\r\n        - blurhash:          1.1.4\r\n        - bokeh:             2.4.3\r\n        - boost:             0.1\r\n        - boto3:             1.23.10\r\n        - botocore:          1.26.10\r\n        - bottleneck:        1.3.4\r\n        - btrees:            4.10.0\r\n        - build:             0.10.0\r\n        - cachetools:        5.2.0\r\n        - cachey:            0.2.1\r\n        - cellmodeller:      b-v4.3-42-g96ab099-\r\n        - certifi:           2022.5.18.1\r\n        - certipy:           0.1.3\r\n        - cffi:              1.15.0\r\n        - cftime:            1.6.0\r\n        - chainer:           7.8.1\r\n        - chameleon:         3.10.1\r\n        - chardet:           4.0.0\r\n        - charset-normalizer: 2.0.12\r\n        - chex:              0.1.3\r\n        - clang:             14.0\r\n        - click:             8.1.3\r\n        - click-plugins:     1.1.1\r\n        - cligj:             0.7.2\r\n        - clikit:            0.6.2\r\n        - cloudpickle:       2.1.0\r\n        - cmaes:             0.9.1\r\n        - cmake:             3.24.1.1\r\n        - cmd2:              2.4.1\r\n        - codecov:           2.1.12\r\n        - colorama:          0.4.4\r\n        - coloredlogs:       15.0.1\r\n        - colorful:          0.5.4\r\n        - colorlog:          6.6.0\r\n        - colorlover:        0.3.0\r\n        - colormath:         3.0.0\r\n        - commonmark:        0.9.1\r\n        - configargparse:    1.5.3\r\n        - configobj:         5.0.6\r\n        - configparser:      5.2.0\r\n        - connection-pool:   0.0.3\r\n        - contextlib2:       21.6.0\r\n        - coverage:          6.4\r\n        - crashtest:         0.3.1\r\n        - cryptography:      38.0.4\r\n        - cucim:             23.2.0\r\n        - cufflinks:         0.17.3\r\n        - cupy-cuda11x:      11.1.0\r\n        - cutadapt:          4.0\r\n        - cutensor:          1.6.0.3\r\n        - cvxopt:            1.3.0\r\n        - cvxpy:             1.2.1\r\n        - cycler:            0.11.0\r\n        - cython:            0.29.30\r\n        - dask:              2022.5.2\r\n        - databricks-cli:    0.17.4\r\n        - datasets:          2.5.1\r\n        - datetime:          4.4\r\n        - datrie:            0.8.2\r\n        - deap:              1.3.1\r\n        - debtcollector:     2.5.0\r\n        - debugpy:           1.6.0\r\n        - decorator:         5.1.1\r\n        - deepdiff:          5.8.1\r\n        - defusedxml:        0.7.1\r\n        - deprecated:        1.2.13\r\n        - deprecation:       2.1.0\r\n        - descartes:         1.1.0\r\n        - dill:              0.3.5.1\r\n        - distributed:       2022.5.2\r\n        - distro:            1.8.0\r\n        - dm-tree:           0.1.7\r\n        - dnaio:             0.9.0\r\n        - dnspython:         2.2.1\r\n        - docker:            6.0.1\r\n        - docker-pycreds:    0.4.0\r\n        - docopt:            0.6.2\r\n        - docstring-parser:  0.15\r\n        - documenttemplate:  4.0\r\n        - docutils:          0.17.1\r\n        - dpath:             2.0.6\r\n        - easydict:          1.9\r\n        - ecos:              2.0.10\r\n        - einops:            0.4.1\r\n        - entrypoints:       0.4\r\n        - envisage:          6.0.1\r\n        - ephem:             4.1.3\r\n        - esda:              2.4.1\r\n        - et-xmlfile:        1.1.0\r\n        - etils:             0.8.0\r\n        - eventlet:          0.33.1\r\n        - evo:               1.18.1\r\n        - executing:         0.8.3\r\n        - extensionclass:    4.6\r\n        - extras:            1.0.0\r\n        - fasteners:         0.17.3\r\n        - fastjsonschema:    2.15.3\r\n        - fastprogress:      1.0.2\r\n        - fastrlock:         0.8\r\n        - filelock:          3.7.0\r\n        - findlibs:          0.0.2\r\n        - fiona:             1.8.22\r\n        - fire:              0.5.0\r\n        - flask:             2.1.2\r\n        - flask-cors:        3.0.10\r\n        - flask-json:        0.3.4\r\n        - flask-restplus:    0.13.0\r\n        - flask-restx:       0.5.1\r\n        - flatbuffers:       1.12\r\n        - flit:              3.7.1\r\n        - flit-core:         3.7.1\r\n        - flowvision:        0.2.0\r\n        - follicle-tracker:  0.1.dev221+gc3cd246\r\n        - fonttools:         4.33.3\r\n        - freetype-py:       2.3.0\r\n        - frozenlist:        1.3.0\r\n        - fsspec:            2022.5.0\r\n        - funcsigs:          1.0.2\r\n        - future:            0.18.2\r\n        - futurist:          2.4.1\r\n        - gast:              0.4.0\r\n        - gdown:             4.4.0\r\n        - geopandas:         0.12.2\r\n        - gevent:            21.12.0\r\n        - giddy:             2.3.3\r\n        - gitdb:             4.0.9\r\n        - gitdb2:            4.0.2\r\n        - gitpython:         3.1.27\r\n        - gmpy2:             2.1.5\r\n        - google-api-core:   2.8.1\r\n        - google-auth:       2.6.6\r\n        - google-auth-oauthlib: 0.4.6\r\n        - google-pasta:      0.2.0\r\n        - googleapis-common-protos: 1.56.2\r\n        - googledrivedownloader: 0.4\r\n        - gpaw:              22.8.0\r\n        - gprmax:            3.1.4\r\n        - gpustat:           0.6.0\r\n        - grabbit:           0.2.6\r\n        - graphtools:        1.5.2\r\n        - greenlet:          1.1.2\r\n        - grpcio:            1.46.3\r\n        - gunicorn:          20.1.0\r\n        - h3:                3.7.4\r\n        - h5py:              3.7.0\r\n        - haversine:         2.5.1\r\n        - hdbscan:           0.8.29\r\n        - heapdict:          1.0.1\r\n        - hiredis:           2.0.0\r\n        - hsluv:             5.0.3\r\n        - html5lib:          1.1\r\n        - httpstan:          4.8.2\r\n        - huggingface-hub:   0.7.0\r\n        - humanfriendly:     10.0\r\n        - hydra-core:        1.2.0\r\n        - hyperopt:          0.2.7\r\n        - idna:              3.3\r\n        - ifcfg:             0.22\r\n        - imagecodecs:       2023.1.23\r\n        - imageio:           2.19.3\r\n        - imageio-ffmpeg:    0.4.7\r\n        - imagesize:         1.3.0\r\n        - importlib-metadata: 4.11.4\r\n        - importlib-resources: 5.7.1\r\n        - in-n-out:          0.1.7\r\n        - inequality:        1.0.0\r\n        - iniconfig:         1.1.1\r\n        - install:           1.3.5\r\n        - iopath:            0.1.6\r\n        - ipdb:              0.13.9\r\n        - ipykernel:         6.13.0\r\n        - ipython:           8.4.0\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        7.7.0\r\n        - isal:              0.11.1\r\n        - iso3166:           2.0.2\r\n        - iso8601:           1.0.2\r\n        - isodate:           0.6.1\r\n        - iteration-utilities: 0.11.0\r\n        - itk:               5.3.0\r\n        - itk-core:          5.3.0\r\n        - itk-filtering:     5.3.0\r\n        - itk-io:            5.3.0\r\n        - itk-numerics:      5.3.0\r\n        - itk-registration:  5.3.0\r\n        - itk-segmentation:  5.3.0\r\n        - itsdangerous:      2.1.2\r\n        - jax:               0.3.23\r\n        - jaxlib:            0.3.22+cuda11.cudnn82\r\n        - jedi:              0.18.1\r\n        - jeepney:           0.8.0\r\n        - jieba:             0.42.1\r\n        - jinja2:            3.1.2\r\n        - jmespath:          1.0.0\r\n        - joblib:            1.1.0\r\n        - json-tricks:       3.16.1\r\n        - json5:             0.9.8\r\n        - jsonargparse:      4.20.0\r\n        - jsonlines:         1.2.0\r\n        - jsonpickle:        2.2.0\r\n        - jsonpointer:       2.3\r\n        - jsonschema:        4.5.1\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.3.1\r\n        - jupyter-console:   6.4.3\r\n        - jupyter-contrib-core: 0.3.3\r\n        - jupyter-core:      4.10.0\r\n        - jupyter-highlight-selected-word: 0.2.0\r\n        - jupyter-server:    1.17.0\r\n        - jupyter-telemetry: 0.1.0\r\n        - jupyterlab:        3.4.2\r\n        - jupyterlab-pygments: 0.2.2\r\n        - jupyterlab-server: 2.14.0\r\n        - jupyterlab-widgets: 1.1.0\r\n        - keras:             2.9.0\r\n        - keras-preprocessing: 1.1.2\r\n        - keyring:           23.5.1\r\n        - kiwisolver:        1.4.2\r\n        - lazy-object-proxy: 1.7.1\r\n        - libclang:          14.0.1\r\n        - libpysal:          4.6.2\r\n        - lightning-utilities: 0.7.1\r\n        - llvmlite:          0.38.1\r\n        - lmdb:              1.4.0\r\n        - locket:            1.0.0\r\n        - logutils:          0.3.5\r\n        - loompy:            3.0.7\r\n        - lxml:              4.8.0\r\n        - lz4:               4.0.1\r\n        - lzstring:          1.0.4\r\n        - mageck:            0.5.9.4\r\n        - magicgui:          0.7.0\r\n        - mako:              1.2.0\r\n        - mapclassify:       2.4.3\r\n        - markdown:          3.3.7\r\n        - markupsafe:        2.1.1\r\n        - marshmallow:       3.18.0\r\n        - mastodon.py:       1.8.0\r\n        - matplotlib:        3.5.2\r\n        - matplotlib-inline: 0.1.3\r\n        - mccabe:            0.7.0\r\n        - mercantile:        1.2.1\r\n        - mgwr:              2.1.2\r\n        - mistune:           0.8.4\r\n        - mlflow:            2.2.1\r\n        - mock:              4.0.3\r\n        - monai:             1.1.0\r\n        - more-itertools:    8.13.0\r\n        - mpi4py:            3.1.4\r\n        - mpmath:            1.2.1\r\n        - msgpack:           1.0.3\r\n        - multidict:         6.0.2\r\n        - multimapping:      4.1\r\n        - multipart:         0.2.4\r\n        - multiprocess:      0.70.13\r\n        - multiqc:           1.13\r\n        - munch:             2.5.0\r\n        - mypy-extensions:   0.4.3\r\n        - napari:            0.4.17\r\n        - napari-console:    0.0.7\r\n        - napari-plugin-engine: 0.2.0\r\n        - napari-svg:        0.1.6\r\n        - natsort:           8.1.0\r\n        - nbclassic:         0.3.7\r\n        - nbclient:          0.6.3\r\n        - nbconvert:         6.5.0\r\n        - nbformat:          5.4.0\r\n        - nbsphinx:          0.8.8\r\n        - nest-asyncio:      1.5.5\r\n        - netaddr:           0.8.0\r\n        - netcdf4:           1.5.8\r\n        - netifaces:         0.11.0\r\n        - networkx:          2.8.2\r\n        - nibabel:           3.2.2\r\n        - ninja:             1.11.1\r\n        - nipy:              0.5.0\r\n        - nltk:              3.7\r\n        - nni:               2.10\r\n        - nose:              1.3.7\r\n        - nose-timer:        1.0.1\r\n        - notebook:          6.4.11\r\n        - notebook-shim:     0.1.0\r\n        - npe2:              0.6.2\r\n        - nptyping:          2.5.0\r\n        - num2words:         0.5.10\r\n        - numba:             0.55.2\r\n        - numexpr:           2.8.1\r\n        - numpy:             1.22.4\r\n        - numpy-groupies:    0.9.16\r\n        - numpy-quaternion:  2022.4.2\r\n        - numpydoc:          1.5.0\r\n        - nvidia-ml-py3:     7.352.0\r\n        - oauthlib:          3.2.0\r\n        - omegaconf:         2.2.2\r\n        - opencensus:        0.9.0\r\n        - opencensus-context: 0.1.2\r\n        - opencv-contrib-python: 4.5.5.64\r\n        - opencv-python:     4.5.5.64\r\n        - openpyxl:          3.0.10\r\n        - openseespy:        3.3.0.1.1\r\n        - openseespylinux:   3.4.0.1\r\n        - openslide-python:  1.1.2\r\n        - opt-einsum:        3.3.0\r\n        - optax:             0.1.2\r\n        - optuna:            3.1.0\r\n        - ordered-set:       4.1.0\r\n        - os-service-types:  1.7.0\r\n        - oslo.i18n:         5.1.0\r\n        - osmnx:             1.2.2\r\n        - osqp:              0.6.2.post5\r\n        - ovary-analysis:    0.0.3\r\n        - overpy:            0.6\r\n        - packaging:         21.3\r\n        - pamela:            1.0.0\r\n        - pandas:            1.4.2\r\n        - pandas-datareader: 0.10.0\r\n        - pandoc:            2.2\r\n        - pandocfilters:     1.5.0\r\n        - parso:             0.8.3\r\n        - partd:             1.2.0\r\n        - paste:             3.5.0\r\n        - pastedeploy:       2.1.1\r\n        - pastel:            0.2.1\r\n        - pathos:            0.2.9\r\n        - pathspec:          0.9.0\r\n        - pathtools:         0.1.2\r\n        - patsy:             0.5.2\r\n        - pbr:               5.9.0\r\n        - persistence:       3.3\r\n        - persistent:        4.9.0\r\n        - pert:              2019.11\r\n        - pexpect:           4.8.0\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.1.1\r\n        - pint:              0.19.2\r\n        - pip:               22.2.2\r\n        - pkginfo:           1.8.2\r\n        - plac:              1.3.5\r\n        - platformdirs:      2.5.2\r\n        - plotly:            5.8.0\r\n        - pluggy:            1.0.0\r\n        - plumbum:           1.7.2\r\n        - ply:               3.11\r\n        - pointpats:         2.2.0\r\n        - pooch:             1.6.0\r\n        - portalocker:       2.4.0\r\n        - pox:               0.3.1\r\n        - ppft:              1.7.6.5\r\n        - prettytable:       3.3.0\r\n        - prometheus-client: 0.14.1\r\n        - promise:           2.3\r\n        - prompt-toolkit:    3.0.29\r\n        - protobuf:          3.19.4\r\n        - psutil:            5.9.1\r\n        - psygnal:           0.8.1\r\n        - ptyprocess:        0.7.0\r\n        - pulp:              2.6.0\r\n        - pure-eval:         0.2.2\r\n        - py:                1.11.0\r\n        - py-spy:            0.3.12\r\n        - py4design:         0.28\r\n        - py4j:              0.10.9.5\r\n        - pyarrow:           9.0.0\r\n        - pyasn1:            0.4.8\r\n        - pyasn1-modules:    0.2.8\r\n        - pybind11:          2.9.2\r\n        - pybis:             1.35.2\r\n        - pybufrkit:         0.2.19\r\n        - pycocotools:       2.0.4\r\n        - pycodestyle:       2.8.0\r\n        - pycollada:         0.7.2\r\n        - pycparser:         2.21\r\n        - pydantic:          1.10.5\r\n        - pydicom:           2.3.1\r\n        - pydot:             1.4.2\r\n        - pyepsg:            0.4.0\r\n        - pyface:            7.4.1\r\n        - pyfaidx:           0.6.4\r\n        - pyflakes:          2.5.0\r\n        - pyglet:            1.5.26\r\n        - pygments:          2.12.0\r\n        - pygsp:             0.5.1\r\n        - pygsti:            0.9.10.1\r\n        - pyinotify:         0.9.6\r\n        - pyjwt:             2.6.0\r\n        - pylev:             1.4.0\r\n        - pymeshfix:         0.16.2\r\n        - pymf:              0.1.9\r\n        - pymongo:           4.1.1\r\n        - pynrrd:            1.0.0\r\n        - pyomo:             6.4.1\r\n        - pyopencl:          2022.1.5\r\n        - pyopengl:          3.1.6\r\n        - pyopenssl:         22.1.0\r\n        - pyparsing:         3.0.9\r\n        - pyperclip:         1.8.2\r\n        - pyproj:            3.4.1\r\n        - pyproject-hooks:   1.0.0\r\n        - pypsa:             0.19.3\r\n        - pyqt5:             5.15.6\r\n        - pyqt5-qt5:         5.15.2\r\n        - pyqt5-sip:         12.10.1\r\n        - pyro4:             4.82\r\n        - pyrsistent:        0.18.1\r\n        - pysam:             0.19.1\r\n        - pyshp:             2.3.0\r\n        - pysimdjson:        3.2.0\r\n        - pysocks:           1.7.1\r\n        - pystan:            3.5.0\r\n        - pytest:            7.1.2\r\n        - python-dateutil:   2.8.2\r\n        - python-engineio:   4.3.2\r\n        - python-gettext:    4.0\r\n        - python-json-logger: 2.0.4\r\n        - python-louvain:    0.16\r\n        - python-magic:      0.4.27\r\n        - python-socketio:   5.6.0\r\n        - pythonwebhdfs:     0.2.3\r\n        - pytoml:            0.1.21\r\n        - pytomlpp:          1.0.11\r\n        - pytools:           2022.1.9\r\n        - pytorch-ignite:    0.4.10\r\n        - pytorch-lightning: 1.9.4\r\n        - pytorch3dunet:     1.3.3\r\n        - pytz:              2022.1\r\n        - pyutilib:          6.0.0\r\n        - pyutillib:         0.3.0\r\n        - pyvista:           0.38.3\r\n        - pywavelets:        1.3.0\r\n        - pyxlsb:            1.0.9\r\n        - pyyaml:            6.0\r\n        - pyzmq:             23.0.0\r\n        - qdldl:             0.1.5.post2\r\n        - qtconsole:         5.3.0\r\n        - qtpy:              2.1.0\r\n        - quantecon:         0.5.3\r\n        - querystring-parser: 1.2.4\r\n        - quilt3:            5.0.0\r\n        - rasterio:          1.3.6\r\n        - rasterstats:       0.18.0\r\n        - ratelimiter:       1.2.0.post0\r\n        - rdflib:            6.1.1\r\n        - readme-renderer:   35.0\r\n        - recommonmark:      0.7.1\r\n        - redis:             4.3.1\r\n        - rednose:           1.3.0\r\n        - regex:             2022.4.24\r\n        - reportlab:         3.6.9\r\n        - repoze.lru:        0.7\r\n        - requests:          2.28.2\r\n        - requests-futures:  1.0.0\r\n        - requests-oauthlib: 1.3.1\r\n        - requests-toolbelt: 0.9.1\r\n        - requests-unixsocket: 0.3.0\r\n        - requestsexceptions: 1.4.0\r\n        - resampy:           0.2.2\r\n        - responses:         0.18.0\r\n        - restrictedpython:  5.3a1.dev0\r\n        - retry:             0.9.2\r\n        - retrying:          1.3.3\r\n        - rfc3986:           2.0.0\r\n        - rich:              12.4.4\r\n        - rich-click:        1.5.2\r\n        - roman:             3.3\r\n        - rosbags:           0.9.11\r\n        - routes:            2.5.1\r\n        - rsa:               4.8\r\n        - rtree:             1.0.0\r\n        - ruamel.yaml:       0.17.21\r\n        - ruamel.yaml.clib:  0.2.6\r\n        - rvlib:             0.0.6\r\n        - s3transfer:        0.5.2\r\n        - salib:             1.4.5\r\n        - schema:            0.7.5\r\n        - scikit-build:      0.16.7\r\n        - scikit-fmm:        2022.3.26\r\n        - scikit-image:      0.19.2\r\n        - scikit-learn:      1.1.1\r\n        - scipy:             1.8.1\r\n        - scons:             4.4.0\r\n        - scooby:            0.7.1\r\n        - scs:               3.2.0\r\n        - seaborn:           0.11.2\r\n        - secretstorage:     3.3.2\r\n        - semver:            2.13.0\r\n        - send2trash:        1.8.0\r\n        - sentence-transformers: 2.2.0\r\n        - sentencepiece:     0.1.96\r\n        - sentry-sdk:        1.5.12\r\n        - serpent:           1.40\r\n        - setproctitle:      1.2.3\r\n        - setuptools:        58.1.0\r\n        - setuptools-scm:    6.4.2\r\n        - shap:              0.41.0\r\n        - shapely:           1.8.5.post1\r\n        - shortuuid:         1.0.9\r\n        - simplegeneric:     0.8.1\r\n        - simplejson:        3.17.6\r\n        - six:               1.16.0\r\n        - slicer:            0.0.7\r\n        - smart-open:        6.0.0\r\n        - smmap:             5.0.0\r\n        - smmap2:            3.0.1\r\n        - snakemake:         7.8.0\r\n        - sniffio:           1.2.0\r\n        - snowballstemmer:   2.2.0\r\n        - snuggs:            1.4.7\r\n        - sortedcontainers:  2.4.0\r\n        - soupsieve:         2.3.2.post1\r\n        - spaghetti:         1.6.5\r\n        - spectra:           0.0.11\r\n        - spglm:             1.0.8\r\n        - sphinx:            4.5.0\r\n        - sphinxcontrib-applehelp: 1.0.2\r\n        - sphinxcontrib-devhelp: 1.0.2\r\n        - sphinxcontrib-htmlhelp: 2.0.0\r\n        - sphinxcontrib-jsmath: 1.0.1\r\n        - sphinxcontrib-qthelp: 1.0.3\r\n        - sphinxcontrib-serializinghtml: 1.1.5\r\n        - sphinxcontrib-websupport: 1.2.4\r\n        - spint:             1.0.7\r\n        - spreg:             1.2.4\r\n        - spvcm:             0.3.0\r\n        - sqlalchemy:        1.4.37\r\n        - sqlparse:          0.4.2\r\n        - stack-data:        0.2.0\r\n        - staticmap:         0.5.5\r\n        - statsd:            3.3.0\r\n        - statsmodels:       0.13.2\r\n        - stevedore:         3.5.0\r\n        - stopit:            1.1.2\r\n        - subprocess32:      3.5.4\r\n        - superqt:           0.4.1\r\n        - svg.path:          6.0\r\n        - sympy:             1.10.1\r\n        - tables:            3.7.0\r\n        - tabulate:          0.8.9\r\n        - tasklogger:        1.1.2\r\n        - tblib:             1.7.0\r\n        - tempita:           0.5.2\r\n        - tenacity:          8.0.1\r\n        - tensorboard:       2.9.0\r\n        - tensorboard-data-server: 0.6.1\r\n        - tensorboard-plugin-wit: 1.8.1\r\n        - tensorboardx:      2.5\r\n        - tensorflow-estimator: 2.9.0\r\n        - tensorflow-gpu:    2.9.1\r\n        - tensorflow-io-gcs-filesystem: 0.26.0\r\n        - termcolor:         1.1.0\r\n        - terminado:         0.15.0\r\n        - terminaltables:    3.1.10\r\n        - termstyle:         0.1.11\r\n        - testpath:          0.6.0\r\n        - testresources:     2.0.1\r\n        - texttable:         1.6.4\r\n        - theano:            1.0.5\r\n        - theano-pymc:       1.1.2\r\n        - threadpoolctl:     3.1.0\r\n        - tifffile:          2022.5.4\r\n        - timezonefinder:    6.0.0\r\n        - tinycss2:          1.1.1\r\n        - tokenizers:        0.12.1\r\n        - toml:              0.10.2\r\n        - tomli:             2.0.1\r\n        - tomli-w:           1.0.0\r\n        - tomlkit:           0.11.0\r\n        - toolz:             0.11.2\r\n        - toposort:          1.7\r\n        - torch:             1.11.0+cu113\r\n        - torch-cluster:     1.6.0\r\n        - torch-fidelity:    0.3.0\r\n        - torch-geometric:   2.0.4\r\n        - torch-scatter:     2.0.9\r\n        - torch-sparse:      0.6.13\r\n        - torch-spline-conv: 1.2.1\r\n        - torchaudio:        0.11.0+cu113\r\n        - torchmetrics:      0.11.3\r\n        - torchvision:       0.12.0+cu113\r\n        - tornado:           6.1\r\n        - tqdm:              4.64.0\r\n        - traitlets:         5.2.1.post0\r\n        - traits:            6.3.2\r\n        - traitsui:          7.3.1\r\n        - transaction:       3.0.1\r\n        - transformers:      4.19.2\r\n        - trimesh:           3.12.5\r\n        - twine:             4.0.1\r\n        - typeguard:         2.13.3\r\n        - typer:             0.7.0\r\n        - typeshed-client:   2.2.0\r\n        - typing-extensions: 4.2.0\r\n        - urllib3:           1.26.9\r\n        - utm:               0.7.0\r\n        - velocyto:          0.17.17\r\n        - vine:              5.0.0\r\n        - vispy:             0.11.0\r\n        - vtk:               9.2.6\r\n        - waitress:          2.1.1\r\n        - wandb:             0.12.17\r\n        - wcwidth:           0.2.5\r\n        - webargs:           8.2.0\r\n        - webencodings:      0.5.1\r\n        - webob:             1.8.7\r\n        - websocket:         0.2.1\r\n        - websocket-client:  1.3.2\r\n        - websockets:        10.4\r\n        - webtest:           3.0.0\r\n        - werkzeug:          2.1.2\r\n        - wget:              3.2\r\n        - wheel:             0.37.1\r\n        - widgetsnbextension: 3.6.0\r\n        - wntr:              0.4.1\r\n        - wrapt:             1.14.1\r\n        - wsgiproxy2:        0.5.1\r\n        - wsme:              0.11.0\r\n        - xarray:            2022.3.0\r\n        - xarray-einstats:   0.2.2\r\n        - xlrd:              2.0.1\r\n        - xlsxwriter:        3.0.3\r\n        - xlwt:              1.3.0\r\n        - xmlrunner:         1.7.7\r\n        - xopen:             1.5.0\r\n        - xxhash:            3.0.0\r\n        - xyzservices:       2022.4.0\r\n        - yacs:              0.1.8\r\n        - yappi:             1.3.5\r\n        - yarl:              1.7.2\r\n        - yaspin:            2.1.0\r\n        - yte:               1.4.0\r\n        - z3c.pt:            3.3.1\r\n        - zc.lockfile:       2.0\r\n        - zconfig:           3.6.0\r\n        - zexceptions:       4.2\r\n        - zict:              2.2.0\r\n        - zipp:              3.8.0\r\n        - zodb:              5.7.0\r\n        - zodbpickle:        2.3\r\n        - zope:              5.5.1\r\n        - zope.annotation:   4.7.0\r\n        - zope.browser:      2.4\r\n        - zope.browsermenu:  4.4\r\n        - zope.browserpage:  4.4.0\r\n        - zope.browserresource: 4.4\r\n        - zope.cachedescriptors: 4.3.1\r\n        - zope.component:    5.0.1\r\n        - zope.configuration: 4.4.1\r\n        - zope.container:    4.5.0\r\n        - zope.contentprovider: 4.2.1\r\n        - zope.contenttype:  4.5.0\r\n        - zope.datetime:     4.3.0\r\n        - zope.deferredimport: 4.4\r\n        - zope.deprecation:  4.4.0\r\n        - zope.dottedname:   4.3\r\n        - zope.event:        4.5.0\r\n        - zope.exceptions:   4.5\r\n        - zope.filerepresentation: 5.0.0\r\n        - zope.globalrequest: 1.5\r\n        - zope.hookable:     5.1.0\r\n        - zope.i18n:         4.9.0\r\n        - zope.i18nmessageid: 5.0.1\r\n        - zope.interface:    5.4.0\r\n        - zope.lifecycleevent: 4.4\r\n        - zope.location:     4.2\r\n        - zope.pagetemplate: 4.6.0\r\n        - zope.processlifetime: 2.3.0\r\n        - zope.proxy:        4.5.0\r\n        - zope.ptresource:   4.3.0\r\n        - zope.publisher:    6.1.0\r\n        - zope.schema:       6.2.0\r\n        - zope.security:     5.3\r\n        - zope.sequencesort: 4.2\r\n        - zope.site:         4.5.0\r\n        - zope.size:         4.3\r\n        - zope.structuredtext: 4.4\r\n        - zope.tal:          4.5\r\n        - zope.tales:        5.1\r\n        - zope.testbrowser:  5.6.1\r\n        - zope.testing:      4.10\r\n        - zope.traversing:   4.4.1\r\n        - zope.viewlet:      4.3\r\n        - zstandard:         0.17.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         \r\n        - python:            3.10.4\r\n        - version:           #1 SMP Tue Nov 8 15:48:59 UTC 2022\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @carmocca @mauvilsa",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16970/comments",
    "author": "leopold-franz",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-03-06T20:33:15Z",
        "body": "Hey, I think the problem is that these keys in the config.yaml are not allowed:\r\n\r\n```\r\nseed_everything_default: null\r\nlog_dir: /cluster/dir/to/log\r\n```\r\n\r\nThey don't match anything in the Trainer. \r\n\r\nPerhaps it should be \r\n```\r\nseed_everything: false\r\ntrainer:\r\n    default_root_dir:  \"/cluster/dir/to/log\"\r\n    ...\r\n```"
      }
    ]
  },
  {
    "number": 16654,
    "title": "dependencies issue working with torch and pytorch_lightning. ",
    "created_at": "2023-02-06T17:16:02Z",
    "closed_at": "2023-02-06T21:09:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16654",
    "body": "### Bug description\n\nI have trouble getting all needed packages to be compatible to run ResNet transfer learning. I trying the import from one of the sample lightening notebooks anf got error. And then trying to install the recommended packages got following incompatibility issue.       \r\ncommand: \r\n`! pip install --quiet \"torchmetrics>=0.7, <0.12\" \"seaborn\" \"ipython[notebook]>=8.0.0, <8.9.0\" \"pytorch-lightning>=1.4, <1.9\" \"torchmetrics >=0.11.0\" \"setuptools==65.6.3\" \"pandas\" \"torchvision\" \"torch>=1.8.1, <1.14.0\"\r\n`\r\nIssue: \r\n```\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nthinc 8.0.1 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.10.4 which is incompatible.\r\nspacy 3.0.1 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.10.4 which is incompatible.\r\nlightning 1.9.0.dev0 requires lightning-utilities<1.0,>=0.4.2, but you have lightning-utilities 0.3.0 which is incompatible.\r\nconda-repo-cli 1.0.27 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\nconda-repo-cli 1.0.27 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\r\n```\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16654/comments",
    "author": "nkay28",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-02-06T20:40:16Z",
        "body": "@nkay28 You have torchmetrics specified twice, once with torchmetrics>=0.7, <0.12 and once with torchmetrics >=0.11.0. Also, I suggest to remove setuptools==65.6.3 which could limit the pip dep resolver. "
      }
    ]
  },
  {
    "number": 7775,
    "title": "training_epoch_end called before all steps of epoch were completed. always at about 0.25 size of steps.",
    "created_at": "2021-05-31T07:10:17Z",
    "closed_at": "2021-06-01T09:40:40Z",
    "labels": [
      "help wanted",
      "question",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7775",
    "body": "## \ud83d\udc1b Bug\r\n\r\n```bash\r\nGPU available: False, used: False\r\nTPU available: None, using: 0 TPU cores\r\nValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\r\n  | Name                | Type                 | Params\r\n-------------------------------------------------------------\r\n\r\n-------------------------------------------------------------\r\n\r\nEpoch 0:   0%|          | 0/13 [00:00<?, ?it/s] \r\nEpoch 0:  23%|\u2588\u2588\u258e       | 3/13 [01:38<05:27, 32.75s/it, loss=4.73, v_num=7]\r\n// training_epoch_end:  outputs = [{'loss': tensor(6.4593)}, {'loss': tensor(5.7653)}, {'loss': tensor(1.9642)}]\r\n\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/10 [00:00<?, ?it/s]\r\nEpoch 0:  38%|\u2588\u2588\u2588\u258a      | 5/13 [01:48<02:54, 21.78s/it, loss=4.73, v_num=7]\r\nEpoch 0:  46%|\u2588\u2588\u2588\u2588\u258c     | 6/13 [01:59<02:19, 19.91s/it, loss=4.73, v_num=7]\r\nEpoch 0:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 7/13 [02:10<01:51, 18.58s/it, loss=4.73, v_num=7]\r\nEpoch 0:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 8/13 [02:20<01:27, 17.60s/it, loss=4.73, v_num=7]\r\nEpoch 0:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 9/13 [02:31<01:07, 16.83s/it, loss=4.73, v_num=7]\r\nEpoch 0:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 10/13 [02:42<00:48, 16.21s/it, loss=4.73, v_num=7]\r\nEpoch 0:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 11/13 [02:52<00:31, 15.71s/it, loss=4.73, v_num=7]\r\nEpoch 0:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [03:04<00:15, 15.34s/it, loss=4.73, v_num=7]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:15<00:00, 15.00s/it, loss=4.73, v_num=7]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:16<00:00, 15.15s/it, loss=4.73, v_num=7]\r\nEpoch 1:  23%|\u2588\u2588\u258e       | 3/13 [01:42<05:42, 34.24s/it, loss=3.39, v_num=7]\r\n// training_epoch_end:  outputs = [{'loss': tensor(2.6766)}, {'loss': tensor(2.3010)}, {'loss': tensor(1.1722)}]\r\nEpoch 1:  31%|\u2588\u2588\u2588       | 4/13 [01:48<04:04, 27.22s/it, loss=3.39, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 1:  38%|\u2588\u2588\u2588\u258a      | 5/13 [02:02<03:15, 24.42s/it, loss=3.39, v_num=7]\r\nCompleted 6.8 MiB/327.9 MiB (48.7 KiB/s) with 2 file(s) remaining\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nEpoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:48<00:00, 17.54s/it, loss=3.39, v_num=7]\r\nEpoch 2:  23%|\u2588\u2588\u258e       | 3/13 [01:44<05:47, 34.72s/it, loss=2.72, v_num=7]\r\nNUM EL TRAINING: 3   [{'loss': tensor(1.2504)}, {'loss': tensor(1.4905)}, {'loss': tensor(1.4158)}]\r\nEpoch 2:  31%|\u2588\u2588\u2588       | 4/13 [01:49<04:07, 27.48s/it, loss=2.72, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:50<00:00, 17.75s/it, loss=2.72, v_num=7]\r\nEpoch 3:  23%|\u2588\u2588\u258e       | 3/13 [01:43<05:46, 34.62s/it, loss=2.27, v_num=7]\r\n//training_epoch_end:   outputs = [{'loss': tensor(0.6632)}, {'loss': tensor(0.9215)}, {'loss': tensor(1.1396)}]\r\nEpoch 3:  31%|\u2588\u2588\u2588       | 4/13 [01:49<04:06, 27.41s/it, loss=2.27, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\n```\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): mac Catalina (this happens on all environments , linux etc)\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration: happens also with 0 gpus.\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7775/comments",
    "author": "ganitps",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T11:47:30Z",
        "body": "As of now, `training_epoch_end` runs before validation starts (validation runs outside the training epoch). The progress bar just shows the combined training + validation steps. So this is fine and you get all the steps for the training epoch.\r\n\r\nAfter #7357, `training_epoch_end` will run after the last validation loop each epoch. \r\nHope this clears it up :) "
      },
      {
        "user": "ganitps",
        "created_at": "2021-05-31T12:44:23Z",
        "body": "Thanks @awaelchli  for your reply\r\nThis is not what I understood form the documentation:\r\n\r\n// the pseudocode for these calls\r\n```\r\ntrain_outs = []\r\nfor train_batch in train_data:\r\n        out = training_step(train_batch)\r\n        train_outs.append(out)\r\ntraining_epoch_end(train_outs)\r\n\r\n```\r\n\r\nSo If I want to take actions when epoch ends I must do it in the last training step? (training_step_end())\r\nCan I take the latest version After #7357?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T13:53:00Z",
        "body": "No this is still all good. The pseudo code is correct, all outputs from training_step get passed into training_epoch_end. You get as many outputs as training steps. In your case, there seems to be three training steps, so you get 3 outputs, correct? And that's when the training epoch ends and the validation starts. "
      },
      {
        "user": "ganitps",
        "created_at": "2021-05-31T14:29:49Z",
        "body": "No... I have 13 steps.\r\ntraining_epoch_end called after the third one... with 3 outputs..."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T14:38:36Z",
        "body": "> No... I have 13 steps.\r\n\r\nHow do you know?\r\nHave you printed `len(dataloader)`? Have you set `limit_train_batches=13`?\r\nFrom the output you shared it just looks like you have 3 training steps and 10 validation steps. \r\n\r\nMaybe I can help better if you try to explain what you want to achieve. "
      }
    ]
  },
  {
    "number": 4940,
    "title": "typeError unexpected closure",
    "created_at": "2020-12-01T21:24:10Z",
    "closed_at": "2020-12-02T11:21:45Z",
    "labels": [
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4940",
    "body": "## \u2753 Questions and Help\r\n\r\nI am using pytorch 1.6 and pytorch lightning 1.0.8.\r\n\r\nBefore I upgrade my pytorch-lighing from 1.0.2 to 1.0.8, everything works fine, today I upgrade to 1.0.8 to try some new metrics, but got this error. \r\n\r\n    main()\r\n  File \"main.py\", line 68, in main\r\n    trainer.fit(model, trainloader, evalloader)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 444, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 63, in train\r\n    results = self.train_or_test()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 74, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 493, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 561, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 728, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 470, in optimizer_step\r\n    optimizer, batch_idx, opt_idx, train_step_and_backward_closure\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 130, in optimizer_step\r\n    using_lbfgs=is_lbfgs\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1270, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\nTypeError: step() got an unexpected keyword argument 'closure'\r\n(py3) yikuan@deepmedicine:~/project/version_control/HiBEHRT-BYOL$ TypeError: step() got an unexpected keyword argument 'closure'",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4940/comments",
    "author": "yikuanli",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T08:48:19Z",
        "body": "Which optimizer and scheduler are you using? If you are using a custom optimizer, you need to update the code of the ```step()``` function to take in a closure. PyTorch optimizer class implements the step method with the closure parameter in the latest version."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T11:19:44Z",
        "body": "Oh, if you were using the LARS wrapper form bolts, then the latest version has the step method taking in a closure. That should solve it.\r\n\r\nShould I close this issue then?"
      }
    ]
  },
  {
    "number": 2451,
    "title": "how to use custom dataset in pytorch-lightning module as I am encountering an error \"AttributeError: 'str' object has no attribute 'size'\"",
    "created_at": "2020-07-01T13:42:42Z",
    "closed_at": "2020-07-30T21:51:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2451",
    "body": "## \u2753 Questions and Help\r\n\r\n\r\n\r\n#### how to use custom dataset in pytorch-lightning module as I am encountering an error \"AttributeError: 'str' object has no attribute 'size'\"?\r\n\r\n#### Code\r\n\r\n\r\nclass CustomDataset(Dataset):\r\n       def read_data_set(self):\r\n       all_img_files = []\r\n       all_labels = []\r\n\r\n    class_names = os.walk(self.data_set_path).__next__()[1]\r\n    \r\n    for index, class_name in enumerate(class_names):\r\n        label = index\r\n        img_dir = os.path.join(self.data_set_path, class_name)\r\n        img_files = os.walk(img_dir).__next__()[2]\r\n        \r\n        \r\n        for img_file in img_files:\r\n            img_file = os.path.join(img_dir, img_file)\r\n            img = Image.open(img_file)\r\n            if img is not None:\r\n                all_img_files.append(img_file)\r\n                all_labels.append(label)\r\n                \r\n    return all_img_files, all_labels, len(all_img_files), len(class_names)\r\n\r\ndef __init__(self, data_set_path, transforms=None):\r\n    self.data_set_path = data_set_path\r\n    self.image_files_path, self.labels, self.length, self.num_classes = self.read_data_set()\r\n    self.transforms = transforms\r\n    \r\ndef __getitem__(self, index):\r\n    image = Image.open(self.image_files_path[index])\r\n    image = image.convert(\"RGB\")\r\n    \r\n    if self.transforms is not None:\r\n        image = self.transforms(image)\r\n        \r\n    return {'image': image, 'label': self.labels[index]}\r\n\r\ndef __len__(self):\r\n    return self.length\r\nclass MNISTClassifier(LightningModule):\r\ndef init(self):\r\nsuper(MNISTClassifier, self).init()\r\nself.layer_1 = torch.nn.Linear(28*28, 128)\r\nself.layer_2 = torch.nn.Linear(128, 256)\r\nself.layer_3 = torch.nn.Linear(256, 10)\r\n\r\ndef forward(self, x):\r\n    batch_size, channels, width, height = x.size()\r\n    \r\n    #(b_s, 1, 28, 28)\r\n    x = x.view(batch_size, -1)\r\n    \r\n    #layer1\r\n    x = self.layer_1(x)\r\n    x = torch.relu(x)\r\n    \r\n    #layer2\r\n    x = self.layer_2(x)\r\n    x = torch.relu(x)\r\n    \r\n    #layer3\r\n    x = self.layer_3()\r\n    \r\n    #probability distribution over labels\r\n    x = torch.log_softmax(x, dim = 1)\r\n    \r\n    return x\r\n\r\n\r\ndef cross_entropy_loss(self, logits, labels):\r\n    return F.null_loss(logits, labels)\r\n\r\n\r\ndef training_step(self, train_batch, batch_idx):\r\n    x, y = train_batch\r\n    logits = self.forward(x.size())\r\n    loss = self.cross_entropy_loss(logits, y)\r\n    \r\n    logs = {'train_loss': loss}\r\n    return {'loss': loss, 'log':logs}\r\n\r\n\r\n\r\n\r\ndef test_step(self, test_batch, batch_idx):\r\n    x, y = test_batch\r\n    logits = self.forward(x.size())\r\n    loss = self.cross_entropy_loss(logits, y)\r\n    \r\n    logs = {'test_loss:': loss}\r\n    return {'val_loss': loss, 'log':logs}\r\n\r\n \r\ndef train_dataloader(self):\r\n    dataset = CustomDataset(data_set_path='./files/MNIST/mnist_png/mnist_png/training/', transforms=transforms.ToTensor())\r\n    train_loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n    return train_loader\r\n\r\n\r\n\r\ndef test_dataloader(self):\r\n    dataset = CustomDataset(data_set_path='files/MNIST/mnist_png/mnist_png/testing/', transforms=transforms.ToTensor())\r\n    test_loader = DataLoader(dataset, batch_size=32, num_workers=4)\r\n    return test_loader\r\n                       \r\ndef configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(),lr=1e-3)\r\n    return optimizer\r\nmodel = MNISTClassifier()\r\ntrainer = pl.Trainer(gpus=1, max_epochs=1)\r\ntrainer.fit(model)\r\n\r\n\r\n\r\n#### What have you tried?\r\nI am getting an error as follows:\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n8 model = MNISTClassifier()\r\n9 trainer = pl.Trainer(gpus=1, max_epochs=1)\r\n---> 10 trainer.fit(model)\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n763\r\n764 elif self.single_gpu:\r\n--> 765 self.single_gpu_train(model)\r\n766\r\n767 elif self.use_tpu: # pragma: no-cover\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n490 self.optimizers = optimizers\r\n491\r\n--> 492 self.run_pretrain_routine(model)\r\n493\r\n494 def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n911\r\n912 # CORE TRAINING LOOP\r\n--> 913 self.train()\r\n914\r\n915 def test(self, model: Optional[LightningModule] = None, test_dataloaders: Optional[DataLoader] = None):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n345 # RUN TNG EPOCH\r\n346 # -----------------\r\n--> 347 self.run_training_epoch()\r\n348\r\n349 # update LR schedulers\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n417 # RUN TRAIN STEP\r\n418 # ---------------\r\n--> 419 _outputs = self.run_training_batch(batch, batch_idx)\r\n420 batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n421\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n594\r\n595 # calculate loss\r\n--> 596 loss, batch_output = optimizer_closure()\r\n597\r\n598 # check if loss or model weights are nan\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n558 opt_idx, self.hiddens)\r\n559 else:\r\n--> 560 output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n561\r\n562 # format and reduce outputs accordingly\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\r\n724 batch = self.transfer_batch_to_gpu(batch, gpu_id)\r\n725 args[0] = batch\r\n--> 726 output = self.model.training_step(*args)\r\n727\r\n728 # TPU support\r\n\r\nin training_step(self, train_batch, batch_idx)\r\n36 def training_step(self, train_batch, batch_idx):\r\n37 x, y = train_batch\r\n---> 38 logits = self.forward(x.size())\r\n39 loss = self.cross_entropy_loss(logits, y)\r\n40\r\n\r\nAttributeError: 'str' object has no attribute 'size'   \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 18.04\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2451/comments",
    "author": "drone-vision",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T17:09:05Z",
        "body": "@tanubapun You are sending `x.size()` in `self.forward(x.size())`. It should be `self.forward(x)`"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T18:21:36Z",
        "body": "Thanks @rohitgr7 for your response. I have changed the code to  self.forward(x) still got the same error.\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-bb768fd558f0> in <module>()\r\n      8 model = MNISTClassifier()\r\n      9 trainer = pl.Trainer(gpus=1, max_epochs=1)\r\n---> 10 trainer.fit(model)\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n    763 \r\n    764         elif self.single_gpu:\r\n--> 765             self.single_gpu_train(model)\r\n    766 \r\n    767         elif self.use_tpu:  # pragma: no-cover\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n    490             self.optimizers = optimizers\r\n    491 \r\n--> 492         self.run_pretrain_routine(model)\r\n    493 \r\n    494     def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n    911 \r\n    912         # CORE TRAINING LOOP\r\n--> 913         self.train()\r\n    914 \r\n    915     def test(self, model: Optional[LightningModule] = None, test_dataloaders: Optional[DataLoader] = None):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n    345                 # RUN TNG EPOCH\r\n    346                 # -----------------\r\n--> 347                 self.run_training_epoch()\r\n    348 \r\n    349                 # update LR schedulers\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    417             # RUN TRAIN STEP\r\n    418             # ---------------\r\n--> 419             _outputs = self.run_training_batch(batch, batch_idx)\r\n    420             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n    421 \r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n    594 \r\n    595                 # calculate loss\r\n--> 596                 loss, batch_output = optimizer_closure()\r\n    597 \r\n    598                 # check if loss or model weights are nan\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n    558                                                                     opt_idx, self.hiddens)\r\n    559                         else:\r\n--> 560                             output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n    561 \r\n    562                         # format and reduce outputs accordingly\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\r\n    724             batch = self.transfer_batch_to_gpu(batch, gpu_id)\r\n    725             args[0] = batch\r\n--> 726             output = self.model.training_step(*args)\r\n    727 \r\n    728         # TPU support\r\n\r\n<ipython-input-7-375f2fcd6b89> in training_step(self, train_batch, batch_idx)\r\n     36     def training_step(self, train_batch, batch_idx):\r\n     37         x, y = train_batch\r\n---> 38         logits = self.forward(x)\r\n     39         loss = self.cross_entropy_loss(logits, y)\r\n     40 \r\n\r\n<ipython-input-7-375f2fcd6b89> in forward(self, x)\r\n      8 \r\n      9     def forward(self, x):\r\n---> 10         batch_size, channels, width, height = x.size()\r\n     11 \r\n     12         #(b_s, 1, 28, 28)\r\n\r\nAttributeError: 'str' object has no attribute 'size'"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T18:28:19Z",
        "body": "@tanubapun Can you share the whole code in a notebook or colab or just LightningModule you created again after you made changes?"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T18:50:10Z",
        "body": "Sure @rohitgr7 \r\nthis is the lightning module I am using\r\n\r\nclass MNISTClassifier(LightningModule):\r\n    def __init__(self):\r\n        super(MNISTClassifier, self).__init__()\r\n        self.layer_1 = torch.nn.Linear(28*28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n        \r\n        \r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n        \r\n        #(b_s, 1, 28, 28)\r\n        x = x.view(batch_size, -1)\r\n        \r\n        #layer1\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n        \r\n        #layer2\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n        \r\n        #layer3\r\n        x = self.layer_3()\r\n        \r\n        #probability distribution over labels\r\n        x = torch.log_softmax(x, dim = 1)\r\n        \r\n        return x\r\n    \r\n    \r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.null_loss(logits, labels)\r\n    \r\n    \r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        \r\n        logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log':logs}\r\n    \r\n    \r\n    \r\n    def test_step(self, test_batch, batch_idx):\r\n        x, y = test_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        \r\n        logs = {'test_loss:': loss}\r\n        return {'val_loss': loss, 'log':logs}\r\n    \r\n\r\n    def train_dataloader(self):\r\n        dataset = CustomDataset(data_set_path='./files/MNIST/mnist_png/mnist_png/training/', transforms=transforms.ToTensor())\r\n        train_loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n        return train_loader\r\n\r\n    \r\n    def test_dataloader(self):\r\n        dataset = CustomDataset(data_set_path='files/MNIST/mnist_png/mnist_png/testing/', transforms=transforms.ToTensor())\r\n        test_loader = DataLoader(dataset, batch_size=32, num_workers=4)\r\n        return test_loader\r\n                           \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(),lr=1e-3)\r\n        return optimizer\r\n        "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T19:04:48Z",
        "body": "@tanubapun In `__getitem__` of your `CustomDataset` you are returning `{'image': image, 'label': self.labels[index]}` but you are using `x, y = train_batch`. Your `train_batch` is still a `dict` here just that pytorch `collate_fn` create a batch in the values of this dict. Either return `image, self.labels[index]` or use `x, y= train_batch['image'], train_batch['label']`. Also change in `validation_step` and `test_step` accordingly.\r\n"
      }
    ]
  },
  {
    "number": 1002,
    "title": "Validation step isn't being ran ",
    "created_at": "2020-03-02T04:22:38Z",
    "closed_at": "2020-03-02T23:38:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1002",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI have been trying to get the trainer to call the validation_step function but it doesn't seem to ever get called.  I assume I am missing something obvious but have looking at the tutorials and docs I haven't been able to find the obvious.  The code for the model and trainer are below.  What might I be missing?  Thank you for the help!\r\n\r\n#### Code\r\n```\r\nclass SegModel(pl.LightningModule):\r\n    def __init__(self, batch_size, lr):\r\n        super(SegModel, self).__init__()\r\n        self.batch_size = batch_size\r\n        self.learning_rate = lr\r\n        self.net = UNet(num_classes=1)\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor()\r\n        ])\r\n        self.trainset = Stacker(input_images, truth_images, transform=self.transform)\r\n        self.validset = Stacker(input_images, truth_images, transform=self.transform)\r\n        self.testset = Stacker(input_images, truth_images, transform=self.transform)\r\n    \r\n    def forward(self, x):\r\n        return self.net(x)\r\n    \r\n    def training_step(self, batch, batch_nb):\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'loss': loss_val, 'log': {'train_loss': loss_val}}\r\n    \r\n    def validation_step(self, batch, batch_nb):\r\n        print(\"RUNNING VALIDATION\")\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'val_loss': loss_val, \r\n                'val_dice': dice(out, mask),\r\n                'val_iou': IoU(out, mask)\r\n               }\r\n    \r\n    def test_step(self, batch, batch_nb):\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'test_loss': loss_val, \r\n                'test_dice': dice(out, mask),\r\n                'test_iou': IoU(out, mask)\r\n               }\r\n    \r\n    def validation_end(self, outputs):\r\n        if len(outputs)==0: return {}\r\n        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        val_dice_mean = torch.stack([x['val_dice'] for x in outputs]).mean()\r\n        val_iou_mean = torch.stack([x['val_iou'] for x in outputs]).mean()\r\n        return {'val_loss': val_loss_mean,\r\n                'log': {\r\n                    'val_loss': val_loss_mean,\r\n                    'val_dice': val_dice_mean,\r\n                    'val_iou': val_iou_mean\r\n                }}\r\n\r\n    def test_end(self, outputs):\r\n        if len(outputs)==0: return {}\r\n        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        test_dice_mean = torch.stack([x['test_dice'] for x in outputs]).mean()\r\n        test_iou_mean = torch.stack([x['test_iou'] for x in outputs]).mean()\r\n        print(test_dice_mean, test_iou_mean)\r\n        return {'test_loss': test_loss_mean,\r\n                'log': {\r\n                    'test_loss': test_loss_mean,\r\n                    'test_dice': test_dice_mean,\r\n                    'test_iou': test_iou_mean\r\n                }}\r\n    \r\n    def configure_optimizers(self):\r\n        opt = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\r\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\r\n        return [opt], [sch]\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True)\r\n\r\n    @pl.data_loader\r\n    def valid_dataloader(self):\r\n        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False)\r\n      \r\n    @pl.data_loader\r\n    def test_dataloader(self):\r\n        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=False)\r\n\r\nmodel = SegModel(1, 0.001)\r\n\r\ntrainer = pl.Trainer(\r\n    gpus=[0], \r\n    early_stop_callback=None, \r\n    max_epochs=40,\r\n    check_val_every_n_epoch=1,\r\n)\r\n\r\ntrainer.fit(model)\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Windows\r\n - Packaging: conda\r\n - Version: 0.6.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1002/comments",
    "author": "RyMo95",
    "comments": [
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-02T22:04:51Z",
        "body": "Hi @RyMo95 , I believe the issue is that your method is not named correctly. \r\n\r\n`valid_dataloader` -> `val_dataloader` should resolve this, please let me know if that doesn\u2019t work!"
      }
    ]
  },
  {
    "number": 7726,
    "title": "on_load_checkpoint never called",
    "created_at": "2021-05-26T14:09:10Z",
    "closed_at": "2021-05-26T14:55:00Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7726",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI am a new user of PL, so this may be an error of API comprehension on my side.\r\n\r\nI fail to get anything done on the loading of the checkpoint when I resume:\r\n\r\n## Please reproduce using the BoringModel\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nimport torch\r\n\r\n\r\nclass Solver(pl.LightningModule):\r\n    def __init__(self,):\r\n        super(Solver, self).__init__()\r\n        dx = 10\r\n        dy = 1\r\n        n = 100\r\n        self.model = torch.nn.Linear(dx, dy)\r\n        self.dataset = list(zip(torch.rand(n, dx), torch.rand(n, dy)))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def configure_optimizers(self,):\r\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\r\n\r\n    def _step(self, batch):\r\n        x, y = batch\r\n        y_hat = self.model(x)\r\n        return torch.nn.functional.mse_loss(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self._step(batch)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._step(batch)\r\n\r\n\r\nclass Checkpoint(ModelCheckpoint):\r\n    def on_load_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"loading...\")\r\n        import pdb # <----------------- Never called?\r\n\r\n        pdb.set_trace()\r\n        foo = checkpoint['bar']\r\n\r\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"saving...\")\r\n        checkpoint[\"foo\"] = \"bar\"\r\n\r\n\r\nsolver = Solver()\r\ncheckpoint = Checkpoint(dirpath=\"./\", save_last=True)\r\ntrainer = pl.Trainer(callbacks=[checkpoint], max_epochs=3)\r\ntrainer.fit(solver)\r\n\r\ntrainer = pl.Trainer(\r\n    callbacks=[checkpoint], resume_from_checkpoint=\"last.ckpt\", max_epochs=5\r\n)\r\ntrainer.fit(solver)\r\n\r\n```\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - Quadro GP100\r\n                - Quadro GP100\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.8.1+cu102\r\n        - pytorch-lightning: 1.3.2\r\n        - tqdm:              4.50.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7726/comments",
    "author": "kingjr",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-05-26T14:50:24Z",
        "body": "Dear @kingjr,\r\n\r\nThis is working.\r\n\r\nExplanation: `on_load_checkpoint` is called only if `on_save_checkpoint` returned something which isn't None. \r\n\r\n```\r\nfrom typing import Callable\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback\r\nimport torch\r\n\r\n\r\nclass Solver(pl.LightningModule):\r\n    def __init__(self,):\r\n        super(Solver, self).__init__()\r\n        dx = 10\r\n        dy = 1\r\n        n = 100\r\n        self.model = torch.nn.Linear(dx, dy)\r\n        self.dataset = list(zip(torch.rand(n, dx), torch.rand(n, dy)))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def configure_optimizers(self,):\r\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\r\n\r\n    def _step(self, batch):\r\n        x, y = batch\r\n        y_hat = self.model(x)\r\n        return torch.nn.functional.mse_loss(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self._step(batch)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._step(batch)\r\n\r\n\r\nclass Checkpoint(ModelCheckpoint):\r\n    def on_load_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"loading...\")\r\n        import pdb; pdb.set_trace()\r\n        foo = checkpoint['bar']\r\n\r\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"saving...\")\r\n        checkpoint[\"foo\"] = \"bar\"\r\n        return checkpoint\r\n\r\n\r\nsolver = Solver()\r\ncheckpoint = Checkpoint(dirpath=\"./\", save_last=True)\r\ntrainer = pl.Trainer(callbacks=[checkpoint], max_epochs=3)\r\ntrainer.fit(solver)\r\n\r\ntrainer = pl.Trainer(\r\n    callbacks=[checkpoint], resume_from_checkpoint=\"last.ckpt\", max_epochs=5\r\n)\r\ntrainer.fit(solver)\r\n```"
      }
    ]
  },
  {
    "number": 5672,
    "title": "Calling trainer.fit fails with: AttributeError: 'dict' object has no attribute 'pretty'",
    "created_at": "2021-01-27T07:03:05Z",
    "closed_at": "2021-02-02T16:40:21Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5672",
    "body": "## \ud83d\udc1b Bug\r\n```\r\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\r\ntrain, val = random_split(dataset, [55000, 5000])\r\n\r\nautoencoder = LitAutoEncoder()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(autoencoder, DataLoader(train))\r\n```\r\n\r\nCalling trainer.fit fails with AttributeError: 'dict' object has no attribute 'pretty'\r\n\r\n\r\n## To Reproduce\r\n\r\n```\r\nGPU available: True, used: False\r\nTPU available: None, using: 0 TPU cores\r\n/home/nithin/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\r\n  warnings.warn(*args, **kwargs)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-ec8282b1f4ec> in <module>\r\n      4 autoencoder = LitAutoEncoder()\r\n      5 trainer = pl.Trainer()\r\n----> 6 trainer.fit(autoencoder, DataLoader(train))\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    508         self.call_hook('on_fit_start')\r\n    509 \r\n--> 510         results = self.accelerator_backend.train()\r\n    511         self.accelerator_backend.teardown()\r\n    512 \r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in train(self)\r\n     54 \r\n     55     def train(self):\r\n---> 56         self.trainer.setup_trainer(self.trainer.model)\r\n     57         return self.train_or_test()\r\n     58 \r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in setup_trainer(self, model)\r\n    442             self.logger.log_hyperparams(ref_model.hparams_initial)\r\n    443             self.logger.log_graph(ref_model)\r\n--> 444             self.logger.save()\r\n    445 \r\n    446         # wait for all to join if on distributed\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py in wrapped_fn(*args, **kwargs)\r\n     38     def wrapped_fn(*args, **kwargs):\r\n     39         if rank_zero_only.rank == 0:\r\n---> 40             return fn(*args, **kwargs)\r\n     41 \r\n     42     return wrapped_fn\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/loggers/tensorboard.py in save(self)\r\n    234         # save the metatags file if it doesn't exist\r\n    235         if not self._fs.isfile(hparams_file):\r\n--> 236             save_hparams_to_yaml(hparams_file, self.hparams)\r\n    237 \r\n    238     @rank_zero_only\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/core/saving.py in save_hparams_to_yaml(config_yaml, hparams)\r\n    386         with fs.open(config_yaml, \"w\", encoding=\"utf-8\") as fp:\r\n    387             try:\r\n--> 388                 OmegaConf.save(hparams, fp)\r\n    389                 return\r\n    390             except (UnsupportedValueType, ValidationError):\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/omegaconf/omegaconf.py in save(config, f, resolve)\r\n    268         :param resolve: True to save a resolved config (defaults to False)\r\n    269         \"\"\"\r\n--> 270         data = config.pretty(resolve=resolve)\r\n    271         if isinstance(f, (str, pathlib.Path)):\r\n    272             with io.open(os.path.abspath(f), \"w\", encoding=\"utf-8\") as file:\r\n\r\nAttributeError: 'dict' object has no attribute 'pretty'\r\n\r\n```\r\n## Expected behavior\r\n\r\nCalling **trainer.fit(autoencoder, DataLoader(train))** should train MNIST classifier without errors.\r\n\r\n## Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1050 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1\r\n\t- pytorch-lightning: 1.1.6\r\n\t- tqdm:              4.50.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.3\r\n\t- version:           #70~18.04.1-Ubuntu SMP Tue Jan 12 17:18:00 UTC 2021\r\n\r\n### Additional context\r\n\r\nThe program was running on **jupyter notebook**.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5672/comments",
    "author": "nithinivi",
    "comments": [
      {
        "user": "omry",
        "created_at": "2021-01-28T05:03:29Z",
        "body": "Try to upgrade to OmegaConf 2.0, you have an incompatible version installed."
      },
      {
        "user": "Borda",
        "created_at": "2021-01-29T21:04:31Z",
        "body": "@nithinivi what OmegaConf are you using?"
      },
      {
        "user": "nithinivi",
        "created_at": "2021-01-30T03:52:21Z",
        "body": "@Borda @omry The version number of OmegaConf  is  2.0.0"
      },
      {
        "user": "omry",
        "created_at": "2021-01-30T09:21:02Z",
        "body": "@nithinivi,\r\n\r\nconfig.pretty() is deprecated in OmegaConf 2.0 and the source code no longer contain calls to it.\r\nThe stack trace is definitely not from OmegaConf 2.0. If you think this is wrong please provide repro instructions from scratch (including the creation of a virtualenv or a conda environment).\r\n\r\nBy the way, please upgrade to latest OmegaConf 2.0 release (currently 2.0.6)."
      },
      {
        "user": "nithinivi",
        "created_at": "2021-02-01T06:31:32Z",
        "body": "I created a new conda env and installed the torch and pytorch-ligthing.\r\nTried out the same code and the issue was not reproduced. So I do believe this was a issues with the environment setup and it's not using OmegaConf 2.0.\r\n\r\nI have installed python using **pyenv**  and executing the code in a jupyter notebook."
      },
      {
        "user": "roytseng-tw",
        "created_at": "2021-02-02T15:26:32Z",
        "body": "I had the same issue with omegaconf 2.0.1rc11.\r\nAfter update to the latest version 2.0.6, the issue is gone."
      }
    ]
  },
  {
    "number": 5641,
    "title": "Log fails: \"Tensors must be CUDA and dense\" with multi-GPUs using ddp",
    "created_at": "2021-01-24T21:26:14Z",
    "closed_at": "2021-01-25T18:30:45Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5641",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm encountering the following error when running my code (see below) with multi-GPUs (single GPU and CPU works fine). `accelerator` used is `ddp`.\r\n```\r\nline 117, in test_epoch_end\r\nwork = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: self.log(\"avg_test_acc\", avg_test_acc, sync_dist=True)Tensors must be CUDA and dense\r\n```\r\nHowever, when I remove the `sync_dist=True` all goes well.\r\n\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\nThe code, at it's core, looks like this:\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.autograd import Variable\r\nfrom argparse import ArgumentParser\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\nfrom torch.nn import functional as F\r\nfrom argparse import ArgumentParser\r\nimport mlflow\r\nfrom data_loading.data_loader import MNISTDataModule\r\nfrom model.model import LightningMNISTClassifier\r\nimport os\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n    def __init__(self, **kwargs):\r\n        super(MNISTDataModule, self).__init__()\r\n        self.df_train = None\r\n        self.df_test = None\r\n        self.train_data_loader = None\r\n        self.test_data_loader = None\r\n        self.args = kwargs\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    def setup(self, stage=None):\r\n        self.df_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        self.df_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.df_train, batch_size=self.args['training_batch_size'], num_workers=self.args[\"num_workers\"], shuffle=True\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n            self.df_test, batch_size=self.args['test_batch_size'], num_workers=self.args[\"num_workers\"], shuffle=False\r\n        )\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self, len_test_set: int, **kwargs):\r\n        super(LightningMNISTClassifier, self).__init__()\r\n        self.optimizer = None\r\n        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\r\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\r\n        self.dropout1 = torch.nn.Dropout2d(0.25)\r\n        self.fc1 = torch.nn.Linear(9216, 128)\r\n        self.dropout2 = torch.nn.Dropout2d(0.25)\r\n        self.fc2 = torch.nn.Linear(128, 10)\r\n        self.args = kwargs\r\n        self.len_test_set = len_test_set\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\"--num_workers\", type=int, default=3, metavar=\"N\", help=\"number of workers (default: 3)\")\r\n        parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate (default: 0.01)\")\r\n        parser.add_argument('--training-batch-size', type=int, default=64, help='Input batch size for training')\r\n        parser.add_argument('--test-batch-size', type=int, default=1000, help='Input batch size for testing')\r\n\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.max_pool2d(x, 2)\r\n        x = torch.flatten(self.dropout1(x), 1)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.dropout2(x)\r\n        x = self.fc2(x)\r\n        output = F.log_softmax(x, dim=1)\r\n\r\n        return output\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        train_avg_loss = torch.stack([train_output[\"loss\"] for train_output in training_step_outputs]).mean()\r\n        self.log(\"train_loss\", train_avg_loss)\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Predicts on the test dataset to compute the current accuracy of the model.\r\n\r\n        :param test_batch: Batch data\r\n        :param batch_idx: Batch indices\r\n\r\n        :return: output - Testing accuracy\r\n        \"\"\"\r\n\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        _, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy(y_hat.cpu(), y.cpu())\r\n        # sum up batch loss\r\n        data, target = Variable(x), Variable(y)\r\n        test_loss = F.nll_loss(output, target, reduction='sum').data.item()\r\n        # get the index of the max log-probability\r\n        pred = output.data.max(1)[1]\r\n        correct = pred.eq(target.data).cpu().sum().item()\r\n        return {\"test_acc\": test_acc, \"test_loss\": test_loss, \"correct\": correct}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n\r\n        :param outputs: outputs after every epoch end\r\n\r\n        :return: output - average test loss\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([test_output[\"test_acc\"] for test_output in outputs]).mean()\r\n        avg_test_loss = sum([test_output[\"test_loss\"] for test_output in outputs])/self.len_test_set\r\n        test_correct = sum([test_output[\"correct\"] for test_output in outputs])\r\n        self.log(\"avg_test_acc\", avg_test_acc, sync_dist=True)\r\n        self.log(\"avg_test_loss\", avg_test_loss, sync_dist=True)\r\n        self.log(\"test_correct\", test_correct, sync_dist=True)\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Prepares the data for training and prediction\r\n        \"\"\"\r\n        return {}\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Initializes the optimizer and learning rate scheduler\r\n\r\n        :return: output - Initialized optimizer and scheduler\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters())\r\n        return [self.optimizer]\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = ArgumentParser(description=\"PyTorch Autolog Mnist Example\")\r\n    use_cuda = torch.cuda.is_available()\r\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n\r\n    parser = pl.Trainer.add_argparse_args(parent_parser=parser)\r\n    parser = LightningMNISTClassifier.add_model_specific_args(parent_parser=parser)\r\n\r\n    mlflow.pytorch.autolog()\r\n    # parse cli arguments\r\n    args = parser.parse_args()\r\n    dict_args = vars(args)\r\n\r\n    set_general_random_seeds(dict_args['general_seed'])\r\n    set_pytorch_random_seeds(dict_args['pytorch_seed'], True)\r\n\r\n    if \"accelerator\" in dict_args and dict_args[\"accelerator\"] == \"None\":\r\n        dict_args[\"accelerator\"] = None\r\n\r\n    dm = MNISTDataModule(**dict_args)\r\n\r\n    dm.prepare_data()\r\n    dm.setup(stage=\"fit\")\r\n    model = LightningMNISTClassifier(len_test_set=len(dm.df_test), **dict_args)\r\n    trainer = pl.Trainer.from_argparse_args(args)\r\n     \r\n\r\n    trainer.deterministic = True\r\n    trainer.benchmark = False\r\n    trainer.fit(model, dm)\r\n    trainer.test()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nTrain and test successfully without mentioned error above when using multiple GPUs (like it runs successfully on single GPU and CPU).\r\n\r\n### Environment\r\n\r\n* CUDA \r\n\t* GPU:\r\n\t\t* NVIDIA [Tesla V100 PCIe 32GB] \r\n\t* available: True\r\n\t* Version 11.2\r\n* Packages\r\n\t* cudatoolkit=10.1\r\n\t* numpy                     =1.19.1\r\n\t* torchvision             =  0.7.0 \r\n\t* pytorch-lightning=1.1.5\r\n\t* pycuda=2019.1.2\r\n\t* python=3.8.2\r\n\t* pytorch=1.6.0\r\n\t\r\n\r\n - OS: Linux Ubuntu Ubuntu 18.04.3 LTS\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5641/comments",
    "author": "Imipenem",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-01-25T01:38:22Z",
        "body": "Remove the .cpu() call \r\n`correct = pred.eq(target.data).cpu().sum().item()`\r\nshould be \r\n`correct = pred.eq(target.data).sum()`"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-25T01:40:28Z",
        "body": "And if you want to compute accuracy on multi gpu correctly, I recommend directly using the Accuracy metric (from pytorch_lightning.metrics)"
      }
    ]
  },
  {
    "number": 4238,
    "title": "Metrics do not support multilabel tasks.",
    "created_at": "2020-10-19T18:00:26Z",
    "closed_at": "2020-10-22T16:13:54Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4238",
    "body": "## \ud83d\udc1b Bug\r\n\r\nScikit-learn metrics deal well will multilabel tasks, but this doesn't seem to be supported in Pytorch-Lightning metrics.  There is this #3350 , but it seems to confuse multiclass with multilabel (multiple values to predict). \r\n\r\n### To Reproduce\r\nGiven predictions tensor: \r\n```\r\ntensor([[0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.]])\r\n```\r\nand labels tensor:\r\n ```\r\ntensor([[1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0]])\r\n\r\n```\r\nThe call to `f1_score(met_preds,labels, class_reduction='macro')` yields `tensor(0.3333)`, because it flattens the tensors and macro-averages per class.\r\n\r\n### Expected behavior\r\n\r\nI would expect it to be consistent with the call to\r\n`sk_f1_score(labels.numpy(), met_preds.numpy(), average='macro')`, which yields `0.0`, because it treats each column separately and macro-averages them per task.\r\n\r\nThis discrepancy also occurs for other metrics. For example sklearn deals with multilabel accuracy by using subset accuracy (0 here), but PL produces an accuracy score of 0.5.\r\n\r\n### Environment\r\n - PyTorch Version : 1.6\r\n - OS (e.g., Linux): OSX\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7.8\r\n- Pytorch-Lightning version : 1.0.2\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4238/comments",
    "author": "jdhorwood",
    "comments": [
      {
        "user": "teddykoker",
        "created_at": "2020-10-20T22:51:12Z",
        "body": "Could you try the class interface? We have tested multilabel f1 with the class metrics, I think we are in the process of making sure we have the same functionality for functional"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-21T07:46:51Z",
        "body": "As @teddykoker stated we are in the process of unifying the metrics class based metrics with the functional metrics (starting with all the regression metrics here #4166 ). Until then, please use the `Fbeta` metric (however, note that there is currently a bug in that metric, that should be taken care of by this PR #4183 ):\r\n```\r\nmetric = Fbeta(beta=1.0, multilabel=True, average='macro)\r\nmetric(preds, target)\r\n>>> tensor(0.)\r\n```"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-22T16:13:49Z",
        "body": "Yes, let's close it. I will unify fbeta functional and class metric within a couple of days, so functional also support multilabel. "
      }
    ]
  },
  {
    "number": 4079,
    "title": "ModelCheckpoint save_function() not set?",
    "created_at": "2020-10-11T15:29:35Z",
    "closed_at": "2020-10-11T15:37:26Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4079",
    "body": "I am training a PL model using the following code snippet:\r\n\r\n```python\r\n    # logger\r\n    tb_logger = pl_loggers.TensorBoardLogger(cfg.logs.path, name='rnn_exp')\r\n\r\n    # checkpoint callback\r\n    checkpoint_callback = ModelCheckpoint(\r\n        filepath=cfg.checkpoint.path + \"encoder_rnn{epoch:02d}\",\r\n        save_top_k=1,\r\n        mode=\"min\" # monitor is defined in val_step: EvalResult(checkpoint_on=val_loss)\r\n    )\r\n\r\n    # early stopping callback\r\n    early_stopping_callback = EarlyStopping(\r\n        monitor=\"val_loss\",\r\n        patience=cfg.val.patience,\r\n        mode=\"min\"\r\n    )\r\n\r\n    tokenizer = ...\r\n    dm = MyDataModule(cfg, tokenizer)\r\n\r\n    model = RNNEncoder(cfg)\r\n\r\n    trainer = Trainer(\r\n        fast_dev_run=False,\r\n        max_epochs=cfg.train.max_epochs,\r\n        gpus=1,\r\n        logger=tb_logger,\r\n        callbacks=[checkpoint_callback, early_stopping_callback]\r\n    )\r\n\r\n    # training\r\n    dm.setup('fit')\r\n    trainer.fit(model, datamodule=dm)\r\n```\r\n\r\nHowever, after the first epoch, the model presents the following error, probably when calling the model checkpoint callback:\r\n\r\n```python\r\n    trainer.fit(model, datamodule=dm)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1073, in fit\r\n    results = self.accelerator_backend.train(model)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_backend.py\", line 51, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 516, in run_training_epoch\r\n    self.run_evaluation(test_mode=False)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 603, in run_evaluation\r\n    self.on_validation_end()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 176, in on_validation_end\r\n    callback.on_validation_end(self, self.get_model())\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 27, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 380, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch, trainer, pl_module)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 421, in _do_check_save\r\n    self._save_model(filepath, trainer, pl_module)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 212, in _save_model\r\n    raise ValueError(\".save_function() not set\")\r\nValueError: .save_function() not set\r\n\r\n```\r\nCould you tell me if I forgot to configure something?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4079/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-10-11T15:33:26Z",
        "body": "currently you need to set the ModelCheckpoint via `Trainer(checkpoint_callback=...)`\r\n#3990 will enable passing it to callbacks"
      }
    ]
  },
  {
    "number": 3634,
    "title": "AttributeError: 'dict' object has no attribute 'callback_metrics' when using validation_epoch_end callbac",
    "created_at": "2020-09-23T22:46:59Z",
    "closed_at": "2020-09-25T20:03:31Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3634",
    "body": "## \ud83d\udc1b Bug\r\n\r\nHello.  I am  trying to setup the early stop callback, and according to the warning that I get I need to use the validation_epoch_end callback. When I do that, I get the following error:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-25-9cbc4363b76a> in <module>()\r\n     10 \r\n     11 # Train the model \u26a1\r\n---> 12 trainer.fit(model)\r\n\r\n10 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)\r\n    419             if isinstance(eval_results, list):\r\n    420                 for eval_result in eval_results:\r\n--> 421                     self.callback_metrics = eval_result.callback_metrics\r\n    422             else:\r\n    423                 self.callback_metrics = eval_results.callback_metrics\r\n\r\nAttributeError: 'dict' object has no attribute 'callback_metrics'\r\n```\r\n\r\n``` python\r\nclass MyNN(pl.LightningModule):\r\n    def __init__(self, input_size=3, seq_len=107, pred_len=68, hidden_size=50, num_layers=1, dropout=0, lr=1e-2):\r\n        super().__init__()\r\n        \r\n        self.pred_len = pred_len\r\n        \r\n        self.lr = lr\r\n        \r\n        self.rnn = nn.LSTM(\r\n            input_size=input_size, \r\n            hidden_size=hidden_size, \r\n            num_layers=num_layers, \r\n            dropout=dropout, \r\n            bidirectional=True,\r\n            batch_first=True\r\n        )\r\n        \r\n        self.linear = nn.Linear(hidden_size*2, 5)\r\n\r\n        self.example_input_array = torch.Tensor(np.zeros(input_size).reshape(1, 1, -1))\r\n    \r\n    def forward(self, X):\r\n        lstm_output, (hidden_state, cell_state) = self.rnn(X)\r\n        \r\n        labels = self.linear(lstm_output[:, :self.pred_len, :])\r\n        \r\n        return labels\r\n    \r\n    def training_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        loss = scoring(self(x.float()), y.float())\r\n\r\n\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, logger=True)\r\n        return result\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x.float())\r\n        loss = scoring(logits, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val_loss', loss)\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\r\n    \r\n    def val_dataloader(self):\r\n        return DataLoader(MyValSet(), batch_size=64)\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(MyDataset(), batch_size=64, shuffle=True)\r\n\r\n\r\n    def training_epoch_end(self, outputs):\r\n        #  the function is called after every epoch is completed\r\n\r\n        # calculating average loss  \r\n        avg_loss = outputs[\"train_loss\"].mean()\r\n\r\n        # creating log dictionary\r\n        tensorboard_logs = {'train_loss': avg_loss}\r\n\r\n        epoch_dictionary={\r\n            # required\r\n            'train_loss': avg_loss,\r\n            \r\n            # for logging purposes\r\n            'log': tensorboard_logs}\r\n\r\n        return epoch_dictionary\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        #  the function is called after every epoch is completed\r\n\r\n        # calculating average loss  \r\n        avg_loss = outputs[\"val_loss\"].mean()\r\n\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        \r\n        epoch_dictionary={\r\n            # required\r\n            'val_loss': avg_loss,\r\n            \r\n            # for logging purposes\r\n            'log': tensorboard_logs}\r\n\r\n        return epoch_dictionary\r\n```\r\n\r\nand my training loop is:\r\n\r\n```python\r\nLEARNING_RATE = 1e-3\r\nNUM_LAYERS = 2\r\nDROPOUT = 0.1\r\nHIDDEN_SIZE = 100\r\nEPOCHS = 100\r\n\r\n# Initialize a trainer\r\ntrainer = pl.Trainer(gpus=1, max_epochs=EPOCHS, progress_bar_refresh_rate=20, early_stop_callback=True, auto_lr_find=True)\r\nmodel = MyNN(num_layers=NUM_LAYERS, dropout=DROPOUT)\r\n\r\n# Train the model \u26a1\r\ntrainer.fit(model)\r\n```\r\n\r\nI am using Google Colab, with the following versions:\r\n```\r\npytorch-lightning==0.9.0\r\ntorch==1.6.0+cu101\r\n```\r\n\r\nAm I doing something wrong, or what is the issue here? \r\nThank you! :)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3634/comments",
    "author": "djrmarques",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T22:34:42Z",
        "body": "Pretty sure the problem is, you cannot mix results objects in your step methods with dict in your epoch_end methods. \r\nUse either dicts everywhere or results everywhere, but not both. Let me know if that solves your problem. \r\n\r\nYou mention colab, if you need further help, mind sharing the colab link so we can have a look help you better."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T22:53:24Z",
        "body": "oh, sorry, did not read the first sentence in your message. \r\nFor early stopping, use this\r\n\r\n```python\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x.float())\r\n        loss = scoring(logits, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)  # <--- add this\r\n        result.log('val_loss', loss)\r\n        return result\r\n```\r\nThere should be no need for the validation_epoch_end, you can savely remove it. the validation loss will be reduced and logged automatically at the end of epoch."
      },
      {
        "user": "djrmarques",
        "created_at": "2020-09-25T20:03:31Z",
        "body": "So I ended up putting it all into dictionaries and it worked. \r\n\r\nBut before that I tried to use the results object in both the validation and train set, and the model was running, but for some reason the train_loss was not logging on tensorboard, but maybe I was doing something wrong. I will leave it for now like this, because I want to finish my model, but after that I will try to set up the results object and see if all goes well. \r\n\r\nThank you! "
      }
    ]
  },
  {
    "number": 3430,
    "title": "Issue with trainer.test in \"ddp\" distributed mode",
    "created_at": "2020-09-09T20:36:18Z",
    "closed_at": "2020-09-10T16:21:19Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3430",
    "body": "Hi -\r\n\r\nI have the following pseudo code workflow:\r\n\r\n> trainer = Trainer(distributed_backend='ddp', ...)\r\nmodel = new custom LightningModule\r\ntrainer.fit(model, ...)\r\nmodel.freeze()\r\ntrain.test(model, ...)\r\n\r\nThe error that I get is this:\r\n\r\n`AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.`\r\n\r\nWhat is the best way to address this?\r\n\r\nThanks very much,\r\nGriffin\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3430/comments",
    "author": "griff4692",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-09T20:51:45Z",
        "body": "That is a PyTorch `AssertionError`. Try it without calling `model.freeze()`?"
      }
    ]
  },
  {
    "number": 3426,
    "title": "Model checkpoint not saving hyperparameters correctly",
    "created_at": "2020-09-09T17:13:40Z",
    "closed_at": "2020-09-09T20:44:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3426",
    "body": "When using the ModelCheckpoint, my hyperparameters are not being saved with the checkpoints.  So I get an AttributeError when attempting to load from checkpoints.\r\n\r\nTo reproduce:\r\n```\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport argparse\r\nfrom bunch import Bunch\r\n\r\nimport pytorch_lightning as pl\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self, args):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n        print('args:', args)\r\n        print(args.to_print)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return pl.TrainResult(loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n    \r\ntrain_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\r\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        os.path.join(os.getcwd(), 'chkpts'),\r\n        save_top_k=1,\r\n        verbose=True,\r\n        monitor='loss',\r\n        mode='min'\r\n    )\r\ntrainer = pl.Trainer(checkpoint_callback=checkpoint_callback,\r\n                    train_percent_check=0.1,\r\n                    val_percent_check=0,\r\n                    max_epochs=1)\r\n\r\nhparams = argparse.Namespace()\r\nhparams.to_print = 'foo'\r\nmodel = LitModel(hparams)\r\n\r\ntrainer.fit(model, train_loader)\r\n\r\nmod = LitModel.load_from_checkpoint(ckpt_path)\r\n```\r\n\r\nProduces the following Error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-38-868d05212321> in <module>\r\n     49 trainer.fit(model, train_loader)\r\n     50 \r\n---> 51 mod = LitModel.load_from_checkpoint(ckpt_path)\r\n\r\n~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, *args, **kwargs)\r\n    151         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\r\n    152 \r\n--> 153         model = cls._load_model_state(checkpoint, *args, strict=strict, **kwargs)\r\n    154         return model\r\n    155 \r\n\r\n~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, strict, *cls_args, **cls_kwargs)\r\n    188             cls_args, cls_kwargs = [], {}\r\n    189 \r\n--> 190         model = cls(*cls_args, **cls_kwargs)\r\n    191         # load the state_dict on the model automatically\r\n    192         model.load_state_dict(checkpoint['state_dict'], strict=strict)\r\n\r\n<ipython-input-38-868d05212321> in __init__(self, args)\r\n     15         self.l1 = torch.nn.Linear(28 * 28, 10)\r\n     16         print('args:', args)\r\n---> 17         print(args.to_print)\r\n     18 \r\n     19     def forward(self, x):\r\n\r\nAttributeError: 'dict' object has no attribute 'to_print'\r\n```\r\n\r\nThe print statements indicate that `args` is an empty dict when attempting to load from checkpoint. \r\n\r\nWhen inspecting the checkpoint\r\n```\r\nckpt_path = os.path.join(os.getcwd(), '_ckpt_epoch_0.ckpt')\r\nckpt = torch.load(ckpt_path)\r\nprint(ckpt.keys())\r\n```\r\n\r\nI get the following:\r\n```\r\ndict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'checkpoint_callback_best_model_score', 'checkpoint_callback_best_model_path', 'optimizer_states', 'lr_schedulers', 'state_dict'])\r\n```\r\n\r\nMy understanding is there should be a `hyper_parameters` in the checkpoint.\r\n\r\n\r\nSystem:\r\n- PyTorch Version 1.3.1\r\n- pytorch-lightning: 0.9.0 installed conda\r\n- OS: Ubuntu 18.04\r\n- Python 3.6",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3426/comments",
    "author": "davidwhealey",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-09T19:29:10Z",
        "body": "you need to call `self.save_hyperparameters()` in `__init__` to make it work."
      }
    ]
  },
  {
    "number": 3113,
    "title": "TypeError in closure_loss = closure_loss / self.accumulate_grad_batches for Cross_entropy loss",
    "created_at": "2020-08-23T14:55:43Z",
    "closed_at": "2020-08-24T18:43:38Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3113",
    "body": "## \ud83d\udc1b Bug\r\n\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n   1055         # (if accumulate_grad_batches = 1 no effect)\r\n   1056         closure_loss = training_step_output.minimize if is_result_obj else training_step_output.batch_loss\r\n-> 1057         closure_loss = closure_loss / self.accumulate_grad_batches\r\n   1058 \r\n   1059         # the loss will get scaled for amp. avoid any modifications to it\r\n\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n#### Code sample\r\n\r\n```\r\nclass CustomModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.cnn_model = nn.Sequential(\r\n            nn.Conv2d(1, 6, kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2),\r\n            nn.Conv2d(6, 16, kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2),\r\n            nn.Conv2d(16,32,kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2))\r\n\r\n        self.fc_model = nn.Sequential(\r\n            nn.Linear(2592, 1024), # (N, 2592) -> (N, 1024)\r\n            nn.ReLU(),\r\n            nn.Linear(1024, 30))  # (N, 1024)  -> (N, 30)) #30 classes\r\n\r\n    def forward(self, x):\r\n        x = self.cnn_model(x)\r\n        # print(x.shape) \r\n        x = x.view(x.size(0), -1)\r\n        # print(x.shape)    \r\n        x = self.fc_model(x)\r\n        # print(x.shape)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        # acc = FM.accuracy(y_hat, y)\r\n        result = pl.TrainResult()\r\n        print('f')\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        acc = FM.accuracy(y_hat, y)\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val_loss', loss, prog_bar=True)\r\n        result.log('val_acc', acc, prog_bar=True)\r\n        print('f')\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\r\n        return optimizer\r\n\r\n    def train_dataloader(self):\r\n        train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=32)\r\n        # print(\"Length of the train_loader:\", len(train_loader))\r\n        return train_loader\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(dataset=val_dataset, shuffle=False, batch_size=32)\r\n```\r\n\r\nThe error occurs when I am fitting the model to train. Using lightning 0.9.0 on colab. I am loading dataset by mounting drive and using torchvision datasets.ImageFolder function.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3113/comments",
    "author": "srijansingh53",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-08-23T15:47:18Z",
        "body": "changing `result = pl.TrainResult()` to `result = pl.TrainResult(minimize=loss)` is all you need."
      }
    ]
  },
  {
    "number": 2282,
    "title": "optimizer got an empty parameter list",
    "created_at": "2020-06-19T20:39:51Z",
    "closed_at": "2020-06-20T23:04:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2282",
    "body": " Hi,\r\nGot the following error:\r\nValueError: optimizer got an empty parameter list with both options below:\r\n\r\ndef configure_optimizers(self):\r\n        # option1 optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        # option 2\r\n        optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)\r\n        return optimizer\r\n\r\nclass Autoencoder(pl.LightningModule):\r\n    \r\n    def __init__(self, hparams: argparse.Namespace):\r\n        super(Autoencoder,self).__init__() \r\n        self.hparams = hparams\r\n           \r\n        self_layer_e_1 = nn.Conv1d(hparams.in_channels, hparams.out_channels, hparams.kernel_size)\r\n        self_layer_e_2 = nn.Conv1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        self_layer_d_1 = nn.ConvTranspose1d(hparams.in_channels,hparams.out_channels,hparams.kernel_size)\r\n        self_layer_d_2 = nn.ConvTranspose1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        \r\n    \r\n    def forward(self,x):\r\n        x = self_layer_e_1(x)\r\n        x = nn.ReLu(x)\r\n        x = self_layer_e_2(x)\r\n        encoded = nn.ReLU(x)\r\n        x = self_layer_d_1(encoded)\r\n        x = nn.ReLU(x)\r\n        decoded = self_layer_d_2(x)\r\n        decoded = self.decoder(encoded)\r\n        return self.decoded, self.encoded\r\n    \r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, _ = batch\r\n        decoded, encoded = self.forward(x)\r\n        loss = MSE(x, decoded)\r\n        return loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'val')\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'test')\r\n        \r\n    def _shared_eval(self, batch, batch_idx, prefix):\r\n        x, y = batch\r\n        decoded, encoded = self.forward(x)\r\n        loss = F.nll_loss(x, decoded)\r\n        return {f'{prefix}_loss': loss}\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(self.CarrierDataset, batch_size=self.hparams.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.CarrierDataset, batch_size=hparams.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self,CarrierDataset, batch_size=hparams.batch_size)\r\n\r\n    def configure_optimizers(self):\r\n        #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)\r\n        return optimizer",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2282/comments",
    "author": "soulhi-vz",
    "comments": [
      {
        "user": "versatran01",
        "created_at": "2020-06-19T21:56:35Z",
        "body": "You need to use `self.xxx = nn.Conv2d(a,b,c)`  instead of `self_xxx = nn.Conv2d(a,b,c)`  for `nn.Module` to register them as parameters, otherwise your module has no paramters, thuse the optimizer gets nothing."
      }
    ]
  },
  {
    "number": 2254,
    "title": "Single node DDP: \"Default process group is not initialized\"",
    "created_at": "2020-06-19T02:37:22Z",
    "closed_at": "2020-07-10T01:20:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2254",
    "body": "## \ud83d\udc1b Bug\r\nUnable to start single node ddp training on 0.8.0\r\n\r\n### To Reproduce\r\n~~was going to run the gpu_template but... #2235~~\r\nboth methods of running the template result in the same error\r\n```\r\n$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp_spawn\r\n$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp\r\n```\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 860, in fit\r\n    self.barrier('fit_prepare_data')\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1261, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 1484, in barrier\r\n    _check_default_pg()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 187, in _check_default_pg\r\n    \"Default process group is not initialized\"\r\nAssertionError: Default process group is not initialized\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2254/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T02:47:50Z",
        "body": "can you post code to reproduce? just a minimal example that breaks\r\n\r\nBTW, the GPU template is fixed..."
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T04:50:07Z",
        "body": "I just tested the merged changes with both ddp and ddp_spawn again got this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 891, in fit\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    self.ddp_train(task, model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 907, in fit\r\n    self.setup()\r\nTypeError: setup() missing 1 required positional argument: 'stage'\r\n    self.spawn_ddp_children(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 441, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\r\n    self.setup()\r\nTypeError: setup() missing 1 required positional argument: 'stage'\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T05:14:30Z",
        "body": "try again. that was a typo"
      },
      {
        "user": "armancohan",
        "created_at": "2020-06-23T05:35:19Z",
        "body": "Still having the `Default process group is not initialized` issue when using trainer.test \r\n"
      },
      {
        "user": "wukailu",
        "created_at": "2020-06-23T06:30:56Z",
        "body": "> Still having the `Default process group is not initialized` issue when using trainer.test\r\n\r\nI still have this bug as well. One temporary solution is creating a new single GPU trainer to do the test.\r\n\r\nLike\r\n\r\n```\r\ntrainer = Trainer(gpus=1, deterministic=True, logger=logger)\r\ntrainer.model = model\r\ntrainer.test()\r\n```"
      },
      {
        "user": "armancohan",
        "created_at": "2020-06-23T19:57:28Z",
        "body": "Right, I know it works on single gpu. I have a large test set and ideally want faster inference using multiple gpus."
      },
      {
        "user": "zackcarson",
        "created_at": "2020-07-02T15:11:23Z",
        "body": "Can we re-open this issue? I am still having the `Default process group is not initialized` issue when I hit `trainer.test()` with ddp (with any number of gpus, even 1). I'm using the latest release from yesterday."
      },
      {
        "user": "jxchen01",
        "created_at": "2020-07-04T05:32:04Z",
        "body": "having the same problem..... I also tried to downgrade pl to an older version, like 0.7.5, and try to using the older version to do the inference. But, the model trained and saved using the 0.8.x seems to not directly be compatible with older version.  "
      },
      {
        "user": "channingxiao",
        "created_at": "2020-07-09T12:11:00Z",
        "body": "version: 0.8.4  train with ddp,  Got \"Default process group is not initialized\" when run trainer.test()"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-09T12:18:32Z",
        "body": "could you try master? this is fixed there"
      },
      {
        "user": "jxchen01",
        "created_at": "2020-08-17T19:13:27Z",
        "body": "@williamFalcon Trying 0.8.5\r\n\r\nTrained with ddp, and testing with ddp, but got the following error message:\r\n\r\n```\r\nAssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.\r\n```\r\n\r\nAny idea?\r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 1665,
    "title": "Trainer add args doesn't add default root dir",
    "created_at": "2020-04-29T15:59:49Z",
    "closed_at": "2020-05-12T12:53:27Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1665",
    "body": "## \ud83d\udc1b Bug\r\n1. When using `parser = Trainer.add_argparse_args(parser)`, it's supposed to put all Trainer's arguments in the argparse with default values. Though currently it doesn't add `default_root_dir` and you get the error:\r\n\r\n```\r\n'Namespace' object has no attribute 'default_root_dir'\r\n```\r\nIt does add `default_save_path` which is deprecated.\r\n\r\n\r\n### To Reproduce\r\n#### Code Sample\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = Trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.default_root_dir)\r\n```\r\n\r\nA similar unit test could also be made, if not there already.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.3\r\n        - tensorboard:       2.2.0\r\n        - tqdm:              4.45.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.6.7\r\n        - version:           #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1665/comments",
    "author": "tshrjn",
    "comments": [
      {
        "user": "olineumann",
        "created_at": "2020-04-30T11:46:49Z",
        "body": "Did you tried to update to 0.7.5. Maybe it is already solved."
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-04T07:15:14Z",
        "body": "Hi @olineumann, yes updating did resolve this. However, the `profiler` arg is now broken. The same demo code above with `profiler` gives the same error `'Namespace' object has no attribute 'profiler'`."
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-04T08:57:26Z",
        "body": "What do you mean 'with profiler'? Do you mean Trainer(..., profiler=True)? But you don't initialize a Trainer.\r\n\r\nRunning your code or this below didn't crash with any error on my machine.\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\ntrainer = Trainer(profiler=True)\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.default_root_dir)\r\n```\r\n\r\nMaybe you could post the complete error message from the python interpreter. "
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-06T22:12:44Z",
        "body": "`add_argparse_args ` is supposed to add the args from trainer to parser. But it doesn't do that for a few args. In this case `profiler`, previously the issue was for `default_root_dir`.\r\n\r\nTry the following code by running:\r\n`python demo.py --profiler True` or  other possibly accepted way `python demo.py --profiler`  with the following code:\r\n\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\ntrainer = Trainer()\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.profiler)\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-12T10:43:03Z",
        "body": "I just created a PR. After looking at the code I found out that add_argparse_args is checking the argument types and is only adding attributes of type str, float, int or bool. The profiler attribute could be of type bool so it should be a bug.\r\n\r\nI saw that get_init_arguments_and_types() is returning profiler as argument but only of type BaseProfiler. After updating typing annotation of profiler argument it worked. Should be available in the next version.\r\n\r\nSee PR #1794 "
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-12T21:06:28Z",
        "body": "A similar issue is with the pickling of the profiler when it's a `Profile` object & the trainer tries to save the `hparams`.\r\n\r\n```python\r\nTypeError: can't pickle Profile objects\r\n```\r\n\r\n\r\nExample code:\r\n\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning import profiler\r\nfrom pl_bolts.models.gans import BasicGAN\r\n\r\ntrainer = Trainer()\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\nmodel = BasicGAN()\r\n\r\ntrainer = Trainer.from_argparse_args(\r\n        args, profiler=profiler.AdvancedProfiler())\r\ntrainer.fit(model)\r\n\r\n```\r\n"
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-13T08:02:45Z",
        "body": "Can't reproduce your issue with pl version 0.7.6rc1. On my machine your code runs and saves checkpoints without crashing. Also this wouldn't belong to the topic of this issue imo. This would be a bug in the saving routine. "
      }
    ]
  },
  {
    "number": 1221,
    "title": "Colab weird behaviour and error when passing values from collate_fn to validation_step",
    "created_at": "2020-03-24T08:37:03Z",
    "closed_at": "2020-04-04T14:33:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1221",
    "body": "The same code that runs perfectly on my local machine (MacOS with Python3.7 and tested on an Ubuntu docker 18.04 with Python3.6.9) fails to run on colab.\r\n\r\nIt performs weirdly, calling the Dataset init() multiple times. However, it fails when the collate function that (correctly) returns (printed out the results) the following:\r\n```\r\nreturn x_tensor, x_lengths, y_tensor\r\n```\r\nget passed in the sanity check right at start in the ``validation_step``:\r\n```\r\ndef validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\r\n        x_tensor, x_lengths, y_tensor = batch <-- here it fails\r\n        .. forward(x_tensor,x _lengths) .. etc.\r\n```\r\nwith the error:\r\n```\r\nline 234, in validation_step\r\n    x_tensor, x_lengths, y_tensor = batch\r\nValueError: too many values to unpack (expected 3)\r\n```\r\nit actually just return the first value (x_tensor). I tried packing them in a dict from the collate function, but in the validation step ``batch`` comes to me just as the first key (as string!) from the dict created in collate_fn.\r\n\r\nThe environment is the same except the Python version (latest torch and lightning versions) across local runtime, docker ubuntu and colab. To ensure the same code is running on all machines, I'm doing just git clone, pip3 install -r requirements and python3 train.py. Am I missing something with Colab that's just not working? I can provide full code if needed.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1221/comments",
    "author": "dumitrescustefan",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:46:55Z",
        "body": "Try running master?"
      },
      {
        "user": "dumitrescustefan",
        "created_at": "2020-04-04T14:33:24Z",
        "body": "@williamFalcon \r\n\r\nConfirm that running on colab with version ``pytorch-lightning==0.7.2.dev0`` (from master directly) fixes the problem above. \r\n\r\nThanks a lot!"
      }
    ]
  },
  {
    "number": 167,
    "title": "Adding Support for Torchtext iterators",
    "created_at": "2019-08-26T01:35:55Z",
    "closed_at": "2019-08-26T23:04:12Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/167",
    "body": "I recently came across pytorch lightning and i am absolutely loving it  until now. Not having to worry about my training cycle and making it super efficient and fast. It has increased the amount of experiments i can pull off and good results have come out from it. \r\n\r\nRight now, i have been using torchtext with its dataset classes and its custom iterators. But when i tried to use the iterators option from torchtext such as Iterator or BucketIterator instead of Dataloader i get the following error:\r\n\r\n``` TypeError: embedding(): argument 'indices' (position 2) must be Tensor, not NoneType```\r\n\r\nThe problem is that instead of getting a Tensor im getting a NoneType. And i dont know why that is.\r\n\r\nNow, i tried to load the Dataset classes from torchtext with the DataLoader itself and i find the next error:\r\n\r\n```TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torchtext.data.example.Example'> ```\r\n\r\nSo, ideally i would really like to have the torchtext iterators supported with pytorch-lighting. But i dont know if there is a way around this issue that i havent found, still using the torchtext Dataset classes. Could anybody help me out with this?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/167/comments",
    "author": "dehoyosb",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-26T01:39:01Z",
        "body": "thanks for bringing this up. \r\nCan you try with the latest version? i think we fixed this. \r\n\r\notherwise, can you post a code snippet that generates this error so we can add a patch?\r\n"
      }
    ]
  }
]