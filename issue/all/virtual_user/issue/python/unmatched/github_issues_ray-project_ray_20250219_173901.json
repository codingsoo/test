[
  {
    "number": 30662,
    "title": "[tune] How to use an imported parameter via argparse in trainable function",
    "created_at": "2022-11-25T17:32:05Z",
    "closed_at": "2022-11-29T20:04:15Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/30662",
    "body": "### What happened + What you expected to happen\n\nI have a tuning task using an imported parameter via argparse in trainable function. The task crashes complaining the argument is not provided. It works fine If I use it outside the trainable function. \n\n### Versions / Dependencies\n\nRay 2.1.0\n\n### Reproduction script\n\nThe script being imported called \u201cinput_param.py\u201d:\r\n\r\n    import sys, argparse\r\n\r\n    parser = argparse.ArgumentParser(description='')\r\n    parser.add_argument('--ttt', type=int, required=True, help='anything > 1')\r\n    args = parser.parse_args()\r\n\r\n    ttt = args.ttt\r\n\r\nThe tuning task code is named as \u2018example.py\u2019:\r\n\r\n    import os\r\n    from ray import tune, air\r\n    from hyperopt import hp\r\n    from ray.tune.search.hyperopt import HyperOptSearch\r\n    import input_param as input_param\r\n\r\n    def trainable(config):\r\n        #print('!! ttt = ', input_param.ttt)\r\n        score = config[\"a\"] ** 2 + config[\"b\"]\r\n        tune.report(SCORE=score)\r\n\r\n\r\n    search_space = {\r\n        \"a\": hp.uniform(\"a\", 0, 1),\r\n        \"b\": hp.uniform(\"b\", 0, 1)\r\n        }\r\n\r\n    raw_log_dir = \"./ray_log\"\r\n    raw_log_name = \"example\"\r\n\r\n    algorithm = HyperOptSearch(search_space, metric=\"SCORE\", mode=\"max\", n_initial_points=1)\r\n\r\n\r\n    tuner = tune.Tuner(trainable,\r\n            tune_config = tune.TuneConfig(\r\n                num_samples = 10,\r\n                search_alg=algorithm,\r\n                ),\r\n            param_space=search_space,\r\n            run_config = air.RunConfig(local_dir = raw_log_dir, name = raw_log_name) #\r\n            )\r\n\r\n    print('!! ttt = ', input_param.ttt)\r\n    results = tuner.fit()\r\n    print(results.get_best_result(metric=\"SCORE\", mode=\"max\").config)\r\n\r\nI run the task via the following command:\r\n\r\n    py example.py --ttt 99\r\n\r\nThe following is part of the error:\r\n\r\n    (pid=19560) default_worker.py: error: the following arguments are required: --ttt\r\n    (pid=19560) 2022-11-23 20:45:01,769     ERROR worker.py:763 -- Worker exits with an exit code 2.\r\n\r\n\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/30662/comments",
    "author": "wxie2013",
    "comments": [
      {
        "user": "justinvyu",
        "created_at": "2022-11-28T17:56:06Z",
        "body": "Is it possible to work around this by passing the arguments into the config? Is there a specific reason why the arguments need to be stored and accessed in the trainable as a separate python module?\r\n\r\n```python\r\nsearch_space = {\r\n    # ...\r\n    \"ttt\": input_param.ttt,\r\n}\r\n```"
      },
      {
        "user": "Yard1",
        "created_at": "2022-11-29T18:45:27Z",
        "body": "Hey @wxie2013, as I mentioned in the discuss thread, this is because the trainable function is ran in a separate process on each Tune worker in parallel. Therefore, argparse will expect arguments that are simply not provided when Ray spawns those processes."
      }
    ]
  },
  {
    "number": 10416,
    "title": "Always getting .nan reward while training with PPO or DQN",
    "created_at": "2020-08-29T04:38:03Z",
    "closed_at": "2020-08-30T02:54:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10416",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Can anyone please give me hints why I am always getting the following while training with PPO or DQN?\r\nepisode_len_mean: .nan\r\nepisode_reward_max: .nan\r\nepisode_reward_mean: .nan\r\nepisode_reward_min: .nan\r\nepisodes_this_iter: 0\r\nepisodes_total: 0\r\n\r\nRay Version: 0.8.7\r\nOS: macOS\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10416/comments",
    "author": "ashutosh1906",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-08-29T07:15:57Z",
        "body": "`episodes_total: 0`. This is the reason. Until an episode has finished, we can't calculate any rewards. Does your env eventually return done=True at some point?"
      }
    ]
  },
  {
    "number": 9863,
    "title": "How to init Ray with a specified GPU id to run all trials of Tune?",
    "created_at": "2020-08-02T12:46:26Z",
    "closed_at": "2020-08-03T02:01:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9863",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\nSay, I have 4 GPUs with ids=[0, 1, 2, 3] and I only want to run all trials for Tune on id=2 and id=3 only. That means I can only maximize the use of the third and fourth GPU without touching the first two GPUs. How can I achieve this? \r\n\r\n```ray.init(num_cpus=num_cpus, num_gpus=num_gpus, temp_dir=ray_log)```\r\n\r\nThe attribute ```num_gpus``` is the number of GPUs ray can use. When setting ```num_gpus=1```, all the trials run on the first device (GPU id=0).  When increasing ```num_gpus```, all the trials will ordinally use GPUs from id=0 to id=3... I want to know how to specify the exact GPU ids, e.g., all trials run on id=2 and id=3.\r\n\r\nI've tried specifying GPU id in the training functions, but raised ```RuntimeError: CUDA error: invalid device ordinal```. \r\n\r\nI'm still new to this great project. Appreciate your warm help!\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nOS: Linux\r\nPython: 3.7.4\r\nRay: 0.8.6",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9863/comments",
    "author": "guoxuxu",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-08-02T17:24:52Z",
        "body": "Try setting the CUDA_VISIBLE_DEVICES before running the ray script?"
      },
      {
        "user": "ndvbd",
        "created_at": "2023-05-16T18:22:39Z",
        "body": "But is there a smarter way, to automatically choose the free gpus from the cluster?"
      }
    ]
  },
  {
    "number": 9309,
    "title": "[rllib] Cannot detect pybullet environments.",
    "created_at": "2020-07-06T00:56:25Z",
    "closed_at": "2020-07-06T03:43:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9309",
    "body": "### Pybullet Environments Cannot Be Detected By Ray/rllib\r\n\r\nHi, I'm trying to use rllib to train pybullet games. My environment is ray 0.8.4, ubuntu 16.04, Pytorch  1.2.0. It seems that ray cannot detect these games and said the game was not registered. But I can make the gym environment outside ray within the same script. I attached a simple code to show what's wrong. Could someone help with this? Thanks!!\r\n\r\n```\r\nimport ray\r\nfrom ray.rllib.agents.ppo import PPOTrainer\r\nfrom ray.tune.registry import register_env\r\nimport gym\r\nimport pybullet_envs\r\n\r\nenv = gym.make('HumanoidBulletEnv-v0')\r\nprint(\"Made Successfully\")\r\n\r\nclass MyEnv(gym.Env):\r\n    def __init__(self, env_config):\r\n        self.env = gym.make('HumanoidBulletEnv-v0')\r\n        self.action_space = self.env.action_space\r\n        self.observation_space = self.env.observation_space\r\n\r\n    def reset(self):\r\n        obs = self.env.reset()\r\n        return obs\r\n\r\n    def step(self, action):\r\n        action = self.action_space.high * action\r\n        obs, reward, done, info = self.env.step(action)\r\n        return obs, reward, done, info\r\n\r\nregister_env(\"myenv\", lambda config: MyEnv(config))\r\n\r\n\r\ndef main():\r\n    ray.init()    \r\n    trainer = PPOTrainer(env=\"myenv\", config={\r\n        \"use_pytorch\": True,\r\n        })\r\n\r\n    for i in range(100):\r\n        trainer.train()\r\n\r\n    trainer.stop()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\nWhen I run the code, the environment outside ray could be made successfully and 'Made Successfully' was printed. But then I get the error that \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"pybullet_train.py\", line 44, in <module>\r\n    main()\r\n  File \"pybullet_train.py\", line 39, in main\r\n    trainer.train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 502, in train\r\n    raise e\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 491, in train\r\n    result = Trainable.train(self)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 150, in _train\r\n    fetches = self.optimizer.step()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py\", line 59, in step\r\n    for e in self.workers.remote_workers()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/memory.py\", line 29, in ray_get_and_free\r\n    result = ray.get(object_ids)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1513, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(UnregisteredEnv): ray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 118, in spec\r\n    return self.env_specs[id]\r\nKeyError: 'HumanoidBulletEnv-v0'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 287, in __init__\r\n    self.env = _validate_env(env_creator(env_context))\r\n  File \"pybullet_train.py\", line 27, in <lambda>\r\n    register_env(\"myenv\", lambda config: MyEnv(config))\r\n  File \"pybullet_train.py\", line 14, in __init__\r\n    self.env = gym.make('HumanoidBulletEnv-v0')\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 142, in make\r\n    return registry.make(id, **kwargs)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 86, in make\r\n    spec = self.spec(path)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 128, in spec\r\n    raise error.UnregisteredEnv('No registered env with id: {}'.format(id))\r\ngym.error.UnregisteredEnv: No registered env with id: HumanoidBulletEnv-v0\r\n```\r\n\r\nI know 'import pybullet_envs' will register the environments in gym. It looked like rollout workers didn't detect these environments. Could someone tell me how to solve this? Thank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9309/comments",
    "author": "KarlXing",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-07-06T02:59:30Z",
        "body": "Can you try moving the import into the constructor for your class? The problem is the import only applies locally and not on the Ray workers."
      }
    ]
  },
  {
    "number": 7849,
    "title": "[rllib] Unable to configure exploration parameters in PPO: Unknown config parameter `explore`",
    "created_at": "2020-04-01T09:25:26Z",
    "closed_at": "2020-05-01T08:08:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7849",
    "body": "Hello,\r\n\r\nI am unable to configure exploration parameters when trying to create a PPO trainer. Dictionary entries \"explore\" and \"exploration_config\" is said to be unknown. Below are the relevant trainer definition and the traceback.\r\n\r\n`trainer = PPOTrainer(\r\n                      env=env_title,\r\n                      config={\r\n                          \r\n                        \"explore\": True,\r\n                        \"exploration_config\": {\r\n                          \"type\": \"EpsilonGreedy\",\r\n                          # Parameters for the Exploration class' constructor:\r\n                          # \"initial_epsilon\"=1.0,  # default is 1.0\r\n                          # \"final_epsilon\"=0.05,  # default is 0.05\r\n                          \"epsilon_timesteps\": max_steps,  # Timesteps over which to anneal epsilon, defult is int(1e5).\r\n                        },\r\n\r\n\r\n                        \"num_workers\": 5,\r\n                        \"num_gpus\": 2,\r\n                        \"model\": nw_model,\r\n                        \"multiagent\": {\r\n                          \"policy_graphs\": policy_graphs,\r\n                          \"policy_mapping_fn\": policy_mapping_fn,\r\n                          \"policies_to_train\": [\"ppo_policy{}\".format(i) for i in range(n_agents)],\r\n                        },\r\n                        \"callbacks\": {\r\n                          \"on_episode_start\": tune.function(on_episode_start),\r\n                          \"on_episode_step\": tune.function(on_episode_step),\r\n                          \"on_episode_end\": tune.function(on_episode_end),\r\n                        },\r\n                        \"log_level\": \"ERROR\",\r\n                      })`\r\n\r\n\r\nFull traceback:\r\n\r\n`Exception                                 Traceback (most recent call last)\r\n<ipython-input-9-252111d46b85> in <module>()\r\n    121                           \"on_episode_end\": tune.function(on_episode_end),\r\n    122                         },\r\n--> 123                         \"log_level\": \"ERROR\",\r\n    124                       })\r\n    125 \r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py in __init__(self, config, env, logger_creator)\r\n     88 \r\n     89         def __init__(self, config=None, env=None, logger_creator=None):\r\n---> 90             Trainer.__init__(self, config, env, logger_creator)\r\n     91 \r\n     92         def _init(self, config, env_creator):\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in __init__(self, config, env, logger_creator)\r\n    370             logger_creator = default_logger_creator\r\n    371 \r\n--> 372         Trainable.__init__(self, config, logger_creator)\r\n    373 \r\n    374     @classmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py in __init__(self, config, logger_creator)\r\n     94         self._restored = False\r\n     95         start_time = time.time()\r\n---> 96         self._setup(copy.deepcopy(self.config))\r\n     97         setup_time = time.time() - start_time\r\n     98         if setup_time > SETUP_TIME_THRESHOLD:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in _setup(self, config)\r\n    476         merged_config = deep_update(merged_config, config,\r\n    477                                     self._allow_unknown_configs,\r\n--> 478                                     self._allow_unknown_subkeys)\r\n    479         self.raw_user_config = config\r\n    480         self.config = merged_config\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/util.py in deep_update(original, new_dict, new_keys_allowed, whitelist)\r\n    158         if k not in original:\r\n    159             if not new_keys_allowed:\r\n--> 160                 raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n    161         if isinstance(original.get(k), dict):\r\n    162             if k in whitelist:\r\n\r\nException: Unknown config parameter `explore` `\r\n\r\n\r\n\r\nI am using Google Colab and Tensorflow 2.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7849/comments",
    "author": "ZekiDorukErden",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:39:56Z",
        "body": "Hi, you are probably on an older version of ray? What's your version number?\r\nFor now, try to remove these two keys (`exploration_config `and `explore`) altogether. You probably should not run PPO with EpsilonGreedy anyways."
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T09:52:13Z",
        "body": "Thanks for the reply! Apparently I am using version 0.8.0.dev5 (I copied the code block for dependencies in Ray with Google Colab tutorial without changing)."
      }
    ]
  },
  {
    "number": 7490,
    "title": "[rllib] why the RolloutWorker uses the default config  everytime",
    "created_at": "2020-03-06T17:57:35Z",
    "closed_at": "2020-03-07T03:58:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7490",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay: 0.8.2\r\nPython: 3.6\r\nTF: 2.0\r\nOS: macOS Catalina\r\n\r\nI  create some RolloutWorker instances in our customized training flow, you can run the code.\r\n\r\n```python\r\nimport argparse\r\n\r\nimport ray\r\nimport gym\r\nimport copy\r\nimport random\r\nimport numpy as np\r\n\r\nfrom ray import tune\r\nfrom ray.rllib.utils import try_import_tf\r\n\r\nfrom ray.rllib.models import ModelCatalog\r\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\r\n\r\nfrom ray.rllib.models.tf.tf_action_dist import Categorical\r\nfrom ray.rllib.agents.pg.pg import PGTFPolicy\r\n\r\nfrom ray.rllib.evaluation import RolloutWorker\r\nfrom ray.rllib.evaluation.metrics import collect_metrics\r\nfrom ray.rllib.policy.sample_batch import SampleBatch\r\nfrom ray.rllib.policy.tests.test_policy import TestPolicy\r\nfrom ray.rllib.policy.tf_policy import TFPolicy\r\n\r\nfrom ray.rllib.offline import NoopOutput, IOContext, OutputWriter, InputReader\r\n\r\nfrom ray.rllib.agents.trainer import with_common_config\r\n\r\nfrom ray.rllib.evaluation.postprocessing import Postprocessing, compute_advantages\r\nfrom ray.rllib.policy.tf_policy_template import build_tf_policy\r\n\r\n\r\nfrom ray.rllib.models.tf.misc import normc_initializer, get_activation_fn\r\n\r\n\r\ntf = try_import_tf()\r\n\r\n\r\nclass CustomCategorical(Categorical):\r\n    def __init__(self, inputs, model=None, temperature=1.0):\r\n        \"\"\" The inputs are action logits \"\"\"\r\n        super().__init__(inputs, model, temperature)\r\n        self.softmax = tf.nn.softmax(inputs)\r\n\r\n    def prob(self, actions):\r\n        _prob_given_action = tf.one_hot(actions, depth=self.softmax.get_shape().as_list()[-1]) * self.softmax\r\n        return tf.reduce_sum(_prob_given_action, axis=-1)\r\n\r\n\r\nclass DemoModel(TFModelV2):\r\n    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\r\n        super(DemoModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\r\n\r\n        self.loss_inputs = [\r\n            ('taken_actions', tf.placeholder(tf.int32, (None,))),\r\n            ('returns', tf.placeholder(tf.float32, (None,)))\r\n        ]\r\n        self.ph_obs_input = tf.placeholder(tf.float32, (None,)+obs_space.shape)\r\n\r\n        self.ph_all_inputs = {k: v for _, (k, v) in enumerate(self.loss_inputs + [(\"obs\", self.ph_obs_input)])}\r\n        self.ph_all_inputs['prev_actions'] = tf.placeholder(tf.int32, (None,))\r\n        self.ph_all_inputs['prev_rewards'] = tf.placeholder(tf.float32, (None,))\r\n\r\n        inputs = tf.keras.layers.Input(\r\n            shape=(np.product(obs_space.shape), ))\r\n\r\n        _layer_out = tf.keras.layers.Dense(\r\n            128,\r\n            activation=tf.nn.tanh,\r\n            kernel_initializer=normc_initializer(1.0))(inputs)\r\n\r\n        self._layer_out = tf.keras.layers.Dense(\r\n            num_outputs,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(1.0))(_layer_out)\r\n\r\n        self._value_out = tf.keras.layers.Dense(\r\n            1,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(0.01))(_layer_out)\r\n\r\n        self.action_dist = CustomCategorical(self._layer_out, None)\r\n        self.predicted_actions = self.action_dist.sample()\r\n\r\n        # self.model and self.base_model are different\r\n        self.model = tf.keras.Model(inputs, [self._layer_out, self._value_out])\r\n        self.register_variables(self.model.variables)\r\n\r\n    def forward(self, input_dict, state, seq_lens):\r\n        model_out, self._value_out = self.model(input_dict[\"obs_flat\"])\r\n        return model_out, state\r\n\r\n    def custom_loss(self, policy_loss, loss_inputs):\r\n        return policy_loss\r\n\r\n    def from_batch(self, train_batch, is_training=True):\r\n        \"\"\"Convenience function that calls this model with a tensor batch.\r\n\r\n        All this does is unpack the tensor batch to call this model with the\r\n        right input dict, state, and seq len arguments.\r\n        \"\"\"\r\n\r\n        input_dict = {\r\n            \"obs\": train_batch[SampleBatch.CUR_OBS],\r\n            \"is_training\": is_training,\r\n        }\r\n        if SampleBatch.PREV_ACTIONS in train_batch:\r\n            input_dict[\"prev_actions\"] = train_batch[SampleBatch.PREV_ACTIONS]\r\n        if SampleBatch.PREV_REWARDS in train_batch:\r\n            input_dict[\"prev_rewards\"] = train_batch[SampleBatch.PREV_REWARDS]\r\n        states = []\r\n        i = 0\r\n        while \"state_in_{}\".format(i) in train_batch:\r\n            states.append(train_batch[\"state_in_{}\".format(i)])\r\n            i += 1\r\n        return self.__call__(input_dict, states, train_batch.get(\"seq_lens\"))\r\n\r\n    def value_function(self):\r\n        return tf.reshape(self._value_out, [-1])\r\n\r\n    def metrics(self):\r\n        \"\"\"Override to return custom metrics from your model.\r\n\r\n        The stats will be reported as part of the learner stats, i.e.,\r\n            info:\r\n                learner:\r\n                    model:\r\n                        key1: metric1\r\n                        key2: metric2\r\n\r\n        Returns:\r\n            Dict of string keys to scalar tensors.\r\n        \"\"\"\r\n        return {}\r\n\r\n\r\ndef pg_tf_loss(policy, model, dist_class, train_batch):\r\n    \"\"\"The basic policy gradients loss.\"\"\"\r\n    logits, _ = model.from_batch(train_batch)\r\n    action_dist = dist_class(logits, model)\r\n    return -tf.reduce_mean(\r\n        action_dist.logp(train_batch[SampleBatch.ACTIONS]) * train_batch[SampleBatch.REWARDS])\r\n\r\n\r\n# def post_process_advantages(policy,\r\n#                             sample_batch,\r\n#                             other_agent_batches=None,\r\n#                             episode=None):\r\n#     \"\"\"This adds the \"advantages\" column to the sample train_batch.\"\"\"\r\n#     return compute_advantages(\r\n#         sample_batch,\r\n#         0.0,\r\n#         policy.config[\"gamma\"],\r\n#         use_gae=False,\r\n#         use_critic=False)\r\n\r\n\r\nDEFAULT_CONFIG = with_common_config({\r\n    # # No remote workers by default.\r\n    # \"num_workers\": 0,\r\n    # # Learning rate.\r\n    # \"lr\": 0.0004,\r\n})\r\n\r\n\r\nCustomPolicy = build_tf_policy(\r\n    name=\"CustomPolicy\",\r\n    get_default_config=lambda: DEFAULT_CONFIG,\r\n    # postprocess_fn=post_process_advantages,\r\n    loss_fn=pg_tf_loss)\r\n\r\n\r\ndef set_variables(policy: CustomPolicy):\r\n    policy._variables = ray.experimental.tf_utils.TensorFlowVariables(\r\n        [], policy._sess, policy.variables())\r\n\r\n\r\ndef training_workflow(config, reporter):\r\n    sess = tf.Session()\r\n    env = gym.make(\"CartPole-v0\")\r\n\r\n    # policy = CustomPolicy(observation_space=env.observation_space, action_space=env.action_space, config=config,\r\n    #                       sess=sess, model=model, loss_inputs=model.loss_inputs, loss='Not None',\r\n    #                       action_sampler=model.predicted_actions, obs_input=model.ph_obs_input)\r\n\r\n    conf = {'config': config, 'sess': sess, 'model': model, 'loss_inputs': model.loss_inputs, 'loss': 'Not None',\r\n            'action_sampler': model.predicted_actions, 'obs_input': model.ph_obs_input}\r\n\r\n    policy = CustomPolicy(env.observation_space, env.action_space,\r\n                          config=config)  # , existing_inputs=model.ph_all_inputs)#, existing_model=model)\r\n    set_variables(policy)\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n\r\n    for i in range(config[\"num_iters\"]):\r\n        # Broadcast weights to the policy evaluation workers\r\n        weights = ray.put({\"default_policy\": policy.get_weights()})\r\n        for w in workers:\r\n            w.set_weights.remote(weights)\r\n\r\n        # Gather a batch of samples\r\n        T1 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Update the remote policy replicas and gather another batch of samples\r\n        # new_value = policy.get_weights()\r\n        # for w in workers:\r\n        #     w.for_policy.remote(lambda p: p.update_some_value(new_value))\r\n\r\n        # Gather another batch of samples\r\n        T2 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Improve the policy using the T1 batch\r\n        policy.learn_on_batch(T1)\r\n\r\n        # Do some arbitrary updates based on the T2 batch\r\n        # print(f'iter: {i}, sum_rewards: {(sum(T2[\"rewards\"])):.2f}')\r\n\r\n        reporter(**collect_metrics(remote_workers=workers))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--gpu\", action=\"store_true\")\r\n    parser.add_argument(\"--num-iters\", type=int, default=3)\r\n    parser.add_argument(\"--num-workers\", type=int, default=1)\r\n    parser.add_argument(\"--num-cpus\", type=int, default=0)\r\n\r\n    args = parser.parse_args()\r\n    ray.init(num_cpus=args.num_cpus or None)\r\n    ModelCatalog.register_custom_model(\"demo_model\", DemoModel)\r\n\r\n    tune.run(\r\n        training_workflow,\r\n        # resources_per_trial={\r\n        #     \"gpu\": 1 if args.gpu else 0,\r\n        #     \"cpu\": 1,\r\n        #     \"extra_cpu\": args.num_workers,\r\n        # },\r\n        config={\r\n            \"num_workers\": args.num_workers,\r\n            \"num_iters\": args.num_iters,\r\n            \"lr\": 1e-3,\r\n            \"model\": {\r\n                \"custom_model\": \"demo_model\",\r\n                \"max_seq_len\": 20,\r\n                \"custom_options\": {\r\n                    \"activation\": tf.nn.tanh,\r\n                }\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nIn\r\n```\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n```\r\nThe RolloutWorker creates a instance by using the default config not the config passed into the workflow. \r\n\r\nAre there any methods to fix it?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7490/comments",
    "author": "GoingMyWay",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-06T21:56:48Z",
        "body": "You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers."
      }
    ]
  },
  {
    "number": 4505,
    "title": "Config Does Not Accept Custom Parameters",
    "created_at": "2019-03-29T02:31:36Z",
    "closed_at": "2019-03-29T08:21:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/4505",
    "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.5\r\n- **Python version**: 3.7\r\n- **Exact command to reproduce**: \r\n`run_experiments({\r\n        \"test\": {\r\n            \"run\": my_trainable_func,\r\n            \"env\": multienv_name,\r\n            \"config\": {\r\n                \"multiagent\": {\r\n                    \"policy_graphs\": policy_graphs,\r\n                    \"policy_mapping_fn\": tune.function(lambda agent_id: f'agent_{agent_id}'),\r\n                },\r\n                \"num_iters\": 5\r\n            },\r\n            \"resources_per_trial\": {\r\n                \"cpu\": 2,\r\n                \"gpu\": 1,\r\n            },\r\n        }\r\n    })`\r\n\r\n### Describe the problem\r\nI am trying to include custom config parameters which my_trainable_func uses, but seem unable to add anything because I get an unknown config parameter error. As per Issue #3160, @ericl has mentioned that many config parameters have been deprecated, but I'm curious to know what the intended way of adding algorithm-specific hyperparameters into the config is.\r\n\r\n### Source code / logs\r\n`Traceback (most recent call last):\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 90, in run\r\n(pid=426)     self._entrypoint()\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 141, in entrypoint\r\n(pid=426)     return self._trainable_func(config, self._status_reporter)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/function_runner.py\", line 249, in _trainable_func\r\n(pid=426)     output = train_func(config, reporter)\r\n(pid=426)   File \"<ipython-input-13-0b4744367540>\", line 103, in fed_train\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py\", line 276, in __init__\r\n(pid=426)     Trainable.__init__(self, config, logger_creator)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\", line 88, in __init__\r\n(pid=426)     self._setup(copy.deepcopy(self.config))\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/agent.py\", line 364, in _setup\r\n(pid=426)     self._allow_unknown_subkeys)\r\n(pid=426)   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/util.py\", line 89, in deep_update\r\n(pid=426)     raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n(pid=426) Exception: Unknown config parameter `num_iters` `\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/4505/comments",
    "author": "kiddyboots216",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-03-29T05:29:00Z",
        "body": "This is probably since you are subclassing agent? Agent checks that no unknown config keys are present, to avoid typos in your experiment config.\r\n\r\nIf you want to add a custom key, you can add it to the default config of your custom agent. You can check out any of the existing agent classes for an example of the config."
      },
      {
        "user": "kiddyboots216",
        "created_at": "2019-03-29T08:04:41Z",
        "body": "Ah, to be clear, I was wondering whether there was a way to do this without subclassing Agent ('my_trainable_func' I am trying to Tune a function and not a class). "
      },
      {
        "user": "ericl",
        "created_at": "2019-03-29T08:18:40Z",
        "body": "The exception you posted is originating from agent, so you must be calling agent code somehow."
      }
    ]
  },
  {
    "number": 3785,
    "title": "Tune doesn't work with multi agent env",
    "created_at": "2019-01-15T19:50:26Z",
    "closed_at": "2019-01-16T17:55:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3785",
    "body": "<!--\r\nGeneral questions should be asked on the mailing list ray-dev@googlegroups.com.\r\n\r\nBefore submitting an issue, please fill out the following form.\r\n-->\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.1\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.1\r\n- **Python version**: 3.6.7\r\n- **Exact command to reproduce**:\r\n\r\n\r\n<!--\r\nYou can obtain the Ray version with\r\n\r\npython -c \"import ray; print(ray.__version__)\"\r\n-->\r\n\r\n### Describe the problem\r\nI am trying to use Tune in combination with RLlib to train with a custom multi agent environment. It works, when I am just using RLlib. But when I try to train using Tune i get `RecursionError: maximum recursion depth exceeded`. Does Tune currently support multi agent environments? Please find my code and the full stack trace here:\r\n\r\n### Source code / logs\r\n```python3\r\nimport ray\r\nimport ray.rllib.agents.ppo as ppo\r\nimport ray.tune as tune\r\nimport ray.tune.schedulers\r\nfrom ray.tune.logger import pretty_print\r\nfrom ray.tune.registry import register_env\r\nimport beer_distribution_game\r\n\r\ndef env_creator(env_config):\r\n    import gym\r\n    import beer_distribution_game\r\n    return beer_distribution_game.BeerDistributionGameV0()\r\n\r\ndef policy_mapper(agent_id):\r\n    return agent_id\r\n\r\nray.init(redis_address='localhost:6379')\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nspace_env = beer_distribution_game.BeerDistributionGameV0()\r\nspaces = space_env.get_spaces()\r\nsingle_config = {\r\n            'model' : {\r\n                'conv_filters' : None,\r\n                'fcnet_activation' : 'relu',\r\n                'fcnet_hiddens': [50, 100, 100]\r\n            },\r\n    'gamma': 0.7\r\n}\r\n\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nconfig = {\r\n    'beer-game-tune': {\r\n        'run': 'PPO',\r\n        'env': 'SimpleBeerGame',\r\n        'stop': {'episode_reward_mean' : -2000},\r\n        'config': {\r\n            'multiagent': {\r\n                'policy_mapping_fn': policy_mapper,\r\n                'policy_graphs': {\r\n                    'manufactorer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['manufactorer']['observation_space'], spaces['manufactorer']['action_space'], single_config),\r\n                    'distributor':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['distributor']['observation_space'], spaces['distributor']['action_space'], single_config),\r\n                    'supplier':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['supplier']['observation_space'], spaces['supplier']['action_space'], single_config),\r\n                    'retailer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['retailer']['observation_space'], spaces['retailer']['action_space'], single_config)\r\n                    },\r\n                'policies_to_train': [\r\n                    'manufactorer', 'distributor', 'supplier', 'retailer']\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nscheduler = ray.tune.schedulers.AsyncHyperBandScheduler(time_attr='training_iteration', reward_attr='episode_reward_mean', max_t=100)\r\n\r\ntrials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n```\r\n\r\n\r\nThis is the stack trace:\r\n```\r\n== Status ==\r\nUsing AsyncHyperBand: num_stopped=0\r\nBracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None\r\nBracket: Iter 90.000: None | Iter 30.000: None\r\nBracket: Iter 90.000: None\r\nResources requested: 0/16 CPUs, 0/0 GPUs\r\nMemory usage on this node: 1.7/16.3 GB\r\n\r\nDeprecation warning: Function values are ambiguous in Tune configuations. Either wrap the function with `tune.function(func)` to specify a function literal, or `tune.sample_from(func)` to tell Tune to sample values from the function during variant generation: <function policy_mapper at 0x7fa09014a8c8>\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-6-6efdb03a13a1> in <module>\r\n----> 1 trials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run_experiments(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\r\n    106     last_debug = 0\r\n    107     while not runner.is_finished():\r\n--> 108         runner.step()\r\n    109         if time.time() - last_debug > DEBUG_PRINT_INTERVAL:\r\n    110             logger.info(runner.debug_string())\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in step(self)\r\n    112             raise TuneError(\"Called step when all trials finished?\")\r\n    113         self.trial_executor.on_step_begin()\r\n--> 114         next_trial = self._get_next_trial()\r\n    115         if next_trial is not None:\r\n    116             self.trial_executor.start_trial(next_trial)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _get_next_trial(self)\r\n    252         trials_done = all(trial.is_finished() for trial in self._trials)\r\n    253         wait_for_trial = trials_done and not self._search_alg.is_finished()\r\n--> 254         self._update_trial_queue(blocking=wait_for_trial)\r\n    255         trial = self._scheduler_alg.choose_trial_to_run(self)\r\n    256         return trial\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _update_trial_queue(self, blocking, timeout)\r\n    362             timeout (int): Seconds before blocking times out.\r\n    363         \"\"\"\r\n--> 364         trials = self._search_alg.next_trials()\r\n    365         if blocking and not trials:\r\n    366             start = time.time()\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in next_trials(self)\r\n     48             trials (list): Returns a list of trials.\r\n     49         \"\"\"\r\n---> 50         trials = list(self._trial_generator)\r\n     51         self._finished = True\r\n     52         return trials\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in _generate_trials(self, unresolved_spec, output_path)\r\n     67             raise TuneError(\"Must specify `run` in {}\".format(unresolved_spec))\r\n     68         for _ in range(unresolved_spec.get(\"num_samples\", 1)):\r\n---> 69             for resolved_vars, spec in generate_variants(unresolved_spec):\r\n     70                 experiment_tag = str(self._counter)\r\n     71                 if resolved_vars:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in generate_variants(unresolved_spec)\r\n     40         \"cpu\": {\"eval\": \"spec.config.num_workers\"}\r\n     41     \"\"\"\r\n---> 42     for resolved_vars, spec in _generate_variants(unresolved_spec):\r\n     43         assert not _unresolved_values(spec)\r\n     44         yield format_vars(resolved_vars), spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    138     for resolved_spec in grid_search:\r\n    139         resolved_vars = _resolve_lambda_vars(resolved_spec, lambda_vars)\r\n--> 140         for resolved, spec in _generate_variants(resolved_spec):\r\n    141             for path, value in grid_vars:\r\n    142                 resolved_vars[path] = _get_value(spec, path)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    121 def _generate_variants(spec):\r\n    122     spec = copy.deepcopy(spec)\r\n--> 123     unresolved = _unresolved_values(spec)\r\n    124     if not unresolved:\r\n    125         yield {}, spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\n... last 1 frames repeated, from the frame below ...\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nThis is the whole config:\r\n```python\r\n{'beer-game-tune': {'config': {'multiagent': {'policies_to_train': ['manufactorer',\r\n                                                                    'distributor',\r\n                                                                    'supplier',\r\n                                                                    'retailer'],\r\n                                              'policy_graphs': {'distributor': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                Box(4,),\r\n                                                                                Discrete(20),\r\n                                                                                {'gamma': 0.7,\r\n                                                                                 'model': {'conv_filters': None,\r\n                                                                                           'fcnet_activation': 'relu',\r\n                                                                                           'fcnet_hiddens': [50,\r\n                                                                                                             100,\r\n                                                                                                             100]}}),\r\n                                                                'manufactorer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                 Box(4,),\r\n                                                                                 Discrete(20),\r\n                                                                                 {'gamma': 0.7,\r\n                                                                                  'model': {'conv_filters': None,\r\n                                                                                            'fcnet_activation': 'relu',\r\n                                                                                            'fcnet_hiddens': [50,\r\n                                                                                                              100,\r\n                                                                                                              100]}}),\r\n                                                                'retailer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}}),\r\n                                                                'supplier': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}})},\r\n                                              'policy_mapping_fn': <function policy_mapper at 0x7f804434a1e0>}},\r\n                    'env': 'SimpleBeerGame',\r\n                    'run': 'PPO',\r\n                    'stop': {'episode_reward_mean': -2000}}}\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3785/comments",
    "author": "MariusDanner",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-01-16T02:06:59Z",
        "body": "The issue is that tune is trying to expand lambda functions to generate trial variants. To fix that, you can 'escape' the policy mapper function with tune.function(func).\r\n\r\nThis is an unfortunate gotcha of the tune API, we should eventually raise an error on raw functions passed in the config."
      }
    ]
  }
]