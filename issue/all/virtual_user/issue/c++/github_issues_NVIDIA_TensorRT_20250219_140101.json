[
  {
    "number": 1678,
    "title": "How to use the ONNX model with int8 calibration? ",
    "created_at": "2021-12-21T09:59:19Z",
    "closed_at": "2022-03-16T01:29:57Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1678",
    "body": "Hi,\r\n\r\nI am trying to following the example in PyTorch-Quantization Toolkit to do the int8 Quantization.\r\nHowever, after I manager to get the generated onnx file. I am not sure how to use this onnx file and use trtexec to complete the inference of python tensorrt.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1678/comments",
    "author": "liuzhuang1024",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-12-22T04:35:06Z",
        "body": "@lzmisscc , you can use \r\n\r\n        trtexec --onnx your_onnx_file --int8\r\n\r\nthanks"
      },
      {
        "user": "liuzhuang1024",
        "created_at": "2021-12-23T07:28:45Z",
        "body": "> @lzmisscc , you can use\r\n> \r\n> ```\r\n>     trtexec --onnx your_onnx_file --int8\r\n> ```\r\n> \r\n> thanks\r\n\r\nBut after I converted to int8, I used tensorrt for reasoning,  video memory did not decrease, only the speed decreased. What's the matter?\r\n\r\nthanks"
      },
      {
        "user": "ttyio",
        "created_at": "2021-12-28T02:54:27Z",
        "body": "> > @lzmisscc , you can use\r\n> > ```\r\n> >     trtexec --onnx your_onnx_file --int8\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > thanks\r\n> \r\n> But after I converted to int8, I used tensorrt for reasoning, video memory did not decrease, only the speed decreased. What's the matter?\r\n> \r\n> thanks\r\n\r\nFrom our previous experience, the most use of gpu memory come from the load of cudnn, cublas library. The weights and activations not contribute to the major of the memory consumption. "
      }
    ]
  },
  {
    "number": 1630,
    "title": "How to save the calibration.bin files?",
    "created_at": "2021-11-20T09:29:05Z",
    "closed_at": "2022-01-25T01:41:56Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1630",
    "body": "## Description\r\nCurrently, I'm trying to generate INT8 TRT engine with calibrations, like that\r\n`calibrator = Calibrator(data_loader=calib_data(), cache=\"identity-calib.cache\")\r\n    build_engine = EngineFromNetwork(\r\n        NetworkFromOnnxPath(\"identity.onnx\"), config=CreateConfig(int8=True, calibrator=calibrator)\r\n    )`\r\nBut I was really confused about the mechanisms:\r\n1. when was calibration performed? within the 'EngineFromNetwork' process? I tried to set break-point at calibration::get_batch(), but It did not works;\r\n2. How to get the calibration.bin files? I have tried to call the function of \"write_calibration_cache(self, cache)\",  But I don't know which 'cache' to pass in.\r\nPlease help me with these two problems, Thanks a lot.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1630/comments",
    "author": "xingyueye",
    "comments": [
      {
        "user": "pranavm-nvidia",
        "created_at": "2021-11-22T14:59:46Z",
        "body": "1. Yes, calibration happens when the engine is being built. `EngineFromNetwork` is lazily evaluated, so you need to call it to build the engine: `engine = build_engine()`. \r\nAlternatively, you can use the immediately evaluated variants (`snake_case` instead of `PascalCase`):\r\n```py\r\nengine = engine_from_network(NetworkFromOnnxPath(\"identity.onnx\"), \r\n                             config=CreateConfig(int8=True, calibrator=calibrator) )\r\n```\r\n\r\n2. The calibrator is an interface that's implemented by the user and called by TRT. So `write_calibration_cache` is not intended to be called by you; instead it will be called once TRT finishes calibrating. "
      },
      {
        "user": "pranavm-nvidia",
        "created_at": "2021-11-23T14:17:14Z",
        "body": "It should ideally be representative of your input data; e.g. a subset of the training data may work well"
      }
    ]
  },
  {
    "number": 1349,
    "title": "Do I have to create and run the engine on the same Nvidia Graphics Card?",
    "created_at": "2021-07-07T03:51:21Z",
    "closed_at": "2021-07-08T02:38:11Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1349",
    "body": "I generate tensorrt model (.trt) from pytorch model in machine A, and want to run the .trt file in machine B, (A and B have different  Graphics Cards). Is it allowed?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1349/comments",
    "author": "BChunlei",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-07-08T02:08:36Z",
        "body": "Hello @BChunlei , it is not allowed, some generated kernel is device dependent in the mode."
      }
    ]
  },
  {
    "number": 1255,
    "title": "Saving a context",
    "created_at": "2021-05-18T08:20:16Z",
    "closed_at": "2021-05-21T07:04:55Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1255",
    "body": "Hi,\r\n\r\nI am using your inference.py algorithm in the demo/BERT folder. To run inference, you need to load the engine to build the context as follows:\r\n\r\n  with open(args.engine, \"rb\") as f, \\\r\n      trt.Runtime(TRT_LOGGER) as runtime, \\\r\n      runtime.deserialize_cuda_engine(f.read()) as engine, \\\r\n      engine.create_execution_context() as context:\r\n\r\nIs it possible to save the context so that we don't have to load it every time we want to run inference? \r\nThank you for your answer!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1255/comments",
    "author": "fdlci",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-05-21T04:52:59Z",
        "body": "Hello @fdlci , you can reuse the context, no need to create it for every inference. Did you hit any error? thanks!"
      },
      {
        "user": "fdlci",
        "created_at": "2021-05-21T07:03:29Z",
        "body": "Yes I can run several inferences without loading the context again. I thought I couldn't do that as I was running inference.py on one example only and everytime I tried a new example I had to run the entire code and reload the context again.\r\nThank you for your answer!\r\n "
      }
    ]
  },
  {
    "number": 1184,
    "title": "A question about TensorRT cancel point and IExecutionContext",
    "created_at": "2021-04-12T16:14:55Z",
    "closed_at": "2021-04-14T03:20:46Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1184",
    "body": "Hello there, I am a developer of inference task serving system. We use TensorRT 6/TensorRT 7 as our inference execute framework. Due to soft realtime limitation, we sometimes need to cancel current context->execute() / context->executeV2() for next inference task running safely.\r\nI didn't find any solution on TensorRT documentation, can TensorRT development team gives me some advice of cancel context->execute()? My context->execute() is running on a single POSIX thread, can I cancel it safely? Or can you give me more information about TensorRT cancellation point? Thanks a lot!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1184/comments",
    "author": "KarKLi",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-04-14T02:08:06Z",
        "body": "Sorry @KarKLi , it is CUDA limitation that we cannot cancel the kernels that already enqueued. even for cudaDeviceReset, it will first flush the work that pending in the queue and wait for GPU idle first."
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T02:29:41Z",
        "body": "> Sorry @KarKLi , it is CUDA limitation that we cannot cancel the kernels that already enqueued. even for cudaDeviceReset, it will first flush the work that pending in the queue and wait for GPU idle first.\r\n\r\nThx. And I have another question that the IExecutionContext created by engine->CreateExecutionContext() / engine->CreateExecutionContextWithoutDeviceMemory() can be reused? The \"reused\" means I don't call ctx->destroy(), save the pointer and use it again for later inference with CUDA stream or just CUDA. Will the inference execute properly?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T02:36:58Z",
        "body": "Hello @KarKLi , yes the `IExecutionContext` can be reused. But do not call `IExecutionContext::enqueue()`  with 2 different cuda stream simultaneously.  This is because intermediate tensor is resource of `IExecutionContext`,  behavior of execute the same context simultaneously on 2 different stream is undefined."
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T02:40:58Z",
        "body": "> Hello @KarKLi , yes the `IExecutionContext` can be reused. But do not call `IExecutionContext::enqueue()` with 2 different cuda stream simultaneously. This is because intermediate tensor is resource of `IExecutionContext`, behavior of execute the same context simultaneously on 2 different stream is undefined.\r\n\r\nthanks for your reply! What if I create two ```IExecutionContext``` pointer by the same engine or different engines and call ```IExecutionContext::enqueue()``` / ```IExecutionContext::enqueueV2()``` with a same cuda stream, will it cause undefined behaviour?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T02:53:26Z",
        "body": "Hello @KarKLi , \r\ncases are valid:\r\n- ctx A and ctx B run on cuda stream A \r\n- ctx A run on cuda stream A and ctx B run on cuda stream B\r\n- ctx A run on cuda stream A, then run on stream B after waiting stream A finished\r\n\r\nonly invalid case:\r\n- ctx A run on cuda stream A, and run on stream B without event sync/wait"
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T03:00:23Z",
        "body": "> Hello @KarKLi ,\r\n> cases are valid:\r\n> \r\n> * ctx A and ctx B run on cuda stream A\r\n> * ctx A run on cuda stream A and ctx B run on cuda stream B\r\n> * ctx A run on cuda stream A, then run on stream B after waiting stream A finished\r\n> \r\n> only invalid case:\r\n> \r\n> * ctx A run on cuda stream A, and run on stream B without event sync/wait\r\n\r\nThanks! I have last question that can the ctx's execution memory be exposed to user by some kind of TensorRT API? If not, forget to record the device memory address when I call ```ctx->setDeviceMemory()``` will cause GPU memory leak?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T03:19:12Z",
        "body": "Helo @KarKLi , \r\ndo you mean activations when you say `execution memory`? activations are shared between contexts for the same engine.\r\ncurrently only the device memory is exposed and you can use `createExecutionContextWithoutDeviceMemory`/`setDeviceMemory` to set them, or use `createExecutionContext` to ask TRT to manage this part of memory. and yes there will be memory leak if you manage it but not proper released."
      }
    ]
  }
]