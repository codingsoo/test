[
  {
    "number": 72695,
    "title": "Does attach part support rollback if it fails?",
    "created_at": "2024-12-02T12:07:39Z",
    "closed_at": "2024-12-02T12:40:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/72695",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nIf ALTER TABLE %s ATTACH PART '%s' fails, is detach rollback supported?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/72695/comments",
    "author": "caicancai",
    "comments": [
      {
        "user": "rschu1ze",
        "created_at": "2024-12-02T12:10:04Z",
        "body": "Maybe I don't understand your question right but if the ATTACH operation fails, the part is not attached. Why should DETACH be rolled back?"
      }
    ]
  },
  {
    "number": 72530,
    "title": "Can i use a nested path in JSONExtractArrayRaw?",
    "created_at": "2024-11-27T05:09:54Z",
    "closed_at": "2024-11-27T14:46:44Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/72530",
    "body": "### Company or project name\r\n\r\n_No response_\r\n\r\n### Question\r\n\r\nClickHouse 24.10.3.21 running on x64 Debian\r\n\r\nI'm trying to extract, ideally as a ClickHouse array, values of a nested array from JSON. I can do it in quite a convoluted way:\r\n\r\n```\r\nSELECT JSONExtractArrayRaw(JSONExtract(JSONExtract('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'String'), 'details', 'String'), 'hobbies')\r\n\r\nQuery id: 2628ff82-8ad1-4669-98df-183700e03779\r\n\r\n   \u250c\u2500JSONExtractArrayRaw(JSONExtract(JSONExtract('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'String'), 'details', 'String'), 'hobbies')\u2500\u2510\r\n1. \u2502 ['{\"key\":1}','{\"key\":2}','{\"key\":3}']                                                                                                                                  \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThat's verbose. \r\n\r\nIs it possible to pass nested path to JSONExtractArrayRaw - something like _user.details.hobbies_?\r\n\r\nI know that the new JSON data type / functions are on their way, but I'd like to stick to stable functionality.\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/72530/comments",
    "author": "pakud",
    "comments": [
      {
        "user": "tiagoskaneta",
        "created_at": "2024-11-27T08:37:33Z",
        "body": "`JSONExtract*` all support multiple keys:\r\n\r\n```\r\nSELECT JSONExtractArrayRaw('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'details', 'hobbies')\r\n```\r\n\r\nYou can also use `JSON_VALUE` if you prefer:\r\n\r\n```\r\nSELECT JSON_VALUE('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', '$.user.details.hobbies') settings function_json_value_return_type_allow_complex=true\r\n```"
      }
    ]
  },
  {
    "number": 71340,
    "title": "clickhouse-keeper path gets deleted after ReplicatedMergeTree table recreation",
    "created_at": "2024-11-01T08:28:27Z",
    "closed_at": "2024-11-01T09:50:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/71340",
    "body": "### Company or project name\r\n\r\n_No response_\r\n\r\n### Question\r\nClickhouse Version: 24.9.1\r\nEnvironment:\r\n- ClickHouse cluster with 7 shards, 2 replicas each\r\n- One machine in shard 6 failed and was replaced\r\n\r\nSteps to reproduce:\r\n1. Replaced failed machine in shard 6\r\n2. Reconfigured the node and rejoined it to the cluster  (replica = 02)\r\n3. Recreated the table using ZooKeeper path: ``` (' /clickhouse/tables/7e5645d5-6728-4c27-ba8b-b96ba2dcb9bd/06','{replica}')```\r\n4. Table creation succeeded, Exec query ``` SELECT\r\n    database,\r\n    `table`,\r\n    is_leader,\r\n    is_readonly,\r\n    total_replicas,\r\n    active_replicas,\r\n    zookeeper_path,\r\n    queue_size,\r\n    inserts_in_queue,\r\n    merges_in_queue,\r\n    log_max_index,\r\n    log_pointer\r\nFROM system.replicas\r\nWHERE database = 'pro2_signoz_traces'  AND `table` = ''signoz_index_v2'') ```\r\n> total_replicas=2 active_replicas=2\r\n5. After a few minutes, the keeper path (/clickhouse/tables/7e5645d5-6728-4c27-ba8b-b96ba2dcb9bd/06/replicas/02) gets automatically deleted\r\n\r\n6. The table still exists in ClickHouse  \r\n> total_replicas=0 active_replicas=0\r\n\r\nQuestions:\r\n1. What could cause the ZooKeeper path to be automatically deleted while the table remains?\r\n2. How to properly recreate a replicated table after node failure?\r\nAny debug logs or configuration details that would be helpful in diagnosing this issue?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/71340/comments",
    "author": "youfu-fun",
    "comments": [
      {
        "user": "panzhilin007",
        "created_at": "2024-11-01T09:32:56Z",
        "body": "By default, ClickHouse will remove the zookeeeper path 480 seconds after you drop the table.\r\n\r\nTry to use `drop table tableName sync` instead.\r\n\r\n"
      }
    ]
  },
  {
    "number": 66925,
    "title": "mysql engine on_duplicate_clause usage",
    "created_at": "2024-07-23T10:29:10Z",
    "closed_at": "2024-07-24T11:34:08Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/66925",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nHi, I would like to ask why there are errors when using the MySQL engine to update data. I hope to receive your help\r\n\r\n```\r\nselect version();\r\n23.7.4.5\r\n```\r\n```\r\nCREATE NAMED COLLECTION mysql_test on cluster ck_cluster AS\r\n        host = 'xxx',\r\n        port = xxxx,\r\n        database = 'xxx',\r\n        user = 'xxx',\r\n        password = 'xxx' ,\r\nreplace_query = 0,\r\non_duplicate_clause = 1;\r\n```\r\n\r\n```\r\nCREATE TABLE test.test_01\r\n(\r\n    id UInt64 ,\r\n  package_name   Nullable(String) ,\r\n  app_name   Nullable(String) ,\r\n  source_app_name   Nullable(String) ,\r\n  update_time   Nullable(DateTime) ,\r\n  modify_time  Nullable(DateTime) \r\n) ENGINE = MySQL(mysql_test ,  table='cz_game_package_mapping')';\r\n```\r\n\r\n```\r\ninsert into test.test_01(id, package_name, app_name, source_app_name, update_time, modify_time)\r\nvalues( 775285052,'test_00',null, 'test_01', null,null) \r\nON DUPLICATE KEY UPDATE source_app_name = source_app_name;\r\n```\r\nerror message:\r\n`Code: 27. DB::ParsingException: Cannot parse input: expected '(' before: 'ON DUPLICATE KEY UPDATE source_app_name = source_app_name;':  at row 1: While executing ValuesBlockInputFormat: data for INSERT was parsed from query. (CANNOT_PARSE_INPUT_ASSERTION_FAILED)`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/66925/comments",
    "author": "12frame",
    "comments": [
      {
        "user": "pmusa",
        "created_at": "2024-07-23T20:49:03Z",
        "body": "Hey @12frame, it seems there is a bug when creating a table using named collections. When I try to create the table with `on_duplicate_clause=1`, clickhouse raises an error.\r\n\r\n```\r\nCREATE TABLE test_mysql3 (\r\n    id Int32,\r\n    name String\r\n) ENGINE = MySQL('localhost:3306', 'test', 'test', 'root', '', 0, 1);\r\n\r\nReceived exception:\r\nCode: 36. DB::Exception: Argument 'on_duplicate_clause' must be a literal with type String, got UInt64. (BAD_ARGUMENTS)\r\n ```\r\n\r\nBut indeed, if I do the same thing with named collections, no errors are raised.\r\n \r\nThe docs are not clear IMHO, but you need to define it as `on_duplicate_clause=UPDATE source_app_name = source_app_name`. And then, not pass anything when doing the insert: `insert into ... values ...;`\r\n\r\nBTW, your duplicate syntax didn't work as expected for me. You might need to look into MySQL correct syntax to update with the new value. It worked with a fixed string though: `on_duplicate_clause = 'UPDATE name=\\'duplicate\\''`."
      }
    ]
  },
  {
    "number": 59251,
    "title": "Port is not opened on keepers only using composable protocols",
    "created_at": "2024-01-26T08:54:26Z",
    "closed_at": "2024-01-26T10:31:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/59251",
    "body": "\r\n**Description**\r\nWhen using the composable protocols configuration for serving prometheus metrics with tls, clickhouse server with only keeper configuration does not open specified port.\r\n\r\n**How to reproduce**\r\n* Clickhouse version 23.3.19.32\r\n\r\nTo reproduce the issue two nodes should be deployed, one as \"data\" node and the other one as a keeper.\r\nExample config:\r\n```xml\r\n<remote_servers>\r\n<example>\r\n  <shard>\r\n    <internal_replication>true</internal_replication>\r\n    <replica>\r\n      <host>data-node.example.com</host>\r\n      <port>9000</port>\r\n      <secure>true</secure>\r\n    </replica>\r\n  </shard>\r\n</example>\r\n</remote_servers>\r\n\r\n<keeper_server>\r\n  <server_id>1</server_id>\r\n  <tcp_port_secure>2181</tcp_port_secure>\r\n  <raft_configuration>\r\n      <secure>true</secure>\r\n\r\n      <server>\r\n          <id>1</id>\r\n          <hostname>keeper-node.example.com</hostname>\r\n          <port>2182</port>\r\n      </server>\r\n  </raft_configuration>\r\n</keeper_server>\r\n\r\n\r\n```\r\n\r\nIn both configs I do specify prometheus metrics endpoint using composable protocols following explanation given in this issue #49003:\r\n```xml\r\n<prometheus>\r\n  <endpoint>/metrics</endpoint>\r\n  <metrics>true</metrics>\r\n  <events>true</events>\r\n  <asynchronous_metrics>true</asynchronous_metrics>\r\n</prometheus>\r\n\r\n<protocols>\r\n  <prometheus_protocol>\r\n      <type>prometheus</type>\r\n      <description>prometheus protocol</description>\r\n  </prometheus_protocol>\r\n\r\n  <prometheus_secure>\r\n      <type>tls</type>\r\n      <impl>prometheus_protocol</impl>\r\n      <port>9999</port>\r\n      <description>prometheus over tls</description>\r\n  </prometheus_secure>\r\n</protocols>\r\n\r\n```\r\n\r\n\r\n**Expected behavior**\r\nPort 9999 is opened on both \"data\" node and keeper node and is under TLS.\r\n\r\n**Actual behavior**\r\nPort 9999 is only opened on the \"data\" and not the keeper node.\r\n\r\n**Logs**\r\nThere were no messages in log with a mention of this prometheus configuration, checked it on trace level.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/59251/comments",
    "author": "ilyatrefilov",
    "comments": [
      {
        "user": "antonio2368",
        "created_at": "2024-01-26T09:51:44Z",
        "body": "Keeper currently doesn't support composable protocols so the \"old\" way should be used of specifying `<port>` under `<prometheus>`."
      },
      {
        "user": "antonio2368",
        "created_at": "2024-01-26T10:31:15Z",
        "body": "@ilyatrefilov I can't promise you anything specific but I think it should be done in the near future along with some small changes to Keeper prometheus metrics (send only Keeper specific data).\r\n\r\nI'll close this issue for now and ping you when there is update for it."
      }
    ]
  },
  {
    "number": 57382,
    "title": "Distributed queries is giving inconsistency output between replicas and shards",
    "created_at": "2023-11-30T10:02:08Z",
    "closed_at": "2023-12-05T12:23:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/57382",
    "body": "I have 3 shards and 2 replicas of clickhouse cluster and also zookeeper with 3 znodes for replication in my eks cluster. In that, I created a database called \"app\" with the replicated engine in all shards and replicas (i.e., in all 6 pods). After I created a mergeTree table called \"alerts_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"alerts\" for inserting and retrieving the data evenly in all pods.\r\n\r\nClickhouse config:\r\n`\r\n\r\n```\r\napiVersion: \"clickhouse.altinity.com/v1\"\r\nkind: \"ClickHouseInstallation\"\r\nmetadata:\r\n  name: \"clickhouse\"\r\nspec:\r\n  defaults:\r\n    replicasUseFQDN: \"no\"\r\n    distributedDDL:\r\n      profile: admin_profile\r\n    templates:\r\n      podTemplate: clickhouse-pod\r\n      dataVolumeClaimTemplate: clickhouse-storage\r\n      serviceTemplate: clickhouse-svc\r\n  configuration:\r\n    users:\r\n      default/networks/host_regexp: clickhouse.svc.cluster.local$\r\n      default/networks/ip:\r\n        - 127.0.0.1\r\n        - 0.0.0.0\r\n      admin/networks/host_regexp: clickhouse.svc.cluster.local$\r\n      admin/networks/ip:\r\n        - 127.0.0.1\r\n        - 0.0.0.0\r\n      admin/k8s_secret_password: clickhouse/clickhouse-operator-altinity-clickhouse-operator/password\r\n      admin/access_management: 1\r\n      admin/named_collection_control: 1\r\n      admin/show_named_collections: 1\r\n      admin/show_named_collections_secrets: 1\r\n      admin/profile: admin_profile\r\n      admin/quote: admin_quote\r\n    profiles:\r\n      admin_profile/max_memory_usage: 600000000000\r\n      admin_profile/readonly: 0\r\n      admin_profile/max_insert_threads: 32\r\n    quotas:\r\n      admin_quota/interval/duration: 3600\r\n      admin_quota/interval/queries: 0\r\n      admin_quota/interval/errors: 0\r\n      admin_quota/interval/result_rows: 0\r\n      admin_quota/interval/read_rows: 0\r\n      admin_quota/interval/execution_time: 0\r\n    settings:\r\n      disable_internal_dns_cache: 1\r\n    files:\r\n      conf.d/chop-generated-storage.xml: |\r\n        <yandex>\r\n            <storage_configuration>\r\n                <disks>\r\n                    <default>\r\n                        <metadata_path>/var/lib/clickhouse/</metadata_path>\r\n                    </default>\r\n                    <fast_ssd>\r\n                        <path>/mnt/fast_ssd/clickhouse/</path>\r\n                    </fast_ssd>\r\n                    <s3>\r\n                        <type>s3</type>\r\n                        <endpoint>S3 URL</endpoint>\r\n                        <use_environment_credentials>true</use_environment_credentials>\r\n                        <metadata_path>/var/lib/clickhouse/disks/s3/</metadata_path>\r\n                    </s3>\r\n                </disks>\r\n                <policies>\r\n                    <moving_from_hot_to_cold>\r\n                        <volumes>\r\n                            <default>\r\n                                <disk>default</disk>\r\n                            </default>\r\n                            <hot>\r\n                                <disk>fast_ssd</disk>\r\n                            </hot>\r\n                            <cold>\r\n                                <disk>s3</disk>\r\n                            </cold>\r\n                        </volumes>\r\n                    </moving_from_hot_to_cold>\r\n            \t</policies>\r\n            </storage_configuration>\r\n        </yandex>\r\n    zookeeper:\r\n      nodes:\r\n        - host: zookeeper-0.zookeepers.zoo-keeper\r\n          port: 2181\r\n        - host: zookeeper-1.zookeepers.zoo-keeper\r\n          port: 2181\r\n        - host: zookeeper-2.zookeepers.zoo-keeper\r\n          port: 2181\r\n    clusters:\r\n      - name: prod-cluster\r\n        templates:\r\n          podTemplate: clickhouse-pod\r\n          dataVolumeClaimTemplate: clickhouse-storage\r\n        layout:\r\n          shardsCount: 3\r\n          replicasCount: 2\r\n          shards:\r\n            - name: shard0\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n            - name: shard1\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n            - name: shard2\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n  templates:\r\n    serviceTemplates:\r\n      - name: clickhouse-svc\r\n        spec:\r\n          ports:\r\n            - name: http\r\n              port: 8123\r\n            - name: tcp\r\n              port: 9000\r\n          externalTrafficPolicy: Cluster\r\n          type: LoadBalancer\r\n    podTemplates:\r\n      - name: clickhouse-pod\r\n        spec:\r\n          containers:\r\n            - name: clickhouse-pod\r\n              image: clickhouse/clickhouse-server:23.7\r\n              resources:\r\n                limits:\r\n                  cpu: \"3\"\r\n                  memory: 8Gi\r\n                requests:\r\n                  cpu: \"2\"\r\n                  memory: 6Gi\r\n              volumeMounts:\r\n                - name: clickhouse-storage\r\n                  mountPath: /var/lib/clickhouse\r\n    volumeClaimTemplates:\r\n      - name: clickhouse-storage\r\n        spec:\r\n          storageClassName: default\r\n          accessModes:\r\n            - ReadWriteOnce\r\n          resources:\r\n            requests:\r\n              storage: 100Gi\r\n```\r\n\r\n`\r\n              \r\n I have inserted million of data in \"alerts\" table in one month. But when I try select number of count() in alerts table where source='high', it giving inconsistency output in replicas and shards\r\n \r\n In 3 pods, chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0 it giving below output\r\n \r\n```\r\n SELECT count()\r\nFROM alerts\r\nWHERE source = 'high'\r\n\r\nQuery id: 81a6ed42-5583-4b9d-957b-9a69bccbac40\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    1389 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.232 sec. Processed 56.50 million rows, 1.02 GB (243.85 million rows/s., 4.39 GB/s.)\r\n```\r\n\r\nIn 3 pods, chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0, it giving below output\r\n\r\n```\r\nSELECT count()\r\nFROM alerts\r\nWHERE source = 'high'\r\n\r\nQuery id: 4c07a903-7ad6-4c92-bbc2-1ac86dcf8691\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    1489 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.216 sec. Processed 56.96 million rows, 1.03 GB (263.13 million rows/s., 4.74 GB/s.)\r\n```\r\n\r\n\r\nWhy there is inconsistency in output between shards and replicas? Can anyone from clickhouse help here.. what might be cause of this issue?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/57382/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-11-30T17:05:50Z",
        "body": ">mergeTree table called \"alerts_storage\"\r\n\r\nMergeTree or ReplicatedMergeTree?\r\nProvide `create table alerts_storage` `create table alerts`"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-01T05:03:39Z",
        "body": "@den-crane MergeTree Table. Identified the cause. I checked my alerts data by date wise and found out that this inconsistency in output occurs at 05/11/2023. Before 05/11/2023, the distributed data is giving correct output in both shard and replica. At 05/11/2023, I have restarted zookeeper pod to reduce its memory and cpu limit. That were problem occurs. After restarting the zookeeper, this inconsistency is starts to happen. Is there any way solve this one?"
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-03T09:43:48Z",
        "body": "Please reset your \"replicasCount: 2\" to \"replicasCount: 1\" if you use MergeTree table engine and query them from a distributed table. Multiple replicas only suitable for Replicated* tables querying from distributed tables consistently."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-03T12:53:05Z",
        "body": "@lampjian If I reset the replicasCount to 1, what will happen to the data in 2nd replicas? Will those data will be moved to shards or deleted? "
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-03T16:46:50Z",
        "body": "The data in 2nd replica still stores somewhere you can not query anymore. It seems to be deleted. You shall transfer the data to 1st replica before you remove the other replica."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-04T04:56:34Z",
        "body": "@lampjian  Okay. I have one more doubt. When during resetting the replicaCount to 1 for 2nd replicas, the shards(chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) and it's pvc storage won't be affected, right? They will remains as it is, right? Or will the pvc storage of above shards will be recreated?"
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-04T14:08:13Z",
        "body": "Probably unchanged, and your local and remote S3 storage configurations are all alike. But you shall merge the data to one replica and backup data first of all, your pod instances may run on different machines after restarting the cluster. Try to test all of the operations as much as possible in a similar smaller environment."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-04T18:03:34Z",
        "body": "@lampjian Tested in dev environment from the sctrach. Found out that this is not zookeeper problem. From my understanding, it's how the clickhouse actually works with my current settings(shardsCount - 3, replicasCount - 2). \r\n        \r\nIn dev enironment, I created a database called \"app\" with the replicated engine in all shards and replicas. After I created a mergeTree table called \"num_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"num\" for inserting and retrieving the data evenly.\r\n        \r\nWhen I inserted 1 to 1000 numbers into the pod chi-clickhouse-prod-cluster-shard0-0-0, the data are distributed evenly inserted across the three shards (chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) only and in its 2nd replicas which is came as independent pod, the data are not inserted.\r\n\r\n\r\n```\r\nchi-clickhouse-prod-cluster-shard0-0-0.chi-clickhouse-prod-cluster-shard0-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 997bd316-c646-49d3-8454-6fea1f7d6e5d\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     351 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard1-0-0.chi-clickhouse-prod-cluster-shard1-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: c00fc7d2-359d-462e-9251-4b953eb0698a\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     315 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard2-0-0.chi-clickhouse-prod-cluster-shard2-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 2fbdbe2a-3728-4d7b-a95c-34f2d3c3c8d7\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     334 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n```\r\n \r\n\r\nWhen I inserted 1 to 10 numbers into pod chi-clickhouse-prod-cluster-shard0-1-0(which is a 2nd replicas of shard0-0), it data is evenly inserted across the 2nd replicas of shards(chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0).\r\n        \r\n ```\r\nchi-clickhouse-prod-cluster-shard0-1-0.chi-clickhouse-prod-cluster-shard0-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 07eb979c-6348-4b6b-8870-c6faa60630e2\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       4 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard1-1-0.chi-clickhouse-prod-cluster-shard1-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 2022a66e-6188-4443-b36c-be685658250c\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard2-1-0.chi-clickhouse-prod-cluster-shard2-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 00f53697-ae60-4e14-8c21-7f1809db002c\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\n```\r\n\r\n\r\nSo In conclusion, When we have clickhouse with 3 shards and 2 replicas count and it's use MergeTree table engine and query them from a distributed table on a replicated database, the clickhouse will consider the shards(chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) as one cluster and its 2nd replicas which came as independent pod(chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0) as a another cluster. \r\n\r\n\r\nIs my conclusion is right? If yes, Could you please tell me what this behaviour of clickhouse is called?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-04T18:24:56Z",
        "body": "@Ragavendra-Vigneshwaran-R \r\n\r\n>database called \"app\" with the replicated engine \r\n\r\nreplicated database does not replicate data. It synchronizes the structures of the tables.\r\nYou need to use replicated **tables**.\r\n"
      }
    ]
  },
  {
    "number": 56017,
    "title": "Aggregated table contains more records then MergeTree table",
    "created_at": "2023-10-25T13:03:44Z",
    "closed_at": "2023-10-26T07:26:45Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/56017",
    "body": "When inserting data into distributed table, from which later materialized view inserts data into aggregated table, in end table there are increased amount of records comparing to data in initial table. When data is inserted into local MergeTree table the amount of records matches the one in aggregated table.\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nThe amount of records is bigger in aggregated table than it is in a base table if you insert data using distributed table. Present in latest  lts version 23.8.2.7. On an outdated version 22.8.14.53 such issue is not present.\r\n\r\n**How to reproduce**\r\nUse version 23.8. Run following scripts to create data structure\r\n\r\n```\r\nCREATE DATABASE IF NOT EXISTS cdp_metrics_distr_no_rep ON CLUSTER 'cluster_cross_r1';\r\n\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.delays_metrics_local ON CLUSTER 'cluster_cross_r1' (\r\n                                                                                                    MetricName String,\r\n                                                                                                    Tags String,\r\n                                                                                                    DateTimeStartPeriod DateTime,\r\n                                                                                                    DelayMs UInt64,\r\n                                                                                                    SlaMs UInt64\r\n)\r\n    ENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/{cluster_name}/tables/{shard}/cdp_metrics_distr_no_rep.delays_metrics_local_1', '{replica}')\r\n        PARTITION BY toYYYYMMDD(DateTimeStartPeriod)\r\n        ORDER BY (\r\n                  MetricName,\r\n                  Tags,\r\n                  DateTimeStartPeriod\r\n            )\r\n        TTL DateTimeStartPeriod + INTERVAL 7 DAY\r\n        SETTINGS ttl_only_drop_parts = 1;\r\n\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.delays_metrics ON CLUSTER 'cluster_cross_r1' AS cdp_metrics_distr_no_rep.delays_metrics_local\r\nENGINE = Distributed(cluster_cross_r1, cdp_metrics_distr_no_rep, delays_metrics_local, cityHash64(MetricName, Tags, toString(DateTimeStartPeriod)));\r\n\r\n\r\n--aggregated table for overall delays metrics\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.overall_delays_metrics_local ON CLUSTER 'cluster_cross_r1' (\r\n        MetricName String,\r\n        Tags String,\r\n        DateTimeStartPeriod DateTime,\r\n        Count AggregateFunction(count, UInt64),\r\n        Quantiles AggregateFunction(quantiles(0.5,0.75,0.95,0.99), UInt64)\r\n)\r\n    ENGINE = ReplicatedAggregatingMergeTree(ENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/{cluster_name}/tables/{shard}/cdp_metrics_distr_no_rep.overall_delays_metrics_1', '{replica}')\r\n        PARTITION BY toYYYYMM(DateTimeStartPeriod)\r\n        ORDER BY (\r\n                  MetricName,\r\n                  Tags,\r\n                  DateTimeStartPeriod\r\n            )\r\n        TTL DateTimeStartPeriod + INTERVAL 60 DAY\r\n        SETTINGS ttl_only_drop_parts = 1;\r\n\r\nCREATE TABLE IF NOT EXISTS cdp_metrics_distr_no_rep.overall_delays_metrics ON CLUSTER 'cluster_cross_r1' AS cdp_metrics_distr_no_rep.overall_delays_metrics_local\r\nENGINE = Distributed(cluster_cross_r1, cdp_metrics_distr_no_rep, overall_delays_metrics_local);\r\n\r\n--materialized view for overall delays metrics\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS cdp_metrics_distr_no_rep.overall_delays_metrics_local_mv ON CLUSTER 'cluster_cross_r1'\r\n        TO cdp_metrics_distr_no_rep.overall_delays_metrics_local\r\nAS SELECT\r\n       MetricName,\r\n       Tags,\r\n       DateTimeStartPeriod,\r\n       countState(DelayMs) AS Count,\r\n       quantilesState(0.5,0.75,0.95,0.99)(DelayMs) AS Quantiles\r\n   FROM cdp_metrics_distr_no_rep.delays_metrics_local\r\n   GROUP BY MetricName, Tags, DateTimeStartPeriod;\r\n```\r\n\r\n\r\n\r\nInsert data\r\n\r\n```\r\nINSERT INTO cdp_metrics_distr_no_rep.delays_metrics VALUES\r\n                                                 ('event_processing_delay', '{\"target\":\"flow_trigger\"}', '2023-10-28 00:00:00', 1272, 20000),\r\n                                                 ('event_processing_delay','{\"target\":\"flow_trigger\"}','2023-10-28 00:00:00',1431,20000),\r\n                                                 ('event_processing_delay', '{\"target\":\"flow_trigger\"}', '2023-10-28 00:00:00', 1222, 20000),\r\n                                                 ('event_processing_delay','{\"target\":\"flow_trigger\"}','2023-10-28 00:00:00',1435,20000),\r\n                                                 ('event_processing_delay', '{\"target\":\"flow_trigger\"}', '2023-10-28 00:00:00', 1242, 20000),\r\n                                                 ('event_processing_delay','{\"target\":\"flow_trigger\"}','2023-10-28 00:00:00',1461,20000)\r\n\r\n```\r\n\r\nCompare data in aggregated table and base table\r\n```\r\nSELECT MetricName, Tags, DateTimeStartPeriod, countMerge(Count), quantilesMerge(0.5,0.75,0.95,0.99)(Quantiles)\r\nfrom cdp_metrics_distr_no_rep.overall_delays_metrics\r\nwhere DateTimeStartPeriod >= '2023-10-28 00:00:00'\r\nGROUP BY MetricName,Tags,DateTimeStartPeriod;\r\n\r\nSELECT  Tags, count()\r\nfrom cdp_metrics_distr_no_rep.delays_metrics\r\nwhere DateTimeStartPeriod >= '2023-10-28 00:00:00'\r\nGROUP BY Tags\r\n```\r\n\r\n**Expected behavior**\r\n\r\nAmount of records in aggregated table and base table match.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/56017/comments",
    "author": "nikola2188",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-10-25T13:46:43Z",
        "body": "What is `cluster_cross_r1` ? What is the configuration of your cluster? provide `<remote_servers>`"
      },
      {
        "user": "den-crane",
        "created_at": "2023-10-25T13:48:36Z",
        "body": "please fix the issues like\r\n\r\n```\r\n    ENGINE = ReplicatedAggregatingMergeTree(ENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/{cluster_name}/tables/{shard}/cdp_metrics_distr_no_rep.overall_delays_metrics_1', '{replica}')\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-10-25T13:59:00Z",
        "body": "Check `<internal_replication>true</internal_replication>`, it must be true in `remote_servers`"
      }
    ]
  },
  {
    "number": 55231,
    "title": "Clickhouse user configuration",
    "created_at": "2023-10-04T11:03:25Z",
    "closed_at": "2023-10-05T01:55:26Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/55231",
    "body": "I have installed Clickhouse with 3 shards and 2 replicas in eks cluster. The user setting are in /etc/clickhouse-server/users.d/ folder in the clickhouse pod. In that folder, I have added the admin users setting. \r\n\r\nProblem description : \r\n\r\nWhen connected to the clickhouse in local( inside the clickhouse pod) with admin user and credentials, it throws the below exception \r\n\r\nException : \r\ncommand used --> clickhouse-client -u admin --password '****'\r\nClickHouse client version 23.7.5.30 (official build).\r\nConnecting to localhost:9000 as user admin.\r\nCode: 516. DB::Exception: Received from localhost:9000. DB::Exception: admin: Authentication failed: password is incorrect, or there is no user with such name.. (AUTHENTICATION_FAILED)\r\n\r\nBut when I connect to the clickhouse through the service URL as host with admin user credentials, it getting connected to clickhouse.\r\n\r\ncommand used --> clickhouse-client -h clickhouse-clickhouse -u admin --password '****'\r\nClickHouse client version 23.7.5.30 (official build).\r\nConnecting to clickhouse-clickhouse:9000 as user admin.\r\nConnected to ClickHouse server version 23.7.5 revision 54465.\r\nchi-clickhouse-prod-cluster-shard0-0-0.chi-clickhouse-prod-cluster-shard0-0.clickhouse.svc.cluster.local :)\r\n\r\n\r\nDoes this mean the admin user setting have to be present in under /etc/clickhouse-server/users.d/ folder and also be present in /etc/clickhouse-server/users.xml file in clickhouse?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/55231/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-10-04T12:31:46Z",
        "body": "it means that you misconfigured `<network>` section and disallowed to connect from a localhost.\r\n\r\nplease share your configs / hide passwords with ****\r\n\r\n"
      }
    ]
  },
  {
    "number": 52516,
    "title": "Possible to connect to secure Keeper with clickhouse-keeper-client?",
    "created_at": "2023-07-24T09:53:55Z",
    "closed_at": "2023-08-03T16:34:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/52516",
    "body": "My Keeper is secured and runs on port 9281. Version 23.6.2.18-stable.\r\n```bash\r\nlsof -nPi | grep LISTEN | grep clickhouse\r\nclickhous 3253968 clickhouse   37u  IPv6 14940597      0t0  TCP *:9444 (LISTEN)\r\nclickhous 3253968 clickhouse   38u  IPv4 14940601      0t0  TCP 10.100.116.91:9281 (LISTEN)\r\n```\r\nConnecting to unsecured Keeper worked fine (before I enabled SSL). \r\n```bash\r\n[root@bdtpsqlfdw01 ~]# clickhouse keeper-client -h bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9181 --query=\"ls /\"\r\nkeeper\r\n```\r\nI tried to connect with clickhouse-keeper-client to secured cluster but it doesn't work.\r\n```bash\r\n[root@bdtpsqlfdw01 clickhouse-keeper]# clickhouse keeper-client -h bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9281 --query=\"ls /\"\r\nCoordination::Exception: Connection loss, path: All connection tries failed while connecting to ZooKeeper. nodes: 10.100.116.91:9281\r\nCode: 999. Coordination::Exception: Unexpected handshake length received: 352518400 (Marshalling error): while receiving handshake from ZooKeeper. (KEEPER_EXCEPTION) (version 23.6.2.18 (official build)), 10.100.116.91:9281\r\nCode: 999. Coordination::Exception: Unexpected handshake length received: 352518400 (Marshalling error): while receiving handshake from ZooKeeper. (KEEPER_EXCEPTION) (version 23.6.2.18 (official build)), 10.100.116.91:9281\r\nCode: 999. Coordination::Exception: Unexpected handshake length received: 352518400 (Marshalling error): while receiving handshake from ZooKeeper. (KEEPER_EXCEPTION) (version 23.6.2.18 (official build)), 10.100.116.91:9281\r\n```\r\nThis is from /var/log/clickhouse-keeper/clickhouse-keeper.log\r\n```bash\r\n2023.07.24 11:49:59.320064 [ 3253971 ] {} <Error> ServerErrorHandler: Code: 210. DB::NetException: SSL Exception: error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER, while reading from socket (10.100.116.91:42634). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000000fd2eb5 in /usr/bin/clickhouse-keeper\r\n1. ? @ 0x0000000001093967 in /usr/bin/clickhouse-keeper\r\n2. DB::ReadBufferFromPocoSocket::nextImpl() @ 0x0000000001093583 in /usr/bin/clickhouse-keeper\r\n3. DB::KeeperTCPHandler::runImpl() @ 0x0000000000c35245 in /usr/bin/clickhouse-keeper\r\n4. Poco::Net::TCPServerConnection::start() @ 0x0000000001335f34 in /usr/bin/clickhouse-keeper\r\n5. Poco::Net::TCPServerDispatcher::run() @ 0x00000000013371b1 in /usr/bin/clickhouse-keeper\r\n6. Poco::PooledThread::run() @ 0x000000000146d807 in /usr/bin/clickhouse-keeper\r\n7. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000000146b1dc in /usr/bin/clickhouse-keeper\r\n (version 23.6.2.18 (official build))\r\n2023.07.24 11:49:59.322919 [ 3253972 ] {} <Error> ServerErrorHandler: Code: 210. DB::NetException: SSL Exception: error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER, while reading from socket (10.100.116.91:42614). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000000fd2eb5 in /usr/bin/clickhouse-keeper\r\n1. ? @ 0x0000000001093967 in /usr/bin/clickhouse-keeper\r\n2. DB::ReadBufferFromPocoSocket::nextImpl() @ 0x0000000001093583 in /usr/bin/clickhouse-keeper\r\n3. DB::KeeperTCPHandler::runImpl() @ 0x0000000000c35245 in /usr/bin/clickhouse-keeper\r\n4. Poco::Net::TCPServerConnection::start() @ 0x0000000001335f34 in /usr/bin/clickhouse-keeper\r\n5. Poco::Net::TCPServerDispatcher::run() @ 0x00000000013371b1 in /usr/bin/clickhouse-keeper\r\n6. Poco::PooledThread::run() @ 0x000000000146d807 in /usr/bin/clickhouse-keeper\r\n7. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000000146b1dc in /usr/bin/clickhouse-keeper\r\n (version 23.6.2.18 (official build))\r\n2023.07.24 11:49:59.323113 [ 3253970 ] {} <Error> ServerErrorHandler: Code: 210. DB::NetException: SSL Exception: error:100000f7:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER, while reading from socket (10.100.116.91:42628). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x0000000000fd2eb5 in /usr/bin/clickhouse-keeper\r\n1. ? @ 0x0000000001093967 in /usr/bin/clickhouse-keeper\r\n2. DB::ReadBufferFromPocoSocket::nextImpl() @ 0x0000000001093583 in /usr/bin/clickhouse-keeper\r\n3. DB::KeeperTCPHandler::runImpl() @ 0x0000000000c35245 in /usr/bin/clickhouse-keeper\r\n4. Poco::Net::TCPServerConnection::start() @ 0x0000000001335f34 in /usr/bin/clickhouse-keeper\r\n5. Poco::Net::TCPServerDispatcher::run() @ 0x00000000013371b1 in /usr/bin/clickhouse-keeper\r\n6. Poco::PooledThread::run() @ 0x000000000146d807 in /usr/bin/clickhouse-keeper\r\n7. Poco::ThreadImpl::runnableEntry(void*) @ 0x000000000146b1dc in /usr/bin/clickhouse-keeper\r\n (version 23.6.2.18 (official build))\r\n```\r\nConnecting through zkCli works:\r\n```bash\r\n[root@bdtpsqlfdw01 clickhouse-keeper]# /usr/share/zookeeper/bin/zkCli.sh -client-configuration /etc/clickhouse-keeper/zkclient.cfg -server bdtpsqlfdw01.bdtest.sit.pbz.hr:9281 ls / | egrep -v 'INFO|ERROR|^$|WATCHER|WatchedEvent'          Connecting to bdtpsqlfdw01.bdtest.sit.pbz.hr:9281\r\n2023-07-24 11:36:49,987 [myid:bdtpsqlfdw01.bdtest.sit.pbz.hr:9281] - WARN  [nioEventLoopGroup-2-1:ClientCnxnSocket@150] - Connected to an old server; r-o mode will be unavailable\r\n[clickhouse, keeper]\r\n```\r\nIs there any configuration that I shoulh setup and that is missing here?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/52516/comments",
    "author": "msestak",
    "comments": [
      {
        "user": "pufit",
        "created_at": "2023-07-29T02:59:23Z",
        "body": "@msestak I believe you just need to add `secure://` before your host.\r\ne.g. `clickhouse keeper-client -h secure://bdtpsqlfdw01.bdtest.sit.pbz.hr -p 9181 --query=\"ls /\"`"
      }
    ]
  },
  {
    "number": 51474,
    "title": "Cannot allocate RAFT instance due to \"Address family not supported by protocol\" (IPv6 disabled)",
    "created_at": "2023-06-27T11:09:14Z",
    "closed_at": "2023-07-10T09:44:41Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51474",
    "body": "\r\nIt's look like that resolving of issue #33381 hasn't fixed the problem with ClickHouse-Keeper on machines with disabled IPv6:\r\n\r\nIn /var/log/clickhouse-server/clickhouse-server.log we have:\r\n\r\n2023.06.27 13:47:36.661896 [ 113898 ] {} <Error> RaftInstance: got exception: open: Address family not supported by protocol [system:97] on port 9234\r\n2023.06.27 13:47:36.661947 [ 113898 ] {} <Debug> KeeperDispatcher: Server initialized, waiting for quorum\r\n2023.06.27 13:50:36.662692 [ 113898 ] {} <Error> void DB::KeeperDispatcher::initialize(const Poco::Util::AbstractConfiguration &, bool, bool): Code: 568. DB::Exception: Failed to wait RAFT initialization. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa3ec73a in /usr/bin/clickhouse\r\n1. DB::KeeperServer::waitInit() @ 0x16062e35 in /usr/bin/clickhouse\r\n2. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool) @ 0x1604df4a in /usr/bin/clickhouse\r\n3. DB::Context::initializeKeeperDispatcher(bool) const @ 0x14875d4d in /usr/bin/clickhouse\r\n4. DB::Server::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa48d880 in /usr/bin/clickhouse\r\n5. Poco::Util::Application::run() @ 0x189a11a6 in /usr/bin/clickhouse\r\n6. DB::Server::run() @ 0xa47e8fe in /usr/bin/clickhouse\r\n7. mainEntryClickHouseServer(int, char**) @ 0xa47be97 in /usr/bin/clickhouse\r\n8. main @ 0xa3da86b in /usr/bin/clickhouse\r\n9. __libc_start_main @ 0x3ad85 in /usr/lib64/libc-2.28.so\r\n10. _start @ 0xa19a46e in /usr/bin/clickhouse\r\n (version 22.8.6.71 (official build))\r\n\r\n...and no listening ports from ClickHouse...\r\nss -tunlp\r\nNetid                 State                  Recv-Q                 Send-Q                                 Local Address:Port                                  Peer Address:Port                Process                \r\nudp                   UNCONN                 0                      0                                            0.0.0.0:111                                        0.0.0.0:*                                          \r\nudp                   UNCONN                 0                      0                                          127.0.0.1:323                                        0.0.0.0:*                                          \r\nudp                   UNCONN                 0                      0                                            0.0.0.0:45755                                      0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:10050                                      0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:111                                        0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:22                                         0.0.0.0:*  \r\n\r\nThere are no uncommented IPv6 wildcards in config.xml, config.d configs and only one default user. To config.xml we added only:\r\n<listen_host>0.0.0.0</listen_host>\r\n<interserver_listen_host>0.0.0.0</interserver_listen_host>\r\nEverything else is default after ClickHouse installed from rpm.\r\n\r\nClickHouse release: 22.8.6.71-lts\r\nOS: RHEL 8\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51474/comments",
    "author": "SavelyevPavel",
    "comments": [
      {
        "user": "antonio2368",
        "created_at": "2023-06-28T12:27:06Z",
        "body": "Can you try setting `keeper_server.enable_ipv6` to `false` in your XML?\r\n`interserver_listen_host` support in Keeper is in 22.9+."
      }
    ]
  },
  {
    "number": 51401,
    "title": "DateTime Default value on missing/empty key/value pair.",
    "created_at": "2023-06-26T06:08:50Z",
    "closed_at": "2023-06-26T13:33:00Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51401",
    "body": "\r\n**Describe what's wrong**\r\n\r\nA DateTime column with Default expr as now() does not populate value from now if the value being inserted is empty or if the column is missing in the data source.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes. 23.2.2.20.\r\n\r\n**How to reproduce**\r\n\r\n* 23.2.2.20\r\n* clickhouse-client\r\n* Non-default settings: NA\r\n*\r\n\r\n```\r\nCREATE TABLE default.data_set\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Memory\r\n```\r\n\r\n\r\n```\r\nCREATE TABLE default.data_set_kafka\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Kafka\r\nSETTINGS kafka_broker_list = 'kafka-host:9092', kafka_topic_list = 'topic-name', kafka_group_name = 'kafka-group-name', kafka_format = 'JSONEachRow', kafka_skip_broken_messages = 10\r\n\r\n\r\nor you can create a regular table just for testing.\r\n\r\nCREATE TABLE default.data_set_kafka\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Memory\r\n```\r\n\r\n\r\n```\r\nCREATE MATERIALIZED VIEW default.data_set_mv TO default.data_set\r\n(\r\n    `createdAt` DateTime\r\n) AS\r\nSELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\nFROM default.data_set_kafka\r\n\r\n\r\nor even if you use just select without multiif, it does not work\r\n\r\n\r\nCREATE MATERIALIZED VIEW default.data_set_mv TO default.data_set\r\n(\r\n    `createdAt` DateTime\r\n) AS\r\nSELECT createdAt FROM default.data_set_kafka\r\n```\r\n\r\n\r\nFor regular kafka table (memory engine)\r\n```\r\nINSERT INTO default.data_set_kafka values ('2023-06-07')\r\nINSERT INTO default.data_set_kafka values (NULL)\r\nINSERT INTO default.data_set_kafka values ()\r\n```\r\n\r\nFor insertion using kcat command.\r\n```\r\n{}\r\n{\"createdAt\":\"\"}\r\n{\"createdAt\":\"2023-06-29\"}\r\n```\r\n\r\n\r\n* select * from default.data_set\r\n\r\n**Expected behavior**\r\n\r\nWe expect values to have the result of now() for missing column or empty value for the createdAt field. Explicitly specifying NULL works, however other options do not work.\r\n\r\n**Error message and/or stacktrace**\r\nNo error messages.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51401/comments",
    "author": "ilugid",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2023-06-26T11:06:08Z",
        "body": "> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\n\r\nBy default fields are non nullable\r\n\r\n> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\n\r\nIn MV you should match not positions of columns, but their names\r\n\r\nIE \r\n\r\n> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt) as createdAt"
      }
    ]
  },
  {
    "number": 47521,
    "title": "OOM on group by query",
    "created_at": "2023-03-13T06:45:57Z",
    "closed_at": "2023-03-14T21:03:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/47521",
    "body": "> You have to provide the following information whenever possible.\r\n\r\nselect v, count(*) ... group by 1 produces OOM\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nI created table from external tsv as MergeTree:\r\n\r\nset max_memory_usage = 8000000000;\r\nset max_memory_usage_for_user = 8000000000;\r\nset max_bytes_before_external_group_by = 10000000;\r\nset max_bytes_before_external_sort = 10000000;\r\n\r\ncreate table if not exists t ENGINE = MergeTree() order by v1 as\r\nselect *\r\nfrom file('/usr/proj/tsv-*', TabSeparated, 'v1 text, v2 text, v3 text, seq bigint');\r\n\r\nTable has 3B rows.\r\n\r\nI run following query:\r\n\r\nselect v1, count(*) from t group by 1 limit 5;\r\n\r\nI receive following error:\r\n\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 7.45 GiB (attempt to allocate chunk of 4225696 bytes), maximum: 7.45 GiB.: While executing SourceFromNativeStream. (MEMORY_LIMIT_EXCEEDED)\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/47521/comments",
    "author": "rikuiki",
    "comments": [
      {
        "user": "nickitat",
        "created_at": "2023-03-14T21:03:40Z",
        "body": "memory limit currently strictly limits only pre aggregation phase (separate aggregation on execution threads). at the time of merging we need to bring smth around `total_aggr_state_size * aggregation_memory_efficient_merge_threads / NUM_BUCKETS (=256)` into memory. and you got error at merging phase as you see.\r\nas a workaround you could try to lower `aggregation_memory_efficient_merge_threads` if increasing memory limit is not an option.\r\n#40588 is aimed to address this issue \r\nP.S. it is not a good idea to set `max_bytes_before_external_group_by` too low. the lower this setting the more intermediate files will be generated. apart from consuming time it also affect memory usage in a bad way: for reading these files we will allocate 1MB buffer for each of them. you could easily get hundreds or even thousands of such small files with really small `max_bytes_before_external_group_by`. good default value is half of the memory limit."
      }
    ]
  },
  {
    "number": 46049,
    "title": "mutation update throw NOT_FOUND_COLUMN_IN_BLOCK exception",
    "created_at": "2023-02-05T08:16:31Z",
    "closed_at": "2023-02-07T03:27:17Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46049",
    "body": "ClickHouse server version 22.9.3 revision 54460\r\n\r\n**Describe what's wrong**\r\n\r\nWhen I try to update the table by `ALTER TABLE events UPDATE costs_usd = cost_usd / 100 WHERE costs_usd = 0 AND cost_usd > 0;`\r\n\r\nI see **Code: 10. DB::Exception: Not found column clickid in block.** in system.mutations table\r\n\r\nthe `show create table events` result:\r\n\r\n```\r\nCREATE TABLE demo.events\r\n(\r\n    `event_time` DateTime('UTC'),\r\n    `queue_msg_id` String DEFAULT '',\r\n    `click_id` FixedString(32),\r\n    `clickid` String DEFAULT '',\r\n    `host` LowCardinality(String),\r\n    `post_status` LowCardinality(String),\r\n    `cost_usd` Int32,\r\n    `costs_usd` Decimal(12, 6) DEFAULT 0,\r\n    `costs_rub` Decimal(12, 6) DEFAULT 0,\r\n    `costs_cny` Decimal(12, 6) DEFAULT 0,\r\n    `costs_gbp` Decimal(12, 6) DEFAULT 0,\r\n    `costs_eur` Decimal(12, 6) DEFAULT 0,\r\n    `cost_rub` Int32,\r\n    `cost_cny` Int32,\r\n    `cost_eur` Int32,\r\n    `cost_gbp` Int32,\r\n    `rev_usd` Int32,\r\n    `revs_usd` Decimal(10, 4) DEFAULT 0,\r\n    `revs_rub` Decimal(10, 4) DEFAULT 0,\r\n    `revs_cny` Decimal(10, 4) DEFAULT 0,\r\n    `revs_gbp` Decimal(10, 4) DEFAULT 0,\r\n    `revs_eur` Decimal(10, 4) DEFAULT 0,\r\n    `rev_rub` Int32,\r\n    `rev_cny` Int32,\r\n    `rev_eur` Int32,\r\n    `rev_gbp` Int32,\r\n    `country` LowCardinality(String),\r\n    `region` LowCardinality(String),\r\n    `city` LowCardinality(String),\r\n    `postal_code` LowCardinality(String),\r\n    `isp` LowCardinality(String),\r\n    `asn` LowCardinality(String) DEFAULT '',\r\n    INDEX clickid clickid TYPE minmax GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(event_time)\r\nORDER BY (owner_id, camp_id, event_time)\r\nSETTINGS min_rows_for_wide_part = 1000000, index_granularity = 8192 \r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46049/comments",
    "author": "jasonbigl",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2023-02-06T14:39:45Z",
        "body": "> version 22.9.3\r\n\r\nFirst of all, this version is obsolete, and you should upgrade. \r\n\r\n> ALTER TABLE events UPDATE costs_usd = cost_usd / 100 WHERE costs_usd = 0 AND cost_usd > 0;\r\n> Not found column clickid in block\r\n\r\nThis ALTER does not use `clickid` column. Probably some previous mutation has failed with this error, so all subsequent mutations simply rethrow the error.\r\n\r\nPlease share `select * from system.mutations where is_done=0` "
      }
    ]
  },
  {
    "number": 45232,
    "title": "CANNOT_PARSE_TEXT errors exceeded 600,000 times",
    "created_at": "2023-01-12T17:48:59Z",
    "closed_at": "2023-01-12T18:48:58Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45232",
    "body": "ClickHouse Version: 22.10.1.1248\r\n\r\nThe following errors occur in /var/log/clickhouse-server/clickhouse-server.err.log almost every second.\r\n```\r\n<Error> TCPHandler: Code: 6. DB::Exception: Cannot parse string '2022-11-30 019:48:33.237' as DateTime64(6): syntax error at position 19 (parsed just '2022-11-30 019:48:3'): while executing 'FUNCTION toDateTime64(time : 0, 6 :: 1) -> toDateTime64(time, 6) DateTime64(6) : 2'. (CANNOT_PARSE_TEXT), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./build_docker/../src/Common/Exception.cpp:69: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb29f568 in /usr/bin/clickhouse\r\n1. DB::throwExceptionForIncompletelyParsedValue(DB::ReadBuffer&, DB::IDataType const&) @ 0x6ed06fc in /usr/bin/clickhouse\r\n2. bool DB::callOnIndexAndDataType<DB::DataTypeDateTime64, DB::FunctionConvert<DB::DataTypeDateTime64, DB::NameToDateTime64, DB::ToDateTimeMonotonicity>::executeInternal(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const::'lambda'(auto const&, auto const&)&, DB::ConvertDefaultBehaviorTag>(DB::TypeIndex, auto&&, DB::ConvertDefaultBehaviorTag&&) @ 0x73cec64 in /usr/bin/clickhouse\r\n3. DB::FunctionConvert<DB::DataTypeDateTime64, DB::NameToDateTime64, DB::ToDateTimeMonotonicity>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x73ca5bc in /usr/bin/clickhouse\r\n4. ./build_docker/../src/Functions/IFunction.cpp:0: DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7b158 in /usr/bin/clickhouse\r\n5. ./build_docker/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:115: DB::IExecutableFunction::executeWithoutSparseColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7ba94 in /usr/bin/clickhouse\r\n6. ./build_docker/../contrib/libcxx/include/vector:399: DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7cf64 in /usr/bin/clickhouse\r\n7. ./build_docker/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:115: DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0xf7d7378 in /usr/bin/clickhouse\r\n8. ./build_docker/../contrib/libcxx/include/vector:505: DB::ExpressionActions::execute(DB::Block&, bool) const @ 0xf7d81d0 in /usr/bin/clickhouse\r\n9. ./build_docker/../contrib/libcxx/include/vector:1416: DB::MergeTreePartition::executePartitionByExpression(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Block&, std::__1::shared_ptr<DB::Context const>) @ 0x106413f0 in /usr/bin/clickhouse\r\n10. ./build_docker/../contrib/libcxx/include/list:916: DB::MergeTreeDataWriter::splitBlockIntoParts(DB::Block const&, unsigned long, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::Context const>) @ 0x106960e0 in /usr/bin/clickhouse\r\n11. ./build_docker/../contrib/libcxx/include/vector:1408: DB::MergeTreeSink::consume(DB::Chunk) @ 0x107b13e4 in /usr/bin/clickhouse\r\n12. ./build_docker/../contrib/libcxx/include/__memory/shared_ptr.h:702: DB::SinkToStorage::onConsume(DB::Chunk) @ 0x10b84270 in /usr/bin/clickhouse\r\n13. ./build_docker/../contrib/libcxx/include/__memory/shared_ptr.h:702: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::ExceptionKeepingTransform::work()::$_1, void ()> >(std::__1::__function::__policy_storage const*) @ 0x10af2474 in /usr/bin/clickhouse\r\n14. ./build_docker/../src/Processors/Transforms/ExceptionKeepingTransform.cpp:122: DB::runStep(std::__1::function<void ()>, DB::ThreadStatus*, std::__1::atomic<unsigned long>*) @ 0x10af2198 in /usr/bin/clickhouse\r\n15. ./build_docker/../contrib/libcxx/include/__functional/function.h:813: DB::ExceptionKeepingTransform::work() @ 0x10af1abc in /usr/bin/clickhouse\r\n16. ./build_docker/../src/Processors/Executors/ExecutionThreadContext.cpp:52: DB::ExecutionThreadContext::executeTask() @ 0x109471a0 in /usr/bin/clickhouse\r\n17. ./build_docker/../src/Processors/Executors/PipelineExecutor.cpp:228: DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x1093c1ac in /usr/bin/clickhouse\r\n18. ./build_docker/../src/Processors/Executors/PipelineExecutor.cpp:127: DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0x1093b654 in /usr/bin/clickhouse\r\n19. ./build_docker/../src/Server/TCPHandler.cpp:713: DB::TCPHandler::processInsertQuery() @ 0x108eba3c i\r\n```\r\n\r\nI didn't call the toDateTime64 function, the only thing that may have affected is this table:\r\n```\r\n-- simplify\r\nCREATE TABLE test.test_tb(\r\n    `time` String,\r\n    a String,\r\n    b String,\r\n    c String\r\n) \r\nENGINE = ReplacingMergeTree()\r\nPARTITION BY toDate(toDateTime64(time, 6))\r\nORDER BY (a, b, c);\r\n```\r\nBut I have also truncate the table data\r\n\r\nWhy does this error keep happening? Is there a good way to locate it?\r\n\r\nThanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45232/comments",
    "author": "Onehr7",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-12T18:44:23Z",
        "body": "> didn't call the toDateTime64 function, the only thing that may have affected is this table:\r\n\r\n`PARTITION BY toDate(          toDateTime64(           time, 6))`\r\n\r\n\r\n```\r\nselect toDateTime64('2022-11-30 019:48:33.237', 6);\r\n\r\nDB::Exception: Cannot parse string '2022-11-30 019:48:33.237' as DateTime64(6):\r\n```\r\n\r\n\r\n```sql\r\nselect parseDateTime64BestEffortOrZero('2022-11-30 019:48:33.237', 6);\r\n\u250c\u2500parseDateTime64BestEffortOrZero('2022-11-30 019:48:33.237', 6)\u2500\u2510\r\n\u2502                                     1970-01-01 00:00:00.000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect toDateTime64OrZero('2022-11-30 019:48:33.237', 6);\r\n\u250c\u2500toDateTime64OrZero('2022-11-30 019:48:33.237', 6)\u2500\u2510\r\n\u2502                        1970-01-01 00:00:00.000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2023-01-12T18:48:31Z",
        "body": "I suggest to use `parseDateTime64BestEffortOrZero`\r\n\r\n```sql\r\nPARTITION BY toDate(parseDateTime64BestEffortOrZero(time, 6))\r\n```"
      }
    ]
  },
  {
    "number": 44474,
    "title": "ClickHouse Reports a lot of CLIENT_HAS_CONNECTED_TO_WRONG_PORT error",
    "created_at": "2022-12-21T03:36:15Z",
    "closed_at": "2022-12-21T05:57:09Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/44474",
    "body": "When I upgrade ClickHouse from 22.4 to 22.11, it began to report errors.\r\n\r\n```\r\nWITH arrayMap(x -> demangle(addressToSymbol(x)), last_error_trace) AS `all`\r\nSELECT\r\n    name,\r\n    arrayStringConcat(`all`, '\\n') AS res\r\nFROM system.errors\r\nWHERE name = 'CLIENT_HAS_CONNECTED_TO_WRONG_PORT'\r\nSETTINGS allow_introspection_functions = 1\r\n\r\nQuery id: 5b03cb4a-7d0e-48d4-b7d0-299294674f08\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname: CLIENT_HAS_CONNECTED_TO_WRONG_PORT\r\nres:  DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, int, bool)\r\nDB::TCPHandler::receiveHello()\r\nDB::TCPHandler::runImpl()\r\nDB::TCPHandler::run()\r\nPoco::Net::TCPServerConnection::start()\r\nPoco::Net::TCPServerDispatcher::run()\r\nPoco::PooledThread::run()\r\nPoco::ThreadImpl::runnableEntry(void*)\r\n\r\n__clone\r\n\r\n1 row in set. Elapsed: 0.003 sec.\r\n```\r\n\r\nAnd it's a lot.\r\n```\r\nRow 6:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname:               CLIENT_HAS_CONNECTED_TO_WRONG_PORT\r\ncode:               217\r\nvalue:              30361\r\nlast_error_time:    2022-12-21 03:31:03\r\nlast_error_message: Client has connected to wrong port\r\nlast_error_trace:   [228653722,339140737,339111492,339193977,387649044,387655611,389289415,389279741,140518016472585,140518015574323]\r\nremote:             0\r\n```\r\n\r\nI use ClickHouse C++ SDK with TCP port 9000, the timeout is 70s.\r\n\r\nThe result of `netstat` is:\r\n```\r\n[root]# netstat -tnlp | grep click\r\ntcp        0      0 0.0.0.0:9004            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9005            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9009            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9363            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:8123            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\ntcp        0      0 0.0.0.0:9000            0.0.0.0:*               LISTEN      55381/clickhouse-se\r\n```\r\n\r\nThe version is 22.11.2.30.\r\n\r\nI have not encountered this problem at 22.4, and the `system.errors` table did not query this error.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/44474/comments",
    "author": "LGDHuaOPER",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2022-12-21T05:57:09Z",
        "body": "IF you will send some non-http data to http port, or if you will use http data to a non-http port, such exception is expected."
      },
      {
        "user": "LGDHuaOPER",
        "created_at": "2022-12-21T15:56:55Z",
        "body": "You're right.I did a health check using `curl localhost:9000`."
      }
    ]
  },
  {
    "number": 44043,
    "title": "Error compiling llvm of Clickhouse ",
    "created_at": "2022-12-08T13:25:43Z",
    "closed_at": "2022-12-08T23:52:54Z",
    "labels": [
      "question",
      "build"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/44043",
    "body": "try to compiling clickhouse of the latest\r\nCmake version\r\n3.22.1\r\nNinja version\r\n1.10.1\r\n\r\nFAILED: contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o \r\n/usr/bin/c++ -DSTD_EXCEPTION_HAS_STACK_TRACE=1 -D_GNU_SOURCE -D_LIBCPP_ENABLE_THREAD_SAFETY_ANNOTATIONS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -I/mnt/chuhedb/ClickHouse/build/contrib/llvm-project/llvm/utils/TableGen -I/mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen -I/mnt/chuhedb/ClickHouse/build/contrib/llvm-project/llvm/include -I/mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/include -I/mnt/chuhedb/ClickHouse/base/glibc-compatibility/memcpy -isystem /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include -isystem /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxxabi/include -isystem /mnt/chuhedb/ClickHouse/contrib/libunwind/include -fdiagnostics-color=always -fsized-deallocation -fcoroutines  -pipe -mssse3 -msse4.1 -msse4.2 -mpclmul -mpopcnt -fasynchronous-unwind-tables -ffile-prefix-map=/mnt/chuhedb/ClickHouse=. -falign-functions=32 -Wa,-mbranches-within-32B-boundaries -w -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -pedantic -Wno-long-long -Wimplicit-fallthrough -Wno-maybe-uninitialized -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wnon-virtual-dtor -Wdelete-non-virtual-dtor -Wsuggest-override -Wmisleading-indentation -fdiagnostics-color -ffunction-sections -fdata-sections -O2 -g -DNDEBUG -O3 -g -gdwarf-4  -fno-pie   -D OS_LINUX -g0 -Werror -nostdinc++ -std=c++14 -MD -MT contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o -MF contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o.d -o contrib/llvm-project/llvm/utils/TableGen/CMakeFiles/llvm-tblgen.dir/CodeGenSchedule.cpp.o -c /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp\r\nIn file included from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__functional/invoke.h:17,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/type_traits:421,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/limits:107,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/math.h:309,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/cmath:309,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/include/llvm/Support/MathExtras.h:19,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/include/llvm/ADT/APInt.h:19,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.h:17,\r\n                 from /mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp:14:\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__type_traits/decay.h: In instantiation of 'struct std::__1::__decay<const llvm::APInt&, true>':\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__type_traits/decay.h:56:89:   required from 'struct std::__1::decay<const llvm::APInt&&>'\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__utility/pair.h:132:16:   required by substitution of 'template<class _Tuple, typename std::__1::enable_if<typename std::__1::conditional<(std::__1::__tuple_like_with_size<_Tuple, 2, typename std::__1::remove_cv<typename std::__1::remove_reference<_Tp>::type>::type>::value && (! std::__1::is_same<typename std::__1::decay<_Tp>::type, std::__1::pair<const llvm::Record*, llvm::OpcodeInfo> >::value)), std::__1::pair<const llvm::Record*, llvm::OpcodeInfo>::_CheckTupleLikeConstructor, std::__1::__check_tuple_constructor_fail>::type::__enable_implicit<_Tuple>(), void>::type* <anonymous> > constexpr std::__1::pair<const llvm::Record*, llvm::OpcodeInfo>::pair(_Tuple&&) [with _Tuple = const llvm::APInt&&; typename std::__1::enable_if<typename std::__1::conditional<(std::__1::__tuple_like_with_size<_Tuple, 2, typename std::__1::remove_cv<typename std::__1::remove_reference<_Tp>::type>::type>::value && (! std::__1::is_same<typename std::__1::decay<_Tp>::type, std::__1::pair<const llvm::Record*, llvm::OpcodeInfo> >::value)), std::__1::pair<const llvm::Record*, llvm::OpcodeInfo>::_CheckTupleLikeConstructor, std::__1::__check_tuple_constructor_fail>::type::__enable_implicit<_Tuple>(), void>::type* <anonymous> = <missing>]'\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp:372:69:   required from here\r\n/mnt/chuhedb/ClickHouse/contrib/llvm-project/libcxx/include/__type_traits/decay.h:47:30: error: forming pointer to reference type 'std::__1::remove_extent<const llvm::APInt&>::type' {aka 'const llvm::APInt&'}\r\n   47 |                      >::type type;\r\n      |                              ^~~~\r\n[35/6130] Building CXX object contrib/llvm-project/llvm/lib/MC/CMakeFiles/LLVMMC.dir/MCContext.cpp.o",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/44043/comments",
    "author": "Grfire",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-12-08T23:52:54Z",
        "body": "```\r\n/usr/bin/c++\r\n```\r\n\r\nOnly `clang` can be used to compile ClickHouse.\r\n`gcc` cannot be used."
      }
    ]
  },
  {
    "number": 41914,
    "title": "Calculate avg from distributed tables returns wrong value",
    "created_at": "2022-09-28T18:14:40Z",
    "closed_at": "2022-09-29T16:57:05Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41914",
    "body": "We having an issue where for a specific dataset (not happening for other customers), the result of calling avg(measurement) always returns 18446744073709552000 which looks like an overflown value and measurement column is of type int64.\r\nThis only happens when running the query agains a virtual distributed table, if we query the actual replicated table it returns the correct value.\r\nWe found a workaround, if we cast measurement value to int64, the avg value returns correct. This is very odd since the column is already an int64 value. The workaround looks something like:\r\n`SELECT avg(toInt64(measurement)) FROM distributed_table`\r\nNot sure why when recasting to int64 or querying the replicated table works and not when querying the distributed table, so we were wondering if this is a known issue or a potential bug.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41914/comments",
    "author": "alazo8807",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-28T21:40:16Z",
        "body": "Please provide `show create table distributed_table` and `show create table local_table` \r\nCheck that columns in both tables have the same types."
      },
      {
        "user": "alazo8807",
        "created_at": "2022-09-29T15:50:03Z",
        "body": "Thanks @den-crane. The distributed table is a virtual one, so the type of the column is the same. \r\nThis is a simplified version of our table definition. In our client application we query the distributed virtual table.\r\n\r\n\r\n```\r\nCREATE TABLE IF NOT EXISTS data.replicated_table_v1\r\nON CLUSTER '{cluster}'\r\n(\r\n    date Date DEFAULT toDate(timestamp, 'UTC') Codec(ZSTD),\r\n    timestamp UInt64 Codec(DoubleDelta, LZ4),\r\n    measure_int Int64 Codec(Gorilla, LZ4),\r\n    ...\r\n)\r\nENGINE = ReplicatedMergeTree(\r\n    '/clickhouse/tables/{shard}/data/replicated_table_v1,\r\n    '{replica}'\r\n)\r\nPARTITION BY toStartOfMonth(date)\r\nORDER BY (\r\n    timestamp\r\n)\r\nTTL date + INTERVAL ${TTL_INTERVAL} DELETE\r\n```\r\n\r\nThen we have a merge virtual table that pulls from all the replicated\r\n```\r\nCREATE TABLE IF NOT EXISTS entity.merged_table\r\nON CLUSTER '{cluster}'\r\nAS data.replicated_table_v1\r\nENGINE = Merge(\r\n    data,\r\n    '^replicated_table_v1'\r\n)\r\n```\r\n\r\nAnd finally another virtual distributed table that pulls from the merged_table\r\n```\r\nCREATE TABLE IF NOT EXISTS entity.distributed_table\r\nAS data.replicated_table_v1\r\nENGINE = Distributed(\r\n    '{cluster}',\r\n    data,\r\n    merged_table\r\n)\r\n\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-29T17:06:13Z",
        "body": "here is an example\r\n\r\n```sql\r\ncreate table local( A Int32, B Int32) Engine=MergeTree order by A;\r\ncreate table distr as local Engine=Distributed('test_shard_localhost', currentDatabase(), local, rand());\r\n\r\nalter table local modify column B Int64;\r\n\r\ndesc local\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\r\n\u2502 A    \u2502 Int32\r\n\u2502 B    \u2502 Int64  -----<<<<<<<\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\ndesc distr;\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\r\n\u2502 A    \u2502 Int32\r\n\u2502 B    \u2502 Int32  -----<<<<<<< NOT INT64 !!!!!!\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\n\r\n\r\ninsert into local values ( 1, toUInt64(-1)/2-100);\r\n\r\nselect * from local;\r\n\u250c\u2500A\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500B\u2500\u2510\r\n\u2502 1 \u2502 9223372036854775807 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n select * from distr;\r\n\u250c\u2500A\u2500\u252c\u2500\u2500B\u2500\u2510\r\n\u2502 1 \u2502 -1 \u2502 -----<<<<<<< NOT 9223372036854775807 !!!!!\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\r\n\r\nalter table distr modify column B Int64; --------<<<<<<<<< FIX !!!!\r\n\r\nselect * from distr;\r\n\u250c\u2500A\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500B\u2500\u2510\r\n\u2502 1 \u2502 9223372036854775807 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "alazo8807",
        "created_at": "2022-09-29T18:17:25Z",
        "body": "In our case I just checked and can confirm by running desc that both the distributed virtual table and the replicated table have the column defined as int64. \r\nAnything else I could look at?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-29T20:44:17Z",
        "body": "Please share the result of \r\n\r\n```sql\r\nSELECT\r\n    type,\r\n    count()\r\nFROM clusterAllReplicas('{cluster}', system.columns)\r\nWHERE name = 'measurement'\r\nGROUP BY type\r\n```"
      },
      {
        "user": "alazo8807",
        "created_at": "2022-09-30T12:10:26Z",
        "body": "It looks like measurement is defined as uint64 in one of our materialized views. Thanks for all the help"
      }
    ]
  },
  {
    "number": 41758,
    "title": "ClickHouse version 21.12.4.1 use BACKUP table async",
    "created_at": "2022-09-26T03:17:53Z",
    "closed_at": "2022-09-26T14:25:27Z",
    "labels": [
      "question",
      "obsolete-version",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41758",
    "body": "ClickHouse version 21.12.4.1 \r\n\r\nBACKUP database default  TO Disk('backups', 'backup-20220926/') ASYNC\r\n\r\nCode: 62. DB::Exception: Syntax error: failed at position 115 ('ASYNC'): ASYNC. Expected one of: FILTER, OVER, SETTINGS, end of query. (SYNTAX_ERROR) (version 21.12.4.1 (official build))\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41758/comments",
    "author": "hbzhu",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-26T14:25:26Z",
        "body": "`BACKUP database` command is available in the later releases. You need to upgrade to 22.9."
      },
      {
        "user": "hbzhu",
        "created_at": "2022-09-27T02:19:03Z",
        "body": "Thanks\uff0casynchronous backup tables need to upgrade to 22.9 to\uff1f"
      }
    ]
  },
  {
    "number": 41714,
    "title": "When I bulk insert the replica table, all the data in the table becomes empty",
    "created_at": "2022-09-23T07:51:23Z",
    "closed_at": "2022-09-23T14:56:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41714",
    "body": "**Describe what's wrong**\r\n\r\n> When I bulk insert the replica table, all the data in the table becomes empty\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\nclickhouse-version: **v22.6.3.35-stable**\r\nzookeeper-version: **3.4.9**\r\n\r\n* Firstly, I create a replica table\r\n```sql\r\nCREATE TABLE IF NOT EXISTS test_db.test1 (\r\n    `token_id` String,\r\n    `event_id` String,\r\n    `login_id` String,\r\n    `distinct_id` String,\r\n    `ctime` DateTime64(3),\r\n    `utime` DateTime64(3),\r\n    `cdate` String,\r\n    `udate` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/test_db/test1', '{replica}')\r\nORDER BY ( distinct_id, login_id, utime, token_id)\r\nPARTITION BY (toYYYYMMDD(utime))\r\nTTL toDateTime(udate) + toIntervalMonth(4)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n* Then I batched 3 times, each batch of 40,000 pieces of data, a total of 120,000 pieces of data were inserted,\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 8b2071f7-7a50-49e2-a5fc-6dc18e46da07\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502  120000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.009 sec.\r\n```\r\n\r\n* Then I inserted 40,000 pieces of data in batches, each batch of 10,000 pieces of data, a total of 4 batches. During the insert process, I keep querying and find that the data has been decreasing\r\n\r\n> **The data inserted above, each batch has more than 100 partitions of data;**\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 174d0072-f9c9-44ea-b8c5-bff5d2527dc4\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   22993 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.014 sec. \r\n\r\n```\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 978eb77c-b517-4296-b84e-f6955efbec03\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    8038 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.007 sec\r\n\r\n```\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: dd5287da-fb75-42f3-b455-225bbfa05803\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.012 sec\r\n\r\n```\r\n* In the end I only get 10000 pieces of data, but I want to get 160000. I don't know why the data is lost, I checked the logs, the local files are all cleaned up\r\n  \r\n```\r\n2022.09.23 15:36:55.302288 [ 16832 ] {} <Trace> test_db.test1 (ReplicatedMergeTreeCleanupThread): Cleared 196 old blocks from ZooKeeper\r\n2022.09.23 15:36:55.302539 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220725_1_6_1\r\n2022.09.23 15:36:55.302574 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220725_1_6_1\r\n2022.09.23 15:36:55.313579 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220726_1_6_1\r\n2022.09.23 15:36:55.313632 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220726_1_6_1\r\n2022.09.23 15:36:55.315411 [ 16720 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000891 - log-0000000891\r\n2022.09.23 15:36:55.319817 [ 16720 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2022.09.23 15:36:55.321111 [ 16663 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Executing DROP_RANGE 20220725_1_6_1\r\n2022.09.23 15:36:55.321264 [ 16663 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Removed 0 entries from queue. Waiting for 0 entries that are currently executing.\r\n2022.09.23 15:36:55.321386 [ 16663 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing parts.\r\n2022.09.23 15:36:55.325987 [ 16663 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 7 parts inside 20220725_1_6_1.\r\n2022.09.23 15:36:55.332289 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220727_1_6_1\r\n2022.09.23 15:36:55.332332 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220727_1_6_1\r\n2022.09.23 15:36:55.334647 [ 16734 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000892 - log-0000000892\r\n2022.09.23 15:36:55.336885 [ 16734 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2022.09.23 15:36:55.337758 [ 16691 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Executing DROP_RANGE 20220726_1_6_1\r\n2022.09.23 15:36:55.337782 [ 16691 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Removed 0 entries from queue. Waiting for 0 entries that are currently executing.\r\n2022.09.23 15:36:55.337837 [ 16691 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing parts.\r\n2022.09.23 15:36:55.342937 [ 16691 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 7 parts inside 20220726_1_6_1.\r\n2022.09.23 15:36:55.348930 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220728_1_6_1\r\n2022.09.23 15:36:55.348958 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220728_1_6_1\r\n...\r\n...\r\n2022.09.23 15:36:57.503607 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Found 560 old parts to remove.\r\n2022.09.23 15:36:57.503822 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing 560 old parts from ZooKeeper\r\n2022.09.23 15:36:57.511469 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_1_1_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512061 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_1_6_1 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512068 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_2_2_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512073 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_3_3_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512078 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_4_4_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512082 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_5_5_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512085 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_6_6_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512091 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_1_1_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512096 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_1_6_1 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512100 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_2_2_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512104 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_3_3_0 in ZooKeeper, it was only in filesystem\r\n...\r\n...\r\n2022.09.23 15:36:57.535842 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 560 old parts from ZooKeeper. Removing them from filesystem.\r\n2022.09.23 15:36:57.536166 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_1_6_1\r\n2022.09.23 15:36:57.536167 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_1_1_0\r\n2022.09.23 15:36:57.536836 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_2_2_0\r\n2022.09.23 15:36:57.537405 [ 16780 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000971 - log-0000000971\r\n2022.09.23 15:36:57.537988 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_3_3_0\r\n2022.09.23 15:36:57.538036 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_4_4_0\r\n2022.09.23 15:36:57.538724 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_5_5_0\r\n2022.09.23 15:36:57.538762 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_6_6_0\r\n2022.09.23 15:36:57.539189 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_1_1_0\r\n2022.09.23 15:36:57.539742 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_1_6_1\r\n2022.09.23 15:36:57.540188 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_2_2_0\r\n2022.09.23 15:36:57.540191 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_3_3_0\r\n...\r\n2022.09.23 15:36:57.702064 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_3_3_0\r\n2022.09.23 15:36:57.702507 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_4_4_0\r\n2022.09.23 15:36:57.702957 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_5_5_0\r\n2022.09.23 15:36:57.702959 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_6_6_0\r\n2022.09.23 15:36:57.704515 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 560 old parts\r\n2022.09.23 15:36:57.721899 [ 16832 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeCleanupThread): Removed 81 old log entries: log-0000000881 - log-0000000961\r\n2022.09.23 15:36:57.724084 [ 16832 ] {} <Trace> test_db.test1 (ReplicatedMergeTreeCleanupThread): Execution took 221 ms.\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41714/comments",
    "author": "JohnZp",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-23T14:12:14Z",
        "body": "@JohnZp check server time `select now()`,\r\nremove TTL and check data insertion without TTL."
      },
      {
        "user": "tavplubix",
        "created_at": "2022-09-23T14:12:58Z",
        "body": "> I can guarantee that the expiration time is not reached\r\n\r\nPlease share a reproducible example then, so we will check if there's a bug in TTL"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T14:15:44Z",
        "body": ">`udate` String\r\n>TTL toDateTime(udate)\r\n\r\nAlso probably your String contains something which is evaluated to `1970-01-01` with toDateTime.\r\n\r\n```\r\nselect toDateTime('haba-01-01 01:21:33');\r\n\u250c\u2500toDateTime('haba-01-01 01:21:33')\u2500\u2510\r\n\u2502               1970-01-01 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T21:35:03Z",
        "body": "```sql\r\nSELECT parseDateTimeBestEffort('20220701')\r\n\r\n\r\n\u250c\u2500parseDateTimeBestEffort('20220701')\u2500\u2510\r\n\u2502                 2022-07-01 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      }
    ]
  },
  {
    "number": 40479,
    "title": "system.replicas error",
    "created_at": "2022-08-22T07:49:38Z",
    "closed_at": "2022-08-22T14:22:40Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40479",
    "body": "execute  (SELECT * FROM `system`.replicas LIMIT 0, 200  \uff09 error\uff1a\r\nSQL Error [1000]: ClickHouse exception, code: 1000, host: 10.99.84.60, port: 31197; Poco::Exception. Code: 1000, e.code() = 2002, e.displayText() = mysqlxx::ConnectionFailed: Can't connect to MySQL server on '10.99.85.241' (115) ((nullptr):0) (version 20.3.8.53 (official build))",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40479/comments",
    "author": "zhwuhuang",
    "comments": [
      {
        "user": "rkozlo",
        "created_at": "2022-08-22T08:38:43Z",
        "body": "Version 20.3 is a version of clickhouse? Start with update because this version years behind its support."
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-22T14:22:40Z",
        "body": "Upgrade your CH to 22.8."
      }
    ]
  },
  {
    "number": 38375,
    "title": "different results returned when using date filter and string(equals to the date)",
    "created_at": "2022-06-24T08:57:44Z",
    "closed_at": "2022-06-25T01:00:47Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38375",
    "body": "version:  21.8.14.5\r\n\r\ndistributed table\r\n```sql\r\nlocalhost :) show create table xdr_fullaudit_monitor;\r\n\r\nSHOW CREATE TABLE xdr_fullaudit_monitor\r\n\r\nQuery id: cc0f4d1e-8156-4a48-871b-7d9bdef6c6f9\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE falcon0325.xdr_fullaudit_monitor\r\n(\r\n    `logid` String,\r\n    `srcip4` Int64,\r\n    `srcip6` String,\r\n    `srcport` Int32,\r\n    `destip4` Int64,\r\n    `destip6` String,\r\n    `destport` Int32,\r\n    `isipv4` UInt8,\r\n    `gathertime` Int64,\r\n    `taskid` Int32,\r\n    `probeid` String,\r\n    `ispid` Int32,\r\n    `roomid` Int32,\r\n    `protocolid` Int32,\r\n    `reftaskid` Int32,\r\n    `interfacetype` Int32,\r\n    `rattype` Int32,\r\n    `proceduretype` Int32,\r\n    `begintime` Int64,\r\n    `endtime` Int64,\r\n    `procedurestatus` Int32,\r\n    `callerphonenumber` String,\r\n    `phonenumber` String,\r\n    `imsi` String,\r\n    `imei` String,\r\n    `nesrcip4` Int64,\r\n    `nesrcip6` String,\r\n    `nedestip4` Int64,\r\n    `nedestip6` String,\r\n    `apn` String,\r\n    `lactac` Int32,\r\n    `cieci` Int32,\r\n    `smssenderphone` String,\r\n    `smsrecieverphone` String,\r\n    `smsdirection` Int32,\r\n    `smscode` Int32,\r\n    `smstime` Int64,\r\n    `smslength` Int32,\r\n    `smscontent` String,\r\n    `pdate` Date,\r\n    `srcip` String,\r\n    `destip` String,\r\n    `nesrcip` String,\r\n    `nedestip` String,\r\n    `mcc` Int32,\r\n    `mnc` Int32\r\n)\r\nENGINE = Distributed('falcon0325', 'falcon0325', 'xdr_fullaudit_monitor_local', rand()) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec.\r\n```\r\n\r\nlocal table\r\n```sql\r\nlocalhost :) show create table xdr_fullaudit_monitor_local;\r\n\r\nSHOW CREATE TABLE xdr_fullaudit_monitor_local\r\n\r\nQuery id: 90d5c288-67f8-4758-b033-685d0d2187ad\r\n\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE falcon0325.xdr_fullaudit_monitor_local\r\n(\r\n    `logid` String,\r\n    `srcip4` Int64,\r\n    `srcip6` String,\r\n    `srcport` Int32,\r\n    `destip4` Int64,\r\n    `destip6` String,\r\n    `destport` Int32,\r\n    `isipv4` UInt8,\r\n    `gathertime` Int64,\r\n    `taskid` Int32,\r\n    `probeid` String,\r\n    `ispid` Int32,\r\n    `roomid` Int32,\r\n    `protocolid` Int32,\r\n    `reftaskid` Int32,\r\n    `interfacetype` Int32,\r\n    `rattype` Int32,\r\n    `proceduretype` Int32,\r\n    `begintime` Int64,\r\n    `endtime` Int64,\r\n    `procedurestatus` Int32,\r\n    `callerphonenumber` String,\r\n    `phonenumber` String,\r\n    `imsi` String,\r\n    `imei` String,\r\n    `nesrcip4` Int64,\r\n    `nesrcip6` String,\r\n    `nedestip4` Int64,\r\n    `nedestip6` String,\r\n    `apn` String,\r\n    `lactac` Int32,\r\n    `cieci` Int32,\r\n    `smssenderphone` String,\r\n    `smsrecieverphone` String,\r\n    `smsdirection` Int32,\r\n    `smscode` Int32,\r\n    `smstime` Int64,\r\n    `smslength` Int32,\r\n    `smscontent` String,\r\n    `pdate` Date,\r\n    `srcip` String,\r\n    `destip` String,\r\n    `nesrcip` String,\r\n    `nedestip` String,\r\n    `mcc` Int32,\r\n    `mnc` Int32\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/xdr_fullaudit_monitor_local', '{replica}')\r\nPARTITION BY pdate\r\nORDER BY (gathertime, srcip4)\r\nSETTINGS index_granularity = 8192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec.\r\n```\r\n\r\nquery result: different results returned\r\n```sql\r\nlocalhost :) select imsi, count(), toDateTime(today())\r\n:-] from xdr_fullaudit_monitor\r\n:-] where phonenumber='8613910000009'\r\n:-] and toDate(gathertime) < '2022-06-24'\r\n:-] group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < '2022-06-24')\r\nGROUP BY imsi\r\n\r\nQuery id: 40159474-b3dd-4559-bb8a-4b4e10447ac6\r\n\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.049 sec. Processed 1.01 million rows, 54.71 MB (20.68 million rows/s., 1.12 GB/s.)\r\n\r\nlocalhost :) select imsi, count(), toDateTime(today())\r\n:-] from xdr_fullaudit_monitor\r\n:-] where phonenumber='8613910000009'\r\n:-] and toDate(gathertime) < today()\r\n:-] group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < today())\r\nGROUP BY imsi\r\n\r\nQuery id: 179157c5-37db-4c50-b6f7-031262ad814f\r\n\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.029 sec. Processed 521.39 thousand rows, 28.14 MB (18.23 million rows/s., 984.12 MB/s.)\r\n\r\nlocalhost :)\r\n```\r\n\r\nexpectation: consistent value returned\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38375/comments",
    "author": "iriszhang1121",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2022-06-24T09:05:54Z",
        "body": "Do \r\n\r\n```\r\nSET send_logs_level='trace' \r\n```\r\n\r\nafter that repeat your queries, and share the output (text please, no  screenshots)."
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T09:24:46Z",
        "body": "```sql\r\nlocalhost :) SET send_logs_level='trace' ;\r\n\r\nSET send_logs_level = 'trace'\r\n\r\nQuery id: 7ad97f44-0f67-412f-aef1-48347feb023b\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nlocalhost :) select imsi, count(), toDateTime(today())\u3000from xdr_fullaudit_monitor\u3000where phonenumber='8613910000009'\u3000and toDate(gathertime) < '2022-06-24'\u3000group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < '2022-06-24')\r\nGROUP BY imsi\r\n\r\nQuery id: f42408de-1d16-40c6-aa4f-4c21020a1f94\r\n\r\n[localhost.localdomain] 2022.06.24 17:23:27.669378 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> executeQuery: (from 192.168.70.74:42489, user: ck) select imsi, count(), toDateTime(today()) from xdr_fullaudit_monitor where phonenumber='8613910000009' and toDate(gathertime) < '2022-06-24' group by imsi;\r\n[localhost.localdomain] 2022.06.24 17:23:27.793402 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:27.795122 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:27.796139 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < '2022-06-24'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 17:23:27.796855 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 17:23:27.797026 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 17:23:27.797183 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 17:23:27.797697 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 19166]), unknown, and, (toDate(column 0) in (-inf, 19166]), and\r\n[localhost.localdomain] 2022.06.24 17:23:27.798080 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 17:23:27.798349 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798342 [ 11063 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798376 [ 22098 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_186_38 with 4 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798491 [ 11078 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_12 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798491 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798695 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798712 [ 11063 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798741 [ 11078 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798785 [ 22098 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798791 [ 11049 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:27.798963 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 10/10 parts by partition key, 8 parts by primary key, 70/72 marks by primary key, 70 marks to read from 8 ranges\r\n[localhost.localdomain] 2022.06.24 17:23:27.799331 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2022.06.24 17:23:27.812845 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.812890 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.812937 [ 11047 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.812978 [ 11047 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.813546 [ 11053 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.813573 [ 11053 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.815083 [ 11044 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:27.815115 [ 11044 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:27.816556 [ 11047 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 886 to 1 rows (from 20.77 KiB) in 0.009364386 sec. (94613.785 rows/sec., 2.17 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:27.817786 [ 11044 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.010620213 sec. (2448.162 rows/sec., 57.38 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.371083 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> executeQuery: (from 192.168.70.77:44342, user: ck, initial_query_id: f42408de-1d16-40c6-aa4f-4c21020a1f94) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < '2022-06-24') GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 17:23:27.819509 [ 11053 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 97 to 1 rows (from 2.27 KiB) in 0.012371923 sec. (7840.333 rows/sec., 183.76 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:27.826737 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.019616167 sec. (2395.983 rows/sec., 56.16 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:27.826802 [ 11038 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 05:55:36.372256 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < '2022-06-24'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 05:55:36.372702 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 05:55:36.372899 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 05:55:36.373453 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 19166]), unknown, and, (toDate(column 0) in (-inf, 19166]), and\r\n[localhost.localdomain] 2021.04.14 05:55:36.463249 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 05:55:36.484728 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 3 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484752 [ 19829 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 22 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484872 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 4 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484903 [ 19829 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 7 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484982 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.484997 [ 19829 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.485070 [ 19702 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 46 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.485060 [ 18981 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:55:36.485318 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 6 parts by primary key, 63/65 marks by primary key, 63 marks to read from 6 ranges\r\n[localhost.localdomain] 2021.04.14 05:55:36.485688 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Reading approx. 491993 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 05:55:36.491037 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 05:55:36.491109 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 05:55:36.493944 [ 17595 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 05:55:36.493998 [ 17595 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 05:55:36.500811 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 05:55:36.500881 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 05:55:36.502973 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 705 to 1 rows (from 16.52 KiB) in 0.016656931 sec. (42324.724 rows/sec., 991.99 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.503033 [ 18912 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.016718803 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.514505 [ 17595 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 153 to 1 rows (from 3.59 KiB) in 0.028188772 sec. (5427.693 rows/sec., 127.21 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.519732 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> AggregatingTransform: Aggregated. 86 to 1 rows (from 2.02 KiB) in 0.033383269 sec. (2576.141 rows/sec., 60.38 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:55:36.519757 [ 19082 ] {634e7358-fb00-4e86-963c-9705be98b623} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 05:55:36.521220 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Information> executeQuery: Read 491993 rows, 25.34 MiB in 0.150027678 sec., 3279348 rows/sec., 168.88 MiB/sec.\r\n[localhost.localdomain] 2021.04.14 05:55:36.521332 [ 25115 ] {634e7358-fb00-4e86-963c-9705be98b623} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 17:23:27.969174 [ 11033 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 17:23:27.969311 [ 11033 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 7.074e-05 sec. (14136.274 rows/sec., 441.76 KiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n[localhost.localdomain] 2022.06.24 17:23:27.984527 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Information> executeQuery: Read 1013387 rows, 52.18 MiB in 0.31498153 sec., 3217290 rows/sec., 165.65 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 17:23:27.984622 [ 14291 ] {f42408de-1d16-40c6-aa4f-4c21020a1f94} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.318 sec. Processed 1.01 million rows, 54.71 MB (3.19 million rows/s., 172.25 MB/s.)\r\n```\r\n\r\n```sql\r\nlocalhost :) select imsi, count(), toDateTime(today())\u3000from xdr_fullaudit_monitor\u3000where phonenumber='8613910000009'\u3000and toDate(gathertime) < today()\u3000group by imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < today())\r\nGROUP BY imsi\r\n\r\nQuery id: bee7b3ed-3ea8-4cbb-b914-2494815fb3a4\r\n\r\n[localhost.localdomain] 2022.06.24 17:23:34.173028 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> executeQuery: (from 192.168.70.74:42489, user: ck) select imsi, count(), toDateTime(today()) from xdr_fullaudit_monitor where phonenumber='8613910000009' and toDate(gathertime) < today() group by imsi;\r\n[localhost.localdomain] 2022.06.24 17:23:34.174579 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:34.175579 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 17:23:34.176518 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < today()\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 17:23:34.176904 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 17:23:34.177044 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 17:23:34.177185 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 17:23:34.177728 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 19166]), unknown, and, (toDate(column 0) in (-inf, 19166]), and\r\n[localhost.localdomain] 2022.06.24 17:23:34.178077 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 17:23:34.178298 [ 11095 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178302 [ 11062 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178313 [ 11068 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_186_38 with 4 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178361 [ 11074 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_12 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178478 [ 11068 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178548 [ 11095 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178600 [ 11068 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178588 [ 11062 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178684 [ 11074 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178673 [ 11062 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 17:23:34.178884 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 10/10 parts by partition key, 8 parts by primary key, 70/72 marks by primary key, 70 marks to read from 8 ranges\r\n[localhost.localdomain] 2022.06.24 17:23:34.179296 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2022.06.24 17:23:34.185100 [ 11087 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.185131 [ 11087 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.185845 [ 11067 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.185881 [ 11067 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.189158 [ 11087 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 886 to 1 rows (from 20.77 KiB) in 0.009313853 sec. (95127.119 rows/sec., 2.18 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 05:51:17.854372 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> executeQuery: (from 192.168.70.77:44493, user: ck, initial_query_id: bee7b3ed-3ea8-4cbb-b914-2494815fb3a4) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDate(gathertime) < today()) GROUP BY imsi\r\n[localhost.localdomain] 2021.04.14 05:51:17.855652 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDate(gathertime) < today()\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 05:51:17.856176 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 05:51:17.856394 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 05:51:17.857024 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Key condition: (toDate(column 0) in (-inf, 18730]), unknown, and, (toDate(column 0) in (-inf, 18730]), and\r\n[localhost.localdomain] 2021.04.14 05:51:17.857514 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 05:51:17.857762 [ 17820 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857803 [ 17817 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857781 [ 23460 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857872 [ 17820 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857900 [ 23460 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857962 [ 17820 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857942 [ 17817 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.857996 [ 23460 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 05:51:17.858295 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Selected 8/8 parts by partition key, 0 parts by primary key, 0/65 marks by primary key, 0 marks to read from 0 ranges\r\n[localhost.localdomain] 2021.04.14 05:51:17.858954 [ 23505 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.000386176 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 05:51:17.858973 [ 23505 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 05:51:17.859760 [ 25670 ] {0b7a147b-165b-4e2f-ab2d-76327caec750} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 17:23:34.189950 [ 11056 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.189974 [ 11056 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.192728 [ 11056 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.012888998 sec. (2017.224 rows/sec., 47.28 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:34.192884 [ 11067 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 97 to 1 rows (from 2.27 KiB) in 0.013075659 sec. (7418.364 rows/sec., 173.87 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:34.195163 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 17:23:34.195193 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 17:23:34.212035 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.03221143 sec. (1459.109 rows/sec., 34.20 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 17:23:34.212064 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2022.06.24 17:23:34.212248 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 17:23:34.212306 [ 22099 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 2.8162e-05 sec. (35508.842 rows/sec., 1.08 MiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n[localhost.localdomain] 2022.06.24 17:23:34.213545 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Information> executeQuery: Read 521394 rows, 26.84 MiB in 0.040438327 sec., 12893560 rows/sec., 663.70 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 17:23:34.213640 [ 14291 ] {bee7b3ed-3ea8-4cbb-b914-2494815fb3a4} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.044 sec. Processed 521.39 thousand rows, 28.14 MB (11.85 million rows/s., 639.84 MB/s.)\r\n\r\nlocalhost :)\r\n```"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T09:54:32Z",
        "body": "> Do\r\n> \r\n> ```\r\n> SET send_logs_level='trace' \r\n> ```\r\n> \r\n> after that repeat your queries, and share the output (text please, no screenshots).\r\n\r\nSorry for the inconvenience caused by screenshots. Already updated."
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T12:49:05Z",
        "body": ">    `gathertime` Int64,\r\n\r\nWhat do you store in `gathertime` ? Number of days after 1970-01-01 ?  19166 ?"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:02:18Z",
        "body": "> > `gathertime` Int64,\r\n> \r\n> What do you store in `gathertime` ? Number of days after 1970-01-01 ? 19166 ?\r\n\r\n```sql\r\nlocalhost :) SELECT\r\n:-]     min(gathertime), max(gathertime)\r\n:-] FROM xdr_fullaudit_monitor\r\n:-] WHERE (phonenumber = '8613910000009');\r\n\r\nSELECT\r\n    min(gathertime),\r\n    max(gathertime)\r\nFROM xdr_fullaudit_monitor\r\nWHERE phonenumber = '8613910000009'\r\n\r\nQuery id: 6551ce6c-2265-4e52-bd92-f5272e036742\r\n\r\n\u250c\u2500min(gathertime)\u2500\u252c\u2500max(gathertime)\u2500\u2510\r\n\u2502      1655776844 \u2502      1656035304 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.030 sec. Processed 1.01 million rows, 26.60 MB (34.10 million rows/s., 894.88 MB/s.)\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T14:15:05Z",
        "body": "try `(toDate(toDateTime(gathertime))` instead of `(toDate(gathertime)`"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:16:47Z",
        "body": "> try `(toDate(toDateTime(gathertime))` instead of `(toDate(gathertime)`\r\n\r\n```sql\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < '2022-06-24 00:00:00')\r\nGROUP BY imsi\r\n\r\nQuery id: 01207a60-d984-496f-b2e5-e97a1ddd970b\r\n\r\n[localhost.localdomain] 2022.06.24 22:10:48.285841 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < '2022-06-24 00:00:00') GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:10:48.288148 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:48.289361 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:48.290817 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < '2022-06-24 00:00:00'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:10:48.291183 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:10:48.291339 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:10:48.291623 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:10:48.292359 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1655999999]), unknown, and, (toDateTime(column 0) in (-inf, 1655999999]), and\r\n[localhost.localdomain] 2022.06.24 22:10:48.293721 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:10:48.293915 [ 11023 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.293923 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.293975 [ 11079 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294139 [ 11023 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_186_39 with 24 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294170 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294291 [ 11023 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294313 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294451 [ 11036 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294499 [ 11079 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 22:10:48.294729 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:10:48.295070 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2022.06.24 22:10:48.300500 [ 11101 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.300551 [ 11101 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.302194 [ 11025 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.302262 [ 11025 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.302346 [ 11025 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 8 to 1 rows (from 192.00 B) in 0.006671096 sec. (1199.203 rows/sec., 28.11 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:48.302605 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.302646 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.304267 [ 11019 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:48.304315 [ 11019 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:48.306937 [ 11019 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.011292142 sec. (2302.486 rows/sec., 53.96 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.971501 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> executeQuery: (from 192.168.70.77:60182, user: ck, initial_query_id: 01207a60-d984-496f-b2e5-e97a1ddd970b) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < '2022-06-24 00:00:00') GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:10:48.309218 [ 11101 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 975 to 1 rows (from 22.85 KiB) in 0.013550795 sec. (71951.498 rows/sec., 1.65 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:48.315363 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.019682012 sec. (2387.967 rows/sec., 55.97 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:48.315384 [ 11071 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:38:31.973452 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < '2022-06-24 00:00:00'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:38:31.974396 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:38:31.974691 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:38:31.975607 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1655999999]), unknown, and, (toDateTime(column 0) in (-inf, 1655999999]), and\r\n[localhost.localdomain] 2021.04.14 10:38:31.976026 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:38:31.976286 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 22 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976298 [ 23484 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 3 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976326 [ 23492 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 4 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976447 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 7 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976490 [ 23492 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976540 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976683 [ 23450 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.976516 [ 23511 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 46 steps\r\n[localhost.localdomain] 2021.04.14 10:38:31.977041 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Selected 8/8 parts by partition key, 6 parts by primary key, 63/65 marks by primary key, 63 marks to read from 6 ranges\r\n[localhost.localdomain] 2021.04.14 10:38:31.977720 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> falcon0325.xdr_fullaudit_monitor_local (840772bd-5435-4788-8407-72bd5435f788) (SelectExecutor): Reading approx. 491993 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:38:31.981474 [ 23512 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.981538 [ 23512 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.982765 [ 23497 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.982800 [ 23497 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.983295 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.983318 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.985750 [ 23479 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:38:31.985869 [ 23479 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:38:31.986257 [ 23512 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 705 to 1 rows (from 16.52 KiB) in 0.007242869 sec. (97337.119 rows/sec., 2.23 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.987588 [ 23479 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 23 to 1 rows (from 552.00 B) in 0.008576776 sec. (2681.660 rows/sec., 62.85 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.988711 [ 23497 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 91 to 1 rows (from 2.13 KiB) in 0.009722873 sec. (9359.374 rows/sec., 219.36 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.999838 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> AggregatingTransform: Aggregated. 125 to 1 rows (from 2.93 KiB) in 0.020862252 sec. (5991.683 rows/sec., 140.43 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:38:31.999870 [ 17818 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:38:32.001901 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Information> executeQuery: Read 491993 rows, 25.34 MiB in 0.030313373 sec., 16230229 rows/sec., 835.82 MiB/sec.\r\n[localhost.localdomain] 2021.04.14 10:38:32.001994 [ 23946 ] {826bbe51-52dd-4f56-8ac4-db1455f6a1ac} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 22:10:48.330437 [ 11026 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:10:48.330591 [ 11026 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 5.7341e-05 sec. (17439.528 rows/sec., 544.99 KiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2190 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2196 Progress: 1.01 million rows, 54.71 MB (20.21 million rows/s., 1.09 GB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:10:48.332382 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Information> executeQuery: Read 1013387 rows, 52.18 MiB in 0.046429682 sec., 21826274 rows/sec., 1.10 GiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:10:48.332568 [ 16980 ] {01207a60-d984-496f-b2e5-e97a1ddd970b} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.050 sec. Processed 1.01 million rows, 54.71 MB (20.12 million rows/s., 1.09 GB/s.)\r\n```\r\n\r\n```sql\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < toDateTime(today()))\r\nGROUP BY imsi\r\n\r\nQuery id: 763fd9d2-369a-4a30-b9c5-220b9989d9f3\r\n\r\n[localhost.localdomain] 2022.06.24 22:10:54.336773 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < toDateTime(today())) GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:10:54.338210 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:54.339737 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:10:54.340636 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < toDateTime(today())\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:10:54.341057 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:10:54.341218 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:10:54.341392 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:10:54.341966 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1655999999]), unknown, and, (toDateTime(column 0) in (-inf, 1655999999]), and\r\n[localhost.localdomain] 2022.06.24 22:10:54.342294 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:10:54.342574 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_81_81_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342572 [ 11061 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_80_41 with 6 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342541 [ 11048 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_187_187_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342620 [ 11055 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_186_39 with 24 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342792 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342868 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_62_68_2 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342884 [ 11061 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_31_7 with 37 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342866 [ 11048 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220623_32_61_6 with 23 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.342948 [ 2976 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2022.06.24 22:10:54.343196 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:10:54.343554 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:43:02.899896 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> executeQuery: (from 192.168.70.77:59473, user: ck, initial_query_id: 763fd9d2-369a-4a30-b9c5-220b9989d9f3) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (toDateTime(gathertime) < toDateTime(today())) GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:10:54.349894 [ 11077 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.349936 [ 11077 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.349972 [ 11053 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.350058 [ 11053 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.350232 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.350257 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.351914 [ 11077 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 370 to 1 rows (from 8.67 KiB) in 0.007825998 sec. (47278.315 rows/sec., 1.08 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:43:02.901143 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"toDateTime(gathertime) < toDateTime(today())\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:43:02.901724 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:43:02.901857 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:43:02.902526 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: (toDateTime(column 0) in (-inf, 1618329599]), unknown, and, (toDateTime(column 0) in (-inf, 1618329599]), and\r\n[localhost.localdomain] 2021.04.14 10:43:02.902908 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:43:02.903159 [ 18980 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220621_0_79_18 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903160 [ 19829 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_0_56_11 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903184 [ 19084 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_0_44_8 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903192 [ 19083 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220622_57_199_30 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903259 [ 18980 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_45_73_14 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903479 [ 19083 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_3_3_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903479 [ 19084 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220623_74_74_0 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903465 [ 19829 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Used generic exclusion search over index for part 20220624_0_2_1 with 1 steps\r\n[localhost.localdomain] 2021.04.14 10:43:02.903709 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 0 parts by primary key, 0/65 marks by primary key, 0 marks to read from 0 ranges\r\n[localhost.localdomain] 2021.04.14 10:43:02.904263 [ 21674 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.000352989 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 10:43:02.904307 [ 21674 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:43:02.904942 [ 25115 ] {66d5fc07-be72-4901-87c3-7c5a7e974e71} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 22:10:54.357201 [ 11053 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 613 to 1 rows (from 14.37 KiB) in 0.013134431 sec. (46671.226 rows/sec., 1.07 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:54.357597 [ 11076 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:10:54.357641 [ 11076 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:10:54.360258 [ 11076 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.016166042 sec. (1608.310 rows/sec., 37.69 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:54.363220 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.01917536 sec. (2451.062 rows/sec., 57.45 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:10:54.363250 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2022.06.24 22:10:54.363453 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:10:54.363517 [ 22082 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 2.9546e-05 sec. (33845.529 rows/sec., 1.03 MiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2191 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2197 Progress: 521.39 thousand rows, 28.14 MB (16.87 million rows/s., 910.41 MB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:10:54.364907 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Information> executeQuery: Read 521394 rows, 26.84 MiB in 0.028065154 sec., 18577984 rows/sec., 956.31 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:10:54.365003 [ 16980 ] {763fd9d2-369a-4a30-b9c5-220b9989d9f3} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n\r\n1 rows in set. Elapsed: 0.031 sec. Processed 521.39 thousand rows, 28.14 MB (16.74 million rows/s., 903.42 MB/s.)\r\n\r\nlocalhost :) \r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T14:17:09Z",
        "body": "BTW\r\n\r\n    `srcip4` Int64,    ---- should be UInt32\r\n    `srcport` Int32,   ---- should be UInt16\r\n    `destip4` Int64,   ---- should be UInt32\r\n    `destport` Int32,   --- should be UInt16\r\n    `gathertime` Int64,   ---- should be UInt32\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T14:22:23Z",
        "body": "Try\r\n\r\n```sql\r\ngathertime < toInt64(toDateTime('2022-06-24 00:00:00'))\r\n\r\ngathertime < toInt64(toDateTime(today()))\r\n```\r\n"
      },
      {
        "user": "iriszhang1121",
        "created_at": "2022-06-24T14:24:47Z",
        "body": "> ```sql\r\n> gathertime < toInt64(toDateTime(today()))\r\n> ```\r\n\r\n```sql\r\n[BEGIN] 2022/6/24 22:23:29\r\nSELECT\r\n:-]     imsi,\r\n:-]     count(),\r\n:-]     toDateTime(today())\r\n:-] FROM xdr_fullaudit_monitor\r\n:-] WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime('2022-06-24 00:00:00'))\r\n:-] GROUP BY imsi;GROUP BY imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime('2022-06-24 00:00:00')))\r\nGROUP BY imsi\r\n\r\nQuery id: 0c8a8095-b9be-4db7-94ad-3a583cb221fd\r\n\r\n[localhost.localdomain] 2022.06.24 22:23:02.045340 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime('2022-06-24 00:00:00')) GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:23:02.046777 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:02.047504 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:02.048228 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:23:02.048615 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:23:02.048851 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:23:02.049003 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:23:02.049596 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:02.049957 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:02.050122 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_81_81_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050112 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_0_80_41 (6 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050129 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_187_187_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050148 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050161 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050159 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050172 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050198 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 6\r\n[localhost.localdomain] 2022.06.24 22:23:02.050185 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050219 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050235 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 5 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050249 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050333 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_0_186_39 (18 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050372 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_0_31_7 (30 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050383 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_32_61_6 (16 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050420 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050402 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050450 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050486 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 30\r\n[localhost.localdomain] 2022.06.24 22:23:02.050535 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050540 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 16\r\n[localhost.localdomain] 2022.06.24 22:23:02.050539 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 18\r\n[localhost.localdomain] 2022.06.24 22:23:02.050603 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 8 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050627 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_62_68_2 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050621 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050663 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:02.050709 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050727 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050760 [ 11092 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050763 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:02.050777 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:02.050852 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.050846 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:02.050898 [ 3060 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.050932 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:02.051002 [ 22099 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:02.051238 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:23:02.051525 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:55:10.607619 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> executeQuery: (from 192.168.70.77:59473, user: ck, initial_query_id: 0c8a8095-b9be-4db7-94ad-3a583cb221fd) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime('2022-06-24 00:00:00'))) GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:23:02.055867 [ 11037 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.055909 [ 11037 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.057562 [ 11054 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.057601 [ 11054 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.057679 [ 11037 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 370 to 1 rows (from 8.67 KiB) in 0.005648689 sec. (65501.924 rows/sec., 1.50 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.058126 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.058154 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.058173 [ 11089 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:02.058207 [ 11089 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:02.061850 [ 11089 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.009813103 sec. (2649.519 rows/sec., 62.10 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.064742 [ 11054 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 613 to 1 rows (from 14.37 KiB) in 0.01273508 sec. (48134.758 rows/sec., 1.10 MiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.066856 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.014862219 sec. (3162.381 rows/sec., 74.12 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:02.066875 [ 11020 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:55:10.608935 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:55:10.609372 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:55:10.609584 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:55:10.610224 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:10.610691 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:10.610921 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220621_0_79_18 (3 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.610960 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.610928 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_0_56_11 (15 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.610964 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_0_44_8 (38 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611005 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 3\r\n[localhost.localdomain] 2021.04.14 10:55:10.610958 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_57_199_30 (4 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611047 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 3 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611042 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611035 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611074 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611122 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 38\r\n[localhost.localdomain] 2021.04.14 10:55:10.611124 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 15\r\n[localhost.localdomain] 2021.04.14 10:55:10.611176 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 11 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611144 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 4\r\n[localhost.localdomain] 2021.04.14 10:55:10.611209 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 7 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611247 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 4 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611396 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_74_74_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611426 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_45_73_14 (7 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611418 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611476 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611430 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:10.611482 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2021.04.14 10:55:10.611510 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:10.611531 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 7\r\n[localhost.localdomain] 2021.04.14 10:55:10.611497 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:10.611515 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:10.611589 [ 21674 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 5 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611573 [ 17595 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611591 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:10.611587 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:10.611698 [ 19829 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.611697 [ 19083 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:10.612045 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 6 parts by primary key, 63/65 marks by primary key, 63 marks to read from 6 ranges\r\n[localhost.localdomain] 2021.04.14 10:55:10.612370 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Reading approx. 491993 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:55:10.616094 [ 19081 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.616143 [ 19081 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.617472 [ 19828 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.617500 [ 19828 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.617593 [ 19827 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.617655 [ 19827 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.618410 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2021.04.14 10:55:10.618478 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2021.04.14 10:55:10.618796 [ 19828 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 23 to 1 rows (from 552.00 B) in 0.005875676 sec. (3914.443 rows/sec., 91.74 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.619917 [ 19081 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 705 to 1 rows (from 16.52 KiB) in 0.007012315 sec. (100537.412 rows/sec., 2.30 MiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.624391 [ 19827 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 91 to 1 rows (from 2.13 KiB) in 0.011497861 sec. (7914.516 rows/sec., 185.50 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.630753 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> AggregatingTransform: Aggregated. 125 to 1 rows (from 2.93 KiB) in 0.017862316 sec. (6997.973 rows/sec., 164.01 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:10.630771 [ 19082 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:55:10.633439 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Information> executeQuery: Read 491993 rows, 19.27 MiB in 0.025711574 sec., 19135079 rows/sec., 749.33 MiB/sec.\r\n[localhost.localdomain] 2021.04.14 10:55:10.633533 [ 25115 ] {0538d035-c00e-40c0-a07d-0ef6a2f45262} <Debug> MemoryTracker: Peak memory usage (for query): 8.29 MiB.\r\n[localhost.localdomain] 2022.06.24 22:23:02.081227 [ 11098 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:23:02.081303 [ 11098 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 3.213e-05 sec. (31123.561 rows/sec., 972.61 KiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    2000 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2192 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2198 Progress: 1.01 million rows, 40.26 MB (23.98 million rows/s., 952.67 MB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:23:02.083091 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Information> executeQuery: Read 1013387 rows, 38.40 MiB in 0.037657394 sec., 26910704 rows/sec., 1019.70 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:23:02.083183 [ 16980 ] {0c8a8095-b9be-4db7-94ad-3a583cb221fd} <Debug> MemoryTracker: Peak memory usage (for query): 9.10 MiB.\r\n\r\n1 rows in set. Elapsed: 0.042 sec. Processed 1.01 million rows, 40.26 MB (23.84 million rows/s., 947.08 MB/s.)\r\n\r\nlocalhost :) SELECT\r\n:-]     imsi,\r\n:-]     count(),\r\n:-]     toDateTime(today())\r\n:-] FROM xdr_fullaudit_monitor\r\n:-] WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime(today()))\r\n:-] GROUP BY imsi;GROUP BY imsi;\r\n\r\nSELECT\r\n    imsi,\r\n    count(),\r\n    toDateTime(today())\r\nFROM xdr_fullaudit_monitor\r\nWHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime(today())))\r\nGROUP BY imsi\r\n\r\nQuery id: 2d446fff-6daa-40e8-863e-b25bbb4a8e00\r\n\r\n[localhost.localdomain] 2022.06.24 22:23:14.195639 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> executeQuery: (from 192.168.70.74:43588, user: ck) SELECT imsi, count(), toDateTime(today()) FROM xdr_fullaudit_monitor WHERE (phonenumber = '8613910000009') AND gathertime < toInt64(toDateTime(today())) GROUP BY imsi;\r\n[localhost.localdomain] 2022.06.24 22:23:14.197818 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:14.199770 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor\r\n[localhost.localdomain] 2022.06.24 22:23:14.200721 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2022.06.24 22:23:14.201161 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2022.06.24 22:23:14.201313 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2022.06.24 22:23:14.201522 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> InterpreterSelectQuery: WithMergeableState -> Complete\r\n[localhost.localdomain] 2022.06.24 22:23:14.202061 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:14.202412 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2022.06.24 22:23:14.202612 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_81_81_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202616 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_187_187_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202638 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202625 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220621_0_80_41 (6 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202638 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202658 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.202678 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.202683 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202682 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202701 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202713 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 6\r\n[localhost.localdomain] 2022.06.24 22:23:14.202731 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 5 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202768 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220622_0_186_39 (18 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202798 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202804 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_32_61_6 (16 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202826 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202821 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 18\r\n[localhost.localdomain] 2022.06.24 22:23:14.202798 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_0_31_7 (30 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202845 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 16\r\n[localhost.localdomain] 2022.06.24 22:23:14.202846 [ 22080 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202862 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 8 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202864 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202906 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220623_62_68_2 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202924 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[localhost.localdomain] 2022.06.24 22:23:14.202908 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 30\r\n[localhost.localdomain] 2022.06.24 22:23:14.202939 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.202939 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.202954 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 2 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202967 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:14.202953 [ 11077 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found continuous range in 9 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.202987 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.203008 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2022.06.24 22:23:14.203010 [ 11090 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.203029 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2022.06.24 22:23:14.203042 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2022.06.24 22:23:14.203057 [ 11052 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2022.06.24 22:23:14.203254 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Selected 9/9 parts by partition key, 7 parts by primary key, 69/71 marks by primary key, 69 marks to read from 7 ranges\r\n[localhost.localdomain] 2022.06.24 22:23:14.203793 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> falcon0325.xdr_fullaudit_monitor_local (27c46ef1-003a-4001-a7c4-6ef1003a6001) (SelectExecutor): Reading approx. 521394 rows with 4 streams\r\n[localhost.localdomain] 2021.04.14 10:55:22.762581 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> executeQuery: (from 192.168.70.77:59473, user: ck, initial_query_id: 2d446fff-6daa-40e8-863e-b25bbb4a8e00) SELECT imsi, count(), toDateTime(today()) FROM falcon0325.xdr_fullaudit_monitor_local WHERE (phonenumber = '8613910000009') AND (gathertime < toInt64(toDateTime(today()))) GROUP BY imsi\r\n[localhost.localdomain] 2022.06.24 22:23:14.210527 [ 2350 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.210562 [ 2350 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.212659 [ 11070 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.212702 [ 11070 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.212703 [ 11085 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.212764 [ 11085 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.213696 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> AggregatingTransform: Aggregating\r\n[localhost.localdomain] 2022.06.24 22:23:14.213739 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Aggregation method: key_string\r\n[localhost.localdomain] 2022.06.24 22:23:14.214211 [ 11085 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 370 to 1 rows (from 8.67 KiB) in 0.009333972 sec. (39640.145 rows/sec., 929.07 KiB/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:22.763663 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"phonenumber = '8613910000009'\" moved to PREWHERE\r\n[localhost.localdomain] 2021.04.14 10:55:22.764097 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> ContextAccess (ck): Access granted: SELECT(gathertime, phonenumber, imsi) ON falcon0325.xdr_fullaudit_monitor_local\r\n[localhost.localdomain] 2021.04.14 10:55:22.764250 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> InterpreterSelectQuery: FetchColumns -> WithMergeableState\r\n[localhost.localdomain] 2021.04.14 10:55:22.764729 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Key condition: unknown, (column 0 in (-inf, 1618329599]), and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:22.765075 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): MinMax index condition: unknown, unknown, and, unknown, and\r\n[localhost.localdomain] 2021.04.14 10:55:22.765290 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_0_44_8 (38 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765280 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220621_0_79_18 (3 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765308 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_57_199_30 (4 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765315 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 37\r\n[localhost.localdomain] 2021.04.14 10:55:22.765317 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220622_0_56_11 (15 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765335 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 38\r\n[localhost.localdomain] 2021.04.14 10:55:22.765331 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 3\r\n[localhost.localdomain] 2021.04.14 10:55:22.765354 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 6 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765345 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 14\r\n[localhost.localdomain] 2021.04.14 10:55:22.765358 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 4\r\n[localhost.localdomain] 2021.04.14 10:55:22.765322 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765373 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 15\r\n[localhost.localdomain] 2021.04.14 10:55:22.765383 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 2 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765392 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 4 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765386 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 3\r\n[localhost.localdomain] 2021.04.14 10:55:22.765413 [ 18911 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 2 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765434 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_45_73_14 (7 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765464 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 6\r\n[localhost.localdomain] 2021.04.14 10:55:22.765499 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 7\r\n[localhost.localdomain] 2021.04.14 10:55:22.765515 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 3 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765647 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_0_2_1 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765657 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220624_3_3_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765669 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Running binary search on index range for part 20220623_74_74_0 (2 marks)\r\n[localhost.localdomain] 2021.04.14 10:55:22.765682 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:22.765675 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:22.765700 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765704 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (LEFT) boundary mark: 1\r\n[localhost.localdomain] 2021.04.14 10:55:22.765720 [ 18136 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765722 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765756 [ 18981 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.765708 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[localhost.localdomain] 2021.04.14 10:55:22.765795 [ 19702 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Found empty range in 1 steps\r\n[localhost.localdomain] 2021.04.14 10:55:22.766024 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> falcon0325.xdr_fullaudit_monitor_local (3f87dfe0-c3a7-4aa0-bf87-dfe0c3a73aa0) (SelectExecutor): Selected 8/8 parts by partition key, 0 parts by primary key, 0/65 marks by primary key, 0 marks to read from 0 ranges\r\n[localhost.localdomain] 2021.04.14 10:55:22.766662 [ 19084 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> AggregatingTransform: Aggregated. 0 to 0 rows (from 0.00 B) in 0.000411476 sec. (0.000 rows/sec., 0.00 B/sec.)\r\n[localhost.localdomain] 2021.04.14 10:55:22.766680 [ 19084 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2021.04.14 10:55:22.767622 [ 25115 ] {c5b4b561-cda0-4aff-a9df-441141b05654} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n[localhost.localdomain] 2022.06.24 22:23:14.216135 [ 11070 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 26 to 1 rows (from 624.00 B) in 0.011253869 sec. (2310.317 rows/sec., 54.15 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:14.219585 [ 2350 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 47 to 1 rows (from 1.10 KiB) in 0.014760929 sec. (3184.081 rows/sec., 74.63 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:14.221452 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> AggregatingTransform: Aggregated. 613 to 1 rows (from 14.37 KiB) in 0.016595627 sec. (36937.441 rows/sec., 865.72 KiB/sec.)\r\n[localhost.localdomain] 2022.06.24 22:23:14.221472 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Merging aggregated data\r\n[localhost.localdomain] 2022.06.24 22:23:14.221645 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Trace> Aggregator: Merging partially aggregated blocks (bucket = -1).\r\n[localhost.localdomain] 2022.06.24 22:23:14.221705 [ 11064 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> Aggregator: Merged partially aggregated blocks. 1 rows, 32.00 B. in 2.6937e-05 sec. (37123.659 rows/sec., 1.13 MiB/sec.)\r\n\u250c\u2500imsi\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u252c\u2500toDateTime(today())\u2500\u2510\r\n\u2502 460001000000009 \u2502    1056 \u2502 2022-06-24 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                                                                                                                                                                                                          \r\n\u2193 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) \r\n\u2199 Progress: 521.39 thousand rows, 20.06 MB (16.25 million rows/s., 625.29 MB/s.)  99%\r\n[localhost.localdomain] 2022.06.24 22:23:14.223533 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Information> executeQuery: Read 521394 rows, 19.13 MiB in 0.027823368 sec., 18739427 rows/sec., 687.65 MiB/sec.\r\n[localhost.localdomain] 2022.06.24 22:23:14.223627 [ 16980 ] {2d446fff-6daa-40e8-863e-b25bbb4a8e00} <Debug> MemoryTracker: Peak memory usage (for query): 9.10 MiB.\r\n\r\n1 rows in set. Elapsed: 0.032 sec. Processed 521.39 thousand rows, 20.06 MB (16.13 million rows/s., 620.70 MB/s.)\r\n\r\nlocalhost :) \r\n[END] 2022/6/24 22:24:05\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-24T17:55:26Z",
        "body": "Do you have 2 servers/shards?  It seems you have different time\r\n\r\n```\r\ntoInt64(toDateTime('2022-06-24 00:00:00'))\r\nKey condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\nKey condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\n\r\ngathertime < toInt64(toDateTime(today()))\r\nKey condition: unknown, (column 0 in (-inf, 1655999999]), and, unknown, and\r\nKey condition: unknown, (column 0 in (-inf, 1618329599]), and, unknown, and\r\n\r\nSELECT toDateTime(1655999999)\r\n\u250c\u2500toDateTime(1655999999)\u2500\u2510\r\n\u2502    2022-06-23 15:59:59 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT toDateTime(1618329599)\r\n\u250c\u2500toDateTime(1618329599)\u2500\u2510\r\n\u2502    2021-04-13 15:59:59 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\nthe first server calculates `toInt64(toDateTime(today())` to 1655999999 == 2022-06-23 15:59:59\r\nthe second server calculates `toInt64(toDateTime(today())` to 1618329599 == 2021-04-13 15:59:59\r\n"
      }
    ]
  },
  {
    "number": 37682,
    "title": "Get max date - partition key",
    "created_at": "2022-05-31T09:41:43Z",
    "closed_at": "2022-06-01T13:12:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37682",
    "body": "Hi,\r\ncould someone explain me why query like ``SELECT max(date) from db.table`` doesn't use index and isn't instantly while column date is partition key? For example\r\n```\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2502         Indexes:                                                              \u2502\r\n\u2502           MinMax                                                              \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 100/100                                                    \u2502\r\n\u2502             Granules: 61718/61718                                             \u2502\r\n\u2502           Partition                                                           \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 100/100                                                    \u2502\r\n\u2502             Granules: 61718/61718                                             \u2502\r\n\u2502           PrimaryKey                                                          \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 100/100                                                    \u2502\r\n\u2502             Granules: 61718/61718                                             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nDon't understand why it is looking in all partitions. It seems to be obvious where it should look for.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37682/comments",
    "author": "rkozlo",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-05-31T11:29:18Z",
        "body": "It's unclear what is you mean. \r\n`max( partition_key )` uses min_max partition index (virtual projection). \r\nBut it reads all records of min_max index because min_max index is unordered and it needs to scan it to find `max`.\r\n\r\n```\r\ncreate table A( date Date, S String) Engine=MergeTree partition by  date order by S;\r\ninsert into A select today()+1, '' from numbers(1000);\r\n\r\nSELECT max(date) FROM A\r\n\u250c\u2500\u2500max(date)\u2500\u2510\r\n\u2502 2022-06-01 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nEXPLAIN SELECT max(date) FROM A\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                         \u2502\r\n\u2502   SettingQuotaAndLimits (Set limits and quota after reading from storage)           \u2502\r\n\u2502     ReadFromStorage (MergeTree(with Aggregate projection _minmax_count_projection)) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nVS without index\r\n```\r\nSELECT max(identity(date))FROM A\r\n\u250c\u2500max(identity(date))\u2500\u2510\r\n\u2502          2022-06-01 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 row in set. Elapsed: 0.004 sec. Processed 1.00 thousand rows, 2.00 KB (268.62 thousand rows/s., 537.23 KB/s.)\r\n\r\nEXPLAIN SELECT max(identity(date)) FROM A\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree (default.A)                                         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "rkozlo",
        "created_at": "2022-06-01T06:42:45Z",
        "body": "I understand what you mean. I'lll give a an example. Have similar table like you, here is engine definition\r\n`\r\nENGINE = MergeTree                                                                                                                     \r\nPARTITION BY date                                                                                                                      \r\nORDER BY date\r\n`\r\nAnd then the same explains\r\n\r\n```\r\nEXPLAIN\r\nSELECT max(identity(date))\r\nFROM db.table\r\n\r\nQuery id: 9777adee-b92c-438b-9817-10278cda3ffb\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n5 rows in set. Elapsed: 0.002 sec. \r\n\r\nclickhouse :) explain select max(date) from db.table\r\n\r\nEXPLAIN\r\nSELECT max(date)\r\nFROM db.table\r\n\r\nQuery id: 026609f3-8c94-4736-aa00-a6c2d7b60bfa\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n5 rows in set. Elapsed: 0.004 sec. \r\n\r\nclickhouse :) explain indexes=1 select max(date) from db.table\r\n\r\nEXPLAIN indexes = 1\r\nSELECT max(date)\r\nFROM db.table\r\n\r\nQuery id: c13c2808-3019-4b13-b572-79c4e14af255\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                   \u2502\r\n\u2502   Aggregating                                                                 \u2502\r\n\u2502     Expression (Before GROUP BY)                                              \u2502\r\n\u2502       SettingQuotaAndLimits (Set limits and quota after reading from storage) \u2502\r\n\u2502         ReadFromMergeTree                                                     \u2502\r\n\u2502         Indexes:                                                              \u2502\r\n\u2502           MinMax                                                              \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 648/648                                                    \u2502\r\n\u2502             Granules: 75668/75668                                             \u2502\r\n\u2502           Partition                                                           \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 648/648                                                    \u2502\r\n\u2502             Granules: 75668/75668                                             \u2502\r\n\u2502           PrimaryKey                                                          \u2502\r\n\u2502             Condition: true                                                   \u2502\r\n\u2502             Parts: 648/648                                                    \u2502\r\n\u2502             Granules: 75668/75668                                             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n18 rows in set. Elapsed: 0.015 sec.\r\n\r\nclickhouse :) select max(date), min(date) from db.table\r\n\r\nSELECT\r\n    max(date),\r\n    min(date)\r\nFROM db.table\r\n\r\nQuery id: 05131eaa-499b-49d2-ae37-eb68eb3d7688\r\n\r\n\u250c\u2500\u2500max(date)\u2500\u252c\u2500\u2500min(date)\u2500\u2510\r\n\u2502 2021-08-18 \u2502 2020-09-25 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.093 sec. Processed 453.63 million rows, 907.26 MB (4.90 billion rows/s., 9.79 GB/s.)\r\n\r\nclickhouse :) select count() from db.table\r\n\r\nSELECT count()\r\nFROM db.table\r\n\r\nQuery id: 114b4125-a58c-467d-9670-a66f30351bbc\r\n\r\n\u250c\u2500\u2500\u2500count()\u2500\u2510\r\n\u2502 453630412 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nIt is a bit confusing. It goes through all rows in table. Btw clickhouse version is 21.8.15"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-01T13:12:09Z",
        "body": ">Btw clickhouse version is 21.8.15\r\n\r\nVirtual projections were introduced in 22.2 or 22.3. You need to upgrade."
      }
    ]
  },
  {
    "number": 36709,
    "title": "Use named connection in  remote(...) ",
    "created_at": "2022-04-27T14:02:08Z",
    "closed_at": "2022-04-27T14:40:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36709",
    "body": "Can we use named connection in  remote(...) ?\r\n\r\nI try:\r\n`SELECT count() FROM remote(mxch, db='mx_master', table='health_watch');\r\n`\r\nbut get:\r\n\r\n`Code: 36. DB::Exception: Unexpected key-value argument.Got: db, but expected: sharding_key. (BAD_ARGUMENTS) (version 22.3.3.44 (official build))`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36709/comments",
    "author": "oleg-savko",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2022-04-27T14:22:31Z",
        "body": "Please try `database` instead of `db`"
      }
    ]
  },
  {
    "number": 35246,
    "title": "How to compile the clickhouse-v20.11.4.13-stable with gcc and libstdc++ instead of libc++",
    "created_at": "2022-03-13T13:44:29Z",
    "closed_at": "2022-03-13T18:30:39Z",
    "labels": [
      "question",
      "not planned",
      "build",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35246",
    "body": null,
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35246/comments",
    "author": "starrysky9959",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-03-13T18:36:35Z",
        "body": "The main asset of ClickHouse is our set of test suites and continuous integration system.\r\nWith our tests, including randomized tests, we find and fix bugs in most of third-party open-source libraries.\r\n\r\nTo ensure that ClickHouse is stable and secure, the only way is to build it with exactly the same versions of libraries with exactly the same patches as we do.\r\n\r\nIf you don't know, it's difficult to imagine how many bugs (race conditions, memory safety issues) exist in C++ libraries."
      }
    ]
  },
  {
    "number": 29181,
    "title": "Can't import large files to server running under docker: \"Broken pipe, while writing to socket\"",
    "created_at": "2021-09-19T21:58:06Z",
    "closed_at": "2021-09-19T23:42:23Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29181",
    "body": "I've started ClickHouse server as a docker container on my Windows machine:\r\n\r\n`docker run --restart always -d --name ch --ulimit nofile=262144:262144 -p 8123:8123 -p 9000:9000 -p 9009:9009 --volume=/e/ClickHouse:/var/lib/clickhouse yandex/clickhouse-server`\r\n\r\nThen I've opened an Ubuntu session (over WSL2)  and tried to import the data (2.1G csv file):\r\n\r\n`clickhouse-client --query \"INSERT INTO test.time_test FORMAT CSV\" --max_insert_block_size=100000 < /mnt/e/temp/time_test.csv`\r\n\r\nBut it failed:\r\n`Code: 210. DB::NetException: I/O error: Broken pipe, while writing to socket (127.0.0.1:9000)`\r\n\r\nIt reproduces for any file large enough. Tiny files are imported fine. Any ideas what could went wrong and how to diagnose it?\r\n\r\nOS: Windows 10\r\nClickHouse version: 21.9.3.30\r\nClickHouse client version: 18.16.1\r\nDocker Desktop: 20.10.8 (over WSL2)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29181/comments",
    "author": "sogawa-sps",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-19T23:37:08Z",
        "body": "18.16.1 is out of support.\r\nMost probably CH server restarts because of lack of memory.\r\nCheck `/var/log/clickhouse-server/clickhouse-server.log` for more info."
      }
    ]
  },
  {
    "number": 28004,
    "title": "build failed error:FAILED: src/libdbms.so ",
    "created_at": "2021-08-23T02:03:36Z",
    "closed_at": "2021-08-25T10:58:00Z",
    "labels": [
      "question",
      "build",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28004",
    "body": "Hi\uff0cI compile cl on ubuntu 20.04, and always encounter this error. I don\u2019t know why. I also agree with the error on other debians.\r\n\r\n```\r\n12/672] Creating preprocessed file...c/lib/gssapi/krb5/gssapi_err_krb5.c\r\n+ /usr/bin/awk -f /home/hrp/Click/ClickHouse/contrib/krb5/src/util/et/et_h.awk outfile=gssapi_err_krb5.h /home/hrp/Click/ClickHouse/contrib/krb5/src/lib/gssapi/krb5/gssapi_err_krb5.et\r\n+ /usr/bin/awk -f /home/hrp/Click/ClickHouse/contrib/krb5/src/util/et/et_c.awk outfile=gssapi_err_krb5.c textdomain= localedir= /home/hrp/Click/ClickHouse/contrib/krb5/src/lib/gssapi/krb5/gssapi_err_krb5.et\r\n[421/672] Linking CXX shared library src/libdbms.so\r\nFAILED: src/libdbms.so\r\n: && /usr/bin/c++ -fPIC -fdiagnostics-color=always -fsized-deallocation  -\r\n\r\n```\r\n\r\nIs this file missing?  src/libdbms.so\uff1f\r\n\r\nI downloaded the source code  with submodules directly\uff0cis right\uff1f\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28004/comments",
    "author": "rouse2617",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-23T10:31:21Z",
        "body": "Check the amount of free space on your machine."
      },
      {
        "user": "rouse2617",
        "created_at": "2021-08-24T01:15:06Z",
        "body": "Thank you for your suggestion, I tried to expand the memory is compiled, but when packaging deb I encountered this situation, in addition, I would like to ask, packaging deb can be continued, each time have to wait a long time, to the end reported wrong, but also to come again, thank you\r\n\r\nthis is package deb error\r\n```\r\n\r\n/home/hrp/ClickHouse/programs/main.cpp:29:5: error: 'ENABLE_CLICKHOUSE_SERVER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_SERVER\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:32:5: error: 'ENABLE_CLICKHOUSE_CLIENT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_CLIENT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:35:5: error: 'ENABLE_CLICKHOUSE_LOCAL' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_LOCAL\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:38:5: error: 'ENABLE_CLICKHOUSE_BENCHMARK' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_BENCHMARK\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:41:5: error: 'ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:44:5: error: 'ENABLE_CLICKHOUSE_COMPRESSOR' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_COMPRESSOR\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:47:5: error: 'ENABLE_CLICKHOUSE_FORMAT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_FORMAT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:50:5: error: 'ENABLE_CLICKHOUSE_COPIER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_COPIER\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:53:5: error: 'ENABLE_CLICKHOUSE_OBFUSCATOR' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_OBFUSCATOR\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:56:5: error: 'ENABLE_CLICKHOUSE_GIT_IMPORT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_GIT_IMPORT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:59:5: error: 'ENABLE_CLICKHOUSE_KEEPER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_KEEPER\r\n```\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-24T12:38:52Z",
        "body": "This is wrong. These options should be enabled.\r\n\r\nI recommend to:\r\n- do static build (as default) instead of shared;\r\n- use all the defaults, don't specify any parameters;\r\n- if you need to build Debian packages, use `docker/packager`."
      }
    ]
  },
  {
    "number": 26544,
    "title": "Failed to use mysql engine when creating database with mysql engine",
    "created_at": "2021-07-20T03:54:27Z",
    "closed_at": "2021-07-21T13:24:43Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/26544",
    "body": "clickhouse's version: 20.8.3.\r\nwhen i type the sql statment:\r\n**_create database  test ENGINE = MYSQL('172.16.0.55:3306','test','root','passw0rd');_**\r\nbut response message:\r\n_**Received exception from server (version 20.8.3):\r\nCode: 36. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Database engine MYSQL cannot have arguments. \r\n0 rows in set. Elapsed: 0.006 sec.**_ \r\n\r\nso what happened about this?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/26544/comments",
    "author": "prudens",
    "comments": [
      {
        "user": "abyss7",
        "created_at": "2021-07-20T10:18:35Z",
        "body": "The engine name is case-sensitive: we have engine `MySQL`, but not `MYSQL`. The error message is misleading in this way. Try and report, if it helps."
      }
    ]
  },
  {
    "number": 25897,
    "title": "clickhouse-local stops working on 21.4.6.55",
    "created_at": "2021-07-01T14:05:33Z",
    "closed_at": "2021-07-01T19:00:38Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25897",
    "body": "Hello all,\r\n\r\nI cannot run clickhouse-local anymore on version 21.4.6.55 but I can run it without any issue on version 18.6.0.\r\n\r\nQuestion 1> What is the issue here and How I can fix it?\r\n\r\nQuestion 2> Why the clickhouse-local requires to write into config.xml?\r\n\r\nThank you\r\n\r\n```\r\n$ echo -e \"1,2\\n3,4\" | clickhouse-local --structure \"a Int64, b Int64\" --input-format \"CSV\" --query \"SELECT * FROM table\"\r\nProcessing configuration file 'config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nCouldn't save preprocessed config to /var/lib/clickhouse/preprocessed_configs/config.xml: Access to file denied: /var/lib/clickhouse/preprocessed_configs/config.xml\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nPoco::Exception. Code: 1000, e.code() = 13, e.displayText() = Access to file denied: /var/log/clickhouse-server/clickhouse-server.log, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x12683f3c in /usr/bin/clickhouse\r\n1. Poco::FileStreamBuf::open(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned int) @ 0x126954e2 in /usr/bin/clickhouse\r\n2. Poco::FileOutputStream::FileOutputStream(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned int) @ 0x1269652d in /usr/bin/clickhouse\r\n3. Poco::LogFileImpl::LogFileImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1269f4c2 in /usr/bin/clickhouse\r\n4. Poco::FileChannel::unsafeOpen() @ 0x126892ee in /usr/bin/clickhouse\r\n5. Poco::FileChannel::open() @ 0x126891e1 in /usr/bin/clickhouse\r\n6. Loggers::buildLoggers(Poco::Util::AbstractConfiguration&, Poco::Logger&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x89c6483 in /usr/bin/clickhouse\r\n7. DB::LocalServer::initialize(Poco::Util::Application&) @ 0x89036a5 in /usr/bin/clickhouse\r\n8. Poco::Util::Application::run() @ 0x125cf046 in /usr/bin/clickhouse\r\n9. mainEntryClickHouseLocal(int, char**) @ 0x890e50b in /usr/bin/clickhouse\r\n10. main @ 0x87f1dce in /usr/bin/clickhouse\r\n11. __libc_start_main @ 0x22505 in /usr/lib64/libc-2.17.so\r\n12. _start @ 0x87bd06e in /usr/bin/clickhouse\r\n (version 21.4.6.55 (official build))\r\n\r\n$ clickhouse-local --version\r\nClickHouse client version 21.4.6.55 (official build).\r\n\r\n-rw-r--r-- 1 clickhouse clickhouse 16235 May 10 08:24 /var/lib/clickhouse/preprocessed_configs/config.xml\r\n```\r\n\r\n=============================\r\n```\r\n$ echo -e \"1,2\\n3,4\" | clickhouse-local --structure \"a Int64, b Int64\" --input-format \"CSV\" --query \"SELECT * FROM table\"\r\n1       2\r\n3       4\r\n$ clickhouse-local --version\r\nClickHouse client version 18.6.0.\r\n\r\n-rw-r--r-- 1 root root 16142 Apr 15  2019 /var/lib/clickhouse/preprocessed_configs/config.xml\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25897/comments",
    "author": "Jack012a",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-01T18:03:09Z",
        "body": "TLDR: run `clickhouse-local` from another directory, without `config.xml`.\r\n\r\nYou are running `clickhouse-local`, it looks for a config in current directory. You are running it inside a directory where the server's config is located. But `clickhouse-server` config does not make sense for `clickhouse-local`.\r\n\r\nIn most cases `clickhouse-local` does not need any config at all."
      },
      {
        "user": "Jack012a",
        "created_at": "2021-07-01T18:11:54Z",
        "body": "This is not true. I didn't run clickhouse-local from a directory where it has config.xml."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-01T18:13:26Z",
        "body": "But it managed to find\r\n`Processing configuration file 'config.xml'.`\r\nanyhow.\r\n\r\nType `ls -l` in the current working directory.\r\n"
      }
    ]
  },
  {
    "number": 21177,
    "title": "How do I enable the compilation option -pie?",
    "created_at": "2021-02-25T08:22:21Z",
    "closed_at": "2021-06-13T21:33:28Z",
    "labels": [
      "question",
      "build",
      "st-fixed"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21177",
    "body": "In CMakeLists.txt\uff0cthere are following compilation options by default:\r\n_set (CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO} -fno-pie\")\r\nset (CMAKE_C_FLAGS_RELWITHDEBINFO \"${CMAKE_C_FLAGS_RELWITHDEBINFO} -fno-pie\")\r\nset (CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-no-pie\")_\r\n\r\nBecause I want to compile clickhouse in a more secure mode\uff0cI need to use \"-fpie\" or \"-pie\" compilation options in compiling\u3002But if I change \"-fno-pie\" to \"-fpie\" and \"-no-pie\" to \"-pie\",I can not complie clickhouse successfully,the following is my compilation command:\r\n_cmake .. -DUSE_INTERNAL_BOOST_LIBRARY=1  -DENABLE_READLINE=1 -DCMAKE_BUILD_TYPE=Release -DENABLE_MYSQL=0 -DENABLE_DATA_SQLITE=0 -DPOCO_ENABLE_SQL_SQLITE=0 -DENABLE_JEMALLOC=ON -DENABLE_EMBEDDED_COMPILER=1  -DENABLE_PARQUET=1  -DENABLE_ORC=1 -DENABLE_PROTOBUF=1 -DENABLE_ODBC=0 -DENABLE_SSL=1  -DNO_WERROR=1 -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DUSE_INTERNAL_ODBC_LIBRARY=1 -DMAKE_STATIC_LIBRARIES=1_\r\n\r\nThe following is error info:\r\n\r\n-- Performing Test HAVE_PTRDIFF_T\r\n-- Performing Test HAVE_PTRDIFF_T - Failed\r\n-- Check size of void *\r\n-- Check size of void * - failed\r\n-- sizeof(void *) is  bytes\r\nCMake Error at contrib/zlib-ng/CMakeLists.txt:419 (message):\r\n  sizeof(void *) is neither 32 nor 64 bit\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\nSo how can I config my compilation options to compile clickhouse with pie enabled successfully? Thank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21177/comments",
    "author": "wallace-clickhouse",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-28T22:55:19Z",
        "body": "Just remove `-fno-pie` and `-Wl,-no-pie`."
      }
    ]
  },
  {
    "number": 19315,
    "title": "ALTER DELETE not working",
    "created_at": "2021-01-20T13:15:53Z",
    "closed_at": "2021-08-17T07:27:13Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19315",
    "body": "I have a table that I want to mutate using the following query:\r\n\r\n```sql\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE (ProjectId, UserId, SessionId) NOT IN (\r\n        SELECT ProjectId, UserId, SessionId\r\n        FROM clarity.page_data_enrich\r\n        GROUP BY ProjectId, UserId, SessionId\r\n        HAVING argMax(IsFavorite, RowVersion)\r\n    );\r\n```\r\n\r\nWhen I run the query, I get the following exception in `system.mutations` table:\r\n\r\n```\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_10.txt\r\ncommand:                    DELETE WHERE (ProjectId, UserId, SessionId) NOT IN (SELECT ProjectId, UserId, SessionId FROM clarity.page_data_enrich GROUP BY ProjectId, UserId, SessionId HAVING argMax(IsFavorite, RowVersion))\r\ncreate_time:                2021-01-20 15:08:28\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [10]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-01-20 15:08:30\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 20.12.5.14 (official build))\r\n```\r\n\r\nEven though the following query runs with no problem:\r\n\r\n```sql\r\nSELECT *\r\nFROM clarity.page_data\r\nWHERE (ProjectId, UserId, SessionId) NOT IN\r\n(\r\n    SELECT ProjectId, UserId, SessionId\r\n    FROM clarity.page_data_enrich\r\n    GROUP BY ProjectId, UserId, SessionId\r\n    HAVING argMax(IsFavorite, RowVersion)\r\n)\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500Timestamp\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500Date\u2500\u252c\u2500ProjectId\u2500\u252c\u2500UserId\u2500\u252c\u2500SessionId\u2500\u252c\u2500PageNum\u2500\u2510\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       1 \u2502\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       2 \u2502\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nI am not sure what is wrong with the `ALTER DELETE` query!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19315/comments",
    "author": "OmarBazaraa",
    "comments": [
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-03T15:17:43Z",
        "body": "I even tried to concatenate the fields instead of comparing tuples, but I get the exact same error:\r\n\r\n```sql\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE concat(toString(ProjectId), toString(UserId), toString(SessionId)) NOT IN (\r\n        SELECT concat(toString(ProjectId), toString(UserId), toString(SessionId))\r\n        FROM clarity.page_data_enrich\r\n        GROUP BY ProjectId, UserId, SessionId\r\n        HAVING argMax(IsFavorite, RowVersion)\r\n    );\r\n```\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\nORDER BY create_time DESC\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_16.txt\r\ncommand:                    DELETE WHERE concat(toString(ProjectId), toString(UserId), toString(SessionId)) NOT IN (SELECT concat(toString(ProjectId), toString(UserId), toString(SessionId)) FROM clarity.page_data_enrich GROUP BY ProjectId, UserId, SessionId HAVING argMax(IsFavorite, RowVersion))\r\ncreate_time:                2021-02-03 17:11:40\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [16]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-02-03 17:11:44\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 21.1.2.15 (official build))\r\n```\r\n\r\nIt's stating that the number of columns in section IN doesn't match!\r\n\r\nAny ideas what is going wrong?!"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-03T16:40:32Z",
        "body": "I think mutations are not designed to handle such `where subqueries`\r\n\r\nas a WA I would create a table Engine=Join and inserted into this Join table IDs which should be deleted using `insert select` \r\nthen run delete like this \r\n```\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE joinHas(, , (ProjectId, serId, SessionId) )"
      },
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-04T13:37:39Z",
        "body": "Thanks @den-crane for your suggestion!\r\n\r\nI tried it but it's giving me the same error...\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\nORDER BY create_time DESC\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_17.txt\r\ncommand:                    DELETE WHERE isNotNull(joinGet('clarity.page_data_retained', 'RowVersion', ProjectId, UserId, SessionId))\r\ncreate_time:                2021-02-04 15:33:34\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [17]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-02-04 15:33:52\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 21.1.2.15 (official build))\r\n```\r\n\r\nAny other possible alternatives to retain/TTL records based on values from other tables?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-04T14:35:41Z",
        "body": "@OmarBazaraa \r\n\r\nHMm, I think this error from the previous mutations.\r\nTry remove failed mutations first:\r\n\r\n```\r\nkill mutation where not is_done;\r\nALTER TABLE clarity.page_data DELETE WHERE joinHas(, , (ProjectId, serId, SessionId) )\r\n```"
      }
    ]
  },
  {
    "number": 19030,
    "title": "MaterializeMySQL will get exception when create database on cluster",
    "created_at": "2021-01-14T07:29:44Z",
    "closed_at": "2021-01-18T10:39:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19030",
    "body": "**Describe the bug**\r\nWhen i create MaterializeMySQL engine database on cluster,I will get the following execption,but i already set allow_experimental_database_materialize_mysql = 1 on every ClickHouse node.\r\nIf i create MaterializeMySQL engine database on single ClickHouse node,it will be successed.\r\n```\r\n2021.01.14 15:09:58.759295 [ 25034 ] {ee3c8887-ef0f-454b-ac10-5089e001fcae} <Error> DDLWorker: Query CREATE DATABASE test_mysql_new UUID '34f48f87-411b-4534-b64e-3bd646dd91f8' ENGINE = MaterializeMySQL('clickhouse2:3307', 'test', 'root', 'root123') wasn't finished successfully: Code: 336, e.displayText() = DB::Exception: MaterializeMySQL is an experimental database engine. Enable allow_experimental_database_materialize_mysql to use it., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::InterpreterCreateQuery::createDatabase(DB::ASTCreateQuery&) @ 0xdc75c5f in /usr/bin/clickhouse\r\n1. DB::InterpreterCreateQuery::execute() @ 0xdc81317 in /usr/bin/clickhouse\r\n2. ? @ 0xe060347 in /usr/bin/clickhouse\r\n3. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>) @ 0xe063377 in /usr/bin/clickhouse\r\n4. DB::DDLWorker::tryExecuteQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::DDLTask const&, DB::ExecutionStatus&) @ 0xda5ec43 in /usr/bin/clickhouse\r\n5. DB::DDLWorker::processTask(DB::DDLTask&) @ 0xda60eb9 in /usr/bin/clickhouse\r\n6. DB::DDLWorker::enqueueTask(std::__1::unique_ptr<DB::DDLTask, std::__1::default_delete<DB::DDLTask> >) @ 0xda5f5e2 in /usr/bin/clickhouse\r\n7. ? @ 0xda6f64d in /usr/bin/clickhouse\r\n8. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x7dc870d in /usr/bin/clickhouse\r\n9. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x7dcac6f in /usr/bin/clickhouse\r\n10. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x7dc5b3d in /usr/bin/clickhouse\r\n11. ? @ 0x7dc96f3 in /usr/bin/clickhouse\r\n12. start_thread @ 0x7dc5 in /usr/lib64/libpthread-2.17.so\r\n13. __clone @ 0xf61cd in /usr/lib64/libc-2.17.so\r\n (version 20.12.5.14 (official build))\r\n\r\n```\r\n**How to reproduce**\r\nClickHouse server version is 20.12.5.14\r\nMySQL server version is 8.0.22\r\nExecute\r\n```\r\ncreate database test_materializemysql on cluster test_cluster engine = MaterializeMySQL('mysqlhost:port', 'test', 'test', 'test');\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19030/comments",
    "author": "Hedwiglzy",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2021-01-18T10:38:52Z",
        "body": "It's because `SET` query changes setting value in current session. Distributed DDL queries are executed in separate session and `SET` query does not affect it. You should enable `allow_experimental_database_materialize_mysql` setting globally in server config to make it work with `ON CLUSTER` queries. Example:\r\n```\r\n    <profiles>\r\n            <default>\r\n                    <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n                    ...\r\n            </default>\r\n            ...\r\n    </profiles>\r\n```\r\n\r\nAnother option is to configure distributed DDL queries to use separate settings profile with all settings you need:\r\n```\r\n    <profiles>\r\n            <default>\r\n                    ...\r\n            </default>\r\n            <dddl>\r\n                    <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n            </dddl>\r\n            ...\r\n    </profiles>\r\n   ...\r\n   <distributed_ddl>\r\n        <path>/clickhouse/task_queue/ddl</path>\r\n        <profile>dddl</profile>\r\n    </distributed_ddl>\r\n```"
      },
      {
        "user": "Hedwiglzy",
        "created_at": "2021-01-25T02:26:16Z",
        "body": "> > It's because `SET` query changes setting value in current session. Distributed DDL queries are executed in separate session and `SET` query does not affect it. You should enable `allow_experimental_database_materialize_mysql` setting globally in server config to make it work with `ON CLUSTER` queries. Example:\r\n> > ```\r\n> >     <profiles>\r\n> >             <default>\r\n> >                     <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n> >                     ...\r\n> >             </default>\r\n> >             ...\r\n> >     </profiles>\r\n> > ```\r\n> > \r\n> > \r\n> > Another option is to configure distributed DDL queries to use separate settings profile with all settings you need:\r\n> > ```\r\n> >     <profiles>\r\n> >             <default>\r\n> >                     ...\r\n> >             </default>\r\n> >             <dddl>\r\n> >                     <allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>\r\n> >             </dddl>\r\n> >             ...\r\n> >     </profiles>\r\n> >    ...\r\n> >    <distributed_ddl>\r\n> >         <path>/clickhouse/task_queue/ddl</path>\r\n> >         <profile>dddl</profile>\r\n> >     </distributed_ddl>\r\n> > ```\r\n> \r\n> I still got the error after setting the server config. Any suggestion else?\r\n\r\nI follow the above method to config clickhouse is OK,you need config two files,<allow_experimental_database_materialize_mysql> in user.xml,<distributed_ddl> in config.xml,did you configuration both?"
      },
      {
        "user": "naveedmm",
        "created_at": "2021-11-03T04:40:57Z",
        "body": "Server doesn't restart after doing to suggested changes\r\n                    `<allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>`\r\nThis is causing the restart to fail"
      },
      {
        "user": "tavplubix",
        "created_at": "2021-11-03T10:02:08Z",
        "body": "> Server doesn't restart after doing to suggested changes `<allow_experimental_database_materialize_mysql>1</allow_experimental_database_materialize_mysql>` This is causing the restart to fail\r\n\r\nCheck server log, it contains error messages explaining why server cannot start. Most likely it's because the setting `allow_experimental_database_materialize_mysql` was renamed to `allow_experimental_database_materialized_mysql`, so server does not start due to unknown setting."
      }
    ]
  },
  {
    "number": 16695,
    "title": "how to change default CSV FILE format_csv_delimiter?",
    "created_at": "2020-11-05T07:51:09Z",
    "closed_at": "2020-11-10T11:12:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16695",
    "body": "Hello,\r\n\r\n\r\nmy setting:\r\n```\r\n# cat /etc/clickhouse-server/config.d/delimiter.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n\t<profiles>\r\n        <default>\r\n\t\t<format_csv_delimiter>|</format_csv_delimiter>\r\n        </default>\r\n    </profiles>\r\n</yandex>\r\n```\r\n\r\nbut throw exception \r\n```\r\nCode: 27. DB::Exception: Cannot parse input: expected ',' before: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: user_ip,         type: String, parsed text: \"\"\r\nERROR: Line feed found where delimiter (,) is expected. It's like your file has less columns than expected.\r\n```\r\n\r\n    please, how to change default CSV FILE format_csv_delimiter?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16695/comments",
    "author": "trollhe",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-05T14:14:00Z",
        "body": ">cat /etc/clickhouse-server/config.d/delimiter.xml\r\n\r\nWrong folder for user settings.\r\nShould be /etc/clickhouse-server/**users.d**/delimiter.xml\r\n\r\n\r\nconfig.d -- server settings (config.xml)\r\nusers.d -- users settings (user.xml)\r\nconf.d -- any (config.xml and user.xml)"
      },
      {
        "user": "trollhe",
        "created_at": "2020-11-06T02:40:08Z",
        "body": "> > cat /etc/clickhouse-server/config.d/delimiter.xml\r\n> \r\n> Wrong folder for user settings.\r\n> Should be /etc/clickhouse-server/**users.d**/delimiter.xml\r\n> \r\n> config.d -- server settings (config.xml)\r\n> users.d -- users settings (user.xml)\r\n> conf.d -- any (config.xml and user.xml)\r\n\r\nthanks den-crane,\r\n\r\ni'm try to it, file already move from  `/etc/clickhouse-server/config.d/` to `/etc/clickhouse-server/users.d/`. and  restarted to clickhouse-server service.\r\n\r\nbut exception  still.\r\n\r\ntips: i used table function file() to select /data/clickhouse/user_files/*.csv. csv delimiter is  \"|\".\r\n\r\n"
      }
    ]
  },
  {
    "number": 16537,
    "title": "Process killed by  Memory limit (total) exceeded",
    "created_at": "2020-10-30T02:45:13Z",
    "closed_at": "2020-11-03T16:22:32Z",
    "labels": [
      "question",
      "obsolete-version",
      "memory"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16537",
    "body": "i start my process 2 hours,only commit not query\u3002\r\n\r\n<max_server_memory_usage_to_ram_ratio>0.8</max_server_memory_usage_to_ram_ratio>\r\nserver RAM :32g\r\n\r\nthe error is :\r\n2020.10.29 23:45:16.320714 [ 10473 ] {171eac9e-a25d-45b0-bd59-7ee8f6533105} <Error> executeQuery: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 24.72 GiB (attempt to allocate chunk of 5075574 bytes), maximum: 24.71 GiB (version 20.7.1.1) (from 127.0.0.1:56612) (in query: INSERT INTO \r\n\r\n0. std::exception::capture() @ 0x66c630e in /usr/bin/clickhouse\r\n1. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8bff4d4 in /usr/bin/clickhouse\r\n2. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x54039a1 in /usr/bin/clickhouse\r\n3. MemoryTracker::alloc(long) @ 0x53ec389 in /usr/bin/clickhouse\r\n4. MemoryTracker::alloc(long) @ 0x53ec3c5 in /usr/bin/clickhouse\r\n5. MemoryTracker::alloc(long) @ 0x53ec3c5 in /usr/bin/clickhouse\r\n6. MemoryTracker::alloc(long) @ 0x53ec3c5 in /usr/bin/clickhouse\r\n7. Allocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long) @ 0x5413acc in /usr/bin/clickhouse\r\n8. void DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 15ul, 16ul>::resize<>(unsigned long) @ 0x549cde5 in /usr/bin/clickhouse\r\n9. ? @ 0x763ac77 in /usr/bin/clickhouse\r\n10. DB::DataTypeString::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0x763b3fc in /usr/bin/clickhouse\r\n11. DB::NativeBlockInputStream::readData(DB::IDataType const&, DB::IColumn&, DB::ReadBuffer&, unsigned long, double) @ 0x7b080a4 in /usr/bin/clickhouse\r\n12. DB::NativeBlockInputStream::readImpl() @ 0x7b087da in /usr/bin/clickhouse\r\n13. DB::IBlockInputStream::read() @ 0x75e1883 in /usr/bin/clickhouse\r\n14. DB::TCPHandler::receiveData(bool) @ 0x7a1d63b in /usr/bin/clickhouse\r\n15. DB::TCPHandler::receivePacket() @ 0x7a1dbf1 in /usr/bin/clickhouse\r\n16. DB::TCPHandler::readDataNext(unsigned long const&, int const&) @ 0x7a1e0b3 in /usr/bin/clickhouse\r\n17. DB::TCPHandler::readData(DB::Settings const&) @ 0x7a1e388 in /usr/bin/clickhouse\r\n18. DB::TCPHandler::processInsertQuery(DB::Settings const&) @ 0x7a1e586 in /usr/bin/clickhouse\r\n19. DB::TCPHandler::runImpl() @ 0x7a1f371 in /usr/bin/clickhouse\r\n20. DB::TCPHandler::run() @ 0x7a203b1 in /usr/bin/clickhouse\r\n21. Poco::Net::TCPServerConnection::start() @ 0x7e331bb in /usr/bin/clickhouse\r\n22. Poco::Net::TCPServerDispatcher::run() @ 0x7e335b1 in /usr/bin/clickhouse\r\n23. Poco::PooledThread::run() @ 0x8c2dc4a in /usr/bin/clickhouse\r\n24. Poco::ThreadImpl::runnableEntry(void*) @ 0x8c2ced4 in /usr/bin/clickhouse\r\n25. start_thread @ 0x7ea5 in /lib64/libpthread-2.17.so\r\n26. clone @ 0xfe8dd in /lib64/libc-2.17.so\r\n\r\n\r\n\r\nrun   select metric,':', formatReadableSize(value) from system.metrics where metric like 'MemoryTracking' the result is:\r\n\r\n\u250c\u2500metric\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500':'\u2500\u252c\u2500formatReadableSize(value)\u2500\u2510\r\n\u2502 MemoryTracking \u2502 :   \u2502 24.71 GiB                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n\r\nbut the real RAM usage is less than 100M, What caused the problem. thanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16537/comments",
    "author": "xiashaohua",
    "comments": [
      {
        "user": "azat",
        "created_at": "2020-10-30T08:32:23Z",
        "body": "> i start my process 2 hours,only commit not query\u3002\r\n\r\nNot sure what \"only commit\" means, but:\r\n\r\n>2020.10.29 23:45:16.320714 [ 10473 ] {171eac9e-a25d-45b0-bd59-7ee8f6533105} executeQuery: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 24.72 GiB (attempt to allocate chunk of 5075574 bytes), maximum: 24.71 GiB (version 20.7.1.1) (from 127.0.0.1:56612) (in query: INSERT INTO\r\n\r\nThere were some significant improvements in the memory tracking, the most relevant for this issue is - #16121\r\n\r\nP.S. label memory is required"
      },
      {
        "user": "xiashaohua",
        "created_at": "2020-10-30T08:56:58Z",
        "body": "> > i start my process 2 hours,only commit not query\u3002\r\n> \r\n> Not sure what \"only commit\" means, but:\r\n> \r\n> > 2020.10.29 23:45:16.320714 [ 10473 ] {171eac9e-a25d-45b0-bd59-7ee8f6533105} executeQuery: Code: 241, e.displayText() = DB::Exception: Memory limit (total) exceeded: would use 24.72 GiB (attempt to allocate chunk of 5075574 bytes), maximum: 24.71 GiB (version 20.7.1.1) (from 127.0.0.1:56612) (in query: INSERT INTO\r\n> \r\n> There were some significant improvements in the memory tracking, the most relevant for this issue is - #16121\r\n> \r\n> P.S. label memory is required\r\n\r\nInsert only no select\u3002and the phenomenon is not stable, sometimes appear in a few hours, sometimes not appear in a few days"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-30T12:57:48Z",
        "body": "It seems this issue will be fixed in the next stable release 20.11"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-30T13:02:01Z",
        "body": "20.7.1.1 -- is obsolete and not stable. You need to upgrade at least to 20.7.4.11. Or better to 20.8.5.45."
      }
    ]
  },
  {
    "number": 16421,
    "title": "cannot start server 20.10.2.20,reports <jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.",
    "created_at": "2020-10-27T08:42:24Z",
    "closed_at": "2020-11-25T10:14:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16421",
    "body": "the old version  19.17.5.18\r\nservice clickhouse-server stop does not work, so I  stop the old server by kill, then run rpm -Uvh to update the software, then service clickhouse-server start failed\r\n\r\n```\r\n[root@localhost ~]# service clickhouse-server stop\r\n[root@localhost ~]# clickhouse-client -m\r\nClickHouse client version 19.17.5.18 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 19.17.5 revision 54428.\r\n\r\nlocalhost :) \\q;\r\nBye.\r\n[root@localhost ~]# ps -ef|grep click\r\nroot     32900 32878  0 Sep18 ?        02:51:20 clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\nroot     50565 50287  0 15:57 pts/1    00:00:00 grep click\r\n[root@localhost ~]# kill 32900\r\n[root@localhost ~]# ps -ef|grep click\r\nroot     50651 50287  0 16:00 pts/1    00:00:00 grep click\r\n\r\n[root@localhost tmp]# rpm -q click*\r\npackage clickhouse-client-20.10.2.20-2.noarch.rpm is not installed\r\npackage clickhouse-common-static-20.10.2.20-2.x86_64.rpm is not installed\r\npackage clickhouse-server-20.10.2.20-2.noarch.rpm is not installed\r\n[root@localhost tmp]# rpm -Uvh click*\r\nwarning: clickhouse-client-20.10.2.20-2.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID e0c56bd4: NOKEY\r\nPreparing...                ########################################### [100%]\r\n   1:clickhouse-common-stati########################################### [ 33%]\r\n   2:clickhouse-client      ########################################### [ 67%]\r\n   3:clickhouse-server      ########################################### [100%]\r\n\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nPath to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/\r\n\r\n[root@localhost tmp]# service clickhouse-server start\r\nStart clickhouse-server service: <jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nPath to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/\r\nChanging owner of [/var/lib/clickhouse/] to [clickhouse:clickhouse]\r\nChanging owner of [/var/log/clickhouse-server/*] to [clickhouse:clickhouse]\r\nChanging owner of [/var/log/clickhouse-server] to [root:clickhouse]\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nUNKNOWN\r\n[root@localhost tmp]# clickhouse-client -m\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nClickHouse client version 20.10.2.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 210. DB::NetException: Connection refused (localhost:9000)\r\n\r\n[root@localhost tmp]# cat /proc/version\r\nLinux version 2.6.32-573.el6.x86_64 (mockbuild@kojibuilder-ve) (gcc version 4.4.7 20120313 (GCC) ) #1 SMP Wed Feb 24 13:34:24 CST 2016\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16421/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-10-27T14:00:42Z",
        "body": "1) please check /var/log/clickhouse-server/clickhouse-server.err.log and /var/log/clickhouse-server/clickhouse-server.log\r\n2) what is your OS? "
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:10:09Z",
        "body": "clickhouse-server.err.log\r\n```\r\n[root@localhost ~]# tail  /var/log/clickhouse-server/clickhouse-server.err.log\r\n24. 0x7fb90d9a3075 DB::NullAndDoCopyBlockInputStream::readImpl() /usr/bin/clickhouse\r\n25. 0x7fb90d8555ca DB::IBlockInputStream::read() /usr/bin/clickhouse\r\n26. 0x7fb90d84e69b DB::AsynchronousBlockInputStream::calculate() /usr/bin/clickhouse\r\n27. 0x7fb90d84ea60 ? /usr/bin/clickhouse\r\n28. 0x7fb90a64917e ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::_List_iterator<ThreadFromGlobalPool>) /usr/bin/clickhouse\r\n29. 0x7fb90a64978e ThreadFromGlobalPool::ThreadFromGlobalPool<ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}>(ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::function<void ()>, int, std::optional<unsigned long>)::{lambda()#3}&&)::{lambda()#1}::operator()() const /usr/bin/clickhouse\r\n30. 0x7fb90a646c4c ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /usr/bin/clickhouse\r\n31. 0x7fb9103d0ec0 ? /usr/bin/clickhouse\r\n\r\n2020.10.27 15:54:07.005337 [ 1 ] {} <Error> Application: DB::Exception: Cannot lock file /var/lib/clickhouse/status. Another server instance in same directory is already running.\r\n\r\n```\r\nclickhouse-server.log\r\n```\r\n[root@localhost ~]# tail -100 /var/log/clickhouse-server/clickhouse-server.log\r\n2020.10.27 15:54:07.005337 [ 1 ] {} <Error> Application: DB::Exception: Cannot lock file /var/lib/clickhouse/status. Another server instance in same directory is already running.\r\n2020.10.27 15:54:07.005839 [ 1 ] {} <Information> Application: shutting down\r\n2020.10.27 15:54:07.005863 [ 1 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2020.10.27 15:54:07.007098 [ 2 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n2020.10.27 15:57:10.521311 [ 87 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::1]:40251\r\n2020.10.27 15:57:10.521527 [ 87 ] {} <Debug> TCPHandler: Connected ClickHouse client version 19.17.0, revision: 54428, user: default.\r\n2020.10.27 15:57:10.530121 [ 88 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::1]:40252\r\n2020.10.27 15:57:10.530263 [ 88 ] {} <Debug> TCPHandler: Connected ClickHouse client version 19.17.0, revision: 54428, user: default.\r\n2020.10.27 15:57:10.532893 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> executeQuery: (from [::1]:40252) SELECT DISTINCT arrayJoin(extractAll(name, '[\\\\w_]{2,}')) AS res FROM (SELECT name FROM system.functions UNION ALL SELECT name FROM system.table_engines UNION ALL SELECT name FROM system.formats UNION ALL SELECT name FROM system.table_functions UNION ALL SELECT name FROM system.data_type_families UNION ALL SELECT name FROM system.settings UNION ALL SELECT concat(func.name, comb.name) FROM system.functions AS func CROSS JOIN system.aggregate_function_combinators AS comb WHERE is_aggregate UNION ALL SELECT name FROM system.databases LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.tables LIMIT 10000 UNION ALL SELECT DISTINCT name FROM system.columns LIMIT 10000) WHERE notEmpty(res)\r\n2020.10.27 15:57:10.536672 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> Join: setSampleBlock: comb.name String String(size = 0)\r\n2020.10.27 15:57:10.539658 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> Join: setSampleBlock: comb.name String String(size = 0)\r\n2020.10.27 15:57:10.542160 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> Join: setSampleBlock: comb.name String String(size = 0)\r\n2020.10.27 15:57:10.543592 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543653 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543702 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543735 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.543816 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544097 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544542 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544685 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544781 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.544985 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.545070 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.545989 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Distinct\r\n  Union\r\n   Distinct \u00d7 6\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Expression\r\n        Expression\r\n         One\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Expression\r\n        CreatingSets\r\n         Lazy\r\n         Expression\r\n          Filter\r\n           Expression\r\n            One\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Limit\r\n        Expression\r\n         Expression\r\n          One\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Limit\r\n        Expression\r\n         Distinct\r\n          Expression\r\n           Tables\r\n   Distinct\r\n    Expression\r\n     Filter\r\n      Converting\r\n       Limit\r\n        Expression\r\n         Distinct\r\n          Expression\r\n           Columns\r\n\r\n2020.10.27 15:57:10.549108 [ 98 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> CreatingSetsBlockInputStream: Creating join. \r\n2020.10.27 15:57:10.550504 [ 98 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2020.10.27 15:57:10.551153 [ 98 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> CreatingSetsBlockInputStream: Created. Join with 9 entries from 9 rows. In 0.001 sec.\r\n2020.10.27 15:57:10.555171 [ 21 ] {} <Trace> SystemLog (system.trace_log): Flushing system log\r\n2020.10.27 15:57:10.555870 [ 21 ] {} <Debug> DiskSpaceMonitor: Reserving 1.00 MiB on disk `default`, having unreserved 23.59 GiB.\r\n2020.10.27 15:57:10.556061 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n2020.10.27 15:57:10.556087 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waited for threads to finish\r\n2020.10.27 15:57:10.556187 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Information> executeQuery: Read 2767 rows, 111.92 KiB in 0.023 sec., 119154 rows/sec., 4.71 MiB/sec.\r\n2020.10.27 15:57:10.556206 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> MemoryTracker: Peak memory usage (for query): 1.04 MiB.\r\n2020.10.27 15:57:10.556268 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n2020.10.27 15:57:10.556282 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Trace> UnionBlockInputStream: Waited for threads to finish\r\n2020.10.27 15:57:10.564214 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Debug> MemoryTracker: Peak memory usage (total): 1.04 MiB.\r\n2020.10.27 15:57:10.564243 [ 21 ] {} <Trace> system.trace_log: Renaming temporary part tmp_insert_202010_63_63_0 to 202010_200_200_0.\r\n2020.10.27 15:57:10.564256 [ 88 ] {19d923ba-9002-4230-ae6d-0f7a1673668a} <Information> TCPHandler: Processed in 0.033 sec.\r\n2020.10.27 15:57:10.564290 [ 88 ] {} <Information> TCPHandler: Done processing connection.\r\n2020.10.27 15:57:19.166554 [ 87 ] {} <Information> TCPHandler: Done processing connection.\r\n2020.10.27 16:00:22.919041 [ 112 ] {} <Information> Application: Received termination signal (Terminated)\r\n2020.10.27 16:00:22.919329 [ 1 ] {} <Debug> Application: Received termination signal.\r\n2020.10.27 16:00:22.919434 [ 1 ] {} <Debug> Application: Waiting for current connections to close.\r\n2020.10.27 16:00:23.831890 [ 1 ] {} <Information> Application: Closed all listening sockets.\r\n2020.10.27 16:00:23.831933 [ 1 ] {} <Information> Application: Closed connections.\r\n2020.10.27 16:00:23.837146 [ 1 ] {} <Information> Application: Shutting down storages.\r\n2020.10.27 16:00:23.939019 [ 1 ] {} <Trace> BackgroundSchedulePool: Waiting for threads to finish.\r\n2020.10.27 16:00:23.939511 [ 1 ] {} <Debug> Application: Shutted down storages.\r\n2020.10.27 16:00:23.942859 [ 1 ] {} <Debug> Application: Destroyed global context.\r\n2020.10.27 16:00:23.943143 [ 1 ] {} <Information> Application: shutting down\r\n2020.10.27 16:00:23.943155 [ 1 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2020.10.27 16:00:23.943207 [ 112 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:13:02Z",
        "body": "same error if i run ```clickhouse-server --config-file=/etc/clickhouse-server/config.xml```\r\n```\r\n[root@localhost ~]# clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nLogging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nLogging trace to console\r\n2020.10.28 07:44:47.987507 [ 25804 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\n2020.10.28 07:44:47.992744 [ 25804 ] {} <Information> Pipe: Cannot get pipe capacity, errno: 22, strerror: Invalid argument. Very old Linux kernels have no support for this fcntl.\r\n2020.10.28 07:44:48.043623 [ 25804 ] {} <Information> : Starting ClickHouse 20.10.2.20 with revision 54441, no build id, PID 25804\r\n2020.10.28 07:44:48.043772 [ 25804 ] {} <Information> Application: starting up\r\n2020.10.28 07:44:48.052278 [ 25804 ] {} <Trace> Application: Will do mlock to prevent executable memory from being paged out. It may take a few seconds.\r\n2020.10.28 07:44:48.084764 [ 25804 ] {} <Trace> Application: The memory map of clickhouse executable has been mlock'ed, total 164.02 MiB\r\n2020.10.28 07:44:48.085210 [ 25804 ] {} <Error> Application: DB::Exception: Effective user of the process (root) does not match the owner of the data (clickhouse). Run under 'sudo -u clickhouse'.\r\n2020.10.28 07:44:48.085253 [ 25804 ] {} <Information> Application: shutting down\r\n2020.10.28 07:44:48.085263 [ 25804 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2020.10.28 07:44:48.087090 [ 25807 ] {} <Trace> BaseDaemon: Received signal -2\r\n2020.10.28 07:44:48.087170 [ 25807 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:18:25Z",
        "body": "run ```sudo -u clickhouse``` seems work now\r\n\r\n```\r\n[root@localhost ~]# sudo -u clickhouse  clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nLogging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nLogging trace to console\r\n2020.10.28 07:47:14.897570 [ 25888 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\n2020.10.28 07:47:14.902881 [ 25888 ] {} <Information> Pipe: Cannot get pipe capacity, errno: 22, strerror: Invalid argument. Very old Linux kernels have no support for this fcntl.\r\n2020.10.28 07:47:14.954047 [ 25888 ] {} <Information> : Starting ClickHouse 20.10.2.20 with revision 54441, no build id, PID 25888\r\n2020.10.28 07:47:14.954201 [ 25888 ] {} <Information> Application: starting up\r\n2020.10.28 07:47:14.961761 [ 25888 ] {} <Trace> Application: Will do mlock to prevent executable memory from being paged out. It may take a few seconds.\r\n2020.10.28 07:47:14.993816 [ 25888 ] {} <Trace> Application: The memory map of clickhouse executable has been mlock'ed, total 164.02 MiB\r\n2020.10.28 07:47:14.993954 [ 25888 ] {} <Debug> Application: rlimit on number of file descriptors is 262144\r\n2020.10.28 07:47:14.993971 [ 25888 ] {} <Debug> Application: Initializing DateLUT.\r\n2020.10.28 07:47:14.993983 [ 25888 ] {} <Trace> Application: Initialized DateLUT with time zone 'Asia/Shanghai'.\r\n2020.10.28 07:47:14.994009 [ 25888 ] {} <Debug> Application: Setting up /var/lib/clickhouse/tmp/ to store temporary data in it\r\n2020.10.28 07:47:14.995115 [ 25888 ] {} <Debug> Application: Configuration parameter 'interserver_http_host' doesn't exist or exists and empty. Will use 'localhost' as replica host.\r\n2020.10.28 07:47:14.996173 [ 25888 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/users.xml'\r\nProcessing configuration file '/etc/clickhouse-server/users.xml'.\r\nInclude not found: networks\r\nSaved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'.\r\n2020.10.28 07:47:14.996946 [ 25888 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performing update on configuration\r\n2020.10.28 07:47:14.997673 [ 25888 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performed update on configuration\r\n2020.10.28 07:47:14.998025 [ 25888 ] {} <Warning> Access(local directory): File /var/lib/clickhouse/access/users.list doesn't exist\r\n2020.10.28 07:47:14.998053 [ 25888 ] {} <Warning> Access(local directory): Recovering lists in directory /var/lib/clickhouse/access/\r\n2020.10.28 07:47:14.998340 [ 25888 ] {} <Information> Application: Setting max_server_memory_usage was set to 56.62 GiB (62.91 GiB available * 0.90 max_server_memory_usage_to_ram_ratio)\r\n2020.10.28 07:47:14.998379 [ 25888 ] {} <Information> Application: Loading metadata from /var/lib/clickhouse/\r\n2020.10.28 07:47:15.005439 [ 25888 ] {} <Information> DatabaseOrdinary (system): Total 1 tables and 0 dictionaries.\r\n2020.10.28 07:47:15.012366 [ 25893 ] {} <Information> BackgroundProcessingPool: Create BackgroundProcessingPool with 16 threads\r\n2020.10.28 07:47:15.013364 [ 25893 ] {} <Debug> system.trace_log: Loading data parts\r\n2020.10.28 07:47:15.059491 [ 25893 ] {} <Debug> system.trace_log: Loaded data parts (5 items)\r\n2020.10.28 07:47:15.059872 [ 25888 ] {} <Information> DatabaseOrdinary (system): Starting up tables.\r\n2020.10.28 07:47:15.063505 [ 25888 ] {} <Information> DatabaseOrdinary (datasets): Total 1 tables and 0 dictionaries.\r\n2020.10.28 07:47:15.064279 [ 25893 ] {} <Debug> datasets.ontime: Loading data parts\r\n2020.10.28 07:47:15.800211 [ 25893 ] {} <Debug> datasets.ontime: Loaded data parts (373 items)\r\n2020.10.28 07:47:15.802813 [ 25888 ] {} <Information> DatabaseOrdinary (datasets): Starting up tables.\r\n2020.10.28 07:47:15.828392 [ 25888 ] {} <Information> DatabaseOrdinary (default): Total 8 tables and 0 dictionaries.\r\n```\r\nand open another terminal\r\n```\r\n[root@localhost ~]# clickhouse-client -m\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nClickHouse client version 20.10.2.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.10.2 revision 54441.\r\n\r\nlocalhost :) show databases;\r\n\r\nSHOW DATABASES\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 _temporary_and_external_tables \u2502\r\n\u2502 datasets                       \u2502\r\n\u2502 default                        \u2502\r\n\u2502 system                         \u2502\r\n\u2502 tpch                           \u2502\r\n\u2502 tutorial                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n6 rows in set. Elapsed: 0.003 sec. \r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T00:24:12Z",
        "body": "but service clickhouse-server start  or sudo service clickhouse-server start still fail\r\n```\r\n[root@localhost ~]# sudo -u clickhouse service clickhouse-server start\r\n/etc/init.d/clickhouse-server: line 387: /var/lock/clickhouse-server: Permission denied\r\n[root@localhost ~]# sudo service clickhouse-server start\r\nStart clickhouse-server service: <jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nPath to data directory in /etc/clickhouse-server/config.xml: /var/lib/clickhouse/\r\nChanging owner of [/var/log/clickhouse-server/*] to [clickhouse:clickhouse]\r\nChanging owner of [/var/log/clickhouse-server] to [root:clickhouse]\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\n\r\nUNKNOWN\r\n[root@localhost ~]# \r\n[root@localhost ~]# clickhouse-client -m\r\n<jemalloc>: perCPU arena getcpu() not available. Setting narenas to 256.\r\nClickHouse client version 20.10.2.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 210. DB::NetException: Connection refused (localhost:9000)\r\n\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-28T14:41:50Z",
        "body": "the real issue is \r\n```\r\n/etc/init.d/clickhouse-server: line 387: /var/lock/clickhouse-server: Permission denied\r\n```\r\n\r\nthe issue is not related to `<jemalloc>: perCPU `\r\n\r\n"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-28T23:55:03Z",
        "body": "how to solve the problem?\r\n```\r\n$ cat /etc/init.d/clickhouse-server |sed -n '381,390p'\r\n(\r\n    if $FLOCK -n 9; then\r\n        main \"$@\"\r\n    else\r\n        echo \"Init script is already running\" && exit 1\r\n    fi\r\n) 9> $LOCKFILE\r\n```"
      },
      {
        "user": "filimonov",
        "created_at": "2020-11-25T10:14:14Z",
        "body": "> same error if i run clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n\r\nIt's completely different... \r\n```\r\n2020.10.28 07:44:48.085210 [ 25804 ] {} <Error> Application: DB::Exception: Effective user of the process (root) does not match the owner of the data (clickhouse). Run under 'sudo -u clickhouse'.\r\n```\r\n\r\n> sudo -u clickhouse service clickhouse-server start\r\n\r\nIf you run clickhouse as  a service, you should run it as root. i.e.:\r\n```\r\n> sudo service clickhouse-server start\r\n```\r\n\r\nThe process will be executed from clickhouse user anyway. \r\n\r\nAlso please consider updating your OS/ linux. \r\n\r\n`perCPU arena getcpu() not available` actually means your linux kernel is quite old. "
      }
    ]
  },
  {
    "number": 16220,
    "title": "Query from distributed tables with sharded data return separate result for each shard",
    "created_at": "2020-10-21T10:58:43Z",
    "closed_at": "2020-11-08T12:56:09Z",
    "labels": [
      "question",
      "st-need-info",
      "st-need-repro"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16220",
    "body": "**Describe the bug**\r\nSelect aggregates from disributed table produce unexpected results.\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\nTested on \r\n1) \r\n```\r\n+----------------+----------------------------------------+\r\n|name            |value                                   |\r\n+----------------+----------------------------------------+\r\n|VERSION_FULL    |ClickHouse 20.3.11.97                   |\r\n|VERSION_DESCRIBE|v20.3.11.97-stable                      |\r\n|VERSION_INTEGER |20003011                                |\r\n|VERSION_GITHASH |952efc395509c10081e8c1b836ebb5c2e0f898fa|\r\n|VERSION_REVISION|54433                                   |\r\n+----------------+----------------------------------------+\r\n\r\n```\r\n\r\n2)\r\n```\r\n+----------------+----------------------------------------+\r\n|name            |value                                   |\r\n+----------------+----------------------------------------+\r\n|VERSION_FULL    |ClickHouse 20.7.2.30                    |\r\n|VERSION_DESCRIBE|v20.7.2.30-stable                       |\r\n|VERSION_INTEGER |20007002                                |\r\n|VERSION_GITHASH |e0529c753f9dd4fc27e38f2f25a14d50beedda65|\r\n|VERSION_REVISION|54437                                   |\r\n+----------------+----------------------------------------+\r\n```\r\n* Non-default settings, if any\r\nprefer_localhost_replica 0\r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\ncreate table test\r\n(\r\n    date Date,\r\n    key  String\r\n) engine MergeTree() partition by date order by key;\r\ncreate table test_dist as test engine Distributed('cubes', default, test, date);\r\n```\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\nOn shard 1: \r\n```\r\ninsert into test values ('2020-01-01', '1');\r\ninsert into test values ('2020-01-01', '2');\r\n```\r\nOn shard 2\r\n```\r\ninsert into test values ('2020-01-01', '1', 1);\r\ninsert into test values ('2020-01-01', '2', 1);\r\ninsert into test values ('2020-01-03', '3', 1);\r\n```\r\n* Queries to run that lead to unexpected result\r\n```\r\nselect toMonday(date) mnt, uniq(key)\r\nfrom test_dist\r\nwhere mnt = '2019-12-30'\r\ngroup by mnt;\r\n```\r\n\r\nResult:\r\n```\r\n+----------+---------+\r\n|mnt       |uniq(key)|\r\n+----------+---------+\r\n|2019-12-30|2        |\r\n|2019-12-30|3        |\r\n+----------+---------+\r\n\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nExpected single row to return with 3 uniq(key)\r\n```\r\n+----------+---------+\r\n|mnt       |uniq(key)|\r\n+----------+---------+\r\n|2019-12-30|3        |\r\n+----------+---------+\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16220/comments",
    "author": "dmgburg",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-21T13:39:56Z",
        "body": "Can you  show cluster `cubes` description?\r\n\r\nAnd `select *, _shard_num from test_dist`"
      },
      {
        "user": "dmgburg",
        "created_at": "2020-10-26T14:43:14Z",
        "body": "@den-crane \r\n Clusters layout\r\n```\r\n <remote_servers>\r\n        <cubes>\r\n            <shard>\r\n                <internal_replication>true</internal_replication>\r\n                <replica>\r\n                    <host>server</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n                <replica>\r\n                    <host>server2</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n        </cubes>\r\n    </remote_servers>\r\n```\r\n\r\nselect *, _shard_num from test_dist;\r\n\r\n```\r\n+----------+---+----------+\r\n|date      |key|_shard_num|\r\n+----------+---+----------+\r\n|2020-01-01|2  |1         |\r\n|2020-01-01|1  |1         |\r\n|2020-01-03|3  |1         |\r\n|2020-01-01|2  |1         |\r\n|2020-01-01|1  |1         |\r\n+----------+---+----------+\r\n\r\n```\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-26T15:00:28Z",
        "body": "@dmgburg you have confused SHARD vs REPLICA\r\n\r\nYou  config for 2 shards  must be \r\n```\r\nshard \r\n   replica\r\nshard\r\n   replica\r\n```\r\n\r\n```\r\n<remote_servers>\r\n       <cubes>\r\n           <shard>\r\n               <internal_replication>true</internal_replication>\r\n               <replica>\r\n                   <host>server</host>\r\n                   <port>9000</port>\r\n               </replica>\r\n           </shard>            \r\n           <shard>             \r\n               <replica>\r\n                   <host>server2</host>\r\n                   <port>9000</port>\r\n               </replica>\r\n           </shard>\r\n       </cubes>\r\n   </remote_servers>\r\n\r\n```\r\n\r\n---\r\n\r\nIf each of 2 shards have 3 replicas config have to be\r\n```\r\nshard \r\n   replica\r\n   replica\r\n   replica\r\nshard\r\n   replica\r\n   replica\r\n   replica\r\n```"
      }
    ]
  },
  {
    "number": 15953,
    "title": "why my sql produces  duplicate records?",
    "created_at": "2020-10-14T02:55:07Z",
    "closed_at": "2020-10-14T11:22:02Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15953",
    "body": "i wish to produce 50000000 different records with following commands, but failed\r\n```sql\r\ncreate table sfz(code Int64)ENGINE = MergeTree()order by code;\r\ninsert into sfz select (110000+n1.number)*1E12 + (20000101+n2.number)*1E4+n3.number from numbers(1000)n1,numbers(500)n2,numbers(100)n3;\r\n\r\nSELECT count(*)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500count()\u2500\u2510\r\n\u2502 50000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT countDistinct(code)\r\nFROM sfz\r\n\r\n\u250c\u2500uniqExact(code)\u2500\u2510\r\n\u2502         3500000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT \r\n    max(code), \r\n    min(code)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u2510\r\n\u2502 110999200006000096 \u2502 110000200001010000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15953/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2020-10-14T04:16:05Z",
        "body": "if i use String data type, the result is right\r\n```sql\r\ncreate table sfz2(code String)ENGINE = MergeTree()order by code;\r\n\r\ninsert into sfz2 select toString(110000+n1.number)|| toString(20000101+n2.number)||'0'||toString(n3.number) from numbers(1000)n1,numbers(500)n2,numbers(100)n3;\r\n\r\nSELECT countDistinct(code)\r\nFROM sfz2\r\n\r\n\u250c\u2500uniqExact(code)\u2500\u2510\r\n\u2502        50000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT \r\n    max(code), \r\n    min(code)\r\nFROM sfz2\r\n\r\n\u250c\u2500max(code)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500min(code)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 11099920000600099 \u2502 1100002000010100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-14T04:22:58Z",
        "body": "i realized the problem was caused by big number\r\n```sql\r\ntruncate table sfz;\r\ninsert into sfz select (110000+n1.number)*toInt64(1E12) + (20000101+n2.number)*toInt64(1E4)+n3.number from numbers(1000)n1,numbers(500)n2,numbers(100)n3;\r\nSELECT \r\n    max(code), \r\n    min(code)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u2510\r\n\u2502 110999200006000099 \u2502 110000200001010000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT countDistinct(code)\r\nFROM sfz\r\n\r\n\u250c\u2500uniqExact(code)\u2500\u2510\r\n\u2502        50000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-10-14T11:21:00Z",
        "body": "Yes, the value may not fit in `Int64` type:\r\n\r\n```\r\nmilovidov-desktop :) SELECT toInt64(1e18)\r\n\r\nSELECT toInt64(1000000000000000000.)\r\n\r\nQuery id: a33fad40-ac0b-491b-813a-41d749f725c5\r\n\r\n\u250c\u2500toInt64(1000000000000000000.)\u2500\u2510\r\n\u2502           1000000000000000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n\r\nmilovidov-desktop :) SELECT toInt64(1e19)\r\n\r\nSELECT toInt64(10000000000000000000.)\r\n\r\nQuery id: 71e95f22-a6a1-4e01-9c0a-4f748ec09023\r\n\r\n\u250c\u2500toInt64(10000000000000000000.)\u2500\u2510\r\n\u2502           -9223372036854775808 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nmilovidov-desktop :) SELECT toUInt64(1e19)\r\n\r\nSELECT toUInt64(10000000000000000000.)\r\n\r\nQuery id: 6ad6fe7a-127d-4733-b5e6-f7ee033ba71c\r\n\r\n\u250c\u2500toUInt64(10000000000000000000.)\u2500\u2510\r\n\u2502            10000000000000000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n\r\nmilovidov-desktop :) SELECT toUInt64(1e20)\r\n\r\nSELECT toUInt64(100000000000000000000.)\r\n\r\nQuery id: ad01db14-3f82-4658-93a9-677103dc02e4\r\n\r\n\u250c\u2500toUInt64(100000000000000000000.)\u2500\u2510\r\n\u2502                                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-15T08:14:12Z",
        "body": "avg() 's result is wrong for Int64\r\n```sql\r\nSELECT\r\n    count(*),\r\n    max(code),\r\n    min(code),\r\n    avg(code),\r\n    substr(toString(code), 1, 4) AS k\r\nFROM sfz\r\nGROUP BY k\r\nORDER BY k ASC\r\n\r\n\u250c\u2500count()\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500avg(code)\u2500\u252c\u2500k\u2500\u2500\u2500\u2500\u2510\r\n\u2502 5000000 \u2502 110099200006000099 \u2502 110000200001010000 \u2502  114208568606.46927 \u2502 1100 \u2502\r\n\u2502 5000000 \u2502 110199200006000099 \u2502 110100200001010000 \u2502   501790570574.8905 \u2502 1101 \u2502\r\n\u2502 5000000 \u2502 110299200006000099 \u2502 110200200001010000 \u2502   889372572543.3118 \u2502 1102 \u2502\r\n\u2502 5000000 \u2502 110399200006000099 \u2502 110300200001010000 \u2502  1276954574511.7332 \u2502 1103 \u2502\r\n\u2502 5000000 \u2502 110499200006000099 \u2502 110400200001010000 \u2502  1664536576480.1545 \u2502 1104 \u2502\r\n\u2502 5000000 \u2502 110599200006000099 \u2502 110500200001010000 \u2502 -1637230236293.3347 \u2502 1105 \u2502\r\n\u2502 5000000 \u2502 110699200006000099 \u2502 110600200001010000 \u2502 -1249648234324.9133 \u2502 1106 \u2502\r\n\u2502 5000000 \u2502 110799200006000099 \u2502 110700200001010000 \u2502  -862066232356.4922 \u2502 1107 \u2502\r\n\u2502 5000000 \u2502 110899200006000099 \u2502 110800200001010000 \u2502  -474484230388.0709 \u2502 1108 \u2502\r\n\u2502 5000000 \u2502 110999200006000099 \u2502 110900200001010000 \u2502  -86902228419.64958 \u2502 1109 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n10 rows in set. Elapsed: 1.667 sec. Processed 50.00 million rows, 400.00 MB (30.00 million rows/s., 240.01 MB/s.)\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-15T08:17:17Z",
        "body": "add toFoloat64() can get right result\r\n```sql\r\nSELECT\r\n    count(*),\r\n    max(code),\r\n    min(code),\r\n    avg(toFloat64(code)),\r\n    substr(toString(code), 1, 4) AS k\r\nFROM sfz\r\nGROUP BY k\r\nORDER BY k ASC\r\n\r\n\u250c\u2500count()\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500max(code)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500min(code)\u2500\u252c\u2500avg(toFloat64(code))\u2500\u252c\u2500k\u2500\u2500\u2500\u2500\u2510\r\n\u2502 5000000 \u2502 110099200006000099 \u2502 110000200001010000 \u2502   110049700003509260 \u2502 1100 \u2502\r\n\u2502 5000000 \u2502 110199200006000099 \u2502 110100200001010000 \u2502   110149700003674400 \u2502 1101 \u2502\r\n\u2502 5000000 \u2502 110299200006000099 \u2502 110200200001010000 \u2502   110249700003453650 \u2502 1102 \u2502\r\n\u2502 5000000 \u2502 110399200006000099 \u2502 110300200001010000 \u2502   110349700003415840 \u2502 1103 \u2502\r\n\u2502 5000000 \u2502 110499200006000099 \u2502 110400200001010000 \u2502   110449700003517810 \u2502 1104 \u2502\r\n\u2502 5000000 \u2502 110599200006000099 \u2502 110500200001010000 \u2502   110549700003510430 \u2502 1105 \u2502\r\n\u2502 5000000 \u2502 110699200006000099 \u2502 110600200001010000 \u2502   110649700003511710 \u2502 1106 \u2502\r\n\u2502 5000000 \u2502 110799200006000099 \u2502 110700200001010000 \u2502   110749700003574190 \u2502 1107 \u2502\r\n\u2502 5000000 \u2502 110899200006000099 \u2502 110800200001010000 \u2502   110849700003585060 \u2502 1108 \u2502\r\n\u2502 5000000 \u2502 110999200006000099 \u2502 110900200001010000 \u2502   110949700003438200 \u2502 1109 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-10-15T08:47:31Z",
        "body": "sum() is also bad, if i use Int128 type, it can do right, but avg() is still not accurate\r\n```sql\r\nSELECT sum(code)\r\nFROM sfz\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(code)\u2500\u2510\r\n\u2502 682658504670491840 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nset allow_experimental_bigint_types =1;\r\ncreate table sfz2(code Int128)ENGINE = MergeTree()order by code;\r\ninsert into sfz2 select * from sfz limt 10000;\r\nSELECT sum(code)\r\nFROM sfz2\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(code)\u2500\u2510\r\n\u2502 1100002000015050495000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nSELECT avg(code)\r\nFROM sfz2\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500avg(code)\u2500\u2510\r\n\u2502 110000200001505060 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      }
    ]
  },
  {
    "number": 14419,
    "title": "No data in replicated table",
    "created_at": "2020-09-02T17:41:29Z",
    "closed_at": "2020-09-03T14:21:46Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14419",
    "body": "I have two replicas. Data are inserted into table in replica A. Table counterpart in replica B is empty though. Data are not replicated between replicas. I tried recreating table in replica B but it did not trigger the sync.\r\n\r\nReplication for other tables is working just fine.\r\n\r\nWhat can I do to debug the issue? `show create table` gives same result for both replicas.\r\n\r\nClickHouse version 20.6",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14419/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-02T17:47:09Z",
        "body": "check (at both nodes) : select * from system.replication_queue \r\n\r\nwrong ZK path? different shards?"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-02T17:50:07Z",
        "body": "I don't have multiple shards.\r\n\r\nreplication queue does not contain any records for the target table\r\n\r\npath is the same (reported by show create table)\r\n\r\n\ud83e\udd14 "
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-02T17:52:25Z",
        "body": "Show `grep tablename clickhouseserver.log`"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-02T17:56:41Z",
        "body": "I tried grepping `cat /var/log/clickhouse-server/clickhouse-server.log` in both replicas but for this particular table contains no logs (it does have some warnings for larger tables though)."
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-02T18:05:33Z",
        "body": "It could be anything. How did you copy/pasted / attached zk path when you created a table?\r\n\r\nFor example these two strings are different. \r\nLatin:    /ccccc/ \r\n\u0421yrrylic: /\u0441\u0441\u0441\u0441\u0441/ \r\n\r\nCheck zookeeper select * from system.zookeeper where path = '/..zk_path/replicas\r\nCheck select * from system.replicas where table = ...\r\n"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-03T13:29:12Z",
        "body": "> select * from system.zookeeper where path = '/..zk_path/replicas\r\nGives result on both replicas. \r\n\r\nBut on the B, there's `replicas\tlast added replica: A` (version: 1)\r\nFor fully replicated tables it's `replicas\tlast added replica: B` (version: 4)\r\n\r\n`system.replicas` says the `total_replicas` is 1. Seems like those tables are disconnected on both replicas. Is there a way to get in in sync somehow?\r\n\r\n> How did you copy/pasted / attached zk path when you created a table?\r\n\r\nI did it a long time ago but It was simple CREATE TABLE ON CLUSTER I believe"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-03T14:18:37Z",
        "body": "Check at both servers that zhash is identical \r\n```\r\n\r\nselect replica_name, total_replicas, active_replicas, zookeeper_path, \r\n           cityHash64(zookeeper_path) zhash, hex(zookeeper_path) \r\nfrom system.replicas\r\nwhere table = '....'\r\n```"
      }
    ]
  },
  {
    "number": 12750,
    "title": "Exception with INSERT INTO ",
    "created_at": "2020-07-24T13:12:44Z",
    "closed_at": "2020-07-24T19:07:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12750",
    "body": "Hello !\r\n\r\n**Describe the bug**\r\nI try to use INSERT INTO  a table (not a view !) using the AggregatingMergeTree() engine, or in a Distributed table created \"on\" the table using the AggregatingMergeTree ; in both case I have the following exception : \r\n> Code: 20. DB::Exception: Received from localhost:9000. DB::Exception: Number of columns doesn't match.\r\n\r\n**How to reproduce**\r\nClickHouse server version 20.5.2 revision 54435\r\nClickHouse client version 20.5.2.7 (official build)\r\n\r\nI didn't change the setting,\r\n\r\n1>  I have created a Distributed table \"visits_distributed_v2\", and filled it.\r\n2> I have created the \"all_visitor\" table :\r\n```\r\nCREATE TABLE poc.all_visitor\r\n(\r\n    `VisitorCode` String,\r\n    `arrayVisitDuration` AggregateFunction(groupArray, Int64)\r\n)\r\nENGINE = AggregatingMergeTree()\r\nPARTITION BY VisitorCode\r\nORDER BY VisitorCode\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\n3> I have created the Distributed table all_visitor_distributed :\r\n```\r\nCREATE TABLE poc.all_visitor_distributed AS poc.all_visitor\r\nENGINE = Distributed(test_shard_localhost, poc, all_visitor, rand())\r\n```\r\n\r\n4> When I try to insert data from the \"visits_distributed_v2\" table in the Distributed Table or the \"source\" one, I have the issue.\r\n```\r\nINSERT INTO poc.all_visitor_distributed SELECT (VisitorCode, groupArrayState(VisitDuration))\r\nFROM poc.visits_distributed_v2\r\nGROUP BY VisitorCode\r\n```\r\n\r\n**Expected behavior**\r\nIt could totally by an error on my side, I already checked the name of the columns or their types, the number of columns seems to match but the error is misleading that's why I need help :)\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12750/comments",
    "author": "RonanMorgan",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-07-24T15:55:21Z",
        "body": ">SELECT (VisitorCode, groupArrayState(VisitDuration))\r\n\r\nbecause of ( )\r\n\r\ntry `SELECT VisitorCode, groupArrayState(VisitDuration)`\r\n\r\n( ) -- makes a special type - Tuple, syntax sugar for a  function -- tuple\r\n```\r\n\r\nselect 1, 2, tuple(1,2), (1,2)\r\n\u250c\u25001\u2500\u252c\u25002\u2500\u252c\u2500tuple(1, 2)\u2500\u252c\u2500tuple(1, 2)\u2500\u2510\r\n\u2502 1 \u2502 2 \u2502 (1,2)       \u2502 (1,2)       \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ndesc(select 1, 2, tuple(1,2) y, (1,2) x)\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\r\n\u2502 1    \u2502 UInt8               \u2502\r\n\u2502 2    \u2502 UInt8               \u2502\r\n\u2502 y    \u2502 Tuple(UInt8, UInt8) \u2502\r\n\u2502 x    \u2502 Tuple(UInt8, UInt8) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\r\n```"
      }
    ]
  },
  {
    "number": 11062,
    "title": "DB::Exception: Cannot get value from Function.",
    "created_at": "2020-05-20T06:18:31Z",
    "closed_at": "2020-05-20T07:28:36Z",
    "labels": [
      "question",
      "st-fixed",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/11062",
    "body": "Hello, are you okay with COVID-19?\r\n\r\nI am using ClickHouse very well, but I got an exception.\r\n\r\nIn the below, I wrote queries to run that lead to unexpected result.\r\n\r\nThank you.\r\n\r\n**Exception**\r\n```\r\nCode: 48. DB::Exception: Received from localhost:19000. DB::Exception: Cannot get value from Function.\r\n```\r\n\r\n**Server Version**\r\n```20.3.5.21```\r\n\r\n**Qeuries for Reproduction**\r\n```SQL\r\nCREATE TABLE test_ga\r\n(\r\n    `d` Date,\r\n    `id` UInt64,\r\n    `duration` UInt64,\r\n    `gender` String\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY d\r\nORDER BY id;\r\n\r\nINSERT INTO test_ga SELECT\r\n    number % 100,\r\n    number,\r\n    number * 1000,\r\n    if((number % 2) = 0, 'm', 'f')\r\nFROM system.numbers\r\nLIMIT 11111110;\r\n```\r\n\r\n```SQL\r\nWITH\r\n    groupArrayIf(duration / 1000, (duration > 0) AND (duration < ((1000 * 3600) * 6))) AS avg_duration_group_array,\r\n    round(medianExactIf(duration, duration > 0) / 1000) AS median_duration,\r\n    round(arrayReduce('sum', arrayFilter(x -> (x < (arrayReduce('avg', avg_duration_group_array) + (2 * arrayReduce('stddevPop', avg_duration_group_array)))), avg_duration_group_array)) / 60) AS avg_duration\r\nSELECT\r\n    median_duration,\r\n    avg_duration\r\nFROM test_ga\r\nGROUP BY gender\r\nORDER BY median_duration DESC\r\nLIMIT 100;\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/11062/comments",
    "author": "achimbab",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-20T07:21:11Z",
        "body": "@achimbab We're alright.\r\n\r\n> 20.3.5.21\r\n\r\nWe have already released four bugfixes after this version, could you please check 20.3.9?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-20T07:27:51Z",
        "body": "I have checked master version and the issue does not reproduce."
      }
    ]
  },
  {
    "number": 10891,
    "title": "How to create table with MATERIALIZED column as select from",
    "created_at": "2020-05-13T14:39:34Z",
    "closed_at": "2020-05-13T14:53:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10891",
    "body": "Please? Help me.\r\n\r\nI want to know how create table with MATERIALIZED column from another select with input variables.\r\nI do next:\r\n\r\nCREATE TABLE ANALYST.DATA_TEST1 (\r\n `ids` UInt16,\r\n `timestamp` String,\r\n `code` UInt64,\r\n `id` UUID MATERIALIZED generateUUIDv4(),\r\n `timemoment` DateTime MATERIALIZED CAST(formatDateTime(toDateTime(substring(timestamp, 1, 19)), '%Y-%m-%d %T'), 'DateTime'),\r\n `adate` Date MATERIALIZED toDate(timemoment),\r\n `idate` Date MATERIALIZED toDate(now()),\r\n `moduleid` UInt16 MATERIALIZED (SELECT if((SELECT uniqExact(ANALYST.HOME.moduleid) FROM ANALYST.HOME WHERE ANALYST.HOME.id_soa = ids) > 1, 379, 339))\r\n) ENGINE = MergeTree(adate, (moduleid, timemoment, code), 8192);\r\n\r\nbut execution finshed with error DB::Exception: Missing columns: 'ids' while processing query: 'SELECT uniqExact(ANALYST.HOME.moduleid) FROM ANALYST.HOME WHERE id_soa = ids', required columns: 'ids' 'moduleid' 'id_soa', source columns: 'moduleid' 'description' 'host_min' 'id_soa' 'network' 'host_max'\r\n\r\nMaybe I'm wrong and it's impossible!? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10891/comments",
    "author": "xap9i",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-05-13T14:50:54Z",
        "body": "It's impossible. You can use `select from` in MATERIALIZED column. You can use only `dictGet<Type>`. "
      }
    ]
  },
  {
    "number": 10673,
    "title": "UInt16\" is greater than max allowed Date value",
    "created_at": "2020-05-05T09:39:38Z",
    "closed_at": "2020-05-05T14:08:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10673",
    "body": "Hi, can someone quickly clarify what error is this?\r\n\r\nI tried loading Parquet file into table i have not used UInt16 data type anywhere in the table but getting the below error\r\n\r\nERROR:  Input value 4294941729 of a column \"UInt16\" is greater than max allowed Date value, which is 49710",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10673/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-05-05T12:06:05Z",
        "body": "It looks like you are importing some column from Parquet into Date type in ClickHouse.\r\nDate type is represented by UInt16 containing the number of days since 1970-01-01.\r\nBut in Parquet file you have some big number (4294941729) or negative number (-25566) instead."
      }
    ]
  },
  {
    "number": 9915,
    "title": "toStartHour doesn't work in MV?",
    "created_at": "2020-03-28T23:16:42Z",
    "closed_at": "2020-03-28T23:36:45Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9915",
    "body": "**Describe the bug**\r\n`toStartOfHour`  in a MV returns 0.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: `20.3.4.10 (official build)` in Ubuntu bionic\r\n* Which interface to use, if matters: `clickhouse-client -m -n`\r\n* Non-default settings, if any: none \r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\nCREATE TABLE test.a (\r\n  t DateTime\r\n)\r\nENGINE=Memory();\r\nCREATE TABLE test.b\r\n(\r\n  t DateTime\r\n)\r\nENGINE=Memory();\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t)\r\nFROM test.a;\r\n```\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\n```\r\nINSERT INTO test.a (t) VALUES ('2020-03-28 11:22:33');\r\n```\r\n* Queries to run that lead to unexpected result\r\n```\r\nSELECT * FROM test.b;\r\n```\r\n**Expected behavior**\r\nI expected one row from table `b`\r\n```\r\n2020-03-28 11:00:00\r\n```\r\nThe actual result is table `b` is empty.\r\n\r\n**Error message and/or stacktrace**\r\nNo errors in the log.\r\n\r\n**Additional context**\r\nOne row is inserted if a column is added.  The value for column `t` is still wrong.\r\n\r\n```\r\nCREATE TABLE test.a (\r\n  t DateTime,\r\n  name String\r\n)\r\nENGINE=Memory();\r\nCREATE TABLE test.b\r\n(\r\n  t DateTime,\r\n  name String\r\n)\r\nENGINE=Memory();\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t),\r\n    name\r\nFROM test.a;\r\nINSERT INTO test.a (t, name) VALUES ('2020-03-28 11:22:33', 'myname');\r\nSELECT * FROM test.b;\r\n```\r\nThe output is\r\n```\r\n0000-00-00 00:00:00\tmyname\r\n```\r\n\r\nThe same SELECT returns correct values if I run it manually in the client.\r\n\r\n```\r\nSELECT\r\n    toStartOfHour(t),\r\n    name\r\nFROM test.a\r\n\r\n\u250c\u2500\u2500\u2500\u2500toStartOfHour(t)\u2500\u252c\u2500name\u2500\u2500\u2500\u2510\r\n\u2502 2020-03-28 11:00:00 \u2502 myname \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9915/comments",
    "author": "knoguchi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-28T23:34:40Z",
        "body": "Column names must be the same in a MV select and the target (TO) table \r\n\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t) **as t**\r\nFROM test.a;\r\n\r\nMV uses column names when it does insert into the target table."
      }
    ]
  },
  {
    "number": 9870,
    "title": "Cannot replicate table from 19.3.3 to 20.3.3",
    "created_at": "2020-03-25T19:17:58Z",
    "closed_at": "2020-03-30T14:24:47Z",
    "labels": [
      "question",
      "backward compatibility",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9870",
    "body": "We created a new node using version 20.3.3 and tried to replicate a number of tables from a 19.3.3 node.  The initial replication worked, but upon restarting the 20.3.3 node got the following failure (this happened for several tables):\r\n\r\n```\r\nExisting table metadata in ZooKeeper differs in index granularity bytes. Stored in\r\nZooKeeper: 10485760, local: 0: Cannot attach table `<db>`.`<table>` from metadata\r\nfile /opt/data/clickhouse/metadata/<db>/<table> from query ATTACH TABLE <table>\r\n(`datetime` DateTime, `kafka_time` DateTime, `hostname` String, `message` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/<db.table>, '{replica}')\r\nPARTITION BY toDate(datetime) ORDER BY (datetime, hostname) SETTINGS\r\nindex_granularity = 8192\r\n```\r\n\r\nIndex granularity on both tables is 8192, metadata .sql file is identical.  metadata from zookeeper node:\r\n\r\n```\r\nmetadata format version: 1\r\ndate column: \r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 0\r\nsign column: \r\nprimary key: datetime, hostname\r\ndata format version: 1\r\npartition key: toDate(datetime)\r\n```\r\n\r\nStack trace:\r\n\r\n```0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x102d352c in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8f2d989 in /usr/bin/clickhouse\r\n2. ? @ 0xd94cdce in /usr/bin/clickhouse\r\n3. DB::StorageReplicatedMergeTree::checkTableStructure(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xd5b426b in /usr/bin/clickhouse\r\n4. DB::StorageReplicatedMergeTree::StorageReplicatedMergeTree(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bo\r\nol, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata const&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char\r\n> > const&, DB::MergeTreeData::MergingParams const&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool) @ 0xd5d9b4b in /usr/bin/clickhouse\r\n5. ? @ 0xd957dba in /usr/bin/clickhouse\r\n6. std::__1::__function::__func<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&), std::__1::allocator<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&)>, std::__1::shared_ptr<DB::IStorage> (DB::Sto\r\nrageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) @ 0xd95b2d3 in /usr/bin/clickhouse\r\n7. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) cons\r\nt @ 0xd4fbc4c in /usr/bin/clickhouse\r\n8. DB::createTableFromAST(DB::ASTCreateQuery, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool\r\n) @ 0xcedc09e in /usr/bin/clickhouse\r\n9. ? @ 0xced2bcf in /usr/bin/clickhouse\r\n10. ? @ 0xced3381 in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8f515e7 in /usr/bin/clickhouse\r\n12. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleI\r\nmpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x8f51c34 in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8f50b07 in /usr/bin/clickhouse\r\n14. ? @ 0x8f4f00f in /usr/bin/clickhouse\r\n15. start_thread @ 0x7e65 in /usr/lib64/libpthread-2.17.so\r\n16. clone @ 0xfe88d in /usr/lib64/libc-2.17.so\r\n (version 20.3.3.6 (official build))\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9870/comments",
    "author": "genzgd",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-25T19:25:37Z",
        "body": "20.3.3 by default uses adaptive index granularity. Such feature is not existed in 19.3.3.\r\n\r\nCreate tables without `adaptive index granularity`\r\n\r\n20.3.3\r\ncreate table ....\r\nsettings index_granularity =8192,  index_granularity_bytes = 0;\r\n\r\n19.3.3\r\ncreate table ....\r\nsettings index_granularity =8192;\r\n\r\nBut there is one problem. LZ4 compression format is incompatible < 19.7 and  >= 19.7 .\r\nYou can temporary use replication 20.3.3 <-> 19.3.3. But you need to upgrade 19.3.3 as soon as possible."
      }
    ]
  },
  {
    "number": 9058,
    "title": "When import file, String type contains \"BACKSLASH TAB\".",
    "created_at": "2020-02-10T01:00:15Z",
    "closed_at": "2020-02-11T00:59:54Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9058",
    "body": "When I import tsv file and there are 64 columns, one String column(column 57) contains \"BACKSLASH and TAB\" so, it can`t import tsv file.\r\n\r\nversion - 20.1.2.4 \r\n\r\n```sql\r\n# cat /home/test/data/group_06/test_data.tsv | clickhouse-client --password=123456 --database=bts --query=\"INSERT INTO t_order_mergetree FORMAT TabSeparatedWithNames\" --input_format_tsv_empty_as_default=1;\r\nCode: 27. DB::Exception: Cannot parse input: expected \\t before: \\nabc\\t20190528-007178\\t2019-05-28 11:28:38\\t1\\t\"\"\\tKRW\\t26345\\t26345\\t0.00\\t\\t2019-05-28 11:25:00\\tcard\\t\\tF\\t\\t0\\tT\\t1.000000\\t34000.00\\t2500.00\\tF\\tF\\tT\\t\"\"\\tNCHECKOUT\\t\"\"\\t\\t0.00\\t: (at row 965285)\r\n\r\nRow 965285:\r\nColumn 0,   name: m_id,                      type: String,                   parsed text: \"abc\"\r\n...\r\nColumn 56,  name: input_channel_detail_type,    type: Nullable(FixedString(4)), parsed text: <EMPTY>\r\nColumn 57,  name: inflow_path,                  type: Nullable(String),         parsed text: \"criteo<BACKSLASH><TAB>\"\r\nColumn 58,  name: app_order_discount_amount,    type: Nullable(Float64),        parsed text: <EMPTY>\r\nColumn 59,  name: app_product_discount_amount,  type: Nullable(Float64),        parsed text: <EMPTY>\r\nColumn 60,  name: deferred_payment_commission,  type: Nullable(Float64),        parsed text: \"0\"\r\nColumn 61,  name: seperate_delivery_count,      type: Nullable(Float64),        parsed text: \"0.00\"\r\nColumn 62,  name: balanced_price,               type: Nullable(Float64),        parsed text: \"0\"\r\nColumn 63,  name: add_paid_amount,              type: Nullable(Float64),        parsed text: \"0\"\r\nERROR: Line feed found where tab is expected. It's like your file has less columns than expected.\r\nAnd if your file have right number of columns, maybe it have unescaped backslash in value before tab, which cause tab has escaped.\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9058/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-10T16:14:43Z",
        "body": "\\ -- (char(92)) is a special character for TSV and must be escaped with `slash (\\)`\r\n\r\n```\r\nSELECT char(92)\r\nFORMAT TSV\r\n\r\n\\\\\r\n```"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-10T23:38:27Z",
        "body": "Can`t I use DOUBLE QUOTE in tsv also?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-11T00:59:06Z",
        "body": "you can.\r\nDOUBLE QUOTE in tsv is a usual character, not special.\r\n\r\n```\r\nSELECT\r\n    '\"aaaa',\r\n    111\r\nFORMAT TSV\r\n\r\n\"aaaa\t111\r\n```"
      }
    ]
  },
  {
    "number": 8592,
    "title": "Memory limit (for query) exceeded on SELECT",
    "created_at": "2020-01-09T13:29:06Z",
    "closed_at": "2020-01-11T17:52:42Z",
    "labels": [
      "question",
      "memory"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8592",
    "body": "Hi,\r\n\r\nI am running a quite complex SELECT query on a clickhouse 19.16.5.15, and I have the following error :\r\n`DB::Exception: Memory limit (for query) exceeded: would use 723.47 MiB (attempt to allocate chunk of 5201580 bytes), maximum: 720.00 MiB.`\r\n\r\nPreviously, playing with max_bytes_before_external_sort and max_bytes_before_external_group_by (setting them to half of the max_memory_usage), allow me to run such queries, but it is no more be the case. \r\nMy current configuration is :\r\n```\r\n<max_memory_usage>754974720</max_memory_usage>\r\n<max_bytes_before_external_sort>377487360</max_bytes_before_external_sort>\r\n<max_bytes_before_external_group_by>377487360</max_bytes_before_external_group_by>\r\n<max_memory_usage_for_all_queries>1509949440</max_memory_usage_for_all_queries>\r\n```\r\n\r\nIf I activated debug log, I could see that the query seems to go on disk (what I expect), since I get several:\r\n`2020.01.08 14:22:46.881201 [ 46 ] {767e6850-f1b4-49ae-af81-a62e8e24573c} <Debug> Aggregator: Writing part of aggregation data into temporary file /data/tmp/tmp30010qaaaaa.`\r\n\r\nNevertheless, I finally got this stacktrace:\r\n```\r\n0. 0x3582798 StackTrace::StackTrace() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n1. 0x358b1df DB::Exception::Exception(std::string const&, int) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n2. 0x5bf6a99 DB::IBlockInputStream::checkTimeLimit() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n3. 0x5bfb2d0 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n4. 0x6240336 DB::FilterBlockInputStream::readImpl() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n5. 0x5bfb2f5 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n6. 0x62373a8 DB::ExpressionBlockInputStream::readImpl() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n7. 0x5bfb2f5 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n8. 0x626c7a9 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n9. 0x626ce6b ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n10. 0x35bf902 ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n11. 0x722562f execute_native_thread_routine /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n12. 0x7fe8c1fdfdd5 start_thread /usr/lib64/libpthread-2.17.so\r\n13. 0x7fe8c1b04ead clone /usr/lib64/libc-2.17.so\r\n```\r\n\r\nI also try to decrease max_insert_block_size and max_block_size, without any improvement.\r\n\r\nDo you know other settings I could play for allowing this query to be executed ? (even if it is slow).\r\n\r\nThe exact query is the following:\r\n```\r\nselect\r\n\t\t/* @TOPN_SELECT_PART@ */\r\n\t\t\r\n\t\tany(ifNull(if(c1 = 0, '(UNDEFINED)',dictGetString('agents', 'name', toUInt64(c1))),'(UNDEFINED)')) as `co1`, \r\n\t\tany(ifNull(if(c2 = 0, '(UNDEFINED)',dictGetString('applications', 'name', toUInt64(c2))),'(UNDEFINED)')) as `co2`, \r\n\t\tany(ifNull(if(c3 = 0, '(UNDEFINED)',dictGetString('netflow_sources', 'name', toUInt64(c3))),'(UNDEFINED)')) as `co3`, \r\n\t\tany(ifNull(if(c4 = 0, '(UNDEFINED)',dictGetString('wakb_ip_protocols', 'name', toUInt64(c4))),'(UNDEFINED)')) as `co4`, \r\n\t\tany(ifNull(if(c5 = 0, '(UNDEFINED)',dictGetString('offices', 'name', toUInt64(c5))),'(UNDEFINED)')) as `co5`, \r\n\t\tany(ifNull(if(c6 = 0, '(UNDEFINED)',dictGetString('offices', 'name', toUInt64(c6))),'(UNDEFINED)')) as `co6`, \r\n\t\tany(ifNull(if(c7 = 0, '(UNDEFINED)',dictGetString('netflow_interfaces', 'name', toUInt64(c7))),'(UNDEFINED)')) as `co7`, \r\n\t\tany(ifNull(if(c8 = 0, '(UNDEFINED)',dictGetString('netflow_interfaces', 'name', toUInt64(c8))),'(UNDEFINED)')) as `co8`, \r\n\t\tany(ifNull(t.`ClientIp`,'(UNDEFINED)')) as `co9`, \r\n\t\tany(ifNull(t.`ServerIp`,'(UNDEFINED)')) as `co10`, \r\n\t\tany(ifNull(if(c11 = 0, '(UNDEFINED)',dictGetString('classes_of_service', 'name', toUInt64(c11))),'(UNDEFINED)')) as `co11`, \r\n\t\tany(ifNull(if(c12 = 0, '(UNDEFINED)',dictGetString('classes_of_service', 'name', toUInt64(c12))),'(UNDEFINED)')) as `co12`, \r\n\t\tany(ifNull(t.`Port`, 0)) as `co13`, \r\n\t\tany(ifNull(t.`DomainName`,'(UNDEFINED)')) as `co14`, \r\n\t\tany(ifNull(t.`Flags`, 0)) as `co15`, \r\n\t\tmax(t.`ClientNetworkTimeMax`) as `co16`, \r\n\t\tmin(t.`ClientNetworkTimeMin`) as `co17`, \r\n\t\tsum(t.`ClientNetworkTimeSum`) as `co18`, \r\n\t\tsum(t.`ClientPackets`) as `co19`, \r\n\t\tsum(t.`ClientBytes`) as `co20`, \r\n\t\tsum(t.`ClientDataBytes`) as `co21`, \r\n\t\tsum(t.`ClientDataPackets`) as `co22`, \r\n\t\tsum(t.`ServerResponseTimeSum`) as `co23`, \r\n\t\tmin(t.`ServerResponseTimeMin`) as `co24`, \r\n\t\tmax(t.`ServerResponseTimeMax`) as `co25`, \r\n\t\tsum(t.`ServerPackets`) as `co26`, \r\n\t\tsum(t.`ServerBytes`) as `co27`, \r\n\t\tsum(t.`1000msResponsesNb`) as `co28`, \r\n\t\tsum(t.`100msResponsesNb`) as `co29`, \r\n\t\tsum(t.`10msResponsesNb`) as `co30`, \r\n\t\tsum(t.`2msResponsesNb`) as `co31`, \r\n\t\tsum(t.`500msResponsesNb`) as `co32`, \r\n\t\tsum(t.`50msResponsesNb`) as `co33`, \r\n\t\tsum(t.`5msResponsesNb`) as `co34`, \r\n\t\tsum(t.`LateResponsesNb`) as `co35`, \r\n\t\tsum(t.`NewConnectionsNb`) as `co36`, \r\n\t\tmax(t.`ResponseTimeMax`) as `co37`, \r\n\t\tmin(t.`ResponseTimeMin`) as `co38`, \r\n\t\tsum(t.`ResponseTimeSum`) as `co39`, \r\n\t\tsum(t.`ResponsesNb`) as `co40`, \r\n\t\tsum(t.`RetransmissionsNb`) as `co41`, \r\n\t\tsum(t.`ServerDataBytes`) as `co42`, \r\n\t\tsum(t.`ServerDataPackets`) as `co43`, \r\n\t\tmax(t.`ServerNetworkTimeMax`) as `co44`, \r\n\t\tmin(t.`ServerNetworkTimeMin`) as `co45`, \r\n\t\tsum(t.`ServerNetworkTimeSum`) as `co46`, \r\n\t\tmax(t.`TotalNetworkTimeMax`) as `co47`, \r\n\t\tmin(t.`TotalNetworkTimeMin`) as `co48`, \r\n\t\tsum(t.`TotalNetworkTimeSum`) as `co49`, \r\n\t\tmax(t.`TotalResponseTimeMax`) as `co50`, \r\n\t\tmin(t.`TotalResponseTimeMin`) as `co51`, \r\n\t\tsum(t.`TotalResponseTimeSum`) as `co52`, \r\n\t\tmax(t.`TotalTransactionTimeMax`) as `co53`, \r\n\t\tmin(t.`TotalTransactionTimeMin`) as `co54`, \r\n\t\tsum(t.`TotalTransactionTimeSum`) as `co55`, \r\n\t\tsum(t.`TransactionsNb`) as `co56`, \r\n\t\tsum(t.`WaasDreInput`) as `co57`, \r\n\t\tsum(t.`WaasDreOutput`) as `co58`, \r\n\t\tsum(t.`WaasInputBytes`) as `co59`, \r\n\t\tsum(t.`WaasLzInput`) as `co60`, \r\n\t\tsum(t.`WaasLzOutput`) as `co61`, \r\n\t\tsum(t.`WaasOutputBytes`) as `co62`, \r\n\t\tany(t.`c1`) as `co63`, \r\n\t\tany(t.`c2`) as `co64`, \r\n\t\tany(t.`c3`) as `co65`, \r\n\t\tany(t.`c4`) as `co66`, \r\n\t\tany(t.`c5`) as `co67`, \r\n\t\tany(t.`c6`) as `co68`, \r\n\t\tany(t.`c7`) as `co69`, \r\n\t\tany(t.`c8`) as `co70`, \r\n\t\tany(t.`c11`) as `co71`, \r\n\t\tany(t.`c12`) as `co72` ,\r\n\t\tany(ranking_row) as final_ranking\r\nfrom (\r\n\tselect\r\n\t\t\t/* @OUTER_SELECT_PART@ */\r\n\t\t\tc9 as `ClientIp`,\r\n\t\t\tc10 as `ServerIp`,\r\n\t\t\tc13 as `Port`,\r\n\t\t\tc14 as `DomainName`,\r\n\t\t\tc15 as `Flags`,\r\n\t\t\tc16 as `ClientNetworkTimeMax`,\r\n\t\t\tc17 as `ClientNetworkTimeMin`,\r\n\t\t\tc18 as `ClientNetworkTimeSum`,\r\n\t\t\tc19 as `ClientPackets`,\r\n\t\t\tc20 as `ClientBytes`,\r\n\t\t\tc21 as `ClientDataBytes`,\r\n\t\t\tc22 as `ClientDataPackets`,\r\n\t\t\tc23 as `ServerResponseTimeSum`,\r\n\t\t\tc24 as `ServerResponseTimeMin`,\r\n\t\t\tc25 as `ServerResponseTimeMax`,\r\n\t\t\tc26 as `ServerPackets`,\r\n\t\t\tc27 as `ServerBytes`,\r\n\t\t\tc28 as `1000msResponsesNb`,\r\n\t\t\tc29 as `100msResponsesNb`,\r\n\t\t\tc30 as `10msResponsesNb`,\r\n\t\t\tc31 as `2msResponsesNb`,\r\n\t\t\tc32 as `500msResponsesNb`,\r\n\t\t\tc33 as `50msResponsesNb`,\r\n\t\t\tc34 as `5msResponsesNb`,\r\n\t\t\tc35 as `LateResponsesNb`,\r\n\t\t\tc36 as `NewConnectionsNb`,\r\n\t\t\tc37 as `ResponseTimeMax`,\r\n\t\t\tc38 as `ResponseTimeMin`,\r\n\t\t\tc39 as `ResponseTimeSum`,\r\n\t\t\tc40 as `ResponsesNb`,\r\n\t\t\tc41 as `RetransmissionsNb`,\r\n\t\t\tc42 as `ServerDataBytes`,\r\n\t\t\tc43 as `ServerDataPackets`,\r\n\t\t\tc44 as `ServerNetworkTimeMax`,\r\n\t\t\tc45 as `ServerNetworkTimeMin`,\r\n\t\t\tc46 as `ServerNetworkTimeSum`,\r\n\t\t\tc47 as `TotalNetworkTimeMax`,\r\n\t\t\tc48 as `TotalNetworkTimeMin`,\r\n\t\t\tc49 as `TotalNetworkTimeSum`,\r\n\t\t\tc50 as `TotalResponseTimeMax`,\r\n\t\t\tc51 as `TotalResponseTimeMin`,\r\n\t\t\tc52 as `TotalResponseTimeSum`,\r\n\t\t\tc53 as `TotalTransactionTimeMax`,\r\n\t\t\tc54 as `TotalTransactionTimeMin`,\r\n\t\t\tc55 as `TotalTransactionTimeSum`,\r\n\t\t\tc56 as `TransactionsNb`,\r\n\t\t\tc57 as `WaasDreInput`,\r\n\t\t\tc58 as `WaasDreOutput`,\r\n\t\t\tc59 as `WaasInputBytes`,\r\n\t\t\tc60 as `WaasLzInput`,\r\n\t\t\tc61 as `WaasLzOutput`,\r\n\t\t\tc62 as `WaasOutputBytes`,\r\n\t\t\tc1 as `c1`,\r\n\t\t\tc2 as `c2`,\r\n\t\t\tc3 as `c3`,\r\n\t\t\tc4 as `c4`,\r\n\t\t\tc5 as `c5`,\r\n\t\t\tc6 as `c6`,\r\n\t\t\tc7 as `c7`,\r\n\t\t\tc8 as `c8`,\r\n\t\t\tc11 as `c11`,\r\n\t\t\tc12 as `c12`,\r\n\t\t\trowNumberInAllBlocks() as ranking_row\r\n\tfrom\r\n\t\t(select \r\n\t\t\t\t/* @AGGR_OUT_PART@ */\r\n\t\t\t\taggr_in.`Agent` as c1,\r\n\t\t\t\taggr_in.`Application` as c2,\r\n\t\t\t\taggr_in.`Source` as c3,\r\n\t\t\t\taggr_in.`Protocol` as c4,\r\n\t\t\t\taggr_in.`ClientOffice` as c5,\r\n\t\t\t\taggr_in.`ServerOffice` as c6,\r\n\t\t\t\taggr_in.`ClientInterface` as c7,\r\n\t\t\t\taggr_in.`ServerInterface` as c8,\r\n\t\t\t\taggr_in.`ClientIp` as c9,\r\n\t\t\t\taggr_in.`ServerIp` as c10,\r\n\t\t\t\taggr_in.`ClientCos` as c11,\r\n\t\t\t\taggr_in.`ServerCos` as c12,\r\n\t\t\t\taggr_in.`Port` as c13,\r\n\t\t\t\taggr_in.`DomainName` as c14,\r\n\t\t\t\taggr_in.`Flags` as c15,\r\n\t\t\t\tmax(aggr_in.`ClientNetworkTimeMax`) as c16,\r\n\t\t\t\tmin(aggr_in.`ClientNetworkTimeMin`) as c17,\r\n\t\t\t\tsum(aggr_in.`ClientNetworkTimeSum`) as c18,\r\n\t\t\t\tsum(aggr_in.`ClientPackets`) as c19,\r\n\t\t\t\tsum(aggr_in.`ClientBytes`) as c20,\r\n\t\t\t\tsum(aggr_in.`ClientDataBytes`) as c21,\r\n\t\t\t\tsum(aggr_in.`ClientDataPackets`) as c22,\r\n\t\t\t\tsum(aggr_in.`ServerResponseTimeSum`) as c23,\r\n\t\t\t\tmin(aggr_in.`ServerResponseTimeMin`) as c24,\r\n\t\t\t\tmax(aggr_in.`ServerResponseTimeMax`) as c25,\r\n\t\t\t\tsum(aggr_in.`ServerPackets`) as c26,\r\n\t\t\t\tsum(aggr_in.`ServerBytes`) as c27,\r\n\t\t\t\tsum(aggr_in.`1000msResponsesNb`) as c28,\r\n\t\t\t\tsum(aggr_in.`100msResponsesNb`) as c29,\r\n\t\t\t\tsum(aggr_in.`10msResponsesNb`) as c30,\r\n\t\t\t\tsum(aggr_in.`2msResponsesNb`) as c31,\r\n\t\t\t\tsum(aggr_in.`500msResponsesNb`) as c32,\r\n\t\t\t\tsum(aggr_in.`50msResponsesNb`) as c33,\r\n\t\t\t\tsum(aggr_in.`5msResponsesNb`) as c34,\r\n\t\t\t\tsum(aggr_in.`LateResponsesNb`) as c35,\r\n\t\t\t\tsum(aggr_in.`NewConnectionsNb`) as c36,\r\n\t\t\t\tmax(aggr_in.`ResponseTimeMax`) as c37,\r\n\t\t\t\tmin(aggr_in.`ResponseTimeMin`) as c38,\r\n\t\t\t\tsum(aggr_in.`ResponseTimeSum`) as c39,\r\n\t\t\t\tsum(aggr_in.`ResponsesNb`) as c40,\r\n\t\t\t\tsum(aggr_in.`RetransmissionsNb`) as c41,\r\n\t\t\t\tsum(aggr_in.`ServerDataBytes`) as c42,\r\n\t\t\t\tsum(aggr_in.`ServerDataPackets`) as c43,\r\n\t\t\t\tmax(aggr_in.`ServerNetworkTimeMax`) as c44,\r\n\t\t\t\tmin(aggr_in.`ServerNetworkTimeMin`) as c45,\r\n\t\t\t\tsum(aggr_in.`ServerNetworkTimeSum`) as c46,\r\n\t\t\t\tmax(aggr_in.`TotalNetworkTimeMax`) as c47,\r\n\t\t\t\tmin(aggr_in.`TotalNetworkTimeMin`) as c48,\r\n\t\t\t\tsum(aggr_in.`TotalNetworkTimeSum`) as c49,\r\n\t\t\t\tmax(aggr_in.`TotalResponseTimeMax`) as c50,\r\n\t\t\t\tmin(aggr_in.`TotalResponseTimeMin`) as c51,\r\n\t\t\t\tsum(aggr_in.`TotalResponseTimeSum`) as c52,\r\n\t\t\t\tmax(aggr_in.`TotalTransactionTimeMax`) as c53,\r\n\t\t\t\tmin(aggr_in.`TotalTransactionTimeMin`) as c54,\r\n\t\t\t\tsum(aggr_in.`TotalTransactionTimeSum`) as c55,\r\n\t\t\t\tsum(aggr_in.`TransactionsNb`) as c56,\r\n\t\t\t\tsum(aggr_in.`WaasDreInput`) as c57,\r\n\t\t\t\tsum(aggr_in.`WaasDreOutput`) as c58,\r\n\t\t\t\tsum(aggr_in.`WaasInputBytes`) as c59,\r\n\t\t\t\tsum(aggr_in.`WaasLzInput`) as c60,\r\n\t\t\t\tsum(aggr_in.`WaasLzOutput`) as c61,\r\n\t\t\t\tsum(aggr_in.`WaasOutputBytes`) as c62\r\n\t\tfrom (\r\n\t\t\tselect\r\n\t\t\t\t\t/* @AGGR_IN_PART@ */\r\n\t\t\t\t\ttoStartOfMinute(data_table.`Timestamp`, 'Europe/Paris') as Timestamp,\r\n\t\t\t\t\tmax(data_table.`ClientNetworkTimeMax`) as `ClientNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ClientNetworkTimeMin`) as `ClientNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ClientNetworkTimeSum`) as `ClientNetworkTimeSum`,\r\n\t\t\t\t\tsum(data_table.`ClientPackets`) as `ClientPackets`,\r\n\t\t\t\t\tsum(data_table.`ClientBytes`) as `ClientBytes`,\r\n\t\t\t\t\tsum(data_table.`ClientDataBytes`) as `ClientDataBytes`,\r\n\t\t\t\t\tsum(data_table.`ClientDataPackets`) as `ClientDataPackets`,\r\n\t\t\t\t\tsum(data_table.`ServerResponseTimeSum`) as `ServerResponseTimeSum`,\r\n\t\t\t\t\tmin(data_table.`ServerResponseTimeMin`) as `ServerResponseTimeMin`,\r\n\t\t\t\t\tmax(data_table.`ServerResponseTimeMax`) as `ServerResponseTimeMax`,\r\n\t\t\t\t\tsum(data_table.`ServerPackets`) as `ServerPackets`,\r\n\t\t\t\t\tsum(data_table.`ServerBytes`) as `ServerBytes`,\r\n\t\t\t\t\tsum(data_table.`1000msResponsesNb`) as `1000msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`100msResponsesNb`) as `100msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`10msResponsesNb`) as `10msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`2msResponsesNb`) as `2msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`500msResponsesNb`) as `500msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`50msResponsesNb`) as `50msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`5msResponsesNb`) as `5msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`LateResponsesNb`) as `LateResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`NewConnectionsNb`) as `NewConnectionsNb`,\r\n\t\t\t\t\tmax(data_table.`ResponseTimeMax`) as `ResponseTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ResponseTimeMin`) as `ResponseTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ResponseTimeSum`) as `ResponseTimeSum`,\r\n\t\t\t\t\tsum(data_table.`ResponsesNb`) as `ResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`RetransmissionsNb`) as `RetransmissionsNb`,\r\n\t\t\t\t\tsum(data_table.`ServerDataBytes`) as `ServerDataBytes`,\r\n\t\t\t\t\tsum(data_table.`ServerDataPackets`) as `ServerDataPackets`,\r\n\t\t\t\t\tmax(data_table.`ServerNetworkTimeMax`) as `ServerNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ServerNetworkTimeMin`) as `ServerNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ServerNetworkTimeSum`) as `ServerNetworkTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalNetworkTimeMax`) as `TotalNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalNetworkTimeMin`) as `TotalNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalNetworkTimeSum`) as `TotalNetworkTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalResponseTimeMax`) as `TotalResponseTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalResponseTimeMin`) as `TotalResponseTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalResponseTimeSum`) as `TotalResponseTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalTransactionTimeMax`) as `TotalTransactionTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalTransactionTimeMin`) as `TotalTransactionTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalTransactionTimeSum`) as `TotalTransactionTimeSum`,\r\n\t\t\t\t\tsum(data_table.`TransactionsNb`) as `TransactionsNb`,\r\n\t\t\t\t\tsum(data_table.`WaasDreInput`) as `WaasDreInput`,\r\n\t\t\t\t\tsum(data_table.`WaasDreOutput`) as `WaasDreOutput`,\r\n\t\t\t\t\tsum(data_table.`WaasInputBytes`) as `WaasInputBytes`,\r\n\t\t\t\t\tsum(data_table.`WaasLzInput`) as `WaasLzInput`,\r\n\t\t\t\t\tsum(data_table.`WaasLzOutput`) as `WaasLzOutput`,\r\n\t\t\t\t\tsum(data_table.`WaasOutputBytes`) as `WaasOutputBytes`,\r\n\t\t\t\t\tdata_table.`Agent` as `Agent`,\r\n\t\t\t\t\tdata_table.`Application` as `Application`,\r\n\t\t\t\t\tdata_table.`Source` as `Source`,\r\n\t\t\t\t\tdata_table.`Protocol` as `Protocol`,\r\n\t\t\t\t\tdata_table.`ClientOffice` as `ClientOffice`,\r\n\t\t\t\t\tdata_table.`ServerOffice` as `ServerOffice`,\r\n\t\t\t\t\tdata_table.`ClientInterface` as `ClientInterface`,\r\n\t\t\t\t\tdata_table.`ServerInterface` as `ServerInterface`,\r\n\t\t\t\t\tdata_table.`ClientIp` as `ClientIp`,\r\n\t\t\t\t\tdata_table.`ServerIp` as `ServerIp`,\r\n\t\t\t\t\tdata_table.`ClientCos` as `ClientCos`,\r\n\t\t\t\t\tdata_table.`ServerCos` as `ServerCos`,\r\n\t\t\t\t\tdata_table.`Port` as `Port`,\r\n\t\t\t\t\tdata_table.`DomainName` as `DomainName`,\r\n\t\t\t\t\tdata_table.`Flags` as `Flags`\r\n\t\t\tfrom\r\n\t\t\t\t\t/* @DATA_TABLE@ */\r\n\t\t\t\t\tavc.topconversationdetails as data_table\r\n\t\t\t\t\t/* @INNER_JOIN_PART@ */\r\n\t\t\t\t\t\r\n\t\t\twhere\r\n\t\t\t\t\t/* @WHERE_PART@ */\r\n\t\t\t\t\tdata_table.`Timestamp` >= toStartOfMinute(toDateTime('2020-01-01 15:54:00','Europe/Paris'), 'Europe/Paris')\r\n\t\t\t\t\tand data_table.`Timestamp` < toStartOfMinute(toDateTime('2020-01-08 15:54:00','Europe/Paris'), 'Europe/Paris')\r\n\t\t\t\t\tand ((toNullable(data_table.`Customer`)= (select a from (select `id` as a from sdm.customers where sdm.customers.`vmId` = 150005) as b)))\r\n\t\t\t\t\tand ((toNullable(data_table.`Service`)= (select a from (select `id` as a from sdm.services where sdm.services.`vmId` = 150006) as b)))\r\n\t\t\tgroup by \r\n\t\t\t\t\t/* @GROUPBY_IN_PART@ */\r\n\t\t\t\t\tTimestamp,\r\n\t\t\t\t\tdata_table.`Agent`,\r\n\t\t\t\t\tdata_table.`Protocol`,\r\n\t\t\t\t\t`ServerIp`,\r\n\t\t\t\t\tdata_table.`ServerCos`,\r\n\t\t\t\t\tdata_table.`ClientOffice`,\r\n\t\t\t\t\tdata_table.`ClientCos`,\r\n\t\t\t\t\t`Flags`,\r\n\t\t\t\t\tdata_table.`ServerInterface`,\r\n\t\t\t\t\tdata_table.`ClientInterface`,\r\n\t\t\t\t\tdata_table.`Application`,\r\n\t\t\t\t\tdata_table.`Source`,\r\n\t\t\t\t\tdata_table.`ServerOffice`,\r\n\t\t\t\t\t`Port`,\r\n\t\t\t\t\t`DomainName`,\r\n\t\t\t\t\t`ClientIp`\r\n\t\t) as aggr_in\r\n\t\t\t/* @MIDDLE_JOIN_PART@ */\r\n\t\t\t\r\n\t\tgroup by\r\n\t\t\t\t/* @GROUPBY_OUT_PART@ */\r\n\t\t\t\taggr_in.`Agent`,\r\n\t\t\t\taggr_in.`Protocol`,\r\n\t\t\t\tc10,\r\n\t\t\t\taggr_in.`ServerCos`,\r\n\t\t\t\taggr_in.`ClientOffice`,\r\n\t\t\t\taggr_in.`ClientCos`,\r\n\t\t\t\tc15,\r\n\t\t\t\taggr_in.`ServerInterface`,\r\n\t\t\t\taggr_in.`ClientInterface`,\r\n\t\t\t\taggr_in.`Application`,\r\n\t\t\t\taggr_in.`Source`,\r\n\t\t\t\taggr_in.`ServerOffice`,\r\n\t\t\t\tc13,\r\n\t\t\t\tc14,\r\n\t\t\t\tc9\r\n\t\torder by\r\n\t\t\t\t/* @ORDERBY_PART@ */\r\n\t\t\t\t( empty(c10) OR c10 = '(UNDEFINED)' OR c10 IS NULL) ASC,\r\n\t\t\t\tc15 IS NULL ASC,\r\n\t\t\t\tc13 IS NULL ASC,\r\n\t\t\t\tc14 IS NULL ASC,\r\n\t\t\t\t( empty(c9) OR c9 = '(UNDEFINED)' OR c9 IS NULL) ASC,\r\n\t\t\t\tc16 desc\r\n\t\t) as aggr_out\r\n\t) as t\r\n\t/* @OUTER_JOIN_PART@ */\r\n\t\r\ngroup by if(ranking_row<100, ranking_row, 101)\r\norder by final_ranking asc\r\n```\r\n\r\nWhere avc.topconversationdetails is a merge tree table, and the other tables are dictionnaries tables.\r\nIt is mostly a top N with others query, with 2 level of aggregation.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8592/comments",
    "author": "edonin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-09T18:27:08Z",
        "body": "723.47 MiB ridiculously unrealistic target. CH was designed to use 10GB as a base point.\r\n\r\ntry `set max_threads=1, max_read_buffer_size=100000, max_compress_block_size=100000, min_compress_block_size=100000`"
      }
    ]
  },
  {
    "number": 8228,
    "title": "mysql connection in clickhouse",
    "created_at": "2019-12-16T07:32:32Z",
    "closed_at": "2019-12-23T18:47:54Z",
    "labels": [
      "question",
      "comp-foreign-db"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8228",
    "body": "I'm using clickhouse for a while now. I have inserted 1 million records so far and I intend to add to it to about 100 billion. It's blazing fast, and I like how it compresses data. \r\n\r\nThe problem is that it keeps throwing an error every now and then, for example when I just login to clickhouse client:\r\n\r\nCannot load data for command line suggestions: Code: 1000, e.displayText() = DB::Exception: Received from localhost:9000. DB::Exception: mysqlxx::ConnectionFailed: Unknown MySQL server host 'host' (-2) ((nullptr):0). (version 19.17.5.18 (official build))\r\n\r\nFor doing ordinary tasks it seems to not affect the performance, but the main problem is that when I want to get partitions using command:\r\n\r\n`SELECT partition FROM system.parts WHERE table='bars'`\r\n\r\nagain it throws the same exception. I went through the documentation, but I couldn't find a solution.\r\n\r\nAny help would be appreciated...\r\n\r\nPS: I used: Engine = MergeTree() Partition by isin Order by time primary key time",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8228/comments",
    "author": "ashkank66",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-12-16T13:39:58Z",
        "body": "It probably means that you have table with `MySQL` engine which can't connect to MySQL.\r\nIt also strange that we have `nullptr` in error message. May be a misconfiguration. \r\n\r\nCan you please check that all you MySQL configurations are correct?\r\nAnd also find full stacktrace after this error in logs?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-16T19:17:50Z",
        "body": "> Unknown MySQL server host 'host' (-2) ((nullptr):0)\r\n\r\nProbably you have erroneously specified `host` as hostname for MySQL server, like this:\r\n`<host>host</host>`\r\n\r\nThe `(nullptr):0` part is Ok - it's what we have as the error message from the library."
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-17T05:43:54Z",
        "body": "I actually haven't configured MySQL on my clickhouse, and to be honest, I have to admit I tried to find a configuration for MySQL but I couldn't.\r\nCould you tell me where should I configure it?"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-17T07:17:18Z",
        "body": "2019.12.17 10:46:30.000314 [ 44 ] {} <Information> Application: MYSQL: Connecting to database@host:0 as user user\r\n2019.12.17 10:46:30.001630 [ 44 ] {} <Error> Application: mysqlxx::ConnectionFailed\r\n2019.12.17 10:46:30.001943 [ 44 ] {} <Error> void DB::AsynchronousMetrics::run(): Poco::Exception. Code: 1000, e.code() = 2005, e.displayText() = mysqlxx::ConnectionFailed: Unknown MySQL server host 'host' (-2) ((nullptr):0) (version 19.17.5.18 (official build)\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-17T16:52:08Z",
        "body": "It looks like you have configured MySQL table actually.\r\n\r\n```\r\ngrep -r -i mysql /etc/clickhouse-server/\r\ngrep -i mysql /etc/metrika.xml\r\ngrep -r -i mysql /var/lib/clickhouse/\r\n```"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-18T09:58:43Z",
        "body": "This is all the responses:\r\n\r\nroot@ashkanPC:/home/ashkan# grep -r -i mysql /etc/clickhouse-server/\r\n/etc/clickhouse-server/users.xml:                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).\r\nroot@ashkanPC:/home/ashkan# grep -i mysql /etc/metrika.xml\r\ngrep: /etc/metrika.xml: No such file or directory\r\nroot@ashkanPC:/home/ashkan# grep -r -i mysql /var/lib/clickhouse/\r\n/var/lib/clickhouse/preprocessed_configs/users.xml:                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:       /etc/clickhouse-server/mysql_dictionary.xml      -->\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:    <comment>This dictionary is set to connect clickhouse to mysql</comment>\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:\t  <mysql>\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:\t  </mysql>\r\n/var/lib/clickhouse/metadata/db_name.sql:ENGINE = MySQL('host:port', 'database', 'user', 'password')\r\n"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-18T10:00:29Z",
        "body": "I created a file mysql_dictionary in hope of getting rid of the error, but no proper result, so I deleted it later"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2019-12-18T10:51:56Z",
        "body": "> /var/lib/clickhouse/metadata/db_name.sql:ENGINE = MySQL('host:port', 'database', 'user', 'password')\r\n\r\nThat means that you have `MySQL` database with name `db_name`, which has incorrect configuration (instead of `'host:port', 'database', 'user', 'password'` must be real values). And this database can't connect to MySql server.\r\n\r\nYou can just run `DROP DATABASE db_name` to remove it.\r\n"
      }
    ]
  },
  {
    "number": 8122,
    "title": "OPTIMIZE FINAL makes skip index no longer work",
    "created_at": "2019-12-10T13:33:11Z",
    "closed_at": "2019-12-11T04:09:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8122",
    "body": "**Describe the bug or unexpected behaviour**\r\noptimize final makes skip index no longer work\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n19.17.4\r\n* Steps to reproduce\r\n```sql\r\nset allow_experimental_data_skipping_indices=1;\r\ncreate table test(I Int64, S String, INDEX s_index (S) TYPE bloom_filter() GRANULARITY 8192) Engine=MergeTree order by I;\r\ninsert into test select number, toString(rand()) from numbers(10000000);\r\ninsert into test values(45645645, '666');\r\nSET send_logs_level = 'trace';\r\nselect * from test where S = '666';\r\n```\r\n\r\nThis is the correct behavior before `optimize final`: 1 marks to read from 1 ranges, Read 1 rows\r\n```\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393157 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> executeQuery: (from 127.0.0.1:36838) SELECT * FROM test WHERE S = '666'\r\n\u2192 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) [bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393593 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"S = '666'\" moved to PREWHERE\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393803 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Key condition: unknown\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.401436 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.402200 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.402954 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.403693 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404496 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404563 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 0 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404598 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Selected 6 parts by date, 1 parts by key, 1 marks to read from 1 ranges\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404671 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Trace> default.test (SelectExecutor): Reading approx. 8192 rows with 1 streams\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404745 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404813 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Expression\r\n  MergeTreeThread\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.405284 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Information> executeQuery: Read 1 rows, 20.00 B in 0.012 sec., 82 rows/sec., 1.62 KiB/sec.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.405305 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> MemoryTracker: Peak memory usage (for query): 10.04 MiB.\r\n\r\n1 rows in set. Elapsed: 0.013 sec.\r\n```\r\n\r\nAfter `optimize table test final`\r\n```sql\r\nselect * from test where S = '666';\r\n```\r\n\r\nthis behavior is unexpected: 1221 marks to read from 1 ranges, Read 10000001 rows\r\n```\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389243 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> executeQuery: (from 127.0.0.1:36838) SELECT * FROM test WHERE S = '666'\r\n\u2197 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) [bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389696 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"S = '666'\" moved to PREWHERE\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389902 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Key condition: unknown\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398603 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 0 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398652 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1221 marks to read from 1 ranges\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398716 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> default.test (SelectExecutor): Reading approx. 10002432 rows with 24 streams\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398974 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.402274 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> executeQuery: Query pipeline:\r\nUnion\r\n Expression \u00d7 24\r\n  Expression\r\n   MergeTreeThread\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418648 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418689 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> UnionBlockInputStream: Waited for threads to finish\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418751 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Information> executeQuery: Read 10000001 rows, 177.89 MiB in 0.029 sec., 339693250 rows/sec., 5.90 GiB/sec.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418778 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> MemoryTracker: Peak memory usage (for query): 12.83 MiB.\r\n\r\n1 rows in set. Elapsed: 0.030 sec. Processed 10.00 million rows, 186.54 MB (329.23 million rows/s., 6.14 GB/s.)\r\n```\r\n\r\n**Expected behavior**\r\nOnly 1 mark to read, but 1221 marks to read\r\nOnly 1 Row should be read, but 10000001 rows were read",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8122/comments",
    "author": "kaijianding",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-12-10T16:53:40Z",
        "body": "First of all -- GRANULARITY 8192 is a nonsense. Your index granula will contain 8192*8192 rows.\r\nTry GRANULARITY 2.\r\n\r\nSecond. Before optimize 666 is stored in a separate part [insert into test values(45645645, '666');]. Only this Skip index's granula contains this value 666. Other parts don't. After optimize this 666 will be in a huge granula which points to 8192*8192 = 67108864 rows.\r\n"
      },
      {
        "user": "amosbird",
        "created_at": "2019-12-11T02:52:20Z",
        "body": "I always find the term \"granularity\" to be overly used. We have `index_granularity` meaning the max row number of a granule, and we have index granularity meaning the granules one index unit covers. "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-11T03:36:10Z",
        "body": "@amosbird Possible solutions:\r\n- rename GRANULARITY to GRANULARITY FACTOR and support old syntax only for ATTACH queries;\r\n- remove GRANULARITY from documentation example (so it will be 1 by default) and only briefly mention it;"
      }
    ]
  },
  {
    "number": 8121,
    "title": "\"Too many open files\" while loading data into table",
    "created_at": "2019-12-10T13:19:59Z",
    "closed_at": "2020-05-17T15:54:31Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8121",
    "body": "Am getting the below error while loading data and only half of the data is being loaded into the table \r\n\r\nDB::Exception: Cannot open file /t-3tb-data/clickhouse/data/database/table/tmp_insert_0c87b3bf0c31a7766299a14d202c8da9_648_648_0/TI_verification_status.mrk, errno: 24, strerror: Too many open files.\r\n\r\nCan someone help me quickly here.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8121/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "byx313",
        "created_at": "2019-12-10T14:25:32Z",
        "body": "> Am getting the below error while loading data and only half of the data is being loaded into the table\r\n> \r\n> DB::Exception: Cannot open file /t-3tb-data/clickhouse/data/database/table/tmp_insert_0c87b3bf0c31a7766299a14d202c8da9_648_648_0/TI_verification_status.mrk, errno: 24, strerror: Too many open files.\r\n> \r\n> Can someone help me quickly here.\r\n\r\nYou got too many files in OS.\r\nMethod 1,increase open files limit\r\ncheck open files \r\n> ulimit -a\r\n\r\nincrease open files \r\n> ulimit -n 65536\r\n\r\nMethod 2,increase messege count in one batch/one insert operation.\r\n"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T14:51:36Z",
        "body": "@byx313 \r\nI tried the first option but the problem is still same :(\r\nMethod2: You mean to say single insert will do than multiple inserts ?"
      },
      {
        "user": "byx313",
        "created_at": "2019-12-10T14:55:41Z",
        "body": "> @byx313\r\n> I tried the first option but the problem is still same :(\r\n> Method2: You mean to say single insert will do than multiple inserts ?\r\n\r\nDo 'ulimit -a' again to check whether the operation work.\r\n\r\n> Method2: You mean to say single insert will do than multiple inserts ?\r\n\r\nYes.10w message a batch a insert is better than 1w message * 10 concurrent insert"
      },
      {
        "user": "byx313",
        "created_at": "2019-12-10T15:00:03Z",
        "body": "> @byx313\r\n> yes, I did ulimit -a to check and yes the change is reflected.\r\n\r\nmay be you should try to change you insert frequency.What's the frequency now?"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T15:06:40Z",
        "body": "@byx313\r\nam loading one file after the other , once the first file is loaded starting with the next one."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-10T20:09:16Z",
        "body": "This happens because you are using too granular partition key in a table.\r\nSolution: do not use `PARTITION BY`."
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-11T09:40:06Z",
        "body": "@alexey-milovidov  I have partitioned the table on state code which has some 60 values \r\n\r\nSo if I don't use the PARTITION BY  it doesn't have impact on queries??"
      },
      {
        "user": "filimonov",
        "created_at": "2019-12-12T00:23:19Z",
        "body": "> @byx313\r\n> yes, I did ulimit -a to check and yes the change is reflected.\r\n\r\nAlso for clickhouse user? What is your OS? How did you install/run clickhouse?\r\n\r\nI'm asking because official packages should extend that limit during installation, and 9fficial docker readme mentions how to increase max number of opened files for clickhouse. "
      }
    ]
  },
  {
    "number": 7926,
    "title": "readonly setting - help",
    "created_at": "2019-11-26T10:57:56Z",
    "closed_at": "2019-11-27T13:28:45Z",
    "labels": [
      "question",
      "operations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7926",
    "body": "in users.xml, \r\n1) I created a new profile with readonly as \r\n```xml\r\n<read>\r\n    <readonly>\r\n            <readonly>1</readonly>\r\n    </readonly>\r\n</read>\r\n```\r\n2) created a new user assigning readonly profile\r\n```xml\r\n<dbread>\r\n    <password>password</password>\r\n    <profile>read</profile>\r\n    <quota>default</quota>\r\n    <networks incl=\"networks\" replace=\"replace\">\r\n       <ip>::/0</ip> \r\n    </networks>\r\n    <readonly>\r\n            <readonly>1</readonly>\r\n    </readonly>\r\n</dbread>\r\n```\r\nlogged in as same user(dbread/password), but I can create and drop table. I am not sure whether I have missed anything.\r\n\r\nalso help me do set `allow_ddl=0`, so that user can not generate DDLs.\r\n\r\nThanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7926/comments",
    "author": "viputh6",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-26T14:38:41Z",
        "body": "```\r\n<profiles>\r\n        <read>\r\n            <readonly>1</readonly>\r\n        </read>\r\n....\r\n</profiles>\r\n\r\n<users>\r\n   <dbread>\r\n                 <profile>read</profile>\r\n            ....\r\n    </dbread>\r\n...\r\n<users>\r\n\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-26T16:16:51Z",
        "body": "```\r\n<profiles>\r\n        <read>\r\n            <readonly>1</readonly>\r\n          <allow_ddl>0</allow_ddl>\r\n        </read>\r\n....\r\n</profiles>\r\n\r\n```"
      }
    ]
  },
  {
    "number": 7888,
    "title": "some users have query_log and some don't",
    "created_at": "2019-11-22T07:32:32Z",
    "closed_at": "2019-11-25T13:06:34Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7888",
    "body": "I can get query log from `system.query_log` by users who execute query from `tabix` .\r\nbut users who execute query by `official jdbc`, I can't find their query_log.\r\n\r\nIs there anything wrong?\r\nI am so confused.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7888/comments",
    "author": "Tasselmi",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2019-11-22T13:20:45Z",
        "body": "There is setting `log_queries`, which enables query logging (disabled by default) and it may have different values for different users and profiles."
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-11-23T02:26:58Z",
        "body": "> There is setting `log_queries`, which enables query logging (disabled by default) and it may have different values for different users and profiles.\r\n\r\nI've setted `log_queries`  in `config.xml`."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-23T16:16:43Z",
        "body": "@Tasselmi it's a user or query level setting, so it must be set in users.xml for a user profile."
      }
    ]
  },
  {
    "number": 7765,
    "title": "Drop in writes on high number of selects.",
    "created_at": "2019-11-13T23:16:06Z",
    "closed_at": "2019-11-14T06:04:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7765",
    "body": "Version: ClickHouse 19.13.1.11\r\nServer has 400GB RAM with 48 CPU cores. and 3.2 TB of HDD.\r\nInsert batch size is 20K records (~40MB). Each insert is taking ~1.5secs.\r\nServer has 12.5B records.\r\nWe are running parallel select queries of ~500. Selects include some count queries, some aggregations and group bys.\r\nCPU and RAM are at their minimals (like 10% of CPU and 10GB of RAM used).\r\nDuring this time, inserts are dropped. Noticed \"TimeoutExceptions\" on client side (Using Http Client for inserts)\r\nGet below exception at high rate. (very few select queries went through)\r\n\r\nCode: 202, e.displayText() = DB::Exception: Too many simultaneous queries. Maximum: 100 (version 19.13.1.11 (official build))\r\n\r\nTried to increase max simultaneous queries in config.xml (to 1000).\r\n <max_concurrent_queries>1000</max_concurrent_queries>\r\n\r\nEven after increasing, exception still says Maximun: 100\r\nCode: 202, e.displayText() = DB::Exception: Too many simultaneous queries. Maximum: 100 (version 19.13.1.11 (official build))\r\n\r\nSeems my change is not taking effect.\r\n\r\nHow can I make sure that writes are not impacted due to reads?\r\nWhy cant I increase simultaneous queries in spite of having RAM and MEMORY available?\r\nHow can we support more selects? (we need little higher selects to build dashboards, aggregations, and for AD purposes)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7765/comments",
    "author": "SreekanthMannari",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-13T23:22:46Z",
        "body": "Did you restart CH after setting max_concurrent_queries ?"
      },
      {
        "user": "SreekanthMannari",
        "created_at": "2019-11-13T23:27:03Z",
        "body": "No. I haven't restarted.\r\nBut config.xml in \"preprocessed_configs\" folder shows my change."
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-13T23:28:59Z",
        "body": ">But config.xml in \"preprocessed_configs\" folder shows my change.\r\n\r\nIt does not matter. All parameters from config.xml (except cluster & dictionaries configurations) require CH reboot to apply. "
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-14T01:27:11Z",
        "body": ">Does reboot needed even for User.xml changes like max memory settings?\r\n\r\nNo, changes in user.xml does not need reboot."
      },
      {
        "user": "haiertashu",
        "created_at": "2021-06-03T06:25:08Z",
        "body": "how to make sure the server settings modified such as 'max_concurrent_queries' taking effect \uff1f where/which system table can show the latest changes\uff1f or any other way"
      }
    ]
  },
  {
    "number": 6063,
    "title": "How to drop database based on MySQL Engine",
    "created_at": "2019-07-19T06:18:38Z",
    "closed_at": "2020-08-05T03:27:42Z",
    "labels": [
      "question",
      "unfinished code"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6063",
    "body": "This problem occurs when I delete the database based on the MySQL storage engine.\r\n\r\nThis is My Create database stament:\r\n`SHOW CREATE DATABASE mydb;\r\nCREATE DATABASE mydb ENGINE = MySQL('localhost:3306', 'docker', 'docker', 'docker')`\r\n\r\nThis is Exception when I try to drop database:\r\n`Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: MySQL database engine does not support remove table..`\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6063/comments",
    "author": "rangez",
    "comments": [
      {
        "user": "zhang2014",
        "created_at": "2019-07-22T02:51:18Z",
        "body": "I'll try to fix it. you can currently drop the database using the following command:\r\n```\r\nclickhouse :) DETACH DATABASE {need drop database name}\r\nclickhouse :) exit\r\n~ cd {clickhouse data path}\r\n~ rm -rf metadata/{need drop database name}\r\n```\r\n\r\n"
      },
      {
        "user": "tonal",
        "created_at": "2019-09-10T04:53:24Z",
        "body": "Also mysql db don`t drop if not connect to it:\r\n```\r\nhost2 :) CREATE DATABASE mysql_db ENGINE = MySQL('yandex.ru:3306', 'test_db', 'yandex_admin', '1234');\r\n\r\nCREATE DATABASE mysql_db\r\nENGINE = MySQL('yandex.ru:3306', 'test_db', 'yandex_admin', '1234')\r\n\r\nOk\r\n0 rows in set. Elapsed: 0.064 sec. \r\n\r\nhost2 :) show databases;\r\n\r\nSHOW DATABASES\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 default  \u2502\r\n\u2502 mysql_db \u2502\r\n\u2502 system   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n4 rows in set. Elapsed: 0.001 sec. \r\n\r\nhost2 :) drop database mysql_db;\r\n\r\nDROP DATABASE mysql_db\r\n\r\nReceived exception from server (version 19.13.3):\r\nCode: 48. DB::Exception: Received from localhost:9000. DB::Exception: MySQL database engine does not support remove table.. \r\n\r\n0 rows in set. Elapsed: 0.313 sec. \r\n```"
      },
      {
        "user": "zhang2014",
        "created_at": "2019-09-10T12:36:41Z",
        "body": "This is a feature that hasn't been implemented for some reason, and I will implement them as soon as possible : )"
      },
      {
        "user": "jigetage",
        "created_at": "2019-10-17T01:46:49Z",
        "body": "detach database test-db"
      },
      {
        "user": "gempir",
        "created_at": "2024-10-01T15:03:38Z",
        "body": "`DETACH DATABASE my_db` just loads forever for me, same with `DROP`\r\n\r\n```\r\nTimeout exceeded while receiving data from server. Waited for 300 seconds, timeout is 300 seconds.\r\nCancelling query.\r\n```\r\n\r\nThe database in question was firewalled and I do not get a response, I think ClickHouse is trying to wait for a response from that server. \r\n\r\nIs there a way to remove the db without having to remove some magic files?"
      }
    ]
  }
]