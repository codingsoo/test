[
  {
    "number": 37478,
    "title": "Fine-tuning of happy eyeballs dns resolution",
    "created_at": "2024-12-03T09:22:31Z",
    "closed_at": "2024-12-18T14:33:53Z",
    "labels": [
      "question",
      "area/load balancing",
      "area/dns"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37478",
    "body": "*Title*: *Fine-tuning of happy eyeballs dns resolution*\r\n\r\n*Description*:\r\n\r\nWith that configuration and clusterType \"LOGICAL_DNS\"\r\n\r\n```\r\n        caresDnsResolverConfig, err := anypb.New(&cares.CaresDnsResolverConfig{\r\n\t\tDnsResolverOptions: &configCoreV3.DnsResolverOptions{\r\n\t\t\tUseTcpForDnsLookups: true,\r\n\t\t},\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Println(\"error converting to Any: %v\\n\", err)\r\n\t}\r\n\tdnsResolverConfig := &configCoreV3.TypedExtensionConfig{\r\n\t\tName:        \"envoy.network.dns_resolver.cares\",\r\n\t\tTypedConfig: caresDnsResolverConfig,\r\n\t}\r\n\r\n\treturn &clusterV3.Cluster{\r\n\t\tName:                   clusterName,\r\n\t\tConnectTimeout:         durationpb.New(time.Duration(connectTimeout) * time.Second),\r\n\t\tClusterDiscoveryType:   &clusterV3.Cluster_Type{Type: clusterType},\r\n\t\tLoadAssignment:         BuildEndpoint(clusterName, host, port),\r\n\t\tDnsLookupFamily:        clusterV3.Cluster_ALL,\r\n\t\tCircuitBreakers:        circuitBreakers,\r\n\t\tTypedDnsResolverConfig: dnsResolverConfig,\r\n\t}\r\n}\r\n```\r\n\r\nWe are having issues to access a system that has both valid IPv6 and IPv4 addresses like www.google.com. We are trying to access it from a proxy that relies on Envoy and when source IP is V4 we are getting \r\n`upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Network is unreachable|remote address:[2404:6800:4004:818::2004]:443`\r\nand it it did not pick the IPv4 address. We have tried adding \"UseTcpForDnsLookups\" with no success. Could you help on that, how can we fine-tune happy eyeballs to pick IPv4 when source is IPv4 and to pick IPv6 when source is IPv6 in order to work for such dual-stack target systems? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37478/comments",
    "author": "VladislavAtanasov95",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2024-12-05T02:43:01Z",
        "body": "Hm. I'm surprised. I would have expected that with `DnsLookupFamily` set to `ALL` that DNS would have returned both IPv4 and IPv6 addresses. Then the Happy Eyeballs code would make connection attempts for each address in the list until the a connection succeeded. Is that not what you're seeing? Do you have a log? "
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-05T10:58:25Z",
        "body": "We sometimes receive\r\n\r\n< HTTP/1.1 503 Service Unavailable\r\n< content-length: 231\r\n< content-type: text/plain\r\n< date: Thu, 21 Nov 2024 13:43:13 GMT\r\n< upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Network is unreachable|remote address:[2404:6800:4004:818::2004]:443\r\n\r\nWhen the client service only has IPv4 and we are trying to reach google.com through our envoy-based proxy, sometimes it picks ipv6, sometime ipv4 of the target url."
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-05T13:16:53Z",
        "body": "The Cluster type is actually \"STRICT_DNS\", not \"LOGICAL_DNS\"(my mistake) when we sporadically receive the error. I ran an example with static envoy locally \r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 2000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager \r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: http\r\n          route_config:\r\n            name: search_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/abv\"\r\n                route:\r\n                  cluster: abv\r\n                  host_rewrite_literal: www.abv.bg\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: google\r\n                  host_rewrite_literal: www.google.com\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 10s\r\n    type: strict_dns\r\n    dns_lookup_family: ALL\r\n    load_assignment:\r\n      cluster_name: google\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.google.com\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.google.com\r\n  - name: abv\r\n    connect_timeout: 10s\r\n    type: logical_dns\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: google\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.abv.bg\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.abv.bg\r\nadmin:\r\n  access_log_path: \"/dev/stdout\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 15000\r\n```\r\n\r\nWhen I change it from STRICT_DNS to LOGICAL_DNS, requests to google are working stable every time, but when it is STRICT_DNS sometimes I receive \r\n`upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: delayed connect error: 61`\r\n Does that make sense?\r\n"
      },
      {
        "user": "akhilsingh-git",
        "created_at": "2024-12-18T14:08:19Z",
        "body": "When Envoy is configured with dns_lookup_family: ALL and receives both IPv4 and IPv6 addresses for a host, it uses a \u201chappy eyeballs\u201d approach to try establishing a connection. This approach attempts to connect to IPv6 first, and if that doesn\u2019t succeed within a certain timeframe, it will try IPv4. However, depending on network conditions, default settings might lead to occasional failures if IPv6 is not truly reachable or if there\u2019s an immediate IPv6 route issue.\r\n\r\nKey Points to Consider:\r\n\t1.\tHappy Eyeballs Mechanism:\r\nBy default, Envoy implements a Happy Eyeballs algorithm (RFC 6555-like behavior) when dns_lookup_family: ALL is set. Envoy will attempt a connection to an IPv6 address first and then, after a delay (happy_eyeballs_connection_delay), attempt IPv4 if IPv6 hasn\u2019t connected. If IPv6 is immediately unreachable, you might see intermittent failures before Envoy attempts IPv4.\r\n\t2.\tStrict DNS vs. Logical DNS:\r\n\t\u2022\tSTRICT_DNS: Envoy continuously re-resolves DNS at runtime and uses all returned IPs (both v4 and v6) as load-balancing endpoints. This can lead to Envoy attempting IPv6 endpoints that aren\u2019t actually reachable, causing occasional errors.\r\n\t\u2022\tLOGICAL_DNS: Envoy resolves DNS once at startup and treats the resulting IP addresses as a logical group, typically sticking to a single address family more consistently. This often appears more stable for dual-stack hosts because Envoy is less aggressive in rotating through all endpoints, but it may not provide the same dynamic behavior as STRICT_DNS.\r\n\t3.\tFine-Tuning Happy Eyeballs Behavior:\r\nEnvoy\u2019s cluster configuration allows you to adjust the Happy Eyeballs timing. By default, the IPv4 connection attempt is delayed by a certain amount of time if IPv6 is available. If IPv6 repeatedly fails immediately, reducing this delay can help Envoy fall back to IPv4 faster, preventing those intermittent \u201cnetwork unreachable\u201d errors.\r\nIn the cluster configuration for your STRICT_DNS cluster, try setting:\r\n\r\nhappy_eyeballs_connection_delay: 50ms\r\n\r\nor another suitably small value. The default is 300ms, so lowering it can speed up IPv4 fallback.\r\nExample:\r\n\r\nclusters:\r\n- name: google\r\n  connect_timeout: 10s\r\n  type: STRICT_DNS\r\n  dns_lookup_family: ALL\r\n  happy_eyeballs_connection_delay: 50ms\r\n  load_assignment:\r\n    cluster_name: google\r\n    endpoints:\r\n      - lb_endpoints:\r\n          - endpoint:\r\n              address:\r\n                socket_address:\r\n                  address: www.google.com\r\n                  port_value: 443\r\n  transport_socket:\r\n    name: envoy.transport_sockets.tls\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n      sni: www.google.com\r\n\r\nWith this change, if IPv6 is unreachable, Envoy should fall back to IPv4 more quickly, reducing the frequency of those \u201cnetwork unreachable\u201d errors.\r\n\r\n\t4.\tIf You Don\u2019t Need IPv6:\r\nIf IPv6 connectivity is not required or not guaranteed from your environment, consider simplifying by using:\r\n\r\ndns_lookup_family: V4_ONLY\r\n\r\nThis will ensure Envoy only attempts IPv4 addresses, eliminating IPv6-related connect errors altogether.\r\n\r\n\t5.\tEnvironment Considerations:\r\nIf your source network is IPv4-only, and the destination returns both IPv6 and IPv4 addresses, Envoy will try IPv6 first (due to Happy Eyeballs). Ensuring proper IPv6 routing, or just disabling it if not needed, is often the simplest solution. If dual-stack support is truly required, fine-tuning happy_eyeballs_connection_delay is your best bet.\r\n\r\nSummary:\r\n\t\u2022\tUse dns_lookup_family: ALL for dual-stack, but tune happy_eyeballs_connection_delay to shorten the fallback time to IPv4.\r\n\t\u2022\tConsider switching from STRICT_DNS to LOGICAL_DNS if that yields more stable behavior in your environment.\r\n\t\u2022\tIf IPv6 isn\u2019t actually needed, use V4_ONLY to avoid complexity.\r\n\t\u2022\tAdjusting the cluster configuration, particularly happy_eyeballs_connection_delay, should help Envoy more reliably fall back to IPv4 when IPv6 fails, reducing the intermittent \u201cnetwork unreachable\u201d errors."
      }
    ]
  },
  {
    "number": 23487,
    "title": "Question about release: 1.22.3 2aca584: envoy/VERSION.txt => 1.22.3-dev",
    "created_at": "2022-10-13T19:53:54Z",
    "closed_at": "2022-10-14T12:22:56Z",
    "labels": [
      "question",
      "area/release"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23487",
    "body": "We use git submodule like this: git submodule status\r\n2aca584b3bca81622d3b009612f0c7be93eeea34 envoy (v1.22.3)\r\n\r\nBut envoy/VERSION.txt contains:\r\n1.22.3-dev\r\n\r\nThus, the RELEASE full string looks like this:\r\n\"version\": \"b87295fae172bc56584b65fdb05a05c31acd2ffa/1.22.3-dev/Clean/RELEASE/BoringSSL\",\r\n\r\n\"xxx-dev\" can be an issue for us if running in the PRODUCTION even though it's official release.  Anyway to overwrite this version txt?\r\n\r\nWe do have c++ filters internal to us, requests us to a full envoy build.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23487/comments",
    "author": "newwuhan5",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-10-14T08:49:42Z",
        "body": "this was a mistake - release 1.22.5 should have the correct versioning"
      }
    ]
  },
  {
    "number": 20607,
    "title": "generate compile_commands.json fail",
    "created_at": "2022-03-31T06:02:12Z",
    "closed_at": "2022-03-31T11:29:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20607",
    "body": "[Wed Mar 30 22:59:31][#120# ] (main)$./tools/gen_compilation_database.py \r\nERROR: /mnt/cache/_bazel_stack/5a617312af51e0dbefb3398d3212136c/external/base_pip3_sphinx/BUILD.bazel:22:11: no such package '@base_pip3_importlib_metadata//': The repository '@base_pip3_importlib_metadata' could not be resolved: Repository '@base_pip3_importlib_metadata' is not defined and referenced by '@base_pip3_sphinx//:pkg'\r\nERROR: Analysis of target '//tools/docs:sphinx_runner' failed; build aborted: \r\nINFO: Elapsed time: 1.008s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (3 packages loaded, 0 targets configured)\r\n    Fetching @emscripten_bin_linux; fetching\r\n    Fetching @base_pip3_pyparsing; fetching\r\n    Fetching @base_pip3_yarl; fetching\r\n    Fetching @base_pip3_python_gnupg; fetching\r\n    Fetching @base_pip3_multidict; fetching\r\n    Fetching @rust_linux_x86_64; fetching\r\n    Fetching @base_pip3_idna; fetching\r\n    Fetching @base_pip3_cffi; fetching\r\nTraceback (most recent call last):\r\n  File \"./tools/gen_compilation_database.py\", line 124, in <module>\r\n    fix_compilation_database(args, generate_compilation_database(args))\r\n  File \"./tools/gen_compilation_database.py\", line 20, in generate_compilation_database\r\n    subprocess.check_call([\"bazel\", \"build\"] + bazel_options + [\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['bazel', 'build', '--config=compdb', '--remote_download_outputs=all', '--aspects=@bazel_compdb//:aspects.bzl%compilation_database_aspect', '--output_groups=compdb_files,header_files', '//source/...', '//test/...', '//tools/...', '//contrib/...']' returned non-zero exit status 1.\r\n\r\n\r\n[Wed Mar 30 22:59:46][#121# ] (main)$pip3 install importlib_metadata\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.8/dist-packages (4.10.1)\r\nRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib_metadata) (1.0.0)\r\n\r\nBut I have already installed the pkg.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20607/comments",
    "author": "zhangbo1882",
    "comments": [
      {
        "user": "zhxie",
        "created_at": "2022-03-31T06:10:10Z",
        "body": "You may require Python 3.10 to refresh compilation database."
      },
      {
        "user": "phlax",
        "created_at": "2022-03-31T09:10:16Z",
        "body": "> You may require Python 3.10 to refresh compilation database.\r\n\r\nyes, this is the issue - python3.10 _doesnt_ require `importlib-metadata` (and its deps) so it is not listed/pinned in the requirements file"
      }
    ]
  },
  {
    "number": 10951,
    "title": "examples/ uses new-style filter names in images that don't support them",
    "created_at": "2020-04-25T22:50:59Z",
    "closed_at": "2020-04-30T19:42:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10951",
    "body": "```\r\n$ cd examples/front-end\r\n\r\n$ docker-compose pull\r\n\r\n$ docker-compose up --build\r\n...\r\nfront-envoy_1  | [2020-04-25 22:46:34.317][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nfront-envoy_1  | [2020-04-25 22:46:34.318][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10951/comments",
    "author": "chadm-sq",
    "comments": [
      {
        "user": "chadm-sq",
        "created_at": "2020-04-26T00:03:12Z",
        "body": "Using `find examples -type f -name docker-compose.yaml -exec echo \\; -exec echo \\; -exec echo found {} and starting it \\; -execdir docker-compose up --quiet-pull --build --abort-on-container-exit \\;`\r\nand `docker kill $(docker ps -q )`\r\n\r\n# examples/front-envoy\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:46:35.190][7][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nfront-envoy_1  | [2020-04-25 23:46:35.191][7][info][main] [source/server/server.cc:602] exiting\r\n```\r\n\r\n# examples/redis\r\n\r\nok!\r\n\r\n# examples/jaeger-tracing\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:49:58.945][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:49:58.945][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\nSee #10952\r\n\r\n# examples/lua\r\n\r\n```\r\nproxy_1        | [2020-04-25 23:50:10.830][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nproxy_1        | Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\n```\r\n\r\n# examples/load-reporting-service\r\n\r\nok!\r\n\r\n# examples/zipkin-tracing\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:52:23.383][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:52:23.383][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\n\r\nSee #10952 \r\n\r\n# examples/mysql\r\n\r\nok!\r\n\r\n# examples/cors/frontend\r\n\r\n```\r\nfront-envoy_1       | [2020-04-25 23:54:08.954][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\nfront-envoy_1       | [2020-04-25 23:54:08.955][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1       | Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\n```\r\n\r\n# examples/cors/backend\r\n\r\n```\r\nfront-envoy_1      | [2020-04-25 23:54:20.504][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\nfront-envoy_1      | [2020-04-25 23:54:20.505][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1      | Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\n```\r\n\r\n# examples/jaeger-native-tracing\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:54:35.120][10][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:54:35.120][10][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\n\r\nSee #10952 \r\n\r\n# examples/grpc-bridge\r\n\r\n```\r\nbuild github.com/envoyproxy/envoy: cannot load github.com/envoyproxy/envoy/examples/grpc-bridge/server/kv: module github.com/envoyproxy/envoy/examples/grpc-bridge/server@latest (v0.0.0-20200425220349-0b0213fdc38e) found, but does not contain package github.com/envoyproxy/envoy/examples/grpc-bridge/server/kv\r\n```\r\n\r\n# examples/fault-injection\r\n\r\n```\r\nenvoy_1    | [2020-04-25 23:54:54.852][1][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nenvoy_1    | [2020-04-25 23:54:54.852][1][info][main] [source/server/server.cc:602] exiting\r\nenvoy_1    | Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\n```"
      },
      {
        "user": "mk46",
        "created_at": "2020-04-27T07:33:02Z",
        "body": "Hi @cmiller-sq, It seems that docker images are outdated. Are you trying to run docker-compose with master branch?"
      },
      {
        "user": "mk46",
        "created_at": "2020-04-29T20:14:59Z",
        "body": "@cmiller-sq, In my opinion, it's not a bug because of older docker image present in localhost stop pulling the updated image from docker hub. you need to remove it and then proceed further."
      }
    ]
  },
  {
    "number": 9904,
    "title": "help(build): ./ci/do_ci.sh: Permission denied",
    "created_at": "2020-02-01T13:34:44Z",
    "closed_at": "2020-02-05T05:28:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9904",
    "body": "when I try to build a dev version, I got this error:\r\n\r\n```shell\r\n[root@instance-1 envoy-1.13.0]# pwd\r\n/root/envoy-1.13.0\r\n[root@instance-1 envoy-1.13.0]# ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev'\r\nbash: ./ci/do_ci.sh: Permission denied\r\n[root@instance-1 envoy-1.13.0]# ll ./ci/do_ci.sh\r\n-rwxrwxrwx. 1 root root 14138 Jan 20 22:06 ./ci/do_ci.sh\r\n\r\n[root@instance-1 envoy-1.13.0]# uname -r\r\n3.10.0-1062.9.1.el7.x86_64\r\n[root@instance-1 envoy-1.13.0]# cat /etc/redhat-release\r\nCentOS Linux release 7.7.1908 (Core)\r\n\r\n[root@instance-1 envoy-1.13.0]# docker version\r\nClient:\r\n Version:         1.13.1\r\n API version:     1.26\r\n Package version: docker-1.13.1-108.git4ef4b30.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      4ef4b30/1.13.1\r\n Built:           Tue Jan 21 17:16:25 2020\r\n OS/Arch:         linux/amd64\r\n\r\nServer:\r\n Version:         1.13.1\r\n API version:     1.26 (minimum version 1.12)\r\n Package version: docker-1.13.1-108.git4ef4b30.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      4ef4b30/1.13.1\r\n Built:           Tue Jan 21 17:16:25 2020\r\n OS/Arch:         linux/amd64\r\n Experimental:    false\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9904/comments",
    "author": "membphis",
    "comments": [
      {
        "user": "kylebevans",
        "created_at": "2020-02-01T17:46:47Z",
        "body": "Try this and then build again:\r\n\r\n`mount -o remount,exec /tmp`\r\n\r\nThe run_envoy_docker.sh creates a build dir in /tmp and I bet CentOS mounts tmp with the noexec flag to disallow executing from /tmp for security reasons."
      }
    ]
  },
  {
    "number": 9084,
    "title": "Simple Ratelimit doesnt work with envoy frontproxy",
    "created_at": "2019-11-20T15:46:42Z",
    "closed_at": "2019-11-20T16:19:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9084",
    "body": "Simple Ratelimit doesnt work with envoy frontproxy\r\n\r\nenvoy front proxy config:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          use_remote_address: true\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n            - name: envoy.file_access_log\r\n              config:\r\n                path: /dev/stdout\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  # Send request to an endpoint in the Google cluster\r\n                  cluster: google\r\n                  host_rewrite: www.google.com\r\n                rate_limits:\r\n                  - actions:\r\n                      - remote_address: {}\r\n          http_filters:\r\n          - name: envoy.rate_limit\r\n            config:\r\n              domain: rate_per_ip\r\n              rate_limit_service:\r\n                grpc_service:\r\n                  envoy_grpc:\r\n                    cluster_name: rate_limit_cluster\r\n                  timeout: 0.25s\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 1s\r\n    type: logical_dns \r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n    hosts: [{ socket_address: { address: google.com, port_value: 443 }}]\r\n    tls_context: { sni: www.google.com }\r\n\r\n  - name: rate_limit_cluster\r\n    type: STATIC \r\n    connect_timeout: 1s\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts: [{ socket_address: { address: 127.0.0.1 , port_value: 8081 }}]\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n\r\n\r\n\r\nratelimit config:\r\n```\r\ndomain: rate_per_ip\r\ndescriptors:\r\n  - key: database\r\n    value: users\r\n    rate_limit:\r\n      unit: minute\r\n      requests_per_unit: 5\r\n  - key: remote_address\r\n    rate_limit:\r\n      unit: minute\r\n      requests_per_unit: 5\r\n```\r\n\r\nI can see both ratelimit and envoy working, and can access them, but envoy doesnt hit ratelimit service at all. Please help me.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9084/comments",
    "author": "nagireddygatla",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2019-11-20T16:10:18Z",
        "body": "in the configuration above the rate_limits field is not indented correctly. The field should be inside of the route field."
      }
    ]
  },
  {
    "number": 8626,
    "title": "gRPC load balancing with Envoy Proxy",
    "created_at": "2019-10-16T11:55:21Z",
    "closed_at": "2019-10-16T18:01:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8626",
    "body": "I am trying to load balance the traffic of two instances of a microservice \"A\" using envoy. \"A\" exposes gRPC services which is being called by another microservice \"B\". I am running a single instance of Envoy with the following configuration.\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 127.0.0.1, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 127.0.0.1, port_value: 10000 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          codec_type: AUTO\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                route: { cluster: some_service }\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - name: some_service\r\n    connect_timeout: 0.25s\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: some_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 8081\r\n\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 8082\r\n```\r\n\r\n**However, when the client service makes a gRPC call, the following exception is being thrown.**\r\n\r\nException in thread \"pool-1-thread-5\" io.grpc.StatusRuntimeException: UNAVAILABLE: upstream connect error or disconnect/reset before headers. reset reason: connection termination\r\n\tat io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:235)\r\n\tat io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:216)\r\n\tat io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:141)\r\n\tat com.sample.grpc.HelloServiceGrpc$HelloServiceBlockingStub.hello(HelloServiceGrpc.java:150)\r\n\tat com.example.demo1.MicroserviceAApplication.lambda$runGrpcClient$0(MicroserviceAApplication.java:38)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n \r\nIs this the right way to call a gRPC service keeping Envoy in the middle? What changes do I need to make ? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8626/comments",
    "author": "iaintamit",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-10-16T16:36:03Z",
        "body": "By default the Cluster's `protocol_selection` field will be `USE_CONFIGURED_PROTOCOL` and the default upstream protocol is http1.1. I think what's happening is that Envoy is converting the inbound grpc (http/2) request to http/1.1 and the upstream is rejecting the connection.\r\n\r\nYou can add `http_protocol_options: {}` to your cluster definition to indicate that it is http/2 only and that should fix the immediate problem. You could also specify `protocol_selection: USE_DOWNSTREAM_PROTOCOL` but that only makes sense if the upstreams support both http 1.1 and 2. \r\n\r\nIn addition, if you don't plan on using http/1.1 on the downstream side, you might also consider setting that `codec_type: HTTP2` to avoid forwarding converted http/1.1 requests."
      },
      {
        "user": "zuercher",
        "created_at": "2019-10-16T21:35:35Z",
        "body": "Just noticed that my comment had a typo. I'll post here in case anyone else comes across this. It should have said to add `http2_protocol_options: {}`."
      }
    ]
  },
  {
    "number": 7309,
    "title": "Compilation stalls when building Envoy in a container",
    "created_at": "2019-06-18T00:59:00Z",
    "closed_at": "2019-06-19T14:37:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7309",
    "body": "*Title*: Build progress stalls in the *envoyproxy/envoy-build-ubuntu* container.\r\n\r\n*Description*:\r\n\r\nI checked out the latest master and followed the instructions in `ci` directory - `./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev` - to build envoy from source inside the ubuntu container. My host OS is a macOS 10.14.5 with docker desktop installed.\r\n\r\nAfter triggering the above command the container is successfully launched but the compilation stalled at:\r\n\r\n```\r\n[2,182 / 2,334] 151 actions, 2 running\r\n    Compiling source/extensions/filters/http/jwt_authn/extractor.cc; 22s local\r\n    Compiling source/common/router/header_formatter.cc; 17s local\r\n    [Analy] Compiling external/io_opencensus_cpp/opencensus/exporters/trace/stackdriver/internal/stackdriver_exporter.cc; 1828s\r\n    [Analy] Compiling source/common/http/context_impl.cc; 1778s\r\n    [Analy] Compiling source/extensions/tracers/common/ot/opentracing_driver_impl.cc; 1778s\r\n    [Analy] Compiling source/common/router/header_parser.cc; 1778s\r\n    [Analy] Compiling source/extensions/filters/http/buffer/buffer_filter.cc; 1778s\r\n    [Analy] Compiling source/extensions/filters/http/lua/wrappers.cc; 1778s ...\r\n``` \r\n\r\nIf I send Ctrl-C on the terminal, it seems Bazel complained as:\r\n\r\n```\r\n^C\r\nSession terminated, terminating shell...\r\nBazel caught interrupt signal; shutting down.\r\n\r\n\r\nCould not interrupt server (Deadline Exceeded)\r\n```\r\nAnd if I use `docker stop [envoy-build-container-id]`, it doesn't work as well.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7309/comments",
    "author": "InfoHunter",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-06-18T15:06:33Z",
        "body": "I suspect your docker VM doesn't have enough memory, causing compilation to grind to a halt. Maybe try increasing the resources given to the VM?"
      }
    ]
  },
  {
    "number": 6930,
    "title": "Envoy doesn't execute automatic retries using 5xx Envoy retry policy ",
    "created_at": "2019-05-14T10:40:44Z",
    "closed_at": "2019-06-20T20:45:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6930",
    "body": "**Envoy doesn't execute automatic retries using 5xx Envoy retry policy**\r\n\r\n*Description*:\r\n\r\nI'm interested in some envoy behavior. I implemented email sending service that is used as cluster in envoy configuration.  I want to retry requests using envoy, if each of 500, 502, 503, 504 status codes return back from service to envoy. \r\n\r\n**Bug Template**\r\n\r\n*Description*:\r\n\r\nIn the first case I'm sending several request in parallel with binary file to service through envoy and get 500 status code in response from service or 503 service code, if the service is shut down. I except that envoy automatically retry requests, but retries are not occur.\r\n\r\nIn the second case I'am implement POST request with empty data and in this case I get 500 status code and retries are successfully happen.\r\n\r\n*Repro steps*:\r\n> There are 20 parallel requests in the first case with data binary and in the second case with empty data\r\n\r\n```\r\nfile=$1\r\ncurl --data-binary @${file}.post  -L --post301 --connect-timeout 300 $envoy-url \r\n```\r\n*Admin and Stats Output*:\r\n\r\n/stats/prometheus | grep email-common\r\n>The first case: sending data binary file\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 4183272\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 8380\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 40\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 0\r\n\r\n```\r\n>The second case: sending request with empty data\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 26950\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 30\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 23045\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 13\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_retry_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 55\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 31\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 3771\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 9\r\n```\r\n*Config*:\r\nEnvoy version is v1.9.0, pulled from docker hub\r\n\r\n```\r\nadmin:\r\n  access_log_path: /app/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 8081 }\r\nstatic_resources:\r\n  listeners:\r\n  - name: https_listener\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n    filter_chains:\r\n      - filters:\r\n        - name: envoy.http_connection_manager\r\n          config:\r\n            codec_type: AUTO\r\n            stat_prefix: ingress\r\n            route_config:\r\n              name: router\r\n              virtual_hosts:\r\n              - name: common\r\n                domains: [\"*\"]\r\n                routes:\r\n                - match: { prefix: \"/\" }\r\n                  route:\r\n                    cluster: email-common\r\n                    auto_host_rewrite: true\r\n                    timeout: 50s\r\n                    retry_policy:\r\n                      retry_on: \"5xx\"\r\n                      num_retries: 5\r\n                      per_try_timeout: 10s\r\n            http_filters:\r\n            - name: envoy.router\r\n              config: { deprecated_v1: true }\r\n  clusters:\r\n    name: email-common\r\n    type: LOGICAL_DNS\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /app/data/fullchain.pem\r\n    hosts:\r\n      - socket_address: { address: email-service, port_value: 443 }\r\n```\r\n\r\n\r\n*Logs*:\r\n>Logs of requests for requests with data binary:\r\n\r\n```\r\n[2019-05-14 10:25:28.054][000036][debug][router] [source/common/router/router.cc:1023] [C782][S9375733934444584087] pool ready\r\n--\r\n\u00a0 | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:615] [C782][S9375733934444584087] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:968] [C782][S9375733934444584087] resetting pool request\r\n\u00a0 | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:270] [C784][S6563403434288646526] cluster 'email-common' match for URL '/email/'\r\n\u00a0 | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:328] [C784][S6563403434288646526] router decoding headers:\r\n\u00a0 | ':authority', 'envoy-proxy'\r\n\u00a0 | ':path', '/email/'\r\n\u00a0 | ':method', 'POST'\r\n\u00a0 | ':scheme', 'http'\r\n\u00a0 | 'content-length', '2266446'\r\n\u00a0 | 'user-agent', 'curl/7.64.0'\r\n\u00a0 | 'accept', '*/*'\r\n\u00a0 | 'authorization', 'none'\r\n\u00a0 | 'content-type', 'application/json'\r\n\u00a0 | 'x-request-id', '693db9a5-46ee-4bd7-9bb3-0766fbe90ed4'\r\n\u00a0 | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n\u00a0 | 'x-forwarded-host', 'envoy-proxy.host.ru;'\r\n\u00a0 | 'x-forwarded-port', '80'\r\n\u00a0 | 'x-forwarded-proto', 'http'\r\n\u00a0 | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;;proto=http'\r\n\u00a0 | 'x-forwarded-for', '10.233.53.119'\r\n\u00a0 | 'x-envoy-internal', 'true'\r\n```\r\n>Logs of requests for requests with empty data:\r\n```\r\n[2019-05-14 10:10:08.241][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n--\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.248][000040][debug][router] [source/common/router/router.cc:1023] [C254][S6944810802321297265] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:615] [C254][S6944810802321297265] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:779] [C254][S6944810802321297265] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:968] [C254][S6944810802321297265] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.260][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.271][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.276][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:270] [C263][S2736688878384099272] cluster 'email-common' match for URL '/email/'\r\n\u00a0 | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:328] [C263][S2736688878384099272] router decoding headers:\r\n\u00a0 | ':authority', 'envoy-proxy'\r\n\u00a0 | ':path', '/email/'\r\n\u00a0 | ':method', 'POST'\r\n\u00a0 | ':scheme', 'http'\r\n\u00a0 | 'content-length', '0'\r\n\u00a0 | 'user-agent', 'curl/7.64.0'\r\n\u00a0 | 'accept', '*/*'\r\n\u00a0 | 'authorization', 'none'\r\n\u00a0 | 'content-type', 'application/json'\r\n\u00a0 | 'x-request-id', 'f627a2c5-a91d-4c71-aa56-61c04163eb88'\r\n\u00a0 | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n\u00a0 | 'x-forwarded-host', 'envoy-proxy.host.ru'\r\n\u00a0 | 'x-forwarded-port', '80'\r\n\u00a0 | 'x-forwarded-proto', 'http'\r\n\u00a0 | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;proto=http'\r\n\u00a0 | 'x-forwarded-for', '10.233.53.119'\r\n\u00a0 | 'x-envoy-internal', 'true'\r\n```\r\n\r\nQuestions:\r\n\r\n1. Why envoy don't execute retries, if I send to it some data binary in requests? The target service return 500 error code, and the envoy can executing retries according '5xx', for example for requests with empty data.\r\n\r\n1. What it can be changed to retry occurring for requests with binary data? Can it is due with some retry policy parameters, such as 'per_try_timeout' or some clusters parameters like type or connect timeout? .\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6930/comments",
    "author": "PetrovMikhail",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-05-14T13:27:21Z",
        "body": "I suspect you're running into the limitation that Envoy is unable to retry requests that are too large to fit into its buffer, evident by the fact that `envoy_cluster_retry_or_shadow_abandoned` is non-zero. Can you try setting `per_connection_buffer_limit_bytes` on the cluster to something higher? The default is 1MiB."
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T14:24:32Z",
        "body": "Yes, I think that your advise in right way, thank you! but I increased this value, and it did not work yet, I think that i have to increase limit file value in another parameters of Envoy too. Do you where it is may be? "
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T20:29:23Z",
        "body": "This is my folder, I didn't understood correct results of tests, which I describe in two previous comments. I already change `per_connection_buffer_limit_bytes` to 30 000 000 value, which just about equal 30 MiB, but when I was testing envoy behavior next, I discovered that envoy still doesn't retry request with files > 1 MiB. Maybe another parameter is occurs which changes buffer size of cluster and so on? "
      },
      {
        "user": "feature-id",
        "created_at": "2021-03-15T12:01:49Z",
        "body": "For all, who got the same problem: there's also per_connection_buffer_limit_bytes option on a listener level, which is 1MB by default."
      }
    ]
  },
  {
    "number": 6412,
    "title": "envoy  proxy Segmentation fault",
    "created_at": "2019-03-28T08:42:50Z",
    "closed_at": "2019-03-29T02:18:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6412",
    "body": "*Title*:  envoy proxy Segmentation fault when I learn examples/redis\r\n\r\n*Description*:\r\n>this is  log\r\n\r\nproxy_1  | [2019-03-28 08:30:11.919][12][debug][redis] [source/extensions/filters/network/redis_proxy/command_splitter_impl.cc:403] redis: splitting '[\"set\", \"name\", \"envoy\"]'\r\nproxy_1  | [2019-03-28 08:30:11.919][12][debug][connection] [source/common/network/connection_impl.cc:644] [C4] connecting to 172.20.0.2:6379\r\nproxy_1  | [2019-03-28 08:30:11.921][12][debug][connection] [source/common/network/connection_impl.cc:653] [C4] connection in progress\r\nproxy_1  | [2019-03-28 08:30:11.921][12][debug][connection] [source/common/network/connection_impl.cc:517] [C4] connected\r\nproxy_1  | [2019-03-28 08:30:11.923][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Segmentation fault, suspect faulting address 0xa\r\nproxy_1  | [2019-03-28 08:30:11.923][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: __restore_rt [0x7f78d87f7390]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #1: [0x88058f]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #2: [0x889b3b]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #3: [0x88b8ee]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #4: [0x88be9f]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #5: [0x8897a9]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #6: [0x88985d]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #7: [0xaa5b4c]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #8: [0xaa23ba]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #9: [0xaa2aca]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #10: [0xa9c78a]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #11: [0xde74c9]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #12: [0xde79ff]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #13: [0xde9608]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #14: [0xa9bdfd]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #15: [0xa96c9e]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #16: [0xfc7575]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: start_thread [0x7f78d87ed6ba]\r\nproxy_1  | Segmentation fault (core dumped)\r\nredis_proxy_1 exited with code 139\r\n\r\nthis is my test steps:\r\n[root@localhost ~]# redis-cli -p 1999\r\n127.0.0.1:1999> set name envoy\r\n(error) no upstream host\r\n127.0.0.1:1999>\r\n\r\nwhat's trouble, I  not charge config  and  follow the examples/redis steps to operate \u3002\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6412/comments",
    "author": "skywli",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-28T16:42:34Z",
        "body": "Please try current master the bug here was reverted."
      }
    ]
  },
  {
    "number": 6159,
    "title": "Bazel build failed in foreign_cc",
    "created_at": "2019-03-04T18:31:44Z",
    "closed_at": "2019-03-04T22:14:04Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6159",
    "body": "**Bug Template**\r\n\r\n*Title*: Bazel build failed in foreign_cc\r\n\r\n*Description*:\r\nWith latest tree, and with bazel 0.21.0,   when \"bazel build //souce/...\", I got\r\n\r\nbazel build //source/exe:envoy-static\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/qiwzhang/github/envoyproxy/envoy/tools/bazel.rc\r\nERROR: /home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/envoy/bazel/foreign_cc/BUILD:86:1: in cmake_external rule @envoy//bazel/foreign_cc:yaml: \r\nTraceback (most recent call last):\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/envoy/bazel/foreign_cc/BUILD\", line 86\r\n                cmake_external(name = 'yaml')\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/rules_foreign_cc/tools/build_defs/cmake.bzl\", line 47, in _cmake_external\r\n                cc_external_rule_impl(ctx, attrs)\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/foreign_cc_impl/framework.bzl\", line 209, in cc_external_rule_impl\r\n                _define_out_cc_info(ctx, attrs, inputs, outputs)\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/foreign_cc_impl/framework.bzl\", line 627, in _define_out_cc_info\r\n                cc_common.create_compilation_context(ctx = ctx, headers = depset([outpu...]), <2 more arguments>)\r\nunexpected keyword 'ctx', for call to method create_compilation_context(headers = unbound, system_includes = unbound, includes = unbound, quote_includes = unbound, defines = unbound) of 'cc_common'\r\nERROR: Analysis of target '//source/exe:envoy-static' failed; build aborted: Analysis of target '@envoy//bazel/foreign_cc:yaml' failed; build aborted\r\nINFO: Elapsed time: 1.483s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (213 packages loaded, 4783 targets configured)\r\n    Fetching @com_github_gperftools_gperftools; Patching repository\r\n    Fetching @com_github_nghttp2_nghttp2; fetching\r\n    Fetching @com_github_c_ares_c_ares; fetching\r\n    Fetching @com_github_circonus_labs_libcircllhist; fetching\r\n    Fetching @boringssl; fetching\r\n    Fetching @com_github_madler_zlib; fetching\r\n\r\n\r\n\r\nBTW,  I also tried with bazel 0.23.0. Same issue\r\n\r\nAny clues?\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6159/comments",
    "author": "qiwzhang",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-03-04T20:28:55Z",
        "body": "Did you try `bazel clean --expunge`? 0.21 doesn't work with latest, but 0.22 or 0.23 should work and they are tested in CI."
      }
    ]
  },
  {
    "number": 5595,
    "title": "Websocket connection does not work between envoy sidecars",
    "created_at": "2019-01-14T15:16:20Z",
    "closed_at": "2019-01-16T15:05:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5595",
    "body": "Hi, I am trying to do a simple WS example which includes `front-envoy` and `service1`. The `service1` is a WS server coupling with an envoy sidecar. `front-proxy` is for passing through the WS request to `service1`. I use the new version to configure WS with `upgrade_type: websocket`. It works when a WS client sends a request to the envoy sidecar of `service1`, but it does not work when sending request to `front-proxy`. It return handshake status 503. Below are the envoy files, and I use the `envoy-alpine:lastest` image. \r\n\r\n*Config envoy for `front-proxy`*:\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/service1\"\r\n                route:\r\n                  cluster: service1\r\n          http_filters:\r\n          - name: envoy.router\r\n            config: {}\r\n  clusters:\r\n  - name: service1\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    hosts:\r\n    - socket_address:\r\n        address: service1\r\n        port_value: 80\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n*Config envoy for `service1`*:\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n                - match:\r\n                    prefix: \"/service1\"\r\n                  route:\r\n                    cluster: local_service\r\n          http_filters:\r\n            - name: envoy.router\r\n              config: {}\r\n  clusters:\r\n  - name: local_service\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 8888\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5595/comments",
    "author": "pltanhthu",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2019-01-15T14:46:33Z",
        "body": "I assume from port 80 these are all http but given the codec is auto, if either is H2 you need to explicitly set `allow_connect in http2_protocol_options.  \r\n\r\nI don't think there's many websocket failure paths which return 50x though - upgrade failures should be 403, and show up in stats as rejected upgrades.    I think I'd need to see header traces to do better here, sorry :-/"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-01-16T15:05:36Z",
        "body": "Sweet, glad to hear it!  Sadly there's not so much we can do about that from a discoverability standpoint other than maybe eventually flipping that true by default - nghttp2 rejects the headers as invalid before we inspect them so it's not obvious to the Envoy code base that an upgrade was rejected in that case."
      }
    ]
  },
  {
    "number": 5410,
    "title": "Building envoy on OS X",
    "created_at": "2018-12-24T15:16:55Z",
    "closed_at": "2018-12-25T09:52:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5410",
    "body": "I'm trying to build envoy on OS X mojave `10.14.2 (18C54)` without success.\r\n\r\nHere is the executed command :\r\n`bazel build --incompatible_package_name_is_a_function=false --action_env=PATH=/usr/local/bin:/opt/local/bin:/usr/bin:/bin //source/exe:envoy-static`\r\n\r\nHere is the result: \r\n```shell\r\nINFO: Invocation ID: 9d5adc00-d36e-4957-b2c0-3f984d6cdc44\r\n/private/var/tmp/_bazel_nicolassterchele/19b5387e66de067346e6f0614dfa8b23/external/envoy_deps/./repositories.sh: line 8: md5: command not found\r\nExternal dependency cache directory /private/var/tmp/_bazel_nicolassterchele/19b5387e66de067346e6f0614dfa8b23/external/envoy_deps_cache_\r\n./build_and_install_deps.sh: line 12: sysctl: command not found\r\nmake: the `-j' option requires a positive integral argument\r\n```\r\nI guess that there is specifics commands for linux that is not available on darwin... Am I right ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5410/comments",
    "author": "sterchelen",
    "comments": [
      {
        "user": "moderation",
        "created_at": "2018-12-24T16:34:14Z",
        "body": "@sterchelen What version of Bazel are your running? You should upgrade to 0.21.0 if you are on something earlier. There were some recent changes for MacOS using that build that work. I build with `bazel build -c opt //source/exe:envoy-static.stripped` and `.bazelrc` is:\r\n```\r\nimport %workspace%/tools/bazel.rc\r\nbuild --announce_rc\r\nbuild --define google_grpc=disabled\r\nbuild --define signal_trace=disabled\r\nbuild --action_env=PATH=/bin:/opt/local/bin:/usr/bin:/usr/local/bin\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2018-12-24T18:24:48Z",
        "body": "This: \r\n\r\n```\r\n./build_and_install_deps.sh: line 12: sysctl: command not found\r\n```\r\n\r\nshould have been fixed on latest master to use the full path, can you try to pull latest?"
      }
    ]
  },
  {
    "number": 4672,
    "title": "[Question] /envoy_shared_memory_110 check user permissions. Error: File exists",
    "created_at": "2018-10-10T14:28:45Z",
    "closed_at": "2018-10-11T01:10:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4672",
    "body": "**Issue Template**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nI started envoy with root user by mistake. I killed it.\r\nThen when i start envoy again with my own user, it says:\r\n[!184 06:42:56  ~]$ [2018-10-10 06:42:56.702][23046][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n\r\nWhich file should i remove before i start envoy with my own users? Thanks\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4672/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-10T14:30:15Z",
        "body": "Full screen output\r\n===============\r\n[!185 07:29:28  ~]$ /home/zapp/apps/envoy -c /home/zapp/apps/11/envoy/config.yaml --base-id 11 --v2-config-only\r\n[2018-10-10 07:29:31.869][26733][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n[2018-10-10 07:29:31.871][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Aborted, suspect faulting address 0x1f40000686d\r\n[2018-10-10 07:29:31.873][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:94] Backtrace thr<0> obj</lib64/libc.so.6> (If unsymbolized, use tools/stack_decode.py):\r\n[2018-10-10 07:29:31.887][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #0 0x7f4a4c829277 raise\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #1 0x7f4a4c82a967 abort\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:31.984][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #2 0x6cf1c4 Envoy::Server::SharedMemory::initialize()\r\n[2018-10-10 07:29:32.068][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #3 0x6cf8b2 Envoy::Server::HotRestartImpl::HotRestartImpl()\r\n[2018-10-10 07:29:32.153][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #4 0x48e3d7 Envoy::MainCommonBase::MainCommonBase()\r\n[2018-10-10 07:29:32.238][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #5 0x48e732 Envoy::MainCommon::MainCommon()\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #6 0x419905 main\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</lib64/libc.so.6>\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #7 0x7f4a4c815444 __libc_start_main\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<0> #8 0x484834 (unknown)\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:121] end backtrace thread 0\r\nAborted (core dumped)\r\n[!186 07:29:32 zapp@5.fet.stg.slv.zuora ~]$\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-10T16:30:01Z",
        "body": "`/dev/shm/envoy_shared_memory_110` on Linux."
      }
    ]
  },
  {
    "number": 1189,
    "title": "TLS between Envoys is not working",
    "created_at": "2017-06-29T00:34:06Z",
    "closed_at": "2017-06-29T16:27:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1189",
    "body": "Hi,\r\n\r\nWe are trying to initiate a TCP connection through two Envoys, like so:\r\n```\r\nClient (nc) --> \"Client\" Envoy ==> \"Server\" Envoy --> Server (nc -l)\r\n```\r\nWhere the `==>` arrow is TLS (server certs, no client certs).\r\n\r\nThe client connection looks like\r\n```\r\ndate | nc -v 127.0.0.1 10000\r\n```\r\n\r\nAnd the server runs as\r\n```\r\nnc -l -k 8080\r\n```\r\n\r\nWhen trying to connect, we get inconsistent behavior.   Sometimes the date appears on the server side `nc`, but often it does not.\r\n\r\nHowever, the client side `nc` always reports \r\n```\r\nConnection to 127.0.0.1 10000 port [tcp/webmin] succeeded!\r\n```\r\n\r\n\r\nOn the server side, the Envoy debug logs are:\r\n```\r\n[2017-06-29 00:32:55.981][116][info][main] source/server/connection_handler_impl.cc:109] [C1] new connection\r\n[2017-06-29 00:32:55.981][116][info][filter] source/common/filter/tcp_proxy.cc:117] [C1] new tcp proxy session\r\n[2017-06-29 00:32:55.981][116][debug][filter] source/common/filter/tcp_proxy.cc:133] [C1] Creating connection to cluster service_local\r\n[2017-06-29 00:32:55.981][116][debug][connection] source/common/network/connection_impl.cc:418] [C2] connecting to 127.0.0.1:8080\r\n[2017-06-29 00:32:55.981][116][debug][connection] source/common/network/connection_impl.cc:427] [C2] connection in progress\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/ssl/connection_impl.cc:128] [C1] handshake error: 5\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:341] [C1] remote close\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:135] [C1] closing socket: 1\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:102] [C2] closing data_to_write=0 type=1\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:135] [C2] closing socket: 2\r\n[2017-06-29 00:32:55.983][116][info][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n```\r\n\r\nAnd the client side, the Envoy debug logs are:\r\n\r\n```\r\n[2017-06-29 00:32:55.980][894][info][main] source/server/connection_handler_impl.cc:109] [C1] new connection\r\n[2017-06-29 00:32:55.980][894][info][filter] source/common/filter/tcp_proxy.cc:117] [C1] new tcp proxy session\r\n[2017-06-29 00:32:55.980][894][debug][filter] source/common/filter/tcp_proxy.cc:133] [C1] Creating connection to cluster service_remote\r\n[2017-06-29 00:32:55.980][894][debug][connection] source/common/network/connection_impl.cc:418] [C2] connecting to 172.17.0.2:10001\r\n[2017-06-29 00:32:55.980][894][debug][connection] source/common/network/connection_impl.cc:427] [C2] connection in progress\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:341] [C1] remote close\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:135] [C1] closing socket: 1\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:102] [C2] closing data_to_write=29 type=1\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/ssl/connection_impl.cc:128] [C2] handshake error: 2\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:135] [C2] closing socket: 2\r\n[2017-06-29 00:32:55.981][894][info][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n```\r\n\r\n\r\n\r\nConfig for \"server\" Envoy:\r\n```json\r\n{\r\n    \"listeners\": [{\r\n        \"address\": \"tcp://0.0.0.0:10001\",\r\n        \"filters\": [{\r\n            \"type\": \"read\",\r\n            \"name\": \"tcp_proxy\",\r\n            \"config\": {\r\n              \"stat_prefix\": \"ingress_tcp\",\r\n              \"route_config\": {\r\n                  \"routes\": [{\r\n                    \"cluster\": \"service_local\"\r\n               }]\r\n            }\r\n          }\r\n        }],\r\n        \"ssl_context\": {\r\n          \"cert_chain_file\": \"certs/server.crt\",\r\n          \"private_key_file\": \"certs/server.key\"\r\n        }\r\n    }],\r\n    \"admin\": {\r\n        \"access_log_path\": \"/tmp/admin_access.log\",\r\n        \"address\": \"tcp://127.0.0.1:9901\"\r\n    },\r\n    \"cluster_manager\": {\r\n        \"clusters\": [{\r\n            \"name\": \"service_local\",\r\n            \"connect_timeout_ms\": 2500,\r\n            \"type\": \"static\",\r\n            \"lb_type\": \"round_robin\",\r\n            \"hosts\": [{\r\n                \"url\": \"tcp://127.0.0.1:8080\"\r\n            }]\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\nConfig for \"client\" Envoy:\r\n```json\r\n{\r\n    \"listeners\": [{\r\n        \"address\": \"tcp://0.0.0.0:10000\",\r\n        \"filters\": [{\r\n            \"type\": \"read\",\r\n            \"name\": \"tcp_proxy\",\r\n            \"config\": {\r\n                \"stat_prefix\": \"ingress_tcp\",\r\n                \"route_config\": {\r\n                    \"routes\": [{\r\n                            \"cluster\": \"service_remote\"\r\n                    }]\r\n                }\r\n            }\r\n        }]\r\n    }],\r\n    \"admin\": {\r\n        \"access_log_path\": \"/tmp/admin_access.log\",\r\n        \"address\": \"tcp://127.0.0.1:9901\"\r\n    },\r\n    \"cluster_manager\": {\r\n        \"clusters\": [{\r\n            \"name\": \"service_remote\",\r\n            \"connect_timeout_ms\": 2500,\r\n            \"type\": \"static\",\r\n            \"lb_type\": \"round_robin\",\r\n            \"hosts\": [{\r\n                \"url\": \"tcp://172.17.0.2:10001\"\r\n            }],\r\n            \"ssl_context\": {\r\n              \"ca_cert_file\": \"certs/ca.crt\"\r\n            }\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\n\r\nAny idea why this setup doesn't work consistently?\r\n\r\nThanks,\r\nAngela and @rosenhouse\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1189/comments",
    "author": "angelachin",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-06-29T14:42:37Z",
        "body": "When you run `date | nc -v 127.0.0.1 10000` the connection will be closed after writing the data. Envoy detects this close and terminates the proxied connection leading to a timing issue. Basically, Envoy does not currently support this case in the TCP proxy (accept client connection, write data, and guarantee that all data gets written if client closes connection). \r\n\r\nIs this a real use case or just a test? If just a test, try `telnet` or `nc` without a pipe so you can keep the connection open."
      }
    ]
  }
]