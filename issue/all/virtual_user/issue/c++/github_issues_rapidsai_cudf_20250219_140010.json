[
  {
    "number": 15614,
    "title": "[BUG] 24.04 cuDF.pandas now errors on mixed dtype comparisons in row-wise functions (didn't in 24.02)",
    "created_at": "2024-04-29T22:45:28Z",
    "closed_at": "2024-04-29T23:16:50Z",
    "labels": [
      "question",
      "0 - Waiting on Author"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/15614",
    "body": "**Describe the bug**\r\nIn the cuDF pandas demo notebooks, we try to run `min()` on mixed dtypes.  It works in pandas, and used to work in cuDF.pandas 24.02.  It fails in 24.04.\r\n**Steps/Code to reproduce bug**\r\n```\r\n%load_ext cudf.pandas\r\nimport pandas as pd\r\n\r\nsmall_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\r\nsmall_df = pd.concat([small_df, small_df])\r\n\r\naxis = 0\r\nfor i in range(0, 2):\r\n    small_df.min(axis=axis)\r\n    axis = 1\r\n\r\ncounts = small_df.groupby(\"a\").b.count()\r\n```\r\noutputs\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:889, in _fast_slow_function_call(func, *args, **kwargs)\r\n    888 fast_args, fast_kwargs = _fast_arg(args), _fast_arg(kwargs)\r\n--> 889 result = func(*fast_args, **fast_kwargs)\r\n    890 if result is NotImplemented:\r\n    891     # try slow path\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)\r\n     29 def call_operator(fn, args, kwargs):\r\n---> 30     return fn(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/frame.py:1715, in Frame.min(self, axis, skipna, numeric_only, **kwargs)\r\n   1680 \"\"\"\r\n   1681 Return the minimum of the values in the DataFrame.\r\n   1682 \r\n   (...)\r\n   1713     Parameters currently not supported are `level`, `numeric_only`.\r\n   1714 \"\"\"\r\n-> 1715 return self._reduce(\r\n   1716     \"min\",\r\n   1717     axis=axis,\r\n   1718     skipna=skipna,\r\n   1719     numeric_only=numeric_only,\r\n   1720     **kwargs,\r\n   1721 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6267, in DataFrame._reduce(self, op, axis, numeric_only, **kwargs)\r\n   6266 elif axis == 1:\r\n-> 6267     return source._apply_cupy_method_axis_1(op, **kwargs)\r\n   6268 else:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6449, in DataFrame._apply_cupy_method_axis_1(self, method, *args, **kwargs)\r\n   6447 kwargs.pop(\"cast_to_int\", None)\r\n-> 6449 prepared, mask, common_dtype = self._prepare_for_rowwise_op(\r\n   6450     method, skipna, numeric_only\r\n   6451 )\r\n   6452 for col in prepared._data.names:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6071, in DataFrame._prepare_for_rowwise_op(self, method, skipna, numeric_only)\r\n   6066 if (\r\n   6067     not numeric_only\r\n   6068     and is_string_dtype(common_dtype)\r\n   6069     and any(not is_string_dtype(dt) for dt in filtered.dtypes)\r\n   6070 ):\r\n-> 6071     raise TypeError(\r\n   6072         f\"Cannot perform row-wise {method} across mixed-dtype columns,\"\r\n   6073         \" try type-casting all the columns to same dtype.\"\r\n   6074     )\r\n   6076 if not skipna and any(col.nullable for col in filtered._columns):\r\n\r\nTypeError: Cannot perform row-wise min across mixed-dtype columns, try type-casting all the columns to same dtype.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[2], line 11\r\n      9 axis = 0\r\n     10 for i in range(0, 2):\r\n---> 11     small_df.min(axis=axis)\r\n     12     axis = 1\r\n     14 counts = small_df.groupby(\"a\").b.count()\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:837, in _CallableProxyMixin.__call__(self, *args, **kwargs)\r\n    836 def __call__(self, *args, **kwargs) -> Any:\r\n--> 837     result, _ = _fast_slow_function_call(\r\n    838         # We cannot directly call self here because we need it to be\r\n    839         # converted into either the fast or slow object (by\r\n    840         # _fast_slow_function_call) to avoid infinite recursion.\r\n    841         # TODO: When Python 3.11 is the minimum supported Python version\r\n    842         # this can use operator.call\r\n    843         call_operator,\r\n    844         self,\r\n    845         args,\r\n    846         kwargs,\r\n    847     )\r\n    848     return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:902, in _fast_slow_function_call(func, *args, **kwargs)\r\n    900         slow_args, slow_kwargs = _slow_arg(args), _slow_arg(kwargs)\r\n    901         with disable_module_accelerator():\r\n--> 902             result = func(*slow_args, **slow_kwargs)\r\n    903 return _maybe_wrap_result(result, func, *args, **kwargs), fast\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)\r\n     29 def call_operator(fn, args, kwargs):\r\n---> 30     return fn(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11630, in DataFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  11622 @doc(make_doc(\"min\", ndim=2))\r\n  11623 def min(\r\n  11624     self,\r\n   (...)\r\n  11628     **kwargs,\r\n  11629 ):\r\n> 11630     result = super().min(axis, skipna, numeric_only, **kwargs)\r\n  11631     if isinstance(result, Series):\r\n  11632         result = result.__finalize__(self, method=\"min\")\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:12385, in NDFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  12378 def min(\r\n  12379     self,\r\n  12380     axis: Axis | None = 0,\r\n   (...)\r\n  12383     **kwargs,\r\n  12384 ):\r\n> 12385     return self._stat_function(\r\n  12386         \"min\",\r\n  12387         nanops.nanmin,\r\n  12388         axis,\r\n  12389         skipna,\r\n  12390         numeric_only,\r\n  12391         **kwargs,\r\n  12392     )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:12374, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\r\n  12370 nv.validate_func(name, (), kwargs)\r\n  12372 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\r\n> 12374 return self._reduce(\r\n  12375     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\r\n  12376 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11549, in DataFrame._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n  11545     df = df.T\r\n  11547 # After possibly _get_data and transposing, we are now in the\r\n  11548 #  simple case where we can use BlockManager.reduce\r\n> 11549 res = df._mgr.reduce(blk_func)\r\n  11550 out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\r\n  11551 if out_dtype is not None and out.dtype != \"boolean\":\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1500, in BlockManager.reduce(self, func)\r\n   1498 res_blocks: list[Block] = []\r\n   1499 for blk in self.blocks:\r\n-> 1500     nbs = blk.reduce(func)\r\n   1501     res_blocks.extend(nbs)\r\n   1503 index = Index([None])  # placeholder\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:404, in Block.reduce(self, func)\r\n    398 @final\r\n    399 def reduce(self, func) -> list[Block]:\r\n    400     # We will apply the function and reshape the result into a single-row\r\n    401     #  Block with the same mgr_locs; squeezing will be done at a higher level\r\n    402     assert self.ndim == 2\r\n--> 404     result = func(self.values)\r\n    406     if self.values.ndim == 1:\r\n    407         res_values = result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11468, in DataFrame._reduce.<locals>.blk_func(values, axis)\r\n  11466         return np.array([result])\r\n  11467 else:\r\n> 11468     return op(values, axis=axis, skipna=skipna, **kwds)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\r\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    146 else:\r\n--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    149 return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\r\n    401 if datetimelike and mask is None:\r\n    402     mask = isna(values)\r\n--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\r\n    406 if datetimelike:\r\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:1098, in _nanminmax.<locals>.reduction(values, axis, skipna, mask)\r\n   1093     return _na_for_min_count(values, axis)\r\n   1095 values, mask = _get_values(\r\n   1096     values, skipna, fill_value_typ=fill_value_typ, mask=mask\r\n   1097 )\r\n-> 1098 result = getattr(values, meth)(axis)\r\n   1099 result = _maybe_null_out(result, axis, mask, values.shape)\r\n   1100 return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:45, in _amin(a, axis, out, keepdims, initial, where)\r\n     43 def _amin(a, axis=None, out=None, keepdims=False,\r\n     44           initial=_NoValue, where=True):\r\n---> 45     return umr_minimum(a, axis, None, out, keepdims, initial, where)\r\n\r\nTypeError: '<=' not supported between instances of 'int' and 'str'\r\n```\r\n\r\nit used to output a warning:\r\n```\r\n/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py:5971: UserWarning: Row-wise operations currently only support int, float and bool dtypes. Non numeric columns are ignored.\r\n  warnings.warn(msg)\r\n```\r\nand then worked:\r\n```\r\na\r\n0    2\r\n1    2\r\n2    2\r\nName: b, dtype: int64\r\n```\r\n\r\n**Expected behavior**\r\n```\r\n>>> counts\r\na\r\n0    2\r\n1    2\r\n2    2\r\nName: b, dtype: int64\r\n```\r\nwhich is what I get in pandas and 24.02 cuDF.pandas\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: [Docker]\r\n - Method of cuDF install: [Docker]\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used:  \r\n   - for 24.02: `docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/notebooks:24.02-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root`\r\n   - For 24.04:  `docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/notebooks:24.04-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root`\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/15614/comments",
    "author": "taureandyernv",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2024-04-29T23:00:26Z",
        "body": "Pandas 2 enforces `numeric_only=False` rather than the prior behavior that filtered down to numeric if things failed.\r\n\r\nSo I believe this is now the expected behavior and we should update our notebooks. @galipremsagar @mroeschke , is that your understanding?"
      },
      {
        "user": "galipremsagar",
        "created_at": "2024-04-29T23:08:40Z",
        "body": "That's right @beckernick, @taureandyernv can you verify if cudf-24.04 matches upto pandas-2.x? Here is what I get for `pandas-2.x`:\r\n\r\n```ipython\r\nIn [1]: import pandas as pd\r\n   ...: \r\n   ...: small_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\r\n   ...: small_df = pd.concat([small_df, small_df])\r\n   ...: \r\n   ...: axis = 0\r\n   ...: for i in range(0, 2):\r\n   ...:     small_df.min(axis=axis)\r\n   ...:     axis = 1\r\n   ...: \r\n   ...: counts = small_df.groupby(\"a\").b.count()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 8\r\n      6 axis = 0\r\n      7 for i in range(0, 2):\r\n----> 8     small_df.min(axis=axis)\r\n      9     axis = 1\r\n     11 counts = small_df.groupby(\"a\").b.count()\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/frame.py:11643, in DataFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  11635 @doc(make_doc(\"min\", ndim=2))\r\n  11636 def min(\r\n  11637     self,\r\n   (...)\r\n  11641     **kwargs,\r\n  11642 ):\r\n> 11643     result = super().min(axis, skipna, numeric_only, **kwargs)\r\n  11644     if isinstance(result, Series):\r\n  11645         result = result.__finalize__(self, method=\"min\")\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/generic.py:12388, in NDFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  12381 def min(\r\n  12382     self,\r\n  12383     axis: Axis | None = 0,\r\n   (...)\r\n  12386     **kwargs,\r\n  12387 ):\r\n> 12388     return self._stat_function(\r\n  12389         \"min\",\r\n  12390         nanops.nanmin,\r\n  12391         axis,\r\n  12392         skipna,\r\n  12393         numeric_only,\r\n  12394         **kwargs,\r\n  12395     )\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\r\n  12373 nv.validate_func(name, (), kwargs)\r\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\r\n> 12377 return self._reduce(\r\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\r\n  12379 )\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/frame.py:11562, in DataFrame._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n  11558     df = df.T\r\n  11560 # After possibly _get_data and transposing, we are now in the\r\n  11561 #  simple case where we can use BlockManager.reduce\r\n> 11562 res = df._mgr.reduce(blk_func)\r\n  11563 out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\r\n  11564 if out_dtype is not None and out.dtype != \"boolean\":\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/internals/managers.py:1500, in BlockManager.reduce(self, func)\r\n   1498 res_blocks: list[Block] = []\r\n   1499 for blk in self.blocks:\r\n-> 1500     nbs = blk.reduce(func)\r\n   1501     res_blocks.extend(nbs)\r\n   1503 index = Index([None])  # placeholder\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/internals/blocks.py:404, in Block.reduce(self, func)\r\n    398 @final\r\n    399 def reduce(self, func) -> list[Block]:\r\n    400     # We will apply the function and reshape the result into a single-row\r\n    401     #  Block with the same mgr_locs; squeezing will be done at a higher level\r\n    402     assert self.ndim == 2\r\n--> 404     result = func(self.values)\r\n    406     if self.values.ndim == 1:\r\n    407         res_values = result\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/frame.py:11481, in DataFrame._reduce.<locals>.blk_func(values, axis)\r\n  11479         return np.array([result])\r\n  11480 else:\r\n> 11481     return op(values, axis=axis, skipna=skipna, **kwds)\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\r\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    146 else:\r\n--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    149 return result\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\r\n    401 if datetimelike and mask is None:\r\n    402     mask = isna(values)\r\n--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\r\n    406 if datetimelike:\r\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/nanops.py:1098, in _nanminmax.<locals>.reduction(values, axis, skipna, mask)\r\n   1093     return _na_for_min_count(values, axis)\r\n   1095 values, mask = _get_values(\r\n   1096     values, skipna, fill_value_typ=fill_value_typ, mask=mask\r\n   1097 )\r\n-> 1098 result = getattr(values, meth)(axis)\r\n   1099 result = _maybe_null_out(result, axis, mask, values.shape)\r\n   1100 return result\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/numpy/core/_methods.py:45, in _amin(a, axis, out, keepdims, initial, where)\r\n     43 def _amin(a, axis=None, out=None, keepdims=False,\r\n     44           initial=_NoValue, where=True):\r\n---> 45     return umr_minimum(a, axis, None, out, keepdims, initial, where)\r\n\r\nTypeError: '<=' not supported between instances of 'int' and 'str'\r\n```"
      }
    ]
  },
  {
    "number": 14395,
    "title": "[QST]use cudf.concat slower than pandas.concat",
    "created_at": "2023-11-10T10:37:20Z",
    "closed_at": "2023-11-10T14:15:25Z",
    "labels": [
      "question",
      "Performance"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/14395",
    "body": "** cudf.concat slower than pandas.concat**\r\n## here is my code:\r\n```\r\nimport os, time, pandas as pd, numpy as np\r\nimport cudf\r\nfrom tqdm import tqdm\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES']='1'\r\ndef pd_concat_test(df):\r\n    st = 10\r\n    tdf = df[:st].copy()\r\n    n = len(df) - st\r\n    ta = time.time()\r\n    for i in tqdm(range(st, len(df))):\r\n        tdf = pd.concat([tdf, df[i:i+1]])\r\n    tb = time.time()\r\n\r\n    print(f'pd concat {n} times cost {tb-ta :.2f} s.')\r\n\r\n\r\ndef cupd_concat_test(cdf):\r\n    st = 10\r\n    tdf = cdf[:st].copy()\r\n    n = len(cdf) - st\r\n    ta = time.time()\r\n    for i in tqdm(range(st, len(cdf))):\r\n        tdf = cudf.concat([tdf, cdf[i:i+1]])\r\n    tb = time.time()\r\n\r\n    print(f'cudf concat {n} times cost {tb-ta :.2f} s.')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    in_csv = 'target.csv'\r\n    \r\n    df = pd.read_csv(in_csv)\r\n    cdf = cudf.read_csv(in_csv)\r\n    print(df.head(5))\r\n    cupd_concat_test(cdf)\r\n    pd_concat_test(df)\r\n```\r\n## here's output:\r\n```\r\n       timestamp     open     high      low    close    volume  quote_volume\r\n0  1577836800000  7189.43  7190.52  7170.15  7171.55  2449.049   17576407.75\r\n1  1577840400000  7171.55  7225.00  7171.10  7210.24  3865.038   27838016.40\r\n2  1577844000000  7210.24  7239.30  7206.46  7237.99  3228.365   23324787.16\r\n3  1577847600000  7237.41  7239.74  7215.00  7221.65  2513.307   18161803.91\r\n4  1577851200000  7221.65  7225.41  7211.22  7213.86  1176.666    8493621.94\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 33398/33398 [01:01<00:00, 542.44it/s]\r\ncudf concat 33398 times cost 61.57 s.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 33398/33398 [00:14<00:00, 2279.08it/s]\r\npd concat 33398 times cost 14.65 s.\r\n```\r\n## here's my env: \r\n* python3.10.4\r\n*  nvcc -V\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n```\r\n* pip list |grep cu\r\n```\r\ncubinlinker-cu11          0.3.0.post1\r\ncucim                     23.10.0\r\ncuda-python               11.8.3\r\ncudf-cu11                 23.10.1\r\ncugraph-cu11              23.10.0\r\ncuml-cu11                 23.10.0\r\ncuproj-cu11               23.10.0\r\ncupy-cuda11x              12.2.0\r\ncuspatial-cu11            23.10.0\r\ncuxfilter-cu11            23.10.0\r\ndask-cuda                 23.10.0\r\ndask-cudf-cu11            23.10.1\r\ndocutils                  0.20\r\nexecuting                 1.2.0\r\nptxcompiler-cu11          0.7.0.post1\r\npylibcugraph-cu11         23.10.0\r\npylibraft-cu11            23.10.0\r\nraft-dask-cu11            23.10.0\r\nrmm-cu11                  23.10.0\r\ntorch                     1.12.0+cu113\r\nucx-py-cu11               0.34.0\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/14395/comments",
    "author": "haoran1062",
    "comments": [
      {
        "user": "shwina",
        "created_at": "2023-11-10T11:44:14Z",
        "body": "Hi @haoran1062 -- thank you for reporting! Please let me know if the below answers your question:\r\n\r\n## Why is it slow?\r\n\r\nGPUs are generally faster because they operate on data in parallel. If you have very small operations (e.g., involving a single row), there is little or no parallelism that the GPU can take advantage of. \r\n\r\n## Operate on larger chunks of data to see the benefit from GPUs\r\n\r\nEach `concat` operation in your example appends a single row to a dataframe. In general, you will not see the benefit of the GPU for very small operations like these. You may even see some slowdown for very small operations compared to the CPU.\r\n\r\nLet's modify the example to use `concat` with larger chunks. In the snippet below, each concat appends a dataframe of size `10_000` to ultimately produce a dataframe of size `10_000_000`. The speedup from using the GPU should be more obvious:\r\n\r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 999/999 [00:02<00:00, 441.12it/s]\r\ncudf concat 999 times cost 2.27 s.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 999/999 [00:34<00:00, 28.90it/s]\r\npd concat 999 times cost 34.57 s.\r\n```\r\n\r\nThe code:\r\n\r\n```python\r\nimport os, time, pandas as pd, numpy as np\r\nimport cudf\r\nfrom tqdm import tqdm\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES']='1'\r\ndef pd_concat_test(df):\r\n    chunk_size = 10_000\r\n    tdf = df[:chunk_size].copy()\r\n    n = len(df) - chunk_size\r\n    ta = time.time()\r\n    rng = range(chunk_size, len(df), chunk_size)\r\n    for i, chunk_start in enumerate(tqdm(rng)):\r\n        chunk_end = chunk_start + chunk_size        \r\n        tdf = pd.concat([tdf, df[chunk_start:chunk_end]])\r\n    tb = time.time()\r\n\r\n    print(f'pd concat {i+1} times cost {tb-ta :.2f} s.')\r\n    return tdf\r\n\r\n\r\ndef cupd_concat_test(cdf):\r\n    chunk_size = 10_000\r\n    tdf = cdf[:chunk_size].copy()\r\n    n = len(cdf) - chunk_size\r\n    ta = time.time()\r\n    rng = range(chunk_size, len(cdf), chunk_size)\r\n    for i, chunk_start in enumerate(tqdm(rng)):\r\n        chunk_end = chunk_start + chunk_size\r\n        tdf = cudf.concat([tdf, cdf[chunk_start:chunk_end]])\r\n    tb = time.time()\r\n\r\n    print(f'cudf concat {i+1} times cost {tb-ta :.2f} s.')\r\n    return tdf\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cdf = cudf.datasets.randomdata(10_000_000)\r\n    df = cdf.to_pandas()\r\n    print(\"Heads:\")\r\n    print(cdf.head())\r\n    print(df.head())\r\n\r\n    print(\"Tails:\")\r\n    print(cdf.tail())\r\n    print(df.tail())\r\n    cdf = cupd_concat_test(cdf)\r\n    df = pd_concat_test(df)\r\n    print(\"Heads:\")\r\n    print(cdf.head())\r\n    print(df.head())\r\n\r\n    print(\"Tails:\")\r\n    print(cdf.tail())\r\n    print(df.tail())\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T12:13:17Z",
        "body": "@shwina thanks for your answer. I tested it according to your suggestions and the results are really good. \r\n```\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1282/1282 [00:17<00:00, 71.86it/s]\r\ncudf concat 1282 times cost 17.84 s.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1282/1282 [02:49<00:00,  7.58it/s]\r\npd concat 1282 times cost 169.24 s.\r\n```\r\nbut I do really have that question Do you have any suitable suggestions how to speed up add columns one by one with high frequency operation"
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T12:17:59Z",
        "body": "@shwina Do you have any suitable suggestions about how to speed up add rows one by one with high frequency operation\uff1f"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T12:20:58Z",
        "body": "Sorry, I missed your question at the end.\r\n\r\n> Do you have any suitable suggestions how to speed up add columns one by one with high frequency operation\uff1f\r\n\r\nDo you mean rows, and not columns?\r\n\r\nCan you provide a bit more information about your use case: where is the data coming from? "
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T12:28:13Z",
        "body": "> Sorry, I missed your question at the end.\r\n> \r\n> > Do you have any suitable suggestions how to speed up add columns one by one with high frequency operation\uff1f\r\n> \r\n> Do you mean rows, and not columns?\r\n> \r\n> Can you provide a bit more information about your use case: where is the data coming from?\r\n\r\nmy bad, add one rows one by one .\r\nthe data is market data, which have timestamp, open, high, low, close, volume, and other data.\r\n"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T14:01:19Z",
        "body": "Thanks! I think the way to do this is to collect your data in batches before appending it to the DataFrame. Here's how I would do it at a high level:\r\n\r\n```python\r\nimport cudf\r\nimport numpy as np\r\n\r\n\r\ndef producer():\r\n    for i in range(1_000_000):\r\n        ts = np.datetime64(\"now\") + np.timedelta64(i, \"s\")\r\n        yield (ts, np.random.rand(), np.random.rand(), np.random.rand(), np.random.rand(), np.random.rand())\r\n\r\n        \r\nif __name__ == \"__main__\":\r\n    batch_size = 100_000\r\n    \r\n    df = cudf.DataFrame()\r\n\r\n    records = np.recarray(batch_size, dtype=[(\"ts\", \"datetime64[ms]\"), (\"a\", \"float64\"), (\"b\", \"float64\"), (\"c\", \"float64\"), (\"d\", \"float64\"), (\"e\", \"float64\")])\r\n\r\n    for i, record in enumerate(producer()):\r\n        print(i)\r\n        # add the record to the batch\r\n        records[i % batch_size] = record\r\n        if i > 0 and (i % batch_size == 0):\r\n            # add the records to the DataFrame\r\n            df = cudf.concat([df, cudf.DataFrame.from_records(records)])\r\n    print(df.head())\r\n        \r\n```\r\n\r\nPlease forgive any minor mistakes there might be in the code above."
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T14:06:27Z",
        "body": "Note that the code above actually won't be much faster than pandas, since most of the time is spent populating the records rather than on any pandas operations."
      }
    ]
  },
  {
    "number": 12980,
    "title": "[QST] What's the cudf overhead for small dataset?",
    "created_at": "2023-03-20T23:29:11Z",
    "closed_at": "2023-03-23T17:45:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12980",
    "body": "**What is your question?**\r\n\r\n```\r\nimport cudf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf1 = pd.DataFrame()\r\ndim = 1000\r\ndf1[\"A\"] = np.random.randint(0, dim, dim)\r\n\r\ndf1_cu = cudf.from_pandas(df1)\r\n\r\n%%time\r\ndf1_cu[\"A\"].sum()\r\n600 \u00b5s\r\n\r\n%%time\r\ndf1[\"A\"].sum()\r\n200 \u00b5s\r\n```\r\n\r\ncudf seems to have some  overhead for small datasets. Where does it come from? It should not from data transfer as \r\ndf1_cu = cudf.from_pandas(df1) has transferred the data.",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12980/comments",
    "author": "zachary62",
    "comments": [
      {
        "user": "bdice",
        "created_at": "2023-03-20T23:46:36Z",
        "body": "@zachary62 I can\u2019t answer this definitively right now, but I would point out that there is still device-host data transfer: the result of the sum must be copied back to the host, incurring a device (or stream) synchronization."
      },
      {
        "user": "zachary62",
        "created_at": "2023-03-20T23:59:36Z",
        "body": "For a task of a large number (e.g., 100000) sum, group-by, max queries, but over small datasets (<10000 rows), is there any way to use cudf for speedup? These queries are independent, and can we exploit inter-query parallelism?"
      },
      {
        "user": "shwina",
        "created_at": "2023-03-23T14:16:57Z",
        "body": "> For a task of a large number (e.g., 100000) sum, group-by, max queries, but over small datasets (<10000 rows), is there any way to use cudf for speedup? These queries are independent, and can we exploit inter-query parallelism?\r\n\r\nOne way would be to leverage groupby.\r\n\r\nSay, for example you have 100 small datasets of 10_000 rows each:\r\n\r\n```python\r\ndfs = [cudf.datasets.randomdata(10_000) for i in range(100)]  # 100 dataframes of 10_000 rows each\r\n```\r\n\r\nYou could compute for example the `min` and `max` of each dataframe as follows:\r\n\r\n```python\r\ndf_stats = [df.agg(['max', 'min']) for df in dfs]\r\nprint(\"\\n\".join(map(str, df_stats[:5])))  # print the first 5 results\r\n         id         x         y\r\nmax  1141.0  0.999934  0.999911\r\nmin   867.0 -0.999895 -0.999854\r\n         id         x         y\r\nmax  1118.0  0.999983  0.999700\r\nmin   890.0 -0.999549 -0.999927\r\n         id         x         y\r\nmax  1104.0  0.999812  0.999611\r\nmin   887.0 -0.999343 -0.999895\r\n         id         x         y\r\nmax  1129.0  0.999822  0.999234\r\nmin   880.0 -0.999846 -0.999479\r\n         id         x         y\r\nmax  1120.0  0.998873  0.999985\r\nmin   884.0 -0.999894 -0.999906\r\n```\r\n\r\nThis is quite slow:\r\n\r\n```python\r\n%%timeit\r\ndf_stats = [df.agg(['max', 'min']) for df in dfs]\r\n\r\n316 ms \u00b1 5.26 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nAnother option would be to use a groupby to compute the `max` and `min` in a single operation. Here's a trick for doing that:\r\n\r\n```python\r\nimport cupy as cp\r\n\r\ndfs_concatenated = cudf.concat(dfs)\r\ngroups = cp.repeat(cp.arange(100), 10_000)\r\ndf_stats = dfs_concatenated.groupby(groups, sort=True).agg(['max', 'min'])\r\nprint(df_stats.head(5))\r\n     id              x                   y          \r\n    max  min       max       min       max       min\r\n0  1141  867  0.999934 -0.999895  0.999911 -0.999854\r\n1  1118  890  0.999983 -0.999549  0.999700 -0.999927\r\n2  1104  887  0.999812 -0.999343  0.999611 -0.999895\r\n3  1129  880  0.999822 -0.999846  0.999234 -0.999479\r\n4  1120  884  0.998873 -0.999894  0.999985 -0.999906\r\n```\r\n\r\nThis is faster:\r\n\r\n```python\r\n%%timeit\r\ngroups = cp.repeat(cp.arange(100), 10_000)\r\ndf_stats = cudf.concat(dfs).groupby(groups, sort=True).agg(['max', 'min'])\r\n\r\n21.4 ms \u00b1 1.54 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 11219,
    "title": "[QST] Disabling  decimal128 support",
    "created_at": "2022-07-07T21:28:34Z",
    "closed_at": "2022-08-29T17:43:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/11219",
    "body": "Hi cudf team,\r\n\r\nI am building from source cudf on pcc64le (Summit cluster)  using CUDA 11.4 and driver 450.80.02\r\n\r\n```\r\n...\r\n[ 11%] Building CXX object CMakeFiles/cudf.dir/src/aggregation/result_cache.cpp.o\r\n[ 11%] Building CXX object CMakeFiles/cudf.dir/src/ast/expression_parser.cpp.o\r\n/sw/summit/ums/gen119/nvrapids/src/nvrapids_v22.06.00_src/cudf_v22.06.00/cpp/include/cudf/utilities/type_dispatcher.hpp(522): error: \"numeric::decimal128\" contains a 128-bit integer, which is not supported in device code\r\n          detected during instantiation of \"decltype(auto) cudf::type_dispatcher(cudf::data_type, Functor, Ts &&...) [with IdTypeMap=cudf::id_to_type_impl, Functor=cudf::detail::unary_relationally_comparable_functor, Ts=<>]\" \r\n/sw/summit/ums/gen119/nvrapids/src/nvrapids_v22.06.00_src/cudf_v22.06.00/cpp/include/cudf/utilities/traits.hpp(149): here\r\n...\r\n```\r\n\r\nIs there way to disable decimal128 support ? \r\n\r\nFrom what I checked so far decimal128 support started in CUDA 11.5 so I would expect support should be disabled when finding older CUDA version, right ?\r\n\r\nThanks,\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11219/comments",
    "author": "benjha",
    "comments": [
      {
        "user": "quasiben",
        "created_at": "2022-08-02T15:01:27Z",
        "body": "I dont' think we can disable decimal 128 support.  Instead of building on 11.4, you could build with 11.5 -> 11.7 then rely on CEC for CUDA 11.0->11.4 backwards compatibility.  This is how we are getting older CUDA Driver/Toolkit while we build conda packages in a 11.7 environment"
      }
    ]
  },
  {
    "number": 7991,
    "title": "[QST] Queston about row number limit in cuDF dataframe",
    "created_at": "2021-04-19T05:13:54Z",
    "closed_at": "2021-04-19T17:25:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7991",
    "body": "Dear cuDF developers,\r\n\r\nI am using Dask to load in some parquet data as a dask-cudf dataframe. When I use `.compute()` to convert from dask-cudf back to cuDF (I need some functions that aren't supported in dask-cudf) I am encountering this error:\r\n\r\n```\r\nRuntimeError: cuDF failure at: ../src/copying/concatenate.cu:365: Total number of concatenated rows exceeds size_type range\r\n```\r\n\r\nMy dataframe has 27 million rows which seems... large but maybe still reasonable? What is the row limit? Is there any way I can increase this limit? \r\n\r\nIf I can provide more info please let me know.\r\n\r\nThank you very much,\r\nLaurie",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7991/comments",
    "author": "lastephey",
    "comments": [
      {
        "user": "davidwendt",
        "created_at": "2021-04-19T12:09:44Z",
        "body": "The row limit for a cudf column/dataframe is 2 billion (2,147,483,647)."
      },
      {
        "user": "beckernick",
        "created_at": "2021-04-19T13:06:00Z",
        "body": "> When I use .compute() to convert from dask-cudf back to cuDF (I need some functions that aren't supported in dask-cudf) I am encountering this error:\r\n\r\nWould you be able to share which operations aren't supported in dask-cudf?\r\n\r\nIn the meantime, could you stay in Dask land but use the cudf operations with `ddf.map_partitions(custom_func)` to operate independently on each individual DataFrame partition?"
      },
      {
        "user": "lastephey",
        "created_at": "2021-04-19T16:32:19Z",
        "body": "Thank you for your quick responses, David and Nick.\r\n\r\nSure, a few of the operations I'm using in cuDF:\r\n* `cudf.melt`\r\n* `cudf.to_datetime`\r\n* `cudf.drop_duplicates`\r\n\r\nThank you for the suggestion-- I can try the map partitions approach and report back. \r\n\r\nI am wondering why I hit this row limit when I am well under 2 billion rows. Does it sound like a possible bug? If so I am happy to file a report.\r\n\r\n"
      },
      {
        "user": "beckernick",
        "created_at": "2021-04-19T17:12:19Z",
        "body": "Thanks Laurie!\r\n\r\nThis likely came up due to non numeric columns in the table. For example, the ~2 billion MAX(int32) limit on string columns presents based on the number of individual characters, rather than rows.\r\n\r\n\r\n> Sure, a few of the operations I'm using in cuDF:\r\n> cudf.melt\r\n> cudf.to_datetime\r\n> cudf.drop_duplicates\r\n\r\ndrop_duplicates and melt should be available as `ddf.drop_duplicates()` and `ddf.melt()`. For `to_datetime`, could you intead explicitly cast the column with `ddf[col].astype(\"datetime64[ms]\")`? If anything of these aren't working, please let us know!"
      },
      {
        "user": "lastephey",
        "created_at": "2021-04-19T17:25:49Z",
        "body": "Thanks Nick. Ok I see, the row limit makes sense.\r\n\r\nThanks for the pointer about melt and drop_duplicates. I see now I was trying to use them incorrectly, like:\r\n\r\n```\r\ndask_cudf.melt(ddf)\r\n```\r\ninstead of\r\n\r\n```\r\nddf.melt()\r\n```\r\n\r\nI think I should be able to make this work within Dask using your suggestions. I'll close this, thank you very much for your help!"
      }
    ]
  },
  {
    "number": 7481,
    "title": "[QST]problems with dask_cudf custom aggregation",
    "created_at": "2021-03-02T04:40:00Z",
    "closed_at": "2021-04-04T09:19:17Z",
    "labels": [
      "question",
      "Python",
      "dask"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7481",
    "body": "**What is your question?**\r\nHi there,\r\n\r\nI'm trying to do a string-join aggregation in dask_cudf groupby dataframe. The input dataframe looks like below:\r\n`documents_categories.compute()`\r\n\r\ndocument_id | kv\r\n-- | --\r\n1595802 | 1611:0.92\r\n1595802 | 1610:0.07\r\n1524246 | 1807:0.92\r\n1524246 | 1608:0.07\r\n\r\n`documents_categories.dtypes`\r\n\r\n> document_id     int64\r\n> kv             object\r\n> dtype: object\r\n\r\nThe expected string-joined result should be:\r\ndocument_id | kv\r\n-- | --\r\n1595802 | 1611:0.92;1610:0.07\r\n1524246 | 1807:0.92;1608:0.07\r\n\r\nI have tried the following codes and other several methods, but still can't get this function running successfully. I'm not a expert in dask_cudf, any suggestions? Thanks!\r\n\r\n```\r\ncustom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\ndocuments_categories.groupby('document_id').agg({\"kv\": custom_join})\r\n```\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py in raise_on_meta_error(funcname, udf)\r\n    179     try:\r\n--> 180         yield\r\n    181     except Exception as e:\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _emulate(func, *args, **kwargs)\r\n   5315     with raise_on_meta_error(funcname(func), udf=kwargs.pop(\"udf\", False)):\r\n-> 5316         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n   5317 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in _groupby_apply_funcs(df, *index, **kwargs)\r\n    920     for result_column, func, func_kwargs in funcs:\r\n--> 921         r = func(grouped, **func_kwargs)\r\n    922 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in _apply_func_to_column(df_like, column, func)\r\n    966 \r\n--> 967     return func(df_like[column])\r\n    968 \r\n\r\n<ipython-input-45-5dd27ef25785> in <lambda>(x)\r\n----> 1 custom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/groupby/groupby.py in __getattribute__(self, key)\r\n     62         try:\r\n---> 63             return super().__getattribute__(key)\r\n     64         except AttributeError:\r\n\r\nAttributeError: 'SeriesGroupBy' object has no attribute 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-46-31b5ac92e045> in <module>\r\n----> 1 documents_categories.groupby('document_id').agg({\"kv\": custom_join})\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in agg(self, arg, split_every, split_out)\r\n   1846     @derived_from(pd.core.groupby.DataFrameGroupBy)\r\n   1847     def agg(self, arg, split_every=None, split_out=1):\r\n-> 1848         return self.aggregate(arg, split_every=split_every, split_out=split_out)\r\n   1849 \r\n   1850 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/groupby.py in aggregate(self, arg, split_every, split_out)\r\n     81 \r\n     82         return super().aggregate(\r\n---> 83             arg, split_every=split_every, split_out=split_out\r\n     84         )\r\n     85 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in aggregate(self, arg, split_every, split_out)\r\n   1842             return self.size()\r\n   1843 \r\n-> 1844         return super().aggregate(arg, split_every=split_every, split_out=split_out)\r\n   1845 \r\n   1846     @derived_from(pd.core.groupby.DataFrameGroupBy)\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in aggregate(self, arg, split_every, split_out)\r\n   1622             split_out=split_out,\r\n   1623             split_out_setup=split_out_on_index,\r\n-> 1624             sort=self.sort,\r\n   1625         )\r\n   1626 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in apply_concat_apply(args, chunk, aggregate, combine, meta, token, chunk_kwargs, aggregate_kwargs, combine_kwargs, split_every, split_out, split_out_setup, split_out_setup_kwargs, sort, ignore_index, **kwargs)\r\n   5267 \r\n   5268     if meta is no_default:\r\n-> 5269         meta_chunk = _emulate(chunk, *args, udf=True, **chunk_kwargs)\r\n   5270         meta = _emulate(\r\n   5271             aggregate, _concat([meta_chunk], ignore_index), udf=True, **aggregate_kwargs\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _emulate(func, *args, **kwargs)\r\n   5314     \"\"\"\r\n   5315     with raise_on_meta_error(funcname(func), udf=kwargs.pop(\"udf\", False)):\r\n-> 5316         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n   5317 \r\n   5318 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    128                 value = type()\r\n    129             try:\r\n--> 130                 self.gen.throw(type, value, traceback)\r\n    131             except StopIteration as exc:\r\n    132                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py in raise_on_meta_error(funcname, udf)\r\n    199         )\r\n    200         msg = msg.format(\" in `{0}`\".format(funcname) if funcname else \"\", repr(e), tb)\r\n--> 201         raise ValueError(msg) from e\r\n    202 \r\n    203 \r\n\r\nValueError: Metadata inference failed in `_groupby_apply_funcs`.\r\n\r\nYou have supplied a custom function and Dask is unable to \r\ndetermine the type of output that that function returns. \r\n\r\nTo resolve this please provide a meta= keyword.\r\nThe docstring of the Dask function you ran should have more information.\r\n\r\nOriginal error is below:\r\n------------------------\r\nAttributeError(\"'SeriesGroupBy' object has no attribute 'str'\")\r\n\r\nTraceback:\r\n---------\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py\", line 180, in raise_on_meta_error\r\n    yield\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py\", line 5316, in _emulate\r\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py\", line 921, in _groupby_apply_funcs\r\n    r = func(grouped, **func_kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py\", line 967, in _apply_func_to_column\r\n    return func(df_like[column])\r\n  File \"<ipython-input-45-5dd27ef25785>\", line 1, in <lambda>\r\n    custom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/groupby/groupby.py\", line 63, in __getattribute__\r\n    return super().__getattribute__(key)\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7481/comments",
    "author": "yuanqingz",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2021-04-01T14:02:14Z",
        "body": "@cocorosiekz we've recently implemented collect list. It looks like it's not cleanly working with Dask (I'll file an issue), but perhaps the following would work for you?\r\n\r\n```python\r\nimport cudf\r\nimport dask_cudf\r\nfrom io import StringIO\r\n\u200b\r\n\u200b\r\ndata = \"\"\"document_id   kv\r\n1595802 1611:0.92\r\n1595802 1610:0.07\r\n1524246 1807:0.92\r\n1524246 1608:0.07\"\"\"\r\n\u200b\r\ndf = cudf.read_csv(StringIO(data), sep=\"\\t\")\r\nddf = dask_cudf.from_cudf(df, 2)\r\n\u200b\r\n\u200b\r\ndef collect_list_agg(df):\r\n    return df.groupby(\"document_id\").agg({\"kv\": list})\r\n\u200b\r\n# ensure every row of a given key is in the same partition\r\npartitioned = ddf.shuffle(on=[\"document_id\"])\r\n\u200b\r\n# run a within-partition cudf groupby collect list\r\nprint(partitioned.map_partitions(collect_list_agg).compute())\r\n                                 kv\r\ndocument_id                        \r\n1595802      [1611:0.92, 1610:0.07]\r\n1524246      [1807:0.92, 1608:0.07]\r\n```\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 6070,
    "title": "[BUG] .str.stod() no longer works on a String column",
    "created_at": "2020-08-22T02:44:33Z",
    "closed_at": "2020-08-24T03:14:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6070",
    "body": "**Describe the bug**\r\nI used `.stod()` to convert a string column to a decimal. However, this has stopped working on nightly 0.15.\r\n\r\n**Steps/Code to reproduce bug**\r\nMinimal example:\r\n\r\n```\r\ndf = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\ndf['string_column'].str.stod()\r\n```\r\n\r\n**Expected behavior**\r\nA columns converted to decimal type.\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: Docker\r\n - Method of cuDF install: Docker\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used\r\n\r\n   - pull: `docker pull rapidsai/rapidsai-nightly:cuda10.2-runtime-ubuntu18.04-py3.7`\r\n   - run: \r\n\r\n```\r\ndocker run --gpus all -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --name kdd_rapids \\\r\n\trapidsai/rapidsai-nightly:cuda10.2-runtime-ubuntu18.04-py3.7\r\n```\r\n\r\n**Environment details**\r\nPlease run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6070/comments",
    "author": "drabastomek",
    "comments": [
      {
        "user": "galipremsagar",
        "created_at": "2020-08-22T17:55:17Z",
        "body": "This behavior was changed with integration of `nvstrings` into `cudf`. We have removed `stod` access via `.str` StringMethods, but instead you can attain the same type-cast by doing `.astype`:\r\n\r\n\r\n```python\r\n>>> import cudf\r\n>>> df = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\n>>> df['string_column'].astype('float64')\r\n0    0.01\r\n1    0.02\r\nName: string_column, dtype: float64\r\n```\r\n\r\nLet us know if this helps?"
      },
      {
        "user": "argenisleon",
        "created_at": "2020-08-22T18:41:31Z",
        "body": "@drabastomek  maybe this could help\r\n```python\r\ndf = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\ncudf.Series(cudf.core.column.string.str_cast.stod(df[\"string_column\"]._column))\r\n```"
      },
      {
        "user": "galipremsagar",
        "created_at": "2020-08-24T02:56:14Z",
        "body": "`.stod` was not exposed via `.str.stod` as we recommend to use `astype` API because, though underlying we call the identical code-path(`stod`) but we have added additional validation if all the string values to be type-casted are capable/valid of being type-casted to float(in this case). "
      }
    ]
  },
  {
    "number": 5830,
    "title": "install error[QST]",
    "created_at": "2020-08-03T12:26:41Z",
    "closed_at": "2020-08-06T05:05:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5830",
    "body": "Dear developer,\r\nThanks for developing nice tool. I would like to install cudf. But when I tried to install cudf with conda, I got following error.\r\ncations were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/linux-64::__cuda==11.0=0\r\n  - feature:|@/linux-64::__cuda==11.0=0\r\n\r\nYour installed CUDA driver is: 11.0\r\n\r\nMy cuder driver version is 450 and nvidia-smi shows cuda version is 11.0. But I installed condatoolkit version 10.1.\r\nSo I think actual cuda version of my env is cuda10.1.\r\nAre there any way to install cudf without downgrading nvidia-drive version?\r\nAny comments a/o suggestions will be greatly appreciated.\r\nThanks in advance.\r\n\r\nTaka",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5830/comments",
    "author": "iwatobipen",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-08-03T14:53:27Z",
        "body": "@iwatobipen those messages related to `__cuda` are a bug in conda and are typically innocuous. Any chance you could share the full output of your conda install/create command to help troubleshoot?"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-03T22:47:59Z",
        "body": "@kkraus14 Thanks for your prompt reply. Here is a full output when I tried to install cudf.\r\n\r\n\r\n$ conda install -c rapidsai cudf=0.13\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: - \r\nFound conflicts! Looking for incompatible packages.\r\nThis can take several minutes.  Press CTRL-C to abort.\r\nfailed                                                                                                                         \r\n\r\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\r\n\r\nOutput in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/linux-64::__cuda==11.0=0\r\n  - feature:|@/linux-64::__cuda==11.0=0\r\n\r\nYour installed CUDA driver is: 11.0\r\n\r\n\r\nAnd list of conda package which has 'cuda' in their name.\r\n\r\n\r\n$ conda list | grep cuda\r\ncudatoolkit               10.1.243             h6bb024c_0    nvidia\r\ncudatoolkit-dev           10.1.243             h516909a_3    conda-forge\r\ncudnn                     7.6.5                cuda10.1_0  \r\nopenmm                    7.4.2           py37_cuda101_rc_1    omnia\r\n\r\n\r\nThanks"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-03T23:57:53Z",
        "body": "Can you dump the full output of `conda list` here?\r\n\r\nDo you have a `.condarc` file that specifies other channels already? If so could you post your channels here as well?"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-04T00:38:16Z",
        "body": "Here is the full list of my env and I don't have a .condarc file now.\r\nThanks\r\n\r\n$ conda list\r\n# packages in environment at /home/iwatobipen/miniconda3/envs/chemoinfo:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\n_py-xgboost-mutex         2.0                       cpu_0  \r\nabsl-py                   0.9.0                    py37_0  \r\nalembic                   1.4.2                      py_0  \r\namberlite                 16.0                     pypi_0    pypi\r\nambertools                17.0                     pypi_0    pypi\r\nambit                     0.3                  h137fa24_1    psi4\r\nappdirs                   1.4.3            py37h28b3542_0  \r\nase                       3.19.2                   pypi_0    pypi\r\nasn1crypto                1.3.0                    py37_1  \r\nattrs                     19.3.0                     py_0  \r\nautograd                  1.3                        py_0    conda-forge\r\nautograd-gamma            0.4.1                      py_0    conda-forge\r\nbackcall                  0.2.0                      py_0  \r\nbcrypt                    3.1.7            py37h7b6447c_1  \r\nblack                     19.10b0                    py_0  \r\nblas                      1.0                         mkl  \r\nbleach                    3.1.5                      py_0  \r\nblosc                     1.19.0               hd408876_0  \r\nbokeh                     2.1.1                    py37_0  \r\nboost-cpp                 1.68.0            h11c811c_1000    conda-forge\r\nbrotlipy                  0.7.0           py37h7b6447c_1000  \r\nbzip2                     1.0.8                h7b6447c_0  \r\nca-certificates           2020.6.24                     0  \r\ncairo                     1.14.12              h8948797_3  \r\ncatch2                    2.11.2               hc9558a2_0    conda-forge\r\ncertifi                   2020.6.20                py37_0  \r\ncffi                      1.14.0           py37he30daa8_1  \r\nchardet                   3.0.4                 py37_1003  \r\nchemps2                   1.8.9                h8c3debe_0    psi4\r\nclang                     10.0.1          default_hde54327_0    conda-forge\r\nclang-tools               10.0.1          default_hde54327_0    conda-forge\r\nclangdev                  10.0.1          default_hde54327_0    conda-forge\r\nclangxx                   10.0.1          default_hde54327_0    conda-forge\r\nclick                     7.1.2                      py_0  \r\ncliff                     3.3.0                      py_0    conda-forge\r\ncloudpickle               1.5.0                      py_0  \r\ncmaes                     0.6.0              pyhbc3b93e_0    conda-forge\r\ncmd2                      0.9.22                   py37_0    conda-forge\r\ncolorama                  0.4.3                      py_0  \r\ncolorlog                  4.2.1                    py37_0  \r\nconfigparser              5.0.0                      py_0  \r\ncryptography              2.9.2            py37h1ba5d50_0  \r\ncudatoolkit               10.1.243             h6bb024c_0    nvidia\r\ncudatoolkit-dev           10.1.243             h516909a_3    conda-forge\r\ncudnn                     7.6.5                cuda10.1_0  \r\ncupy                      7.7.0            py37h0632833_0    conda-forge\r\ncurl                      7.69.1               hbc83047_0  \r\ncycler                    0.10.0                   py37_0  \r\ncython                    0.29.21          py37he6710b0_0  \r\ncytoolz                   0.10.1           py37h7b6447c_0  \r\ndask                      2.20.0                     py_0  \r\ndask-core                 2.20.0                     py_0  \r\ndatabricks-cli            0.9.1                      py_0    conda-forge\r\ndbus                      1.13.16              hb2f20db_0  \r\ndecorator                 4.4.2                      py_0  \r\ndeepdiff                  3.3.0                    py37_1    psi4\r\ndefusedxml                0.6.0                      py_0  \r\ndgl-cu101                 0.4.3.post2              pypi_0    pypi\r\ndgllife                   0.2.4                    pypi_0    pypi\r\ndistributed               2.20.0                   py37_0  \r\ndkh                       1.2                  h173d85e_2    psi4\r\ndocker-py                 4.2.2                    py37_0  \r\ndocker-pycreds            0.4.0                      py_0  \r\nentrypoints               0.3                      py37_0  \r\nexpat                     2.2.9                he6710b0_2  \r\nfastcache                 1.1.0            py37h7b6447c_0  \r\nfastrlock                 0.4              py37he6710b0_0  \r\nfftw3f                    3.3.4                         2    omnia\r\nflake8                    3.8.3                      py_0  \r\nflask                     1.1.2                      py_0  \r\nfontconfig                2.13.0               h9420a91_0  \r\nfpsim2                    0.2.3           py37_1_g29b1e09    efelix\r\nfreetype                  2.10.2               he06d7ca_0    conda-forge\r\nfsspec                    0.7.4                      py_0  \r\nfuture                    0.18.2                   py37_1  \r\ngau2grid                  1.3.1                h035aef0_0    psi4\r\ngdma                      2.2.6                h0e1e685_6    psi4\r\ngitdb                     4.0.5                      py_0  \r\ngitpython                 3.1.3                      py_1  \r\nglib                      2.65.0               h3eb4bd4_0  \r\ngoogledrivedownloader     0.4                      pypi_0    pypi\r\ngorilla                   0.3.0                      py_0    conda-forge\r\ngst-plugins-base          1.14.0               hbbd80ab_1  \r\ngstreamer                 1.14.0               hb31296c_0  \r\ngunicorn                  20.0.4                   py37_0  \r\nh5py                      2.10.0                   pypi_0    pypi\r\nhdf4                      4.2.13               h3ca952b_2  \r\nhdf5                      1.10.2               hba1933b_1  \r\nheapdict                  1.0.1                      py_0  \r\nhyperopt                  0.2.4                    pypi_0    pypi\r\nicu                       58.2                 he6710b0_3  \r\nidna                      2.10                       py_0  \r\nimportlib-metadata        1.7.0                    py37_0  \r\nimportlib_metadata        1.7.0                         0  \r\nintel-openmp              2020.1                      217  \r\nipykernel                 5.3.4            py37h5ca1d4c_0  \r\nipython                   7.16.1           py37h5ca1d4c_0  \r\nipython_genutils          0.2.0                    py37_0  \r\nipywidgets                7.5.1                      py_0  \r\nisodate                   0.6.0                    pypi_0    pypi\r\nisort                     5.0.9                    py37_0  \r\nitsdangerous              1.1.0                    py37_0  \r\njedi                      0.17.1                   py37_0  \r\njinja2                    2.11.2                     py_0  \r\njoblib                    0.16.0                     py_0  \r\njpeg                      9b                   h024ee3a_2  \r\njsonpickle                1.4.1                      py_0  \r\njsonschema                3.2.0                    py37_1  \r\njupyter                   1.0.0                      py_2    conda-forge\r\njupyter_client            6.1.6                      py_0  \r\njupyter_console           6.1.0                      py_0  \r\njupyter_core              4.6.3                    py37_0  \r\nkiwisolver                1.2.0            py37hfd86e86_0  \r\nkrb5                      1.17.1               h173b8e3_0  \r\nlcms2                     2.11                 h396b838_0  \r\nld_impl_linux-64          2.33.1               h53a641e_7  \r\nlibboost                  1.67.0               h46d08c1_4  \r\nlibclang                  10.0.1          default_hde54327_0    conda-forge\r\nlibclang-cpp              10.0.1          default_hde54327_0    conda-forge\r\nlibclang-cpp10            10.0.1          default_hde54327_0    conda-forge\r\nlibcurl                   7.69.1               h20c2e04_0  \r\nlibedit                   3.1.20191231         h14c3975_1  \r\nlibffi                    3.3                  he6710b0_2  \r\nlibgcc-ng                 9.1.0                hdf63c60_0  \r\nlibgfortran-ng            7.3.0                hdf63c60_0  \r\nlibiconv                  1.15              h516909a_1006    conda-forge\r\nlibint                    1.2.1                hb4a4fd4_6    psi4\r\nlibllvm10                 10.0.1               he513fc3_0    conda-forge\r\nlibnetcdf                 4.4.1.1             hfc65e7b_11    conda-forge\r\nlibpng                    1.6.37               hed695b0_1    conda-forge\r\nlibpq                     12.2                 h20c2e04_0  \r\nlibprotobuf               3.12.3               hd408876_0  \r\nlibsodium                 1.0.18               h7b6447c_0  \r\nlibssh2                   1.9.0                h1ba5d50_1  \r\nlibstdcxx-ng              9.1.0                hdf63c60_0  \r\nlibtiff                   4.1.0                h2733197_1  \r\nlibuuid                   1.0.3                h1bed415_2  \r\nlibxc                     4.3.4                h7b6447c_0    psi4\r\nlibxcb                    1.14                 h7b6447c_0  \r\nlibxgboost                1.1.1                he1b5a44_0    conda-forge\r\nlibxml2                   2.9.10               he19cac6_1  \r\nlibxslt                   1.1.34               hc22bd24_0  \r\nlifelines                 0.25.0                     py_0    conda-forge\r\nlightgbm                  2.3.0            py37he6710b0_0  \r\nllvm-tools                10.0.1               he513fc3_0    conda-forge\r\nllvmdev                   10.0.1               he513fc3_0    conda-forge\r\nllvmlite                  0.33.0                   pypi_0    pypi\r\nlocket                    0.2.0                    py37_1  \r\nlz4-c                     1.9.2                he6710b0_1  \r\nlzo                       2.10                 h7b6447c_2  \r\nmako                      1.1.3                      py_0  \r\nmarkupsafe                1.1.1            py37h14c3975_1  \r\nmatplotlib                3.3.0                         1    conda-forge\r\nmatplotlib-base           3.3.0            py37hd478181_1    conda-forge\r\nmccabe                    0.6.1                    py37_1  \r\nmesalib                   18.3.1               h590aaf7_0    conda-forge\r\nmistune                   0.8.4           py37h14c3975_1001  \r\nmkl                       2020.1                      217  \r\nmkl-service               2.3.0            py37he904b0f_0  \r\nmkl_fft                   1.1.0            py37h23d657b_0  \r\nmkl_random                1.1.1            py37h0573a6f_0  \r\nml-metrics                0.1.4                    pypi_0    pypi\r\nmlflow                    1.2.0                      py_1    conda-forge\r\nmmpbsa-py                 16.0                     pypi_0    pypi\r\nmongodb                   4.0.3                h597af5e_0  \r\nmongoengine               0.20.0           py37hc8dfbb8_2    conda-forge\r\nmore-itertools            8.4.0                      py_0  \r\nmsgpack-c                 3.2.0                hc5b1762_0    conda-forge\r\nmsgpack-python            1.0.0            py37hfd86e86_1  \r\nmypy_extensions           0.4.3                    py37_0  \r\nnbconvert                 5.6.1                    py37_1  \r\nnbformat                  5.0.7                      py_0  \r\nnccl                      2.7.8.1              h51cf6c1_0    conda-forge\r\nncurses                   6.2                  he6710b0_1  \r\nnetworkx                  2.4                        py_1  \r\nngboost                   0.2.1              pyh9f0ad1d_0    conda-forge\r\nnotebook                  6.0.3                    py37_0  \r\nnumba                     0.50.1                   pypi_0    pypi\r\nnumexpr                   2.7.1            py37h423224d_0  \r\nnumpy                     1.19.1           py37hbc911f0_0  \r\nnumpy-base                1.19.1           py37hfa32c7d_0  \r\nolefile                   0.46                     py37_0  \r\nopenforcefield            0.7.1+45.g6426b42a          pypi_0    pypi\r\nopenforcefields           1.2.0                    py37_0    omnia\r\nopenmm                    7.4.2           py37_cuda101_rc_1    omnia\r\nopenssl                   1.1.1g               h7b6447c_0  \r\nopenvr                    1.0.17               h6bb024c_1    schrodinger\r\nopt-einsum                3.0.0                      py_0    conda-forge\r\noptuna                    2.0.0                      py_0    conda-forge\r\npackaging                 20.4                       py_0  \r\npackmol-memgen            1.0.5rc0                 pypi_0    pypi\r\npandas                    1.0.5            py37h0573a6f_0  \r\npandoc                    2.10                          0  \r\npandocfilters             1.4.2                    py37_1  \r\nparmed                    3.2.0                    pypi_0    pypi\r\nparso                     0.7.0                      py_0  \r\npartd                     1.1.0                      py_0  \r\npathspec                  0.7.0                      py_0  \r\npatsy                     0.5.1                    py37_0  \r\npbr                       5.4.5                      py_0  \r\npcmsolver                 1.2.1            py37h142c950_0    psi4\r\npcre                      8.44                 he6710b0_0  \r\npdb4amber                 1.7.dev0                 pypi_0    pypi\r\npexpect                   4.8.0                    py37_1  \r\npickleshare               0.7.5                 py37_1001  \r\npillow                    7.2.0            py37hb39fc2d_0  \r\npint                      0.10                       py_0    psi4\r\npip                       20.1.1                   py37_1  \r\npixman                    0.40.0               h7b6447c_0  \r\nplotly                    4.8.2                      py_0  \r\npluggy                    0.13.1                   py37_0  \r\npmw                       2.0.1           py37hc8dfbb8_1002    conda-forge\r\npostgresql                12.2                 h20c2e04_0  \r\nprettytable               0.7.2                      py_3    conda-forge\r\nprometheus_client         0.8.0                      py_0  \r\nprompt-toolkit            3.0.5                      py_0  \r\nprompt_toolkit            3.0.5                         0  \r\nprotobuf                  3.12.3           py37he6710b0_0  \r\npsi4                      1.3.2+ecbda83    py37h31b3128_0    psi4\r\npsutil                    5.7.0            py37h7b6447c_0  \r\npsycopg2                  2.8.5            py37hb09aad4_1    conda-forge\r\nptyprocess                0.6.0                    py37_0  \r\npy                        1.9.0                      py_0  \r\npy-boost                  1.67.0           py37h04863e7_4  \r\npy-cpuinfo                7.0.0                      py_0  \r\npy-xgboost                1.1.1            py37hc8dfbb8_0    conda-forge\r\npy3dmol                   0.8.0                      py_0    conda-forge\r\npychembldb                0.4.1                     dev_0    <develop>\r\npycodestyle               2.6.0                      py_0  \r\npycparser                 2.20                       py_2  \r\npydantic                  1.5.1            py37h7b6447c_0  \r\npyflakes                  2.2.0                      py_0  \r\npygments                  2.6.1                      py_0  \r\npymol                     2.5.0a0                  pypi_0    pypi\r\npymongo                   3.9.0            py37he6710b0_0  \r\npyopenssl                 19.1.0                     py_1  \r\npyparsing                 2.4.7                      py_0  \r\npyperclip                 1.8.0              pyh9f0ad1d_0    conda-forge\r\npyqt                      5.9.2            py37h05f1152_2  \r\npyrsistent                0.16.0           py37h7b6447c_0  \r\npyside2                   5.9.0a1          py37h4dc837a_0    conda-forge\r\npysocks                   1.7.1                    py37_1  \r\npytables                  3.4.4            py37ha205bf6_0  \r\npytest                    5.4.3                    py37_0  \r\npython                    3.7.7                hcff3b4d_5  \r\npython-dateutil           2.8.1                      py_0  \r\npython-editor             1.0.4                      py_0  \r\npython_abi                3.7                     1_cp37m    conda-forge\r\npytraj                    2.0.5                    pypi_0    pypi\r\npytz                      2020.1                     py_0  \r\npyyaml                    5.3.1            py37h7b6447c_1  \r\npyzmq                     19.0.1           py37he6710b0_1  \r\nqcelemental               0.4.2                      py_0    psi4\r\nqcengine                  0.8.2                      py_0    conda-forge\r\nqcfractal                 0.7.2                      py_0    conda-forge\r\nqcportal                  0.7.2                      py_0    conda-forge\r\nqt                        5.9.7                h5867ecd_1  \r\nqtconsole                 4.7.5                      py_0  \r\nqtpy                      1.9.0                      py_0  \r\nquerystring_parser        1.2.4                      py_0    conda-forge\r\nrazi                      0.0.0                    pypi_0    pypi\r\nrdflib                    5.0.0                    pypi_0    pypi\r\nrdkit                     2020.03.3.0      py37hc20afe1_1    rdkit\r\nrdkit-postgresql          2020.03.3.0          h8ea0133_0    rdkit\r\nreadline                  8.0                  h7b6447c_0  \r\nregex                     2020.6.8         py37h7b6447c_0  \r\nrequests                  2.24.0                     py_0  \r\nresp                      0.8.1              pyha93d1a2_0    psi4\r\nretrying                  1.3.3                    py37_2  \r\nsander                    16.0                     pypi_0    pypi\r\nscikit-learn              0.23.1           py37h423224d_0  \r\nscipy                     1.5.0            py37h0b6359f_0  \r\nseaborn                   0.10.1                        1    conda-forge\r\nseaborn-base              0.10.1                     py_1    conda-forge\r\nsend2trash                1.5.0                    py37_0  \r\nsetuptools                49.2.0                   py37_0  \r\nsimint                    0.7                  h642920c_1    psi4\r\nsimplejson                3.17.0           py37h7b6447c_0  \r\nsip                       4.19.8           py37hf484d3e_0  \r\nsix                       1.15.0                     py_0  \r\nsmmap                     3.0.2                      py_0  \r\nsnappy                    1.1.8                he6710b0_0  \r\nsortedcontainers          2.2.2                      py_0  \r\nsqlalchemy                1.3.18           py37h8f50634_0    conda-forge\r\nsqlite                    3.32.3               h62c20be_0  \r\nsqlparse                  0.3.1                      py_0  \r\nstatsmodels               0.11.1           py37h7b6447c_0  \r\nstevedore                 3.2.0            py37hc8dfbb8_0    conda-forge\r\ntabulate                  0.8.3                    py37_0  \r\ntblib                     1.6.0                      py_0  \r\nterminado                 0.8.3                    py37_0  \r\ntestpath                  0.4.4                      py_0  \r\nthreadpoolctl             2.1.0              pyh5ca1d4c_0  \r\ntk                        8.6.10               hbc83047_0  \r\ntoml                      0.10.1                     py_0  \r\ntoolz                     0.10.0                     py_0  \r\ntorch                     1.5.0+cu101              pypi_0    pypi\r\ntorch-cluster             1.5.6                    pypi_0    pypi\r\ntorch-geometric           1.6.0                    pypi_0    pypi\r\ntorch-scatter             2.0.5                    pypi_0    pypi\r\ntorch-sparse              0.6.6                    pypi_0    pypi\r\ntorch-spline-conv         1.2.0                    pypi_0    pypi\r\ntorchvision               0.6.0+cu101              pypi_0    pypi\r\ntornado                   6.0.4            py37h7b6447c_1  \r\ntqdm                      4.48.0                   pypi_0    pypi\r\ntraitlets                 4.3.3                    py37_0  \r\ntyped-ast                 1.4.1            py37h7b6447c_0  \r\ntyping_extensions         3.7.4.2                    py_0  \r\nurllib3                   1.25.9                     py_0  \r\nwcwidth                   0.2.5                      py_0  \r\nwebencodings              0.5.1                    py37_1  \r\nwebsocket-client          0.57.0                   py37_1  \r\nwerkzeug                  1.0.1                      py_0  \r\nwheel                     0.34.2                   py37_0  \r\nwidgetsnbextension        3.5.1                    py37_0  \r\nxfeat                     0.1.0                     dev_0    <develop>\r\nxgboost                   1.1.1            py37h3340039_0    conda-forge\r\nxz                        5.2.5                h7b6447c_0  \r\nyaml                      0.2.5                h7b6447c_0  \r\nzeromq                    4.3.2                he6710b0_2  \r\nzict                      2.0.0                      py_0  \r\nzipp                      3.1.0                      py_0  \r\nzlib                      1.2.11               h7b6447c_3  \r\nzstd                      1.4.5                h9ceee32_0  \r\n\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-04T01:13:48Z",
        "body": "Could you try changing your install command to `conda install -c rapidsai -c nvidia -c conda-forge -c defaults cudf=0.14`?\r\n\r\nFrom the looks of your environment what stands out most to me is you have `pandas 1.0.5` while v0.14 of RAPIDS requires `pandas 0.25.3` as well as having a bunch of pip packages installed which can play havoc on solving / finding dependencies properly.\r\n\r\nIf using a new environment is an option I'd strongly suggest taking that route and using: `conda create -c rapidsai -c nvidia -c conda-forge -c defaults cudf=0.14`"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-05T06:57:06Z",
        "body": "Hopefully, I would like to install cudf in the existing environment but I got following error (it was picked up cudf related).\r\nBut new env is acceptable if it is difficult to solve the conflict.\r\nPackage pyarrow conflicts for:\r\ncudf=0.14 -> pyarrow=0.15.0\r\nqcportal -> pyarrow[version='>=0.13.0']\r\nqcfractal -> qcfractal-core[version='>=0.13.1,<0.13.2.0a0'] -> pyarrow[version='>=0.13.0']\r\n\r\nThanks.\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-05T14:15:52Z",
        "body": "Is there any additional conflicts being shown? That looks solvable with pyarrow 0.15.\r\n"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-06T05:05:03Z",
        "body": "Hi, I tried to remove some packages and install cudf in my env and found that new version of rdkit  202003.03 and rdkit-postgresql cause the issue. When I uninstalled rdkit-postgresql and downgrade rdkit version to 2018, installation was succeeded. \r\nAnd I would like to close the issue.\r\nThank you for taking your time.\r\n"
      }
    ]
  },
  {
    "number": 4893,
    "title": "[QST] I need a \"reduce\" operation",
    "created_at": "2020-04-14T09:46:42Z",
    "closed_at": "2020-04-15T14:22:30Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4893",
    "body": "Hi!\r\n\r\nI have grouped my cudf stream and now I need to reduce values by unique values.\r\n For example, My stream has:\r\na   b\r\n1  1\r\n1  2\r\n2  3\r\n2  4\r\n\r\nI need to get stream with have unique value of column 'a' and result of some function from column 'b'\r\na  f(a)\r\n1  10\r\n2  20\r\n\r\nHow I can do it? Thanks for advice!",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4893/comments",
    "author": "schernolyas",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-04-14T18:31:34Z",
        "body": "@schernolyas it sounds like you're doing a groupby. Can you give more information about what `f(a)` looks like? Do built in groupby aggregations not suffice for your use case?"
      },
      {
        "user": "schernolyas",
        "created_at": "2020-04-14T18:52:58Z",
        "body": "Hi @kkraus14 !\r\nDo you mean that I can decrease count of rows by groupby? I tried groupby without success."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-04-14T18:55:56Z",
        "body": "@schernolyas Yes, in your above example, you could do something like `df.groupby(['a']).sum()` which would return you two rows of `[3, 7]` which is the sum of the `b` column."
      }
    ]
  },
  {
    "number": 4874,
    "title": "[QST] ai.rapids:cudf:cuda10 for CUDA 10.2",
    "created_at": "2020-04-10T10:44:23Z",
    "closed_at": "2020-04-14T09:09:59Z",
    "labels": [
      "question",
      "Java"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4874",
    "body": "Hi!\r\nI need artifact\r\n`<dependency>\r\n        <groupId>ai.rapids</groupId>\r\n        <artifactId>cudf</artifactId>\r\n        <classifier>cuda10-1</classifier>\r\n        <version>0.9.2</version>\r\n    </dependency>`\r\nfor CUDA 10.2. How I can build it?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4874/comments",
    "author": "schernolyas",
    "comments": [
      {
        "user": "jlowe",
        "created_at": "2020-04-10T14:05:54Z",
        "body": "`<classifier>cuda10-1</classifier>` indicates it is built for CUDA 10.1.  The cudf 0.9.2 release does not support CUDA 10.2.  CUDA 10.2 is supported starting with 0.13.\r\n\r\nNote that multiple CUDA runtime versions can be installed on a system because the kernel driver is guaranteed to be backwards compatible.   Therefore the 10.2-compatible driver also supports running applications using the 10.1 runtime.  The easiest solution may be to install the CUDA 10.1 runtime libraries on the system somewhere and update `LD_LIBRARY_PATH` if necessary so the application can find the 10.1 libraries."
      }
    ]
  },
  {
    "number": 2958,
    "title": "cudf flatten API [QST]",
    "created_at": "2019-10-03T20:42:50Z",
    "closed_at": "2019-12-12T20:52:24Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2958",
    "body": "**What is your question?**\r\nDoes CUDF  support an api to flat the dataframe like pd.values.flatten() ? Thanks you.\r\n```python\r\nIn [40]: df_matchColor1                                       \r\nOut[40]: \r\n      color  key_index\r\n0   #ffffd9          0\r\n1   #7fcdbb          1\r\n2   #41b6c4          2\r\n3   #1d91c0          3\r\n4   #ffffd9          4\r\n5   #7fcdbb          5\r\n6   #41b6c4          6\r\n7   #1d91c0          7\r\n8   #ffffd9          8\r\n9   #7fcdbb          9\r\n10  #41b6c4         10\r\n11  #1d91c0         11\r\n\r\nIn [53]: df_matchColor1.to_pandas().values.flatten()                                 \r\nOut[53]: \r\narray(['#ffffd9', 0, '#7fcdbb', 1, '#41b6c4', 2, '#1d91c0', 3, '#ffffd9',\r\n       4, '#7fcdbb', 5, '#41b6c4', 6, '#1d91c0', 7, '#ffffd9', 8,\r\n       '#7fcdbb', 9, '#41b6c4', 10, '#1d91c0', 11], dtype=object)\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2958/comments",
    "author": "MikeChenfu",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-10-10T22:30:10Z",
        "body": "@MikeChenfu in your use case you're utilizing Pandas/Numpy's ability to fall back to using Python objects to created a mixed dtype array. This is extremely detrimental to performance and won't be supported in cuDF for mixed dtypes.\r\n\r\nOtherwise, you should be able to use the same code as `df.values.flatten()` that will go from cuDF --> cuPy --> flattened cuPy array."
      }
    ]
  },
  {
    "number": 2871,
    "title": "[QST] Out of Memory when counting number of rows using multiple parquet fles",
    "created_at": "2019-09-26T05:58:12Z",
    "closed_at": "2019-09-27T20:50:12Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2871",
    "body": "I have 500 parquet files. Each parquet file is SNAPPY compressed and is around 200MB of size. Parquet consists of three columns. Each column is a string.\r\n\r\nI am trying to find the number of rows using the following code:\r\n\r\n```python\r\ncluster = LocalCUDACluster(ip=sched_ip, n_workers=num_of_gpus)\r\nclient = Client(cluster)\r\ndf = dask_cudf.read_parquet(path_to_local, columns=['col1','col2'])\r\nrows, cols = df.shape\r\nnum_rows = rows.compute()\r\n```\r\n\r\nIt throws a Runtime Exception: ```Exception: RuntimeError('parallel_for failed: out of memory')```\r\n\r\nI am using an EC2 instance (p3.8xlarge) with following configuration:\r\n1) **RAM**: 244GB\r\n2) **vCPU**: 32\r\n3) **GPU RAM**: 64GB\r\n4) **GPUs**: 4 Tesla V100\r\n\r\nIs this expected behaviour? Or are there any workarounds?\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2871/comments",
    "author": "chochim",
    "comments": [
      {
        "user": "OlivierNV",
        "created_at": "2019-09-27T00:40:59Z",
        "body": "read_parquet_metadata() is probably what you want (no need to actually read column data)"
      }
    ]
  },
  {
    "number": 1406,
    "title": "[QST] Regarding the usage of CMAKE_CXX11_ABI flag in cudf/cpp/CMakeLists.txt ",
    "created_at": "2019-04-11T12:46:24Z",
    "closed_at": "2019-04-11T18:10:33Z",
    "labels": [
      "question",
      "CMake"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1406",
    "body": "**What is your question?**\r\n1. I can see that  `CMAKE_CXX11_ABI` flag  set by default without explicitly setting it up for `GNU compiler`. \r\n[ without setting `-DCMAKE_CXX11_ABI` in `CMAKE_COMMON_VARIABLES` ] \r\n \r\n``` \r\nif(CMAKE_COMPILER_IS_GNUCXX)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\r\n\r\n    option(CMAKE_CXX11_ABI \"Enable the GLIBCXX11 ABI\" ON)\r\n    if(CMAKE_CXX11_ABI)\r\n        message(STATUS \"CUDF: Enabling the GLIBCXX11 ABI\")\r\n```\r\nBuild log snippet: \r\n``` \r\n-- CUDF: Enabling the GLIBCXX11 ABI\r\n   \r\n-std=c++14 -x cu -c $SRC_DIR/cpp/src/comms/ipc/ipc.cu -o CMakeFiles/cudf.dir/src/comms/ipc/ipc.cu.o\r\n```\r\nDoes that mean for a default case of `GNU compiler` cuDF supports `CXX11_ABI`  and `CMAKE_CXX_STANDARD` and `CMAKE_CUDA_STANDARD` are set to `14` ? \r\n\r\n2.  `set(CMAKE_CXX_STANDARD 14) set(CMAKE_CUDA_STANDARD 14)` are these the mandatory requirement for latest `cuDF` version to build. ? \r\n\r\nThanks in advance for your response ! ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1406/comments",
    "author": "pradghos",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-04-11T17:46:48Z",
        "body": "@pradghos Yes, the default case for `GNU compiler` is to use the new ABI, denoted by `CXX11_ABI`. A user can change back to the old ABI by passing `-DCMAKE_CXX11_ABI=OFF` if they want.\r\n\r\nThe library uses a bunch of C++14 features, so `STANDARD 14` is required and is not an option."
      }
    ]
  }
]