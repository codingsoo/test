[
  {
    "number": 3816,
    "title": "System.ArgumentException: 'Length of memory  must match product of dimensions.",
    "created_at": "2019-06-04T11:37:53Z",
    "closed_at": "2019-06-10T09:47:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/dotnet/machinelearning/issues/3816",
    "body": "- windows 10\r\n- 4.7.NET Version  \r\n- ML dotnet 1.0\r\n- Visual Studio 15.9.12\r\n\r\nIssue:\r\n\r\nI'm quite new to .net, however I'm trying to replicate CRNN model developed on keras to ML dotnet. I successfully converted the model to onnx format. But when I try to make a prediction I'm getting this:\r\n _System.ArgumentException: 'Length of memory (9600) must match product of dimensions (3200).'_\r\nI didn't find any issue or something so that is way I'm writing here.\r\n\r\nI can assume that the problem might be somewhere in image transformation. My model is built for grayscale images with the shape of (1, 1, 32, 100) and there I have this conflict between:\r\n1x32x100 = 3200 (should be)  vs  3x32x100 = 9600 (actually is)\r\n\r\nI've tried to transform images to grayscale, but it doesn't work (perhaps I do it in a wrong way).\r\n\r\nThis is my snipped code for building the pipeline:\r\n\r\n`\r\n\r\n        int imageHeight = 32;\r\n        int imageWidth = 100;\r\n        bool ChannelsLast = false;\r\n        string ModelInput = \"conv2d_1_input_01\";\r\n        string ModelOutput = \"dense_1_add_0\";\r\n\r\n\r\n        var pipeline = mLContext.Transforms.LoadImages(outputColumnName: \"conv2d_1_input_01\",\r\n                                                           imageFolder: imagesLocation,\r\n                                                           inputColumnName: nameof(ImageData.ImagePath))\r\n            .Append(mLContext.Transforms.ResizeImages(outputColumnName: \"conv2d_1_input_01\",\r\n                                                            imageWidth: imageWidth,\r\n                                                            imageHeight: imageHeight))\r\n            .Append(mLContext.Transforms.ConvertToGrayscale(outputColumnName:\r\n                                                            \"conv2d_1_input_01\"))\r\n            .Append(mLContext.Transforms.ExtractPixels(outputColumnName: \"conv2d_1_input_01\", interleavePixelColors: ImageSettings.ChannelsLast))\r\n            .Append(mLContext.Transforms.ApplyOnnxModel(modelFile: modelLocation,\r\n                                                            outputColumnNames: new[] { ModelOutput },\r\n                                                            inputColumnNames: new[] { ModelInput }));\r\n`\r\n\r\nI would appreciate any help or comments.",
    "comments_url": "https://api.github.com/repos/dotnet/machinelearning/issues/3816/comments",
    "author": "cemicel",
    "comments": [
      {
        "user": "cemicel",
        "created_at": "2019-06-06T12:36:45Z",
        "body": "I was able to omit the problem by changing model's input shape to consume 3-channel images.\r\nIt is not the solution."
      },
      {
        "user": "yaeldekel",
        "created_at": "2019-06-07T20:08:51Z",
        "body": "Hi @cemicel, thank you for your question. The `ConvetToGrayscale` transform converts the image to gray scale, but it keeps all the channels (alpha, R, G and B). the `ExtractPixels` transform has an option called `colorsToExtract`, where you can specify that you would only like to extract the alpha channel:\r\n```\r\nmLContext.Transforms.ExtractPixels(outputColumnName: \"conv2d_1_input_01\", colorsToExtract: ImagePixelExtractingEstimator.ColorBits.Alpha)\r\n```\r\n"
      }
    ]
  }
]