[
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/76040",
    "source": {
      "issue_number": 76040
    },
    "initial_question": {
      "title": "How to configure to stop insert data into local table in specific machine with cluster table scenario?",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nIn such situation, a clickhouse cluster with 4 machines: ck0, ck1, ck2, ck3,\nand local table test_tb in all 4 machines, and corresponding cluster table test_tb_cluster in ck0.\nSelect test_tb_cluster table data in ck0 will query every test_tb table data from all 4 machines.\nWhat I want is store table test_tb data in just ck1, ck2, ck3 these 3 machines, not store in ck0(reduct insert stress and select stress in this machine, just use for insert&select task distribution), \nMust create table test_tb in ck0, right? Otherwise operate test_tb_cluster will raise error: \nThere is no table `dbxxx`.`test_tb ` on server: ck0:9000\n\nSo how can I config table test_tb in ck0 to make it just as a empty local table? \nInsert data into test_tb_cluster will never choice test_tb in ck0 to store."
    },
    "satisfaction_conditions": [
      "Data must be distributed only across ck1, ck2, and ck3 nodes, with no data stored in ck0",
      "The local table test_tb must exist on ck0 to prevent query errors",
      "Queries through test_tb_cluster on ck0 must successfully retrieve data from all other nodes",
      "Insert operations through test_tb_cluster must distribute data only to ck1, ck2, and ck3",
      "The solution must maintain cluster functionality without modifying existing table structures"
    ],
    "created_at": "2025-02-13T09:03:51Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/74005",
    "source": {
      "issue_number": 74005
    },
    "initial_question": {
      "title": "A lot of folders with the prefix clone_ in the detached directory under the ClickHouse local table folder.",
      "body": "### Company or project name\n\ncompany\uff1acoohom\uff1b\r\nUse clickhouse to store monitoring logs\uff1b\n\n### Question\n\nThanks for your work\uff01\r\n\r\nThis is table structure:\r\n````sql\r\nCREATE TABLE monitor.qunhe_log\r\n(\r\n    `timestamp` DateTime64(3, 'Asia/Shanghai'),\r\n    `hostGroup` String,\r\n    `ip` String,\r\n    `podname` String,\r\n    `level` String,\r\n    `cls` String,\r\n    `behavior` String,\r\n    `message` String,\r\n    `id` String DEFAULT '',\r\n    INDEX message_index message TYPE tokenbf_v1(30720, 3, 0) GRANULARITY 1\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/qunhe_log_20240112_v1', '{replica}')\r\nPARTITION BY toYYYYMMDD(timestamp)\r\nORDER BY (level, hostGroup, podname, ip, timestamp)\r\nSETTINGS storage_policy = 'hot_and_cold', index_granularity = 8192 \r\n\r\n1 row in set. Elapsed: 0.002 sec. \r\n````\r\n\r\nBut has a lot of folders with the prefix clone_ in the detached directory under the ClickHouse local table folder. There is no operation about ```ALTER TABLE ... FREEZE```\r\n````shell\r\n[root@10 detached]# pwd\r\n/data-ssd/clickhouse-data/store/815/815edd8b-ced9-4459-be8b-ec82e7b95cb0/detached\r\n[root@10 detached]# ls -l\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5406_5412_1\r\n......\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5416_5421_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5422_5422_0\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5423_5452_2\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5471_5478_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5483_5491_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5638_5643_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5644_5649_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5801_5801_0\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5878_5882_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5893_5898_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_5969_5976_1\r\ndrwxr-x--- 2 101 101 4096 Dec 31 06:52 clone_20241231_6070_6070_0\r\n[root@10 detached]# ls -l|grep \"clone_\"|wc -l\r\n1184\r\n[root@10 detached]# \r\n````\r\nThese folders will take up a lot of disk space. I don't know under what circumstances such folders will be created. Is there any readme text to explain it? Thanks"
    },
    "satisfaction_conditions": [
      "Clone directories in detached folder are explained and understood",
      "Disk space management approach is identified",
      "System behavior is confirmed as normal",
      "Related system configuration is identified"
    ],
    "created_at": "2024-12-31T06:17:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/73873",
    "source": {
      "issue_number": 73873
    },
    "initial_question": {
      "title": "Upgrade to 24.12.1.1614",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nI want to upgrade ClickHouse (version: 24.5.4.49) to 24.12.1.1614, but I noticed that the structure of configuration items like config.xml has changed. Originally, it was:\r\n```\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n   Configuration content\r\n</yandex>\r\n```\r\nNow, it is:\r\n```\r\n<clickhouse>\r\n   Configuration content\r\n</clickhouse>\r\n```\r\n1.Do I need to modify all configuration files to the new structure now? This seems like a very risky operation.\r\n2. Additionally, I need to replace Zookeeper   with Keeper. Is it sufficient to only edit metrika.xml? The Keeper cluster has already been deployed.\r\nContent of metrika.xml:\r\n```\r\n<yandex>\r\n    <clickhouse_remote_servers>\r\n        <cluster_1s_1r>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>server01</host>\r\n                    <port>9000</port>\r\n                    <user>default</user>\r\n                    <password>_CLUster0369</password>\r\n                </replica>\r\n            </shard>\r\n          </cluster_1s_1r>\r\n    </clickhouse_remote_servers>\r\n\r\n    <zookeeper-servers>\r\n        <node index=\"1\">\r\n            <host>172.16.13.11</host>\r\n            <port>2181</port>\r\n        </node>\r\n\t\t<node index=\"2\">\r\n            <host>172.16.13.12</host>\r\n            <port>2181</port>\r\n        </node>\r\n\t\t<node index=\"3\">\r\n            <host>172.16.13.13</host>\r\n            <port>2181</port>\r\n        </node>\r\n    </zookeeper-servers>\r\n\r\n    <macros>\r\n        <layer>01</layer>\r\n        <shard>01</shard>\r\n        <replica>cluster01-01-1</replica>\r\n    </macros>\r\n    <networks>\r\n        <ip>::/0</ip>\r\n    </networks>\r\n\r\n    <clickhouse_compression>\r\n        <case>\r\n            <min_part_size>10000000000</min_part_size>\r\n            <min_part_size_ratio>0.01</min_part_size_ratio>\r\n            <method>lz4</method>\r\n        </case>\r\n    </clickhouse_compression>\r\n</yandex>\r\n```"
    },
    "satisfaction_conditions": [
      "Configuration files remain functional during and after upgrade",
      "Configuration structure follows current recommended practices",
      "All essential configuration elements are preserved",
      "Configuration changes can be implemented incrementally"
    ],
    "created_at": "2024-12-27T08:28:57Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/71394",
    "source": {
      "issue_number": 71394
    },
    "initial_question": {
      "title": "How to force ClickHouse to return column names prefixed with table names when INNER JOIN?",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nFor example:\r\n\r\nCREATE TABLE ta\r\n(\r\n    `a` UInt32,\r\n    `id` UInt32\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\n\r\nCREATE TABLE tb\r\n(\r\n    `b` UInt32,\r\n    `id` UInt32\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\n\r\ninsert into ta(*) VALUES (0,1)\r\ninsert into tb(*) VALUES (1,1)\r\n\r\nWITH\r\n    a AS\r\n    (\r\n        SELECT\r\n            a,\r\n            id\r\n        FROM ta\r\n    ),\r\n    b AS\r\n    (\r\n        SELECT\r\n            b,\r\n            id\r\n        FROM tb\r\n    )\r\nSELECT *\r\nFROM a\r\nINNER JOIN b ON a.id = b.id\r\n\r\nThis returns\r\n\u250c\u2500a\u2500\u252c\u2500id\u2500\u252c\u2500b\u2500\u252c\u2500b.id\u2500\u2510\r\n\u2502 0     \u2502  1    \u2502 1     \u2502    1 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nHow to force the returned qualified column names, i.e. a.a, a.id, b.b and b.id?"
    },
    "satisfaction_conditions": [
      "Query results display column names with table name prefixes",
      "All columns from the original JOIN query remain present in results",
      "Column name prefixes match their respective source tables/CTEs",
      "Query continues to return the same data values"
    ],
    "created_at": "2024-11-01T21:45:03Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/70599",
    "source": {
      "issue_number": 70599
    },
    "initial_question": {
      "title": "ClickHouse distributed JOIN vs common JOIN",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\n1. Set up\r\n\r\nClickHouse cluster with 2 shards, 1 replica on each shard\r\nLocal table testjoin on each replica\r\n```\r\nCREATE TABLE testjoin\r\n(\r\n    `user` String,\r\n    `type` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/testjoin', '{replica}')\r\nPARTITION BY type\r\nORDER BY type\r\n```\r\nDistributed table testall\r\n```\r\nCREATE TABLE testall\r\n(\r\n    `user` String,\r\n    `type` String\r\n)\r\nENGINE = Distributed('cluster', 'default', 'testjoin', rand())\r\n```\r\nShard 1 Replica 1 has the following data\r\nSELECT *\r\nFROM testjoin\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u2510\r\n\u2502 user1     \u2502 type1     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nShard 2 Replica 1 has the following data\r\nSELECT *\r\nFROM testjoin\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u2510\r\n\u2502 user1     \u2502 type2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2. Query with INNER JOIN and GLOBAL INNER JOIN yields the same result. distributed_product_mode = 'allow' or  'local' or 'deny' makes no difference either. Is this expected behavior?\r\n\r\nWITH\r\n    t1 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type1'\r\n    ),\r\n    t2 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type2'\r\n    )\r\nSELECT *\r\nFROM t1\r\nINNER JOIN t2 ON t1.user = t2.user\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nWITH\r\n    t1 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type1'\r\n    ),\r\n    t2 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type2'\r\n    )\r\nSELECT *\r\nFROM t1\r\nGLOBAL INNER JOIN t2 ON t1.user = t2.user\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n"
    },
    "satisfaction_conditions": [
      "JOIN queries with subqueries produce identical results regardless of JOIN type (regular vs GLOBAL)",
      "JOIN operations with subqueries are executed on the initiating node",
      "distributed_product_mode setting has no effect on subquery JOIN results",
      "Results are consistent whether using CTEs (WITH clause) or inline subqueries"
    ],
    "created_at": "2024-10-12T19:24:28Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/70548",
    "source": {
      "issue_number": 70548
    },
    "initial_question": {
      "title": "To upgrade Clickhouse from 22.x to 24.x",
      "body": "### Company or project name\n\nTimePlay \n\n### Question\n\nI set up a test Clickhouse server running version 24.8.4.13 and noticed the data layout in the S3 bucket is different from that of  Clickhouse server running version 22.8.6.71.\r\n\r\nEssentially, with Clickhouse version 22.8.6.71, the S3 bucket contains object keys without  a prefix, for example,  \"aaapkcyerlxwvoeuyqfeqasxbsxpkdtq\", while with version 24.8.4.13, the object keys have a prefix, for example, \"abq/prlqajkzrohcpycwzbabdnnoehhls\"\r\n\r\nI wonder if this discrepancy in object key naming would cause any issues if I am to upgrade a  Clickhouse cluster from version 22.8.6.71 to 24.8.4.13.\r\n\r\nThank you."
    },
    "satisfaction_conditions": [
      "Data accessibility must be maintained across versions",
      "Metadata file references must resolve correctly",
      "Storage functionality must continue uninterrupted",
      "Backward compatibility must be preserved"
    ],
    "created_at": "2024-10-10T05:11:39Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/69232",
    "source": {
      "issue_number": 69232
    },
    "initial_question": {
      "title": "s3 table function fetch the whole parquet instead of metadata for count when it's a small file",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\ni observed an issue with select count(*) from s3() reading parquet files that are small (900-1000KB)\r\nwhen i check how much byte it read\r\nit shows ReadBufferFromS3Bytes = sum of size of all files\r\nbut if i merge 2 files together and size is about 1.8MB\r\n```\r\nINSERT INTO FUNCTION s3(gcs_login, filename = 'clickhouse/test_merged.parquet') SELECT *\r\nFROM s3(gcs_login, filename = 'clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=..../time=..../{0,1}.parquet', format = 'Parquet')\r\n```\r\nand try again, it only fech 64KB which i think it's the metadata\r\n\r\n\r\n```\r\nSELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('85c68382-d77f-4f23-929b-27752199a873', '209b6fd3-7dd8-432b-9c91-d7adbb9efd6d')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC;\r\n\r\nQuery id: 6f912e2d-ded3-4a29-ba2f-004bff20ff47\r\n\r\n   \u250c\u2500query\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500bytes\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 select count(*) from s3(gcs_login,filename='clickhouse/test_merged.parquet', format='Parquet');                                                                                        \u2502 64.00 KiB \u2502\r\n2. \u2502 select count(*) from s3(gcs_login,filename='clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=..../time=..../*', format='Parquet'); \u2502 66.17 MiB \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2 rows in set. Elapsed: 0.005 sec.\r\n```\r\n\r\nis there any threshold that decide to fetch the whole file or just the metadata? can i force clickhouse to only read the metadata? we need to run qa validation script on few TB data to monitor the count and most of them are small files (<1.5MB) so it only make sense if we can just fetch the metadata.\r\n\r\nThanks"
    },
    "satisfaction_conditions": [
      "Count query only reads parquet metadata instead of entire file",
      "Solution maintains consistent behavior across different file sizes",
      "Data accuracy is preserved",
      "Minimal S3/GCS data transfer"
    ],
    "created_at": "2024-09-03T19:37:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/68596",
    "source": {
      "issue_number": 68596
    },
    "initial_question": {
      "title": "Docker Image mount point for data not properly documented?",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nAccording to the docker image documentation, I have to mount\r\n\r\n`/var/lib/clickhouse/ - main folder where ClickHouse stores the data`\r\n\r\nfor persistence. This doesnt work. After a restart, the data is gone.\r\n\r\nInspecting the container, I can find the `/data` folder with the expected data, but contents are symlinked to `/store`.\r\n\r\nSo from my understanding, `/data`, `/store` and `/var/lib/clickhouse` have to be mounted somehow?\r\n\r\nIs that correct? I would like to get some background about this, as when I start clickhouse server locally, everything is being stored in the current folder."
    },
    "satisfaction_conditions": [
      "Data persists across container restarts",
      "Correct storage path configuration is established",
      "Configuration file is properly mounted and accessible",
      "Storage directory structure matches ClickHouse's expectations"
    ],
    "created_at": "2024-08-20T09:42:44Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/67976",
    "source": {
      "issue_number": 67976
    },
    "initial_question": {
      "title": "When the clickhouse distributed table query, why only read very little data from the node\uff0cHow did this do\uff1f",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nAfter testing, I found that when the distributed table query data of Clickhouse is not simply, it is not simply to gather all data from other nodes to nodes that initiate requests.\r\n\r\nIn fact, there are very few data read from other nodes\uff0cBut I did not find related articles to explain its principle or algorithm.\r\n\r\nDo you have related articles or blogs?"
    },
    "satisfaction_conditions": [
      "Query results must be accurate when data is distributed across multiple nodes",
      "Network data transfer between nodes must be optimized",
      "Results must maintain correct ordering and limit constraints",
      "Query processing must be distributed across nodes",
      "Final results must be correctly aggregated at a single point"
    ],
    "created_at": "2024-08-07T11:39:03Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/67878",
    "source": {
      "issue_number": 67878
    },
    "initial_question": {
      "title": "Compression and uncompression of data flowing over a network",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nI want to copy data between clickhouses using the REMOTE function.\r\nWill the data flow over the network in compressed or uncompressed form?"
    },
    "satisfaction_conditions": [
      "Network data compression must be configurable",
      "Default compression behavior must be defined",
      "Compression settings must affect network transfer performance",
      "Compression configuration must be adjustable at runtime",
      "Performance metrics must be available"
    ],
    "created_at": "2024-08-06T04:43:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/65392",
    "source": {
      "issue_number": 65392
    },
    "initial_question": {
      "title": "Restrict a clickhouse user with limited permission to execute queries on system database",
      "body": "### Company or project name\n\nSelf\n\n### Question\n\nQuestion : How to restrict a clickhouse user with limited permission to execute queries on tables under system database\r\n\r\nSteps:\r\n1) Create a clickhouse db : \r\n    CREATE DATABASE IF NOT EXISTS <db> ON CLUSTER <cluster>\r\n2) Create a clickhouse db user with limited permission :  \r\n    CREATE USER IF NOT EXISTS <username> ON CLUSTER <cluster> IDENTIFIED BY '$password'\r\n    GRANT SELECT ON <database>.* TO <username>\r\n\r\nWith above configuration, the user is still table to query data from tables under system database e.g. select version() , select * from system.settings\r\n    "
    },
    "satisfaction_conditions": [
      "Core system functionality must remain accessible",
      "User permissions must be limited to specified database",
      "Security sensitive system information must remain protected",
      "User authentication must be maintained"
    ],
    "created_at": "2024-06-18T13:23:40Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/65199",
    "source": {
      "issue_number": 65199
    },
    "initial_question": {
      "title": "Temporary stopping automatic dictionary updates",
      "body": "### Company or project name\n\n_No response_\n\n### Question\n\nWhen a dictionary is based on a clickhouse table with a lifetime, what should be done when updating the source table (truncate table source_table; insert into source_table...) to prevent clickhouse from updating the dictionary with partial or no data?"
    },
    "satisfaction_conditions": [
      "Dictionary data remains consistent during source table updates",
      "No interruption to dictionary service with invalid/incomplete data",
      "Source table update process completes atomically"
    ],
    "created_at": "2024-06-13T10:23:46Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/63100",
    "source": {
      "issue_number": 63100
    },
    "initial_question": {
      "title": "Not executing fetch of part xxx because 8 fetches already executing, max 8",
      "body": "My ch version\r\n\r\nClickHouse client version 23.8.8.20 (official build).\r\n\r\nI have a lot of queues \uff0cI want to set background_fetches_pool_size = 32 but not work/\r\n\r\n```python\r\nSELECT\r\n    database,\r\n    table,\r\n    type,\r\n    max(last_exception),\r\n    max(postpone_reason),\r\n    min(create_time),\r\n    max(last_attempt_time),\r\n    max(last_postpone_time),\r\n    max(num_postponed) AS max_postponed,\r\n    max(num_tries) AS max_tries,\r\n    min(num_tries) AS min_tries,\r\n    countIf(last_exception != '') AS count_err,\r\n    countIf(num_postponed > 0) AS count_postponed,\r\n    countIf(is_currently_executing) AS count_executing,\r\n    count() AS count_all\r\nFROM system.replication_queue\r\nGROUP BY\r\n    database,\r\n    table,\r\n    type\r\nORDER BY count_all DESC\r\n\r\nQuery id: 345b6e7c-e993-4227-bc60-939ac2ee23a7\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u252c\u2500max(last_exception)\u2500\u252c\u2500max(postpone_reason)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500min(create_time)\u2500\u252c\u2500max(last_attempt_time)\u2500\u252c\u2500max(last_postpone_time)\u2500\u252c\u2500max_postponed\u2500\u252c\u2500max_tries\u2500\u252c\u2500min_tries\u2500\u252c\u2500count_err\u2500\u252c\u2500count_postponed\u2500\u252c\u2500count_executing\u2500\u252c\u2500count_all\u2500\u2510\r\n\u2502 xxx    \u2502 xxx \u2502 GET_PART \u2502                     \u2502 Not executing fetch of part ff8d5acf92437a06b529a9152e275fbc_4379_4379_0 because 8 fetches already executing, max 8. \u2502 2024-04-27 22:51:46 \u2502    2024-04-29 12:15:05 \u2502     2024-04-29 12:15:08 \u2502          2221 \u2502         1 \u2502         0 \u2502         0 \u2502          673114 \u2502               1 \u2502    673114 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT\r\n    type,\r\n    count(*)\r\nFROM system.replication_queue\r\nGROUP BY type\r\n\r\nQuery id: 0b339b1e-323d-4069-b2a8-8fc8222c65b3\r\n\r\n\u250c\u2500type\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 GET_PART \u2502  672841 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE name IN ('background_fetches_pool_size', 'background_schedule_pool_size', 'background_pool_size')\r\n\r\nQuery id: 5136cca9-d3e9-4682-9125-3a9c6628a240\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2510\r\n\u2502 background_pool_size          \u2502 16    \u2502\r\n\u2502 **\r\n\r\n> **background_fetches_pool_size**\r\n\r\n**  \u2502 16    \u2502\r\n\u2502 background_schedule_pool_size \u2502 128   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n``` \r\n\r\n"
    },
    "satisfaction_conditions": [
      "The number of concurrent fetches must not exceed the configured maximum"
    ],
    "created_at": "2024-04-29T04:17:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/59780",
    "source": {
      "issue_number": 59780
    },
    "initial_question": {
      "title": "[Question]: Working with File with Native format, how to create an empty file?",
      "body": "In CSVWithNames it's simple, just need to add headers and put the file in the `/data/default/[table]/data.CSVWithNames` after that I could select it without any problem:\r\n\r\n```bash\r\n# 1. Set up the file_engine_table table:\r\nCREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(CSVWithNames);\r\n\r\n# 2. Manually create /var/lib/clickhouse/data/default/file_engine_table/data.CSVWithNames containing:\r\n$ cat data.CSVWithNames \r\nname,value\r\n\r\n# 3. Query the data:\r\nSELECT *\r\nFROM file_engine_table\r\n\r\nQuery id: 5a865226-6674-4190-a0a6-2d606f823f17\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.008 sec.\r\n```\r\n\r\nBut with `Native` I don't know if it's ok to just touch the file ( 0 bytes )."
    },
    "satisfaction_conditions": [
      "Empty dataset representation is valid and queryable",
      "Table structure is properly specified",
      "Data format follows Native format specification",
      "Queries execute without errors",
      "Column information is retrievable"
    ],
    "created_at": "2024-02-08T20:37:35Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/58932",
    "source": {
      "issue_number": 58932
    },
    "initial_question": {
      "title": "Does the Kafka engine automatically decompress messages when LZ4 compression is used?",
      "body": "Does the Kafka engine automatically decompress messages when LZ4 compression is used? I am currently using Azure Event Hubs and hoping to reduce data processing fees."
    },
    "satisfaction_conditions": [
      "Messages compressed with LZ4 are successfully decompressed when received",
      "No additional decompression configuration is required",
      "Decompression works with messages from external sources"
    ],
    "created_at": "2024-01-18T02:03:41Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/58030",
    "source": {
      "issue_number": 58030
    },
    "initial_question": {
      "title": "distributed table can't query in cluster dicovery mode",
      "body": "(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\nI trying to deploy clickhouse in cluster dicovery mode, but it seems the data can't be query out.\r\n\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use -- 23.4\r\n\r\nI deploy test cluster by docker-compose, manifest is like below. In summary:\r\n - Cluster contains three node: chnode1-3, which are organized in cluster-discovery mode\r\n - create local table and distributed table on cluster\r\n - after insert a single row into local table in chnode-1, expect to query that out in chnode2 and chnode3 by distributed table. But got nothing.\r\n - What's more, I can get data when I using manual-config to deploy cluster.\r\n\r\n## Test env\r\n### common part\r\n#### docker-compose.yml\r\n```yaml\r\nversion: '3.3'\r\nservices:\r\n  zookeeper:\r\n    image: zookeeper:3.4.9\r\n    container_name: ch-zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 500M\r\n    environment:\r\n      - TZ=Asia/Shanghai\r\n      - ZOO_MY_ID=1\r\n  chnode1:\r\n    image: clickhouse/clickhouse-server:23.4\r\n    container_name: chnode1\r\n    depends_on:\r\n      - zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 1G\r\n    volumes:\r\n      - type: 'bind'\r\n        source: './config/remote-servers.xml'\r\n        target: '/etc/clickhouse-server/config.d/remote-servers.xml'\r\n      - type: 'bind'\r\n        source: './config/use-keeper.xml'\r\n        target: '/etc/clickhouse-server/config.d/use-keeper.xml'\r\n  chnode2:\r\n    image: clickhouse/clickhouse-server:23.4\r\n    container_name: chnode2\r\n    depends_on:\r\n      - zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 1G\r\n    volumes:\r\n      - type: 'bind'\r\n        source: './config/remote-servers.xml'\r\n        target: '/etc/clickhouse-server/config.d/remote-servers.xml'\r\n      - type: 'bind'\r\n        source: './config/use-keeper.xml'\r\n        target: '/etc/clickhouse-server/config.d/use-keeper.xml'\r\n  chnode3:\r\n    image: clickhouse/clickhouse-server:23.4\r\n    container_name: chnode3\r\n    depends_on:\r\n      - zookeeper\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"1\"\r\n          memory: 1G\r\n    volumes:\r\n      - type: 'bind'\r\n        source: './config/remote-servers.xml'\r\n        target: '/etc/clickhouse-server/config.d/remote-servers.xml'\r\n      - type: 'bind'\r\n        source: './config/use-keeper.xml'\r\n        target: '/etc/clickhouse-server/config.d/use-keeper.xml'\r\n  ```\r\n#### ./config/use-keeper.xml\r\n```xml\r\n<clickhouse>\r\n    <zookeeper>\r\n        <node index=\"1\">\r\n            <host>ch-zookeeper</host>\r\n            <port>2181</port>\r\n        </node>\r\n    </zookeeper>\r\n</clickhouse>\r\n```\r\n\r\n### deploy with cluster discovery\r\n#### ./config/remote-servers.xml\r\n```xml\r\n<clickhouse>\r\n    <allow_experimental_cluster_discovery>1</allow_experimental_cluster_discovery>\r\n    <remote_servers replace=\"true\">\r\n        <cluster_NS_1R>\r\n            <discovery>\r\n                <internal_replication>false</internal_replication>\r\n                <path>/clickhouse/discovery/cluster_NS_1R</path>\r\n                <shard>1</shard>\r\n            </discovery>\r\n        </cluster_NS_1R>\r\n    </remote_servers>\r\n</clickhouse>\r\n```\r\n\r\n### deploy with manual cluster setting\r\n#### ./config/remote-servers.xml\r\n```xml\r\n<clickhouse>\r\n    <remote_servers replace=\"true\">\r\n        <cluster_NS_1R>\r\n            <secret>mysecretphrase</secret>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>chnode1</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>chnode2</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n            <shard>\r\n                <internal_replication>false</internal_replication>\r\n                <replica>\r\n                    <host>chnode3</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n        </cluster_NS_1R>\r\n    </remote_servers>\r\n</clickhouse>\r\n```\r\n\r\n## Test to query from distributed table after insert into local table\r\n```\r\n-- on chnode1\r\nCREATE DATABASE db1 ON CLUSTER cluster_NS_1R;\r\n\r\nCREATE TABLE db1.table1 ON CLUSTER cluster_NS_1R\r\n(\r\n    `id` UInt64,\r\n    `column1` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nCREATE TABLE db1.table1_dist ON CLUSTER cluster_NS_1R\r\n(\r\n    `id` UInt64,\r\n    `column1` String\r\n)\r\nENGINE = Distributed('cluster_NS_1R', 'db1', 'table1', rand());\r\n\r\nINSERT INTO db1.table1 (id, column1) VALUES (3, 'abc');\r\n\r\nselect * from db1.table1_dist; -- return one row, is ok\r\n\r\n\r\n-- on chnode2 or chnode3\r\nselect * from db1.table1_dist; -- return none in cluster-discover mode, but ok in manual-config\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Cluster discovery configuration properly identifies and connects all nodes"
    ],
    "created_at": "2023-12-19T12:27:07Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/56454",
    "source": {
      "issue_number": 56454
    },
    "initial_question": {
      "title": "distributed engine inserts exceed memory, even if there is no limit set",
      "body": "Creating new ticket as #50744 is closed and issue is not resolved.\r\n\r\nBackground inserts into distributed tables started throwing exception:\r\n DB::Exception: Memory limit (for query) exceeded: would use 9.31 GiB (attempt to allocate chunk of 4360448 bytes), maximum: 9.31 GiB\r\n\r\nEven if i run SYSTEM FLUSH DISTRIBUTED ON CLUSTER cluster default.table, i get the same error.\r\n\r\nInserts on local node work ok. It also works ok with insert_distributed_sync=1. But as i would prefer to use async, i would like to go back to background inserts.\r\n\r\nMemory limits are the same on all nodes:\r\n```\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default\u2500\u252c\u2500value\u2500\u2510\r\n\u2502 max_memory_usage                 \u2502 0       \u2502 0     \u2502\r\n\u2502 max_memory_usage_for_user        \u2502 0       \u2502 0     \u2502\r\n\u2502 max_memory_usage_for_all_queries \u2502 0       \u2502 0     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nsystem.distribution_queue has 2 entries(1 for each node it is trying to insert to). \r\ndata_compressed_bytes: 9692170902\r\n\r\nEach shard has a queue of around 13k files, ~10G in size. Even if i leave just 1 file in the queue, it still throws memory exceeded.\r\nIf i remove the first file, i get file not found exception.\r\n\r\nHow do i tell clickhouse to not use 10G memory limit? \r\n\r\n"
    },
    "satisfaction_conditions": [
      "Distributed table must successfully process queued data without memory errors",
      "Existing queued data must be recoverable or processable",
      "Table functionality must be restored for future operations",
      "Original data integrity must be maintained",
      "Settings from original insert operations must be addressed"
    ],
    "created_at": "2023-11-08T10:02:42Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/56330",
    "source": {
      "issue_number": 56330
    },
    "initial_question": {
      "title": "Delete query timeout on table that is being optimized atm",
      "body": "Hi there.\r\nI have java service that is making frequent inserts in the specific table based on ReplacingMergeTree engine. Target table has around 470 millions rows.\r\nDue to the business specific requirements i also have to use delete query following each insert query.\r\nOn top of that i'm using scheduled optimize on daily basis at 5.20 am.\r\n\r\nThe problem i encountered is following:\r\n- due to the table size optimize process takes some significant time meanwhile delete queries execution time also increases drastically. Some times it causes my service to shut down with connection exception: **Connection is not available, request timed out after 30000ms**. There are no exceptions in clickhouse logs when it happens.\r\n\r\nQuestions are:\r\n- Are there any table locks during optimize process that are slowing down delete queries? May be it is a stupid question, but i see no difference in performance of insert queries during optimize process.\r\n- Are there any workarounds to solve that issue except increasing max timeout?"
    },
    "satisfaction_conditions": [
      "Table data consistency must be maintained"
    ],
    "created_at": "2023-11-04T10:18:46Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/55286",
    "source": {
      "issue_number": 55286
    },
    "initial_question": {
      "title": "Internal_replication is set to true or false for replicated database engine.",
      "body": "Let's assume I have 3 shards and 2 replicas of clickhouse cluster and also zookeeper with 3 znodes for replication in my eks cluster. In that, I created a database called \"app\" with the replicated engine in all shards and replicas (i.e., in all 6 pods). After I created a mergeTree table called \"num_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"num\" for inserting and retrieving the data evenly in shards. If I insert some data into the \"num\" table ( which is a distributed table), the data will distributed in all shards and data will be replicated into the replica respectively. In this case, do I need to set the internal_replication as true or false? Which one will be best case for above scenario?"
    },
    "satisfaction_conditions": [
      "Data consistency is maintained across all replicas",
      "Data distribution works correctly across shards",
      "Replication configuration prevents duplicate data replication",
      "Cluster configuration aligns with the replicated database engine setup"
    ],
    "created_at": "2023-10-07T08:18:43Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/54875",
    "source": {
      "issue_number": 54875
    },
    "initial_question": {
      "title": "Cannot convert string 1999-1-1 00:00:00 to type DateTime64(3)",
      "body": "Hello, I changed `date_time_input_format` to best_effort.\r\n\r\nSchema\r\n\r\n```\r\nCREATE TABLE default.datetime64_test\r\n(\r\n    `occur_time` DateTime64(3),\r\n)\r\nENGINE = MergeTree\r\nORDER BY occur_time\r\nSETTINGS index_granularity = 8192;\r\n```\r\n\r\nInserting data:\r\n\r\n```\r\ninsert into default.datetime64_test(occur_time) values ('1999-1-1 00:00:00');\r\n```\r\n\r\nThe failing query :\r\n\r\n```\r\nselect * from default.datetime64_testwhere occur_time='1999-1-1 00:00:00';\r\n````\r\n\r\nCode: 53. DB::Exception: Received from localhost:29010. DB::Exception: Cannot convert string 1999-1-1 00:00:00 to type DateTime64(3): while executing 'FUNCTION equals(occur_time : 0, '1999-1-1 00:00:00' : 1) -> equals(occur_time, '1999-1-1 00:00:00') UInt8 : 2'. (TYPE_MISMATCH)\r\n\r\nExpected result - no error (1 row match)\r\n\r\n\r\nclickhouse version 23.3.14.1\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Query preserves millisecond precision support"
    ],
    "created_at": "2023-09-21T10:17:44Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/54127",
    "source": {
      "issue_number": 54127
    },
    "initial_question": {
      "title": "Does ReplacingMergeTree have the capability to identify partitions that have changed?",
      "body": "I am currently using version 21.8 of `ReplacingMergeTree` frequently. I have noticed that after I insert data, it seems that all partitions undergo automatic merging, even if some partitions do not require merging.\r\nI would like to know if the current version of `ReplacingMergeTree` has the capability to identify `partitions` that have changed, so that only the necessary `partitions` can be automatically merged. If the latest version does not have this functionality, perhaps I can contribute by adding this optimization.\r\nexample:\r\n```\r\nCREATE TABLE test_table\r\n(\r\n    `key` UInt64,\r\n    `someCol` String\r\n)\r\nENGINE = ReplacingMergeTree\r\nPARTITION BY modulo(key, 40) \r\nPRIMARY KEY key \r\nORDER BY key;\r\n```\r\nThen insert a large amount of data into it, covering all partitions from 0 to 39.\r\nThen `optimize table test_table final;`\r\nAfterwards, insert two datas.\r\n```\r\ninsert into test_table(key, someCol) values(1, 'test1');\r\ninsert into test_table(key, someCol) values(5, 'test5');\r\n```\r\nWould `ReplacingMergeTree` only automatically merge `partition(1)` and `partition(5)` at this point? Or will all `partitions (0-39)` be automatically merged after a certain period of time?"
    },
    "satisfaction_conditions": [
      "The merge operation must be configurable to operate only on necessary partitions",
      "The optimization setting must be compatible with ReplacingMergeTree engine",
      "The merge behavior must be controllable through table optimization commands"
    ],
    "created_at": "2023-08-31T13:34:43Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/53397",
    "source": {
      "issue_number": 53397
    },
    "initial_question": {
      "title": "INSERTing into a table named 'table' fails",
      "body": "```sql\r\n-- this works\r\nCREATE TABLE tab (val1 UInt32, val2 UInt32) ENGINE=Memory;\r\nINSERT INTO tab VALUES (42, 24);\r\n\r\n-- this doesn't:\r\nCREATE TABLE table (val1 UInt32, val2 UInt32) ENGINE=Memory;\r\nINSERT INTO table VALUES (42, 24); -- Code: 62. DB::Exception: Syntax error: failed at position 27 ('42'): 42, 24);. Expected one of: list of elements, insert element, COLUMNS matcher, COLUMNS, qualified asterisk, compound identifier, identifier, asterisk. (SYNTAX_ERROR)\r\n```"
    },
    "satisfaction_conditions": [
      "SQL INSERT statement successfully executes when targeting a table named 'table'",
      "Query syntax properly distinguishes the table name from the SQL keyword 'table'",
      "Solution maintains standard SQL INSERT statement structure"
    ],
    "created_at": "2023-08-14T08:39:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/52620",
    "source": {
      "issue_number": 52620
    },
    "initial_question": {
      "title": "(branch 23.5) `Looking for mutations for part` appears in client with `send_logs_level=debug`",
      "body": "```sql\r\nALTER TABLE tableA MATERIALIZE INDEX ginidx;\r\n\r\nSET send_logs_level=debug;\r\n\r\nINSERT INTO tableB SELECT * FROM  tableA LIMIT 1000000\r\n\r\nQuery id: 865a8654-159d-40a3-ada1-90febde40416\r\n\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.162059 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> executeQuery: Query span trace_id for opentelemetry log: 00000000-0000-0000-0000-000000000000\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.163246 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> executeQuery: (from 127.0.0.1:57642, user: operator) INSERT INTO tableB SELECT * FROM  tableA LIMIT 1000000; (stage: Complete)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167003 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 00538dfb52fdf41c1a36ff5f8a9b3f86_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167077 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0298f752bf557334fc80759843abe567_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167104 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0413d695373e7a54f73ae0886afed664_0_0_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167272 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0413d695373e7a54f73ae0886afed664_1_1_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167298 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 04ccd2b5bf76dec05640587b037abdcf_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167367 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0816cf91eb7efa18de40dcdb1c8ed388_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167410 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 093315f287eafd6f14001e3a1f1ae873_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167481 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 098efa9cec0b236ab538700d75525ef8_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167514 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 09d7a7a3a5c5c0cb9867f4c2d26b760b_0_0_0_5 (part data version 5, part metadata version 2)\r\n...(repeated logs)\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Debug logs showing 'Looking for mutations' messages are expected behavior when executing SELECT queries",
      "Mutation checks occur for each table part, even when no active mutations are present",
      "Debug-level logging accurately reflects internal mutation verification processes",
      "Mutation verification occurs independently of system.mutations table status"
    ],
    "created_at": "2023-07-26T12:50:22Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/52434",
    "source": {
      "issue_number": 52434
    },
    "initial_question": {
      "title": "How to efficiently materialize data into a table when a field is set with default_expression?",
      "body": "Our application scenario requires fast updates on a specific column, with data volume ranging from over 30 billion to even hundreds of billions. To achieve fast updates, our current approach involves synchronizing the required data column and associated keys to ClickHouse using Join Engine to create a table.\r\n\r\nWe then add a new field to the target table and set its default_expression as follows:\r\n\r\nALTER TABLE db.tab_1 ADD COLUMN col_new type DEFAULT joinGet(join_storage_table, `value_column`, join_keys);\r\n\r\nAfter that, we change the field names using the following command:\r\n\r\nALTER TABLE db.tab_1\r\n    RENAME COLUMN col TO col_old,\r\n    RENAME COLUMN col_new TO col,\r\n    DROP COLUMN col_old;\r\n\r\nThis way, the logical modification is completed almost instantly, and we can use the updated table. However, we are aware that the current value in col is obtained through joinGet from the join_storage_table. If the join_storage_table is deleted, col becomes unusable. Hence, we would like to inquire whether there is a method to efficiently materialize the data of this column into the table, without depending on join_storage_table.\r\n\r\nthank\r\n"
    },
    "satisfaction_conditions": [
      "Column data must be physically stored in the table rather than computed via default_expression",
      "Operation must maintain data consistency",
      "Column must function independently after join_storage_table removal"
    ],
    "created_at": "2023-07-21T15:25:54Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/52233",
    "source": {
      "issue_number": 52233
    },
    "initial_question": {
      "title": "Insertion stuck on ReplicatedMergeTree when keeper connection lost.",
      "body": "I setup a cluster with 2 nodes. each node run a clickhouse server as well as an embedded keeper.\r\nIf i shut down one node. The keeper node will fail because quorum is 2 and only 1 keeper alive.\r\nIn this case, ReplicatedMergeTree tables should be readonly on the remaining node.\r\n\r\nBut when I do insertion on the table. it never returns and stuck. Is this expected behavior?\r\n"
    },
    "satisfaction_conditions": [
      "System correctly transitions to read-only state when quorum is lost",
      "Read-only state transition time is configurable"
    ],
    "created_at": "2023-07-18T08:50:44Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/51875",
    "source": {
      "issue_number": 51875
    },
    "initial_question": {
      "title": "about summingmergetree Keep the earliest records",
      "body": "create table test (\r\nid int64,\r\ncreated_time DateTime,\r\ncnt Int64\r\n)ENGINE=ReplicatedSummingMergeTree() order by id \r\n\r\ninsert into test (1,'2023-07-05 01:00:00',1)\r\ninsert into test (1,'2023-07-05 02:00:00',2)\r\ninsert into test (1,'2023-07-05 03:00:00',3)\r\nAfter data merging\r\nselect * from test:\r\nI hope it's the following result\r\n1    '2023-07-05 01:00:00'   6\r\nrather than\r\n1    '2023-07-05 03:00:00'   6\r\n\r\nWhat do I need to do\uff1f\r\n\r\n\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "The table must aggregate records with the same ID while preserving the earliest timestamp",
      "The cnt values must be summed correctly for matching IDs",
      "The table structure must support both aggregation and timestamp preservation",
      "Data merging operations must maintain data consistency"
    ],
    "created_at": "2023-07-06T04:00:16Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/50591",
    "source": {
      "issue_number": 50591
    },
    "initial_question": {
      "title": "Why SYSTEM RESTORE REPLICA ... is not supported for Materialized views with Replica engines?",
      "body": "I am using the `SYSTEM RESTORE REPLICA` functionality to restore a MV with `ReplicatedAggregatingMergeTree`. In the docs it is just stated that the restore will only work for replicated tables, but nothing about not supporting it from MV. \r\nThis is needed in case the source table of MV has retention (TTL), then recreating the MV will not work\r\nA workaround would be to create another table localle and send all the data there\r\nI can also provide the data schema if this is unexpected behavior respectively a bug\r\n"
    },
    "satisfaction_conditions": [
      "Data from materialized view with TTL-enabled source tables must be recoverable",
      "Restoration method must work with replicated storage engines",
      "Access to underlying storage table data must be possible"
    ],
    "created_at": "2023-06-05T15:01:50Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/49854",
    "source": {
      "issue_number": 49854
    },
    "initial_question": {
      "title": "Support more filtering conditions for read in order optimization",
      "body": "I need to pull huge chunks of data from table with (category, time) primary key, for set of category values and stream result set in time order.\r\nThe amount of data can be really huge so I'd like to avoid full sort on top of query by any cost.\r\nWhen I filter with category = value, the RAM profile is low -- afaik thanks to MergeTreeInOrder read optimization.\r\nHowever, if I use other filtering function, like category in (...), the full sort seems to be applied and RAM usage sky rocket.\r\n\r\nCREATE TABLE order_test_1 (`ts` Int64,`lc` Int32) ENGINE = MergeTree PRIMARY KEY (lc) ORDER BY (lc, ts)\r\nas (select (number + 1000000000) as ts, (number % 16) as lc from numbers(1000000000));\r\n\r\n```\r\nselect version();\r\n23.5.1.1169\r\n\r\n-- this query looks good, takes ~35M memory \r\nexplain pipeline\r\nselect * from order_test_1 where lc = 0 order by ts;\r\n\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 20 \u2192 1\r\n    (Expression)\r\n    ExpressionTransform \u00d7 20\r\n      (ReadFromMergeTree)\r\n      MergeTreeInOrder \u00d7 20 0 \u2192 1\r\n\r\n-- but this takes ~1.5G memory to run full sort\r\nexplain pipeline\r\nselect * from order_test_1 where lc in (0,1) order by ts; -- format Null;\r\n\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 16 \u2192 1\r\n    MergeSortingTransform \u00d7 16\r\n      LimitsCheckingTransform \u00d7 16\r\n        PartialSortingTransform \u00d7 16\r\n          (Expression)\r\n          ExpressionTransform \u00d7 16\r\n            (ReadFromMergeTree)\r\n            MergeTreeThread \u00d7 16 0 \u2192 1\r\n```\r\n\r\nInterestingly, I can force ClickHouse into required behavior to some extent, by using VIEW with UNION ALL pre-filtered subqueries\r\n```\r\n-- memory ~75M\r\nexplain pipeline\r\nselect * from (\r\n\tselect * from order_test_1 where lc = 0 order by ts\r\n\tunion all\r\n\tselect * from order_test_1 where lc = 1 order by ts\r\n\tunion all\r\n\tselect * from order_test_1 where lc = 2 order by ts\r\n\tunion all\r\n\tselect * from order_test_1 where lc = 3 order by ts\r\n) where lc in (0, 1) order by ts;\r\n\r\n(Expression)\r\nExpressionTransform\r\n  (Sorting)\r\n  MergingSortedTransform 46 \u2192 1\r\n    (Union)\r\n      (Expression)\r\n      ExpressionTransform \u00d7 20\r\n        (Filter)\r\n        FilterTransform \u00d7 20\r\n          (ReadFromMergeTree)\r\n          MergeTreeInOrder \u00d7 20 0 \u2192 1\r\n      (Expression)\r\n      ExpressionTransform \u00d7 20\r\n        (Filter)\r\n        FilterTransform \u00d7 20\r\n          (ReadFromMergeTree)\r\n          MergeTreeInOrder \u00d7 20 0 \u2192 1\r\n      (Expression)\r\n      ExpressionTransform \u00d7 5\r\n        (Filter)\r\n        FilterTransform \u00d7 5\r\n          (ReadFromMergeTree)\r\n          MergeTreeInOrder \u00d7 5 0 \u2192 1\r\n      (Expression)\r\n      ExpressionTransform\r\n        (Filter)\r\n        FilterTransform\r\n          (ReadFromMergeTree)\r\n```\r\n\r\nUnfortunately, I can't use the same in real-life scenario due to much higher cardinality of category values.\r\n\r\nFor some more context -- we're evaluating ClickHouse as a direct replacement for own in-house solution. One of existing application processes time-series data replay (simulation) streamed in time order. 1 minute of data is ~50G, and we need to process hours of data in one run. Ideally, we'd like to make one query and receive continuous stream of data which will not OOM the server."
    },
    "satisfaction_conditions": [
      "Query execution must maintain low memory usage when filtering multiple category values",
      "Results must be delivered in time-ordered sequence",
      "Query must support filtering multiple category values simultaneously"
    ],
    "created_at": "2023-05-13T10:23:33Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/49379",
    "source": {
      "issue_number": 49379
    },
    "initial_question": {
      "title": "How to return an error from External UDFs?",
      "body": "How do you return an error from an external UDF?   An external UDF is a daemon-like process that constantly running, reading from STDIN, and writing response to STDOUT.   One way is to let the process die when an error happens but it's not ideal because starting up the process is costly, and also I cannot return a meaningful error message.\r\n\r\nSuppose there is an external UDF `f(key)`.  A key is one of `a` or `b`.  When something else is passed, I would like to raise \"invalid key\" error, or at least raise a generic error without interrupting the process.\r\n\r\n"
    },
    "satisfaction_conditions": [
      "UDF must be able to communicate error states without terminating the process",
      "Error information must be accessible for each individual row",
      "Error states must be distinguishable from valid results",
      "Custom error messages must be supported",
      "Solution must work within the constraints of STDIN/STDOUT communication"
    ],
    "created_at": "2023-05-01T21:02:02Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/48836",
    "source": {
      "issue_number": 48836
    },
    "initial_question": {
      "title": "How clickhouse respect alter delete and insert order ",
      "body": "I have some ETL task that collects data and insert them into clickhouse (22.3.* version). Typically data are inserted into distributed + replicated table (4x4) in bathes. If there is some problem with one or more batches we need to reprocess some of them. Data in reprocessed batch may be same as data inserted before or differ, entirely or partially. Reprocess may occur when we spot problem after few minutes or few days there is no rule here (Mention this to address possibile problem with deduplication mechanism). On reprocessing we do query like: \r\n\r\n```\r\nALTER TABLE table ON CLUSTER cluster DELETE \r\nWHERE event_date > {batch_start_date} AND event_date <= {batch_end_date}\r\n```\r\n\r\nThis will create mutation and my question is whenever execution time of this mutation influence insert that occurs right after ALTER TABLE query?  If data inserted after ALTER QUERY called and before actual execution of mutation may disappear if they meet WHERE clause conditions? "
    },
    "satisfaction_conditions": [
      "Data inserted after an ALTER TABLE DELETE mutation must be preserved",
      "Mutation operations must maintain data consistency across all affected partitions",
      "The system must properly track and handle version differences between old and new data",
      "Data operations must be correctly ordered regardless of execution timing",
      "Merging operations must preserve the intended data state"
    ],
    "created_at": "2023-04-17T07:01:08Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/48149",
    "source": {
      "issue_number": 48149
    },
    "initial_question": {
      "title": "partition replace",
      "body": "Hello!\r\nHow can the be executed following query?\r\nalter table target_tbl replace partition (\r\n    select top 1 partition from `system`.parts where table in ('tbl_cache') /*or any question returns partition name */\r\n) from tbl_cache\r\n;\r\nThis is required to complete the next case.\r\nI want to implement the following behavior of system tables: target_tbl and tbl_cache. Tables has identical structure and partitioned by \"actuality\" (Data). Everyday a new portion of data is placed in target_tbl. One partition is formed and named like 'YYYY-MM-DD'. Fore example '2023-03-29'. Further i want add single partition of tbl_cache into target_tbl as new if the partition is not there or replace existing one. \r\nI think best way for this is command **alter table ... replace ...** . And i want do this command independent from partition name. It is possible?\r\nMaybe there is another way to implement a similar scenario without explicitly specifying the date?\r\n\r\nThank you!"
    },
    "satisfaction_conditions": [
      "Must be able to identify and reference the source partition from tbl_cache without hardcoding the partition name",
      "Must work with tables partitioned by date ('actuality')",
      "Must be automatable for daily execution"
    ],
    "created_at": "2023-03-29T09:00:09Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/48107",
    "source": {
      "issue_number": 48107
    },
    "initial_question": {
      "title": "generate xml access control configuration from SQL ",
      "body": "I created a user with specific role configuration using the SQL-driven Access Control and Account Management, \r\nis it possible to generate the xml user/role configuration from the previous queries to copy it to a production server ? "
    },
    "satisfaction_conditions": [
      "User must understand that SQL and XML configurations cannot be automatically converted between each other"
    ],
    "created_at": "2023-03-28T12:15:06Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/47949",
    "source": {
      "issue_number": 47949
    },
    "initial_question": {
      "title": "Default compression behavior",
      "body": "I have a CH DB version 22.03 with some data running with compression method zstd.\r\nI have changed in the file config.xml to compression method lz4\r\nAfter restart CH, I have noted that only new data inserted in a table has parts with file default_compression_codec.txt CODEC(LZ4) and the old data previous to the change has parts with: CODEC(ZSTD(1))\r\nMy question is:\r\nA change in the default compression method only works for new data inserted or I need to run OPTIMIZE TABLE ... FINAL in all the tables to apply the changes?"
    },
    "satisfaction_conditions": [
      "New data compression must reflect updated configuration",
      "Existing data compression remains unchanged without explicit action",
      "Method exists to recompress existing data",
      "Configuration changes persist after system restart"
    ],
    "created_at": "2023-03-23T16:54:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/47369",
    "source": {
      "issue_number": 47369
    },
    "initial_question": {
      "title": "The splitByChar function cannot use invisible characters",
      "body": "My business data is split per row with '\\036'. I want to split columns using splitByChar, but an error is reported.\r\n\r\nThe usage is as follows:\r\n\r\nselect splitByChar('\\036',message) as signSec from kafka.kafka_padx_otc_v2_log limit 10;\r\n\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception:  Illegal separator for function splitByChar. Must be exactly one byte.."
    },
    "satisfaction_conditions": [
      "The separator character must be properly encoded in a format accepted by the splitByChar function",
      "The function must successfully split the input string into an array of substrings",
      "The solution must handle invisible/control characters as separators",
      "The resulting split operation must preserve all data between separators"
    ],
    "created_at": "2023-03-08T14:51:42Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/46943",
    "source": {
      "issue_number": 46943
    },
    "initial_question": {
      "title": "Naming convention about 'abstract' class",
      "body": "I am learning from CH code and have a question about naming convention for abstract class. I see some classes called 'I-***', for example IColumn, IDataType, have defined both pure virtual functions and normal virtual functions. In Java/C# context, these classes are 'abstract' classes. I am wondering if the classes with 'I-***' names are representing both abstract classes and interfaces in CH source code. \r\n\r\nBy the way, is there no specific distinction between the abstract class and the interface in CH source code? Just want to follow the convention.\r\n\r\nThank you. "
    },
    "satisfaction_conditions": [
      "Explanation clarifies the relationship between C++ abstract classes and interfaces",
      "Explanation addresses the meaning of 'I-' prefix naming convention in C++ context",
      "Clarification of how C++'s inheritance model differs from Java/C#",
      "Explanation of why C++ doesn't require strict interface/abstract class separation"
    ],
    "created_at": "2023-02-27T09:35:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/46757",
    "source": {
      "issue_number": 46757
    },
    "initial_question": {
      "title": "Join in materialized view is 60x slower than in select statement.",
      "body": "**Describe the situation**\r\n\r\nSeems materialized view with join not working with **FillRightFirst** plan, please loot at the script blew.\r\n\r\nThank you for your time. \r\n\r\nSelect statement statistics: \r\n`0 rows in set. Elapsed: 0.021 sec. Processed 262.02 thousand rows, 2.10 MB (12.69 million rows/s., 101.50 MB/s.)`\r\n\r\nMaterialized view statistics(a insert statement to source table):\r\n\r\n`1 row in set. Elapsed: 1.422 sec.`\r\n\r\n**How to reproduce**\r\n1. Create null table t_null;\r\n2. Create mergetree table t_numbers with 100000000 rows;\r\n3. Execute Select Statement 1 as blow code;\r\n4. Create materialized view with the same query;\r\n5. Insert only 1 record into t_null to trigger materialized view;\r\n\r\n\r\n* Which ClickHouse server version to use\r\nClickHouse client version 22.8.13.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 22.8.13 revision 54460.\r\n\r\n* Non-default settings, if any\r\nNone\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\n`CREATE TABLE t_numbers\r\n(\r\n    `A` UInt64,\r\n    `B` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY A;`\r\n\r\n `CREATE TABLE t_null\r\n(\r\n    `A` UInt64,\r\n    `B` String\r\n)\r\nENGINE = Null;`\r\n\r\n`CREATE MATERIALIZED VIEW mv_null\r\nENGINE = MergeTree\r\nORDER BY A AS\r\nSELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A;`\r\n\r\n* Sample data for all these tables\r\n\r\n`INSERT INTO t_numbers SELECT\r\n    number,\r\n    toString(cityHash64(number))\r\nFROM numbers(100000000)`\r\n\r\n* Queries to run that lead to slow performance\r\n\r\nQuery 1, which runs very fast!\r\n\r\n`SELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A;\r\n`\r\nQuery 2, which runs very slow!\r\n\r\n`INSERT INTO t_null VALUES(1, 'insert');`\r\n \r\n\r\n**Expected performance**\r\nStatement in materialized view runs as fast as select statement, instead of 67x slow.\r\n\r\n**Additional context**\r\n\r\nIf we check the plan, clickhouse choose FillRightFirst for both inner and outer join, may be materialized view is not using the same plan?\r\n\r\nEXPLAIN\r\nSELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A\r\n\r\nQuery id: 0e4c95b7-204e-4922-8d01-414f6ad86758\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                                      \u2502\r\n\u2502   Join (JOIN **FillRightFirst**)                                                                     \u2502\r\n\u2502     Expression (Before JOIN)                                                                     \u2502\r\n\u2502       ReadFromStorage (Null)                                                                     \u2502\r\n\u2502     Expression ((Joined actions + (Rename joined columns + (Projection + Before ORDER BY))))     \u2502\r\n\u2502       Join (JOIN **FillRightFirst**)                                                                 \u2502\r\n\u2502         Expression (Before JOIN)                                                                 \u2502\r\n\u2502           ReadFromMergeTree (rtd.t_numbers)                                                      \u2502\r\n\u2502         Expression ((Joined actions + (Rename joined columns + (Projection + Before ORDER BY)))) \u2502\r\n\u2502           ReadFromStorage (Null)                                                                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n"
    },
    "satisfaction_conditions": [
      "Query performance with empty tables matches expected baseline",
      "Efficient filtering of large joined datasets",
      "Materialized view correctly triggers on relevant table changes"
    ],
    "created_at": "2023-02-23T02:52:11Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/46637",
    "source": {
      "issue_number": 46637
    },
    "initial_question": {
      "title": "Distributed table with distributed_group_by_no_merge settings",
      "body": "im using clickhouse version 22.9.7.34 with 2 shards\r\n\r\neach shard contains replicated and distributed table(to query across shard). below the details:\r\nshard 1\r\n```\r\nCREATE TABLE default.repli          (id String default '', `rank` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/000-000/repli', 'clickhouse00')  order by (id)     SETTINGS index_granularity = 8192;\r\nCREATE TABLE default.distri    (id String default '', `rank` String)\r\nENGINE = Distributed('local-clickhouse', 'default', 'repli', rand());\r\ninsert into default.repli(id, rank) values ('1','a');\r\n```\r\n\r\nshard 2\r\n```\r\nCREATE TABLE default.repli          (id String default '', `rank` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/001-000/repli', 'clickhouse1')  order by (id)     SETTINGS index_granularity = 8192;\r\nCREATE TABLE default.distri    (id String default '', `rank` String)\r\nENGINE = Distributed('local-clickhouse', 'default', 'repli', rand());\r\ninsert into default.repli(id, rank) values ('2','a');\r\n```\r\n\r\n\r\nshard 1\r\n```\r\nselect any(id),rank from default.distri group by rank settings distributed_group_by_no_merge = 1;// return 2 rows\r\nselect any(id),rank from default.distri group by rank limit 1 settings distributed_group_by_no_merge = 1;// still return 2 rows\r\n```\r\n\r\nis distributed_group_by_no_merge executed after limit and offset?. Please help to solve this issue, thanks."
    },
    "satisfaction_conditions": [
      "Aggregation results are consistent across distributed shards"
    ],
    "created_at": "2023-02-21T09:05:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/46470",
    "source": {
      "issue_number": 46470
    },
    "initial_question": {
      "title": "Is there any way to view keeper data without system.zookeeper table?",
      "body": "I deployed a keeper cluster as a separate service. Is there a client or api that allows me to operate the keeper service directly ?"
    },
    "satisfaction_conditions": [
      "Successful external access to keeper data is achieved",
      "Compatible client/API communication with the keeper service",
      "Direct interaction with keeper service without ClickHouse dependency"
    ],
    "created_at": "2023-02-16T09:47:03Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/45951",
    "source": {
      "issue_number": 45951
    },
    "initial_question": {
      "title": "Why size field of StringRef is 64bit (8 bytes)",
      "body": "StringRef is usually used for representing a string and contains a pointer and size. A pointer has to be 64 bit in my x64 machine, however the size is not necessarily 64bit in my opinion, because usually string's length is less then 65535 and two bytes is enough. \r\n\r\nFor each string, 6 bytes are wasted. For big amount of strings, the wasted memory is considerable.\r\n\r\nWhy we choose 64bit (size_t) for string's size? "
    },
    "satisfaction_conditions": [
      "Must support strings larger than 4GB in size",
      "Must maintain data structure alignment requirements for the target architecture",
      "Must be compatible with x64 architecture memory addressing",
      "Must handle the memory size vs performance tradeoff appropriately for the use case"
    ],
    "created_at": "2023-02-02T08:17:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/45945",
    "source": {
      "issue_number": 45945
    },
    "initial_question": {
      "title": "light weight `ALTER DROP INDEX`",
      "body": "Can I just do `ALTER DROP INDEX`, followed by `KILL MUTATION`, to make it a lightweight operation, that only deletes the index metadata and leave the data parts as is ?"
    },
    "satisfaction_conditions": [
      "Index metadata is removed without full data rewrite",
      "Original table data remains accessible and functional",
      "Operation completes faster than standard index drop",
      "System tolerates residual data artifacts"
    ],
    "created_at": "2023-02-02T04:41:25Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/45616",
    "source": {
      "issue_number": 45616
    },
    "initial_question": {
      "title": "What endpoint to check with HAProxy",
      "body": "Hello, we're using HAProxy to route http reqs to our cluster.\r\n\r\nWhat is the best endpoint to check the replica is alive? We've tried `/replicas_status` or `/ping`.\r\n\r\nIs there any recommended one? E.g. `/replicas_status` often gives errs when ZK decides to put a table to RO mode."
    },
    "satisfaction_conditions": [
      "The health check endpoint must reliably indicate service availability",
      "The endpoint should be independent of distributed system states",
      "The endpoint response should be deterministic"
    ],
    "created_at": "2023-01-25T16:26:27Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/45477",
    "source": {
      "issue_number": 45477
    },
    "initial_question": {
      "title": "Question: PARTITION BY efficiency in MergeTree table.",
      "body": "Hello!\r\nSorry for disturbing, but I want to clarify can I use PARTITION BY in my case.\r\n**The case:**\r\n-  We have table with metrics for multitenant setup. We have columns like tenant_id and event_time. \r\n-  By requirements this table should be optimized for per-tenant use. \r\n-  Almost always tenant will query data for the LAST MONTH or for the LAST day. So we choose ORDER BY (tenant_id, event_time) key. And this works great.\r\n-  Analytics want to use this database also for aggregated statistics (query will have range for event_time and no tenant_id specified). So we decided to add PARTITION BY toYYYYMM(event_time). \r\n- Table will contain data only for one year (will be controlled by database (TTL feature)). So we will get only 12 unique partition keys.\r\n- Table engine will be ReplicatedMergeTree\r\n\r\n**Our results**\r\nTable with PARTITION BY gives us 25% less rows scanned for **analytical** queries on synthetic data. Tenant's queries are good in both cases.\r\n**Question**\r\nIn many places it is pointed that PARTITION BY is not recommended to use for MergeTree-family engines. So I'm afraid that we can run into issues on production because of this decision.\r\nCan we use PARTITION BY in this case or should we refuse to do this despite of better performance in our tests?\r\n\r\nThank you!\r\n"
    },
    "satisfaction_conditions": [
      "Query performance for tenant-specific queries must remain efficient",
      "Analytical queries across all tenants must show improved performance",
      "Partitioning strategy must align with the data retention period",
      "Solution must be compatible with table replication requirements",
      "Partitioning must effectively handle time-based data access patterns"
    ],
    "created_at": "2023-01-20T16:22:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/44878",
    "source": {
      "issue_number": 44878
    },
    "initial_question": {
      "title": "String values in Parquet files",
      "body": "Somewhat related to #43970, I've noticed ClickHouse doesn't mark string fields as Strings when exporting to Parquet. BigQuery will treat the fields as binary (unless overridden) as will QGIS when opening PQ files generated by ClickHouse.\r\n\r\n```bash\r\n$ cat California.jsonl \\\r\n    | clickhouse local \\\r\n          --input-format JSONEachRow \\\r\n          -q \"SELECT *\r\n              FROM table\r\n              FORMAT Parquet\" \\\r\n    > cali.snappy.pq\r\n```\r\n\r\n```python\r\nIn [1]: import pyarrow.parquet as pq\r\n\r\nIn [2]: pf = pq.ParquetFile('cali.snappy.pq')\r\n\r\nIn [3]: pf.schema\r\nOut[3]: \r\n<pyarrow._parquet.ParquetSchema object at 0x105ccc940>\r\nrequired group field_id=-1 schema {\r\n  optional int64 field_id=-1 release;\r\n  optional binary field_id=-1 capture_dates_range;\r\n  optional binary field_id=-1 geom;\r\n}\r\n```\r\n\r\nMost other tools, including the Rust release of Arrow will mark these fileds as strings.\r\n\r\n```bash\r\n$ json2parquet \\\r\n      -c snappy \\\r\n      California.jsonl \\\r\n      California.snappy.pq\r\n```\r\n\r\n```python\r\nIn [1]: import pyarrow.parquet as pq\r\n\r\nIn [2]: pf = pq.ParquetFile('California.snappy.pq')\r\n\r\nIn [3]: pf.schema\r\nOut[3]: \r\n<pyarrow._parquet.ParquetSchema object at 0x109a11380>\r\nrequired group field_id=-1 arrow_schema {\r\n  optional binary field_id=-1 capture_dates_range (String);\r\n  optional binary field_id=-1 geom (String);\r\n  optional int64 field_id=-1 release;\r\n}\r\n```"
    },
    "satisfaction_conditions": [
      "Parquet schema must correctly identify string fields as string type rather than binary",
      "Solution must maintain compatibility with ClickHouse's string data handling capabilities",
      "Parquet output must be readable by standard Parquet tools",
      "Schema metadata must be properly preserved in the Parquet output"
    ],
    "created_at": "2023-01-03T18:58:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/44621",
    "source": {
      "issue_number": 44621
    },
    "initial_question": {
      "title": "Is 'ON CLUSTER' clause necessary for 'ALTER TABLE DELETE|UPDATE|ADD|DROP' for ReplicatedMergeTree?",
      "body": "Saying I have a ReplicatedMergeTree `t1` created by the folllowing query:\r\n```SQL\r\ncreate table db1.events on cluster 'ch-pro-cluster' (ID Int64, x01 String, x02 String) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/dd', '{replica}') order by ID partition by ID % 5  settings min_rows_for_wide_part=10, min_bytes_for_wide_part=1;\r\n```\r\n\r\nAnd the table's data has been initialized by query \r\n\r\n`insert into db1.events(ID, x01, x02) select number, leftPad(toString(number + 01), 8, '0'), toString(number + 02) from numbers(5000000)`.\r\n\r\nThen I want to delete, update, add or drop table. I found omitting 'ON CLUSTER' the synchronization can be also done by mechanism of ReplicatedMergeTree.\r\n\r\nFor example, `alter table db1.events delete where ID % 10 = 3 settings mutations_sync=2;` can spread the changes over cluster. Of course, query `alter table db1.events on cluster 'ch-pro-cluster' delete where ID % 10 = 3 settings mutations_sync=2;` also work but the difference is that the later query will add a zk node in `/clickhouse/task_queue/ddl ` and the first query won't. \r\n\r\nWhat is the best practice? Shall I omit 'ON CLUSTER' for doing 'ALTER TABLE ...' on ReplicatedMergeTree tables?"
    },
    "satisfaction_conditions": [
      "ALTER operations must successfully propagate to all relevant replicas within the same shard",
      "ALTER operations must successfully propagate across all shards when multiple shards exist",
      "Table creation and deletion operations must be executed across all nodes in the cluster",
      "Data modifications must maintain consistency with the cluster's replication configuration"
    ],
    "created_at": "2022-12-27T03:26:23Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/42858",
    "source": {
      "issue_number": 42858
    },
    "initial_question": {
      "title": "ParsingException when importing Point data from CSV",
      "body": "When importing CSV files with geo Point data, I always get this error:\r\n\r\n> `Code: 27. DB::ParsingException: Cannot parse input: expected '\"' before: '(10, 10)\"'`\r\n\r\nTo reproduce the problem under simple conditions, I created a table with a Point column:\r\n`CREATE TABLE default.geo_point (p Point) ENGINE = Memory()`\r\n\r\nI also created a geo_point.csv file with a single point in it:\r\n```\r\n\"(10, 10)\"\r\n```\r\n\r\nI import the file with the clickhouse client (version 22.9.3.18) like this:\r\n`cat geo_point.csv | clickhouse-client -q 'INSERT INTO default.geo_point FORMAT CSV'`\r\n\r\nI get the above error, even when removing the double quotes. Not sure if this is a bug, or if I need to pass a certain flag to the clickhouse-client (I didn't find a flag that works). \r\n\r\nOther datatypes are imported just fine. Also, when I change the data type from Point to String, the import works fine.\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Point data must be successfully imported into ClickHouse",
      "Input format must match ClickHouse's Point data type parsing expectations",
      "No ParsingException errors occur during import",
      "Solution must work with bulk data imports from external sources",
      "Imported Point data must maintain geometric integrity"
    ],
    "created_at": "2022-11-01T11:03:47Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/42557",
    "source": {
      "issue_number": 42557
    },
    "initial_question": {
      "title": "Join generated data",
      "body": "Hello clickhouse community,\r\n\r\nI'm stuck on a statement that I can't make.\r\n\r\nI want to get a table with two columns containing generated data, let me explain:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)(price)\r\n        from pricing\r\n        limit 1)\r\n    ) as quant_min\r\n;\r\n```\r\n\r\nIt gives me:\r\n\r\n```txt\r\nquant_min\r\n0.0000056\r\n0.00850023\r\n0.013097947\r\n0.020124\r\n0.032167118\r\n0.0437904\r\n0.051556416\r\n0.0644\r\n0.0896712\r\n0.1346728\r\n```\r\n\r\nNow this one:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)(price)\r\n        from pricing\r\n        limit 1)\r\n    ) as quant_max\r\n;\r\n\r\ngive me:\r\n\r\n```txt\r\nquant_max\r\n0.00850023\r\n0.0131056\r\n0.02019656\r\n0.032187376\r\n0.0437904\r\n0.0515056\r\n0.06435264\r\n0.0894664\r\n0.1344904\r\n34.22802\r\n```\r\nI want a request where the result is:\r\n\r\n```txt\r\nquant_min       |     quant_max\r\n0.0000056       |     0.00850023\r\n0.00850023      |     0.0131056\r\n0.013097947     |     0.02019656\r\n0.020124        |     0.032187376\r\n0.032167118     |     0.0437904\r\n0.0437904       |     0.0515056\r\n0.051556416     |     0.06435264\r\n0.0644          |     0.0894664\r\n0.0896712       |     0.1344904\r\n0.1346728       |     34.22802\r\n```\r\n\r\nI had try this request:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)(price)\r\n        from zone_hit\r\n        where\r\n        event_date > today() - 3\r\n        and rtb_bid_price > 0\r\n        and hit_type_id = 'rtb'\r\n        limit 1)\r\n    ) as quant_min,\r\n    arrayJoin(\r\n        (select quantilesExact(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)(price)\r\n         from zone_hit\r\n         where event_date > today() - 3\r\n           and rtb_bid_price > 0\r\n           and hit_type_id = 'rtb'\r\n         limit 1)\r\n    ) as quant_max\r\n;\r\n```\r\n\r\nBut it give me 100 results because every quant_min is join to every quant_max.\r\n\r\nIf a life saver is in the neighberhood ?\r\n\r\nThanks"
    },
    "satisfaction_conditions": [
      "Query must return exactly two columns named quant_min and quant_max",
      "Each row must pair corresponding quantile values from both arrays in order",
      "Query must return exactly 10 rows",
      "Values in quant_max column must be greater than or equal to corresponding quant_min values",
      "Query must preserve the original order of quantile values"
    ],
    "created_at": "2022-10-21T08:58:34Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/42375",
    "source": {
      "issue_number": 42375
    },
    "initial_question": {
      "title": "Is there a way to wait while a replicated table will have consistent state between masters?",
      "body": "In my project I often rename table or alter it's partitions. When the next time process connects to other master, it might try to change table where partitions wasn't changed yet. And this leads to data loss and mysterious behaviour.\r\n\r\n If there a way to wait while all manipulations with a replicated table are done on all masters before I'll apply further changes?"
    },
    "satisfaction_conditions": [
      "Table modifications must be synchronized across all master nodes",
      "System must provide confirmation that replication is complete",
      "Solution must handle both structural changes (renames) and partition modifications",
      "Subsequent operations must be blocked until replication is complete"
    ],
    "created_at": "2022-10-17T06:41:00Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/42318",
    "source": {
      "issue_number": 42318
    },
    "initial_question": {
      "title": "Example of how to use `remove` override in yaml format?",
      "body": "I'd like to leave the main config file untouched and simply remove the entries for `postgresql_port` and `mysql_port` in a separate config file. I'm using yaml rather than XML. I'm not seeing an example of this in the docs. Thanks!"
    },
    "satisfaction_conditions": [
      "Configuration entries for postgresql_port and mysql_port are effectively removed",
      "Main configuration file remains unmodified",
      "Removal is achieved through YAML-compatible syntax",
      "Changes are applied through separate configuration file(s)"
    ],
    "created_at": "2022-10-14T14:58:20Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/41101",
    "source": {
      "issue_number": 41101
    },
    "initial_question": {
      "title": "The size of processed data",
      "body": "```sql\r\nlocalhouse :) SELECT * FROM hits WHERE URL LIKE '%google%' ORDER BY EventTime LIMIT 10;\r\n...\r\n10 rows in set. Elapsed: 6.029 sec. Processed 100.00 million rows, 25.45 GB (16.58 million rows/s., 4.22 GB/s.) \r\n\r\nlocalhost :) SELECT * FROM hits ORDER BY EventTime LIMIT 10 Format Null\r\n...\r\n0 rows in set. Elapsed: 9.844 sec. Processed 100.00 million rows, 82.77 GB (10.16 million rows/s., 8.41 GB/s.)\r\n```\r\n\r\nThe size of processed data of the first SQL is 25.45 GB which is much less than the second SQL. That looks so cool. Can the primary index used in this case? Or any other magic technology here like deferred materialize? Any docs about this?"
    },
    "satisfaction_conditions": [
      "Query execution must show reduced data processing volume for filtered query compared to unfiltered query",
      "The optimization must maintain query result accuracy while reducing processed data volume",
      "The optimization must be automatic without requiring manual query rewrites",
      "The solution must explain the column-specific nature of the optimization"
    ],
    "created_at": "2022-09-08T10:14:14Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/41073",
    "source": {
      "issue_number": 41073
    },
    "initial_question": {
      "title": "CREATE   EMPTY AS SELECT  ERROR on 22.8",
      "body": "hello !\r\nwhen i run \r\n create table xx1 empty as select * from xx;\r\n\r\nCREATE TABLE xx1 EMPTY AS\r\nSELECT *\r\nFROM xx\r\n\r\nQuery id: fd018c3a-79db-4fbd-8e34-c893307ce9b6\r\n\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\nReceived exception from server (version 22.8.4):\r\nCode: 119. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED)\r\n\r\nI tried  the table engine of  MergeTree() and Replicated_MergeTree().\r\nall of these engines  i got this error!\r\n\r\nwhat can i do?\r\n"
    },
    "satisfaction_conditions": [
      "Table creation query must include a valid storage engine specification",
      "Table creation query must include required engine-specific parameters",
      "The created table must maintain the column structure from the source SELECT query",
      "The EMPTY keyword functionality must be preserved in the final solution"
    ],
    "created_at": "2022-09-07T08:33:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/41051",
    "source": {
      "issue_number": 41051
    },
    "initial_question": {
      "title": "\u3010Merge Engine related\u3011How to use Merge Engine for distributed query ?",
      "body": "Build distributed engine  on Merge Engine and Merge Engine on ReplicatedMergeTree Engine,\r\nOr build Merge Engine on distributed engine and  distributed engine on merge tree,\r\nwhich is better ?\r\n\r\nThank you guys.\r\n"
    },
    "satisfaction_conditions": [
      "Query execution works correctly across distributed nodes",
      "Performance meets operational requirements"
    ],
    "created_at": "2022-09-06T14:14:54Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/40427",
    "source": {
      "issue_number": 40427
    },
    "initial_question": {
      "title": "Don't know how to debug MySQLHandler: DB::Exception: Cannot read all data. ",
      "body": "Hi,\r\n\r\nI have ClickHouse 22.1.3.7 installed and it is working fine and my inserts are going as expected. However in log I have multiple lines like this:\r\n\r\n`<Error> MySQLHandler: DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 3.`\r\n\r\nI have tried to set logging level to test, but I am unable to get any useful information to understand what is going on.\r\n\r\nCan you possibly hint me to the right direction?\r\n\r\nThanks!"
    },
    "satisfaction_conditions": [
      "Error message context is properly understood",
      "Normal system operation is confirmed",
      "Connection handling behavior is clarified",
      "Performance implications are understood"
    ],
    "created_at": "2022-08-19T22:42:09Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/40384",
    "source": {
      "issue_number": 40384
    },
    "initial_question": {
      "title": "Configured background pool size does not match system.settings",
      "body": "**Describe what's wrong**\r\n\r\nValue of background_fetches_pool_size configured in config.xml (as per #36425):\r\n\r\n```xml\r\n<background_fetches_pool_size>64</background_fetches_pool_size>\r\n```\r\n\r\ndoes not show system.settings which has the default value instead:\r\n\r\n```sql\r\nSELECT name, value FROM system.settings WHERE name LIKE 'background_fetches_pool_size'\r\n\u250c\u2500value\u2500\u2510\r\n\u2502 8     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nwhile the correct value shows in the log:\r\n\r\n```\r\nInitialized background executor for fetches with num_threads=64, num_tasks=64\r\n```\r\n\r\nand BackgroundFetchesPoolTask sometimes exceeds the default so it looks like it's actually using the configured value\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes (suppose 22.8 LTS will be added to version_date.tsv)\r\n\r\n**How to reproduce**\r\n\r\nClickHouse server version 22.8.1.2097"
    },
    "satisfaction_conditions": [
      "Background executor initialization matches configured value",
      "Documentation/system accurately indicates setting status",
      "Background fetch operations function with configured capacity"
    ],
    "created_at": "2022-08-19T07:31:43Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/40327",
    "source": {
      "issue_number": 40327
    },
    "initial_question": {
      "title": "Extract specific symbols in the string",
      "body": "Hi!\r\n\r\nI would be grateful if you help me to find specific symbols in the string. \r\nI have a table **'comment'** with the column **'text'**\r\nIn that column i have some text, for example : \r\n\"_Address: Chicago, 15 and Number of people: **2**_\" \r\n\r\nHow can I extract \"**2**\" out of this text? I mean I need only \"**2**\" in this string\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Extract the numeric value that appears after 'Number of people:' in the text",
      "Handle variable text content before 'Number of people:'",
      "Return the number in numeric format",
      "Handle multi-digit numbers correctly",
      "Process data from a database table column named 'text'"
    ],
    "created_at": "2022-08-18T08:06:20Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/39920",
    "source": {
      "issue_number": 39920
    },
    "initial_question": {
      "title": "How to using a select SQL with case insensitive\uff1f",
      "body": "hi, team,\r\n\r\nI'm sorry to have your attention, but i just want to know how to make query condition case insensitive like MySQL query. just only can using lower function convert it  like this?\r\n\r\n`select * from log_table_test where lower(file) = lower('File_name')` \r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Query returns matching results regardless of text case",
      "Query syntax is valid for ClickHouse database",
      "Query maintains the original data integrity",
      "Query performs string comparison on the 'file' column"
    ],
    "created_at": "2022-08-05T09:42:41Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/39616",
    "source": {
      "issue_number": 39616
    },
    "initial_question": {
      "title": "How to quickly load file data into local tables\uff1f",
      "body": "hello everyone\r\n\r\nI wanted to load some data to CH for using  it.\r\n\r\nAfter spending a long time with CH, found that write Distributed table is faster then local table\r\n\r\nFirst file: csv.data  **60GB+**\r\nSample data:\r\n\r\n```\r\n1,SEX,1,2022-06-12 00:00:00,1\r\n2,SEX,1,2022-06-12 00:00:00,1\r\n3,SEX,1,2022-06-12 00:00:00,1\r\n```\r\n\r\nSecond file: This is the local table  load.\r\n\r\n```\r\nCREATE TABLE tag.tag_test_base_info\r\n(\r\n     `offset` UInt64,\r\n    `tag_code` String,\r\n    `tag_value` String,\r\n     `ts` DateTime,\r\n    `sign` Int8\r\n)\r\nENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/{shard}/tag_test_base_info', '{replica}', sign)\r\nPARTITION BY ts\r\nORDER BY (ts, tag_code, offset)\r\nSETTINGS storage_policy = 'default', use_minimalistic_part_header_in_zookeeper = 1, index_granularity = 8192\r\n```\r\n\r\nTesting CSV load:\r\n```\r\ncat csv.dat | clickhouse-client -h 127.0.0.1 -d default -m -u default --password 123456 --format_csv_delimiter= ',' --query=\"INSERT INTO tag.tag_test_base_info FORMAT CSV\";\r\n\r\n```\r\n\r\nSo it took **39:12.29** seconds.\r\n\r\nThree file:This is the Distributed table load.\r\n\r\n```\r\nCREATE TABLE tag.tag_test_base_info\r\n(\r\n     `offset` UInt64,\r\n    `tag_code` String,\r\n    `tag_value` String,\r\n     `ts` DateTime,\r\n    `sign` Int8\r\n)\r\nENGINE = Distributed('tagclickhouse', 'tag', 'tag_test_base_info', rand())\r\n```\r\n\r\nTesting CSV load:\r\n```\r\ncat csv.dat | clickhouse-client -h 127.0.0.1 -d default -m -u default --password 123456 --format_csv_delimiter= ',' --query=\"INSERT INTO tag.tag_test_base_info_dist FORMAT CSV\";\r\n\r\n```\r\n\r\nSo it took **16:50.01** seconds.\r\n\r\n\r\nThe  Server is  6  virtual machine  **16C+64g+500G** \r\n\r\nThat means using Distributed table  is = **2X** faster (39:12.29 seconds / 16:50.01 seconds)\r\nOr local table  is **2X** slower! \r\n\r\n\r\nMy question\r\n\r\n1. Why is it faster to load Distributed tables than local tables\r\n2. How to quickly load data into local tables\uff1f(i 'am using ``` --input_format_parallel_parsing=0 --compression=0``` ,args There is no change in speed) \r\n\r\n\r\n\r\nthank you\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Solution works within the given infrastructure constraints"
    ],
    "created_at": "2022-07-26T12:35:50Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/39549",
    "source": {
      "issue_number": 39549
    },
    "initial_question": {
      "title": "TCPHandlerFactory creating TCPhandler is slow. Takes 3 sec within intranet",
      "body": "checked log file, /var/log/clickhouse-server/clickhouse-server.log, I got logs below,\r\n\r\n2022.07.25 17:02:23.535994 [ 12473 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::ffff:172.22.254.xx]:57308\r\n2022.07.25 17:02:26.554058 [ 12473 ] {} <Debug> TCPHandler: Connected ClickHouse client version 1.1.0, revision: 54380, database: xxxxx, user: xxxx.\r\n\r\nIt takes 3 sec to create a TCP connection in my opion,\r\n\r\nAlready done below,\r\n1. Ping is OK, stable and fast. Intranet.\r\n2. It returns the correct results, just slow.\r\n3. ClickHouse Server version: 20.8.3 revision 54438\r\n4. Stop the firewall on both machines.\r\n5. No error logs in /var/log/clickhouse-server/clickhouse-server.err.log\r\n\r\nAny suggestion will be grateful. \r\n\r\nWarm Regards,\r\n"
    },
    "satisfaction_conditions": [
      "System maintains existing functionality while improving performance"
    ],
    "created_at": "2022-07-25T09:17:05Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/39548",
    "source": {
      "issue_number": 39548
    },
    "initial_question": {
      "title": "Updating a dictionary by `update_field`, even if there is no data to update",
      "body": "```\r\nselect version();\r\n\u250c\u2500version()\u2500\u2500\u2500\u2510\r\n\u2502 22.7.1.2484 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nCREATE TABLE test_dictionary_time\r\n(\r\n    `ID` UInt64,\r\n    `Value` String,\r\n    ModifiedTime DateTime default now()\r\n)\r\nENGINE = MergeTree\r\nORDER BY ID;\r\n\r\nINSERT INTO test_dictionary_time (ID, Value) SELECT     number,     concat('value_', toString(number)) FROM numbers(100000);\r\n\r\ncreate dictionary test_dictionary_updated_field\r\n(\r\n  ID UInt64,\r\n  Value String\r\n)\r\nPRIMARY KEY ID\r\nSOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' PASSWORD '' DB 'default' TABLE 'test_dictionary_time' update_field 'ModifiedTime'))\r\nLIFETIME(30)\r\nLAYOUT(Complex_Key_Hashed ());\r\n\r\nselect * from test_dictionary_updated_field;\r\n```\r\nDictionary loaded\r\n```\r\nselect name, status, type, element_count, load_factor, lifetime_min, lifetime_max, loading_start_time, last_successful_update_time, loading_duration, last_exception, comment from system.dictionaries where name = 'test_dictionary_updated_field';\r\n```\r\nData is not added to the table\r\nAfter 30 seconds of seconds in the logs the message\r\n```\r\n2022.07.25 08:04:10.682885 [ 17232 ] {} <Debug> executeQuery: (internal) SELECT `ID`, `Value` FROM `default`.`test_dictionary_time` WHERE ModifiedTime >= '2022-07-25 08:04:04'; (stage: Complete)\r\n```\r\nRepeating in the database\r\n```\r\nSELECT\r\n    ID,\r\n    Value\r\nFROM default.test_dictionary_time\r\nWHERE ModifiedTime >= '2022-07-25 08:04:04'\r\n\r\nQuery id: 01279644-53c6-4df7-a87f-330a8ff93de0\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.008 sec. Processed 1.00 million rows, 4.00 MB (124.75 million rows/s., 499.02 MB/s.)\r\n\r\n```\r\nWatching dictionary update time\r\n```\r\nselect name, status, type, element_count, load_factor, lifetime_min, lifetime_max, loading_start_time, last_successful_update_time, loading_duration, last_exception, comment from system.dictionaries where name = 'test_dictionary_updated_field';\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500status\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500element_count\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500load_factor\u2500\u252c\u2500lifetime_min\u2500\u252c\u2500lifetime_max\u2500\u252c\u2500\u2500loading_start_time\u2500\u252c\u2500last_successful_update_time\u2500\u252c\u2500loading_duration\u2500\u252c\u2500last_exception\u2500\u252c\u2500comment\u2500\u2510\r\n\u2502 test_dictionary_updated_field \u2502 LOADED \u2502 ComplexKeyHashed \u2502       1000000 \u2502 0.476837158203125 \u2502            0 \u2502           30 \u2502 2022-07-25 08:05:55 \u2502         2022-07-25 08:05:56 \u2502            0.426 \u2502                \u2502         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nThe dictionary is still reloaded even if there is no data to update.\r\nThe dictionary is loaded into memory every 30 seconds, even if no data is added to the table."
    },
    "satisfaction_conditions": [
      "Dictionary performs incremental updates based on the ModifiedTime field",
      "System accurately tracks dictionary update operations",
      "Dictionary maintains data consistency with source table",
      "Update mechanism properly handles empty result sets"
    ],
    "created_at": "2022-07-25T08:54:14Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/39407",
    "source": {
      "issue_number": 39407
    },
    "initial_question": {
      "title": "Clickhouse analogue function",
      "body": "What will be the analogue of function DAYOFWEEK( date_format(b.createdon_date, '%Y-01-06'))? \r\n\r\nAnd also I need help to know how to start the week not from monday or sunday but from Thursday on clickhouse? \r\n"
    },
    "satisfaction_conditions": [
      "Function correctly converts input dates to corresponding day-of-week numbers",
      "Supports custom week start day configuration",
      "Maintains consistent 1-7 numbering sequence",
      "Handles date format conversion correctly"
    ],
    "created_at": "2022-07-20T08:28:46Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/39068",
    "source": {
      "issue_number": 39068
    },
    "initial_question": {
      "title": "Is it possible to change sharding key of a Distributed table",
      "body": "Currently, I have a history(almost 2yrs) distributed table with rand sharding key.  However,  recently I come across lots of business scenarios where I have to do distribute join on business key, such as userid(**which is evenly distributed by nature**). So I want to change the sharing key to userid.\r\n1. it is possible at the moment\r\n2. if not, is there any possible way i can make the minimal change to get it done\r\n"
    },
    "satisfaction_conditions": [
      "The distributed table must be accessible with the new sharding key (userid)",
      "Historical data must remain accessible during and after the change"
    ],
    "created_at": "2022-07-10T09:00:13Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/38591",
    "source": {
      "issue_number": 38591
    },
    "initial_question": {
      "title": "can clickhouse do the whole subquery in remote server when using remote function?",
      "body": "1. db.table_1 created in server 10.1.4.160, table ddl as below\r\n\r\n```\r\nCREATE TABLE db.table_1\r\n(\r\n\r\n    `column1` String,\r\n\r\n    `column2` String,\r\n\r\n    `value1` Int32,\r\n\r\n    `value2` Int32\r\n)\r\nENGINE = MergeTree\r\nORDER BY (column1,\r\n column2)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\n2. subquery using remote function in another server (exp: 10.1.4.159)\r\n\r\n```\r\nselect column1, sum(value1) as value1_sum from (\r\n  select * from remote('10.1.4.160:9000', `db.table_1`) where value1 > 0\r\n) group by column1 order by value1_sum\r\n```\r\n\r\n3. select from 10.1.4.160's system.query_log, \r\n\r\n```\r\nselect query from system.query_log where type='QueryFinish' and has(databases, 'db') order by event_time desc\r\n```\r\n\r\n\r\n4. I find the query executed in remote is as below\r\n\r\n```\r\nSELECT `column1`, `value1` FROM `db`.`table_1` WHERE `value1` > 0\r\n```\r\n\r\n5 finally, my question is that, can I make the whole subquery being executed in remote server, and how can I do that?\r\n\r\nthe whole subquery I expected is as below\r\n\r\n```\r\nSELECT `column1`, sum(`value1`) AS `value1_sum` FROM `db`.`table_1` WHERE `value1` > 0 GROUP BY `column1` ORDER BY `value1_sum` ASC\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Query results must match those of the original query",
      "The solution must be verifiable through system.query_log on the remote server"
    ],
    "created_at": "2022-06-29T16:04:07Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/38358",
    "source": {
      "issue_number": 38358
    },
    "initial_question": {
      "title": "When final query processing is done at Initiator node?",
      "body": " In terms of memory usage , \r\nIs there any difference when final query processing is done on the initiator node and when it is done on the shard and initiator only proxy the data.\r\n\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Memory usage must be lower when aggregation processing is done on shards vs initiator node",
      "Query must complete successfully without out-of-memory errors",
      "Performance impact must be considered alongside memory usage",
      "Solution must handle distributed query processing appropriately"
    ],
    "created_at": "2022-06-23T19:07:23Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/38193",
    "source": {
      "issue_number": 38193
    },
    "initial_question": {
      "title": "Error after changing clickhouse data dirs",
      "body": "Hi,\r\n\r\nClickhouse by default storing metadata/data into /var/lib/clickhouse. '/var/lib' is typically used for s/w install. I moved contents in /var/lib/clickhouse/* to ~/clickhouse after stopping server\r\n\r\nAlso ownership of ~/clickhouse was given to clickhouse:clickhouse recursively and also updated paths in config.xml \r\n\r\nMove is done as below \r\n\r\ncp -al /var/lib/clickhouse/* ~/clickhouse/\r\nrm -rf /var/lib/clickhouse\r\n\r\n\r\nServer is up after above activity and also client is connected and also executing sql successfully but there are lot of errors in server log. \r\n\r\nPlease suggest\r\n\r\nThank you\r\n\r\n\r\n```\r\n2022.06.18 19:38:45.358360 [ 19006 ] {} <Error> void DB::BackgroundJobsAssignee::threadFunc(): Code: 214. DB::ErrnoException: Could not calculate available disk space (statvfs), errno: 2, strerror: No such file or directory. (CANNOT_STATVFS), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb8a147a in /usr/bin/clickhouse\r\n1. DB::throwFromErrnoWithPath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int) @ 0xb8a252a in /usr/bin/clickhouse\r\n2. DB::getStatVFS(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xb8e4e5a in /usr/bin/clickhouse\r\n3. DB::DiskLocal::getAvailableSpace() const @ 0x15970be1 in /usr/bin/clickhouse\r\n4. DB::DiskLocal::getUnreservedSpace() const @ 0x15970d7a in /usr/bin/clickhouse\r\n5. DB::StoragePolicy::getMaxUnreservedFreeSpace() const @ 0x15ca8819 in /usr/bin/clickhouse\r\n6. DB::MergeTreeDataMergerMutator::getMaxSourcePartsSizeForMerge(unsigned long, unsigned long) const @ 0x16bf9cfa in /usr/bin/clickhouse\r\n7. DB::StorageMergeTree::selectPartsToMerge(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, std::__1::unique_lock<std::__1::mutex>&, std::__1::shared_ptr<DB::MergeTreeTransaction> const&, bool, DB::SelectPartsDecision*) @ 0x169808cd in /usr/bin/clickhouse\r\n8. DB::StorageMergeTree::scheduleDataProcessingJob(DB::BackgroundJobsAssignee&) @ 0x16984c3e in /usr/bin/clickhouse\r\n9. DB::BackgroundJobsAssignee::threadFunc() @ 0x16acab47 in /usr/bin/clickhouse\r\n10. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x1556c2f8 in /usr/bin/clickhouse\r\n11. DB::BackgroundSchedulePool::threadFunction() @ 0x1556f5b6 in /usr/bin/clickhouse\r\n12. ? @ 0x1557042e in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xb94d0b7 in /usr/bin/clickhouse\r\n14. ? @ 0xb9504dd in /usr/bin/clickhouse\r\n15. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n16. /build/glibc-CVJwZb/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: clone @ 0x12161f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n (version 22.6.1.1985 (official build))\r\n2022.06.18 19:38:45.791476 [ 19010 ] {} <Error> void DB::BackgroundJobsAssignee::threadFunc(): Code: 214. DB::ErrnoException: Could not calculate available disk space (statvfs), errno: 2, strerror: No such file or directory. (CANNOT_STATVFS), Stack trace (when copying this message, always include the lines below):\r\n\r\n```"
    },
    "satisfaction_conditions": [
      "ClickHouse server must have proper filesystem permissions for the new data directory",
      "All directory paths must be correctly updated in ClickHouse configuration files"
    ],
    "created_at": "2022-06-18T11:56:03Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/37776",
    "source": {
      "issue_number": 37776
    },
    "initial_question": {
      "title": "\u3010Query Limit related\u3011will system.processes table reflect all the queries in the very moment?",
      "body": "- We are on 21.8.5.7-lts now\r\n- When we encounter an \"Too many simultaneous queries. Maximum:  150\" exception,  we immediately log into the server  and execute select count(*) from system.processes. but it seems that it seldom give us the expected result. some time it is far away from the limit which we set (150 max_concurrent_queries)\r\n- The processes table seems to  reflects the queries in the very moment, but isn't it weird that we never find the total items in it  is in a reasonable range ?\r\nThank you guys!"
    },
    "satisfaction_conditions": [
      "Query activity monitoring must capture peak concurrent query periods",
      "Query analysis must provide time-based aggregation of concurrent queries",
      "Monitoring method must persist data about transient query spikes",
      "Analysis must identify periods when concurrent queries exceed or approach the 150 limit"
    ],
    "created_at": "2022-06-02T09:14:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/37692",
    "source": {
      "issue_number": 37692
    },
    "initial_question": {
      "title": "Merge Map by GROUP BY",
      "body": "There is a table with map data\r\n```sql\r\nCREATE TABLE test.table1\r\n(\r\n    `id` String,\r\n    `test_map` Map(String, String)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nINSERT INTO test.table1 VALUES \r\n(1, {'a': '1','b': 'b', 'c': '2'})\r\n(2, {'d': 'd', 'a': '2'})\r\n(1, {'d': 'd', 'a': '2'});\r\n```\r\nCan I get the following data by `GROUP BY` id?\r\n```\r\nid      test_map\r\n1\t{'a':'2','b':'b','c':'2','d':'d'}\r\n2\t{'d':'d','a':'2'}\r\n```\r\n\r\nThanks."
    },
    "satisfaction_conditions": [
      "Query must group records by the 'id' column",
      "Maps with the same id must be merged to include all unique keys",
      "For duplicate keys across maps with same id, the latest/maximum value must be retained",
      "Output must maintain the Map(String, String) data type structure",
      "Query must work with string values in maps"
    ],
    "created_at": "2022-05-31T12:40:58Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/37669",
    "source": {
      "issue_number": 37669
    },
    "initial_question": {
      "title": "\u3010Atomic database related\u3011will the mix usage of atomic database and ordinary database in the same shard cause any problem?",
      "body": "- The clickhouse version is 21.8.5.7\r\n- The database type is originally  Ordinary. \r\n- During the replacement of some fault node in certain shard, we need to re-create the database and table.\r\n- Because clickhouse have updated the default database type to Atomic, now we has a mixed database type of the same database on \r\ndifferent replica.\r\n- will it cause some potential problem? do we need to re create the database as an Ordinary one and re create all the tables  and let the part replication process execute again ?"
    },
    "satisfaction_conditions": [
      "Database operations continue to function normally with mixed database types",
      "Existing data and tables remain accessible",
      "System stability is maintained",
      "Optional upgrade path is available"
    ],
    "created_at": "2022-05-31T02:05:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/37310",
    "source": {
      "issue_number": 37310
    },
    "initial_question": {
      "title": "Question about ALTER TABLE ADD COLUMN IF NOT EXISTS ... , modify order by () ....",
      "body": "While i create a table on clickhouse\r\n\r\n```\r\nCREATE TABLE TEST\r\n(\r\n\tID String,\r\n    NAME String\r\n)\r\nENGINE = MergeTree\r\nORDER BY (ID)\r\n```\r\n\r\nnow i want to **add column** and modify to **order by**\r\n\r\n```\r\nalter table TEST add column if not exists VERSION String, modify order by (ID, VERSION)\r\n\r\nalter table TEST add column if not exists HELLO String, modify order by (ID, VERSION, HELLO)\r\n```\r\n\r\nok\r\n\r\nThe table structure at this time is as follows\r\n\r\n```\r\nCREATE TABLE TEST\r\n(\r\n\r\n    ID String,\r\n\r\n    NAME String,\r\n\r\n    VERSION String,\r\n\r\n    HELLO String\r\n)\r\nENGINE = MergeTree\r\nPRIMARY KEY ID\r\nORDER BY (ID,VERSION,HELLO)\r\n```\r\n\r\nbut, When I execute the following statement again\r\n\r\n`alter table TEST add column if not exists VERSION String, modify order by (ID, VERSION)`\r\n\r\nok\r\n\r\nThe table structure at this time is as follows\r\n\r\n```\r\nCREATE TABLE TEST\r\n(\r\n\r\n    ID String,\r\n\r\n    NAME String,\r\n\r\n    VERSION String,\r\n\r\n    HELLO String\r\n)\r\nENGINE = MergeTree\r\nPRIMARY KEY ID\r\nORDER BY (ID,VERSION)\r\n```\r\n\r\n**My question is**, if the condition of **IF NOT EXISTES** is not established, will it not take effect for the following **modify order by** statement?\r\n\r\n\r\nWhat should I do so that the **column exists** without modification **order by**\r\n\r\nplease help me, thanks.\r\n\r\n"
    },
    "satisfaction_conditions": [
      "ALTER TABLE operations must execute independently of each other",
      "Column existence check must be performed separately from ORDER BY modification",
      "ORDER BY modifications must maintain data consistency",
      "ORDER BY clause must allow reduction of trailing columns",
      "Multiple ALTER TABLE operations must be supported in a single statement"
    ],
    "created_at": "2022-05-18T03:44:09Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/37180",
    "source": {
      "issue_number": 37180
    },
    "initial_question": {
      "title": "how often does clickhouse do sum in summingmergetree?",
      "body": "From document\r\n\r\n> ClickHouse merges the inserted parts of data periodically...\r\n\r\n**My questions is how often do it merge? is therey any way to control it?**\r\n\r\nI'm considering using summingmergetree for my analysis flow, about 10 million events a day, each event is ~1kb, summingmergetree seems a perfect solution for saving disk space and speed performance, the only concern is when data is not mergeed, I have do `optimize table`, which is costly, especially in large table.\r\n"
    },
    "satisfaction_conditions": [
      "Data merging occurs automatically based on system-defined criteria",
      "Query results must be accurate regardless of merge state",
      "System performance remains sustainable without manual optimization",
      "Merge behavior is predictable based on documented parameters"
    ],
    "created_at": "2022-05-13T08:47:46Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/37062",
    "source": {
      "issue_number": 37062
    },
    "initial_question": {
      "title": "column and table access management ",
      "body": "In the official documentation and other documents, I could only find access management of database/row in the following formats (if not using sql)\r\n`<user1>\r\n    <databases>\r\n        <database_name>\r\n            <table1>\r\n                <filter>id = 1000</filter>\r\n            </table1>\r\n        </database_name>\r\n    </databases>\r\n</user1>`\r\n\r\n`        <allow_databases>\r\n           <database>test</database>\r\n        </allow_databases>`\r\n\r\nis there any way to manage table/column access using xml? eg. granting user with access to only databaseA.tableA and not databaseA.tableB.\r\n"
    },
    "satisfaction_conditions": [
      "Configuration must be compatible with automated generation by internal services"
    ],
    "created_at": "2022-05-10T07:30:28Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/36927",
    "source": {
      "issue_number": 36927
    },
    "initial_question": {
      "title": "Clickhouse use multiple columns in group by clause",
      "body": "\r\nHi All, I am using the below query to generate materialized view for clickhouse but i want the result to be seperate by both the names ( name_top_apps, name_remote_top_emdpoints) can anyone help on this one\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS analytics.uflow_topapps_bytes_flowdir_or_013_mv\r\n  ENGINE = SummingMergeTree\r\n  PARTITION BY toYYYYMMDD ( Timestamp )\r\n  ORDER BY (Timestamp)\r\n  POPULATE\r\n  AS SELECT\r\n  toString(AppId) as name_top_apps,\r\n  IPv6NumToString(DstIP) as name_remote_top_emdpoints,\r\n  sum(FlowStatsBytesFwd) as upload_bytes,\r\n  sum(FlowStatsBytesRev) as download_bytes, \r\n  sum(FlowStatsBytesFwd + FlowStatsBytesRev) as cumulative_bytes,\r\n  sum(FlowStatsPktsFwd) as upload_flows,\r\n  sum(FlowStatsPktsFwd) as download_flows,\r\n  sum(FlowStatsPktsFwd + FlowStatsPktsRev) as cumulative_flows,\r\n  toInt64((sum(FlowStatsBytesFwd)* 8)/least(sum(Duration),60)) as upload_rate,\r\n  toInt64((sum(FlowStatsBytesRev)* 8)/least(sum(Duration),60)) as download_rate,\r\n  toInt64(((sum(FlowStatsBytesRev) + sum(FlowStatsBytesFwd)) * 8)/least(sum(Duration),60)) as cumulative_rate,\r\n  toStartOfInterval(`Timestamp`, INTERVAL 300 second) AS Timestamp,\r\n  CpeCNID as  CpeCNID\r\n  from analytics.sampled_uflow where AccessDenied = 0 and ( FlowDir == 2 or FlowDir == 0 or FlowDir == 3 )\r\n  Group by ( CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)\r\n"
    },
    "satisfaction_conditions": [
      "Aggregation must be performed separately for each unique combination of CpeCNID, name_top_apps, name_remote_top_emdpoints, and Timestamp",
      "View must maintain correct time-based partitioning"
    ],
    "created_at": "2022-05-05T06:14:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/36903",
    "source": {
      "issue_number": 36903
    },
    "initial_question": {
      "title": "Truncate replicated cluster",
      "body": "What is the point of this error? \r\n```\r\ntruncate table numbers on cluster stage_cluster\r\nCode: 371, e.displayText() = DB::Exception: For a distributed DDL on circular replicated cluster its table name must be qualified by database name. (version 21.8.14.5 (official build))\r\n```\r\nstage cluster has 2 replicas per shard. Wanted to truncate tables in db1.numbers and db2.numbers\r\n\r\nApplication during its run is writing data to table for other applications. After some time it is to be cleared and feeded again. For long time it was solved by clearing one replica and it was cleared on other replicas in shard. This solution is not good because one node can be not present and then data will not be cleared or will be delayed causing weird situations. So it is not reliable at all."
    },
    "satisfaction_conditions": [
      "Database name must be explicitly specified in the truncate command for circular replicated clusters",
      "Each affected database in the cluster must be handled appropriately"
    ],
    "created_at": "2022-05-04T12:06:34Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/36823",
    "source": {
      "issue_number": 36823
    },
    "initial_question": {
      "title": "INSERT INTO ... SELECT shows Memory limit (for query) exceeded",
      "body": "I'm running a simple query that:\r\n\r\n\r\nINSERT INTO order_book_2022_04_20 SELECT *\r\nFROM stock.order_book\r\nWHERE TradingDay = '2022-04-20'\r\n\r\nwhere the destination table `order_book_2022_04_20` is a temporary table and source table stock.order_book is a distributed table.\r\n\r\nIt shows `DB::Exception: Memory limit (for query) exceeded: would use 9.32 GiB (attempt to allocate chunk of 4228000 bytes), maximum: 9.31 GiB.` by default.\r\n\r\nWhen I adjust max_memory_size to zero(unlimited)\r\nIt shows that the peak memory usage is event higher than the query result.\r\n\r\nI've tried to adjust max_insert_block_size/max_block_size etc but no one take effects. I also tried to add --max-insert-block-size 1024 to  clickhouse-client but no helps.\r\n\r\nHere's the debug log:\r\n\r\n```\r\nch-shard-2-rep-1 :) set max_memory_usage=42949672960\r\n\r\nSET max_memory_usage = 42949672960\r\n\r\nQuery id: 5476a5e8-543b-40c0-b569-fc7bbfc7a55b\r\n\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:48.992649 [ 99630 ] {5476a5e8-543b-40c0-b569-fc7bbfc7a55b} <Debug> executeQuery: (from 10.20.140.3:43396) set max_memory_usage=42949672960\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:48.992765 [ 99630 ] {5476a5e8-543b-40c0-b569-fc7bbfc7a55b} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\nch-shard-2-rep-1 :) insert into order_book_2022_04_20 select * from stock.order_book where TradingDay = '2022-04-20'\r\n\r\nINSERT INTO order_book_2022_04_20 SELECT *\r\nFROM stock.order_book\r\nWHERE TradingDay = '2022-04-20'\r\n\r\nQuery id: 6d125a8a-3b1d-4277-a039-3ff9ab0157d5\r\n\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.342436 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> executeQuery: (from 10.20.140.3:43396) insert into order_book_2022_04_20 select * from stock.order_book where TradingDay = '2022-04-20'\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.342552 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: INSERT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON _temporary_and_external_tables.`_tmp_f3c2b45f-b805-4e73-8a5b-8bf9e5c44e58`\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.342916 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343211 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343749 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343921 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.343979 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344100 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> InterpreterSelectQuery: Complete -> Complete\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344250 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Key condition: (column 0 in [19102, 19102])\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344489 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): MinMax index condition: (column 0 in [19102, 19102])\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344630 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1107_1137_2_1138 (2506 marks)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344623 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_962_1106_3_1138 (11817 marks)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344653 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344655 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 2097\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344671 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 11817\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344684 [ 11512 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 28 steps\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344667 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2506\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344695 [ 2225 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.344793 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Selected 2/203 parts by partition key, 2 parts by primary key, 12224/14321 marks by primary key, 12224 marks to read from 2 ranges\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.345003 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Reading approx. 99771210 rows with 24 streams\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.347756 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> Connection (10.20.131.97:9000): Connecting. Database: (not specified). User: default\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.348851 [ 64248 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.128.197:9000): Sent data for 2 scalars, total 2 rows in 3.9413e-05 sec., 49820 rows/sec., 68.00 B (1.60 MiB/sec.), compressed 0.4594594594594595 times to 148.00 B (3.48 MiB/sec.)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.349023 [ 64248 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.128.197:9000): Sent data for 1 external tables, total 0 rows in 0.000104251 sec., 0 rows/sec., 384.00 B (3.49 MiB/sec.), compressed 1.1671732522796352 times to 329.00 B (2.99 MiB/sec.)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.349157 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Trace> Connection (10.20.131.97:9000): Connected to ClickHouse server version 22.3.2.\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.352876 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> executeQuery: (from 10.20.131.199:56640, initial_query_id: 6d125a8a-3b1d-4277-a039-3ff9ab0157d5) SELECT `order_book_local`.`TradingDay`, `order_book_local`.`Channel`, `order_book_local`.`ID`, `order_book_local`.`OrderID`, `order_book_local`.`ExchTimeOffsetUs`, `order_book_local`.`Symbol`, `order_book_local`.`Volume`, `order_book_local`.`Price`, `order_book_local`.`OrderKind`, `order_book_local`.`FunctionCode`, `order_book_local`.`TradeFlag`, `order_book_local`.`BidOrderID`, `order_book_local`.`AskOrderID`, `order_book_local`.`Type`, `order_book_local`.`LocalTimeStamp` FROM `stock`.`order_book_local` WHERE `TradingDay` = '2022-04-20'\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.354820 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.131.97:9000): Sent data for 2 scalars, total 2 rows in 2.5096e-05 sec., 78137 rows/sec., 68.00 B (2.53 MiB/sec.), compressed 0.4594594594594595 times to 148.00 B (5.49 MiB/sec.)\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.354942 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> Connection (10.20.131.97:9000): Sent data for 1 external tables, total 0 rows in 6.4903e-05 sec., 0 rows/sec., 384.00 B (5.61 MiB/sec.), compressed 1.1671732522796352 times to 329.00 B (4.81 MiB/sec.)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.360077 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> executeQuery: (from 10.20.131.199:40512, initial_query_id: 6d125a8a-3b1d-4277-a039-3ff9ab0157d5) SELECT `order_book_local`.`TradingDay`, `order_book_local`.`Channel`, `order_book_local`.`ID`, `order_book_local`.`OrderID`, `order_book_local`.`ExchTimeOffsetUs`, `order_book_local`.`Symbol`, `order_book_local`.`Volume`, `order_book_local`.`Price`, `order_book_local`.`OrderKind`, `order_book_local`.`FunctionCode`, `order_book_local`.`TradeFlag`, `order_book_local`.`BidOrderID`, `order_book_local`.`AskOrderID`, `order_book_local`.`Type`, `order_book_local`.`LocalTimeStamp` FROM `stock`.`order_book_local` WHERE `TradingDay` = '2022-04-20'\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354036 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354209 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354279 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354476 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Key condition: (column 0 in [19102, 19102])\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354768 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): MinMax index condition: (column 0 in [19102, 19102])\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354985 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1122_1127_1_1138 (493 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354993 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1084_1115_2_1138 (2616 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355021 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354993 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1128_1133_1_1138 (492 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355038 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1055_1083_2_1138 (2370 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355052 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 493\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354995 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1134_1137_1_1138 (301 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355078 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355100 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355107 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2370\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355120 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 301\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355125 [ 8099 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355141 [ 8084 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355000 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_849_1021_3_1138 (14103 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355030 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354993 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1116_1121_1_1138 (492 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355058 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355221 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355079 [ 15820 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355234 [ 4681 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.354996 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1022_1054_2_1138 (2697 marks)\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355185 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 11324\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355284 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355297 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 14103\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355301 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2697\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355209 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355311 [ 8061 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355308 [ 4651 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 26 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355211 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2616\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355317 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355332 [ 4691 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355329 [ 15833 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Trace> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355516 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Selected 8/215 parts by partition key, 8 parts by primary key, 12232/23556 marks by primary key, 12232 marks to read from 8 ranges\r\n[ch-shard-1-rep-1] 2022.04.30 18:41:50.355933 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> stock.order_book_buff_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Reading approx. 99771761 rows with 24 streams\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.363439 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.364077 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> ContextAccess (default): Access granted: SELECT(TradingDay, Channel, ID, OrderID, ExchTimeOffsetUs, Symbol, Volume, Price, OrderKind, FunctionCode, TradeFlag, BidOrderID, AskOrderID, Type, LocalTimeStamp) ON stock.order_book_local\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.364290 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.364894 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Key condition: (column 0 in [19102, 19102])\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.365509 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): MinMax index condition: (column 0 in [19102, 19102])\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366027 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1025_1058_2_1138 (2778 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366159 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366123 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1126_1131_1_1138 (492 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366182 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1059_1087_2_1138 (2369 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366216 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_848_1024_3_1138 (14383 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366282 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366226 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2778\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366185 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1120_1125_1_1138 (492 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366365 [ 13656 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366234 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366401 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366445 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366232 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1132_1137_1_1138 (464 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366481 [ 13590 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366472 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 492\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366057 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Running binary search on index range for part 202204_1088_1119_2_1138 (2616 marks)\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366310 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 11358\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366632 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366324 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2369\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366691 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 2616\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366712 [ 13207 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366744 [ 2181 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 23 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366535 [ 2239 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366650 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 14383\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.366527 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367013 [ 13672 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 26 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367072 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found (RIGHT) boundary mark: 464\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367114 [ 2210 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Trace> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Found continuous range in 17 steps\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.367878 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Selected 7/198 parts by partition key, 7 parts by primary key, 12229/23587 marks by primary key, 12229 marks to read from 7 ranges\r\n[ch-shard-3-rep-2] 2022.04.30 18:41:50.369109 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> stock.order_book_local (3b2d01f3-1192-49de-bb2d-01f3119239de) (SelectExecutor): Reading approx. 99773104 rows with 24 streams\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:50.873240 [ 9201 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 1.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:51.458831 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 2.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:52.044369 [ 44900 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 3.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:52.628235 [ 9201 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 4.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:53.217460 [ 2274 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 5.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:53.835947 [ 9201 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 6.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:54.467806 [ 44883 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 7.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:55.080716 [ 2274 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 8.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:55.727570 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 9.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:56.367678 [ 9181 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 10.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:57.003147 [ 9166 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 11.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:57.610519 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 12.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:58.239731 [ 9166 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 13.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:41:59.228963 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 14.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:00.932248 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 15.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:02.895774 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 16.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:04.811238 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 17.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:06.595207 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 18.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:08.389497 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 19.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:10.052112 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 20.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:11.793655 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 21.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:13.650309 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 22.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:15.414734 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 23.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:17.309921 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 24.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:19.041601 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 25.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:20.814243 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 26.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:22.551550 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 27.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:24.539744 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 28.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:26.432611 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 29.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:28.284181 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 30.01 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:30.169253 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 31.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:32.018803 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 32.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:33.605974 [ 9086 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 33.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:35.421458 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 34.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:37.094259 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 35.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:38.976578 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 36.01 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:40.617896 [ 9087 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 37.00 GiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:42.465243 [ 9151 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Current memory usage (for query): 38.00 GiB.\r\n[ch-shard-3-rep-2] 2022.04.30 18:42:42.514774 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Information> executeQuery: Read 99773104 rows, 8.76 GiB in 52.154523129 sec., 1913028 rows/sec., 172.05 MiB/sec.\r\n[ch-shard-3-rep-2] 2022.04.30 18:42:42.528780 [ 72444 ] {94ccd8ae-2238-467b-8e17-c82601b1c31a} <Debug> MemoryTracker: Peak memory usage (for query): 178.93 MiB.\r\n[ch-shard-1-rep-1] 2022.04.30 18:42:43.436844 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Information> executeQuery: Read 99771761 rows, 8.68 GiB in 53.083883545 sec., 1879511 rows/sec., 167.47 MiB/sec.\r\n[ch-shard-1-rep-1] 2022.04.30 18:42:43.443990 [ 95302 ] {c9acbab0-f246-43f2-a7ce-1af1bb7419b0} <Debug> MemoryTracker: Peak memory usage (for query): 178.63 MiB.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:43.680592 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Information> executeQuery: Read 299316075 rows, 27.01 GiB in 53.338099769 sec., 5611674 rows/sec., 518.56 MiB/sec.\r\n[ch-shard-2-rep-1] 2022.04.30 18:42:43.690263 [ 99630 ] {6d125a8a-3b1d-4277-a039-3ff9ab0157d5} <Debug> MemoryTracker: Peak memory usage (for query): 38.41 GiB.\r\nOk.\r\n\r\n0 rows in set. Elapsed: 53.350 sec. Processed 299.32 million rows, 29.00 GB (5.61 million rows/s., 543.63 MB/s.)\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Data from source table must be successfully transferred to destination table"
    ],
    "created_at": "2022-04-30T10:50:07Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/36527",
    "source": {
      "issue_number": 36527
    },
    "initial_question": {
      "title": "intersect in clickhouse",
      "body": "Does clickhouse support interest keywords?\r\n\r\nWhen I use intersect in clickhouse, there is a syntax error, it seems that clickhouse can only support union all?\r\n"
    },
    "satisfaction_conditions": [
      "ClickHouse version supports INTERSECT keyword functionality",
      "Version used is stable and production-ready"
    ],
    "created_at": "2022-04-22T03:43:40Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/36105",
    "source": {
      "issue_number": 36105
    },
    "initial_question": {
      "title": "What's the process in Memory Engine for update/delete mutation operation?",
      "body": "I would use Memory engine for some special requirement. After the 'ALTER UPDATE/DELET' operation, can I query this table immediately?  Is the mutation operation in Memory engine is also an asyc action? How can I check whether the \"UPDATE/DELETE\" action is done for Memory Engine/Table?  Thanks!"
    },
    "satisfaction_conditions": [
      "Query results must be immediately available after mutation operations",
      "Mutation operations must be synchronous",
      "Single-threaded mutation processing must be confirmed",
      "No manual thread configuration required"
    ],
    "created_at": "2022-04-10T13:54:58Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/35896",
    "source": {
      "issue_number": 35896
    },
    "initial_question": {
      "title": "How many tables can be created in ClickHouse?",
      "body": "VERSION: 21.3\r\nQUESTION: How many tables can be created in ClickHouse? For the whole, or one database\r\n\r\nI have a demand to build one table for one goods every, and the number of goods may have tens of thousands to hundreds of millions, so I need to know the relevant situation.\r\n\r\nPlease give me some advice. Fuzzy is OK also. Only give the order of magnitude is OK also.\r\n"
    },
    "satisfaction_conditions": [
      "Data model supports efficient querying of goods",
      "Solution accounts for table type limitations"
    ],
    "created_at": "2022-04-04T03:48:40Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/35829",
    "source": {
      "issue_number": 35829
    },
    "initial_question": {
      "title": "Is there any transaction isolation in clickhouse?",
      "body": "for example, there is a merge table with 100 columns.\r\n\r\nwhen client A insert some rows, and client B query same row at same time, does client B would get only 50 columns avaiable? \r\n\r\nduring the mutation (delete or update data) process, does client would get outdated columns with uptodate columns?  "
    },
    "satisfaction_conditions": [
      "Row atomicity must be maintained during inserts",
      "Row-level consistency during mutations",
      "Table-level mutation visibility",
      "Part-level write completeness"
    ],
    "created_at": "2022-04-01T07:43:48Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/35605",
    "source": {
      "issue_number": 35605
    },
    "initial_question": {
      "title": "Details about how distribute_group_by_no_merge works",
      "body": "I'm wondering how distribute_group_by_no_merge = 1 works. \r\n\r\nI'm using distribute_group_by_no_merge=1 to optimize the following query\r\n\r\noriginal query: \r\n\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\nFROM (\r\n  SELECT client_id\r\n  , Date(ext_ingest_time) AS ext_ingest_date\r\n  FROM test\r\n  WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nAnd I wrote two queries using distributed_group_by_no_merge = 1 as optimized versions\r\n\r\nA:\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\nFROM (\r\n    SELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\n    FROM (\r\n      SELECT client_id\r\n      , Date(ext_ingest_time) AS ext_ingest_date\r\n      FROM test\r\n      WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\n    )\r\n    GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nB: \r\n```\r\nWITH Date(now()) AS in_endDate\r\n    SELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\n    FROM (\r\n        SELECT\r\n            COUNT(DISTINCT client_id) AS tmp\r\n            , Date(ext_ingest_time) AS ext_ingest_date\r\n            FROM test_table\r\n        WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n        AND ext_ingest_time <= in_endDate\r\n        GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nAnd I found the distributed_group_by_no_merge = 1 only works for B.  It seems because I imposed distributed_group_by_no_merge = 1 to a subquery in query A so it doesn't work. \r\n\r\nBut I need professional interpretation about the difference between A and B, as well as how distributed_group_by_no_merge = 1  influences these two queries.\r\n\r\nBesides, I have a few more questions:\r\n1. I know distributed_group_by_no_merge = 1  can be used to optimize `count distinct`. Does it also work for other aggregations like Min, Max, Sum, and Count?\r\n2. Can we use distributed_group_by_no_merge = 1  along with window functions?\r\n\r\nWait for your response! Thanks a lot!"
    },
    "satisfaction_conditions": [
      "Query execution location must be correctly handled between shards and initiator",
      "Data aggregation must occur at shard level before transfer to initiator",
      "Aggregation functions must be compatible with distributed execution",
      "Query structure must avoid unnecessary data movement"
    ],
    "created_at": "2022-03-25T03:10:24Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/35444",
    "source": {
      "issue_number": 35444
    },
    "initial_question": {
      "title": "data_compressed_bytes and data_uncompressed_bytes in system.columns table are zero",
      "body": "Clickhouse version: 22.3.2.1\r\n\r\nI create a MergeTree table and add some data to it, then I want to see how much space is used by its columns querying `system.columns` table, but it returns 0 for both `data_compressed_bytes` and `data_uncompressed_bytes`. I believe it worked a few months ago.\r\n\r\n```sql\r\ncreate table t (number Int32) engine = MergeTree() order by number as\r\nselect * from numbers(100000);\r\n\r\nselect name, type, data_compressed_bytes, data_uncompressed_bytes, compression_codec\r\nfrom system.columns where table = 't';\r\n```\r\nI get the following results:\r\n| name | type | data\\_compressed\\_bytes | data\\_uncompressed\\_bytes | compression\\_codec |\r\n| :--- | :--- | :--- | :--- | :--- |\r\n| number | Int32 | 0 | 0 |  |"
    },
    "satisfaction_conditions": [
      "System.columns table must show non-zero byte values for Wide part types",
      "System.columns table may show zero byte values for Compact part types",
      "Part type status must be queryable through system.parts table",
      "Storage format must be configurable"
    ],
    "created_at": "2022-03-20T14:23:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/35402",
    "source": {
      "issue_number": 35402
    },
    "initial_question": {
      "title": "Error create database with ENGINE = PostgreSQL",
      "body": "hi!\r\non clickhouse-server version  21.4.6.55 i am successfully create database with engine postgresql\r\nCREATE DATABASE b2b ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\nbut on clickhouse-server version 22.2.2.1 i get an error:\r\n```\r\n<Error> executeQuery: Code: 170. DB::Exception: Bad get: has UInt64, requested String. (BAD_GET) (version 22.2.2.1) (from 12\r\n7.0.0.1:35954) (in query: CREATE DATABASE b2b2 ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xaebed1a in /usr/bin/clickhouse\r\n1. DB::Exception::Exception<std::__1::basic_string_view<char, std::__1::char_traits<char> >, DB::Field::Types::Which const&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allo\r\ncator<char> > const&, std::__1::basic_string_view<char, std::__1::char_traits<char> >&&, DB::Field::Types::Which const&) @ 0xafdf5a0 in /usr/bin/clickhouse\r\n2. auto& DB::Field::safeGet<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >() @ 0xba85663 in /usr/bin/clickhouse\r\n3. ? @ 0x14bcd930 in /usr/bin/clickhouse\r\n4. DB::DatabaseFactory::getImpl(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>) @ 0x14bc\r\nb7eb in /usr/bin/clickhouse\r\n5. DB::DatabaseFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>) @ 0x14bc8f47\r\n in /usr/bin/clickhouse\r\n6. DB::InterpreterCreateQuery::createDatabase(DB::ASTCreateQuery&) @ 0x14bb0237 in /usr/bin/clickhouse\r\n7. DB::InterpreterCreateQuery::execute() @ 0x14bc5e1b in /usr/bin/clickhouse\r\n8. ? @ 0x14ee8a79 in /usr/bin/clickhouse\r\n9. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x14ee65f5 in\r\n /usr/bin/clickhouse\r\n10. DB::TCPHandler::runImpl() @ 0x159ef43a in /usr/bin/clickhouse\r\n11. DB::TCPHandler::run() @ 0x15a03419 in /usr/bin/clickhouse\r\n12. Poco::Net::TCPServerConnection::start() @ 0x18667a0f in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerDispatcher::run() @ 0x18669e61 in /usr/bin/clickhouse\r\n14. Poco::PooledThread::run() @ 0x1881a549 in /usr/bin/clickhouse\r\n15. Poco::ThreadImpl::runnableEntry(void*) @ 0x18817c40 in /usr/bin/clickhouse\r\n16. start_thread @ 0x817a in /usr/lib64/libpthread-2.28.so\r\n17. __clone @ 0xfcdc3 in /usr/lib64/libc-2.28.so\r\n```\r\nbut if i am manually create database and create tables with engine postgres, everything is fine and I don't get any errors\r\nfor example create table like this work fine:\r\n```\r\nCREATE TABLE b2b.intouch_district(    id Int32,    name String,    city_id Int32)ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'intouch_district', 'login', 'password');\r\n```"
    },
    "satisfaction_conditions": [
      "Maintains compatibility with table creation operations"
    ],
    "created_at": "2022-03-18T10:01:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/35342",
    "source": {
      "issue_number": 35342
    },
    "initial_question": {
      "title": "AggregateFunction is not backward compatible",
      "body": "**Describe the issue**\r\nWe were upgrading our Clickhouse cluster from **21.8.3.44** to **22.1.3.7** and found a lot of our queries are failing due to an error explained below \r\n\r\n**How to reproduce**\r\nCreate table 1\r\n```\r\nCREATE TABLE test.table_1 on cluster '{cluster}'\r\n(\r\n  `date` Date,\r\n  `uniques` AggregateFunction(uniqCombined64(17), Nullable(String))\r\n)\r\nENGINE = MergeTree()\r\nORDER BY date\r\n```\r\n\r\nCreate table 2\r\n```\r\nCREATE TABLE test.table_2 on cluster '{cluster}'\r\n(\r\n  `date` Date,\r\n  `id`  Nullable(String)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY date\r\n```\r\n\r\nInsert data in table 2\r\n```\r\nINSERT INTO test.table_2 (*) VALUES ('2022-04-01', null), ('2022-04-01', '1'), ('2022-04-01', '2'), ('2022-04-01', '3'), ('2022-04-01', '3'), ('2022-04-01', '4'), ('2022-04-01', '5');\r\n```\r\n\r\nInsert data into table 1 using table 1 data\r\n```\r\nINSERT INTO test.table_1 \r\nSELECT\r\n  date,\r\n  uniqCombined64State(17)(id)\r\nFROM test.table_2\r\nGROUP BY date\r\n```\r\n\r\nRun aggregate query on table 1\r\n```\r\nSELECT\r\n  date,\r\n  coalesce(uniqCombined64Merge(uniques), 0) AS uniques\r\nFROM test.table_1\r\nGROUP BY date \r\n```\r\n\r\nVersion **21.8.3.44** gives result\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500uniques\u2500\u2510\r\n\u2502 2022-04-01 \u2502       5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nVersion **22.1.3.7** throws error\r\n```\r\nCode: 43. DB::Exception: Received from localhost:9000. DB::Exception: Illegal type AggregateFunction(uniqCombined64(17), Nullable(String)) of argument for aggregate function uniqCombined64Merge, expected AggregateFunction(uniqCombined64, Nullable(String)) or equivalent type. (ILLEGAL_TYPE_OF_ARGUMENT)\r\n```\r\n\r\n\r\n**Additional context**\r\nThis is blocking us to upgrade to newer version.\r\n"
    },
    "satisfaction_conditions": [
      "Solution must work with Nullable(String) data type"
    ],
    "created_at": "2022-03-16T18:32:44Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/34987",
    "source": {
      "issue_number": 34987
    },
    "initial_question": {
      "title": "Confuse about the difference between background_schedule_pool_size  and background_fetches_pool_size ",
      "body": "I think ReplicatedMerge table has two type background tasks , one is merge and another is fetch parts from another replica. \r\nIn my opinion, background_fetches_pool_size is for fetch and background_pool_size  is for merge, so i confuse why the document for clickhouse say background_schedule_pool_size is about background task for replicated task. Is there another backgraound task for replicated table ? "
    },
    "satisfaction_conditions": [
      "Clear distinction between different background task pools must be explained",
      "Relationship between pool types and specific ReplicatedMergeTree operations must be identified",
      "Historical context of pool separation must be addressed"
    ],
    "created_at": "2022-03-02T10:10:48Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/34861",
    "source": {
      "issue_number": 34861
    },
    "initial_question": {
      "title": "Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build))",
      "body": "\r\nCode: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build))\r\n\r\nhello,\r\nwe build a clickhouse cluster in three machine, after insert data to distribution table,we use cmd 'optimize table dis_alerts on cluster default_cluster FINAL' to merge repeated record,but errors below:\r\n`\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 9.0.16.11 \u2502 9000 \u2502     48 \u2502 Code: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build)) \u2502                   2 \u2502                0 \u2502\r\n\u2502 9.0.16.17 \u2502 9000 \u2502     48 \u2502 Code: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build)) \u2502                   1 \u2502                0 \u2502\r\n\u2502 9.0.16.4  \u2502 9000 \u2502     48 \u2502 Code: 48, e.displayText() = DB::Exception: Method optimize is not supported by storage Distributed (version 21.3.9.83 (official build)) \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n`\r\nfollowing:\r\n\r\n`\r\ndisk table:\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE dis_test.test_alerts\r\n(\r\n    `tenant_id` UInt32,\r\n    `alert_id` String,\r\n    `timestamp` DateTime CODEC(Delta(4), LZ4),\r\n    `alert_data` String,\r\n    `acked` UInt8 DEFAULT 0,\r\n    `ack_time` DateTime DEFAULT toDateTime(0),\r\n    `ack_user` LowCardinality(String) DEFAULT ''\r\n)\r\nENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{layer}-{shard}/dis_test/test_alerts', '{replica}', ack_time)\r\nPARTITION BY tenant_id % 10\r\nORDER BY (tenant_id, timestamp, alert_id)\r\nSETTINGS index_granularity = 8192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nlogic table:\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE TABLE dis_test.dis_alerts\r\n(\r\n    `tenant_id` UInt32,\r\n    `alert_id` String,\r\n    `timestamp` DateTime CODEC(Delta(4), LZ4),\r\n    `alert_data` String,\r\n    `acked` UInt8 DEFAULT 0,\r\n    `ack_time` DateTime DEFAULT toDateTime(0),\r\n    `ack_user` LowCardinality(String) DEFAULT ''\r\n)\r\nENGINE = Distributed('default_cluster', 'dis_test', 'test_alerts', tenant_id) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n`"
    },
    "satisfaction_conditions": [
      "Optimization command must target the underlying storage table, not the Distributed table",
      "Command execution must be performed across all cluster nodes",
      "Duplicate records in the underlying storage tables must be merged",
      "Table structure and distribution logic must remain unchanged"
    ],
    "created_at": "2022-02-24T07:27:33Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/34474",
    "source": {
      "issue_number": 34474
    },
    "initial_question": {
      "title": "Breaking a source table row into multiple rows using materialized views",
      "body": "Is it possible to create two rows in a MV from a single row in a source table?\r\n\r\nExample:\r\nInsert in source table:\r\n|when | col_1 | col_2|\r\n|-----|-------|------|\r\n|datetime | val_1 | val_2|\r\n\r\nI would like to end up with something like this in the Materialized view:\r\n| when | col_x |\r\n|------|-------|\r\n|datetime | val_1|\r\n|datetime | val_2|\r\n\r\nIs it possible to do that? Thanks!!"
    },
    "satisfaction_conditions": [
      "Single source rows must be transformed into multiple result rows",
      "Original column values must be preserved but redistributed",
      "Result must support conditional value placement",
      "Output must maintain data relationships"
    ],
    "created_at": "2022-02-09T19:36:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/34093",
    "source": {
      "issue_number": 34093
    },
    "initial_question": {
      "title": "EXPLAIN SYNTAX doesn't report more than one column in GROUP BY",
      "body": "For instance a query like this:\r\n\r\n```sql\r\nexplain syntax (SELECT sum(number) _number, count(), number, toDate(now()) date FROM numbers(10) GROUP BY number, date);\r\n\r\nEXPLAIN SYNTAX\r\nSELECT\r\n    sum(number) AS _number,\r\n    count(),\r\n    number,\r\n    toDate(now()) AS date\r\nFROM numbers(10)\r\nGROUP BY\r\n    number,\r\n    date\r\n\r\nQuery id: 985a47d6-644e-4821-8a6d-83de0925cfdd\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 SELECT                      \u2502\r\n\u2502     sum(number) AS _number, \u2502\r\n\u2502     count(),                \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     toDate(now()) AS date   \u2502\r\n\u2502 FROM numbers(10)            \u2502\r\n\u2502 GROUP BY number             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nshould report: `GROUP BY number, date` in the last line.\r\n\r\nTested on these versions with the same result: 20.7.2.30, 21.7.4.18, 21.9.5.16, 21.12.3.32, 22.1.2.2"
    },
    "satisfaction_conditions": [
      "EXPLAIN SYNTAX output must accurately reflect the presence of non-constant expressions in GROUP BY clause",
      "Constant expressions must be automatically eliminated from GROUP BY output explanation",
      "Multiple GROUP BY columns must be displayed when they are non-constant"
    ],
    "created_at": "2022-01-28T12:21:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/34009",
    "source": {
      "issue_number": 34009
    },
    "initial_question": {
      "title": "Do we consider add alignment for `ThreadStatus`",
      "body": "Do we consider add alignment for class `ThreadStatus` like this to reduce false sharing of cpu cache between multiple threads ? \r\n``` cpp\r\nclass __attribute__((__aligned__(64))) ThreadStatus : public boost::noncopyable\r\n``` \r\n"
    },
    "satisfaction_conditions": [
      "Thread stack variables must maintain cache-efficient access patterns",
      "Thread-local data must be sufficiently isolated in memory",
      "Performance impact from cache line sharing between threads must be minimized",
      "The solution must not introduce unnecessary memory overhead"
    ],
    "created_at": "2022-01-26T11:01:34Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/33498",
    "source": {
      "issue_number": 33498
    },
    "initial_question": {
      "title": "Unexpected behaviour of AggregatingMergeTree",
      "body": "**Describe what's wrong**\r\n\r\n> **AggregatingMergeTree** does not respect expected behaviour for fields of type `SimpleAggregateFunction(max, Array(Tuple(url String, status_code UInt16)))`. \r\n\r\n**Does it reproduce on recent release?**\r\n\r\n> Tested on Clickhouse 21.13.1.1 \r\n\r\n**How to reproduce**\r\n\r\n* ClickHouse server version: 21.13.1.1\r\n* Create a table \r\n> `CREATE TABLE test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples` SimpleAggregateFunction(groupUniqArrayArray(2), Array(Tuple(\r\n        url String,\r\n        status_code UInt16\r\n    )))\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id`\r\n\r\n* Inspect created table: `SHOW CREATE TABLE test_tuple_aggreg`\r\n> Result: `CREATE TABLE default.test_tuple_aggreg\r\n(\r\n    `id` UInt32,\r\n    `session_date` Date,\r\n    `tuples.url` Array(String),\r\n    `tuples.status_code` Array(UInt16)\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY session_date\r\nORDER BY id\r\nSETTINGS index_granularity = 8192`\r\n\r\nWe loose track of `SimpleAggregateFunction`.\r\n\r\n* Populate the table to see if correct aggregations are used\r\n>  * `INSERT INTO `test_tuple_aggreg` VALUES (1, '2022-01-02', ['url1'], [500])`\r\n>  * `INSERT INTO `test_tuple_aggreg` VALUES (1, '2022-01-03', ['url2'], [404])`\r\n>  * `INSERT INTO `test_tuple_aggreg` VALUES (1, '2022-01-04', ['url3'], [503])`\r\n\r\n* When we inspect the data we see all 3 rows\r\n> select * from test_tuple_aggreg \r\n   `\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1']   \u2502 [500]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1']   \u2502 [500]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-04 \u2502 ['url3']   \u2502 [503]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518`\r\n\r\n* When we optimise it we see that wrong aggregation is picked for `tuples` fields\r\n> select * from test_tuple_aggreg final\r\n   `\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1']   \u2502 [500]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518`\r\n\r\n**Expected behavior**\r\n\r\nPreserved schema with `SimpleAggregateFunction` and correct aggregation of rows according to created schema `groupUniqArrayArray(2)`.\r\n\r\nExpected aggregation result:\r\n`\u250c\u2500id\u2500\u252c\u2500session_date\u2500\u252c\u2500tuples.url\u2500\u252c\u2500tuples.status_code\u2500\u2510\r\n\u2502  1 \u2502   2022-01-02 \u2502 ['url1', 'url2']   \u2502 [500, 404]              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518`"
    },
    "satisfaction_conditions": [
      "Partitioning rules must be respected for data aggregation"
    ],
    "created_at": "2022-01-10T10:49:03Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/33029",
    "source": {
      "issue_number": 33029
    },
    "initial_question": {
      "title": "Why can not the \u201cmodify setting\u201d operation be synchronized to the other replica",
      "body": "If I modify setting in one replica, the other replica will not be modified synchronously.\r\n\r\n```\r\nvoid StorageReplicatedMergeTree::alter(\r\n    const AlterCommands & commands, const Context & query_context, TableLockHolder & table_lock_holder)\r\n{\r\n    assertNotReadonly();\r\n\r\n    auto table_id = getStorageID();\r\n\r\n    if (commands.isSettingsAlter()) \r\n    {\r\n        /// We don't replicate storage_settings_ptr ALTER. It's local operation.\r\n        /// Also we don't upgrade alter lock to table structure lock.\r\n        StorageInMemoryMetadata future_metadata = getInMemoryMetadata();\r\n        commands.apply(future_metadata, query_context);\r\n\r\n        merge_strategy_picker.refreshState();\r\n\r\n        changeSettings(future_metadata.settings_changes, table_lock_holder);\r\n\r\n        DatabaseCatalog::instance().getDatabase(table_id.database_name)->alterTable(query_context, table_id, future_metadata);\r\n        return;\r\n    }\r\n ...\r\n}\r\n```\r\n\r\nCould you tell me why not the \u201cmodify setting\u201d operation be synchronized to the other replica.\r\n"
    },
    "satisfaction_conditions": [
      "Storage settings must remain independent between replicas",
      "Non-storage settings must still synchronize between replicas",
      "Storage setting modifications must persist locally",
      "The system must clearly distinguish between storage and non-storage settings"
    ],
    "created_at": "2021-12-22T02:25:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/32813",
    "source": {
      "issue_number": 32813
    },
    "initial_question": {
      "title": "Clickhouse Materialized Views select column with different date",
      "body": "Hi guys,\r\n\r\ni have an issue when select MV column with different date period, example:\r\n\r\nsource table\r\n```\r\ncreate table if not exists source(\r\n      `original_timestamp` DateTime64(3) DEFAULT parseDateTimeBestEffort('0001-1-1 23:00:00') CODEC(DoubleDelta, LZ4),\r\n      `event` LowCardinality(String) DEFAULT '',\r\n      `identity` String DEFAULT ''\r\n    ) \r\n    ENGINE = ReplacingMergeTree()\r\n    partition by toWeek(identity,original_timestamp)\r\n        order by (id)\r\n        SETTINGS index_granularity = 8192\r\n```\r\ntarget table\r\n```\r\ncreate table if not exists target(\r\n     day DateTime,\r\n     identity String,\r\n     login AggregateFunction(countIf, String, UInt8),\r\n     register AggregateFunction(countIf, String, UInt8)\r\n    ) \r\n    ENGINE = AggregatingMergeTree()\r\n    partition by toWeek(day)\r\n        order by (day,identity)\r\n        SETTINGS index_granularity = 8192\r\n```\r\nmv table\r\n```\r\nCREATE MATERIALIZED VIEW if not exists target_mv \r\n    to target as\r\n    select\r\n    toStartOfDay(original_timestamp) as day,\r\n    identity,\r\n    countIf(event, event='login') as login,\r\n    countIf(event, event='register') as register\r\n    from source\r\n    group by identity, day\r\n```\r\n\r\nquery:\r\nselect\r\ncountIfMerge(login) as totalLoginEvent, //login event in the last 3 days\r\ncountIfMerge(register) as totalRegisterEvent //register event in the last 7 days\r\nfrom target\r\ngroup by identity,day\r\n\r\nexample expected result: totalLoginEvent is 10 (in the last 3 days) and totalRegisterEvent is 40 event in the last 7 days\r\n\r\nplease help, thanks."
    },
    "satisfaction_conditions": [
      "Query must correctly aggregate login events from the last 3 days",
      "Query must correctly aggregate register events from the last 7 days",
      "Results must be aggregated at a daily granularity",
      "Query must handle data across different time periods without double counting",
      "Results must be filterable by event type"
    ],
    "created_at": "2021-12-15T18:08:29Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/31306",
    "source": {
      "issue_number": 31306
    },
    "initial_question": {
      "title": "Is shard weight change applied dynamically?",
      "body": "Hi all!\r\nI have a question relating to the re-sharding of the cluster. We have expanded our cluster by adding one more shard. Since we want the new shard to handle more load in terms of writes, we adjusted our weights in the configs for each pertaining shard, e.g., \r\n`<weight>9</weight>`. Everything seems working fine; however, we wanted to know whether the restart is required for this to take into effect or is it picked up and applied dynamically? Please advise."
    },
    "satisfaction_conditions": [
      "Shard weight changes must be reflected in the cluster configuration without system restart",
      "Weight changes must be observable in the system"
    ],
    "created_at": "2021-11-11T16:44:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/30814",
    "source": {
      "issue_number": 30814
    },
    "initial_question": {
      "title": "How to add versioning column for existing ReplicatedReplacingMergedTree-engine table",
      "body": "Hi guys,\r\n\r\nI created a table with ReplicatedReplacingMergedTree engine (no versioning column specifed).\r\n\r\nI would like to ask if there is a way to ALTER the table and add a versioning column? \r\n\r\nI have 500GB of data on the table already so it would be not easy to create a new table and insert everything again.\r\n\r\nMany thanks!"
    },
    "satisfaction_conditions": [
      "The table must maintain data integrity during version column addition",
      "The versioning column must be successfully added to the ReplicatedReplacingMergedTree table structure",
      "All replicas must remain consistent after the modification",
      "The solution must handle ongoing replication configuration"
    ],
    "created_at": "2021-10-28T15:29:49Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/29975",
    "source": {
      "issue_number": 29975
    },
    "initial_question": {
      "title": "Is it possible to calculate number of distinct keys in map?",
      "body": "I have a map type field. Is there a way to calculate how many times each of distinct keys encounter in the whole table?\r\n\r\nSay I have a table from the example in the docs:\r\n\r\n`CREATE TABLE table_map (a Map(String, UInt64)) ENGINE=Memory;`\r\n`INSERT INTO table_map VALUES ({'key1':1, 'key2':10}), ({'key1':2,'key2':20}), ({'key1':3,'key2':30});`\r\n\r\nCan I write a query like this?\r\n\r\n`SELECT\r\n    a.keys,\r\n    count(a.keys) AS cnt\r\nFROM table_map\r\nGROUP BY a.keys\r\nORDER BY cnt DESC`\r\n\r\nThis query counts distinct set of keys, but can it be rewritten to count keys?"
    },
    "satisfaction_conditions": [
      "Query returns the count of occurrences for each unique key in the map",
      "Results must be presented as key-count pairs",
      "Query must handle multiple map entries across different rows",
      "Query processes map keys independently of their associated values"
    ],
    "created_at": "2021-10-11T01:49:23Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/29620",
    "source": {
      "issue_number": 29620
    },
    "initial_question": {
      "title": "Non standart aggregation with GROUP BY",
      "body": "** SQL **\r\n```\r\nSELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL;\r\n```\r\nCurrent result:\r\n\r\n```\r\n\u250c\u2500max(1)\u2500\u2510\r\n\u2502      0 \r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nExpected SQL Standart behavior (MySQL, sqlite, etc):\r\n\r\n```\r\nmysql> SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL;\r\nEmpty set (0,00 sec)\r\n```\r\n\r\nBut with a little changed SQL all ok\r\n\r\n```\r\nSELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0;\r\n```\r\n\r\nClickhouse\r\n```\r\n\u250c\u2500max(1)\u2500\u2510\r\n\u2502      0\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nMysql\r\n```\r\nmysql> SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0;\r\n+--------+\r\n| MAX(1) |\r\n+--------+\r\n|   NULL |\r\n+--------+\r\n1 row in set (0,00 sec)\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Query returns empty result set when using GROUP BY NULL with no matching rows",
      "Aggregate function returns NULL for empty input when specified",
      "Configuration option exists to control empty aggregate behavior",
      "Version compatibility is maintained for expected behavior"
    ],
    "created_at": "2021-10-01T12:31:12Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/29113",
    "source": {
      "issue_number": 29113
    },
    "initial_question": {
      "title": "Quota doesn't seem to work",
      "body": "clickhouse version: 21.8.5.7\r\n\r\nwhy user1 queries  is 265? any config missed?\r\n```sql\r\nSELECT\r\n    user,\r\n    count()\r\nFROM system.processes\r\nGROUP BY user\r\n\r\n\u250c\u2500user\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 user1                           \u2502     265 \u2502\r\n\u2502 default                         \u2502       3 \u2502\r\n\u2502 user2                           \u2502      88 \u2502\r\n\u2502 user3                           \u2502       1 \u2502\r\n\u2502 user4                           \u2502       5 \u2502\r\n\u2502 user5                           \u2502      11 \u2502\r\n\u2502 user6                           \u2502       6 \u2502\r\n\u2502 user7                           \u2502      12 \u2502\r\n\u2502 user8                           \u2502       1 \u2502\r\n\u2502 user9                           \u2502     109 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    name,\r\n    storage,\r\n    apply_to_list\r\nFROM quotas\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500storage\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500apply_to_list\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 default                                \u2502 users.xml       \u2502 ['default']                         \u2502\r\n\u2502 qps_20_user1                           \u2502 local directory \u2502 ['user1']                           \u2502\r\n\u2502 qps_20_user2                           \u2502 local directory \u2502 ['user2']                           \u2502\r\n\u2502 qps_20_user3                           \u2502 local directory \u2502 ['user3']                           \u2502\r\n\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    quota_name,\r\n    duration,\r\n    max_queries,\r\n    max_query_selects,\r\n    max_query_inserts\r\nFROM quota_limits\r\n\r\n\u250c\u2500quota_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500duration\u2500\u252c\u2500max_queries\u2500\u252c\u2500max_query_selects\u2500\u252c\u2500max_query_inserts\u2500\u2510\r\n\u2502 default                                \u2502     3600 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502              \u1d3a\u1d41\u1d38\u1d38 \u2502              \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2502 qps_20_user1                           \u2502        1 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502                20 \u2502                20 \u2502\r\n\u2502 qps_20_user2                           \u2502        1 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502                20 \u2502                20 \u2502\r\n\u2502 qps_20_user3                           \u2502        1 \u2502        \u1d3a\u1d41\u1d38\u1d38 \u2502                20 \u2502                10 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nanother question. should i create a quota for each user?"
    },
    "satisfaction_conditions": [
      "Quota enforcement must be active for queries on real tables",
      "Query limits must be properly enforced within the specified time interval",
      "Quota configuration must be applied to all intended users",
      "System must distinguish between different query types for quota enforcement"
    ],
    "created_at": "2021-09-17T07:19:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/28903",
    "source": {
      "issue_number": 28903
    },
    "initial_question": {
      "title": "clickhouse server crashes after starts ",
      "body": "error message below\r\n```\r\n2021.09.12 01:23:44.437039 [ 18936 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.09.12 01:23:44.437087 [ 18936 ] {} <Fatal> BaseDaemon: (version 20.8.3.18, no build id) (from thread 18818) (no query) Received signal Segmentation fault (11)\r\n2021.09.12 01:23:44.437113 [ 18936 ] {} <Fatal> BaseDaemon: Address: 0x7f78fb378000 Access: write. Attempted access has violated the permissions assigned to the memory area.\r\n2021.09.12 01:23:44.437126 [ 18936 ] {} <Fatal> BaseDaemon: Stack trace: 0x14b99c2f 0x14b9e7ad 0x14b9fd36 0x14b98e78 0x14ba17ef 0x14ba1b33 0x9e7e7ea 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d\r\n2021.09.12 01:23:44.437163 [ 18936 ] {} <Fatal> BaseDaemon: 3. ? @ 0x14b99c2f in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437171 [ 18936 ] {} <Fatal> BaseDaemon: 4. ? @ 0x14b9e7ad in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437177 [ 18936 ] {} <Fatal> BaseDaemon: 5. ? @ 0x14b9fd36 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437186 [ 18936 ] {} <Fatal> BaseDaemon: 6. unw_step @ 0x14b98e78 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437193 [ 18936 ] {} <Fatal> BaseDaemon: 7. ? @ 0x14ba17ef in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437203 [ 18936 ] {} <Fatal> BaseDaemon: 8. _Unwind_Resume @ 0x14ba1b33 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437212 [ 18936 ] {} <Fatal> BaseDaemon: 9. ? @ 0x9e7e7ea in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437224 [ 18936 ] {} <Fatal> BaseDaemon: 10. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437236 [ 18936 ] {} <Fatal> BaseDaemon: 11. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437259 [ 18936 ] {} <Fatal> BaseDaemon: 12. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437267 [ 18936 ] {} <Fatal> BaseDaemon: 13. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437274 [ 18936 ] {} <Fatal> BaseDaemon: 14. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437281 [ 18936 ] {} <Fatal> BaseDaemon: 15. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437288 [ 18936 ] {} <Fatal> BaseDaemon: 16. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437295 [ 18936 ] {} <Fatal> BaseDaemon: 17. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437302 [ 18936 ] {} <Fatal> BaseDaemon: 18. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437309 [ 18936 ] {} <Fatal> BaseDaemon: 19. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437315 [ 18936 ] {} <Fatal> BaseDaemon: 20. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437322 [ 18936 ] {} <Fatal> BaseDaemon: 21. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437329 [ 18936 ] {} <Fatal> BaseDaemon: 22. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437341 [ 18936 ] {} <Fatal> BaseDaemon: 23. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437348 [ 18936 ] {} <Fatal> BaseDaemon: 24. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437355 [ 18936 ] {} <Fatal> BaseDaemon: 25. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437362 [ 18936 ] {} <Fatal> BaseDaemon: 26. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437369 [ 18936 ] {} <Fatal> BaseDaemon: 27. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437376 [ 18936 ] {} <Fatal> BaseDaemon: 28. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437382 [ 18936 ] {} <Fatal> BaseDaemon: 29. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437389 [ 18936 ] {} <Fatal> BaseDaemon: 30. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437396 [ 18936 ] {} <Fatal> BaseDaemon: 31. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n\r\n```\r\nScenario\uff1aour clickhouse cluster runs stably for a time, yesterday a beginner developer runs a lot of \"alter table\" query try to \"update\" a field in a big table, the server crashed. then we add \r\n```\r\n<cleanup_delay_period>60</cleanup_delay_period>\r\n<task_max_lifetime>60</task_max_lifetime>\r\n<max_tasks_in_queue>200</max_tasks_in_queue>\r\n```\r\nthis to the clickhouse config to clean up the DDL queue. after that, some of clickhouse servers could not start up.\r\nAny ideas"
    },
    "satisfaction_conditions": [
      "System has appropriate mutation queue limits configured"
    ],
    "created_at": "2021-09-11T18:16:54Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/28849",
    "source": {
      "issue_number": 28849
    },
    "initial_question": {
      "title": "What if version column in ReplacingMergeTree overflows?",
      "body": "I know there is a possibility to use type `UInt256` for version column in `ReplacingMergeTree`, in which the overflow is probably unrealistic, but still possible. What happens then, when I reach the maximum \"version\" and I need to create a new one? How does ClickHouse handle it? Or how should I handle it? Thank you!"
    },
    "satisfaction_conditions": [
      "System behavior during version overflow must be clearly defined",
      "Version column type must accommodate the expected update frequency",
      "Alternative approaches for version tracking must be considered"
    ],
    "created_at": "2021-09-10T07:29:11Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/28681",
    "source": {
      "issue_number": 28681
    },
    "initial_question": {
      "title": "How to prevent ATTACH in old versions previous to DETACH ... PERMANENTLY",
      "body": "Hello,\r\nI have a problem with an old CH version, where old materializations are reattached after each startup.\r\nHow to remove a materialized view permanently (so it doesn\u2019t reattach on startup) in older ch versions where detach \u2026 permanently is not present? Should I just delete it from the `metadata` folder after DETACH?\r\n\r\nThank you!"
    },
    "satisfaction_conditions": [
      "Materialized view remains detached after system restart",
      "Materialized view definition is permanently removed from system",
      "Solution works with older ClickHouse versions lacking DETACH PERMANENTLY"
    ],
    "created_at": "2021-09-07T10:19:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/28608",
    "source": {
      "issue_number": 28608
    },
    "initial_question": {
      "title": "How to load csv contain json column into clickhouse?",
      "body": "csv\r\n```\r\n\"id\",\"operateDate\",\"operateTime\",\"userid\",\"usertype\",\"targetId\",\"targetName\",\"logType\",\"logSmallType\",\"operateType\",\"clientIp\",\"oldValues\",\"newValues\",\"description\",\"params\",\"logTypeLabel\",\"logSmallTypeLabel\",\"belongtype\",\"belongTypeLabel\",\"belongTypeTargetId\",\"belongTypeTargetName\",\"isDetail\",\"mainId\",\"belongMainId\",\"groupId\",\"groupNameLabel\",\"operateAuditType\",\"isArchived\",\"deviceType\"\r\n1,\"2020-03-16\",\"11:33:14\",1,1,\"1\",\"sysadmin\",3,16,\"LOGIN\",\"192.168.42.51\",\"\",\"\",\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\",\"{\\\"deviceType\\\":1,\\\"lastName\\\":\\\"sysadmin\\\",\\\"loginId\\\":\\\"sysadmin\\\",\\\"clientIp\\\":\\\"192.168.42.51\\\",\\\"userId\\\":1,\\\"desc\\\":\\\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\\\"}\",179,506994,16,506994,\\N,\\N,0,\\N,\\N,\\N,0,\"ERROR\",\\N,\"PC\"\r\n```\r\nThe params column in mysql is of type text\r\n```\r\n| params               | text          | YES  |     | NULL    |                |\r\n```\r\n\r\nThe params column in clickhouse is of type String\r\n```\r\n`params` String COMMENT '\u53c2\u6570',\r\n```\r\nhow can i load this csv into clickhouse?\r\n\r\n```\r\n# clickhouse-client -u default --password superpass --host 127.0.0.1 --port 9000 --format_csv_delimiter=\",\" --query \"INSERT INTO rd_auditlog.ecology_biz_log FORMAT CSVWithNames\" --max_insert_block_size=100000 < infra_fanweioa.ecology_biz_log.000000000.csv \r\nCode: 27. DB::ParsingException: Cannot parse input: expected ',' before: 'deviceType\\\\\":1,\\\\\"lastName\\\\\":\\\\\"sysadmin\\\\\",\\\\\"loginId\\\\\":\\\\\"sysadmin\\\\\",\\\\\"clientIp\\\\\":\\\\\"192.168.42.51\\\\\",\\\\\"userId\\\\\":1,\\\\\"desc\\\\\":\\\\\"\u6388\u6743\u4fe1\u606f\u9519\u8bef\\\\\"}\",179,506994,16,50': \r\nRow 1:\r\nColumn 0,   name: id,                   type: Int64,    parsed text: \"1\"\r\nColumn 1,   name: operateDate,          type: String,   parsed text: \"<DOUBLE QUOTE>2020-03-16<DOUBLE QUOTE>\"\r\nColumn 2,   name: operateTime,          type: String,   parsed text: \"<DOUBLE QUOTE>11:33:14<DOUBLE QUOTE>\"\r\nColumn 3,   name: userid,               type: Int32,    parsed text: \"1\"\r\nColumn 4,   name: usertype,             type: Int32,    parsed text: \"1\"\r\nColumn 5,   name: targetId,             type: String,   parsed text: \"<DOUBLE QUOTE>1<DOUBLE QUOTE>\"\r\nColumn 6,   name: targetName,           type: String,   parsed text: \"<DOUBLE QUOTE>sysadmin<DOUBLE QUOTE>\"\r\nColumn 7,   name: logType,              type: String,   parsed text: \"3\"\r\nColumn 8,   name: logSmallType,         type: String,   parsed text: \"16\"\r\nColumn 9,   name: operateType,          type: String,   parsed text: \"<DOUBLE QUOTE>LOGIN<DOUBLE QUOTE>\"\r\nColumn 10,  name: clientIp,             type: String,   parsed text: \"<DOUBLE QUOTE>192.168.42.51<DOUBLE QUOTE>\"\r\nColumn 11,  name: oldValues,            type: String,   parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nColumn 12,  name: newValues,            type: String,   parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nColumn 13,  name: description,          type: String,   parsed text: \"<DOUBLE QUOTE>\u6388\u6743\u4fe1\u606f\u9519\u8bef<DOUBLE QUOTE>\"\r\nColumn 14,  name: params,               type: String,   parsed text: \"<DOUBLE QUOTE>{<BACKSLASH><DOUBLE QUOTE>\"\r\nERROR: There is no delimiter (,). \"d\" found instead.\r\n\r\n: data for INSERT was parsed from stdin: (at row 1)\r\n```"
    },
    "satisfaction_conditions": [
      "Double quotes within JSON strings in CSV must be properly escaped",
      "CSV data must be successfully parsed by Clickhouse",
      "JSON structure in the params column must be preserved",
      "Non-ASCII characters must be correctly preserved",
      "Column data types must match between source and destination"
    ],
    "created_at": "2021-09-04T10:32:39Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/28506",
    "source": {
      "issue_number": 28506
    },
    "initial_question": {
      "title": "i got a exception when create a table use RabbitMQ engine",
      "body": "the clickhouse run in docker\r\n```\r\nsudo mkdir /var/docker/clickhouse/\r\nsudo mkdir /var/docker/clickhouse/config\r\nsudo mkdir /var/docker/clickhouse/config/config.d\r\necho \"<yandex>\r\n     <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->\r\n    <listen_host>::</listen_host>\r\n    <listen_host>0.0.0.0</listen_host>\r\n    <listen_try>1</listen_try>\r\n\r\n    <!--\r\n    <logger>\r\n        <console>1</console>\r\n    </logger>\r\n    -->\r\n</yandex>\" | sudo tee /var/docker/clickhouse/config/config.d/docker_related_config.xml\r\necho \"<yandex>\r\n <rabbitmq>\r\n    <username>guest</username>\r\n    <password>guest</password>\r\n </rabbitmq>\r\n</yandex>\" | sudo tee /var/docker/clickhouse/config/config.d/rabbit.xml\r\n\r\ndocker container stop clickhouse && docker container rm clickhouse\r\ndocker run -d \\\r\n  --name clickhouse \\\r\n  --restart on-failure \\\r\n  --ulimit nofile=262144:262144 \\\r\n  -p 8123:8123 \\\r\n  -p 9000:9000 \\\r\n  --volume=/var/docker/clickhouse:/var/lib/clickhouse \\\r\n  --volume=/var/docker/clickhouse/config/users.d:/etc/clickhouse-server/users.d \\\r\n  --volume=/var/docker/clickhouse/config/config.d:/etc/clickhouse-server/config.d \\\r\n  yandex/clickhouse-server\r\n\r\n```\r\nrabbitMQ is docker also\r\n```\r\nsudo docker run \\\r\n  -d \\\r\n  --name rabbitmq \\\r\n  -p 5672:5672 \\\r\n  -p 15672:15672 \\\r\n  rabbitmq:management\r\n```\r\n\r\nthen i create a teble and query it\r\n```\r\nCREATE TABLE queue\r\n(\r\n\tkey   UInt64,\r\n\tvalue UInt64,\r\n\tdate  DateTime\r\n) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672',\r\n\trabbitmq_exchange_name = 'exchange1',\r\n\trabbitmq_format = 'JSONEachRow',\r\n\trabbitmq_num_consumers = 5,\r\n\tdate_time_input_format = 'best_effort';\r\n\r\nselect *\r\nfrom queue;\r\n```\r\n\r\ni got a exception\r\n`Code: 530, e.displayText() = DB::Exception: RabbitMQ setup not finished. Connection might be lost (version 21.8.4.51 (official build))`\r\n\r\nthe log of clickhouse\r\n```\r\n2021.09.02\u00a009:10:26.877248\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0RabbitMQConnectionTask:\u00a0Execution\u00a0took\u00a04002\u00a0ms.\r\n2021.09.02\u00a009:10:27.377358\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Trying\u00a0to\u00a0restore\u00a0connection\u00a0to\u00a0localhost:5672\r\n2021.09.02\u00a009:10:27.577754\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Error>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Library\u00a0error\u00a0report:\u00a0connection\u00a0lost\r\n2021.09.02\u00a009:10:31.379727\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0RabbitMQConnectionTask:\u00a0Execution\u00a0took\u00a04002\u00a0ms.\r\n2021.09.02\u00a009:10:31.879822\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Trying\u00a0to\u00a0restore\u00a0connection\u00a0to\u00a0localhost:5672\r\n2021.09.02\u00a009:10:32.080192\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Error>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Library\u00a0error\u00a0report:\u00a0connection\u00a0lost\r\n```\r\n\r\nclickhouse :\r\nselect version();=21.8.4.51\r\nrabbitmq is version 3.9.5\r\n\r\nsystem: windows 10 wsl debian\r\n"
    },
    "satisfaction_conditions": [
      "Network connectivity exists between ClickHouse and RabbitMQ containers",
      "RabbitMQ host address is correctly resolvable from ClickHouse container",
      "ClickHouse can establish and maintain a persistent connection to RabbitMQ",
      "RabbitMQ connection parameters in table creation match actual RabbitMQ endpoint"
    ],
    "created_at": "2021-09-02T09:17:27Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/28005",
    "source": {
      "issue_number": 28005
    },
    "initial_question": {
      "title": "MergeTreeThread has feature spill data to disk?",
      "body": "when use sql to query distributed tables \"select * from dbname.tablename_all\", no limitations, it is easy to appear such exception:\r\n\"_DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 93.38 GiB (attempt to allocate chunk of 362340215 bytes), maximum: 93.13 GiB: **While executing MergeTreeThread.**_\"\r\n\r\nwhen the memory is not enough, should be spill on disk, does it has this feature \"While executing MergeTreeThread\" "
    },
    "satisfaction_conditions": [
      "Query execution completes without memory limit exceptions",
      "Large dataset query completes successfully",
      "Resource consumption remains within system limits"
    ],
    "created_at": "2021-08-23T03:29:16Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/27470",
    "source": {
      "issue_number": 27470
    },
    "initial_question": {
      "title": "hdfs engine with hive default delimiter '0x01'",
      "body": "```sql\r\ncreate table hdfs_engine_table_1 on cluster datacenter\r\n(\r\n    name String,\r\n    address String\r\n)\r\n    engine = HDFS('hdfs://ns/user/hive/warehouse/a/b/*', 'CSV');\r\n```\r\n\r\nwhat format should i use?"
    },
    "satisfaction_conditions": [
      "CSV format configuration must support 0x01 (SOH) delimiter",
      "HDFS engine must correctly read delimited data from specified path",
      "Field values must correctly map to the defined String columns"
    ],
    "created_at": "2021-08-09T12:49:30Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/26528",
    "source": {
      "issue_number": 26528
    },
    "initial_question": {
      "title": "difference between truncate table default.SampleTable vs delete /var/lib/clickhouse/data/default/SampleTable",
      "body": "Hello all,\r\n\r\nTo remove all data from a table(i.e.  default.SampleTable),\r\n\r\n```\r\ntruncate table default.SampleTable;\r\n\r\nrm -fr /var/lib/clickhouse/data/default/SampleTable/*\r\n```\r\n\r\nIs there any difference between the above two operations?\r\n\r\nThank you\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Data removal operation must maintain database consistency",
      "Operation must be safe with concurrent operations",
      "Server state must remain valid after operation",
      "Operation must respect table engine requirements"
    ],
    "created_at": "2021-07-19T20:59:31Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/26493",
    "source": {
      "issue_number": 26493
    },
    "initial_question": {
      "title": "[PART AND PARTITION] Would all the parts of one partition be merged into a single part finally? ",
      "body": "We are trying to use  clickhouse-local to do pre-computing of part\u3002 \r\n\r\nAs we all know\uff0cone partition has many parts\uff0c but will all the parts in one partition be merged into one part finally\uff1f\r\n\r\nIf they will  be merged into one part some how,  will single execution of  \"OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [PARTITION partition | PARTITION ID 'partition_id'] [FINAL]\" command give the final result ?\r\n\r\nThx!\r\n\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Parts merging behavior must be correctly understood as non-guaranteed",
      "Merge limitations must be accurately identified",
      "OPTIMIZE FINAL command behavior must be correctly explained",
      "Resource dependency must be acknowledged"
    ],
    "created_at": "2021-07-19T08:28:00Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/26242",
    "source": {
      "issue_number": 26242
    },
    "initial_question": {
      "title": "how to write the sql using clickhouse, if i want to get the aggregate results fo each items select from table?",
      "body": "search all the ways from google to sf, no same questions.\r\n\r\nsay i have a table\uff0cwhich is about 1T large:\r\n\r\nuser_id|  pay_time | pay_info\r\n-|-|-\r\n1 | 1232323 | {'num':10, \"total\":100}\r\n1  |1232324 | {'num':11, \"total\":110}\r\n1  |1232325  |{'num':12, \"total\":120}\r\n2  |1232326 | {'num':13, \"total\":130}\r\n2  |1232327 | {'num':14, \"total\":140}\r\n2  |1232328 | {'num':15, \"total\":150}\r\n2  |1232329 | {'num':16, \"total\":160}\r\n\r\nhow i get each user' sum total or sum num when he make payments,   he has already spent, which is pay_time less than this current payment's paytime. results as follows:\r\n\r\nuser_id | pay_time | sum_num| sum_total\r\n-|-|-|-\r\n1 | 1232323  |0 |0\r\n1|  12323234 |10| 100\r\n1 | 12323234 |21 |210\r\n2 | 1232326  |0 |0 \r\n2| 1232327   |13 |130\r\n2  |1232328  |27 |270\r\n2 | 1232329  |42 |420\r\n\r\ni have read the docs, but it seems no results.  \r\n\r\nand allow_experimental_window_functions seems not working, dont know why.\r\n\r\nthanks in advance\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Query must calculate running totals excluding current row",
      "Results must be grouped by user_id",
      "Results must be ordered by user_id and pay_time",
      "First row for each user must show zero totals",
      "Query must handle JSON-formatted pay_info column",
      "Query must be compatible with large datasets"
    ],
    "created_at": "2021-07-12T10:16:19Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/25953",
    "source": {
      "issue_number": 25953
    },
    "initial_question": {
      "title": "Is it possible to change zkpath for a ReplicatedMergeTree table?",
      "body": "At first I have created a distributed table (let us name it **table_A**). And then I decided to move the data to a ReplicatedMergeTable. I did it as below:\r\n\r\n`create table table_B as table_A engine=ReplicatedMergeTree('/clickhouse/tables/01/{database}/{table}', '{replica}');`\r\n`insert into table_B select * from table_A;`\r\n\r\nSo far so good. After that, I drop the distributed table and rename the new one to the old name.\r\n\r\n`drop table table_A;`\r\n`rename table table_B to table_A;`\r\n\r\nThen I tried to insert some data into table_A, it threw an error:\r\n\r\n`Table is in readonly mode (zookeeper path: /clickhouse/tables/01/default/table_B)`\r\n\r\nI have tried to find some command to fix the zookeeper path for this table as alter table and etc. but nothing found.\r\n\r\nIs it a solution to make it out?\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Table becomes writable after solution implementation",
      "ZooKeeper path reference matches the actual table name",
      "Changes persist after ClickHouse service restarts"
    ],
    "created_at": "2021-07-03T18:14:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/25698",
    "source": {
      "issue_number": 25698
    },
    "initial_question": {
      "title": "Populating a materialized view results in unexpected values",
      "body": "Please forgive me if i'm missing something fairly obvious here.\r\n\r\n**Describe the unexpected behaviour**\r\nPopulating a materialized view results in unexpected values.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: 21.3.13.9\r\n\r\n```bash\r\ndocker run -d --name some-clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:21.3.13.9\r\ndocker exec -it some-clickhouse-server clickhouse-client\r\n\r\n:) CREATE TABLE tmp_aggregated\r\n(\r\n  `window_start` DateTime64 Codec(DoubleDelta, LZ4),\r\n  `metrics_name` Array(LowCardinality(String)) Codec(LZ4),\r\n  `organization_id` LowCardinality(String) Codec(LZ4)\r\n)\r\nENGINE MergeTree()\r\nPARTITION BY (organization_id) ORDER BY (window_start)\r\n\r\n:) create materialized view tmp_names (\r\n  organization_id LowCardinality(String),\r\n  metric_names SimpleAggregateFunction(groupUniqArrayArray, Array(String)),\r\n  window_start_day DateTime64\r\n)\r\nEngine=MergeTree()\r\norder by (window_start_day)\r\npopulate as select\r\n  organization_id,\r\n  groupUniqArray(metrics_name),\r\n  toStartOfDay(window_start)\r\nfrom tmp_aggregated array join metrics_name\r\ngroup by toStartOfDay(window_start), organization_id\r\n\r\n:) insert into tmp_aggregated values ('2021-06-24 07:15:09.000', ['metric1'], 'org-id');\r\n\r\n:) select * from tmp_names \\G\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\norganization_id:  org-id\r\nmetric_names:     []\r\nwindow_start_day: 1970-01-01 00:00:00\r\n\r\n:) select * from tmp_aggregated \\G\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nwindow_start:    2021-06-24 07:15:09.000\r\nmetrics_name:    ['metric1']\r\norganization_id: org-id\r\n\r\n```\r\n\r\n**Expected behavior**\r\n\r\nWhen executing `select * from tmp_names \\G` I expected values stored in tmp_names to be:\r\n\r\n```\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\norganization_id:  org-id\r\nmetric_names:      ['metric1']\r\nwindow_start_day: 2021-06-24 07:15:09.000\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n"
    },
    "satisfaction_conditions": [
      "Materialized view must correctly preserve the data types from source to target"
    ],
    "created_at": "2021-06-25T06:19:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/25322",
    "source": {
      "issue_number": 25322
    },
    "initial_question": {
      "title": "Kafka _timestamp / _timestamp_ms not working?",
      "body": "**Describe the bug**\r\n\r\nAppears that _timestamp and _timestamp_ms virtual columns are returning 0 for kafka. Perhaps it is somehow due to JSONEachRow format?\r\n\r\nIssue is in docker 21.6.4.26 and 21.2.2.8 at least.\r\n\r\nI'm using amazon MSK 2.6.1\r\n\r\n**How to reproduce**\r\n\r\n```\r\ncreate table kafka ( name String ) ENGINE=Kafka() SETTINGS kafka_broker_list = '...', kafka_topic_list = 'test-events', kafka_group_name='test', kafka_format='JSONEachRow';\r\ncreate table t (time DateTime64(3), name String) ENGINE=MergeTree() ORDER BY tuple();\r\ncreate materialized view kafka_mv to t AS select _timestamp_ms, name FROM kafka;\r\n```\r\n\r\n```\r\necho '{\"name\":\"test\"}' | bin/kafka-console-producer.sh ...\r\n```\r\n\r\nClickhouse table just looks like:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u252c\u2500name\u2500\u2510\r\n\u2502 1970-01-01 00:00:00.000 \u2502 test \u2502\r\n...\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIf checking with kafka consumer:\r\n\r\n```\r\nbin/kafka-console-consumer.sh ... --from-beginning --property print.timestamp=true\r\nCreateTime:1623839664050        {\"name\":\"test\"}\r\nCreateTime:1623839989619        {\"name\":\"test\"}\r\nCreateTime:1623840562909        {\"name\":\"test\"}\r\nCreateTime:1623840285113        {\"name\":\"test\"}\r\n```\r\n\r\nI see the same with _timestamp col"
    },
    "satisfaction_conditions": [
      "Column mapping between source and target must be explicitly defined"
    ],
    "created_at": "2021-06-16T10:57:19Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/24575",
    "source": {
      "issue_number": 24575
    },
    "initial_question": {
      "title": "Question about shared_ptr_helper",
      "body": "I have a question about this template class.\r\n```c++\r\n/** Allows to make std::shared_ptr from T with protected constructor.\r\n  *\r\n  * Derive your T class from shared_ptr_helper<T> and add shared_ptr_helper<T> as a friend\r\n  *  and you will have static 'create' method in your class.\r\n  */\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\n\r\nMany places use this pattern\r\nclass StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageReplicatedMergeTree>, public MergeTreeData\r\n{\r\n    friend struct ext::shared_ptr_helper<StorageReplicatedMergeTree>;\r\n..\r\n};\r\n\r\nBut I think the friend class is redundant.\r\n\r\nFor example\r\n\r\n#include <iostream>\r\n#include <memory>\r\n\r\nusing namespace std;\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\nclass A:public shared_ptr_helper<A>{\r\n    public:\r\n    int a;\r\n};\r\n\r\nint main(){\r\n    std::shared_ptr<A> aObj = A::create();\r\n    cout << aObj->a << endl;\r\n}\r\n```\r\n\r\nThis code also can use the create method. Here is the question, what does the friend class do in this pattern. \r\nThanks.\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Class constructor accessibility must be controllable",
      "Helper class must be able to create instances despite constructor restrictions",
      "Public creation method must be available",
      "Objects must be returned as shared pointers",
      "Constructor arguments must be forwarded correctly"
    ],
    "created_at": "2021-05-27T13:44:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/24251",
    "source": {
      "issue_number": 24251
    },
    "initial_question": {
      "title": " DB::Exception: Aggregate function sum(postition) is found inside another aggregate function in query: While processing sum(postition) AS postition",
      "body": "Hi\uff0c\r\n    When I execute this query sql :\r\nSELECT\r\n    avg(postition) AS avg,\r\n    sum(postition) AS postition\r\nFROM system.columns;\r\nand the exception happened,which was:\r\nReceived exception from server (version 21.4.4):\r\nCode: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function sum(postition) is found inside another aggregate function in query: While processing sum(postition) AS postition. \r\n\r\nBut this sql can run correctly in MySQL. This is Clickhouse's  special syntax ? \r\n        Thanks.\r\n                     Best Regards.\r\n                             Eward\r\n "
    },
    "satisfaction_conditions": [
      "Query executes without aggregate function nesting error",
      "Solution accounts for ClickHouse's specific aggregate function handling"
    ],
    "created_at": "2021-05-18T14:52:41Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/24072",
    "source": {
      "issue_number": 24072
    },
    "initial_question": {
      "title": "Should uniq or uniqCombined be used for sharded, aggregated uniques?",
      "body": "the documentation for uniqCombined says:\r\n\r\n```\r\nCompared to the uniq function, the uniqCombined:\r\nConsumes several times less memory.\r\nCalculates with several times higher accuracy.\r\nUsually has slightly lower performance. In some scenarios, uniqCombined can perform better than uniq, for example, with distributed queries that transmit a large number of aggregation states over the network.\r\n```\r\n\r\nThe documentation for uniq says:\r\n\r\n```\r\nWe recommend using this function in almost all scenarios.\r\n```\r\n\r\nWe have \"distributed queries that transmit a large number of aggregation states\" -- so should we be using uniqCombined?  If so should the \"uniq\" recommendation be updated to reflect that?  Or is the uniqCombined documentation out of date?\r\n\r\n(On a side note, I don't suppose there's a way to convert uniqState to uniqCombinedState for existing data?)"
    },
    "satisfaction_conditions": [
      "Query performance improves for distributed scenarios with large aggregation states",
      "Memory usage is optimized compared to standard unique counting",
      "Accuracy remains within acceptable error margins",
      "Storage space requirements are reduced for aggregation states",
      "Aggregation merge operations execute efficiently"
    ],
    "created_at": "2021-05-12T20:21:43Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/24034",
    "source": {
      "issue_number": 24034
    },
    "initial_question": {
      "title": "how to pass client settings such like max_memory_useage for  http query",
      "body": "in some cases,i can only run select  query via http interface ,like curl or wget,but some times it will out of memory limit\r\n\r\nhow to set these client settings just like in the clickhouse-client when using http interface? \r\n\r\n\r\nby setting  a small value in settings at the end of  select  query,it shows Memory limit (for query) exceeded\r\n\r\n\r\nSELECT\r\n    CAST('2021-05-10', 'date') AS log_date,\r\n    countDistinct(map_uid) AS uid_cnt\r\nFROM\r\n(\r\n    SELECT\r\n        map_uid,\r\n        countDistinct(type) AS type_cnt\r\n    FROM\r\n    (\r\n        SELECT\r\n            map_uid,\r\n            if(os_type = 'Mobile', 'mobile', 'desktop') AS type\r\n        FROM login\r\n        WHERE (toDate(log_time) >= subtractDays(CAST('2021-05-10', 'date'), 28)) AND (toDate(log_time) <= subtractDays(CAST('2021-05-10', 'date'), 1))\r\n        GROUP BY\r\n            map_uid,\r\n            type\r\n    )\r\n    GROUP BY map_uid\r\n)\r\nWHERE type_cnt = 2\r\nGROUP BY log_date\r\nSETTINGS max_memory_usage = 20\r\n\r\n\u2193 Progress: 12.01 million rows, 156.14 MB (37.41 million rows/s., 486.35 MB/s.)  0%\r\nReceived exception from server (version 20.9.3):\r\nCode: 241. DB::Exception: Received from  DB::Exception: Received from clickhouse_node4_4_1:9000. DB::Exception: Memory limit (for query) exceeded: would use 4.22 MiB (attempt to allocate chunk of 4421564 bytes), maximum: 20.00 B.\r\n\r\n\r\n\r\nbut i change the  max_memory_usage value to a very big value such as 10000000000000000, it doesn't work.\r\n\r\nfinaly it show the errors:\r\nmemory limit Received exception from server (version 20.9.3):\r\nCode: 241. DB::Exception: Received from 1 DB::Exception: Memory limit (for query) exceeded: would use 9.38 GiB (attempt to allocate chunk of 133939184 bytes), maximum: 9.31 GiB: While executing AggregatingTransform.\r\n\r\n\r\nand i use set max_memory_usage=10000000000000;  then run the query it works.\r\n\r\nso ,how to set the client settings via http interface ,pls help thx.\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Memory limit settings must be successfully applied when using HTTP interface",
      "Query must complete successfully without memory limit errors",
      "Configuration method must be compatible with HTTP query interface"
    ],
    "created_at": "2021-05-12T01:26:43Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/24014",
    "source": {
      "issue_number": 24014
    },
    "initial_question": {
      "title": "How to append an element after each array in a 2-D Array",
      "body": "Suppose we have two arrays with the same length.  \r\nA = [[1,2],[3,4]].  \r\nB = [5,6].  \r\nIs there a easy way to get a new array C = [[1,2,5],[3,4,6]]?    \r\n     \r\nGenerally,   \r\nif A is with a shape of (n,a) and B is with a shape of (n,b),    \r\ncan we get a new array C with a shape of (n,a+b)?\r\n"
    },
    "satisfaction_conditions": [
      "Output array must maintain the same number of subarrays as input array A",
      "Each subarray in output must contain all elements from corresponding subarray in A plus the corresponding element from B",
      "Length of array B must equal number of subarrays in A",
      "Original element order within each subarray must be preserved",
      "Each element from B must be appended at the end of corresponding subarray"
    ],
    "created_at": "2021-05-11T09:36:23Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/23958",
    "source": {
      "issue_number": 23958
    },
    "initial_question": {
      "title": "Dictionary with postgres source fails",
      "body": "Hi,\r\n\r\nI created a dictionary with a postgres source and creation works fine. But when I try to query it, it says that the source table in postgres doesn't exist.\r\n\r\n```\r\n SQL Error [156]: ClickHouse exception, code: 156, host: my.clickhouse.server, port: 8123; Code: 156, e.displayText() = DB::Exception: Failed to load dictionary 'chdatabase.dictionary_table': std::exception. Code: 1001, type: pqxx::undefined_table, e.what() = ERROR:  relation \"pgschema.pgtable\" does not exist\r\nLINE 1: ...e\", \"iso3_code\", \"continent_id\", \"adjective\" FROM \"analytics...\r\n                                                             ^\r\n, (version 21.3.9.83 (official build))\r\n```\r\n\r\nThe dictionary is configured using a DDL statement:\r\n\r\n```\r\nCREATE DICTIONARY chdatabase.dictionary_table (\r\n\tid Int32,\r\n\tname String,\r\n\tiso2_code String,\r\n\tiso3_code String\r\n)\r\nPRIMARY KEY id\r\nSOURCE(POSTGRESQL(\r\n  port 5432 \r\n  host 'postgreshost' \r\n  user  'postgresuser'\r\n  password 'postgrespassword'\r\n  db 'pgdatabase'\r\n  table 'pgschema.pgtable'\r\n  )\r\n)\r\nLIFETIME(MIN 300 MAX 360)\r\nLAYOUT(COMPLEX_KEY_HASHED());\r\n```\r\n\r\nI've confirmed that the user defined in in the postgresql source has access to the table and can query it."
    },
    "satisfaction_conditions": [
      "PostgreSQL connection parameters allow successful authentication and table access",
      "Dictionary definition syntax is compatible with the ClickHouse version being used"
    ],
    "created_at": "2021-05-08T12:02:31Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/23914",
    "source": {
      "issue_number": 23914
    },
    "initial_question": {
      "title": "all_part merge without insert or alter",
      "body": "I did not perform any INSERT and ALTER operations. Today, I monitored several background parts to perform a merge operation. Why is this merge performed? Does this operation have any effect? Is there any way to avoid this operation, because when you merge the disk space will suddenly surge and then fall?\r\n\r\nI hope someone can help me solve this problem, thank you very much\r\n"
    },
    "satisfaction_conditions": [
      "Resource utilization during merges must be controlled",
      "Merge behavior must be predictable across configured storage locations"
    ],
    "created_at": "2021-05-06T06:39:25Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/23698",
    "source": {
      "issue_number": 23698
    },
    "initial_question": {
      "title": "Kernel panic: \"Bad page state in process BgSchPool\" unsing KafkaEngine",
      "body": "**Describe the bug**\r\nSpontaneous kernel panic using tables with Kafka engine\r\n\r\n**How to reproduce**\r\n* ClickHouse server version 20.8.6 revision 54438\r\n* Clickhouse cluster of 3 replicas\r\n* Kafka cluster of 3 replicas\r\n* 60 tables with kafka engine + materialized views to ReplicatedMergeTree target tables\r\n* changed background_schedule_pool_size=100\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2021-04-26 @ 16:18:53.709513,935,843\r\nhost\r\n2021.04.26 16:18:53.709596 [ 28230 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128404_31024 covered by 202104_0_128407_31027 (state Committed)\r\n2021-04-26 @ 16:18:55.479513,936,026\r\nhost\r\n2021.04.26 16:18:55.479125 [ 28239 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128408_31028 covered by 202104_0_128411_31031 (state Committed)\r\n2021-04-26 @ 16:20:42.197513,936,209\r\nhost\r\n2021.04.26 16:20:42.197118 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Checking part 202104_0_128603_31217\r\n2021-04-26 @ 16:20:42.212513,936,362\r\nhost\r\n2021.04.26 16:20:42.212608 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 202104_0_128603_31217.\r\n2021-04-26 @ 16:20:42.217513,936,541\r\nhost\r\n2021.04.26 16:20:42.217504 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 202104_0_128603_31217. Hoping that it will eventually appear as a result of a merge.\r\n2021-04-26 @ 16:20:42.256513,936,827\r\nhost\r\n2021.04.26 16:20:42.256184 [ 28228 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128601_31215 covered by 202104_0_128603_31217 (state Committed)\r\n2021.04.26 16:26:49.174485 [ 17341 ] {} <Error> ServerErrorHandler: Poco::Exception. Code: 1000, e.code() = 104, e.displayText() = Connection reset by peer, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::IOException::IOException(int) @ 0x18bc80ff in /usr/bin/clickhouse\r\n1. Poco::Net::ConnectionResetException::ConnectionResetException(int) @ 0x18ab943d in /usr/bin/clickhouse\r\n2. ? @ 0x18ad61ea in /usr/bin/clickhouse\r\n3. Poco::Net::SocketImpl::receiveBytes(void*, int, int) @ 0x18ad48e3 in /usr/bin/clickhouse\r\n4. Poco::Net::HTTPSession::receive(char*, int) @ 0x18aa2c58 in /usr/bin/clickhouse\r\n5. Poco::Net::HTTPSession::get() @ 0x18aa2cc3 in /usr/bin/clickhouse\r\n6. Poco::Net::HTTPHeaderStreamBuf::readFromDevice(char*, long) @ 0x18a9787a in /usr/bin/clickhouse\r\n7. Poco::BasicBufferedStreamBuf<char, std::__1::char_traits<char>, Poco::Net::HTTPBufferAllocator>::underflow() @ 0x18a941d8 in /usr/bin/clickhouse\r\n8. std::__1::basic_streambuf<char, std::__1::char_traits<char> >::uflow() @ 0x19b3ef0e in ?\r\n9. std::__1::basic_istream<char, std::__1::char_traits<char> >::get() @ 0x19b46506 in ?\r\n10. Poco::Net::HTTPRequest::read(std::__1::basic_istream<char, std::__1::char_traits<char> >&) @ 0x18a9a957 in /usr/bin/clickhouse\r\n11. Poco::Net::HTTPServerRequestImpl::HTTPServerRequestImpl(Poco::Net::HTTPServerResponseImpl&, Poco::Net::HTTPServerSession&, Poco::Net::HTTPServerParams*) @ 0x18aa0d85 in /usr/bin/clickhouse\r\n12. Poco::Net::HTTPServerConnection::run() @ 0x18a9f74c in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerConnection::start() @ 0x18add87b in /usr/bin/clickhouse\r\n14. Poco::Net::TCPServerDispatcher::run() @ 0x18addd0b in /usr/bin/clickhouse\r\n15. Poco::PooledThread::run() @ 0x18c5c7e6 in /usr/bin/clickhouse\r\n16. Poco::ThreadImpl::runnableEntry(void*) @ 0x18c57be0 in /usr/bin/clickhouse\r\n17. start_thread @ 0x7fa3 in /usr/lib/x86_64-linux-gnu/libpthread-2.28.so\r\n18. __clone @ 0xf94cf in /usr/lib/x86_64-linux-gnu/libc-2.28.so\r\n (version 20.8.6.6 (official build))\r\n\r\n2021-04-26 @ 16:26:49.1305200172.356694] BUG: Bad page state in process BgSchPool  pfn:1fcdb35\r\n2021-04-26 @ 16:26:49.1305200172.394974] page:ffffe7c0ff36cd40 count:0 mapcount:0 mapping:ffff9324edb25b00 index:0x0\r\n2021-04-26 @ 16:26:49.1305200172.444695] flags: 0x17fffc000000000()\r\n2021-04-26 @ 16:26:49.1305200172.468918] raw: 017fffc000000000 dead000000000100 dead000000000200 ffff9324edb25b00\r\n2021-04-26 @ 16:26:49.1305200172.517059] raw: 0000000000000000 0000000000270027 00000000ffffffff ffff93406e2dc000\r\n2021-04-26 @ 16:26:49.1305200172.565211] page dumped because: page still charged to cgroup\r\n2021-04-26 @ 16:26:49.1305200172.601395] page->mem_cgroup:ffff93406e2dc000\r\n2021-04-26 @ 16:26:49.1305200172.629259] Modules linked in: drbg ansi_cprng authenc echainiv xfrm6_mode_tunnel xfrm4_mode_tunnel binfmt_misc xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo ifb sch_fq_codel bonding ip6table_mangle nf_log_ipv6 ip6t_REJECT nf_reject_ipv6 iptable_nat nf_nat_ipv4 nf_nat xt_connmark xt_DSCP xt_length xt_dscp iptable_mangle nf_log_ipv4 nf_log_common xt_set ipt_REJECT nf_reject_ipv4 xt_owner xt_multiport xt_LOG xt_limit xt_policy xt_conntrack xt_tcpudp ip_set_hash_ip ip_set nfnetlink ip6table_filter ip6_tables iptable_filter nls_ascii nls_cp437 vfat fat intel_rapl sb_edac x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass mgag200 crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel intel_cstate drm_kms_helper mxm_wmi intel_uncore efi_pstore mei_me intel_rapl_perf\r\n2021-04-26 @ 16:26:49.1305200173.070265]  iTCO_wdt pcc_cpufreq drm efivars pcspkr joydev evdev mei iTCO_vendor_support sg ioatdma ipmi_ssif wmi acpi_power_meter acpi_pad button nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 libcrc32c ipmi_si ipmi_devintf ipmi_msghandler efivarfs ip_tables x_tables autofs4 ext4 crc16 mbcache jbd2 crc32c_generic fscrypto ecb dm_mod hid_generic usbhid hid sd_mod ahci libahci xhci_pci ehci_pci xhci_hcd ehci_hcd crc32c_intel libata megaraid_sas aesni_intel aes_x86_64 crypto_simd i40e(OE) usbcore igb cryptd i2c_algo_bit scsi_mod usb_common lpc_ich i2c_i801 glue_helper mfd_core dca\r\n2021-04-26 @ 16:26:49.1305200173.070307] CPU: 20 PID: 28333 Comm: BgSchPool Tainted: G    B   W  OEL    4.19.0-13-amd64 #1 Debian 4.19.160-2\r\n2021-04-26 @ 16:26:49.1305200173.070312] Hardware name: Intel Corporation S2600WT2R/S2600WT2R, BIOS SE5C610.86B.01.01.0020.122820161512 12/28/2016\r\n2021-04-26 @ 16:26:49.1305200173.516116] Call Trace:\r\n2021-04-26 @ 16:26:49.1305200173.533853]  dump_stack+0x66/0x90\r\n2021-04-26 @ 16:26:49.1305200173.556753]  bad_page.cold.116+0x7f/0xb2\r\n2021-04-26 @ 16:26:49.1305200173.583290]  free_pcppages_bulk+0x4b4/0x660\r\n2021-04-26 @ 16:26:49.1305200173.611387]  free_unref_page_list+0x111/0x190\r\n2021-04-26 @ 16:26:49.1305200173.640536]  release_pages+0x215/0x450\r\n2021-04-26 @ 16:26:49.1305200173.666167]  tlb_flush_mmu_free+0x3d/0x60\r\n2021-04-26 @ 16:26:49.1305200173.693343]  arch_tlb_finish_mmu+0x89/0x100\r\n2021-04-26 @ 16:26:49.1305200173.721564]  tlb_finish_mmu+0x1f/0x30\r\n2021-04-26 @ 16:26:49.1305200173.746536]  zap_page_range+0xde/0x140\r\n2021-04-26 @ 16:26:49.1305200173.772146]  ? do_futex+0xc8/0xbe0\r\n2021-04-26 @ 16:26:49.1305200173.795671]  __x64_sys_madvise+0x663/0x7c0\r\n2021-04-26 @ 16:26:49.1305200173.823237]  ? do_syscall_64+0x53/0x110\r\n2021-04-26 @ 16:26:49.1305200173.849265]  ? __ia32_sys_madvise+0x7c0/0x7c0\r\n2021-04-26 @ 16:26:49.1305200173.878400]  do_syscall_64+0x53/0x110\r\n2021-04-26 @ 16:26:49.1305200173.903382]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\r\n2021-04-26 @ 16:26:49.1305200173.936667] RIP: 0033:0x7f126714c2d7\r\n2021-04-26 @ 16:26:49.1305200173.961237] Code: ff ff ff ff c3 48 8b 15 b7 6b 0c 00 f7 d8 64 89 02 b8 ff ff ff ff eb bc 66 2e 0f 1f 84 00 00 00 00 00 90 b8 1c 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 89 6b 0c 00 f7 d8 64 89 01 48\r\n2021-04-26 @ 16:26:49.1305200174.078867] RSP: 002b:00007f12082ac648 EFLAGS: 00000206 ORIG_RAX: 000000000000001c\r\n2021-04-26 @ 16:26:49.1305200174.127252] RAX: ffffffffffffffda RBX: 0000000000008000 RCX: 00007f126714c2d7\r\n2021-04-26 @ 16:26:49.1305200174.173033] RDX: 0000000000000004 RSI: 000000000003c000 RDI: 00007f0aa007e000\r\n2021-04-26 @ 16:26:49.1305200174.218905] RBP: 00007f12082ada60 R08: 000000000003c000 R09: 0000000000000014\r\n2021-04-26 @ 16:26:49.1305200174.264703] R10: 0000000000000000 R11: 0000000000000206 R12: 00007f0763210540\r\n2021-04-26 @ 16:26:49.1305200174.310591] R13: 00007f12082ac6d0 R14: 00007f123c0008c0 R15: 00007f123c004510\r\n```\r\n\r\n**Additional context**\r\nWe faced the problem twice, but unfortunately we cannot reproduce it. The problem occurs spontaneously and rarely, but because of this, our server crashes.\r\n"
    },
    "satisfaction_conditions": [
      "System stability is restored without kernel panics",
      "Memory management remains stable under ClickHouse workload",
      "Solution addresses system-level rather than application-level issues"
    ],
    "created_at": "2021-04-27T14:38:05Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/23356",
    "source": {
      "issue_number": 23356
    },
    "initial_question": {
      "title": "A \"versioned\" version of AggregatingMergeTree",
      "body": "We have a very specific criteria for storing data which we cannot find a very good solution for. We would like to do something like a versioned AggregatingMergeTree but it doesn't exist so is there maybe a different solution?\r\n\r\nWe have a setup that looks like this:\r\n```\r\ncreate table if not exists test.test\r\n(\r\n    country        LowCardinality(String),\r\n    text           String CODEC (ZSTD(3)),\r\n    number         SimpleAggregateFunction( max, UInt32),\r\n    related_number Float32,\r\n    first_seen     SimpleAggregateFunction( min, Date)\r\n)\r\n    engine = AggregatingMergeTree()\r\n        PARTITION BY country\r\n        ORDER BY (country, text)\r\n        SETTINGS index_granularity = 256;\r\n\r\ninsert into test\r\nvalues ('us', 'foo', 1, 30, '2020-01-01'),\r\n       ('us', 'foo', 2, 20, '2021-01-02'),\r\n       ('us', 'foo', 3, 10, '2021-01-03');\r\n```\r\nIf we do select all data with final (`select * from test final`) we get:\r\n\r\n| country | text | number | related_number | first_seen |\r\n| ------------- | ------------- | ------------- | ------------- | ------------- |\r\n| us | foo  | 3  | 30 | 2020-01-01  |\r\n\r\nwhich is what one would expect. \r\n\r\nBut we would like \"save\" the `related_number` that correspond to max `number` while maintaining the lowest `first_seen` date as shown in the table below (with the _**bold italic records highligthed**_).\r\n\r\n(`select * from test`)\r\n| country | text | number | related_number | first_seen |\r\n| ------------- | ------------- | ------------- | ------------- | ------------- |\r\n| us | foo  | 1  | 30 | **_2020-01-01_**  |\r\n| us | foo  | 2  | 20 | 2020-01-02  |\r\n| **_us_** | **_foo_**  | **_3_**  | **_10_** | 2020-01-03  |\r\n| us | foo  | 1  | 30 | 2020-01-04  |\r\n\r\nSo essentially we are looking for the behavior of the combination of a ReplacingMergeTree (with `number` as the version column) and a AggregatingMergeTree -> a VersionedAggregatingMergeTree.\r\n\r\nAny idea how we could solve this?\r\n\r\nedit: changed the last table to better show what we want"
    },
    "satisfaction_conditions": [
      "Must maintain the highest 'number' value for each unique (country, text) combination",
      "Must preserve the 'related_number' that corresponds to the highest 'number' value",
      "Must retain the earliest 'first_seen' date regardless of other values",
      "Must handle non-sequential updates to 'number' values"
    ],
    "created_at": "2021-04-20T11:09:57Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/23299",
    "source": {
      "issue_number": 23299
    },
    "initial_question": {
      "title": "The performance diff between Map and Array",
      "body": "Hi, I want to know the query performance between `Map` and `Array`. If I want to store multiple-value in one column, I can choose `Array` or `Map`, and use `has()` or `mapContains()` to filter the data in query. Which one will be faster, I found no obvious difference during the test."
    },
    "satisfaction_conditions": [
      "Query performance metrics must be comparable between Map and Array implementations",
      "Data structure must support storage of multiple values in a single column",
      "Data structure must support value existence checking functionality",
      "Solution must handle the specified data volume effectively",
      "Data structure must be compatible with the SummingMergeTree engine"
    ],
    "created_at": "2021-04-19T09:32:31Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/22985",
    "source": {
      "issue_number": 22985
    },
    "initial_question": {
      "title": "Conflicts and unexpected result of CAST in where condition",
      "body": "Dear authors,\r\n\r\nIn my case, values of colomn _note_ is always **NULL**, but got unexpected query result:\r\n\r\nquery: `select record_id, note from demo where CAST(note AS DECIMAL(18,0)) is not null limit 3;`\r\nresult is:\r\n```\r\n\u250c\u2500record_id\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500note\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 2_120000_120000 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2502 2_120000_120001 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2502 2_120000_120002 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nI think the result should be 0 rows, but got 3? Obviously, it is different from Mysql.\r\n\r\nAnd then I try to find out the value of cast by:\r\nquery: `select CAST(note AS DECIMAL(18,0)), record_id from demo where CAST(note AS DECIMAL(18,0)) is not null limit 3;`\r\nbut got: `DB::Exception: Cannot convert NULL value to non-Nullable type`\r\n\r\nSo if the convertion would fail, why it is not blocked by cast condition in advance?\r\nand vise versa, If where-cast condition tells it is not null, why the convertion failed, it should assign certain value to results.\r\n\r\nIf it's not a bug, I think there must be some reasons for the design of this feature."
    },
    "satisfaction_conditions": [
      "NULL handling behavior is configurable or explicitly defined"
    ],
    "created_at": "2021-04-12T04:22:14Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/22483",
    "source": {
      "issue_number": 22483
    },
    "initial_question": {
      "title": "Modify ttl does not affects old parts.",
      "body": "To my invesigation, ClickHouse uses old parts' ttl as a new part's ttl when merging, so it would not delete old parts desipte modifying table's ttl.\r\nMaybe clickhouse can use modified table's ttl as new part's ttl when merging to delete old parts\u2018 data."
    },
    "satisfaction_conditions": [
      "TTL modifications must affect existing data parts",
      "System must be running a compatible ClickHouse version",
      "TTL modifications must be effectively materialized"
    ],
    "created_at": "2021-04-02T02:49:00Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/22269",
    "source": {
      "issue_number": 22269
    },
    "initial_question": {
      "title": "insert into select from hdfs engine table can be parallel ?",
      "body": ""
    },
    "satisfaction_conditions": [
      "Data reading operations must demonstrate parallel processing capability",
      "Input data must be structured to allow parallel access",
      "System configuration must enable parallel insert operations"
    ],
    "created_at": "2021-03-29T08:11:17Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/22267",
    "source": {
      "issue_number": 22267
    },
    "initial_question": {
      "title": " clickhouse-client --format_csv_delimiter='@@@' or --format_csv_delimiter=$'\\@\\@\\@' got Exception",
      "body": "(you don't have to strictly follow this form)\r\n\r\nimport csv data into database ,like this\r\nclickhouse-client --format_csv_delimiter='@@@' --query=\"insert into default.tb_name select col1,col2,col3 from file('csv_file_name.csv','CSVWithNames','col1 String,col2 String,col3 String')\"\r\n\r\ngot \r\nCode: 19. DB::Exception: A setting's value string has to be an exactly one character long\r\n\r\nhow to translate Symbol @ ,I try \\@ not working too."
    },
    "satisfaction_conditions": [
      "CSV delimiter must be exactly one character long"
    ],
    "created_at": "2021-03-29T06:46:29Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/22141",
    "source": {
      "issue_number": 22141
    },
    "initial_question": {
      "title": "Top N of unique string",
      "body": "Hi,\r\n\r\nIn the main table, there is a string column for `ip` which is unique (mostly) per document. I want to return the top 10 IPs for this table (few billions of documents). Is there any performant way to do so?"
    },
    "satisfaction_conditions": [
      "Query returns top 10 IP addresses sorted by their frequency count in descending order",
      "Query must complete successfully on a dataset of billions of documents",
      "Results must include both IP address and its count in the output",
      "Query execution time must be reasonable for operational use",
      "Query must handle memory constraints of the system"
    ],
    "created_at": "2021-03-25T16:33:28Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/21894",
    "source": {
      "issue_number": 21894
    },
    "initial_question": {
      "title": "Distributed Tables missing records. \"Exception: Too large string size.\" in logs.",
      "body": "Setup:\r\n```\r\n         <shard>\r\n              <weight>1</weight>\r\n              <internal_replication>false</internal_replication>\r\n              <replica>\r\n                  <host><server1domainname></host>\r\n                  <port>9000</port>\r\n              </replica>\r\n              <replica>\r\n                  <host><server2domainname></host>\r\n                  <port>9000</port>\r\n              </replica>\r\n          </shard>\r\n\r\n```\r\nOur code writes to a Distributed table on either server, and then clickhouse writes the record to the underlying real tables on both servers. \r\n\r\nNo zookeeper.\r\n\r\nThis has been working for a few years.\r\n\r\nIncident.\r\nTo get multivolume support, I upgraded from 18.14.19 to 20.8.12.2. I had a number of problems, and ran out of time to debug/fix, so I rolled back to 18.14.19. My problem probably started then.\r\n\r\nNow we have discovered that some records are not getting replicated to one of the servers. Or maybe replication is working from server1 -> server2, but not the other way. I'm not 100% sure. When reads randomly choose a server, they get different results.\r\n\r\nI have these error messages constantly on one server:\r\n\r\n`2021.03.19 02:35:13.294443 [ 39 ] {} <Error> <table>.Distributed.DirectoryMonitor: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = Exception: Too large string size., e.what() = Exception\r\n`\r\n\r\nI also noticed a small number of these errors on the other server, but they are not constant. Only a handful from a few hours ago.\r\n\r\n`2021.03.18 19:22:36.647630 [ 38 ] {} <Error> <table>.Distributed.DirectoryMonitor: Code: 252, e.displayText() = DB::Exception: Received from <serverdomainname>:9000, <ipaddress> DB::Exception: Too many parts (304). Merges are processing significantly slower than inserts..\r\n`\r\n\r\nI did some research into the first error, but I don't really understand which directory, or which string it is referring to.\r\n\r\nThanks."
    },
    "satisfaction_conditions": [
      "The 'Too large string size' error messages stop appearing in logs",
      "The Distributed table's directory structure is properly organized",
      "New write operations successfully propagate to all servers",
      "The Distributed table remains operational during and after changes"
    ],
    "created_at": "2021-03-18T21:48:33Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/21575",
    "source": {
      "issue_number": 21575
    },
    "initial_question": {
      "title": "How to import csv file with  delimiter character ascll ",
      "body": "How to import csv file with ascll delimiter character, example:\r\n\r\nLO\u0003OG_NFO\u0003110\u0003OU07\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nLN\u0003OG_NFO\u0003110\u0003OU0705\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nAN\u0003OG_NFO\u0003110\u0003OU075\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nLN\u0003OG_FO\u0003110\u0003OU005\u00030\u0003\u00032014-03-21-01.57.30.000000\r\n\r\ndelimiter character \"\u0003\" is \"0X03\",\r\n"
    },
    "satisfaction_conditions": [
      "CSV file must be correctly parsed using ASCII character 0x03 as delimiter",
      "Solution must handle non-standard delimiter characters"
    ],
    "created_at": "2021-03-10T03:20:15Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/21503",
    "source": {
      "issue_number": 21503
    },
    "initial_question": {
      "title": "Date value is inconsistency between format",
      "body": "I have a schema:\r\n```sql\r\nCREATE TABLE test(\r\n `field1` Int64,\r\n `field2` Int32,\r\n `field3` String,\r\n `field4` String,\r\n `field5` String,\r\n `field6` Int32,\r\n `field7` Int64,\r\n `field8` String,\r\n `time` Int64,\r\n `field9` Int64,\r\n `field10` Int64,\r\n `field11` Int32,\r\n `field12` String,\r\n `field13` Int32,\r\n `field14` Int32,\r\n `field15` Int64,\r\n `field16` Int64,\r\n `field17` Int64,\r\n `field18` Int64,\r\n `field19` String,\r\n `field20` String,\r\n `field21` Int64,\r\n `field22` Int32,\r\n `field23` String,\r\n `field24` Int32,\r\n `field25` Int32,\r\n `field26` Int64,\r\n `field27` Int64,\r\n `field28` Int64,\r\n `field29` Int64,\r\n `field30` String,\r\n `field31` String,\r\n `date_time` DateTime64\r\n) ENGINE =  ReplicatedMergeTree('/data/clickhouse/replicated/test_repl', 'replica_1')\r\nPARTITION BY toYYYYMM(date_time)\r\nORDER BY (time, field1, field2) SETTINGS index_granularity = **8192**\r\n```\r\nI perform SQL:\r\n\r\n```sql\r\nSELECT toDate(date_time), toYYYYMM(date_time), date_time FROM test\r\nWHERE toYYYYMM(date_time) BETWEEN 202101 AND 202102\r\nAND voucher_id=123456789;\r\n```\r\nThe result:\r\n| toDate      | toYYYYMM | date_time\r\n| ----------- | ----------- | ----------- |\r\n| 2021-02-01      | 202102       | 2021-01-31T17:00:00+00:00\r\n\r\nI confused about this result. Why toDate and toYYYYMM different  date_time? \r\nI expected toDate: 2021-31-01 and toYYYYMM: 202001.\r\n \r\n"
    },
    "satisfaction_conditions": [
      "Queries must account for the application's timezone context",
      "Timezone information must be explicitly handled in datetime operations"
    ],
    "created_at": "2021-03-07T09:14:42Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/21473",
    "source": {
      "issue_number": 21473
    },
    "initial_question": {
      "title": "How to downgrade from version 20.12.7.3 to 20.4.4.18 with Atomic database created",
      "body": "I created some Atomic databases in 20.12.7.3, when I need to downgrade the server to 20.4.4.18 there's always errors like below: \r\n`{} <Error> Application: DB::Exception: Syntax error (in file /var/lib/clickhouse/metadata/default.sql): failed at position 19 (line 1, col 19): UUID '6ef8d876-bd4e-44bc-bc44-2f2e950e3f20'\r\nENGINE = Atomic\r\n. Expected one of: storage definition, ENGINE, ON\r\n`\r\nHow can I smoothly do the downgrade? \r\nI tried to set allow_experimental_database_atomic=1 in users.xml, but it doesn't work. "
    },
    "satisfaction_conditions": [
      "Database data must be preserved after downgrade",
      "Database must be accessible in older version 20.4.4.18",
      "Database structure must be compatible with older version",
      "All tables must remain queryable after downgrade",
      "No UUID-related syntax errors should occur after downgrade"
    ],
    "created_at": "2021-03-05T10:27:05Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/21099",
    "source": {
      "issue_number": 21099
    },
    "initial_question": {
      "title": "ALTER TABLE UPDATE referencing field from table being updated",
      "body": "Hello. Thanks for the amazing software, I \u2764\ufe0f Clickhouse.\r\n\r\nWe had a bug in our app code that generated some bad data in the sessions table. I need to regenerate the `exit_page` field for all the historical data in our sessions table (CollapsingMergeTree).\r\n\r\nOur table layout is simpler but similar to the Yandex.Metrica example. Some names are different but you should get the idea `hits -> events, visits -> sessions`. Not that much data yet, less than a billion rows in the sessions table.\r\n\r\nComing from regular SQL, this was my first instinct:\r\n```sql\r\nALTER TABLE sessions UPDATE exit_page=(SELECT anyLast(pathname) FROM events WHERE events.session_id=sessions.session_id ORDER BY timestamp);\r\n```\r\nbut I get the following error:\r\n```\r\nCode: 47. DB::Exception: Received from clickhouse-server:9000. DB::Exception: Missing columns: 'sessions.session_id' while processing query: 'SELECT anyLast(pathname) FROM plausible_dev.events WHERE (domain = 'localtest.me') AND (session_id = sessions.session_id)', required columns: 'domain' 'pathname' 'session_id' 'sessions.session_id', source columns: [...]\r\n```\r\n\r\nIn regular SQL I am used to being able to reference columns from the row that I'm updating. I realize Clickhouse has very different semantics for updating and this might not be supported. Any other ideas how one might go about updating a field like this?"
    },
    "satisfaction_conditions": [
      "The exit_page field in the sessions table must be updated with the last pathname from corresponding events",
      "The solution must maintain the relationship between sessions and events tables using session_id",
      "The solution must work with CollapsingMergeTree table engine",
      "The update must preserve existing data in other columns"
    ],
    "created_at": "2021-02-23T09:31:42Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/20612",
    "source": {
      "issue_number": 20612
    },
    "initial_question": {
      "title": "Exception: Nested type Array(String) cannot be inside Nullable type (version 20.9.2.20 (official build))",
      "body": "i was created table:\r\nCREATE TABLE compare.test1\r\n(\r\n\tevent Nullable(String)\r\n) \r\nENGINE = MergeTree ORDER BY tuple();\r\n\r\nwhen i have construction as \r\nSELECT\r\n\tJSONExtractString(event, 'event_time') as s\r\n\t,event\r\n\t,splitByChar('T', JSONExtractString(event, 'event_time'))[1]\r\nfrom compare.test1\r\n\r\ni have error:\r\nException: Nested type Array(String) cannot be inside Nullable type (version 20.9.2.20 (official build))\r\n\r\nWhen in table i implement event String insted of Nullable(String) it's ok, i don't have any error"
    },
    "satisfaction_conditions": [
      "Query successfully processes nullable string columns without throwing nested type errors",
      "String splitting operation completes successfully on the JSON-extracted data",
      "Null values in the input column are handled gracefully",
      "Query maintains data integrity when processing JSON fields"
    ],
    "created_at": "2021-02-17T09:36:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/20553",
    "source": {
      "issue_number": 20553
    },
    "initial_question": {
      "title": "Select * memory usage optimization",
      "body": "I use ClickHouse 20.12.3.3 to store normalized logs. A table consists of 160 columns, primary key and order by key is Timestamp (int64).\r\n\r\nOften I need to find some logs extracting all table columns. When I use the following query to find logs in 1 month period (approx. 2 TB of compressed data) on a single node - it consumes 20+ GB of memory:\r\n\r\nSELECT * FROM events WHERE Column1 = 'value1' AND Column2 = 'value2' AND Timestamp > [START] AND Timestamp < [END] ORDER BY Timestamp DESC LIMIT **250**;\r\n\r\nThe wider search period is, the more RAM is consumed. The less columns appear in SELECT, the less memory consumed.\r\n\r\nFolks from CH telegram group told me that part of this memory is allocated for column buffers (1MB for each column that appears in SELECT) by each thread. Let's say query is executed by 32 threads: 32t * 160c * 1MB = 5 GB RAM. So it is not clear why CH needs another 15 GB to execute this query. \r\n\r\nIs there a way to use a pipeline like the following one?\r\n\r\n1. Read only columns that appear in WHERE and ORDER BY clauses from disk;\r\n2. Mark locations of each row that satisfy WHERE clause, heap sort on-the-fly;\r\n3. Extract marked rows after scan is finished.\r\n\r\nThis way I wouldn't need tens of GB of RAM to perform deep \"historical\" searches.\r\n\r\nIf that is not possible at the moment, are there any plans to introduce such a pipeline in a future releases?\r\n"
    },
    "satisfaction_conditions": [
      "Memory usage must be significantly reduced when querying large datasets with many columns",
      "Solution must work with distributed table queries"
    ],
    "created_at": "2021-02-16T10:28:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/20471",
    "source": {
      "issue_number": 20471
    },
    "initial_question": {
      "title": "parseDateTimeBestEffortUS doesn't support OrNull modifier",
      "body": "Works:\r\n```sql\r\nselect parseDateTimeBestEffortUS('30/01/2021')\r\n```\r\nWorks:\r\n```sql\r\nselect parseDateTimeBestEffortOrNull('30/01/2021')\r\n```\r\nDoesn't work:\r\n```sql\r\nselect parseDateTimeBestEffortUSOrNull('30/01/2021')\r\n```\r\n> DB::Exception: Unknown function parseDateTimeBestEffortUSOrNull. Maybe you meant: ['parseDateTimeBestEffortOrNull', 'parseDateTime64BestEffortOrNull']: While processing parseDateTimeBestEffortUSOrNull('30/01/2021')"
    },
    "satisfaction_conditions": [
      "The parseDateTimeBestEffortUSOrNull function must successfully parse US-formatted dates",
      "The function must be available in the ClickHouse version being used",
      "The function must return datetime values in the correct format when given valid input"
    ],
    "created_at": "2021-02-13T22:35:55Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/20273",
    "source": {
      "issue_number": 20273
    },
    "initial_question": {
      "title": "how to kill long query?",
      "body": "KILL QUERY WHERE query_id='xxx' doesn't work, it just return waiting. \r\n\r\nIs it true that a sql cannot be killed unless it is executed? But what is the point of killing if all executions are completed? The CPU is full, and the purpose of kill query is to immediately stop the executing sql and reduce the CPU usage\r\n"
    },
    "satisfaction_conditions": [
      "Query termination must be confirmed or clear failure reason provided",
      "Query termination method must work on actively executing queries",
      "Query identification method must be able to target specific queries"
    ],
    "created_at": "2021-02-10T04:26:47Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/20162",
    "source": {
      "issue_number": 20162
    },
    "initial_question": {
      "title": "Do we need to still use GROUP BY in MATERIALIZED VIEWS?",
      "body": "Following on from #16954, does it still make sense to write:\r\n\r\n```\r\nCREATE TABLE agg (\r\n) ENGINE = AggregatingMergeTree()\r\n\r\nCREATE MATERIALIZED VIEW mv TO agg AS SELECT ... FROM src GROUP BY ...\r\n```\r\n\r\nis it basically identical performance-wise to write it without the `GROUP BY` without any overheads or other costs (and probably making it much easier to insert bulk data without having to go via a Null table per #17239) ?"
    },
    "satisfaction_conditions": [
      "Data aggregation accuracy must be preserved",
      "Solution must work correctly with the optimize_on_insert setting",
      "Compatibility with AggregatingMergeTree engine must be maintained"
    ],
    "created_at": "2021-02-06T20:20:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19955",
    "source": {
      "issue_number": 19955
    },
    "initial_question": {
      "title": "How to decrease the number of ClickHouse operations on ZooKeeper?",
      "body": "```\r\njava.lang.Exception: shutdown Leader! reason: zxid lower 32 bits have rolled over, forcing re-election, and therefore new epoch start\r\n```\r\n\r\nIs there any other way to restart ZooKeeper on a regular basis?"
    },
    "satisfaction_conditions": [
      "ZooKeeper transaction load must be reduced below the threshold that causes zxid rollover",
      "ClickHouse database operations continue to function correctly",
      "Database architecture optimizations reduce unnecessary ZooKeeper interactions"
    ],
    "created_at": "2021-02-02T02:58:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19943",
    "source": {
      "issue_number": 19943
    },
    "initial_question": {
      "title": "Will the data be cached when clickhouse-benchmark tool is used for multiple queries?",
      "body": "Like `select * from table`,`select count(*) from table `"
    },
    "satisfaction_conditions": [
      "Data caching behavior is controllable via configuration parameters",
      "Direct I/O operations are configurable",
      "Query optimization settings are adjustable",
      "Performance metrics are measurable and comparable"
    ],
    "created_at": "2021-02-01T15:53:29Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19878",
    "source": {
      "issue_number": 19878
    },
    "initial_question": {
      "title": "Altering Enum in ordering key doesn't work on ReplicatedMergeTree",
      "body": "**Describe the bug**\r\nReproduces at 20.8.12.2 Clickhouse version.\r\n\r\n```\r\nCREATE TABLE t ON CLUSTER replicated\r\n(\r\n    `a` UInt64,\r\n    `b` Enum8('a' = 1, 'b' = 2)\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/default/{shard}/t', '{replica}')\r\nORDER BY (b, a)\r\n\r\nINSERT INTO t VALUES (1,1),(2,2)\r\n\r\nALTER TABLE t ON CLUSTER replicated MODIFY COLUMN b Enum8('a' = 1)\r\n```\r\nThe last statement falls into error: `Cannot execute replicated DDL query, maximum retires exceeded`\r\n\r\nIn case of local alter: \r\n`ALTER TABLE t ON CLUSTER replicated MODIFY COLUMN b Enum8('a' = 1)`\r\n\r\nthe error is next:\r\n```\r\nReceived exception from server (version 20.8.12):\r\nCode: 524. DB::Exception: Received from localhost:9100. DB::Exception: ALTER of key column b from type Enum8('a' = 1, 'b' = 2) to type Enum8('a' = 1) must be metadata-only.\r\n```\r\n\r\n\r\n**Expected behavior**\r\nThe same as on non-Replicated MergeTree"
    },
    "satisfaction_conditions": [
      "Enum modifications in ordering keys must preserve existing data representation",
      "Only additive enum changes are permitted in ordering keys",
      "The behavior must be consistent with documented safety constraints",
      "The operation must maintain data integrity across all replicas"
    ],
    "created_at": "2021-01-31T11:07:29Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19797",
    "source": {
      "issue_number": 19797
    },
    "initial_question": {
      "title": "Is it possible to disable using timezone and set only UTC for DateTime64 format?",
      "body": "\r\nIs it possible to disable using timezone and set only UTC for DateTime64 format?\r\n\r\nThanks.\r\n"
    },
    "satisfaction_conditions": [
      "DateTime64 values are consistently handled in UTC timezone",
      "System processes DateTime64 values without timezone-specific modifications",
      "Configuration change persists across DateTime64 operations"
    ],
    "created_at": "2021-01-29T07:18:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19658",
    "source": {
      "issue_number": 19658
    },
    "initial_question": {
      "title": "Execute Clickhouse compressor -- decompress to return xshell",
      "body": "SELECT *\r\nFROM mt2\r\n\r\n\u250c\u2500a\u2500\u252c\u2500\u2500b\u2500\u252c\u2500\u2500c\u2500\u2510\r\n\u2502 3 \u2502  4 \u2502 10 \u2502\r\n\u2502 3 \u2502  5 \u2502  9 \u2502\r\n\u2502 3 \u2502  6 \u2502  8 \u2502\r\n\u2502 3 \u2502  7 \u2502  7 \u2502\r\n\u2502 3 \u2502  8 \u2502  6 \u2502\r\n\u2502 3 \u2502  9 \u2502  5 \u2502\r\n\u2502 3 \u2502 10 \u2502  4 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\r\n\r\nThe data directory is as follows\r\n\r\n[root@ck mt2]# tree\r\n.\r\n\u251c\u2500\u2500 3_1_1_0\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 a.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 a.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 b.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 b.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 checksums.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 columns.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 count.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 minmax_a.idx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 partition.dat\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 primary.idx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 skp_idx_idx_c.idx\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 skp_idx_idx_c.mrk\r\n\u251c\u2500\u2500 detached\r\n\u2514\u2500\u2500 format_version.txt\r\n\r\nExecute clickhouse-compressor like this\r\n\r\n[root@ck mt2]# clickhouse-compressor --decompress < 3_1_1_0/b.bin2                                                                                                                \t\r\n[root@ck mt2]# Xshell\r\n"
    },
    "satisfaction_conditions": [
      "Command execution must preserve the original data integrity"
    ],
    "created_at": "2021-01-26T13:14:08Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19531",
    "source": {
      "issue_number": 19531
    },
    "initial_question": {
      "title": "Data stored in store directory in version 21?",
      "body": "Recently I have installed a clickhouse of version 21.1.2.15. I found that the data is stored in the store directory(CLICKHOUSEPATH/store) with the name of random string, but not in the data directory as before. \r\n\r\nInstead, in the data directory there are some symbolic links to the data directory in the store directory. \r\nIs it a new feature of the new version?\r\n\r\nWhen I tried to freeze the table, I found the directory with random name in the shadow directory(same as the one in store directory) but not like \"/database/tablename\" as before. \r\nIt seems make the restore from the freeze file more complicated and makes no benefits.\r\n\r\nIs there any suggestion with the backup/restore work in the new version? Thanks!"
    },
    "satisfaction_conditions": [
      "Data storage location change is correctly identified and accessible",
      "Database engine type is properly configured",
      "Symbolic links correctly reference actual data locations",
      "Database operations (CREATE/DROP/RENAME) function as expected"
    ],
    "created_at": "2021-01-24T16:38:54Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19223",
    "source": {
      "issue_number": 19223
    },
    "initial_question": {
      "title": "Clickhouse failed to start, permission denied",
      "body": "Hello\uff1a\r\nCentOS Linux release 7.9\r\nClickHouse server version 20.12.5.14 (official build).\r\nyum installed.\r\n\r\nwhen I use command \"systemctl start clickhouse-server\"   to start clickhouse , and Failed to start , \r\nclickhouse-server.err.log and clickhouse-server.log is empty .\r\n\r\nin /var/log/message\uff0cis\uff1a\r\nclickhouse-server: Processing configuration file '/etc/clickhouse-server/config.xml'.\r\nclickhouse-server: std::exception. Code: 1001, type: std::__1::__fs::filesystem::filesystem_error, e.what() = filesystem error: in posix_stat:  failed to determine attributes for the specified path: Permission denied [/etc/clickhouse-server/config.xml], Stack trace (when copying this message, always include the   \r\nlines below):\r\nclickhouse-server: 0. std::__1::system_error::system_error(std::__1::error_code, std::__1::basic_string<char, std::__1::char_traits<char>, std::    \r\n__1::allocator<char> > const&) @ 0x123f8d83 in ?\r\n\r\nI checked the directory and file permissions\uff0cis OK\r\n-rw-rw---- 1 clickhouse clickhouse 33809 Jan 18 09:06 config.xml\r\n\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 access\r\ndrwxr-x--- 10 clickhouse clickhouse 4096 Jan 18 09:30 data\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 format_schemas\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:30 log\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 tmp\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 user_files\r\n\r\nBut when I use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start \uff0cis OK\r\n\r\nCan you take a look for me? Thank you very much.\r\n"
    },
    "satisfaction_conditions": [
      "Directory /etc/clickhouse-server must have appropriate execute permissions for the clickhouse service to access its contents",
      "Clickhouse server process must run under the clickhouse user",
      "Service must be able to read the configuration file at /etc/clickhouse-server/config.xml",
      "Service must start and remain running when initiated via systemctl",
      "Directory ownership and permissions must maintain security while allowing necessary access"
    ],
    "created_at": "2021-01-18T03:02:17Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/19102",
    "source": {
      "issue_number": 19102
    },
    "initial_question": {
      "title": "what 's real mean ClickHouseProfileEvents_MergeTreeDataWriterRows",
      "body": "    HELP ClickHouseProfileEvents_MergeTreeDataWriterRows Number of rows INSERTed to MergeTree tables.\r\n    TYPE ClickHouseProfileEvents_MergeTreeDataWriterRows counter\r\n\r\nonly one table at my cluster. and replicatedMergeTree table.  two replicas\r\n\r\nthis is my query result.\r\n```SQL\r\nSELECT \r\n    count(1),\r\n    toStartOfMinute(ptime) AS time\r\nFROM xxx\r\nGROUP BY time\r\nORDER BY time DESC\r\nLIMIT 10\r\n\r\n\u250c\u2500count(1)\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u2510\r\n\u2502  7088853 \u2502 2021-01-15 17:23:00 \u2502\r\n\u2502 10248679 \u2502 2021-01-15 17:22:00 \u2502\r\n\u2502  8418958 \u2502 2021-01-15 17:21:00 \u2502\r\n\u2502  5623445 \u2502 2021-01-15 17:20:00 \u2502\r\n\u2502  7268165 \u2502 2021-01-15 17:19:00 \u2502\r\n\u2502  7144866 \u2502 2021-01-15 17:18:00 \u2502\r\n\u2502  2571437 \u2502 2021-01-15 17:17:00 \u2502\r\n\u2502  3132464 \u2502 2021-01-15 17:16:00 \u2502\r\n\u2502  4344607 \u2502 2021-01-15 17:15:00 \u2502\r\n\u2502  7879506 \u2502 2021-01-15 17:14:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nand ClickHouseProfileEvents_MergeTreeDataWriterRows between 1 minute 200Mil.\r\nso anybody help me ?"
    },
    "satisfaction_conditions": [
      "The explanation must address background merge operations"
    ],
    "created_at": "2021-01-15T09:30:04Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18872",
    "source": {
      "issue_number": 18872
    },
    "initial_question": {
      "title": "apply max_execution_time but without using an estimate?",
      "body": "**Describe the bug**\r\n\r\nI have been using `max_execution_time` to limit query complexity. Normally this works great but there are some situations where very complex queries (1 page of sql so won't include below) are estimated at 1800s but actually complete in 4s. Because the estimate was 1800s I get the error message \"Estimated query execution time (1807.380046338318 seconds) is too long. Maximum: 60. Estimated rows to process: 2871497: While executing MergeTreeThread (version 20.11.3.3 (official build))\".\r\n\r\nAs there will always be issues around estimating query execution time, my request is to have an option alongside max_execution_time which will say whether to rely on estimates or whether to just run it and abort after the specified time."
    },
    "satisfaction_conditions": [
      "Query execution must be controlled by actual runtime rather than estimated time",
      "Complex queries that complete quickly must be allowed to run to completion"
    ],
    "created_at": "2021-01-08T16:41:16Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18854",
    "source": {
      "issue_number": 18854
    },
    "initial_question": {
      "title": "why select count(*) from numbers(10000000) cannot run in readonly mode?",
      "body": "Code: 164, e.displayText() = DB::Exception: play: Cannot execute query in readonly mode (version 20.13.1.5552 (official build))\r\n"
    },
    "satisfaction_conditions": [
      "Query successfully executes in readonly mode",
      "Returns correct count of numbers up to 10000000",
      "Uses operations permitted in readonly mode"
    ],
    "created_at": "2021-01-08T01:18:28Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18759",
    "source": {
      "issue_number": 18759
    },
    "initial_question": {
      "title": "ClickHouseProfileEvents_ZooKeeperWaitMicroseconds very big",
      "body": "ClickHouseProfileEvents_ZooKeeperWaitMicroseconds 6564505184722\r\nafter 1min\r\nClickHouseProfileEvents_ZooKeeperWaitMicroseconds 6567706573267\r\n\r\nAbout 3000 seconds\r\n\r\nplease help."
    },
    "satisfaction_conditions": [
      "Understanding that high ZooKeeper wait times are normal with concurrent operations",
      "Confirmation that cumulative wait time can exceed real time",
      "Clarification of the ZooKeeperWaitMicroseconds metric meaning",
      "Verification that asynchronous processing is functioning"
    ],
    "created_at": "2021-01-05T03:34:12Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18551",
    "source": {
      "issue_number": 18551
    },
    "initial_question": {
      "title": "[Guidance] Table Migration (due to changing primary key)",
      "body": "Working with Clickhouse and all is going well, but today I ran into the problem of needing to a change the name of a primary key on a table. \r\n\r\nAfter some research, it appears that currently a migration of data to a new table is needed to do this. It would be helpful to have some basic syntax and guidelines for how a table migration should happen with Clickhouse. \r\n\r\n- What is the preferred approach for a non-zookeeper managed server?"
    },
    "satisfaction_conditions": [
      "Original table data is preserved in its entirety",
      "Operation completes successfully without Zookeeper dependency"
    ],
    "created_at": "2020-12-27T13:31:36Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18346",
    "source": {
      "issue_number": 18346
    },
    "initial_question": {
      "title": "Subquery optimization question",
      "body": "I tried to look at the docs and an explain query but couldn't tell... but if I have a query that contains a subquery and the outer query doesn't use all of the fields of the subquery, are those columns ever read?\r\n\r\nFor example:\r\n\r\n```sql\r\nSELECT count(id)\r\nFROM\r\n(\r\n   SELECT id,\r\n    any(country) as country,\r\n    any(state)  as state\r\n  FROM items\r\n  GROUP BY id\r\n) x\r\n```\r\n\r\nWould the country and state fields be read by clickhouse even though the outer query never uses them, i.e. would they be pruned?\r\n"
    },
    "satisfaction_conditions": [
      "Query execution time should be comparable between optimized and non-optimized versions",
      "Unused columns in subqueries should not significantly impact performance",
      "Query optimizer must demonstrate column pruning capability"
    ],
    "created_at": "2020-12-22T04:08:19Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18123",
    "source": {
      "issue_number": 18123
    },
    "initial_question": {
      "title": "What's the difference between Atomic and Ordinary database engine",
      "body": "I've searched the official doc and hasn't found anything about this.\r\n"
    },
    "satisfaction_conditions": [
      "Information about UUID persistence and storage structure must be explained",
      "Table deletion behavior differences must be described",
      "RENAME operation handling must be explained",
      "Information about concurrent query handling must be included"
    ],
    "created_at": "2020-12-16T03:08:25Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18085",
    "source": {
      "issue_number": 18085
    },
    "initial_question": {
      "title": "What is the difference between attach/detach and move partition",
      "body": "Hi Team,\r\nMy requirement is move partition from one table and another table. \r\nBoth `ATTACH PARTITION FROM` and `MOVE PARTITION TO TABLE` could meet my requirement, but what is the difference and which one has better performance?\r\n\r\nThanks!\r\nWenjun"
    },
    "satisfaction_conditions": [
      "Behavior regarding source partition retention is clearly defined",
      "Table type compatibility is properly handled"
    ],
    "created_at": "2020-12-15T03:45:35Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/18001",
    "source": {
      "issue_number": 18001
    },
    "initial_question": {
      "title": "Columns are from different tables while processing dateDiff",
      "body": "I'm trying to calc the next day retention login user with ClickHouse.\r\n\r\nThe table structure of `t_user_login` is:\r\n\r\n```\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 user    \u2502 String                    \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2502 log_day \u2502 DateTime('Asia/Shanghai') \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd the SQL is:\r\n\r\n```sql\r\nSELECT DISTINCT log_day,a.user as user_day0,b.user as user_day1\r\nFROM (\r\n  SELECT min(log_day) as log_day, user\r\n  FROM t_user_login\r\n  GROUP BY user\r\n) a\r\nLEFT JOIN t_user_login b\r\nON dateDiff('day', b.log_day, a.log_day) = 1 AND a.user = b.user;\r\n```\r\n\r\nBut received an exception:\r\n\r\n> Received exception from server (version 20.11.4):\r\nCode: 403. DB::Exception: Received from localhost:9000. DB::Exception: Invalid columns in JOIN ON section. Columns b.log_day and log_day are from different tables.: While processing dateDiff('day', b.log_day, log_day) = 1.\r\n\r\nThis really confused me for a long time. Anyone can help me, thanks."
    },
    "satisfaction_conditions": [
      "Date comparison must maintain the original one-day difference logic",
      "Query must handle DateTime timezone considerations",
      "Results must maintain distinct user login patterns"
    ],
    "created_at": "2020-12-11T09:58:36Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17999",
    "source": {
      "issue_number": 17999
    },
    "initial_question": {
      "title": "now()    timezone is client timezone not server timezone?",
      "body": "sql \uff1aselect now(),toString(now())\r\nresult:\r\n|now()|toString(now())|\r\n|-----|---------------|\r\n|2020-12-11 06:37:09|2020-12-10 22:37:09|\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Time zone handling must be consistent with client/server architecture",
      "Raw timestamp and formatted string representations must be distinguishable",
      "Time zone conversion must maintain correct temporal relationship",
      "Query results must display both raw and formatted timestamp values"
    ],
    "created_at": "2020-12-11T06:37:35Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17885",
    "source": {
      "issue_number": 17885
    },
    "initial_question": {
      "title": "how to make groupArray more faster?",
      "body": "My query result has 1 million and need to page it,  now implemented by groupArray, but it is slow, is there any way to make it faster ?\r\neg, when click page \"3\" , I need return No. 31~40 row. "
    },
    "satisfaction_conditions": [
      "Query returns correct paginated results",
      "Query performance is improved compared to groupArray approach",
      "Memory usage remains manageable with large datasets"
    ],
    "created_at": "2020-12-08T06:07:55Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17648",
    "source": {
      "issue_number": 17648
    },
    "initial_question": {
      "title": "Failed to determine user credentials",
      "body": "(you don't have to strictly follow this form)\r\n\r\n**Describe the bug**\r\nThe version is : 20.11.4.13\r\nUbuntu: 18.04.5 LTS\r\nWhen run command: sudo /etc/init.d/clickhouse-server start, I got an err message:\r\nDec  1 10:35:24 apps-domain systemd[7878]: clickhouse-server.service: Failed to determine user credentials: No such process\r\nDec  1 10:35:24 apps-domain systemd[7878]: clickhouse-server.service: Failed at step USER spawning /usr/bin/clickhouse-server: No such process\r\n\r\nWhoever has encountered it\uff0ccan reply to the solution\u3002\r\nThank you very much!"
    },
    "satisfaction_conditions": [
      "ClickHouse server process starts successfully with proper user permissions",
      "Service runs using the designated 'clickhouse' system user",
      "Server process can access and use the correct configuration file"
    ],
    "created_at": "2020-12-01T02:41:01Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17312",
    "source": {
      "issue_number": 17312
    },
    "initial_question": {
      "title": "PDO integration",
      "body": "**Describe the bug**\r\n\r\nI can't connect to ClickHouse via the mysql interface using PDO\r\n\r\n**How to reproduce**\r\n* version 20.11.2.1 (official build)\r\n* PHP 7.4.3\r\n\r\n```php\r\n$pdo=new PDO(\"mysql:host=127.0.0.1;port=8123;dbname=test\", \"default\",\"\",[PDO::ATTR_TIMEOUT => 5,PDO::ATTR_ERRMODE => PDO::ERRMODE_EXCEPTION]);\r\n```"
    },
    "satisfaction_conditions": [
      "MySQL protocol compatibility is maintained"
    ],
    "created_at": "2020-11-23T14:58:22Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17282",
    "source": {
      "issue_number": 17282
    },
    "initial_question": {
      "title": "Last in the selection ReplacingMergeTree.",
      "body": "I use ReplacingMergeTree, after a while old data versions lose. I want to ask how to configure that time? And I want to configure ReplacingMergeTree every time I push data to save only the latest version?"
    },
    "satisfaction_conditions": [
      "Data versioning system must maintain access to historical records when required",
      "System must be able to retrieve the latest version of data records",
      "Solution must account for data ingestion frequency",
      "Data consistency guarantees must be clearly defined"
    ],
    "created_at": "2020-11-23T02:24:07Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17272",
    "source": {
      "issue_number": 17272
    },
    "initial_question": {
      "title": "why query Memory engine table is slower than MergeTree table",
      "body": "```\r\nVM_0_52_centos :) create table sfz_mem engine=Memory as select * from sfz;\r\n\r\nCREATE TABLE sfz_mem\r\nENGINE = Memory AS\r\nSELECT *\r\nFROM sfz\r\n\r\nQuery id: 0007c552-2137-4857-a0ba-d12808c4b9e7\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 1.642 sec. Processed 49.61 million rows, 396.89 MB (30.20 million rows/s., 241.64 MB/s.)\r\n\r\n\r\nVM_0_52_centos :) select sum(toInt128(code)),floor(code/1e15)b from sfz_mem group by b;\r\n\r\nSELECT\r\n    sum(toInt128(code)),\r\n    floor(code / 1000000000000000.) AS b\r\nFROM sfz_mem\r\nGROUP BY b\r\n\r\nQuery id: 668733c8-3d03-4039-a4f2-fe86e6aeed8d\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(toInt128(code))\u2500\u252c\u2500\u2500\u2500b\u2500\u2510\r\n\u2502 5481829832813867983754400 \u2502 110 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.828 sec. Processed 49.61 million rows, 396.89 MB (59.89 million rows/s., 479.13 MB/s.)\r\n\r\nVM_0_52_centos :) select sum(toInt128(code)),floor(code/1e15)b from sfz group by b;\r\n\r\nSELECT\r\n    sum(toInt128(code)),\r\n    floor(code / 1000000000000000.) AS b\r\nFROM sfz\r\nGROUP BY b\r\n\r\nQuery id: 38393b3e-10ae-4b4f-992b-a968fd84357c\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500sum(toInt128(code))\u2500\u252c\u2500\u2500\u2500b\u2500\u2510\r\n\u2502 5481829832813867983754400 \u2502 110 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.289 sec. Processed 49.61 million rows, 396.89 MB (171.83 million rows/s., 1.37 GB/s.)\r\n\r\nVM_0_52_centos :) desc sfz;\r\n\r\nDESCRIBE TABLE sfz\r\n\r\nQuery id: 7be0cb1c-908f-4a0d-88bc-3c3630352112\r\n\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 code \u2502 Int64 \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.021 sec.\r\n\r\nVM_0_52_centos :) desc sfz_mem;\r\n\r\nDESCRIBE TABLE sfz_mem\r\n\r\nQuery id: 0b6b2fc5-d08c-45d8-af43-34b6762c10e7\r\n\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 code \u2502 Int64 \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.026 sec.\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Query performance on MergeTree table must be faster than Memory table when using parallel execution",
      "Both table types must return identical query results",
      "Performance difference must be attributable to table engine characteristics rather than data differences",
      "Performance advantage of MergeTree must be linked to parallel processing capabilities"
    ],
    "created_at": "2020-11-22T10:37:11Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17230",
    "source": {
      "issue_number": 17230
    },
    "initial_question": {
      "title": "Storing huge amount of log entries",
      "body": "Hello,\r\n\r\nI would like to use ClickHouse for storing huge amount of log entries (sorted without ORDER BY) and after some time dump it to disk and send to the clients. \r\nI have tried to use tables with mergetree table engine but, as the description of mergetree engine says it was not sorted, because not all parts were merged. \r\n\r\nIs there a way how to achieve desired behavior? I would like to have sorted logs without ORDER BY expression. Is it even possible with some engines?\r\nI have look into Log engine but it does not support TTL and replication.\r\n\r\nThank you for help"
    },
    "satisfaction_conditions": [
      "Data must be retrievable in sorted order without excessive memory usage",
      "System must handle large volumes of log entries",
      "System must allow sufficient time for data organization",
      "Storage solution must support data retention policies"
    ],
    "created_at": "2020-11-20T13:30:25Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/17054",
    "source": {
      "issue_number": 17054
    },
    "initial_question": {
      "title": "why the first 1e8 rows inserted and the 2nd 1e8 rows failed",
      "body": "```\r\ncreate table sfz15y engine=MergeTree()order by id as select a.number*10000+b.number id from numbers(50000)a,numbers(10000)b;\r\ninsert into  sfz15y select (a.number+50000)*10000+b.number id from numbers(50000)a,numbers(10000)b;\r\ninsert into  sfz15y select (a.number)*10000+b.number id from numbers(50000)a,numbers(10000)b where b.number%5=0;\r\ncreate table sfzcm engine=MergeTree()order by id as select id,count(*)c from sfz15y group by id having count(*)>1;\r\n```\r\n**failed, but the table was created** , then i try to reduce the rows.\r\n```\r\ninsert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;\r\nQuery id: da659738-3916-466c-a392-718224d9e178\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 10.615 sec. Processed 120.01 million rows, 960.10 MB (11.31 million rows/s., 90.45 MB/s.)\r\n\r\nDESKTOP-RS3EG9A.localdomain :) select count(*) from sfzcm;\r\n\r\nQuery id: bd043a67-f81a-475e-90da-ebc6a44aabef\r\n\r\n\u250c\u2500\u2500count()\u2500\u2510\r\n\u2502 20000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec.\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;\r\n```\r\nReceived exception from server (version 20.11.3):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (total) exceeded: would use 11.08 GiB (attempt to allocate chunk of 6291456 bytes), maximum: 11.08 GiB: While executing AggregatingTransform.\r\n\r\n0 rows in set. Elapsed: 2.440 sec. Processed 92.93 million rows, 743.44 MB (38.08 million rows/s., 304.67 MB/s.)"
    },
    "satisfaction_conditions": [
      "Memory settings must be sufficient to handle the large aggregation operation",
      "Query must successfully process and aggregate data for the full range of IDs",
      "System settings must be queryable to verify configuration",
      "Memory usage must be monitorable during query execution"
    ],
    "created_at": "2020-11-16T05:12:07Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/16799",
    "source": {
      "issue_number": 16799
    },
    "initial_question": {
      "title": "DB::Exception: Unknown function avg (version 20.9.3.45 (official build))",
      "body": "```sql\r\nSELECT\r\n\t(intDiv(toUInt32(log_time), 1) * 1) * 1000 as t,\r\n\tavg(`request_time`) as a\r\nFROM\r\n\tELB_LOG.api_log\r\nWHERE\r\n\t\"log_time\" >= toDateTime(1604315080)\r\nGROUP BY\r\n\tt,\r\n\ta\r\nORDER BY\r\n\tt\r\n```"
    },
    "satisfaction_conditions": [
      "Time-based grouping must be preserved"
    ],
    "created_at": "2020-11-09T03:18:18Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/16690",
    "source": {
      "issue_number": 16690
    },
    "initial_question": {
      "title": "How can i obtain table comment and column comment",
      "body": "hi everyone, I want to ask how can I get the table comment and column comment when I writing sql query. \r\n"
    },
    "satisfaction_conditions": [
      "Column comments must be retrievable through SQL queries",
      "Query results must display both column metadata and associated comments",
      "Query must work with specified table and database context",
      "Results must include essential column attributes"
    ],
    "created_at": "2020-11-05T03:40:00Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/16254",
    "source": {
      "issue_number": 16254
    },
    "initial_question": {
      "title": "Why CH always restart automatically after killing it?",
      "body": "I found that CH server can always restart itself in about half a minute after stopping it, or killing it.\r\nIs there any backgound process to keep the CH server alive?\r\nI tried to remove the file from /etc/init.d/clickhouse-server, but the problem occurs.\r\nActually, It's not a problem, just wondering why...\r\nThanks.\r\n"
    },
    "satisfaction_conditions": [
      "The automatic restart behavior is explained by identifying the system process or configuration responsible",
      "The restart timing pattern matches the observed behavior (approximately 30 seconds or periodic checks)",
      "The restart mechanism persists even after removing /etc/init.d/clickhouse-server",
      "The mechanism is part of the system's service management infrastructure"
    ],
    "created_at": "2020-10-22T07:14:27Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/16251",
    "source": {
      "issue_number": 16251
    },
    "initial_question": {
      "title": "ALTER DROP doesn't consider size of a partition correctly",
      "body": "Hi. When I try to drop partition for a certain hour:\r\n```alter table db.table drop partition '2020-10-19 18:00:00';```\r\nI sometimes get this error:\r\n```\r\n[2020-10-22 07:21:16] Code: 359, e.displayText() = DB::Exception: Table or Partition in db.table was not dropped.\r\n[2020-10-22 07:21:16] Reason:\r\n[2020-10-22 07:21:16] 1. Size (52.01 GB) is greater than max_[table/partition]_size_to_drop (50.00 GB)\r\n[2020-10-22 07:21:16] 2. File '/var/lib/clickhouse/flags/force_drop_table' intended to force DROP doesn't exist\r\n```\r\nHowever, if I run this:\r\n```select formatReadableSize(sum(bytes_on_disk)) from (select bytes_on_disk from system.parts where table = 'table' and partition = '2020-10-19 18:00:00');```\r\nI see that the size of this partition is much lower: 48.43 GiB. So why does it tell me that I'm dropping too big partition and why does CH allow such big partitions at all if it prohibits dropping them in the end?"
    },
    "satisfaction_conditions": [
      "Size calculations must correctly account for GiB (1024-based) vs GB (1000-based) units",
      "Partition size must be accurately compared against max_partition_size_to_drop limit",
      "Size limit changes must be properly applied when modified"
    ],
    "created_at": "2020-10-22T01:27:13Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15959",
    "source": {
      "issue_number": 15959
    },
    "initial_question": {
      "title": "clickhouse integrations hdfs csv file",
      "body": "Hello,How to create a CSV table split by '|'\uff1f\r\n\r\nhdfs file content\r\n```\r\nssss|aaaa|pppp|\r\n```\r\n\r\nclickhouse create table:\r\n```\r\ncreate table xxx.xxxx\r\n(\r\n...\r\n)\r\nENGINE = HDFS('hdfs://xxx:9000/ext/ntbcp/*', 'CSV');\r\n```\r\n\r\n**how to setting format_csv_delimiter '|' ?????**\r\n"
    },
    "satisfaction_conditions": [
      "CSV file delimiter is correctly recognized as '|' character",
      "Configuration change persists across query executions"
    ],
    "created_at": "2020-10-14T10:09:11Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15903",
    "source": {
      "issue_number": 15903
    },
    "initial_question": {
      "title": "DB::Exception: Unknown data type family: DateTime64 while import from tsv, csv",
      "body": "When I import data from external file (tsv, csv) I got error:\r\n\r\n**Code: 50. DB::Exception: Unknown data type family: DateTime64**\r\n\r\nServer version 20.9.2.\r\n\r\n```\r\n24b27b0d4af5 :) SELECT * FROM system.data_type_families WHERE name LIKE 'DateTime%';\r\n\r\nSELECT * FROM system.data_type_families WHERE name LIKE 'DateTime%'\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500case_insensitive\u2500\u252c\u2500alias_to\u2500\u2510\r\n\u2502 DateTime   \u2502                1 \u2502          \u2502\r\n\u2502 DateTime64 \u2502                1 \u2502          \u2502\r\n\u2502 DateTime32 \u2502                1 \u2502          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nDateTime64 is present.\r\n\r\nMy data:\r\n\r\n\r\n**database**\r\n```\r\nCREATE TABLE log_viewing  \r\n(\r\n  id UInt32,\r\n  ts DateTime64(6, 'Europe/Moscow'),\r\n  document_id UInt16,\r\n  user_id UInt16,\r\n  element_id_max UInt16,\r\n  element_id_max_child UInt16,\r\n  element_id_min UInt16,\r\n  element_id_min_child UInt16,\r\n  status UInt8,\r\n  tz_offset Int16,\r\n  ts_local DateTime64(6, 'Europe/Moscow'),\r\n  source UInt8,\r\n  duration UInt16\r\n) ENGINE Log;\r\n```\r\n\r\nFile **log_viewing-0.tsv**\r\n```\r\nid\tts\tdocument_id\tuser_id\telement_id_min\telement_id_min_child\telement_id_max\telement_id_max_child\tstatus\ttz_offset\tts_local\tsource\tduration\r\n1592845\t2019-07-23 12:31:31.997075\t4\t2\t1\t1\t10\t10\t2\t-180\t2019-07-23 12:31:31.997075\t1\t0\r\n1592846\t2019-07-23 12:31:33.997075\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:33.997075\t1\t2000\r\n1592847\t2019-07-23 12:31:35.497075\t4\t2\t2\t2\t6\t6\t2\t-180\t2019-07-23 12:31:35.497075\t1\t1000\r\n1592848\t2019-07-23 12:31:36.497075\t4\t2\t1\t1\t4\t4\t2\t-180\t2019-07-23 12:31:36.497075\t1\t1000\r\n1592849\t2019-07-23 12:31:37.997075\t4\t2\t2\t2\t5\t5\t2\t-180\t2019-07-23 12:31:37.997075\t1\t2000\r\n1592850\t2019-07-23 12:31:39.497075\t4\t2\t1\t1\t4\t4\t2\t-180\t2019-07-23 12:31:39.497075\t1\t1000\r\n1592851\t2019-07-23 12:31:40.497075\t4\t2\t2\t2\t4\t4\t2\t-180\t2019-07-23 12:31:40.497075\t1\t1000\r\n1592852\t2019-07-23 12:31:40.997075\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:40.997075\t1\t1000\r\n1592854\t2019-07-23 12:31:48.191737\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:48.191737\t1\t7000\r\n```\r\n\r\nCommand for import:\r\n```\r\nclickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv\r\n```"
    },
    "satisfaction_conditions": [
      "Client version must support DateTime64 data type",
      "Client and server versions must be compatible",
      "Data must be successfully imported with correct data types",
      "Input file format must match the specified TabSeparatedWithNames format"
    ],
    "created_at": "2020-10-13T09:40:24Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15851",
    "source": {
      "issue_number": 15851
    },
    "initial_question": {
      "title": "How to remove default value for column?",
      "body": "Hello. Here's a table for example:\r\n```sql\r\nCREATE TABLE test.table1\r\n(\r\n        EventDate Date,\r\n        Id Int32,\r\n        Value String default 'strstrstr'\r\n)\r\nEngine = MergeTree()\r\nPARTITION BY toYYYYMM(EventDate)\r\nORDER BY Id;\r\n```\r\n\r\nI can modify default value for \"Value\" column like: `ALTER TABLE test.table1 MODIFY COLUMN Value DEFAULT 'mystring'`\r\nBut how can I remove this default value? Even if I execute `ALTER TABLE test.table1 MODIFY COLUMN Value DEFAULT ''`, it just defaults to an empty string.\r\nAnd also I cannot do this for types like Int32, because this will throw an error on future SELECT\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Default value constraint is completely removed from the column",
      "Solution works for both String and Int32 data types"
    ],
    "created_at": "2020-10-12T08:22:40Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15848",
    "source": {
      "issue_number": 15848
    },
    "initial_question": {
      "title": "Must there be enough memory to use GROUP BY?",
      "body": "The computer used for testing has 16GB of RAM.\r\nThese configurations have been set:\r\n`<max_memory_usage_for_all_queries>12000000000</max_memory_usage_for_all_queries>\r\n            <max_bytes_before_external_group_by>6000000000</max_bytes_before_external_group_by>\r\n            <max_memory_usage>12000000000</max_memory_usage>`\r\nBut still got an error:\r\nDB::Exception: Memory limit (total) exceeded: would use 13.95 GiB (attempt to allocate chunk of 134217760 bytes), maximum: 13.90 GiB: While executing AggregatingTransform."
    },
    "satisfaction_conditions": [
      "Memory usage stays below configured system limits",
      "GROUP BY queries complete successfully",
      "Memory configuration parameters are properly balanced",
      "System logs provide diagnostic information"
    ],
    "created_at": "2020-10-12T04:30:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15835",
    "source": {
      "issue_number": 15835
    },
    "initial_question": {
      "title": "tokenbf_v2 index does not drop rows in aggregation query",
      "body": "The tokenbf_v2 index does not drop rows when used in an aggregation query. That is, according to the query trace, it is used and rows are said to be dropped, but eventually all rows are scanned anyway as if the index didn't exist.\r\n\r\nIn my case I have a (large) table like:\r\n\r\n```\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64\r\n)\r\nENGINE = ReplicatedMergeTree\r\nPARTITION BY Year,\r\nORDER BY (Month, Route, Count),\r\nINDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1,\r\nSETTINGS index_granularity = 128;\r\n```\r\n\r\nRoute is a comma separated string of id's, like '242341,345223,12341'. There can be hundreds of id's in a Route.\r\n\r\nWhen I query: \r\n\r\n`select Count(*) from MYTABLE where Year = '2020' and hasToken(Route, '12341')`\r\n\r\nthe query trace shows this:\r\n\r\nKey condition: (column 2 in ['2020, '2020']), unknown, and\r\nMinMax index condition: (column 0 in ['2020', '2020']), unknown, and\r\nIndex `route_index` has dropped 254959 granules.\r\nSelected 1 parts by date, 1 parts by key, 17961 marks to read from 7839 ranges\r\nReading approx. 2299008 rows with 2 streams\r\n\r\nwhich looks good, but then it proceeds to read all rows anyway:\r\n\r\nAggregated. 233990 to 1 rows (from 0.225 MiB) in 5.498 sec. (42560.946 rows/sec., 0.041 MiB/sec.)\r\nAggregated. 104612 to 1 rows (from 0.102 MiB) in 5.774 sec. (18119.055 rows/sec., 0.018 MiB/sec.)\r\nAggregated. 130101 to 1 rows (from 0.127 MiB) in 5.775 sec. (22529.594 rows/sec., 0.022 MiB/sec.)\r\nAggregator: Merging aggregated data\r\nexecuteQuery: Read **34925324** rows, 16.27 GiB in 5.966 sec., 5853956 rows/sec., 2.73 GiB/sec.\r\n\r\nActually if I run the query like this, which does not invoke the route_index:\r\n\r\n`select Count(*) from MYTABLE where Year = '2020' and Route LIKE '%12341%'`\r\n\r\nin this case the query will be faster! The reason is it also scans all rows but does not have the extra first step of the tokenbf index.\r\n\r\nExpected behaviour:\r\n\r\nSince the tokenbf_v2 filter was able to skip 34925324 - 2299008 rows, which is 93% of the total number of rows, I expected the hasToken query to be faster than the LIKE query which didn't use any index.\r\nI don't understand why the hasToken query, after initially dropping all those rows, proceeds to scan all rows anyway."
    },
    "satisfaction_conditions": [
      "Query performance with tokenbf index must be faster than equivalent LIKE query when searching for tokens in comma-separated strings",
      "Index granularity setting must balance index analysis overhead with row filtering precision",
      "Index must correctly identify and skip irrelevant data blocks"
    ],
    "created_at": "2020-10-11T08:56:05Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15802",
    "source": {
      "issue_number": 15802
    },
    "initial_question": {
      "title": "Connecttion refused after kill the clickhouse server process",
      "body": "I have a clickhouse table containing more than 40 billion records.  One day I want to modify a parameter in my config.xml. So I attempted to stop the server but it was stuck. I guess there were many background processes working. Then I kill the clickhouse server with \"kill -9\".\r\nHowever when I tried to restart the server and run clickhouse-client again, I got \"connection refused\". \r\nIf I remove all data partitions and the corresponding table metadata (sql), it works fine again. Once I move the table data back to its position, I got the same error \"connection refused\" again.\r\n\r\n**How to reproduce**\r\nif I tried to stop the clickhosue server process with \"kill -9\", but not \"clickhouse-server stop\". the error occurs. Especially when its a massive table.\r\n\r\n* Which ClickHouse server version to use\r\nversion 20.0.10\r\n\r\nIf any ideas? Thanks."
    },
    "satisfaction_conditions": [
      "Server successfully starts and accepts connections after waiting sufficient time",
      "System handles large metadata loads during startup",
      "Server startup time is proportional to metadata size",
      "Connection attempts are properly timed relative to server readiness"
    ],
    "created_at": "2020-10-10T02:18:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15464",
    "source": {
      "issue_number": 15464
    },
    "initial_question": {
      "title": "Can't Import Parquet on macOS",
      "body": "MacOS Catalina\r\n\r\n```\r\n\u279c  ~ docker ps\r\nCONTAINER ID        IMAGE                                COMMAND             CREATED             STATUS              PORTS                                                      NAMES\r\nb9d8daab2501        yandex/clickhouse-server:20.9.2.20   \"/entrypoint.sh\"    2 hours ago         Up 2 hours          0.0.0.0:8123->8123/tcp, 0.0.0.0:9000->9000/tcp, 9009/tcp   adoring_nash\r\n\r\n\u279c  ~ cat ~/Downloads/cleand.parquet | clickhouse-client --query=\"INSERT INTO xm_rspd_data FORMAT Parquet\"\r\nCode: 73. DB::Exception: Unknown format Parquet: data for INSERT was parsed from stdin\r\n\r\n\u279c  ~ clickhouse-client\r\nClickHouse client version 20.10.1.4800 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.9.2 revision 54439.\r\n\r\nClickHouse server version is older than ClickHouse client. It may indicate that the server is out of date and can be upgraded.\r\n```"
    },
    "satisfaction_conditions": [
      "Client must be able to successfully connect to the ClickHouse server"
    ],
    "created_at": "2020-09-30T08:45:34Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15431",
    "source": {
      "issue_number": 15431
    },
    "initial_question": {
      "title": "How to convert ColumnPtr object to ColumnUInt64 * object?",
      "body": "I define a class A, which has a member `ColumnUInt64 * col` in A.h file. Then, in A.cpp, I initialize the member `col` in the way:\r\n```\r\ncol = ColumnUInt64::create();\r\n```\r\nand I got a error\r\n```\r\ncan not convert COWHelper<DB::ColumnVectorHelper, DB::ColumnVector<long unsigned int> >::MutablePtr {aka \u2018COW<DB::IColumn>::mutable_ptr<DB::ColumnVector<long unsigned int> >\u2019} to non-scalar std::__1::unique_ptr<DB::ColumnVector<long unsigned int> >\r\n```"
    },
    "satisfaction_conditions": [
      "Type conversion must maintain proper ownership semantics",
      "Resulting pointer must be compatible with ColumnUInt64 * type",
      "Original smart pointer functionality must remain intact",
      "Code must compile without type conversion errors"
    ],
    "created_at": "2020-09-29T07:12:53Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15314",
    "source": {
      "issue_number": 15314
    },
    "initial_question": {
      "title": "Fetch Single Row From Select Query Performance",
      "body": "In a SELECT query, I want to return a single row record out of billions of rows in ClickHouse. I know that Clickhouse is not meant for single queries but here I have no other choice. I would like to enhance the performance of the query as much as possible. The query is the following:\r\n\r\n```select * from products where un_id='us-f032f8df-65c9-4f0b-8df2-ddb3a436ae7e' and organization_id='test' and request_time >= '2020-09-25 00:00:00' limit 1```\r\n\r\norganization_id and request_time are both partitioning keys. \r\n\r\nMy default settings for max_threads are 4 while for max_block_size is 65505. I have also tried setting max_threads=1, max_block_size=1024 (answer from a previous post here) but this did not really help with the speed of the query. \r\n\r\n I would like to achieve a response in less than 4 seconds. Is sth like this possible with this amount of data (billions of records)? \r\n\r\nThanks in advance"
    },
    "satisfaction_conditions": [
      "Query must support filtering by un_id without requiring request_time",
      "Solution must work with distributed table architecture",
      "Data retention period of 30 days must be supported"
    ],
    "created_at": "2020-09-25T16:00:13Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15295",
    "source": {
      "issue_number": 15295
    },
    "initial_question": {
      "title": "Can I create MaterializedView over remote table?",
      "body": "There are 2 clusters.\r\nI create table `base` and table `destination` on `cluster_1`.\r\n```\r\ncreate table base (id Int8, name String) ENGINE=MergeTree() order by id;\r\ncreate table destination (id Int8, cnt Int8) ENGINE=MergeTree() order by id;\r\n```\r\nI create materialized view `view` on `cluster_1` and it's working as expected.\r\n```\r\ncreate MATERIALIZED VIEW view to destination as select id,count(name) as cnt from test;\r\n```\r\n**And here is the problem**. I create table `destination` and materialized view `view` on `cluster_2` which is based on table `test` from `cluster_1`.\r\n```\r\ncreate table destination (id Int8, cnt Int8) ENGINE=MergeTree() order by id;\r\ncreate MATERIALIZED VIEW view to destination as select id,count(name) as cnt from remote('cluster_1',default.test,'default','') group by id;\r\n```\r\nI get exception:\r\n```\r\nReceived exception from server (version 20.4.4):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Logical error while creating StorageMaterializedView. Could not retrieve table name from select query..\r\n```\r\nIt seems like materializedView source cannot be a remote table. And then I try to create a remote materializedView on `cluster_1`.\r\n```\r\ncreate MATERIALIZED VIEW remote_view to remote('cluster_2',default.destination,'default','') as select id,count(name) as cnt from test\r\n```\r\nFail again:\r\n```\r\nSyntax error: failed at position 48:\r\nExpected one of: CONSTRAINT, identifier, column declaration, INDEX, list of elements, columns or indices declaration list, table property (column, index, constraint) declaration\r\n```\r\n\r\nDo you know how to create MaterializedView over remote tables?"
    },
    "satisfaction_conditions": [
      "MaterializedView must be created on the source cluster where data insertions occur"
    ],
    "created_at": "2020-09-25T09:14:18Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15278",
    "source": {
      "issue_number": 15278
    },
    "initial_question": {
      "title": "Add summarized column to SummingMergeTree",
      "body": "Hello guys,I have a SummingMergeTree engine table like \r\n`ENGINE = SummingMergeTree((value_a,value_b))`\r\nNow I would like to add a `value_c` UInt64 to table,is that a way I can add value `value_c` to summarized column array?"
    },
    "satisfaction_conditions": [
      "The SummingMergeTree table must include the new column in its summarization process",
      "The existing table data must be preserved and remain accessible",
      "The table must remain operational after the modification",
      "The table's SummingMergeTree engine configuration must reflect the updated column list"
    ],
    "created_at": "2020-09-25T06:50:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/15161",
    "source": {
      "issue_number": 15161
    },
    "initial_question": {
      "title": "Table system.query_log does not exist for few systems",
      "body": "Hi,\r\nWe are using clickhouse on our production servers. I have noticed that query_log table sometimes exist on some server and on some it does not exist.\r\nLike I am running clickhouse on two servers with same default configs, no changes except for clickhouse version:\r\n1)Clickhouse version 20..6.3 (here the table exists) \r\n2)Clickhouse version 20..4.6 (here the table does not exist)\r\nQuestion: \r\n1) does query_log is enabled by default in latest version??\r\n2) Does it depend on disk space also?? i mean the table will grow over time and does it get deleted afterwards or something like that?? When does the table flushes??"
    },
    "satisfaction_conditions": [
      "Query log table availability matches Clickhouse version expectations",
      "Query log configuration is modifiable",
      "Query log data persistence functions correctly",
      "Query log size management options are available"
    ],
    "created_at": "2020-09-22T17:32:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14881",
    "source": {
      "issue_number": 14881
    },
    "initial_question": {
      "title": "clickhouse-local and table with 10K columns",
      "body": "need to convert TSV to Native but schema is too big for command-line..."
    },
    "satisfaction_conditions": [
      "Large schema definition must be processable without command-line limitations",
      "Table schema must be properly recognized by clickhouse-local",
      "Solution must support data input via standard input (stdin)",
      "Table definition must be accessible through a persistent storage method"
    ],
    "created_at": "2020-09-16T13:29:04Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14830",
    "source": {
      "issue_number": 14830
    },
    "initial_question": {
      "title": "Cant' execute grant SQL",
      "body": "Clikchouse Version : 20.3.17.173\r\nProblem of RBAC:\r\nWhen i execute the follow SQL, it's oK.\r\n```\r\nCREATE ROLE devgroup;\r\nGRANT SELECT,INSERT ON *.* TO devgroup;\r\n ````\r\n  \r\nBut when I `GRANT SOURCES` ,don't work;\r\n```\r\njyw-centos7-bd04 :) GRANT SOURCES ON *.* TO devgroup;\r\n\r\nSyntax error: failed at position 15:\r\n\r\nGRANT SOURCES ON *.* TO devgroup;\r\n\r\nExpected one of: EXCEPT, Comma, At, TO, token\r\n```\r\n\r\nHow can i grant privileges  about SOURCES? Thanks for you help.\r\n       \r\n  \r\n   "
    },
    "satisfaction_conditions": [
      "RBAC functionality remains operational for existing permissions"
    ],
    "created_at": "2020-09-15T02:51:44Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14794",
    "source": {
      "issue_number": 14794
    },
    "initial_question": {
      "title": "About performance between hash join and partial merge join ",
      "body": "Hi ,\r\nNow we face some join case, and I had test the newest  version about the join algorithm between hash join and partial merge join:\r\n\r\n```\r\nSET join_algorithm = 'partial_merge'\r\nSELECT number * 200000 as n, j FROM numbers(5) nums ANY LEFT JOIN (     SELECT number * 2 AS n, number AS j     FROM numbers(10000000) ) js2 USING n;\r\n\r\nMemoryTracker: Peak memory usage (for query): 457.46 MiB.\r\n5 rows in set. Elapsed: 0.918 sec. Processed 10.02 million rows, 80.18 MB (10.92 million rows/s., 87.39 MB/s.) \r\n\r\nSET join_algorithm = 'hash'\r\nSELECT number * 200000 as n, j FROM numbers(5) nums ANY LEFT JOIN (     SELECT number * 2 AS n, number AS j     FROM numbers(10000000) ) js2 USING n;\r\n\r\nMemoryTracker: Peak memory usage (for query): 845.12 MiB.\r\n5 rows in set. Elapsed: 2.023 sec. Processed 10.02 million rows, 80.18 MB (4.95 million rows/s., 39.63 MB/s.)\r\n```\r\n\r\nSeems the partial merge join has two times better than hash join in respect of memory/time cost at least. And also do some test against business data,  give the same result.\r\n\r\nAfter profiling the hash join, found all the cost from building memory table for right table. But how the partial merge join work? seems no detail doc about this.  Just plan to upgrade to the partial merge join version and want make sure the partial merge join is really good for join case. big thanks.\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Performance comparison between join algorithms must be measurable through memory usage and execution time",
      "Join algorithm behavior must be explained in terms of data sorting and processing patterns",
      "Performance characteristics must be evaluated across different data distributions",
      "Memory usage patterns must be explained for both algorithms",
      "Trade-offs between algorithms must be identified for different use cases"
    ],
    "created_at": "2020-09-14T09:55:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14687",
    "source": {
      "issue_number": 14687
    },
    "initial_question": {
      "title": "can clickhouse-copier copy data to another cluster using different timezone?",
      "body": "I don't know much about the migration principle of clickhouse-copier.\r\nI have two clusters A B, A uses Asia/Shanghai time zone, and B uses Etc/UTC.\r\nI used clickhouse-copier to migrate the data and found that the Datetime columns were actually eight hours apart.\r\n\r\nBut if I manually import it in the following way, the data is correct.\r\n```\r\nclickhouse-client -udefault -hB --query=\"select * from db.table1\" --format=CSV> table1.csv\r\nclickhouse-client -u default -hA --database=broker --query=\"INSERT INTO db.table1 FORMAT CSV\" <table1.csv\r\n```\r\nI read the log and thought clickhouse-copier was also insert after select, but the result was problematic. So I want to know the difference between clickhouse-copier's  and \"select to csv then insert\""
    },
    "satisfaction_conditions": [
      "Datetime values must be consistently represented across different time zones",
      "Time zone differences must be properly handled during data transfer",
      "Data transfer must preserve the original temporal meaning of datetime values",
      "Client/server timezone settings must be configurable"
    ],
    "created_at": "2020-09-10T09:38:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14543",
    "source": {
      "issue_number": 14543
    },
    "initial_question": {
      "title": "Dump of create tables sql scripts for database",
      "body": "Is there a standard way to get a sql script in clickhouse to create all tables in a database, or all tables and databases in general? It would be convenient to have an analogue of pg_dump to get such a script."
    },
    "satisfaction_conditions": [
      "Query output must contain valid CREATE TABLE statements for the requested database(s)",
      "Output must be in a format that can be executed as a SQL script",
      "Solution must allow filtering for specific databases or tables",
      "Output must capture complete table definitions"
    ],
    "created_at": "2020-09-07T12:56:13Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14456",
    "source": {
      "issue_number": 14456
    },
    "initial_question": {
      "title": "toUUID function executed on false IF statement branch on select",
      "body": "\r\n```\r\n\r\n\r\nSELECT if(1 = 2, 'A', 'B')\r\nFORMAT CSV\r\n\r\n\"B\"\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n```\r\n\r\nThe expected result should also be 'B' as above??  but seems to be executing toUUID\r\n```\r\nSELECT if(1 = 2, toUUID('1dd20d3aa81350af566a117a23c80aba2fbf'), NULL)\r\nFORMAT CSV\r\n\r\nReceived exception from server (version 20.7.2):\r\nCode: 6. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse string '1dd20d3aa81350af566a117a23c80aba2fbf' as UUID: syntax error at position 32 (parsed just '1dd20d3aa81350af566a117a23c80aba')\r\n```\r\n\r\nBTW the behavior of toUUID has changed from 20.5 -> 20.6, 20.7 it use to not throw an error\r\n\r\n\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Performance optimization must be maintained"
    ],
    "created_at": "2020-09-03T20:27:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/14009",
    "source": {
      "issue_number": 14009
    },
    "initial_question": {
      "title": "How to set: joined_subquery_requires_alias=0 in config.xml",
      "body": "We recently upgraded our Clickhouse server and started getting: \r\n\r\n```\r\nDB::Exception: No alias for subquery or table function in JOIN (set joined_subquery_requires_alias=0 to disable restriction).\r\n```\r\n\r\nI can change this setting in the command line client, but I want to change it in the server's config.xml. I've tried putting it under the `<yandex>` tag, and under the default user profile but neither work. Is there some special tagging that needs to be used around this specific setting? Thanks,\r\n\r\nMatt"
    },
    "satisfaction_conditions": [
      "The joined_subquery_requires_alias setting must be successfully configured server-side",
      "The configuration must persist across server restarts",
      "The setting must be applied at the user profile level",
      "The configured setting must resolve the 'No alias for subquery or table function in JOIN' error"
    ],
    "created_at": "2020-08-24T16:44:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13878",
    "source": {
      "issue_number": 13878
    },
    "initial_question": {
      "title": "How can I using Json-related-format to import multi-level nested Json data?",
      "body": "Each line of my json data looks like:\r\n```\r\n{\r\n    \"id\": 1, \r\n    \"source\": \"china\",\r\n    \"sentences\":[\r\n          { content:\"I loved apples\",\r\n            words: [ {\"content\": \"I\", \"stem\": \"i\", weight: 5}, \r\n                     {\"content\": \"loved\", \"stem\": \"love\", weight: 10}, \r\n                     {\"content\": \"apples\", \"stem\": \"apple\", weight: 1}]}, \r\n         { content:\"My parents have many apples\",\r\n            words: [ {\"content\": \"My\", \"stem\": \"my\", weight: 6}, \r\n                     {\"content\": \"parentes\", \"stem\": \"parent\", weight: 5}, \r\n                     ......\r\n                     {\"content\": \"apples\", \"stem\": \"apple\", weight: 1}]}\r\n    ]\r\n}\r\n```\r\n\r\n\"sentences\" is an array, and \"words\" is an array too. \r\n\r\n\r\nHow can I load this json data to table with JsonEachRow\uff1f\r\nBecause I want to each domain in my json schema like id, source, sentences.content, sentences.words.content, sentences.words.stem, sentences.words.weight is stored separatly. So it will use the benefits of column storage."
    },
    "satisfaction_conditions": [
      "JSON data must be successfully loaded into a columnar storage format",
      "Nested array structures must be preserved and accessible",
      "Individual JSON fields must be separately queryable",
      "Data type integrity must be maintained during import",
      "Multi-level array extraction must be supported"
    ],
    "created_at": "2020-08-19T08:07:01Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13835",
    "source": {
      "issue_number": 13835
    },
    "initial_question": {
      "title": "ALTER MODIFY ORDER BY does not work",
      "body": "Hi guys,\r\n\r\nMy clickhouse version is 20.3.10.75. When altering the table order by expression, I got the exception message as follows:\r\n\r\n```\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: Existing column version is used in the expression that was added to the sorting key. You can add expressions that use only the newly added columns.\r\n```\r\n\r\nThe table is defined as follows:\r\n```\r\nCREATE TABLE default.users_online\r\n(\r\n    `when` DateTime,\r\n    `uid` UInt64,\r\n    `duration` UInt64,\r\n    `version` Int32\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\nORDER BY (uid, when)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\nThe ALTER MODIFY ORDER BY command is ```ALTER TABLE users_online MODIFY ORDER BY (uid, when, version)```.\r\n\r\nThe expected behavior is table's order by expression should be modified.\r\nThanks."
    },
    "satisfaction_conditions": [
      "The ORDER BY modification must preserve existing sort columns",
      "The ORDER BY modification affects only new data parts",
      "The table structure must remain compatible with MergeTree engine requirements"
    ],
    "created_at": "2020-08-17T13:12:30Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13327",
    "source": {
      "issue_number": 13327
    },
    "initial_question": {
      "title": "joinGet result invalid.",
      "body": "**Describe the bug**\r\nclickhouse version: 20.6.1.4066\r\n\r\n\r\n* Queries to run that lead to unexpected result\r\n select joinGet('db.T2','id',tid) as nodeId,count(*) from db.T1 where tid='1000' group by nodeId\r\n\uff08db.T2 use storageJoin engine, join type parameter: left)\r\nresult:\r\nnodeId  count(*)\r\n0\t593\r\n43\t70\r\n\r\n**Expected behavior**\r\nexpected result:\r\nnodeId  count(*)\r\n43\t663\r\n\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "JoinGet operation must execute where the joined data exists",
      "Query results must match when the same tid value is queried",
      "Left join behavior must return actual matches instead of zeros when matches exist"
    ],
    "created_at": "2020-08-04T09:57:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13256",
    "source": {
      "issue_number": 13256
    },
    "initial_question": {
      "title": "\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 CREATE USER",
      "body": "ClickHouse server version 20.5.4 revision 54435\r\n\r\n<access_management>1</access_management>\r\n\r\nCode: 514. DB::Exception: Received from localhost:9000. DB::Exception: Not found a storage to insert user"
    },
    "satisfaction_conditions": [
      "Access management must be enabled in the system"
    ],
    "created_at": "2020-08-03T04:05:49Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13195",
    "source": {
      "issue_number": 13195
    },
    "initial_question": {
      "title": "the equal function like MySQL's group_concat",
      "body": " is there a function as MySQL's group_concat?\r\nthe example show in MySQL:\r\n>create table kpi(emp_no varchar(8),performance varchar(32),month varchar(32));\r\n>insert into kpi values (10,'A','2020-01'),(10,'A','2020-02'),(10,'C','2020-03'),(10,'B','2020-04'),(10,'A','2020-05'),(10,'A','2020-06');\r\n>insert into kpi values (20,'A','2020-01'),(20,'B','2020-02'),(20,'C','2020-03'),(20,'C','2020-04'),(20,'A','2020-05'),(20,'D','2020-06'); \r\n>insert into kpi values (30,'C','2020-03'),(30,'C','2020-04'),(30,'B','2020-05'),(30,'B','2020-06');\r\n\r\n>mysql> select emp_no,group_concat(performance order by month separator '-') kpi_list,group_concat(distinct performance order by month separator '-') kpi_uniq,group_concat(distinct performance order by month desc separator '-') kpi_uniq_desc from kpi group by emp_no;  \r\n>+--------+-------------+----------+---------------+\r\n| emp_no | kpi_list    | kpi_uniq | kpi_uniq_desc |\r\n+--------+-------------+----------+---------------+\r\n| 10     | A-A-C-B-A-A | A-C-B    | B-C-A         |\r\n| 20     | A-B-C-C-A-D | A-B-C-D  | D-C-B-A       |\r\n| 30     | C-C-B-B     | C-B      | B-C           |\r\n+--------+-------------+----------+---------------+\r\n3 rows in set (0.00 sec)\r\n\r\nBy the way i want to get each user's the count of performance level ,for examle emp_no=10 have A 4 times\r\n"
    },
    "satisfaction_conditions": [
      "Must concatenate multiple values from grouped rows into a single string",
      "Must support distinct value aggregation",
      "Must support custom value separation",
      "Must maintain group-level aggregation",
      "Must support counting occurrences of specific values within groups"
    ],
    "created_at": "2020-07-31T17:26:58Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13109",
    "source": {
      "issue_number": 13109
    },
    "initial_question": {
      "title": "How does concurrency control work?",
      "body": "Hi. I'd like to know how **ClickHouse** ensures *concurrency control* at table level. For example, if there's many applications writing in the same table at the same time, won't data get corrupted?\r\n"
    },
    "satisfaction_conditions": [
      "Concurrent write operations must not result in data corruption",
      "Read operations must access consistent data snapshots",
      "Write operations must be atomic",
      "Multiple operations (reads, writes, alterations) must be able to execute simultaneously",
      "Concurrency control mechanism must be documented and explained"
    ],
    "created_at": "2020-07-30T13:43:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/13066",
    "source": {
      "issue_number": 13066
    },
    "initial_question": {
      "title": "What does the tailing number of system.query_log_N mean?",
      "body": "ClickHouse is an excellent OLAP database\r\n\r\nWhile I'm looking for the slow queries, I found many `query_log_N` and `query_thread_log_N` tables.\r\n\r\n```SQL\r\n\r\nSHOW TABLES\r\n\r\n...\r\n\u2502 query_log                      \u2502\r\n\u2502 query_log_0                    \u2502\r\n\u2502 query_log_1                    \u2502\r\n\u2502 query_log_2                    \u2502\r\n\u2502 query_log_3                    \u2502\r\n\u2502 query_log_4                    \u2502\r\n\u2502 query_thread_log               \u2502\r\n\u2502 query_thread_log_0             \u2502\r\n\u2502 query_thread_log_1             \u2502\r\n\u2502 query_thread_log_2             \u2502\r\n\u2502 query_thread_log_3             \u2502\r\n\u2502 query_thread_log_4             \u2502\r\n...\r\n```\r\n\r\nI am developing a `monitoring system for slow queries` and considered to use the query_log table,\r\n\r\nbut there are other tables with the same postfix of names.\r\n\r\nI wonder what the tailing numbers of tables mean.\r\n\r\nThank you."
    },
    "satisfaction_conditions": [
      "Explanation clarifies the relationship between numbered tables and schema changes"
    ],
    "created_at": "2020-07-29T12:28:12Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/12694",
    "source": {
      "issue_number": 12694
    },
    "initial_question": {
      "title": "how to kill optimize FINAL  in backgroud",
      "body": "i optimize a big table (replicedReplacingMergeTree)   , it makes io  100%   ,so i want to kill this optimize FINAL  \r\nhow can i do ?\r\n\r\n<Debug> default.users (ReplicatedMergeTreeQueue): Not executing log entry MERGE_PARTS for part 20200630_0_307423_23 because source parts size (67.06 GiB) is greater than the current maximum (7.58 GiB).\r\n\r\nthanks advance"
    },
    "satisfaction_conditions": [
      "Background merge operations must be controllable",
      "System I/O load must be reduced",
      "Table functionality must be preserved",
      "Solution must account for table engine type",
      "Resource utilization must be configurable"
    ],
    "created_at": "2020-07-22T22:56:03Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/12565",
    "source": {
      "issue_number": 12565
    },
    "initial_question": {
      "title": "Support configuration hot reload of merge_tree_settings?",
      "body": "Hi team:\r\n    I found there no hot reload ability of merge_tree_settings like max_parts_in_total, parts_to_delay_insert and so on. Are there any ways to implement hot reload?"
    },
    "satisfaction_conditions": [
      "Configuration changes must take effect without system restart",
      "Changes must affect merge_tree_settings parameters",
      "Changes must persist at the table level"
    ],
    "created_at": "2020-07-17T14:57:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/11888",
    "source": {
      "issue_number": 11888
    },
    "initial_question": {
      "title": "Multithreading reading the same FD problem.",
      "body": "```seek``` and ```read``` are not atomic operations.I didn't see the lock.Will there be concurrency issues?\r\n"
    },
    "satisfaction_conditions": [
      "File descriptor access must be thread-safe",
      "Each thread must have exclusive access to its file descriptor",
      "File integrity must be maintained during concurrent operations"
    ],
    "created_at": "2020-06-23T12:54:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/11396",
    "source": {
      "issue_number": 11396
    },
    "initial_question": {
      "title": "Does CK support sharding_key policy that write to all shards?",
      "body": "Hey guys, I'm new to CK. So now I want to build a distributed table with a sharding_key policy that when I write data to the distributed table, the data will be sent to all shards. \r\n\r\nI know CK now only supports rand() or built-in hash functions or UserID. Btw, does it mean that each time the data will only be sent to one shard?\r\n "
    },
    "satisfaction_conditions": [
      "Data must be replicated across all shard nodes",
      "Dimension table must be accessible for joins from all shards",
      "Solution must work across different shards, not just within replicas",
      "Must support small dimension table data distribution"
    ],
    "created_at": "2020-06-03T03:11:47Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/11056",
    "source": {
      "issue_number": 11056
    },
    "initial_question": {
      "title": "Get the value for the previous point of time. IOT case",
      "body": "I have the table with keys (time, facility). For each key, the (value) received from the sensor is stored. For instance:\r\n\r\n\u250c\u2500date\u2500\u2500\u2500\u2500\u252cfacility\u252cvalue\u252c\r\n\u2502 2017-09-09 \u2502 10002\u2502 10  \u2502\r\n\u2502 2017-09-10 \u2502 10001\u2502 12      \u2502\r\n\u2502 2017-09-12 \u2502 10002\u2502 15      \u2502\r\n\u2502 2017-09-15 \u2502 10001\u2502 17      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2518\r\n\r\nI want to calculate an increase in the current value compared to the previous one. Something like this:\r\n\r\n\u250c\u2500date\u2500\u2500\u2500\u2500\u252cfacility\u252c\u2500value\u252c\r\n\u2502 2017-09-12 \u2502 10002\u2502 15/10 \u2502 // 15 current, 10 previous for facility 10002\r\n\u2502 2017-09-15 \u2502 10001\u2502 17/12 \u2502 // 17 current, 12 previous for facility 10002\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2518\r\n\r\nTo get this result, I need to use JOIN, but as I understood from the documentation, the comparison condition in JOIN operation is only for equality, and inequalities cannot be used.\r\n\r\nOf course, I can do a Cartesian product, then do a date comparison (get all dates less than the current one) in the WHERE section and then select the maximum date from the filtered ones. But it is very time and memory consuming!\r\n\r\nPlease help me create an optimal query, because this case is typical for IOT"
    },
    "satisfaction_conditions": [
      "Query must return both current and previous values for each facility",
      "Results must be grouped by facility ID",
      "Time ordering must be preserved within each facility's data",
      "Query must handle irregular time intervals between measurements"
    ],
    "created_at": "2020-05-20T01:21:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/10883",
    "source": {
      "issue_number": 10883
    },
    "initial_question": {
      "title": "Multiple clusters, same servers",
      "body": "I have  a distributed table with replication (using zookeeper).\r\n\r\n```\r\n\r\ncreate table s_actions (...)\r\nPARTITION BY toMonday(createdon)\r\nORDER BY\r\n  (createdon, user__id) SAMPLE BY user__id SETTINGS index_granularity = 8192\r\n \r\n```\r\n\r\nI have primary replicas with 3 servers with a lot of memory and cpu . Second replica  has slow ssd ,less cpu and ram and is used for replication backup and then daily backups (FREEZE PARTITION).\r\n\r\nI have distributed table like\r\n\r\n````\r\n CREATE TABLE actions (\r\n....\r\n) ENGINE = Distributed(\r\n  rep,\r\n  actions,\r\n  s_actions,\r\n  cityHash64(toString(user__id))\r\n)\r\n\r\n```` \r\nrep cluster has only one replica for each shard. So If any server from primary replica fails everything will be broken. I want to create rep_write cluster in clickhouse config with secondary replicas to allow writes to secondary or primary replicas . Reads are not needed to be protected. \r\n\r\nProblem is that I'm using hashing function instead of random to optimize performance. Is it safe to define separate clusters with same (by order) servers (with extra replicas) and use distributed tables with same hashing function? \r\n"
    },
    "satisfaction_conditions": [
      "Multiple clusters using identical server ordering must maintain consistent shard distribution",
      "Hash-based distribution must remain functional across all configured clusters",
      "Cluster configuration must be verifiable through system tables"
    ],
    "created_at": "2020-05-13T10:26:06Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/10252",
    "source": {
      "issue_number": 10252
    },
    "initial_question": {
      "title": "Partitions have  active and is_frozen  both flags at the same time",
      "body": "Hi mates.\r\nI couldn't find any information about my trouble\r\nSo,\r\nI have big table with engine ReplicatedMergeTree. I hold the data during 90 days there.\r\nthe structure is\r\n```\r\nCREATE TABLE IF NOT EXISTS default.message_by_chid (\r\n...\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/cluster_cross_r1/tables/{shard}/message_by_chid', '{replica}')\r\nPARTITION BY toMonday(logs_SendDateTime)\r\nORDER BY (logs_SMSChannelId, logs_SendDateTime, prices_SMSChannelId) TTL logs_SendDateTime + toIntervalDay(90)\r\nSETTINGS index_granularity = 8192;\r\n```\r\nSo, I have a problem that old data was not deleted.\r\nI've checked  system.parts table to get more information about it. I saw that a lot partitions have flags\r\nactive and is_frozen = 1. However, I had not setup these partitions like snapshots. I have other tables  with TTL and  the same engine, but I don't see the same behavior there. \r\n\r\n```\r\n | database | table | partition | name | rows | is_frozen | active | refcount | level | data_version | modification_time | remove_time | min_time | max_time |\r\n | default | message_by_chid | 2019-12-30 | 20191230_0_639598_7365_639615 | 0 | 1 | 1 | 1 | 7365 | 639615 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 |\r\n | default | message_by_chid | 2020-01-06 | 20200106_0_606705_8405_606719 | 0 | 1 | 1 | 1 | 8405 | 606719 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 | 0000-00-00 00:00:00 |\r\n | default | message_by_chid | 2020-01-13 | 20200113_0_537334_6644_537348 | 559835705 | 1 | 1 | 1 | 6644 | 537348 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-13 00:00:00 | 2020-01-19 23:59:59 |\r\n | default | message_by_chid | 2020-01-20 | 20200120_0_452519_8594_466711 | 684299622 | 1 | 1 | 1 | 8594 | 466711 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-20 00:00:00 | 2020-01-26 23:59:59 |\r\n | default | message_by_chid | 2020-01-20 | 20200120_452520_466699_47_466711 | 28731947 | 1 | 1 | 1 | 47 | 466711 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-21 09:00:00 | 2020-01-26 15:21:11 |\r\n | default | message_by_chid | 2020-01-27 | 20200127_0_337054_4649_337066 | 885381133 | 1 | 1 | 1 | 4649 | 337066 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-01-27 00:00:00 | 2020-02-02 23:59:59 |\r\n | default | message_by_chid | 2020-02-03 | 20200203_0_412266_7930_412278 | 802769920 | 1 | 1 | 1 | 7930 | 412278 | 2020-04-14 08:50:59 | 0000-00-00 00:00:00 | 2020-02-03 00:00:00 | 2020-02-09 23:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_0_210652_42_412585 | 974574244 | 0 | 1 | 1 | 42 | 412585 | 2020-04-14 09:16:57 | 0000-00-00 00:00:00 | 2020-02-10 00:00:00 | 2020-02-16 18:39:43 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_210653_348867_4786_412585 | 70452324 | 0 | 1 | 1 | 4786 | 412585 | 2020-04-14 08:52:50 | 0000-00-00 00:00:00 | 2020-02-11 13:15:54 | 2020-02-16 23:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_348868_360495_36_412585 | 12137004 | 1 | 1 | 1 | 36 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-15 13:00:00 | 2020-02-16 07:10:47 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_367751_371362_35_412585 | 4304951 | 1 | 1 | 1 | 35 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 03:00:00 | 2020-02-16 10:28:42 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_360496_367750_34_412585 | 2074170 | 1 | 1 | 1 | 34 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 03:00:00 | 2020-02-16 08:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_371363_387310_37_412585 | 7889891 | 1 | 1 | 1 | 37 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 09:00:00 | 2020-02-16 14:59:59 |\r\n | default | message_by_chid | 2020-02-10 | 20200210_387311_398516_53_412585 | 4373265 | 1 | 1 | 1 | 53 | 412585 | 2020-04-14 08:51:00 | 0000-00-00 00:00:00 | 2020-02-16 09:00:00 | 2020-02-16 22:59:59 |\r\n\r\n```\r\n\r\nCould  you explain a logic for that? "
    },
    "satisfaction_conditions": [
      "TTL functionality operates independently of partition frozen status",
      "Partition frozen status is consistent across replicas",
      "TTL execution timing aligns with ttl.txt specifications"
    ],
    "created_at": "2020-04-14T11:18:48Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/10062",
    "source": {
      "issue_number": 10062
    },
    "initial_question": {
      "title": "Duplicated primary key in materialized view",
      "body": "I have a table\r\n```sql\r\nCREATE TABLE user_video_view (\r\n\tuser_id\t\t\t\tUInt64,\r\n\tvideo_id\t\t\tUInt64,\r\n\tvisitor_session_id\tUUID,\r\n\tvisitor_id\t\t\tUInt64,\r\n\tvisitor_ip\t\t\tString,\r\n\tvisitor_user_agent\tString,\r\n\tcreated_at\t\t\tDateTime\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY (toYYYYMM(created_at), user_id, video_id)\r\nORDER BY (created_at, user_id, video_id, visitor_session_id)\r\n```\r\nand materialized view based on it\r\n```sql\r\nCREATE MATERIALIZED VIEW grouped_user_video_view\r\nENGINE = SummingMergeTree()\r\nPARTITION BY week\r\nORDER BY (user_id, week)\r\nPOPULATE\r\nAS SELECT\r\n\tintDiv(toRelativeWeekNum(created_at) - toRelativeWeekNum(toDateTime('2020-03-23 00:00:00')), 2) AS week,\r\n\tuser_id,\r\n\tcount() AS view_count\r\nFROM user_video_view\r\nGROUP BY user_id, week\r\n```\r\n\r\nFor some combination of `user_id` and `week` I have duplicated rows in response:\r\n```\r\n\u250c\u2500week\u2500\u252c\u2500user_id\u2500\u252c\u2500view_count\u2500\u2510\r\n\u2502    0 \u2502     159 \u2502          1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500week\u2500\u252c\u2500user_id\u2500\u252c\u2500view_count\u2500\u2510\r\n\u2502    0 \u2502       5 \u2502          2 \u2502\r\n\u2502    0 \u2502      15 \u2502          5 \u2502\r\n\u2502    0 \u2502      16 \u2502          4 \u2502\r\n\u2502    0 \u2502      17 \u2502          1 \u2502\r\n\u2502    0 \u2502      42 \u2502          2 \u2502\r\n\u2502    0 \u2502      45 \u2502          3 \u2502\r\n\u2502    0 \u2502     159 \u2502          2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIs this an expected behavior?"
    },
    "satisfaction_conditions": [
      "Query results must include final aggregation using GROUP BY",
      "Duplicate rows in materialized view results must be acknowledged as expected behavior",
      "Aggregation operations must correctly sum the view_count values",
      "Performance impact of aggregation approach must be considered"
    ],
    "created_at": "2020-04-06T09:16:00Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9701",
    "source": {
      "issue_number": 9701
    },
    "initial_question": {
      "title": "how to make MATERIALIZED VIEW update automatically when several origin-tables were inserted",
      "body": "Create two origin-tables with only two fields, associated by id\uff1a\r\n```\r\nCREATE TABLE default.test0 (\r\n`id` String,\r\n `name` String\r\n) ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\n\r\nCREATE TABLE default.test00 (\r\n`id` String,\r\n `name2` String\r\n) ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\n```\r\ncreate the MATERIALIZED VIEW\uff1a\r\n```\r\ncreate  MATERIALIZED VIEW default.test_view  ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\nAS select t0.id,name,name2 from `default`.test0 t0 join `default`.test00 t00 on t0.id=t00.id\r\n```\r\nInsert into origin-tables\uff0ctest0 and test00\uff1a\r\n```\r\ninsert into `default`.test0 values ('1','name1')\r\n\r\ninsert into `default`.test00 values ('1','name10')\r\n```\r\nthen select from the view\uff1a\r\n`select * from default.test_view `\r\n\r\nbut the resultset is empty.\r\n\r\n> id|name|name2|\r\n> --|----|-----|\r\n\r\n\r\nBut if I create a 'MATERIALIZED VIEW' for a single table, the view can auto update after the single table was inserted.like:\r\n```\r\ncreate  MATERIALIZED VIEW default.test_view0  ENGINE = MergeTree PARTITION BY id ORDER BY id SETTINGS index_granularity = 8192\r\nAS select  id,name FROM `default`.test0 \r\n```\r\n\r\n\r\nPlease tell me how to make the MATERIALIZED VIEW update automatically after origin-tables were inserted?  Thank you!"
    },
    "satisfaction_conditions": [
      "Solution must work within ClickHouse's materialized view update mechanics"
    ],
    "created_at": "2020-03-17T06:42:49Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9616",
    "source": {
      "issue_number": 9616
    },
    "initial_question": {
      "title": "upgrade clickhouse cluster",
      "body": "Now version:19.16.2.2 (official build)\r\nupgrade to:20.1.6.30-2\r\n\r\nIs this ok?\r\nWill it affect zookeeper?"
    },
    "satisfaction_conditions": [
      "Upgrade path from version 19.16.2.2 to 20.1.6.30-2 is technically feasible",
      "Existing Zookeeper integration continues to function after upgrade"
    ],
    "created_at": "2020-03-12T05:17:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9541",
    "source": {
      "issue_number": 9541
    },
    "initial_question": {
      "title": "Materialized View with targeting past data",
      "body": "I am having an issue with a Materialized View which targets the past data. I know that MV works like a trigger for inserts to a table. \r\n\r\nI need a Materialized View only for yesterday. I have the following MV:\r\n\r\n```\r\nCREATE MATERIALIZED VIEW default.chart_yesterday\r\nENGINE = ReplicatedSummingMergeTree(\r\n     '/clickhouse/tables/{shard}/default/chart_yesterday',\r\n     '{replica}')\r\n     PARTITION BY toYYYYMM(date)\r\n     ORDER BY (date, hour, cityHash64(organization_id)\r\n)\r\nSAMPLE BY cityHash64(organization_id)\r\nSETTINGS index_granularity = 8192\r\nPOPULATE AS\r\nSELECT\r\n     SUM(rejected) AS clr,\r\n     (count() - clr) AS cla,\r\n     toDate(request_time) AS date,\r\n     toHour(request_time) as hour,\r\n     organization_id\r\nFROM mytable_sharded\r\nWHERE date = yesterday()\r\nGROUP BY date, hour, organization_id\r\nORDER BY hour;\r\n```\r\n\r\nAfter creating the VM, I have data only for yesterday, everything fine. but after a day, the VM has no data\r\n\r\nDoes it mean that since there is no trigger for yesterday's data, so VM doesn't get triggered, so no data?"
    },
    "satisfaction_conditions": [
      "Historical data older than yesterday must not appear in the view",
      "View must maintain correct aggregations of the source data"
    ],
    "created_at": "2020-03-06T16:06:16Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9504",
    "source": {
      "issue_number": 9504
    },
    "initial_question": {
      "title": "Error in system.replication_queue ",
      "body": "version  20.2.1\r\n\r\nThere is  an error in table system.replication_queue  .\r\n\r\n Not executing log entry for part 20200213_2040_2040_1_2016 because another log entry for the same part is being processed. This shouldn't happen often.\r\n\r\n\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500replica_name\u2500\u252c\u2500position\u2500\u252c\u2500node_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500required_quorum\u2500\u252c\u2500source_replica\u2500\u252c\u2500new_part_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500parts_to_merge\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500is_detach\u2500\u252c\u2500is_currently_executing\u2500\u252c\u2500num_tries\u2500\u252c\u2500last_exception\u2500\u252c\u2500\u2500\u2500last_attempt_time\u2500\u252c\u2500num_postponed\u2500\u252c\u2500postpone_reason\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500last_postpone_time\u2500\u2510\r\n\u2502 my_sdap  \u2502 dm_user_behavior_events \u2502 replica-001  \u2502        0 \u2502 queue-0000104788 \u2502 MERGE_PARTS \u2502 2020-03-04 07:46:52 \u2502               0 \u2502 replica-001    \u2502 20200213_2040_2040_1_2016 \u2502 ['20200213_2040_2040_0_2016'] \u2502         0 \u2502                      0 \u2502         0 \u2502                \u2502 0000-00-00 00:00:00 \u2502          3801 \u2502 Not executing log entry for part 20200213_2040_2040_1_2016 because another log entry for the same part is being processed. This shouldn't happen often. \u2502 2020-03-04 08:36:43 \u2502"
    },
    "satisfaction_conditions": [
      "System continues normal operation despite replication queue postponements",
      "Replication queue messages are correctly interpreted as status notifications rather than errors",
      "Multiple postponements of the same part operation are handled gracefully by the system"
    ],
    "created_at": "2020-03-04T00:38:39Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9406",
    "source": {
      "issue_number": 9406
    },
    "initial_question": {
      "title": "Can I controll the insert rate of syntex 'INSERT INTO SELECT'?",
      "body": "I would like to know what is the factor that controll the insert rate of syntex 'INSERT INTO SELECT'?It depends on ClickHouse internal calculation?\r\nAny parameter can I change to increase or decrease the insert rate?"
    },
    "satisfaction_conditions": [
      "The solution must provide a way to control data insertion speed",
      "The solution must be implementable within the current system constraints",
      "The controlled rate must be measurable"
    ],
    "created_at": "2020-02-27T08:31:12Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9115",
    "source": {
      "issue_number": 9115
    },
    "initial_question": {
      "title": "How to update using Join with 2 join condition",
      "body": "I want to make below query as clickhouse query.\r\n```sql\r\nupdate table1 \r\nset table1.col4 = table2.col4\r\nfrom table2 \r\nwhere table1.col1 = table2.col1 and table1.col2 = table2.col2 and table2.col3='2020-01-02';\r\n```\r\n\r\nI made a query like below, But, I got error and don`t know how to make 2 join condition.\r\n\r\n```sql\r\n\r\nCREATE TABLE test1\r\n(\r\n    `col1` Int8, \r\n    `col2` String, \r\n    `col3` Date, \r\n    `col4` UInt16\r\n)\r\nENGINE = Log\r\n\r\nINSERT INTO test1 VALUES(1,'001','2020-01-01', 1)(1,'002','2020-01-01', 1)(2,'001','2020-01-01', 2)(2,'002','2020-01-02', 3)(2,'003','2020-01-04', 5);\r\n\r\n-- create join engine\r\nCREATE TABLE test_join AS test1\r\nENGINE = Join(ANY, LEFT, col1, col2)\r\n\r\nOk.\r\n\r\nINSERT INTO test_join SELECT *\r\nFROM test1\r\nWHERE col3 = '2020-01-02'\r\n\r\n-- update\r\n:) ALTER TABLE test1 UPDATE col4 = joinGet('test_join', 'col4', col1, col2);\r\n\r\nSyntax error: failed at position 73 (end of query):\r\n\r\nALTER TABLE test1 UPDATE col4 = joinGet('test_join', 'col4', col1, col2);\r\n\r\nExpected one of: AND, OR, token, WHERE, NOT, BETWEEN, LIKE, IS, NOT LIKE, IN, NOT IN, GLOBAL IN, GLOBAL NOT IN, Comma, QuestionMark\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "Data from table2 must correctly update matching records in table1 based on multiple join conditions",
      "The update operation must respect the date filter condition (col3='2020-01-02')",
      "The solution must be compatible with ClickHouse's update semantics",
      "Data integrity must be maintained during the update operation"
    ],
    "created_at": "2020-02-14T10:03:02Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9083",
    "source": {
      "issue_number": 9083
    },
    "initial_question": {
      "title": "multiple update queries at the same time",
      "body": "If I send more than one update query at the same time and the queries update same data, How does the ClickHouse handle data??\r\n\r\nDoes the ClickHouse handle query sequentially? If so, afterward update query is waiting for forward update query? \r\n\r\nIf the queries update data at the same time, How does the ClickHouse keep data integrity?"
    },
    "satisfaction_conditions": [
      "Multiple update queries must be processed in a sequential order",
      "Data consistency must be guaranteed after all updates complete",
      "System must handle intermediate state visibility during updates",
      "Multiple mutations affecting the same data must be coordinated"
    ],
    "created_at": "2020-02-12T01:42:16Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9045",
    "source": {
      "issue_number": 9045
    },
    "initial_question": {
      "title": "drop partition for all shard in one DDL execution",
      "body": "version:19.11.3\r\nbackground:\r\nthree servers:A,B,C ares running clickhouse cluster.\r\n2 shard 1 replicas config looks like below:\r\n``` \r\n<cluster_test>\r\n        <shard>\r\n            <replica>\r\n                <host>B</host>\r\n                <port>9000</port>\r\n            </replica>\r\n        </shard>\r\n        <shard>\r\n            <replica>\r\n                <host>C</host>\r\n                <port>9000</port>\r\n            </replica>\r\n        </shard>\r\n</cluster_test>  \r\n```\r\n\r\nIn server A,there is a test.hist_all table created by below script:\r\n**CREATE TABLE hits_all AS tutorial.hits_local_all\r\nENGINE = Distributed(cluster_test, test, hits_local, rand());**\r\n\r\nIn server B&C,there are concrete table test.hist to store data created by below script:\r\n**CREATE TABLE test.hits_local (...) ENGINE = MergeTree PARTITION BY dayno ....**\r\n\r\nQuestion:\r\nCan I drop patition data with SQL below **in just one execution**.\r\n#ALTER TABLE hist on cluster cluster_test DROP PARTITION 20200114;#"
    },
    "satisfaction_conditions": [
      "Partition data must be removed from all shards in the cluster",
      "Operation must be initiated through a single SQL command",
      "Operation must target the concrete MergeTree tables, not the Distributed table",
      "Command must be compatible with the cluster configuration"
    ],
    "created_at": "2020-02-07T09:03:53Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9044",
    "source": {
      "issue_number": 9044
    },
    "initial_question": {
      "title": "Import tsv exception: Cannot parse input: expected \\t ",
      "body": "I imported tsv file to CH, but, I got an error.\r\nHow can I fix this?\r\n\r\ntsv file for import\r\n```tsv\r\n2010-01-01\tabc\t1\t\t\t2020-02-07\r\n2020-01-02\t\t2\t\t\t\r\n2020-01-03\taaa\t\t\t\t2020-02-04\r\n```\r\n\r\ntable\r\n```sql\r\nCREATE TABLE default.test3\r\n(\r\n    `EventDate` Date, \r\n    `CounterID` Nullable(String), \r\n    `UserID` Nullable(UInt32), \r\n    `day1` Nullable(Date), \r\n    `day2` Nullable(Date), \r\n    `day3` Nullable(Date)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY EventDate\r\n```\r\n\r\nError\r\n```\r\n# cat test.tsv | clickhouse-client --query=\"INSERT INTO test3 FORMAT TSV\"\r\nCode: 27, e.displayText() = DB::Exception: Cannot parse input: expected \\t before: -07\\n2020-01-02\\t\\t2\\t\\t\\t\\n2020-01-03\\taaa\\t\\t\\t\\t2020-02-04\\n: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: EventDate, type: Date,             parsed text: \"2010-01-01\"\r\nColumn 1,   name: CounterID, type: Nullable(String), parsed text: \"abc\"\r\nColumn 2,   name: UserID,    type: Nullable(UInt32), parsed text: \"1\"\r\nColumn 3,   name: day1,      type: Nullable(Date),   parsed text: \"<TAB><TAB>2020-02\"\r\nERROR: garbage after Nullable(Date): \"-07<LINE FEED>2020-0\"\r\n\r\n, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. 0xbc31d9c Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n1. 0x4f6ccd9 DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n2. 0x496bab9 ?  in /usr/bin/clickhouse\r\n3. 0x92ed647 DB::TabSeparatedRowInputFormat::readRow(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, DB::RowReadExtension&)  in /usr/bin/clickhouse\r\n4. 0x97e5f69 DB::IRowInputFormat::generate()  in /usr/bin/clickhouse\r\n5. 0x91a4c27 DB::ISource::work()  in /usr/bin/clickhouse\r\n6. 0x9169435 DB::InputStreamFromInputFormat::readImpl()  in /usr/bin/clickhouse\r\n7. 0x8a6d32f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n8. 0x94eb632 DB::ParallelParsingBlockInputStream::parserThreadFunction(unsigned long)  in /usr/bin/clickhouse\r\n9. 0x4fa4657 ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>)  in /usr/bin/clickhouse\r\n10. 0x4fa4c84 ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n11. 0x4fa3b77 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n12. 0x4fa212f ?  in /usr/bin/clickhouse\r\n13. 0x7e25 start_thread  in /usr/lib64/libpthread-2.17.so\r\n14. 0xfebad clone  in /usr/lib64/libc-2.17.so\r\n (version 20.1.2.4 (official build))\r\nCode: 27. DB::Exception: Cannot parse input: expected \\t before: -07\\n2020-01-02\\t\\t2\\t\\t\\t\\n2020-01-03\\taaa\\t\\t\\t\\t2020-02-04\\n: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: EventDate, type: Date,             parsed text: \"2010-01-01\"\r\nColumn 1,   name: CounterID, type: Nullable(String), parsed text: \"abc\"\r\nColumn 2,   name: UserID,    type: Nullable(UInt32), parsed text: \"1\"\r\nColumn 3,   name: day1,      type: Nullable(Date),   parsed text: \"<TAB><TAB>2020-02\"\r\nERROR: garbage after Nullable(Date): \"-07<LINE FEED>2020-0\"\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "TSV file with empty fields must be successfully imported into ClickHouse table",
      "Empty fields in TSV must be properly interpreted as NULL values",
      "Data types in imported rows must match the table schema",
      "Original data structure and field separation must be preserved"
    ],
    "created_at": "2020-02-07T08:49:13Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/8999",
    "source": {
      "issue_number": 8999
    },
    "initial_question": {
      "title": "Create MATERIALIZED VIEW against ReplicatedMergeTree and Distributed tables",
      "body": "I am trying to create VM for my cluster. before getting to the point here is the details:\r\n\r\nI have 2 shards and 2 replicas in each.\r\n\r\nDetails:\r\n\r\ncluster name: _clicks_cluster_\r\n\r\nI have a replicated table:\r\n```\r\nCREATE TABLE default.clicks_replicated\r\n(\r\n    ...\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated', '{replica}')\r\nPARTITION BY (...)\r\nORDER BY (...)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n\r\nThen I created the distributed from it:\r\n```\r\nCREATE TABLE IF NOT EXISTS default.clicks_distributed AS default.clicks_replicated\r\nENGINE = Distributed(clicks_cluster, default, clicks_replicated, cityHash64(my_column));\r\n```\r\n\r\nNow I want to create a VM.:\r\n\r\nBut I found out I don't get the new data if I create it against `clicks_distributed`  with `ENGINE = SummingMergeTree` \r\n\r\nAlso creating it against `clicks_replicated` will lead to incomplete data per replica. \r\n\r\nwhat would be the query for creating VM in this case?\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Materialized view must capture all new data insertions",
      "View configuration must be identical across all nodes"
    ],
    "created_at": "2020-02-04T17:48:51Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/8686",
    "source": {
      "issue_number": 8686
    },
    "initial_question": {
      "title": "New installation on Ubuntu VM: Password required for user default. ",
      "body": "I can't get the initial setup to work on my newly created Ubuntu 18.04.3 LTS virtual machine.\r\nI followed the instructions on the website by executing the following terminal commands:\r\n\r\n```\r\nsudo apt-get install dirmngr    # optional\r\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4    # optional\r\nsudo apt-get update\r\nsudo apt-get install clickhouse-client clickhouse-server\r\n```\r\n\r\nThis should suffice, right? I then start the server, try to start the client and this is what happens:\r\n```\r\ntaxel@taxel-VirtualBox:~$ sudo service clickhouse-server start\r\ntaxel@taxel-VirtualBox:~$ clickhouse-client\r\nClickHouse client version 19.17.6.36 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 194. DB::Exception: Received from localhost:9000. DB::Exception: Password required for user default. \r\n```\r\nI also tried changing the default password from `<password></password>` to `<password>123</password>` and logging in via `clickhouse-client --password=123` but it outputs that the password is wrong (and yes, I have ensured the xml file is saved and the server is restarted)\r\n\r\nAny help would be much appreciated.\r\n"
    },
    "satisfaction_conditions": [
      "ClickHouse service must be running and accessible"
    ],
    "created_at": "2020-01-16T15:05:24Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/8531",
    "source": {
      "issue_number": 8531
    },
    "initial_question": {
      "title": "About deleting new values every day affects performance",
      "body": "\r\nA multi-million data table needs to delete a part of the data and add a new part every day. Will this affect the performance of ck query?\r\n\r\n ENGINE = MergeTree() ORDER BA_MONTH"
    },
    "satisfaction_conditions": [
      "Data deletion operations must not degrade ongoing query performance",
      "Deletion operation must be resource-efficient",
      "Solution must support regular (daily) data cycling",
      "Operation must reliably complete on large datasets"
    ],
    "created_at": "2020-01-05T09:02:19Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/8506",
    "source": {
      "issue_number": 8506
    },
    "initial_question": {
      "title": "MemoryTracker exception despite unlimited memory setting",
      "body": "Received the following exception multiple times during writes to a node (stack trace at end).\r\n\r\n`B::Exception: Memory limit (total) exceeded: would use 74.51 GiB (attempt to allocate chunk of 4217732 bytes), maximum: 74.51 GiB (version 19.17.4.11) `\r\n\r\nI can't figure out what memory limit is being exceeded.  (These errors are happening during writes).  The 74.51GiB value is not configured anywhere, and the box itself has 792G of total memory, of which we are only using a small fraction.  The only configured limit on the default profile is per query of 100GiB `max_memory_usage_per_query = 107374182400`.\r\n\r\nThese errors seem to correspond to large merges; when the merge finally completed the errors cleared up.  Is ClickHouse possibly misreading the total available system memory? \r\n\r\n\r\n> 0. 0x3512b60 StackTrace::StackTrace() /usr/bin/clickhouse\r\n> 1. 0x351195e MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 2. 0x3510d39 MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 3. 0x3510d39 MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 4. 0x3510d39 MemoryTracker::alloc(long) /usr/bin/clickhouse\r\n> 5. 0x3561138 DB::WriteBufferFromFileBase::WriteBufferFromFileBase(unsigned long, char*, unsigned long) /usr/bin/clickhouse\r\n> 6. 0x35443fb DB::WriteBufferFromFileDescriptor::WriteBufferFromFileDescriptor(int, unsigned long, char*, unsigned long) /usr/bin/clickhouse\r\n> 7. 0x6bf42bc DB::WriteBufferFromFile::WriteBufferFromFile(std::string const&, unsigned long, int, unsigned int, char*, unsigned long) /usr/bin/clickhouse\r\n> 8. 0x6c062d6 DB::createWriteBufferFromFileBase(std::string const&, unsigned long, unsigned long, unsigned long, int, unsigned int, char*, unsigned long) /usr/bin/clickhouse\r\n> 9. 0x696c006 DB::IMergedBlockOutputStream::ColumnStream::ColumnStream(std::string const&, std::string const&, std::string const&, std::string const&, std::string const&, std::shared_ptr<DB::ICompressionCodec> const&, unsigned long, unsigned long, unsigned long) /usr/bin/clickhouse\r\n> 10. 0x696c2e1 ? /usr/bin/clickhouse\r\n> 11. 0x696a3c3 DB::IMergedBlockOutputStream::addStreams(std::string const&, std::string const&, DB::IDataType const&, std::shared_ptr<DB::ICompressionCodec> const&, unsigned long, bool) /usr/bin/clickhouse\r\n> 12. 0x650fc0c DB::MergedBlockOutputStream::MergedBlockOutputStream(DB::MergeTreeData&, std::string const&, DB::NamesAndTypesList const&, std::shared_ptr<DB::ICompressionCodec>, bool) /usr/bin/clickhouse\r\n> 13. 0x64dde7f DB::MergeTreeDataWriter::writeTempPart(DB::BlockWithPartition&) /usr/bin/clickhouse\r\n> 14. 0x651b7c4 DB::ReplicatedMergeTreeBlockOutputStream::write(DB::Block const&) /usr/bin/clickhouse\r\n> 15. 0x67a8726 DB::PushingToViewsBlockOutputStream::write(DB::Block const&) /usr/bin/clickhouse\r\n> 16. 0x67b3f01 DB::SquashingBlockOutputStream::finalize() /usr/bin/clickhouse\r\n> 17. 0x67b41d1 DB::SquashingBlockOutputStream::writeSuffix() /usr/bin/clickhouse\r\n> 18. 0x609d2a5 DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::atomic<bool>*) /usr/bin/clickhouse\r\n> 19. 0x62d73b1 DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::function<void (std::string const&)>, std::function<void (std::string const&)>) /usr/bin/clickhouse\r\n> 20. 0x359e471 DB::HTTPHandler::processQuery(Poco::Net::HTTPServerRequest&, HTMLForm&, Poco::Net::HTTPServerResponse&, DB::HTTPHandler::Output&) /usr/bin/clickhouse\r\n> 21. 0x35a14b1 DB::HTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&) /usr/bin/clickhouse\r\n> 22. 0x6dbcc59 Poco::Net::HTTPServerConnection::run() /usr/bin/clickhouse\r\n> 23. 0x6db98bf Poco::Net::TCPServerConnection::start() /usr/bin/clickhouse\r\n> 24. 0x6db9fb5 Poco::Net::TCPServerDispatcher::run() /usr/bin/clickhouse\r\n> 25. 0x723f481 Poco::PooledThread::run() /usr/bin/clickhouse\r\n> 26. 0x723b208 Poco::ThreadImpl::runnableEntry(void*) /usr/bin/clickhouse\r\n> 27. 0x791d69f ? /usr/bin/clickhouse\r\n> 28. 0x7f5f8e934dd5 start_thread /usr/lib64/libpthread-2.17.so\r\n> 29. 0x7f5f8e459ead __clone /usr/lib64/libc-2.17.so\r\n> "
    },
    "satisfaction_conditions": [
      "Memory limit configuration must be correctly identified and resolved"
    ],
    "created_at": "2020-01-02T21:19:41Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/8017",
    "source": {
      "issue_number": 8017
    },
    "initial_question": {
      "title": "What it depends on that ClickHouse will trigger DELETE/UPDATE syntaxs",
      "body": "When I used the \r\n\r\n> ALTER TABLE [db.]table DELETE WHERE filter_expr\r\n\r\nI found out that it took so many hours to delete all data.\r\nI know the delete/update process is running in background,so I would like to know the mechanism that what it depends on that ClickHouse will trigger the operation?"
    },
    "satisfaction_conditions": [
      "Background execution of DELETE operations must be confirmed",
      "Operation blocking conditions must be identifiable",
      "Data rewrite process must complete successfully",
      "Performance impact must be predictable",
      "Alternative operations for full data removal must be available"
    ],
    "created_at": "2019-12-03T23:03:28Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7917",
    "source": {
      "issue_number": 7917
    },
    "initial_question": {
      "title": "Confusion about compression",
      "body": "When I go deep into ClickHouse compression,I got some questions.Hope to get the reply\r\n(1)Q1\r\n`<compression incl=\"clickhouse_compression\">\r\n    <case>\r\n        <min_part_size>10000000000</min_part_size>\r\n        <min_part_size_ratio>0.01</min_part_size_ratio>\r\n        <method>zstd</method>\r\n    </case>\r\n</compression>\r\n`\r\n\r\n> ClickHouse checks min_part_size and min_part_size_ratio and processes the case blocks that match these conditions. If none of the <case> matches, ClickHouse applies the lz4 compression algorithm\r\n\r\nIs it right that when I do a INSERT query and the data size is greater than 100MB(10000000000 *  0.01),the zstd compression algorithm will be used?What should I do if I would like to change the default algorithm to zstd not lz4?\r\n(2)Q2\r\n`value Float32 CODEC(Delta, ZSTD)`\r\nThe pipeline codec really confuse me.The second parameter is the compression algorithm,in my opinion,the first parameter is used to show the column data type clearly and help to get a better compression ratio and speed?\r\n"
    },
    "satisfaction_conditions": [
      "Compression method selection correctly follows part size thresholds",
      "Default compression algorithm is configurable",
      "Multiple codecs can be chained in correct order",
      "Codec selection optimizes for data type characteristics",
      "Compression settings persist across operations"
    ],
    "created_at": "2019-11-25T23:55:52Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7872",
    "source": {
      "issue_number": 7872
    },
    "initial_question": {
      "title": "Access outer fields in subquery",
      "body": "Hi, how can I query something like this (like in MySQL) with 2 MergeTree tables\r\n`SELECT\r\n  TABLE_A.col1_from_a,\r\n  (SELECT col1_from_b FROM TABLE_B WHERE col2_from_b = TABLE_A.col2_from_a LIMIT 1) AS some_alias\r\nFROM TABLE_A\r\nWHERE <some filter from TABLE_A>`\r\n\r\nIt queries a field from TABLE_B using a field from TABLE_A.\r\nI tried using JOIN like this\r\n`SELECT\r\n    TABLE_A.col1_from_a,\r\n    TABLE_B.col1_from_b\r\nFROM TABLE_A\r\nANY LEFT JOIN TABLE_B ON TABLE_A.col2_from_a = TABLE_B.col2_from_b\r\nWHERE <some filter from TABLE_A>`\r\nbut it showed processing all the rows from TABLE_B (tens of millions of rows).\r\n`col2_from_a` is the sorting key of `TABLE_A`, `col2_from_b` is the sorting key of `TABLE_B`.\r\nAny suggestion would be appreciated."
    },
    "satisfaction_conditions": [
      "Query must retrieve data from TABLE_A while incorporating matching records from TABLE_B",
      "Query must return one matching record from TABLE_B for each TABLE_A record",
      "Query must maintain performance efficiency with large datasets",
      "Query must support filtering conditions on TABLE_A",
      "Query must execute successfully within ClickHouse's supported syntax"
    ],
    "created_at": "2019-11-21T08:15:48Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7865",
    "source": {
      "issue_number": 7865
    },
    "initial_question": {
      "title": "Escape double quote sign in CSV",
      "body": "I have CSV file with comma as separator sign, but without quotes for column entries.\r\n\r\nIssue I have is with double quote sign existing in string value for column,\r\nwhen I try to escape it with another double quote ( \"\" ) for command line clickhouse-client\r\nI got error and insert fails.\r\n\r\nWhen I escape one double quote with backslash ( \\\" ) then it works but got backslash and quote in entry in column.\r\n\r\nWhat is the proper way to escape double quote in CSV structure like I have ?\r\n\r\nClickhouse server version is 19.11.12\r\n\r\nhere is example for structures and data:\r\n```\r\n# testdata.csv\r\n749a2c8c-3682-4745-aefe-c21b3164bade,name,\"\"MY COMPANY\"\" COM\r\n749a2c8c-3682-4745-aefe-c21b3164bade,hash,67FF87AF9E9E4BA9E4C03FAC4A23F21C\r\n\r\n# table structure\r\nCREATE TABLE temp.events (`event_id` String, `property_name` String, `property_value` String) ENGINE = MergeTree() PARTITION BY tuple() ORDER BY event_id SETTINGS index_granularity = 8192\r\n\r\n#shell script to insert data\r\ncat testdata.csv | clickhouse-client --host=localhost --query='INSERT INTO temp.events (event_id, property_name, property_value) FORMAT CSV'\r\n\r\nCode: 117. DB::Exception: Expected end of line: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: event_id,       type: String, parsed text: \"749a2c8c-3682-4745-aefe-c21b3164bade\"\r\nColumn 1,   name: property_name,  type: String, parsed text: \"name\"\r\nColumn 2,   name: property_value, type: String, parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nERROR: There is no line feed. \"M\" found instead.\r\n It's like your file has more columns than expected.\r\nAnd if your file have right number of columns, maybe it have unquoted string value with comma.\r\n```\r\n"
    },
    "satisfaction_conditions": [
      "CSV data containing double quotes must be successfully imported into ClickHouse without data corruption",
      "The imported data must match the expected table schema (event_id, property_name, property_value)",
      "The CSV parsing must correctly handle strings containing double quotes without breaking column separation",
      "The solution must be compatible with ClickHouse's CSV import functionality",
      "The original meaning of the data must be preserved after import"
    ],
    "created_at": "2019-11-20T16:39:48Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7849",
    "source": {
      "issue_number": 7849
    },
    "initial_question": {
      "title": "Avoid `Too many partitions for single INSERT block` in Kafka Engine",
      "body": "Hi! I'm using Kafka engine and getting this error: ```DB::Exception: Too many partitions for single INSERT block (more than 100). The limit is controlled by 'max_partitions_per_insert_block' setting. ...```\r\nI understand why this error can occur in small inserts by hand (using clickhouse-client or http interface), but in case with Kafka engine documentation says: \r\n```\r\nTo improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn't formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block.\r\n```\r\nSo if I understand correctly Kafka engine should merge blocks before insert to increase performance and avoid this error. Also I can suspect the root of that error in my case is that in every kafka message is one `row` of data, so I suppose that blocks are merged but this does not reduce parts count. Is there a way to overcome this? Is it a bug? I'm hoping to avoid writing middleware pre-batching service, since Kafka engine does almost all needed things\r\n\r\nI'm using version 19.15.5.18. Thanks in advance!\r\n"
    },
    "satisfaction_conditions": [
      "Number of partitions per INSERT block must not exceed the max_partitions_per_insert_block limit (100)",
      "Table partitioning scheme must result in fewer distinct partitions than the limit"
    ],
    "created_at": "2019-11-19T17:58:02Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7794",
    "source": {
      "issue_number": 7794
    },
    "initial_question": {
      "title": "distributed_ddl_task_timeout",
      "body": "when I `CREATE MATERIALIZED VIEW v  ON CLUSTER xxx populate ` I have a problem.\r\nhow to solve it ?\r\n\r\n```\r\n/clickhouse/task_queue/ddl/query-0000000131 is executing longer than distributed_ddl_task_timeout (=180)\r\n```"
    },
    "satisfaction_conditions": [
      "The distributed DDL task timeout value must be increased to accommodate the materialized view creation",
      "The timeout configuration must persist for the duration of the materialized view creation",
      "The configuration change must be applied in a way that's accessible to the cluster's DDL operations"
    ],
    "created_at": "2019-11-15T16:03:49Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7711",
    "source": {
      "issue_number": 7711
    },
    "initial_question": {
      "title": "cannot parse CSV with '\\x7F' as delmiter. \\x7F is ascii code.",
      "body": "$clickhouse-client --host dbt20 --port 9000 --format_csv_delimiter='\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt \r\nCode: 19, e.displayText() = DB::Exception: A setting's value string has to be an exactly one character long, Stack trace:\r\n\r\n0. 0x563fac3bd7b0 StackTrace::StackTrace() /usr/bin/clickhouse\r\n1. 0x563fac3bd585 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse\r\n2. 0x563fac0ba051 ? /usr/bin/clickhouse\r\n3. 0x563fb03d2c5d boost::program_options::variables_map::notify() /usr/bin/clickhouse\r\n4. 0x563fac48413c DB::Client::init(int, char**) /usr/bin/clickhouse\r\n5. 0x563fac46feef mainEntryClickHouseClient(int, char**) /usr/bin/clickhouse\r\n6. 0x563fac2f9fed main /usr/bin/clickhouse\r\n7. 0x7f1c99a2e3d5 __libc_start_main /usr/lib64/libc-2.17.so\r\n8. 0x563fac3632ea _start /usr/bin/clickhouse\r\n (version 19.16.2.2 (official build))\r\n"
    },
    "satisfaction_conditions": [
      "The delimiter character must be properly interpreted as a single ASCII character",
      "The shell command syntax must correctly pass the delimiter to clickhouse-client"
    ],
    "created_at": "2019-11-11T09:27:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7647",
    "source": {
      "issue_number": 7647
    },
    "initial_question": {
      "title": "Change bitmapBuild result type from default UInt8",
      "body": "Hi there! Can i somehow change result type of bitmapBuild([1,2,3,4]) not to ```AggregateFunction(groupBitmap, UInt8)``` which i assume selects type by selecting max integer in set, but to ```AggregateFunction(groupBitmap, UInt32)``` without using hacks like bitmapBuild([1,2,3,4, 4294967295])"
    },
    "satisfaction_conditions": [
      "The bitmapBuild function must return AggregateFunction with UInt32 type instead of UInt8",
      "The solution must work with the original input array values without requiring additional numbers",
      "The type conversion must be explicit rather than implicit"
    ],
    "created_at": "2019-11-06T10:03:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7642",
    "source": {
      "issue_number": 7642
    },
    "initial_question": {
      "title": "When Join Clause in Select query, The query use index of join column?",
      "body": "orders - pk: order_id\r\norder_product - pk: (order_id, product_no)\r\n\r\n```sql\r\nselect o.o_zipcode, op.opt_id \r\nfrom orders o \r\n    inner join order_product op \r\n        on o.order_id = op.order_id;\r\n```\r\n\r\nTwo tables has `order_id` as primary key. When I join two tables in select query, Does the query use index?"
    },
    "satisfaction_conditions": [
      "Query execution behavior regarding index usage must be accurately explained",
      "Join operation's internal mechanism must be described",
      "Memory handling during join operation must be explained"
    ],
    "created_at": "2019-11-06T01:48:38Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7503",
    "source": {
      "issue_number": 7503
    },
    "initial_question": {
      "title": "How to shutdown clickhouse instance ?",
      "body": "First time to use clickhouse, how can i close or shutdown clickhouse ? Can only use \"kill -9 pid\" ?\r\nThx.  "
    },
    "satisfaction_conditions": [
      "ClickHouse server process terminates gracefully",
      "System-appropriate shutdown method is used",
      "No data loss or corruption occurs during shutdown"
    ],
    "created_at": "2019-10-28T08:26:21Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7502",
    "source": {
      "issue_number": 7502
    },
    "initial_question": {
      "title": "Threads control",
      "body": "When I use different index, The Clickhouse use different number of parallel threads(streams).\r\n\r\nI can refer to the max_threads settings in clickhouse documents.\r\n\r\nbut, It is only for max_threads.\r\n\r\nIs there any other ways to control thread number by different index?\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Thread count control is configurable",
      "Thread count respects system constraints",
      "Configuration changes persist appropriately"
    ],
    "created_at": "2019-10-28T07:41:44Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7499",
    "source": {
      "issue_number": 7499
    },
    "initial_question": {
      "title": "How does the Clickhouse find other column data?",
      "body": "When I run a query with where statement like `select * from tbl1 where col1 = 'CH' `, Clickhouse find other columns which correspond with `col1 = 'CH'`.\r\n\r\nUnlike row-base DBMS, ClickHouse is column-base DBMS that saves data by column. Then How CH find other column data with same row?\r\n\r\nCould you explain a process step by step that the ClickHouse finds other columns data?"
    },
    "satisfaction_conditions": [
      "Must explain how row relationships are maintained across columns in a column-based storage system",
      "Must demonstrate how filtering on one column affects data retrieval from other columns",
      "Must preserve row position/ordering information",
      "Must handle boolean filtering operations",
      "Must describe a complete data retrieval process"
    ],
    "created_at": "2019-10-28T05:31:51Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7489",
    "source": {
      "issue_number": 7489
    },
    "initial_question": {
      "title": "Strange Null literal handling behavior?",
      "body": "I am observing this counterintuitive behavior with special `Null` value:\r\n```\r\nSELECT isNull(CAST('Null', 'Nullable(String)'))\r\n```\r\nreturns `0`\r\n\r\nWhereas:\r\n```\r\nSELECT isNull(CAST(Null, 'Nullable(String)'))\r\n```\r\nreturns `1`\r\n\r\nand\r\n```\r\nSELECT isNull(CAST('Null', 'Nullable(Int32)'))\r\n```\r\nreturns `1`\r\n\r\nN.B.: single quotes and their absence around `Null`.\r\n\r\nIs this a bug or a feature?"
    },
    "satisfaction_conditions": [
      "String 'Null' must be treated as a non-null string value when cast to Nullable(String)",
      "Unquoted Null keyword must be treated as a NULL value when cast to Nullable types",
      "Invalid string-to-number conversions must result in NULL values",
      "Type casting behavior must be consistent with SQL NULL semantics"
    ],
    "created_at": "2019-10-25T13:00:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7443",
    "source": {
      "issue_number": 7443
    },
    "initial_question": {
      "title": "Is there any way to predaggregate uniqState?",
      "body": "There is no function RunningDifference for uniqState.\r\n\r\nBut two uniqStates can be summed by +\r\n\r\nSo RunningDifference can be emulated like this\r\n```\r\ncreate table z(d Date, z String, u String)\r\nEngine=MergeTree partition by tuple() order by tuple();\r\n\r\nCREATE MATERIALIZED VIEW mvz\r\nENGINE = AggregatingMergeTree order by (z,d) settings index_granularity = 8 \r\nas select d, z,uniqState(u) as us from z group by z,d;\r\n\r\ninsert into z select today()+1, 'g1' , toString(number) from numbers(1000);\r\ninsert into z select today()+2, 'g1' , toString(number+100) from numbers(1000);\r\ninsert into z select today()+3, 'g1' , toString(number+200) from numbers(1000);\r\ninsert into z select today()+4, 'g1' , toString(number+200) from numbers(1000);\r\ninsert into z select today()+5, 'g1' , toString(number+300) from numbers(1000);\r\n\r\nselect m1, m2 from (\r\nSELECT\r\n        groupArray(d) AS gd,\r\n        arrayMap(x -> toString(gd[x+1])||' - '||toString(gd[x+2]), range(toUInt64(length(gd)-1))) m1,\r\n        groupArray(us) AS gus,\r\n        arrayMap(x -> (arrayReduce('uniqMerge', [gus[x+1]+gus[x+2]]) - arrayReduce('uniqMerge', [gus[x+2]])) , range(toUInt64(length(gd)-1))) m2\r\n          from (select d, us FROM mvz  order by d ) )\r\n    Array Join m1, m2\r\n\r\n\u250c\u2500m1\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500m2\u2500\u2510\r\n\u2502 2019-10-24 - 2019-10-25 \u2502 100 \u2502\r\n\u2502 2019-10-25 - 2019-10-26 \u2502 100 \u2502\r\n\u2502 2019-10-26 - 2019-10-27 \u2502   0 \u2502\r\n\u2502 2019-10-27 - 2019-10-28 \u2502 100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThere is only problem that it's working only if state fully aggregated by date.\r\nAnd there is no function partialUniqMerge to merge by date but leave states.\r\n`select d, partialUniqMerge(us) FROM mvz group by d order by d`\r\n\r\nBut probably such function exists internally because Distributed gets such data from shards."
    },
    "satisfaction_conditions": [
      "Must support aggregation of uniqState values across multiple dates",
      "Must correctly calculate the difference between consecutive aggregated states",
      "Must work with materialized views and the AggregatingMergeTree engine"
    ],
    "created_at": "2019-10-23T02:12:23Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7421",
    "source": {
      "issue_number": 7421
    },
    "initial_question": {
      "title": "I wonder how storage policy do on disk failure.",
      "body": "My ClickHouse configurations are like below.\r\n\r\n* Node1\r\n  - two disks \r\n    * /data1\r\n    * /data2\r\n\r\n* Node2\r\n  - two disks\r\n    * /data1\r\n    * /data2\r\n\r\nData is replicated between Node1 and Node2 by ReplicatedMergeTree. \r\nAnd each ReplicatedMergeTree uses the below storage_policy.\r\nSo ReplicatedMergeTree splits data into two disks; /data1 and /data2.\r\n\r\n**Settings**\r\n```XML\r\n<storage_configuration>                                                          \r\n    <disks>                                                                      \r\n        <jbod1>                                                                  \r\n            <path>./jbod1/</path>                                                \r\n            <keep_free_space_bytes>1024</keep_free_space_bytes>                  \r\n        </jbod1>                                                                 \r\n        <jbod2>                                                                  \r\n            <path>./jbod2/</path>                                                \r\n            <keep_free_space_bytes>1024</keep_free_space_bytes>                  \r\n        </jbod2>                                                                 \r\n    </disks>                                                                     \r\n                                                                                 \r\n    <policies>                                                                   \r\n        <jbods>                                                                  \r\n            <volumes>                                                            \r\n                <main>                                                           \r\n                    <disk>jbod1</disk>                                           \r\n                    <disk>jbod2</disk>                                           \r\n                </main>                                                          \r\n            </volumes>                                                           \r\n        </jbods>                                                                 \r\n    </policies>                                                                  \r\n</storage_configuration>                                                         \r\n```\r\n\r\nWhat happen If `/data1` on Node1 is broken?\r\nDoes Node1 lose the data on `/data1`?\r\nOr Does Node1 copy data from Node2?\r\n\r\nThank you in advance."
    },
    "satisfaction_conditions": [
      "Data recovery is possible after disk failure",
      "Data consistency is maintained between replicas",
      "System remains operational after implementing recovery method",
      "Missing data parts are retrievable from replica nodes",
      "Storage configuration remains adjustable after disk failure"
    ],
    "created_at": "2019-10-22T08:19:00Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7391",
    "source": {
      "issue_number": 7391
    },
    "initial_question": {
      "title": "DDL background thread is not initialized",
      "body": "hey , guys.\r\n\r\nTO support clickhouse , I bought 11 machines each with 64 cores/256GB RAM/2TB SSD. they cost me about  230K us dollar.\r\n\r\nRecently I am doing some testing. But `DDL background thread is not initialized` made me confused.\r\n\r\n```\r\nclickhouse version:  ClickHouse server version 19.15.3 revision 54426\r\nos: centos 7.4\r\nshards: 11 (yes lack of money, so 1 replica)\r\ninternal_replication = false\r\n```\r\n\r\nI don't know why:\r\n```\r\nCREATE TABLE IF NOT EXISTS test.user_log ON CLUSTER cluster_1st\r\n(\r\n    `dt` Date,\r\n    `app_id` Int8,\r\n    `app_version_name` String,\r\n    `event_id` String\r\n)\r\nENGINE = MergeTree('/clickhouse/tables/{layer}-{shard}/user_log', '{replica}')\r\nPARTITION BY dt\r\nORDER BY (app_id, app_version_name)\r\nSETTINGS index_granularity = 8192\r\n\r\nReceived exception from server (version 19.15.3):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: DDL background thread is not initialized..\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n```\r\n\r\nSOS~~~\r\n"
    },
    "satisfaction_conditions": [
      "Cluster configuration must have internal_replication set to false"
    ],
    "created_at": "2019-10-19T14:28:01Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7312",
    "source": {
      "issue_number": 7312
    },
    "initial_question": {
      "title": "What is \"active\" znode means in Zookeeper?",
      "body": "When I check my zk, I found a 'actived' znode in the clickhouse path.I check the document but nothing found out.\r\nso what's this znode means?\r\nbyw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk? "
    },
    "satisfaction_conditions": [
      "The meaning of 'active' znode status must be explained in relation to replica functionality",
      "Clarification must be provided about znode lifecycle management",
      "The explanation must address concerns about znode accumulation during insert operations",
      "The response must indicate whether manual maintenance tasks are required"
    ],
    "created_at": "2019-10-14T12:13:18Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7151",
    "source": {
      "issue_number": 7151
    },
    "initial_question": {
      "title": "Log file not recreated if deleted",
      "body": "Hi,\r\n\r\nMy ClickHouse is integrated with a system that regularly take the log files, backup them, and delete them locally. It is a common system to lots of services.\r\n\r\nNevertheless, I got an issue with clickhouse. If you delete one ClickHouse log file when ClickHouse is up and running, the log file will not be recreated, and all new clickhouse logs will be lost.\r\n\r\nForcing the reload of the config file or flushing the log  (SYSTEM RELOAD CONFIG or SYSTEM FLUSH LOGS) does not seems to fix the issue.\r\n\r\nSo is there a way to force clickhouse to recreate the log file ?"
    },
    "satisfaction_conditions": [
      "Log file is recreated after deletion during runtime",
      "Solution works while ClickHouse server remains running"
    ],
    "created_at": "2019-09-30T09:42:09Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/7147",
    "source": {
      "issue_number": 7147
    },
    "initial_question": {
      "title": "I think sth is wrong with arrayDifference",
      "body": "THIS IS FINE \r\n\r\n```\r\nSELECT arrayDifference([1, 2, 3, 4])\r\n\r\n\u250c\u2500arrayDifference([1, 2, 3, 4])\u2500\u2510\r\n\u2502 [0,1,1,1]                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nBUT what `-1` mean ?\r\n```\r\nSELECT arrayDifference([1, 2, 2, 3, 3, 2])\r\n\r\n\u250c\u2500arrayDifference([1, 2, 2, 3, 3, 2])\u2500\u2510\r\n\u2502 [0,1,0,1,0,-1]                      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd this is not reasonable:\r\n```\r\nSELECT arrayDifference(['a', 'b'])\r\n\r\nReceived exception from server (version 19.5.3):\r\nCode: 43. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: arrayDifference cannot process values of type String.\r\n```\r\narray(T) takes T type, but a function can not support String type......"
    },
    "satisfaction_conditions": [
      "Function must correctly calculate differences between consecutive array elements for numeric arrays",
      "Function must handle negative difference values",
      "Function must reject non-numeric array inputs with appropriate error message",
      "Alternative solution must be able to process consecutive duplicate values in arrays",
      "Solution must work with arrays of any supported data type"
    ],
    "created_at": "2019-09-30T07:58:53Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6973",
    "source": {
      "issue_number": 6973
    },
    "initial_question": {
      "title": "Data written because of max_bytes_before_external_*",
      "body": "Hello.\r\n\r\nIs it possible to see how much data was written to disk during query because of max_bytes_before_external_* ?"
    },
    "satisfaction_conditions": [
      "Query data volume information must be accessible",
      "Data must be specific to max_bytes_before_external operations",
      "Information must be query-specific",
      "Measurement must capture actual bytes written"
    ],
    "created_at": "2019-09-18T11:33:22Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6671",
    "source": {
      "issue_number": 6671
    },
    "initial_question": {
      "title": "Collapsing Merge Tree Using FINAL in the SQL",
      "body": "Hello,\r\n\r\nI decided to use CollapsingMergeTree for a test table. This table basically keeps the football players name and their teams. For example ;\r\n\r\n```\r\nCREATE TABLE players\r\n    (\r\n     `team_id` UInt32,\r\n     `player_name` String,\r\n      `pid` UInt32,\r\n      sign Int8 DEFAULT 1\r\n    ) ENGINE = CollapsingMergeTree(sign) ORDER BY (player_name, pid) SETTINGS index_granularity = 8192\r\n```\r\nSo I understood the usage of sign for the querying with 'having' but what I want to ask is when I want to get the player count by grouping by team_id, having sum(sign) > 0 ' won't work properly since it will aggregate all the player's sign values. \r\n\r\nSo what I would like to ask is if I use FINAL in my SQL all the time, will it decrease performance dramatically? (There could be around 50M-100M rows at maximum!)\r\n\r\nThanks in the advance!"
    },
    "satisfaction_conditions": [
      "Query performance impact must be understood and acceptable for the data volume"
    ],
    "created_at": "2019-08-26T17:21:09Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6484",
    "source": {
      "issue_number": 6484
    },
    "initial_question": {
      "title": " DB::Exception: Bad get: has UInt64, requested String.",
      "body": "When I set up the distributed tables, there was such a mistake.\r\n`node01 :) create table dummy (p Date, k UInt64, d String) ENGINE = MergeTree(p, k, 8192)\r\n\r\nCREATE TABLE dummy\r\n(\r\n    `p` Date, \r\n    `k` UInt64, \r\n    `d` String\r\n)\r\nENGINE = MergeTree(p, k, 8192)\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.020 sec. \r\nnode01 :) create table distributed (p Date, k UInt64, d String) ENGINE = Distributed(cluster-1, 'default', 'dummy')\r\n\r\nCREATE TABLE distributed\r\n(\r\n    `p` Date, \r\n    `k` UInt64, \r\n    `d` String\r\n)\r\nENGINE = Distributed(cluster - 1, 'default', 'dummy')\r\n\r\nReceived exception from server (version 19.9.5):\r\nCode: 170. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Bad get: has UInt64, requested String. \r\n\r\n0 rows in set. Elapsed: 0.001 sec. \r\n`\r\n I don't know why. I tried many times.\r\n"
    },
    "satisfaction_conditions": [
      "Table schema in distributed table must match the source table"
    ],
    "created_at": "2019-08-14T05:44:51Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6353",
    "source": {
      "issue_number": 6353
    },
    "initial_question": {
      "title": "Using runningDifference() and quantilesExactWeighted() correctly on time series data",
      "body": "We need to calculate exact value quantiles on large non-uniformly sampled time series data. The schema is (String sensor_path, DateTime timestamp, Float64 value). \r\n\r\nIt would be perfect if we could use something like \u201eSELECT quantilesExactWeighted(...)(value, delta)...\u201c,\r\nwhere \u201edelta\u201c is the result of \u201erunningDifference(timestamp)\u201c. \r\nUnfortunately, this does not work, because for each row, delta is the time difference between the previous and the current row instead of the time difference between the current and the next row, as needed for the weights parameter of quantilesExactWeighted(). \r\n\r\nSelf-joining the time series to shift delta forward one row will probably not work when the time series does not fit into memory, right?\r\n\r\nIs there maybe another, more efficient solution?\r\n\r\nI\u2019d be glad to try to submit a patch with a new variant of runningDifference() if there is no other solution. \r\n\r\nMany thanks for your excellent work on ClickHouse!\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Time differences between consecutive timestamps must be calculated correctly",
      "Must handle large datasets that exceed available memory",
      "Must maintain correct timestamp ordering for calculations",
      "Must support weighted quantile calculations using time differences",
      "Must work with the given schema (sensor_path, timestamp, value)"
    ],
    "created_at": "2019-08-05T20:47:57Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6064",
    "source": {
      "issue_number": 6064
    },
    "initial_question": {
      "title": "Clickhouse count is not working",
      "body": "Hi,\r\n\r\nI have some issues when trying to count the number of rows of a table using a simple query like:\r\n`SELECT count()\r\nFROM mop3 \r\nWHERE (key = category) AND (value = lips)`\r\n\r\nThe table is\r\n`CREATE TABLE mop3\r\n(\r\n    customer_id Int32,\r\n    order_id Int64,\r\n    order_date_created DateTime,\r\n    key String,\r\n    value String,\r\n    quantity Int32,\r\n    unit_amount Decimal32(4),\r\n    total_amount Decimal32(4)\r\n) ENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(order_date_created)\r\nORDER BY (key, value)\r\n`\r\n\r\nThank you very much\r\n"
    },
    "satisfaction_conditions": [
      "Query syntax correctly handles string literals in WHERE clause",
      "Query executes without column reference errors",
      "Query is compatible with the table's MergeTree engine and schema"
    ],
    "created_at": "2019-07-19T07:31:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6050",
    "source": {
      "issue_number": 6050
    },
    "initial_question": {
      "title": "19.11-*-stable dictionaries loading failed",
      "body": "After upgrading to 19.11 dictionaries loading fail with error:\r\n\r\n```\r\n2019.07.18 13:46:47.501892 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'interserver_http_host', expected 'dictionary'\r\n2019.07.18 13:46:47.502034 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host', expected 'dictionary'\r\n2019.07.18 13:46:47.502177 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host[1]', expected 'dictionary'\r\n2019.07.18 13:46:47.502256 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host[2]', expected 'dictionary'\r\n2019.07.18 13:46:47.502324 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'logger', expected 'dictionary'\r\n2019.07.18 13:46:47.502388 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'profiles', expected 'dictionary'\r\n2019.07.18 13:46:47.502472 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'max_concurrent_queries', expected 'dictionary'\r\n2019.07.18 13:46:47.502539 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'zookeeper-servers', expected 'dictionary'\r\n\r\n```\r\n\r\n cat /etc/clickhouse-server/dnl_dictionary.xml\r\n\r\n```\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n  <dictionary>\r\n    <name>hosts</name>\r\n    <source>\r\n      <odbc>\r\n        <connection_string>DSN=PostgreSQLCHglobal</connection_string>\r\n        <table>hosts</table>\r\n      </odbc>\r\n......\r\n......\r\n......\r\n</yandex>\r\n\r\n```\r\nWorks on 19.9.4.34 and older."
    },
    "satisfaction_conditions": [
      "Dictionaries must be successfully loaded and accessible",
      "Dictionary state must change from NOT_LOADED to LOADED when required",
      "System must handle configuration file preprocessing correctly"
    ],
    "created_at": "2019-07-18T12:47:34Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6030",
    "source": {
      "issue_number": 6030
    },
    "initial_question": {
      "title": "order by with formatReadableSize orders alphabetically instead of numerically",
      "body": "This is probably due to the `formatReadableSize` being applied before the results get displayed, but it would be super nice if order by still ordered by the original values."
    },
    "satisfaction_conditions": [
      "Results must be ordered by the original numeric size values, not their formatted string representations",
      "The output must still display human-readable size formats",
      "The query must follow SQL ordering principles",
      "The formatted output must maintain data type consistency"
    ],
    "created_at": "2019-07-17T01:37:49Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/6013",
    "source": {
      "issue_number": 6013
    },
    "initial_question": {
      "title": "Table info  isn't show on system.parts ",
      "body": "I created table (for example XX), queried successfully. \r\nHow ever I can't see any info of table XX when I query info from system.parts. \r\nWhen I run `Select table from system.parts` that didn't display XX. It displayed 4 names: cpu, cpu, cpu, tags.\r\n\r\nCould I find the information of a table I created in database (such name, size,..)?"
    },
    "satisfaction_conditions": [
      "User must be able to access table information based on the table's storage engine type",
      "Empty tables must be handled appropriately in information queries",
      "Clear explanation of system.parts scope must be provided"
    ],
    "created_at": "2019-07-15T15:57:24Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5853",
    "source": {
      "issue_number": 5853
    },
    "initial_question": {
      "title": "How to pretty print a generated RowBinary file?",
      "body": "Hello all,\r\n\r\nGiven a valid ClickHouse RowBinary file, how I can prettyprint the row data within the file?\r\n\r\nThank you"
    },
    "satisfaction_conditions": [
      "Input file must be successfully read in RowBinary format",
      "Data structure must be correctly specified",
      "Output must be human-readable",
      "All rows from input file must be represented in output"
    ],
    "created_at": "2019-07-03T14:20:07Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5677",
    "source": {
      "issue_number": 5677
    },
    "initial_question": {
      "title": "How much RAM is required for ZooKeeper?",
      "body": "Is there a guide for memory usage of ZooKeeper for ClickHouse?\r\n\r\nMy system uses the ReplicatedMergeTree, creates 3000 parts for one day and stores it about one year.\r\n\r\nI set 8GB memory for ZooKeeper. I don't know whether is it enough or not.\r\n\r\nI hope your help."
    },
    "satisfaction_conditions": [
      "ZooKeeper memory usage must be accurately measurable",
      "Memory allocation must accommodate both data size and operational overhead",
      "Memory usage optimization settings must be verifiable"
    ],
    "created_at": "2019-06-19T01:48:52Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5592",
    "source": {
      "issue_number": 5592
    },
    "initial_question": {
      "title": "ALTER column datatype timeouts. What's the best way?",
      "body": "When trying to change column's datatype\r\n```\r\n:) alter table sflow modify column column_1 LowCardinality(UInt8);\r\n\r\nALTER TABLE t1\r\n    MODIFY COLUMN\r\n    `column_1` LowCardinality(UInt8)\r\n```\r\nGetting response\r\n```\r\nTimeout exceeded while receiving data from server. Waited for 300 seconds, timeout is 300 seconds.\r\nCancelling query.\r\n```\r\n\r\nIs the only way to increase timeout / recreate the table?"
    },
    "satisfaction_conditions": [
      "Database operation continues despite client timeout"
    ],
    "created_at": "2019-06-12T09:11:26Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5512",
    "source": {
      "issue_number": 5512
    },
    "initial_question": {
      "title": "Is there accumulate for array?",
      "body": "data : [1, 4, 6, 8]\r\n\r\nresult : 1 * 4 * 6 * 8 = 192"
    },
    "satisfaction_conditions": [
      "Query returns the product of all array elements",
      "Query handles arbitrary numeric arrays",
      "Result is returned as a single numeric value"
    ],
    "created_at": "2019-06-02T09:10:53Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5351",
    "source": {
      "issue_number": 5351
    },
    "initial_question": {
      "title": "Clickhouse Add date column to new table with specific date",
      "body": "table 1----- without date column\r\ntable 2----- want to insert date column with a specific date\r\n\r\nQuery--- insert into table 2\r\n               select a,b,c,d,NOW() from table 1 \r\n\r\nNOW() will insert current date but I want to insert a specific date  into table 2\r\n\r\nEX: 2019-12-09\r\n\r\nPlease help me with the query as early as possible\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Date format in target table matches Clickhouse date data type requirements",
      "Query syntax is valid for Clickhouse SQL dialect"
    ],
    "created_at": "2019-05-20T18:07:04Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5205",
    "source": {
      "issue_number": 5205
    },
    "initial_question": {
      "title": "Error when try get data from Dictionary by ip as Nullabel(String)",
      "body": "I am using Maxmind GeoIp database as Dictionary\r\n\r\n```\r\n<dictionary>\r\n        <name>geoip_city_blocks_ipv4</name>\r\n        <source>\r\n            <file>\r\n                <path>/etc/clickhouse-server/geoip/City/GeoLite2-City-Blocks-IPv4.csv</path>\r\n                <format>CSVWithNames</format>\r\n            </file>\r\n        </source>\r\n        <lifetime>300</lifetime>\r\n        <layout>\r\n            <ip_trie/>\r\n        </layout>\r\n        <structure>\r\n            <key>\r\n                <attribute>\r\n                    <name>prefix</name>\r\n                    <type>String</type>\r\n                </attribute>\r\n            </key>\r\n            <attribute>\r\n                <name>geoname_id</name>\r\n                <type>UInt32</type>\r\n                <null_value>0</null_value>\r\n            </attribute>\r\n...\r\n```\r\n\r\nDictionary have data like:\r\n\r\n```\r\n\u2500prefix\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500geoname_id\u2500\u2510\r\n\u2502 100::/24      \u2502    2070830 \u2502\r\n\u2502 100:100::/24  \u2502    1811017 \u2502\r\n\u2502 100:200::/23  \u2502    1811017 \u2502\r\n\u2502 100:400::/22  \u2502    2077456 \u2502\r\n\u2502 100:800::/21  \u2502    1809935 \u2502\r\n\u2502 100:1000::/20 \u2502    1861060 \u2502\r\n\u2502 100:2000::/19 \u2502    1809935 \u2502\r\n\u2502 100:4000::/23 \u2502    1862415 \u2502\r\n\u2502 100:4200::/23 \u2502    1850147 \u2502\r\n\u2502 100:4400::/22 \u2502    1863018 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd I have table with ips\r\n\r\n```\r\nCREATE TABLE mautic.test (\r\n  `id` Int32,\r\n  `ip` Nullable(String)\r\n) ENGINE = MergeTree() ORDER BY (id);\r\n\r\nINSERT INTO mautic.test VALUES\r\n    (1, '174.105.199.64'),\r\n    (2, '40.107.219.92'),\r\n    (3, '40.107.219.59'),\r\n    (4, '65.246.27.210'),\r\n    (5, '50.98.35.219'),\r\n    (6, '70.67.156.137');\r\n```\r\n\r\nI am trying get data from dictionary\r\n\r\n```\r\nSELECT\r\n    ip,\r\n    dictGet('geoip_city_blocks_ipv4', 'geoname_id', tuple(IPv4StringToNum(ip))) AS geoname_id\r\nFROM\r\n    test_ip\r\n```\r\n\r\nBut getting error \r\n\r\n```\r\nCode: 53, e.displayText() = DB::Exception: Key does not match, expected either UInt32 or FixedString(16)\r\n```\r\n\r\nBut if change `ip` field type to just `String`, the same request work without any errors.\r\n\r\nI don't know is it bug or I make something wrong."
    },
    "satisfaction_conditions": [
      "Dictionary lookup must successfully process nullable IP address fields",
      "Solution must preserve the nullable nature of the IP field in the table structure"
    ],
    "created_at": "2019-05-06T14:08:43Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/5105",
    "source": {
      "issue_number": 5105
    },
    "initial_question": {
      "title": "How to reduce memory usage with arrayEnumerate?",
      "body": "I have to use arrayEnumerate because of nested ( ['a','a'] k, [1,2] v ) k='a' -> [1,2]\r\nAnd I noticed that arrayEnumerate eats memory for long arrays (for example 1000 elements)\r\n\r\ncreate table X(A Array(String)) engine = Memory;\r\ninsert into X select arrayMap(x->toString (x) , range(1000)) from numbers(10);\r\n\r\nselect arrayFilter(x->x='777', A) from X format Null;\r\nPeak memory usage (for query): 1.21 MiB\r\n\r\nselect arrayFilter(x->A[x]='777', arrayEnumerate(A)) from X format Null;\r\nPeak memory usage (for query): **257.33 MiB**\r\n\r\n\r\ninsert into X select arrayMap(x->toString (x) , range(1000)) from numbers(1000);\r\n\r\nselect arrayFilter(x->x='777', A) from X format Null;\r\nPeak memory usage (for query): 14.01 MiB.\r\n\r\nselect arrayFilter(x->A[x]='777', arrayEnumerate(A)) from X format Null;\r\nPeak memory usage (for query): **16.02 GiB**\r\n\r\nselect arrayFilter(x->A[x]='777', range(length(A))) from X format Null;\r\nPeak memory usage (for query): **16.03 GiB.**\r\n\r\nHow I can reduce memory usage in this case?"
    },
    "satisfaction_conditions": [
      "Memory usage remains manageable with large arrays",
      "Works with array enumeration use cases"
    ],
    "created_at": "2019-04-24T22:00:30Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4893",
    "source": {
      "issue_number": 4893
    },
    "initial_question": {
      "title": "ngrambf_v1 index not accessible",
      "body": "Hello \r\n\r\n\r\nI'm trying to create or alter a table to index of type ngrambf_v1.  But the server refuse the creation of the index  with the following error message : Index type 'ngrambf_v1'. Available index types: set, minmax\r\n\r\n**How to reproduce**\r\njust create of alter a table to add an index of type : ngrambf or tokenbf\r\n\r\n* Which ClickHouse server version to use\r\n\r\n19.4.3.11\r\n\r\n* Which interface to use, if matters\r\nHTTP Interface thru TABIX\r\n\r\n* Non-default settings, if any\r\nallow_experimental_data_skipping_indices=1\r\n\r\n**Expected behavior**\r\nIndex of the specified type created.\r\n\r\n**Error message and/or stacktrace**\r\n2019.04.03 12:26:29.478866 [ 28 ] {5371c57b-1508-4038-ad3d-c95cbd8983c0} <Error> executeQuery: Code: 80, e.displayText() = DB::Exception: Unknown Index type 'ngrambf_v1'. Available index types: set, minmax (from [::ffff:10.1.30.136]:62140) (in query:  /*TABIX_QUERY_ID_lG85IYRl*/    alter table IPANEMA.FLOW add INDEX src_site_idx (src_site,lower(src_site)) TYPE ngrambf_v1(3, 256, 4, 0) GRANULARITY 4), Stack trace:\r\n\r\n0. /usr/bin/clickhouse-server(StackTrace::StackTrace()+0x16) [0x6f73536]\r\n1. /usr/bin/clickhouse-server(DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)+0x22) [0x3712ab2]\r\n2. /usr/bin/clickhouse-server(DB::MergeTreeIndexFactory::get(DB::NamesAndTypesList const&, std::shared_ptr<DB::ASTIndexDeclaration>, DB::Context const&) const+0x485) [0x6735bd5]\r\n3. /usr/bin/clickhouse-server(DB::MergeTreeData::setPrimaryKeyIndicesAndColumns(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::IAST>, DB::ColumnsDescription const&, DB::IndicesDescription const&, bool)+0xb0b) [0x66ebb5b]\r\n4. /usr/bin/clickhouse-server(DB::MergeTreeData::checkAlter(DB::AlterCommands const&, DB::Context const&)+0x845) [0x66efc95]\r\n5. /usr/bin/clickhouse-server(DB::StorageMergeTree::alter(DB::AlterCommands const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, DB::Context const&)+0xc4) [0x664ab54]\r\n6. /usr/bin/clickhouse-server(DB::InterpreterAlterQuery::execute()+0x647) [0x6a0c8e7]\r\n7. /usr/bin/clickhouse-server() [0x655cb1f]\r\n8. /usr/bin/clickhouse-server(DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, DB::Context&, std::function<void (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>, std::function<void (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>)+0xfa) [0x655e8aa]\r\n9. /usr/bin/clickhouse-server(DB::HTTPHandler::processQuery(Poco::Net::HTTPServerRequest&, HTMLForm&, Poco::Net::HTTPServerResponse&, DB::HTTPHandler::Output&)+0x35f6) [0x372e1b6]\r\n10. /usr/bin/clickhouse-server(DB::HTTPHandler::handleRequest(Poco::Net::HTTPServerRequest&, Poco::Net::HTTPServerResponse&)+0x3f1) [0x3730181]\r\n11. /usr/bin/clickhouse-server(Poco::Net::HTTPServerConnection::run()+0x32c) [0x707ceec]\r\n12. /usr/bin/clickhouse-server(Poco::Net::TCPServerConnection::start()+0xf) [0x7076eef]\r\n13. /usr/bin/clickhouse-server(Poco::Net::TCPServerDispatcher::run()+0xe9) [0x7077629]\r\n14. /usr/bin/clickhouse-server(Poco::PooledThread::run()+0x81) [0x712a761]\r\n15. /usr/bin/clickhouse-server(Poco::ThreadImpl::runnableEntry(void*)+0x38) [0x7126928]\r\n16. /usr/bin/clickhouse-server() [0xadef39f]\r\n17. /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7ff1ad3a16ba]\r\n18. /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7ff1ac9c241d]\r\n\r\n\r\nThanks in advance \r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Experimental data skipping indices setting must be enabled"
    ],
    "created_at": "2019-04-03T12:32:25Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4762",
    "source": {
      "issue_number": 4762
    },
    "initial_question": {
      "title": "ch complains \"Password required for user default., e.what() = DB::Exception.\" when drop a partition .But actually it has the user. And when try many times it will succeed",
      "body": "**Version of ch:**\r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) select version()\r\n\r\nSELECT version()\r\n\r\n\u250c\u2500version()\u2500\u2510\r\n\u2502 18.14.19  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n**Cluster Config:**\r\n\r\n> SELECT * FROM system.clusters \r\n\r\n\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500shard_num\u2500\u252c\u2500shard_weight\u2500\u252c\u2500replica_num\u2500\u252c\u2500host_name\u2500\u2500\u2500\u2500\u2500\u252c\u2500host_address\u2500\u2500\u252c\u2500port\u2500\u252c\u2500is_local\u2500\u252c\u2500user\u2500\u2500\u2500\u2500\u252c\u2500default_database\u2500\u2510\r\n\u2502 ads_model_ck_cluster \u2502         1 \u2502            1 \u2502           1 \u2502 xx.xx.30.65  \u2502 xx.xx.30.65  \u2502 9000 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         1 \u2502            1 \u2502           2 \u2502 xx.xx.40.123 \u2502 xx.xx.40.123 \u2502 9000 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         2 \u2502            1 \u2502           1 \u2502 xx.xx.30.64  \u2502 xx.xx.30.64  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         2 \u2502            1 \u2502           2 \u2502 xx.xx.30.69  \u2502 xx.xx.30.69  \u2502 9000 \u2502              0 \u2502 default \u2502                  \u2502\r\n....\r\n\u2502 ads_model_ck_cluster \u2502        15 \u2502            1 \u2502           1 \u2502 xx.xx.30.86  \u2502 xx.xx.30.86  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502        15 \u2502            1 \u2502           2 \u2502 xx.xx.30.83  \u2502 xx.xx.30.83  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n**Then Create test ReplicatedMergeTree loal table and Distribute table** \r\n\r\n> CREATE TABLE default.test1 on cluster ads_model_ck_cluster ( id UInt64,  name String, d Date) ENGINE =ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/default/test1', '{replica}') PARTITION BY toMonday(d) ORDER BY (id, d) SETTINGS index_granularity = 8192;\r\n\r\n> CREATE TABLE default.test1_all on cluster ads_model_ck_cluster (id UInt64,  name String, d Date) ENGINE = Distributed('ads_model_ck_cluster', 'test', 'test1', rand());\r\n\r\nINSERT some volume data into the table \r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :)  select count() from test1_all;\r\n\r\nSELECT count()\r\nFROM test1_all \r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502 4390912 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n**Get Detailed info of the partitions:**\r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) select database,table,partition_id,partition,name,active,rows,path,modification_time from system.parts where table='test1';\r\n\r\nSELECT \r\n    database, \r\n    table, \r\n    partition_id, \r\n    partition, \r\n    name, \r\n    active, \r\n    rows, \r\n    path, \r\n    modification_time\r\nFROM system.parts \r\nWHERE table = 'test1'\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500partition_id\u2500\u252c\u2500partition\u2500\u2500\u2500\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500active\u2500\u252c\u2500\u2500\u2500rows\u2500\u252c\u2500path\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500modification_time\u2500\u2510\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_25_0 \u2502      0 \u2502      1 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_25_0/ \u2502 2019-03-22 16:05:07 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_30_1 \u2502      0 \u2502    123 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_30_1/ \u2502 2019-03-22 16:05:14 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_35_2 \u2502      0 \u2502   4642 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_35_2/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_40_3 \u2502      0 \u2502 146165 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_40_3/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_43_4 \u2502      1 \u2502 290307 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_43_4/ \u2502 2019-03-22 16:07:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_26_26_0 \u2502      0 \u2502      8 \u2502 /export/data/clickhouse/data/default/test1/20190318_26_26_0/ \u2502 2019-03-22 16:05:11 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_27_27_0 \u2502      0 \u2502     11 \u2502 /export/data/clickhouse/data/default/test1/20190318_27_27_0/ \u2502 2019-03-22 16:05:11 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_28_28_0 \u2502      0 \u2502     17 \u2502 /export/data/clickhouse/data/default/test1/20190318_28_28_0/ \u2502 2019-03-22 16:05:12 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_29_29_0 \u2502      0 \u2502     26 \u2502 /export/data/clickhouse/data/default/test1/20190318_29_29_0/ \u2502 2019-03-22 16:05:13 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_30_30_0 \u2502      0 \u2502     60 \u2502 /export/data/clickhouse/data/default/test1/20190318_30_30_0/ \u2502 2019-03-22 16:05:13 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_31_31_0 \u2502      0 \u2502    142 \u2502 /export/data/clickhouse/data/default/test1/20190318_31_31_0/ \u2502 2019-03-22 16:05:14 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_32_32_0 \u2502      0 \u2502    305 \u2502 /export/data/clickhouse/data/default/test1/20190318_32_32_0/ \u2502 2019-03-22 16:05:15 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_33_33_0 \u2502      0 \u2502    553 \u2502 /export/data/clickhouse/data/default/test1/20190318_33_33_0/ \u2502 2019-03-22 16:05:15 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_34_34_0 \u2502      0 \u2502   1182 \u2502 /export/data/clickhouse/data/default/test1/20190318_34_34_0/ \u2502 2019-03-22 16:05:16 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_35_35_0 \u2502      0 \u2502   2337 \u2502 /export/data/clickhouse/data/default/test1/20190318_35_35_0/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_36_36_0 \u2502      0 \u2502   4585 \u2502 /export/data/clickhouse/data/default/test1/20190318_36_36_0/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_37_37_0 \u2502      0 \u2502   9213 \u2502 /export/data/clickhouse/data/default/test1/20190318_37_37_0/ \u2502 2019-03-22 16:05:34 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_38_38_0 \u2502      0 \u2502  18316 \u2502 /export/data/clickhouse/data/default/test1/20190318_38_38_0/ \u2502 2019-03-22 16:05:55 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_39_39_0 \u2502      0 \u2502  36600 \u2502 /export/data/clickhouse/data/default/test1/20190318_39_39_0/ \u2502 2019-03-22 16:05:58 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_40_40_0 \u2502      0 \u2502  72809 \u2502 /export/data/clickhouse/data/default/test1/20190318_40_40_0/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_41_41_0 \u2502      0 \u2502    501 \u2502 /export/data/clickhouse/data/default/test1/20190318_41_41_0/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_42_42_0 \u2502      0 \u2502  72817 \u2502 /export/data/clickhouse/data/default/test1/20190318_42_42_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_43_43_0 \u2502      0 \u2502  70824 \u2502 /export/data/clickhouse/data/default/test1/20190318_43_43_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_44_44_0 \u2502      1 \u2502   3065 \u2502 /export/data/clickhouse/data/default/test1/20190318_44_44_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n**Try to delete one partition using alter table** \r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) alter table default.test1 on cluster ads_model_ck_cluster drop partition '2019-03-18';\r\n\r\nALTER TABLE default.test1 ON CLUSTER ads_model_ck_cluster\r\n    DROP PARTITION '2019-03-18'\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.66  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  29 \u2502               18 \u2502\r\n\u2502 xx.xx.30.69  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  28 \u2502               18 \u2502\r\n\u2502 xx.xx.30.70  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  27 \u2502               18 \u2502\r\n\u2502 xx.xx.30.76  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  26 \u2502               18 \u2502\r\n\u2502 xx.xx.30.71  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  25 \u2502               18 \u2502\r\n\u2502 xx.xx.30.79  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.82:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  24 \u2502               18 \u2502\r\n\u2502 xx.xx.217.46 \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  23 \u2502               18 \u2502\r\n\u2502 xx.xx.30.86  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  22 \u2502               18 \u2502\r\n\u2502 xx.xx.30.65  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  21 \u2502               18 \u2502\r\n\u2502 xx.xx.40.123 \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  20 \u2502               18 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.72  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  19 \u2502               12 \u2502\r\n\u2502 xx.xx.30.67  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  18 \u2502               12 \u2502\r\n\u2502 xx.xx.217.49 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  17 \u2502               12 \u2502\r\n\u2502 xx.xx.30.81  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.217.52:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  16 \u2502               12 \u2502\r\n\u2502 xx.xx.217.47 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  15 \u2502               12 \u2502\r\n\u2502 xx.xx.30.74  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.78:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  14 \u2502               12 \u2502\r\n\u2502 xx.xx.30.85  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  13 \u2502               12 \u2502\r\n\u2502 xx.xx.30.77  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  12 \u2502               12 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.78  \u2502 9000 \u2502      0 \u2502       \u2502                  11 \u2502                9 \u2502\r\n\u2502 xx.xx.30.73  \u2502 9000 \u2502      0 \u2502       \u2502                  10 \u2502                9 \u2502\r\n\u2502 xx.xx.217.45 \u2502 9000 \u2502      0 \u2502       \u2502                   9 \u2502                9 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.50 \u2502 9000 \u2502      0 \u2502       \u2502                   8 \u2502                8 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.52 \u2502 9000 \u2502      0 \u2502       \u2502                   7 \u2502                7 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.82 \u2502 9000 \u2502      0 \u2502       \u2502                   6 \u2502                6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.68  \u2502 9000 \u2502      0 \u2502       \u2502                   5 \u2502                3 \u2502\r\n\u2502 xx.xx.40.126 \u2502 9000 \u2502      0 \u2502       \u2502                   4 \u2502                3 \u2502\r\n\u2502 xx.xx.30.64  \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.48 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                0 \u2502\r\n\u2502 xx.xx.30.80  \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 xx.xx.30.83  \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nReceived exception from server (version 18.14.19):\r\nCode: 194. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: There was an error on [xx.xx.30.79:9000]: Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.82:9000. DB::Exception: Password required for user default., e.what() = DB::Exception. \r\n\r\n30 rows in set. Elapsed: 1.072 sec. \r\n\r\n**Still some node has data** \r\nxx.xx.30.68 \r\n291660\r\nxx.xx.30.67 \r\n291660\r\nxx.xx.30.78 \r\n291577\r\nxx.xx.30.74 \r\n291577\r\nxx.xx.30.79 \r\n293352\r\nxx.xx.30.82 \r\n293352\r\nxx.xx.30.81 \r\n293279\r\nxx.xx.217.52 \r\n293279\r\n\r\n**Then I try it for the third Time** \r\nckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) alter table default.test1 on cluster ads_model_ck_cluster drop partition '2019-03-18';\r\n\r\nALTER TABLE default.test1 ON CLUSTER ads_model_ck_cluster\r\n    DROP PARTITION '2019-03-18'\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.78  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  29 \u2502               15 \u2502\r\n\u2502 xx.xx.30.67  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  28 \u2502               15 \u2502\r\n\u2502 xx.xx.30.72  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  27 \u2502               15 \u2502\r\n\u2502 xx.xx.30.81  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.217.52:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  26 \u2502               15 \u2502\r\n\u2502 xx.xx.30.65  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  25 \u2502               15 \u2502\r\n\u2502 xx.xx.30.86  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  24 \u2502               15 \u2502\r\n\u2502 xx.xx.217.47 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  23 \u2502               15 \u2502\r\n\u2502 xx.xx.30.66  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  22 \u2502               15 \u2502\r\n\u2502 xx.xx.30.69  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  21 \u2502               15 \u2502\r\n\u2502 xx.xx.30.70  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  20 \u2502               15 \u2502\r\n\u2502 xx.xx.30.76  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  19 \u2502               15 \u2502\r\n\u2502 xx.xx.30.71  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  18 \u2502               15 \u2502\r\n\u2502 xx.xx.30.85  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  17 \u2502               15 \u2502\r\n\u2502 xx.xx.30.82  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  16 \u2502               15 \u2502\r\n\u2502 xx.xx.30.77  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  15 \u2502               15 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.45 \u2502 9000 \u2502      0 \u2502       \u2502                  14 \u2502               12 \u2502\r\n\u2502 xx.xx.217.46 \u2502 9000 \u2502      0 \u2502       \u2502                  13 \u2502               12 \u2502\r\n\u2502 xx.xx.40.123 \u2502 9000 \u2502      0 \u2502       \u2502                  12 \u2502               12 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.52 \u2502 9000 \u2502      0 \u2502       \u2502                  11 \u2502               11 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.50 \u2502 9000 \u2502      0 \u2502       \u2502                  10 \u2502                6 \u2502\r\n\u2502 xx.xx.30.73  \u2502 9000 \u2502      0 \u2502       \u2502                   9 \u2502                6 \u2502\r\n\u2502 xx.xx.40.126 \u2502 9000 \u2502      0 \u2502       \u2502                   8 \u2502                6 \u2502\r\n\u2502 xx.xx.217.48 \u2502 9000 \u2502      0 \u2502       \u2502                   7 \u2502                6 \u2502\r\n\u2502 xx.xx.30.79  \u2502 9000 \u2502      0 \u2502       \u2502                   6 \u2502                6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.68  \u2502 9000 \u2502      0 \u2502       \u2502                   5 \u2502                4 \u2502\r\n\u2502 xx.xx.217.49 \u2502 9000 \u2502      0 \u2502       \u2502                   4 \u2502                4 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.64 \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                2 \u2502\r\n\u2502 xx.xx.30.74 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.80 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 xx.xx.30.83 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nReceived exception from server (version 18.14.19):\r\nCode: 194. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: There was an error on [xx.xx.30.67:9000]: Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception. \r\n\r\n30 rows in set. Elapsed: 1.071 sec. \r\n\r\n**Although there are errors warning, the data is deleted successfully.**\r\nThis is very strange. Can anyone give a hint on this?  \r\n"
    },
    "satisfaction_conditions": [
      "DDL operations must complete successfully across all cluster nodes despite authentication errors",
      "Partition data must be consistently removed from all nodes"
    ],
    "created_at": "2019-03-22T08:58:55Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4737",
    "source": {
      "issue_number": 4737
    },
    "initial_question": {
      "title": "Confused about server binaries size (deb)",
      "body": "Hello. \r\nI'm a little confused about clickhouse binaries size. I was going to upgrade version from 18 to 19 and noticed deb packets size: \r\n\r\n    clickhouse-server-base_18.16.1_amd64.deb                339M\r\n    clickhouse-server-base_19.4.1.3_amd64.deb               734M\r\n\r\nTwo questions:\r\n1) Is it correct that size doubles for a major version?\r\n2) What is included in a distribution that takes ~1.8G for a server binary (/usr/bin/clickhouse unarchived) in 19.4.1.3 version? "
    },
    "satisfaction_conditions": [
      "Package size difference is explained by identifiable factors",
      "Correct package selection guidance is provided",
      "Binary size increase is justified by specific feature additions",
      "Debug information handling is clarified",
      "Version-to-version size changes are quantified"
    ],
    "created_at": "2019-03-20T14:20:05Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4716",
    "source": {
      "issue_number": 4716
    },
    "initial_question": {
      "title": "Question around set indexes",
      "body": "Hi there,\r\n\r\nI'm a bit confused about set indexes and where they kick in. I've tried the examples in the tests folder and they work as expected however if I try to apply them in my case granules are never dropped. I've tried with larger values and more partitions (use toStartOfHour(ts) below) but set indexes never kick in. Am I mistaken in their use/purpose and have to wait for bloom indices or am I using them incorrectly?\r\n\r\nThanks!\r\n\r\n```\r\nSET allow_experimental_data_skipping_indices = 1;\r\n\r\nDROP TABLE IF EXISTS test.idx_test;\r\n\r\nCREATE TABLE test.idx_test (\r\n                s_key UInt64,\r\n                id UInt32,\r\n                ts DateTime,\r\n                value UInt64,\r\n                INDEX s_key_idx (s_key) TYPE set(0) GRANULARITY 1000\r\n) ENGINE = MergeTree\r\nORDER BY (id, ts)\r\nSETTINGS index_granularity = 32;\r\n\r\nINSERT INTO test.idx_test\r\nSELECT\r\n                cityHash64(id) AS s_key,\r\n                number AS id,\r\n                toDateTime(1551398400 + rand(1)%86400) AS ts,\r\n                rand(2) AS value\r\nFROM system.numbers LIMIT 100000;\r\n\r\nSET send_logs_level = 'debug';\r\n\r\nselect * from test.idx_test where id = 3000 format PrettySpace;\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.564572 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> executeQuery: (from 127.0.0.1:34334) select * from test.idx_test where id = 3000 format PrettySpace;\r\n[clickhouse-demo] 2019.03.17 22:01:10.566056 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> test.idx_test (SelectExecutor): Key condition: (column 0 in [3000, 3000])\r\n[clickhouse-demo] 2019.03.17 22:01:10.566108 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> test.idx_test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1 marks to read from 1 ranges\r\n[clickhouse-demo] 2019.03.17 22:01:10.566366 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> executeQuery: Query pipeline:\r\n\r\nExpression\r\nExpression\r\n  Filter\r\n   MergeTreeThread\r\n\r\n               s_key     id                    ts        value\r\n\r\n16286406272394286119   3000   2019-03-01 13:11:22   3080386888\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.568391 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Information> executeQuery: Read 32 rows, 768.00 B in 0.004 sec., 8682 rows/sec., 203.51 KiB/sec.\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.568446 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> MemoryTracker: Peak memory usage (for query): 1.44 MiB.\r\n\r\n1 rows in set. Elapsed: 0.004 sec.\r\n\r\nselect * from test.idx_test where s_key = 16286406272394286119 format PrettySpace;\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:35.857723 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> executeQuery: (from 127.0.0.1:34334) select * from test.idx_test where s_key = 16286406272394286119 format PrettySpace;\r\n[clickhouse-demo] 2019.03.17 22:01:35.858430 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"s_key = 16286406272394286119\" moved to PREWHERE\r\n[clickhouse-demo] 2019.03.17 22:01:35.858879 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Key condition: unknown\r\n[clickhouse-demo] 2019.03.17 22:01:35.859056 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Index `s_key_idx` has dropped 0 granules.\r\n[clickhouse-demo] 2019.03.17 22:01:35.859096 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 3125 marks to read from 1 ranges\r\n[clickhouse-demo] 2019.03.17 22:01:35.859269 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> executeQuery: Query pipeline:\r\nExpression\r\nExpression\r\n  MergeTreeThread\r\n\r\n               s_key     id                    ts        value\r\n16286406272394286119   3000   2019-03-01 13:11:22   3080386888\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:35.864492 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Information> executeQuery: Read 100000 rows, 781.27 KiB in 0.007 sec., 14970210 rows/sec., 114.22 MiB/sec.\r\n[clickhouse-demo] 2019.03.17 22:01:35.864554 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> MemoryTracker: Peak memory usage (for query): 3.99 MiB.\r\n\r\n1 rows in set. Elapsed: 0.008 sec. Processed 100.00 thousand rows, 800.02 KB (12.81 million rows/s., 102.50 MB/s.)\r\n```"
    },
    "satisfaction_conditions": [
      "Index creation parameters are valid and accepted"
    ],
    "created_at": "2019-03-17T22:20:10Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4628",
    "source": {
      "issue_number": 4628
    },
    "initial_question": {
      "title": "Top N elements group by time",
      "body": "Following is a sample schema\r\n\r\n```\r\n{\r\n    name String,\r\n    dateTime String,\r\n    ....\r\n    value UInt32\r\n}\r\n```\r\n\r\nWe would like to group them by timeseries. However, we are interested only in the top 5 elements with the highest value sum. \r\n\r\n```\r\nSELECT any(name),\r\n       sum(value)\r\nFROM TABLE\r\nWHERE name IN\r\n    (SELECT name\r\n     FROM TABLE\r\n     GROUP BY name\r\n     ORDER BY sum(value) DESC\r\n     LIMIT 5)\r\nGROUP BY name,\r\n         toRelativeMinuteNum(dateTime)\r\n```\r\n\r\nThe above gives the top 5 elements in the `IN` call and which are then used to again be grouped by name with the time bucket.\r\n\r\nHowever, we would also like to get the calculated internal `sum(value)` also separately. So\r\n\r\n```\r\nSELECT any(name),\r\n       sum(value),\r\n  (SELECT name\r\n   FROM TABLE\r\n   GROUP BY name\r\n   ORDER BY sum(value) DESC\r\n   LIMIT 5) AS topSumNames\r\nFROM TABLE\r\nWHERE name IN topSumNames\r\nGROUP BY name,\r\n         toRelativeMinuteNum(dateTime)\r\n```\r\n\r\nThis above fails with the error \r\n```\r\nDB::Exception: Scalar subquery returned more than one row\r\n```\r\n\r\nQuestions:\r\n\r\n1. How to get the internal sub query value also in the select output\r\n2. Is there a better way to perform this top 5 elements\r\n\r\nThe `LIMIT BY clause` is not working, when we put in the `dateTime` in the grouping."
    },
    "satisfaction_conditions": [
      "Query must return time-series data grouped by name and time intervals",
      "Results must be limited to only the top 5 entities by total value sum",
      "Query results must include both time-series data AND the overall sum that determined top 5 selection",
      "Query must execute without scalar subquery errors",
      "Query performance must be optimized for the given schema"
    ],
    "created_at": "2019-03-08T16:14:16Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4544",
    "source": {
      "issue_number": 4544
    },
    "initial_question": {
      "title": "Should Select be optimized for MATERIALIZED VIEW?",
      "body": "When using materialized view as a trigger, that said used for inserting data to table A when data is inserted to table B:\r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS tableBToTableA TO tableA\r\nAS\r\nSELECT\r\n    some_fields\r\nFROM tableB\r\nGROUP BY some_fields\r\n```\r\n\r\nIs it also required to optimize the MV select so it uses partitioning key and sorting keys of `tableB`? Or it doesn't matter?"
    },
    "satisfaction_conditions": [
      "Materialized view functionality remains efficient without requiring optimization of SELECT statement",
      "Ongoing updates process correctly through insert buffer"
    ],
    "created_at": "2019-03-01T10:10:30Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4512",
    "source": {
      "issue_number": 4512
    },
    "initial_question": {
      "title": "[Question] Too many parts error Monitoring",
      "body": "Hello. \r\nAccording to our last experience with the `Too many parts` errors we would like to start monitor this metric.\r\nHow I can do this ?\r\nFor example I think about such kind of query:\r\n`select count(*) from system.merges`\r\nor maybe\r\n`select count(*) from system.merges` + `select count(*) from system.replication_queue`\r\n?\r\nOr maybe there is some already built-in metric ?"
    },
    "satisfaction_conditions": [
      "Provides a method to monitor partition count metrics",
      "Returns quantifiable measurement data",
      "Monitors system-level partition information",
      "Can be used for continuous monitoring"
    ],
    "created_at": "2019-02-26T12:54:53Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4455",
    "source": {
      "issue_number": 4455
    },
    "initial_question": {
      "title": "GROUP BY Enum attribute WITH ROLLUP ",
      "body": "This question may become a bug, that's why I created issue.\r\n\r\nHere is test table with three versions of  the same Enum attribute.\r\n\r\n```\r\nCREATE table tmp_test_rollup\r\n(\r\n  UserGroupZeroBased Enum8('new' = 0, 'certain' = 1, 'test' = 2),\r\n  UserGroupOneBased  Enum8('new' = 1, 'certain' = 2, 'test' = 3),\r\n  UserGroupWithAll   Enum8('all' = 0, 'new' = 1, 'certain' = 2, 'test' = 3),\r\n  Period             UInt64,\r\n  Metric             Int64\r\n)\r\n  engine = TinyLog;\r\n\r\nINSERT INTO tmp_test_rollup (UserGroupZeroBased, UserGroupOneBased, UserGroupWithAll, Period, Metric)\r\nVALUES ('new', 'new', 'new', 1, 1000),\r\n       ('new', 'new', 'new', 2, 2000),\r\n       ('certain', 'certain', 'certain', 1, 500),\r\n       ('certain', 'certain', 'certain', 1, 100);\r\n```\r\nIf I want group by UserGroup with rollup, I can do three versions of query.\r\n\r\n1. In results of this query I'll get value `new` in rows where I expect get empty value. \r\n```\r\nSELECT Period, UserGroupZeroBased, sum(Metric)\r\nFROM tmp_test_rollup\r\nGROUP BY ROLLUP (Period, UserGroupZeroBased);\r\n``` \r\n\r\n2. In results of this query I'll get exception `Unexpected value 0 for type Enum8('new' = 1, 'certain' = 2, 'test' = 3)`\r\n```\r\nSELECT Period, UserGroupOneBased, sum(Metric)\r\nFROM tmp_test_rollup\r\nGROUP BY ROLLUP (Period, UserGroupOneBased);\r\n```\r\n3. In this query I'll get the expected correct result, but through special expression of `UserGroupWithAll` which was written for this case.\r\n```\r\nSELECT Period, UserGroupWithAll, sum(Metric)\r\nFROM tmp_test_rollup\r\nGROUP BY ROLLUP (Period, UserGroupWithAll);\r\n```\r\n\r\nResults in the first and second case correspond to expected results?\r\nSolution of this problem by extension of Enum list (as in the third example) is supposed?\r\n\r\n\r\nCH  version: 19.1.6"
    },
    "satisfaction_conditions": [
      "ROLLUP operation must handle default/empty values consistently",
      "Query results must not generate type exceptions during ROLLUP operations",
      "Default values in ROLLUP results must be clearly distinguishable from actual data values",
      "Solution must work within ClickHouse's non-NULL type system constraints"
    ],
    "created_at": "2019-02-20T09:26:06Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4414",
    "source": {
      "issue_number": 4414
    },
    "initial_question": {
      "title": "What is a \"granule\"?",
      "body": "The documentation on data skipping indexes states:\r\n\r\n> These indices aggregate some information about the specified expression on blocks, which consist of granularity_value granules, then these aggregates are used in SELECT queries for reducing the amount of data to read from the disk by skipping big blocks of data where where query cannot be satisfied.\r\n\r\nWhat exactly is a granule? Is it a row?\r\n\r\nAs a related question: are there plans for an index type similar to btree/hash secondary indexes of traditional RDBMS so a WHERE could efficiently look up rows without needing to be part of a prefix of the primary key or scanning all rows for the given column?\r\nAs I understand it, the current data skipping indexes basically allow only to answer the question \"does this block of rows contain the value that I am looking for?\" instead of \"which rows in this block contain the value that I am looking for\"."
    },
    "satisfaction_conditions": [
      "Definition of granule size must be clearly specified",
      "Relationship between granules and primary key indexing must be explained",
      "Data skipping index behavior must be accurately described",
      "Storage efficiency implications must be addressed",
      "Explanation of mark files and their purpose must be provided"
    ],
    "created_at": "2019-02-15T21:31:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/4350",
    "source": {
      "issue_number": 4350
    },
    "initial_question": {
      "title": "Table functions are forbidden in readonly mode...",
      "body": "i've configured user readonly to be used to only request data...\r\ni've created un view on clickhouse to external mysql database.\r\nwhen try to request (only read only request!) to clickhouse to this view with readonly user i receive this error:\r\n\"Table functions are forbidden in readonly mode\"\r\n\r\nthere a way to use view to external db with readonly user ?\r\n\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Read-only user must be able to query data from external MySQL database through ClickHouse",
      "Solution must work within ClickHouse's read-only mode restrictions",
      "Solution must maintain read-only security constraints"
    ],
    "created_at": "2019-02-11T18:32:59Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/3978",
    "source": {
      "issue_number": 3978
    },
    "initial_question": {
      "title": "Question:How to kill last executed select query which cost more time than expected through http interface?",
      "body": "When querying clickhouse through http interface, if the query time beyond the expectation, we want to kill this query and save the server's resources. Is there any way for this purpose?"
    },
    "satisfaction_conditions": [
      "Query execution must be terminable after a specified time threshold",
      "Solution must work through HTTP interface",
      "Time limit must be configurable per query",
      "Time limit must apply to distributed queries across all involved servers"
    ],
    "created_at": "2019-01-03T02:10:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/3971",
    "source": {
      "issue_number": 3971
    },
    "initial_question": {
      "title": "ALL JOIN inflating numbers",
      "body": "Hi there,\r\nimagine a simple query with a join like the following. When using an ANY join I get the same result as without a join but as soon as I use ALL, the result from the local fields (impressions, value..) is inflated. Usually this inflation is just 1-4% but in some cases it is 10-100 times. What would be the best course of action here?\r\n\r\n```\r\nSELECT\r\n    category,\r\n    count() AS impressions,\r\n    uniq(sessionId) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM impressions ALL LEFT JOIN\r\n(\r\n    SELECT\r\n        CounterID,\r\n        engaged,\r\n        visible\r\n    FROM visits\r\n    GROUP BY CounterID\r\n) USING CounterID\r\nGROUP BY category\r\n```"
    },
    "satisfaction_conditions": [
      "Query results must correctly handle duplicate impressionUUID entries in meta table",
      "Aggregated metrics (count, sum) must reflect actual data without artificial inflation",
      "Join operation must preserve one-to-many relationship integrity",
      "Aggregation results must match those of a query without joins",
      "Boolean flag values (visible/engaged) must be correctly aggregated"
    ],
    "created_at": "2018-12-30T13:55:37Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/3784",
    "source": {
      "issue_number": 3784
    },
    "initial_question": {
      "title": "Question: How to load fast big flat files ?",
      "body": "The best method I found is to use the table function `file(path, format, structure)`. It takes as an input parameter the relative path to the file from the setting `user_files_path`. One can change this setting in `/etc/clickhouse-server/config.xml` \r\n\r\n**Question:** Is it possible to change `user_files_path` in a clickhouse-client session with an `sql` command ?\r\n\r\nI suppose an alternative method instead of copying/placing the flat-file under `user_files_path` is to pipe the flat-file to command line client (`clickhouse-client`) but that requires access to the file system and the command has to be invoked from my python application.\r\n\r\nIs there another method to load fast big flat-files (millions of rows) ?\r\n\r\n**Clarification:** I want to load the data from flat-files to a temporary clickhouse table engine e.g. merge tree, log, memory, so that I can read and process column data fast and use these as an input to my TRIADB clickhouse table engines."
    },
    "satisfaction_conditions": [
      "Data must be successfully loaded into a temporary ClickHouse table",
      "Solution must handle large data volumes efficiently",
      "Data must be accessible for subsequent processing in ClickHouse",
      "Solution must work within file access security constraints",
      "Solution must be executable from a Python application context"
    ],
    "created_at": "2018-12-07T13:47:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/3659",
    "source": {
      "issue_number": 3659
    },
    "initial_question": {
      "title": "Pushing WHERE conditions from the view to underlying table ",
      "body": "Can you add parameters to the view\uff0c If there are no parameters, then every request must be queried before filtering, resulting in unnecessary waste of computing resources. In addition, JDBC query avoids transferring a large amount of SQL code.\r\nLook forward to your reply\r\nthanks "
    },
    "satisfaction_conditions": [
      "Query predicates (WHERE conditions) must be pushed down to the base table level",
      "Query performance must improve when filtering conditions are applied",
      "View definition must work with the database's replication system",
      "View behavior must be configurable through system settings"
    ],
    "created_at": "2018-11-25T01:00:09Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/3434",
    "source": {
      "issue_number": 3434
    },
    "initial_question": {
      "title": "Kafka EOF reached for partition ...",
      "body": "```\r\nClickHouse client version 18.14.9.\r\nConnecting to database monitoring at localhost:9000 as user broker.\r\nConnected to ClickHouse server version 18.14.9 revision 54409.\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u252c\u2500changed\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_insert_block_size    \u2502 1048576 \u2502       0 \u2502 The maximum block size for insertion, if we control the creation of blocks for insertion. \u2502\r\n\u2502 stream_flush_interval_ms \u2502 7500    \u2502       0 \u2502 Timeout for flushing data from streaming storages.                                        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\nI just upgrade from 1.1.54385 to 18.14.9 and I'm receiving \"EOF reached for partition...\" when any of my materialized views are running. Queues with little traffic don't show this message so much but a queue that gets 100+ messages per second is constantly getting this message and some queue messages aren't written to the MergeTree table. Any advice would be greatly appreciated.\r\n\r\n```\r\n2018.10.21 08:57:38.093341 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146709\r\n2018.10.21 08:57:38.620841 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146723\r\n2018.10.21 08:57:38.767349 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146724\r\n2018.10.21 08:57:38.973400 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146727\r\n2018.10.21 08:57:39.254782 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146730\r\n2018.10.21 08:57:39.706890 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146739\r\n2018.10.21 08:57:40.088991 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146745\r\n2018.10.21 08:57:40.242327 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146746\r\n2018.10.21 08:57:40.766095 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146749\r\n2018.10.21 08:57:41.090802 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146751\r\n2018.10.21 08:57:41.216236 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146752\r\n2018.10.21 08:57:41.587863 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146756\r\n2018.10.21 08:57:41.705616 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146759\r\n2018.10.21 08:57:41.933883 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146763\r\n2018.10.21 08:57:42.111605 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146765\r\n2018.10.21 08:57:42.585086 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146773\r\n```"
    },
    "satisfaction_conditions": [
      "Trace-level logs remain accessible for debugging"
    ],
    "created_at": "2018-10-21T09:09:39Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/2897",
    "source": {
      "issue_number": 2897
    },
    "initial_question": {
      "title": "[Question] ALTER DELETE in Materialized Views",
      "body": "Hello, will alter delete work for materialized views as well? Thanks"
    },
    "satisfaction_conditions": [
      "Mutations (ALTER DELETE) can be successfully applied to the materialized view's data",
      "The underlying storage structure is compatible with mutations",
      "The correct target table for mutations is identified"
    ],
    "created_at": "2018-08-20T15:28:41Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/2895",
    "source": {
      "issue_number": 2895
    },
    "initial_question": {
      "title": "Is there a way to check mutation is in progress?",
      "body": "I recently triggered a DELETE mutation on relatively small table. Is there a way I can see it is in progress or approximatelly determine when it will finish? \r\nIn `mutations` table I can only see `parts_to_do=1` but nothing else.\r\n\r\nThanks"
    },
    "satisfaction_conditions": [
      "Ability to monitor mutation progress status",
      "Information available through system tables",
      "Works with DELETE mutations",
      "Handles replicated and non-replicated tables"
    ],
    "created_at": "2018-08-20T12:57:39Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/2892",
    "source": {
      "issue_number": 2892
    },
    "initial_question": {
      "title": "Ways to include per user configuration from external files",
      "body": "Hello!\r\n\r\nI'm working on some ways to simplify user management for Clickhouse.\r\n\r\nGlobal user configuration file \"/etc/clickhouse-server/users.xml\" does not work for me very well. Because it requires complicated logic when we add/remove users.\r\n\r\nI have two options to maintain it properly:\r\n- Regenerate this user.xml file each time when we add new user external source (JSON/YAML). But it requires external code to generate it and increases complexity.\r\n- Read content of existing file, add new section, write changes. But it also involves pretty tricky XML processing and can break something.\r\n\r\nI'm interested in extracting this information to separate files:\r\n```\r\n<admin>\r\n  <password>new_password</password>\r\n  <networks incl=\"networks\" replace=\"replace\">\r\n    <ip>::/0</ip>\r\n  </networks>\r\n  <profile>default</profile>\r\n  <quota>default</quota>\r\n</admin>\r\n```\r\n\r\nIs there is any way to extract configuration for each use in separate file? \r\n\r\nThank you!"
    },
    "satisfaction_conditions": [
      "User configurations must be manageable independently",
      "Configuration changes must be recognized by the system without restart",
      "Each user's configuration must maintain the required XML structure",
      "Solution must reduce complexity of user management operations",
      "Configuration system must maintain data integrity"
    ],
    "created_at": "2018-08-19T11:34:32Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/2746",
    "source": {
      "issue_number": 2746
    },
    "initial_question": {
      "title": "What is the difference between version v18.x.x and v1.1.xxxx?",
      "body": "I notice there is a naming pattern change on ClickHouse version code.\r\nI wonder whether both are compatible, and I am safe to upgrade from v1.1.xxx to v18.xx? "
    },
    "satisfaction_conditions": [
      "Version compatibility between v1.1.xxx and v18.xx.x is confirmed",
      "Upgrade safety between versions is addressed"
    ],
    "created_at": "2018-07-28T03:41:02Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/2203",
    "source": {
      "issue_number": 2203
    },
    "initial_question": {
      "title": "[Question] Parallel execution of subqueries",
      "body": "If we have a query Q like:\r\n\r\n```sql\r\n\r\nselect \r\n  c1, \r\n  c2,\r\n  c3 / c4 \r\nfrom \r\n  ( select  c1,\r\n                c2,\r\n                count() as c3\r\n    from T1\r\n    where ...\r\n    group by c1, c2\r\n  )\r\n  ALL LEFT JOIN\r\n  ( select c1, \r\n               c2,\r\n               count() as c4\r\n    from T2\r\n    where ...\r\n    group by c1, c2\r\n  ) \r\n  USING (c1, c2)\r\n```\r\nT1 and T2 are distributed tables with merge tree engine, will Clickhouse execute those two sub queries   in parallel and then join the  subquery results?  In our test settings, they seems execute sequentially. just want to check if there are any settings to control this behavior.  \r\n\r\nThanks."
    },
    "satisfaction_conditions": [
      "Query execution behavior must be correctly explained regarding parallelism",
      "The explanation must address why the observed sequential execution occurs",
      "The explanation must account for the JOIN operation's requirements",
      "Memory handling behavior must be explained"
    ],
    "created_at": "2018-04-10T15:15:14Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/972",
    "source": {
      "issue_number": 972
    },
    "initial_question": {
      "title": "Data duplication",
      "body": "We have a Distributed table with two-node cluster with ReplicatedMergeTree tables. Once in 3 secs we make an insert to the Distributed table and see that some of the data are duplicated. Why and how we can avoid this?\r\n\r\nClickHouse server version 1.1.54236"
    },
    "satisfaction_conditions": [
      "Data distribution configuration must properly handle internal replication"
    ],
    "created_at": "2017-07-12T08:43:22Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/467",
    "source": {
      "issue_number": 467
    },
    "initial_question": {
      "title": "How to use `Distributed` with `MaterializedView`",
      "body": "```\r\nCREATE TABLE IF NOT EXISTS insert_view(...) ENGINE = Null;\r\n\r\nCREATE MATERIALIZED VIEW data_local ENGINE = AggregatingMergeTree(..., sumState(num1) as num1,sumState(num2) as num2,sumState(num3) as num3,minState(num4) as num4,maxState(num5) as num5,sumState(num6) as num6 FROM insert_view GROUP BY xxxx;\r\n\r\nCREATE TABLE data as data_local ENGINE = Distributed(perftest_2shards_1replicas, default, data_local, rand());\r\n```\r\n\r\nBut all record insert in a shard?\r\nso, how to use `Distributed` with `MaterializedView`"
    },
    "satisfaction_conditions": [
      "Materialized views must update consistently when source data changes",
      "Local materialized views must process data from their respective local tables",
      "Distributed table wrapper must correctly route queries to local tables"
    ],
    "created_at": "2017-02-08T10:12:51Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/9032",
    "source": {
      "issue_number": 9032
    },
    "initial_question": {
      "title": "result set is encoded through the MySQL client ",
      "body": "Hi All,\r\n\r\n\r\nI am trying to connect the Clickhouse with the MySQL client . But, the result set is coming something encoded .\r\n\r\nAnything I missed ?\r\n\r\n\r\nmysql -h127.0.0.1 -P9001 -udefault -pSakthi@321\r\n\r\nmysql> show databases;\r\n+------------------+\r\n| name             |\r\n+------------------+\r\n| 0x64656661756C74 |\r\n| 0x6A65737573     |\r\n| 0x73797374656D   |\r\n+------------------+\r\n3 rows in set (0.00 sec)\r\nRead 3 rows, 354.00 B in 0.000 sec., 6478 rows/sec., 746.55 KiB/sec.\r\n"
    },
    "satisfaction_conditions": [
      "Database names must display in human-readable text format",
      "MySQL client must successfully connect to ClickHouse server",
      "Character encoding compatibility between client and server",
      "Query results must be correctly formatted in tabular output"
    ],
    "created_at": "2020-02-06T10:45:56Z"
  },
  {
    "id": "https://github.com/ClickHouse/ClickHouse/issues/503",
    "source": {
      "issue_number": 503
    },
    "initial_question": {
      "title": "How to copy big data  to ClickHouse",
      "body": "I used tpch make 20G data in my OS.\r\nI know used command\r\n ```\r\n  time clickhouse-client --query=\"INSERT INTO NATION FORMAT CSV\" < some.csv\r\n```\r\ntpch make   data format is |  ,I read document can't find customer define  format .\r\n\r\nhow to copy this data to ClickHouse?\r\n\r\nThanks...."
    },
    "satisfaction_conditions": [
      "Data format transformation must convert TPC-H pipe-delimited format to a format compatible with ClickHouse"
    ],
    "created_at": "2017-02-17T05:56:57Z"
  }
]