[
  {
    "id": "https://github.com/rapidsai/cudf/issues/15614",
    "source": {
      "issue_number": 15614
    },
    "initial_question": {
      "title": "[BUG] 24.04 cuDF.pandas now errors on mixed dtype comparisons in row-wise functions (didn't in 24.02)",
      "body": "**Describe the bug**\r\nIn the cuDF pandas demo notebooks, we try to run `min()` on mixed dtypes.  It works in pandas, and used to work in cuDF.pandas 24.02.  It fails in 24.04.\r\n**Steps/Code to reproduce bug**\r\n```\r\n%load_ext cudf.pandas\r\nimport pandas as pd\r\n\r\nsmall_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\r\nsmall_df = pd.concat([small_df, small_df])\r\n\r\naxis = 0\r\nfor i in range(0, 2):\r\n    small_df.min(axis=axis)\r\n    axis = 1\r\n\r\ncounts = small_df.groupby(\"a\").b.count()\r\n```\r\noutputs\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:889, in _fast_slow_function_call(func, *args, **kwargs)\r\n    888 fast_args, fast_kwargs = _fast_arg(args), _fast_arg(kwargs)\r\n--> 889 result = func(*fast_args, **fast_kwargs)\r\n    890 if result is NotImplemented:\r\n    891     # try slow path\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)\r\n     29 def call_operator(fn, args, kwargs):\r\n---> 30     return fn(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/frame.py:1715, in Frame.min(self, axis, skipna, numeric_only, **kwargs)\r\n   1680 \"\"\"\r\n   1681 Return the minimum of the values in the DataFrame.\r\n   1682 \r\n   (...)\r\n   1713     Parameters currently not supported are `level`, `numeric_only`.\r\n   1714 \"\"\"\r\n-> 1715 return self._reduce(\r\n   1716     \"min\",\r\n   1717     axis=axis,\r\n   1718     skipna=skipna,\r\n   1719     numeric_only=numeric_only,\r\n   1720     **kwargs,\r\n   1721 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6267, in DataFrame._reduce(self, op, axis, numeric_only, **kwargs)\r\n   6266 elif axis == 1:\r\n-> 6267     return source._apply_cupy_method_axis_1(op, **kwargs)\r\n   6268 else:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6449, in DataFrame._apply_cupy_method_axis_1(self, method, *args, **kwargs)\r\n   6447 kwargs.pop(\"cast_to_int\", None)\r\n-> 6449 prepared, mask, common_dtype = self._prepare_for_rowwise_op(\r\n   6450     method, skipna, numeric_only\r\n   6451 )\r\n   6452 for col in prepared._data.names:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6071, in DataFrame._prepare_for_rowwise_op(self, method, skipna, numeric_only)\r\n   6066 if (\r\n   6067     not numeric_only\r\n   6068     and is_string_dtype(common_dtype)\r\n   6069     and any(not is_string_dtype(dt) for dt in filtered.dtypes)\r\n   6070 ):\r\n-> 6071     raise TypeError(\r\n   6072         f\"Cannot perform row-wise {method} across mixed-dtype columns,\"\r\n   6073         \" try type-casting all the columns to same dtype.\"\r\n   6074     )\r\n   6076 if not skipna and any(col.nullable for col in filtered._columns):\r\n\r\nTypeError: Cannot perform row-wise min across mixed-dtype columns, try type-casting all the columns to same dtype.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[2], line 11\r\n      9 axis = 0\r\n     10 for i in range(0, 2):\r\n---> 11     small_df.min(axis=axis)\r\n     12     axis = 1\r\n     14 counts = small_df.groupby(\"a\").b.count()\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:837, in _CallableProxyMixin.__call__(self, *args, **kwargs)\r\n    836 def __call__(self, *args, **kwargs) -> Any:\r\n--> 837     result, _ = _fast_slow_function_call(\r\n    838         # We cannot directly call self here because we need it to be\r\n    839         # converted into either the fast or slow object (by\r\n    840         # _fast_slow_function_call) to avoid infinite recursion.\r\n    841         # TODO: When Python 3.11 is the minimum supported Python version\r\n    842         # this can use operator.call\r\n    843         call_operator,\r\n    844         self,\r\n    845         args,\r\n    846         kwargs,\r\n    847     )\r\n    848     return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:902, in _fast_slow_function_call(func, *args, **kwargs)\r\n    900         slow_args, slow_kwargs = _slow_arg(args), _slow_arg(kwargs)\r\n    901         with disable_module_accelerator():\r\n--> 902             result = func(*slow_args, **slow_kwargs)\r\n    903 return _maybe_wrap_result(result, func, *args, **kwargs), fast\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)\r\n     29 def call_operator(fn, args, kwargs):\r\n---> 30     return fn(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11630, in DataFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  11622 @doc(make_doc(\"min\", ndim=2))\r\n  11623 def min(\r\n  11624     self,\r\n   (...)\r\n  11628     **kwargs,\r\n  11629 ):\r\n> 11630     result = super().min(axis, skipna, numeric_only, **kwargs)\r\n  11631     if isinstance(result, Series):\r\n  11632         result = result.__finalize__(self, method=\"min\")\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:12385, in NDFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  12378 def min(\r\n  12379     self,\r\n  12380     axis: Axis | None = 0,\r\n   (...)\r\n  12383     **kwargs,\r\n  12384 ):\r\n> 12385     return self._stat_function(\r\n  12386         \"min\",\r\n  12387         nanops.nanmin,\r\n  12388         axis,\r\n  12389         skipna,\r\n  12390         numeric_only,\r\n  12391         **kwargs,\r\n  12392     )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:12374, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\r\n  12370 nv.validate_func(name, (), kwargs)\r\n  12372 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\r\n> 12374 return self._reduce(\r\n  12375     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\r\n  12376 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11549, in DataFrame._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n  11545     df = df.T\r\n  11547 # After possibly _get_data and transposing, we are now in the\r\n  11548 #  simple case where we can use BlockManager.reduce\r\n> 11549 res = df._mgr.reduce(blk_func)\r\n  11550 out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\r\n  11551 if out_dtype is not None and out.dtype != \"boolean\":\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1500, in BlockManager.reduce(self, func)\r\n   1498 res_blocks: list[Block] = []\r\n   1499 for blk in self.blocks:\r\n-> 1500     nbs = blk.reduce(func)\r\n   1501     res_blocks.extend(nbs)\r\n   1503 index = Index([None])  # placeholder\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:404, in Block.reduce(self, func)\r\n    398 @final\r\n    399 def reduce(self, func) -> list[Block]:\r\n    400     # We will apply the function and reshape the result into a single-row\r\n    401     #  Block with the same mgr_locs; squeezing will be done at a higher level\r\n    402     assert self.ndim == 2\r\n--> 404     result = func(self.values)\r\n    406     if self.values.ndim == 1:\r\n    407         res_values = result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11468, in DataFrame._reduce.<locals>.blk_func(values, axis)\r\n  11466         return np.array([result])\r\n  11467 else:\r\n> 11468     return op(values, axis=axis, skipna=skipna, **kwds)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\r\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    146 else:\r\n--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    149 return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\r\n    401 if datetimelike and mask is None:\r\n    402     mask = isna(values)\r\n--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\r\n    406 if datetimelike:\r\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:1098, in _nanminmax.<locals>.reduction(values, axis, skipna, mask)\r\n   1093     return _na_for_min_count(values, axis)\r\n   1095 values, mask = _get_values(\r\n   1096     values, skipna, fill_value_typ=fill_value_typ, mask=mask\r\n   1097 )\r\n-> 1098 result = getattr(values, meth)(axis)\r\n   1099 result = _maybe_null_out(result, axis, mask, values.shape)\r\n   1100 return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:45, in _amin(a, axis, out, keepdims, initial, where)\r\n     43 def _amin(a, axis=None, out=None, keepdims=False,\r\n     44           initial=_NoValue, where=True):\r\n---> 45     return umr_minimum(a, axis, None, out, keepdims, initial, where)\r\n\r\nTypeError: '<=' not supported between instances of 'int' and 'str'\r\n```\r\n\r\nit used to output a warning:\r\n```\r\n/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py:5971: UserWarning: Row-wise operations currently only support int, float and bool dtypes. Non numeric columns are ignored.\r\n  warnings.warn(msg)\r\n```\r\nand then worked:\r\n```\r\na\r\n0    2\r\n1    2\r\n2    2\r\nName: b, dtype: int64\r\n```\r\n\r\n**Expected behavior**\r\n```\r\n>>> counts\r\na\r\n0    2\r\n1    2\r\n2    2\r\nName: b, dtype: int64\r\n```\r\nwhich is what I get in pandas and 24.02 cuDF.pandas\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: [Docker]\r\n - Method of cuDF install: [Docker]\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used:  \r\n   - for 24.02: `docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/notebooks:24.02-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root`\r\n   - For 24.04:  `docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/notebooks:24.04-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root`\r\n"
    },
    "satisfaction_conditions": [
      "Mixed dtype operations (int and string) must produce consistent behavior between cuDF and pandas versions",
      "Error messages must accurately reflect the current pandas behavior for mixed dtype operations",
      "Version compatibility changes must be clearly documented"
    ],
    "created_at": "2024-04-29T22:45:28Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/14395",
    "source": {
      "issue_number": 14395
    },
    "initial_question": {
      "title": "[QST]use cudf.concat slower than pandas.concat",
      "body": "** cudf.concat slower than pandas.concat**\r\n## here is my code:\r\n```\r\nimport os, time, pandas as pd, numpy as np\r\nimport cudf\r\nfrom tqdm import tqdm\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES']='1'\r\ndef pd_concat_test(df):\r\n    st = 10\r\n    tdf = df[:st].copy()\r\n    n = len(df) - st\r\n    ta = time.time()\r\n    for i in tqdm(range(st, len(df))):\r\n        tdf = pd.concat([tdf, df[i:i+1]])\r\n    tb = time.time()\r\n\r\n    print(f'pd concat {n} times cost {tb-ta :.2f} s.')\r\n\r\n\r\ndef cupd_concat_test(cdf):\r\n    st = 10\r\n    tdf = cdf[:st].copy()\r\n    n = len(cdf) - st\r\n    ta = time.time()\r\n    for i in tqdm(range(st, len(cdf))):\r\n        tdf = cudf.concat([tdf, cdf[i:i+1]])\r\n    tb = time.time()\r\n\r\n    print(f'cudf concat {n} times cost {tb-ta :.2f} s.')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    in_csv = 'target.csv'\r\n    \r\n    df = pd.read_csv(in_csv)\r\n    cdf = cudf.read_csv(in_csv)\r\n    print(df.head(5))\r\n    cupd_concat_test(cdf)\r\n    pd_concat_test(df)\r\n```\r\n## here's output:\r\n```\r\n       timestamp     open     high      low    close    volume  quote_volume\r\n0  1577836800000  7189.43  7190.52  7170.15  7171.55  2449.049   17576407.75\r\n1  1577840400000  7171.55  7225.00  7171.10  7210.24  3865.038   27838016.40\r\n2  1577844000000  7210.24  7239.30  7206.46  7237.99  3228.365   23324787.16\r\n3  1577847600000  7237.41  7239.74  7215.00  7221.65  2513.307   18161803.91\r\n4  1577851200000  7221.65  7225.41  7211.22  7213.86  1176.666    8493621.94\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 33398/33398 [01:01<00:00, 542.44it/s]\r\ncudf concat 33398 times cost 61.57 s.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 33398/33398 [00:14<00:00, 2279.08it/s]\r\npd concat 33398 times cost 14.65 s.\r\n```\r\n## here's my env: \r\n* python3.10.4\r\n*  nvcc -V\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n```\r\n* pip list |grep cu\r\n```\r\ncubinlinker-cu11          0.3.0.post1\r\ncucim                     23.10.0\r\ncuda-python               11.8.3\r\ncudf-cu11                 23.10.1\r\ncugraph-cu11              23.10.0\r\ncuml-cu11                 23.10.0\r\ncuproj-cu11               23.10.0\r\ncupy-cuda11x              12.2.0\r\ncuspatial-cu11            23.10.0\r\ncuxfilter-cu11            23.10.0\r\ndask-cuda                 23.10.0\r\ndask-cudf-cu11            23.10.1\r\ndocutils                  0.20\r\nexecuting                 1.2.0\r\nptxcompiler-cu11          0.7.0.post1\r\npylibcugraph-cu11         23.10.0\r\npylibraft-cu11            23.10.0\r\nraft-dask-cu11            23.10.0\r\nrmm-cu11                  23.10.0\r\ntorch                     1.12.0+cu113\r\nucx-py-cu11               0.34.0\r\n```"
    },
    "satisfaction_conditions": [
      "GPU operations must process data in sufficiently large batches",
      "Performance must improve compared to CPU-based processing when using appropriate batch sizes",
      "Solution must handle high-frequency row additions efficiently"
    ],
    "created_at": "2023-11-10T10:37:20Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/12980",
    "source": {
      "issue_number": 12980
    },
    "initial_question": {
      "title": "[QST] What's the cudf overhead for small dataset?",
      "body": "**What is your question?**\r\n\r\n```\r\nimport cudf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf1 = pd.DataFrame()\r\ndim = 1000\r\ndf1[\"A\"] = np.random.randint(0, dim, dim)\r\n\r\ndf1_cu = cudf.from_pandas(df1)\r\n\r\n%%time\r\ndf1_cu[\"A\"].sum()\r\n600 \u00b5s\r\n\r\n%%time\r\ndf1[\"A\"].sum()\r\n200 \u00b5s\r\n```\r\n\r\ncudf seems to have some  overhead for small datasets. Where does it come from? It should not from data transfer as \r\ndf1_cu = cudf.from_pandas(df1) has transferred the data."
    },
    "satisfaction_conditions": [
      "Performance overhead for small datasets must be explained",
      "Data transfer considerations must be addressed",
      "Solution must demonstrate improved performance for multiple small-scale operations",
      "Parallel processing capability must be utilized effectively",
      "Batch processing approach must maintain result accuracy"
    ],
    "created_at": "2023-03-20T23:29:11Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/11219",
    "source": {
      "issue_number": 11219
    },
    "initial_question": {
      "title": "[QST] Disabling  decimal128 support",
      "body": "Hi cudf team,\r\n\r\nI am building from source cudf on pcc64le (Summit cluster)  using CUDA 11.4 and driver 450.80.02\r\n\r\n```\r\n...\r\n[ 11%] Building CXX object CMakeFiles/cudf.dir/src/aggregation/result_cache.cpp.o\r\n[ 11%] Building CXX object CMakeFiles/cudf.dir/src/ast/expression_parser.cpp.o\r\n/sw/summit/ums/gen119/nvrapids/src/nvrapids_v22.06.00_src/cudf_v22.06.00/cpp/include/cudf/utilities/type_dispatcher.hpp(522): error: \"numeric::decimal128\" contains a 128-bit integer, which is not supported in device code\r\n          detected during instantiation of \"decltype(auto) cudf::type_dispatcher(cudf::data_type, Functor, Ts &&...) [with IdTypeMap=cudf::id_to_type_impl, Functor=cudf::detail::unary_relationally_comparable_functor, Ts=<>]\" \r\n/sw/summit/ums/gen119/nvrapids/src/nvrapids_v22.06.00_src/cudf_v22.06.00/cpp/include/cudf/utilities/traits.hpp(149): here\r\n...\r\n```\r\n\r\nIs there way to disable decimal128 support ? \r\n\r\nFrom what I checked so far decimal128 support started in CUDA 11.5 so I would expect support should be disabled when finding older CUDA version, right ?\r\n\r\nThanks,\r\n\r\n"
    },
    "satisfaction_conditions": [
      "The code must successfully compile without decimal128-related errors",
      "The solution must be compatible with CUDA 11.4 environment"
    ],
    "created_at": "2022-07-07T21:28:34Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/7991",
    "source": {
      "issue_number": 7991
    },
    "initial_question": {
      "title": "[QST] Queston about row number limit in cuDF dataframe",
      "body": "Dear cuDF developers,\r\n\r\nI am using Dask to load in some parquet data as a dask-cudf dataframe. When I use `.compute()` to convert from dask-cudf back to cuDF (I need some functions that aren't supported in dask-cudf) I am encountering this error:\r\n\r\n```\r\nRuntimeError: cuDF failure at: ../src/copying/concatenate.cu:365: Total number of concatenated rows exceeds size_type range\r\n```\r\n\r\nMy dataframe has 27 million rows which seems... large but maybe still reasonable? What is the row limit? Is there any way I can increase this limit? \r\n\r\nIf I can provide more info please let me know.\r\n\r\nThank you very much,\r\nLaurie"
    },
    "satisfaction_conditions": [
      "Data processing operations must complete without row limit errors",
      "All required data transformations (melting, datetime conversion, duplicate removal) must be supported",
      "Solution must handle string columns within their character limits",
      "Data processing must work with the existing 27 million row dataset"
    ],
    "created_at": "2021-04-19T05:13:54Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/7481",
    "source": {
      "issue_number": 7481
    },
    "initial_question": {
      "title": "[QST]problems with dask_cudf custom aggregation",
      "body": "**What is your question?**\r\nHi there,\r\n\r\nI'm trying to do a string-join aggregation in dask_cudf groupby dataframe. The input dataframe looks like below:\r\n`documents_categories.compute()`\r\n\r\ndocument_id | kv\r\n-- | --\r\n1595802 | 1611:0.92\r\n1595802 | 1610:0.07\r\n1524246 | 1807:0.92\r\n1524246 | 1608:0.07\r\n\r\n`documents_categories.dtypes`\r\n\r\n> document_id     int64\r\n> kv             object\r\n> dtype: object\r\n\r\nThe expected string-joined result should be:\r\ndocument_id | kv\r\n-- | --\r\n1595802 | 1611:0.92;1610:0.07\r\n1524246 | 1807:0.92;1608:0.07\r\n\r\nI have tried the following codes and other several methods, but still can't get this function running successfully. I'm not a expert in dask_cudf, any suggestions? Thanks!\r\n\r\n```\r\ncustom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\ndocuments_categories.groupby('document_id').agg({\"kv\": custom_join})\r\n```\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py in raise_on_meta_error(funcname, udf)\r\n    179     try:\r\n--> 180         yield\r\n    181     except Exception as e:\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _emulate(func, *args, **kwargs)\r\n   5315     with raise_on_meta_error(funcname(func), udf=kwargs.pop(\"udf\", False)):\r\n-> 5316         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n   5317 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in _groupby_apply_funcs(df, *index, **kwargs)\r\n    920     for result_column, func, func_kwargs in funcs:\r\n--> 921         r = func(grouped, **func_kwargs)\r\n    922 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in _apply_func_to_column(df_like, column, func)\r\n    966 \r\n--> 967     return func(df_like[column])\r\n    968 \r\n\r\n<ipython-input-45-5dd27ef25785> in <lambda>(x)\r\n----> 1 custom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/groupby/groupby.py in __getattribute__(self, key)\r\n     62         try:\r\n---> 63             return super().__getattribute__(key)\r\n     64         except AttributeError:\r\n\r\nAttributeError: 'SeriesGroupBy' object has no attribute 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-46-31b5ac92e045> in <module>\r\n----> 1 documents_categories.groupby('document_id').agg({\"kv\": custom_join})\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in agg(self, arg, split_every, split_out)\r\n   1846     @derived_from(pd.core.groupby.DataFrameGroupBy)\r\n   1847     def agg(self, arg, split_every=None, split_out=1):\r\n-> 1848         return self.aggregate(arg, split_every=split_every, split_out=split_out)\r\n   1849 \r\n   1850 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/groupby.py in aggregate(self, arg, split_every, split_out)\r\n     81 \r\n     82         return super().aggregate(\r\n---> 83             arg, split_every=split_every, split_out=split_out\r\n     84         )\r\n     85 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in aggregate(self, arg, split_every, split_out)\r\n   1842             return self.size()\r\n   1843 \r\n-> 1844         return super().aggregate(arg, split_every=split_every, split_out=split_out)\r\n   1845 \r\n   1846     @derived_from(pd.core.groupby.DataFrameGroupBy)\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in aggregate(self, arg, split_every, split_out)\r\n   1622             split_out=split_out,\r\n   1623             split_out_setup=split_out_on_index,\r\n-> 1624             sort=self.sort,\r\n   1625         )\r\n   1626 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in apply_concat_apply(args, chunk, aggregate, combine, meta, token, chunk_kwargs, aggregate_kwargs, combine_kwargs, split_every, split_out, split_out_setup, split_out_setup_kwargs, sort, ignore_index, **kwargs)\r\n   5267 \r\n   5268     if meta is no_default:\r\n-> 5269         meta_chunk = _emulate(chunk, *args, udf=True, **chunk_kwargs)\r\n   5270         meta = _emulate(\r\n   5271             aggregate, _concat([meta_chunk], ignore_index), udf=True, **aggregate_kwargs\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _emulate(func, *args, **kwargs)\r\n   5314     \"\"\"\r\n   5315     with raise_on_meta_error(funcname(func), udf=kwargs.pop(\"udf\", False)):\r\n-> 5316         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n   5317 \r\n   5318 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    128                 value = type()\r\n    129             try:\r\n--> 130                 self.gen.throw(type, value, traceback)\r\n    131             except StopIteration as exc:\r\n    132                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py in raise_on_meta_error(funcname, udf)\r\n    199         )\r\n    200         msg = msg.format(\" in `{0}`\".format(funcname) if funcname else \"\", repr(e), tb)\r\n--> 201         raise ValueError(msg) from e\r\n    202 \r\n    203 \r\n\r\nValueError: Metadata inference failed in `_groupby_apply_funcs`.\r\n\r\nYou have supplied a custom function and Dask is unable to \r\ndetermine the type of output that that function returns. \r\n\r\nTo resolve this please provide a meta= keyword.\r\nThe docstring of the Dask function you ran should have more information.\r\n\r\nOriginal error is below:\r\n------------------------\r\nAttributeError(\"'SeriesGroupBy' object has no attribute 'str'\")\r\n\r\nTraceback:\r\n---------\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py\", line 180, in raise_on_meta_error\r\n    yield\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py\", line 5316, in _emulate\r\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py\", line 921, in _groupby_apply_funcs\r\n    r = func(grouped, **func_kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py\", line 967, in _apply_func_to_column\r\n    return func(df_like[column])\r\n  File \"<ipython-input-45-5dd27ef25785>\", line 1, in <lambda>\r\n    custom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/groupby/groupby.py\", line 63, in __getattribute__\r\n    return super().__getattribute__(key)\r\n```"
    },
    "satisfaction_conditions": [
      "Groups data correctly by document_id",
      "Preserves the original kv value format",
      "Works with dask_cudf distributed dataframe"
    ],
    "created_at": "2021-03-02T04:40:00Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/6070",
    "source": {
      "issue_number": 6070
    },
    "initial_question": {
      "title": "[BUG] .str.stod() no longer works on a String column",
      "body": "**Describe the bug**\r\nI used `.stod()` to convert a string column to a decimal. However, this has stopped working on nightly 0.15.\r\n\r\n**Steps/Code to reproduce bug**\r\nMinimal example:\r\n\r\n```\r\ndf = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\ndf['string_column'].str.stod()\r\n```\r\n\r\n**Expected behavior**\r\nA columns converted to decimal type.\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: Docker\r\n - Method of cuDF install: Docker\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used\r\n\r\n   - pull: `docker pull rapidsai/rapidsai-nightly:cuda10.2-runtime-ubuntu18.04-py3.7`\r\n   - run: \r\n\r\n```\r\ndocker run --gpus all -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --name kdd_rapids \\\r\n\trapidsai/rapidsai-nightly:cuda10.2-runtime-ubuntu18.04-py3.7\r\n```\r\n\r\n**Environment details**\r\nPlease run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n"
    },
    "satisfaction_conditions": [
      "String column values must be successfully converted to decimal/float numbers",
      "Resulting numeric values must maintain decimal precision",
      "Operation must work within the cuDF framework",
      "Solution must handle string input column type",
      "Resulting output must be a numeric dtype"
    ],
    "created_at": "2020-08-22T02:44:33Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/5830",
    "source": {
      "issue_number": 5830
    },
    "initial_question": {
      "title": "install error[QST]",
      "body": "Dear developer,\r\nThanks for developing nice tool. I would like to install cudf. But when I tried to install cudf with conda, I got following error.\r\ncations were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/linux-64::__cuda==11.0=0\r\n  - feature:|@/linux-64::__cuda==11.0=0\r\n\r\nYour installed CUDA driver is: 11.0\r\n\r\nMy cuder driver version is 450 and nvidia-smi shows cuda version is 11.0. But I installed condatoolkit version 10.1.\r\nSo I think actual cuda version of my env is cuda10.1.\r\nAre there any way to install cudf without downgrading nvidia-drive version?\r\nAny comments a/o suggestions will be greatly appreciated.\r\nThanks in advance.\r\n\r\nTaka"
    },
    "satisfaction_conditions": [
      "CUDA package dependencies must be compatible with the installed environment",
      "Package version conflicts must be resolved",
      "cudf must successfully install and be importable"
    ],
    "created_at": "2020-08-03T12:26:41Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/4893",
    "source": {
      "issue_number": 4893
    },
    "initial_question": {
      "title": "[QST] I need a \"reduce\" operation",
      "body": "Hi!\r\n\r\nI have grouped my cudf stream and now I need to reduce values by unique values.\r\n For example, My stream has:\r\na   b\r\n1  1\r\n1  2\r\n2  3\r\n2  4\r\n\r\nI need to get stream with have unique value of column 'a' and result of some function from column 'b'\r\na  f(a)\r\n1  10\r\n2  20\r\n\r\nHow I can do it? Thanks for advice!"
    },
    "satisfaction_conditions": [
      "Output data must contain unique values in column 'a'",
      "Output must aggregate values from column 'b' for each unique value in column 'a'",
      "Output must maintain the same data structure with two columns"
    ],
    "created_at": "2020-04-14T09:46:42Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/4874",
    "source": {
      "issue_number": 4874
    },
    "initial_question": {
      "title": "[QST] ai.rapids:cudf:cuda10 for CUDA 10.2",
      "body": "Hi!\r\nI need artifact\r\n`<dependency>\r\n        <groupId>ai.rapids</groupId>\r\n        <artifactId>cudf</artifactId>\r\n        <classifier>cuda10-1</classifier>\r\n        <version>0.9.2</version>\r\n    </dependency>`\r\nfor CUDA 10.2. How I can build it?"
    },
    "satisfaction_conditions": [
      "Application must be able to execute with CUDA 10.2 driver installed",
      "Required CUDA runtime libraries must be accessible to the application",
      "Maven dependency resolution must succeed without errors"
    ],
    "created_at": "2020-04-10T10:44:23Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/2958",
    "source": {
      "issue_number": 2958
    },
    "initial_question": {
      "title": "cudf flatten API [QST]",
      "body": "**What is your question?**\r\nDoes CUDF  support an api to flat the dataframe like pd.values.flatten() ? Thanks you.\r\n```python\r\nIn [40]: df_matchColor1                                       \r\nOut[40]: \r\n      color  key_index\r\n0   #ffffd9          0\r\n1   #7fcdbb          1\r\n2   #41b6c4          2\r\n3   #1d91c0          3\r\n4   #ffffd9          4\r\n5   #7fcdbb          5\r\n6   #41b6c4          6\r\n7   #1d91c0          7\r\n8   #ffffd9          8\r\n9   #7fcdbb          9\r\n10  #41b6c4         10\r\n11  #1d91c0         11\r\n\r\nIn [53]: df_matchColor1.to_pandas().values.flatten()                                 \r\nOut[53]: \r\narray(['#ffffd9', 0, '#7fcdbb', 1, '#41b6c4', 2, '#1d91c0', 3, '#ffffd9',\r\n       4, '#7fcdbb', 5, '#41b6c4', 6, '#1d91c0', 7, '#ffffd9', 8,\r\n       '#7fcdbb', 9, '#41b6c4', 10, '#1d91c0', 11], dtype=object)\r\n```"
    },
    "satisfaction_conditions": [
      "The operation must convert a cuDF DataFrame into a flattened 1-dimensional array",
      "The operation must maintain consistent data types within the resulting array",
      "The result must remain on the GPU if input was on GPU"
    ],
    "created_at": "2019-10-03T20:42:50Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/2871",
    "source": {
      "issue_number": 2871
    },
    "initial_question": {
      "title": "[QST] Out of Memory when counting number of rows using multiple parquet fles",
      "body": "I have 500 parquet files. Each parquet file is SNAPPY compressed and is around 200MB of size. Parquet consists of three columns. Each column is a string.\r\n\r\nI am trying to find the number of rows using the following code:\r\n\r\n```python\r\ncluster = LocalCUDACluster(ip=sched_ip, n_workers=num_of_gpus)\r\nclient = Client(cluster)\r\ndf = dask_cudf.read_parquet(path_to_local, columns=['col1','col2'])\r\nrows, cols = df.shape\r\nnum_rows = rows.compute()\r\n```\r\n\r\nIt throws a Runtime Exception: ```Exception: RuntimeError('parallel_for failed: out of memory')```\r\n\r\nI am using an EC2 instance (p3.8xlarge) with following configuration:\r\n1) **RAM**: 244GB\r\n2) **vCPU**: 32\r\n3) **GPU RAM**: 64GB\r\n4) **GPUs**: 4 Tesla V100\r\n\r\nIs this expected behaviour? Or are there any workarounds?\r\n"
    },
    "satisfaction_conditions": [
      "Total row count is obtained without loading full dataset into memory"
    ],
    "created_at": "2019-09-26T05:58:12Z"
  },
  {
    "id": "https://github.com/rapidsai/cudf/issues/1406",
    "source": {
      "issue_number": 1406
    },
    "initial_question": {
      "title": "[QST] Regarding the usage of CMAKE_CXX11_ABI flag in cudf/cpp/CMakeLists.txt ",
      "body": "**What is your question?**\r\n1. I can see that  `CMAKE_CXX11_ABI` flag  set by default without explicitly setting it up for `GNU compiler`. \r\n[ without setting `-DCMAKE_CXX11_ABI` in `CMAKE_COMMON_VARIABLES` ] \r\n \r\n``` \r\nif(CMAKE_COMPILER_IS_GNUCXX)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\r\n\r\n    option(CMAKE_CXX11_ABI \"Enable the GLIBCXX11 ABI\" ON)\r\n    if(CMAKE_CXX11_ABI)\r\n        message(STATUS \"CUDF: Enabling the GLIBCXX11 ABI\")\r\n```\r\nBuild log snippet: \r\n``` \r\n-- CUDF: Enabling the GLIBCXX11 ABI\r\n   \r\n-std=c++14 -x cu -c $SRC_DIR/cpp/src/comms/ipc/ipc.cu -o CMakeFiles/cudf.dir/src/comms/ipc/ipc.cu.o\r\n```\r\nDoes that mean for a default case of `GNU compiler` cuDF supports `CXX11_ABI`  and `CMAKE_CXX_STANDARD` and `CMAKE_CUDA_STANDARD` are set to `14` ? \r\n\r\n2.  `set(CMAKE_CXX_STANDARD 14) set(CMAKE_CUDA_STANDARD 14)` are these the mandatory requirement for latest `cuDF` version to build. ? \r\n\r\nThanks in advance for your response ! "
    },
    "satisfaction_conditions": [
      "GNU compiler must use a compatible ABI setting",
      "C++14 language standard support must be available",
      "Build system must properly communicate ABI configuration",
      "CUDA compilation must align with C++ standard requirements"
    ],
    "created_at": "2019-04-11T12:46:24Z"
  }
]