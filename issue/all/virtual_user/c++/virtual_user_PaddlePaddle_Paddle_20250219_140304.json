[
  {
    "id": "https://github.com/PaddlePaddle/Paddle/issues/25288",
    "source": {
      "issue_number": 25288
    },
    "initial_question": {
      "title": "How to skip to validate selected output tensors in python unit tests?",
      "body": "Hi,\r\n\r\nThere are operators like LRN which are having one of an outputs (MidOut) that is no use for user , but it contains intermediate data\r\nthat are speeding computation of LRN grad operator. In unit tests, (test_check_output) this MidOut is also verified if it contains valid / expected data. Problem is that what is inside MidOut is implementation specific. oneDNN LRN will produce MidOut with diffrent kind of data, because LRN oneDNN grad op is diffrent in implementation to Paddle CPU LRN grad. So unit test may fail\r\ndue to diffrent kind of data being put into MidOut. So my question is : is it possible to skip comparing MidOut tensor in Unit tests of LRN oneDNN ? If positive then how can I do that? "
    },
    "satisfaction_conditions": [
      "Selected output tensor must be excluded from validation checks",
      "Test framework must allow whitelisting of tensors to be excluded",
      "Unit tests must continue to pass despite implementation differences"
    ],
    "created_at": "2020-06-30T15:20:42Z"
  },
  {
    "id": "https://github.com/PaddlePaddle/Paddle/issues/23021",
    "source": {
      "issue_number": 23021
    },
    "initial_question": {
      "title": "\u52a8\u6001\u56fe\u4f7f\u7528fluid.dygraph.Layer.create_parameter\u62a5\u544a\u7f3a\u5c11\u53c2\u6570\u9519\u8bef",
      "body": "\u00a01\uff09PaddlePaddle\u7248\u672c\uff1a1.7.0\r\n\u52a8\u6001\u56fe\u4f7f\u7528create_parameter\u51fa\u73b0\u7f3a\u5c11\u53c2\u6570self\u9519\u8bef,\u53ef\u662f\u5b98\u65b9\u6587\u6863\u91cc\u5e76\u6ca1\u6709\u8fd9\u4e2a\u53c2\u6570\u3002\r\n\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\uff0c\u5982\u679c\u662f\u6211\u4f7f\u7528\u6709\u95ee\u9898\u8fd8\u5e0c\u671b\u80fd\u591f\u7ed9\u4e00\u4e2a\u4f8b\u5b50\uff0c\u5982\u4f55\u4f7f\u7528\u3002\r\nimport paddle.fluid as fluid\r\nwith fluid.dygraph.guard():\r\n   w=fluid.dygraph.Layer.create_parameter(dtype='float64',shape=[2,2])\r\n\u62a5\u9519\uff1a\r\nTraceback (most recent call last):\r\n  File \"14.py\", line 3, in <module>\r\n    w=fluid.dygraph.Layer.create_parameter(dtype='float64',shape=[2,2])\r\nTypeError: create_parameter() missing 1 required positional argument: 'self'\r\n"
    },
    "satisfaction_conditions": [
      "Code must be executed within a fluid.dygraph.guard() context",
      "Parameter creation must specify valid dtype and shape arguments"
    ],
    "created_at": "2020-03-16T02:27:37Z"
  },
  {
    "id": "https://github.com/PaddlePaddle/Paddle/issues/152",
    "source": {
      "issue_number": 152
    },
    "initial_question": {
      "title": "questions about sparse multiplication",
      "body": "I verified  sparse multiplication according to your codes.  I designed my demo  which is as follows:\n# include<iostream>\n# include<stdlib.h>\n\nusing namespace std;\nint main()\n{\n    void colVecAddTo(int\\* a, int\\* b, int c, int len, int aWidth, int bWidth);\n    int a[3] = {1, 1, 1};\n\n```\n                                                    //b=[0, 1, 0, 2, 0; 1, 0, 0, 0, 0; 0, 0, 0, 2, 5]  SPARSE_CSR\nint b_value[5] = {1, 2, 1, 2, 5};\nint b_col[5] = {1, 3, 0, 3, 4};\nint b_row[4] = {0, 2, 3, 5};\nint f[5] = {0, 0, 0, 0, 0};\n\nint *A = a;\nint *B = b_value;\nint *C = f;\n\nint *rows = b_row;\nint *cols = b_col;\n\nint m = 3;                    //a->getWidth()   \nint width_ = 5;               //b->getWidth()\nint height_ = 1;              //a->getHeight()\n\nfor (int j = 0 ;j < 3; ++j )   //3 is b->getHeight()\n{\n    int start = b_row[j];             //start = b->getRowStartIdx(j)\n    int end = b_row[j + 1];           //end = b->getRowStartIdx(j + 1)\n    for (int i = start; i < end; ++i)\n    {\n        colVecAddTo(C + cols[i], A + j, B[i], height_, width_, 3);\n        //cout<<*(C + cols[i])<<endl;\n    }\n}\n\nsystem(\"pause\");\nreturn 0;\n```\n\n}\n\n void colVecAddTo(int\\* aa, int\\* bb, int cc, int len, int aWidth, int bWidth) \n{\n      for ( int ii = 0; ii < len; ++ii)\n     {\n         aa[ii \\* aWidth] += bb[ii \\* bWidth] \\* cc;\n     cout<<aa[ii \\* aWidth]<<endl;\n     }\n}\nThe correct result should be 1 1 0 4 5,   but the rusult of my demo is  1 2 1 4 5  . Could you help me ?\n\nWhat's more , could you show me more details or reference materials about sparse multiplication? Thank you very much.\n"
    },
    "satisfaction_conditions": [
      "Final output array must contain correct sparse matrix multiplication results [1,1,0,4,5]",
      "Intermediate calculation results must not be mistaken for final results",
      "Sparse matrix format must be correctly interpreted"
    ],
    "created_at": "2016-09-30T09:56:02Z"
  },
  {
    "id": "https://github.com/PaddlePaddle/Paddle/issues/94",
    "source": {
      "issue_number": 94
    },
    "initial_question": {
      "title": "Install whl failed",
      "body": "I can successfully build out the paddle but while I install \"py_paddle-0.8.0b0-cp27-cp27mu-linux_x86_64.whl\", it failed and promt:\n~/workspace/Paddle/build$ sudo pip install /opt/paddle/share/wheels/*.whl\npy_paddle-0.8.0b0-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.\nStoring debug log for failure in /home/yu/.pip/pip.log\n\nHow shall I fix it or work around it?\n\nMy configuration:\ncmake -DWITH_GPU=ON -DWITH_DOC=OFF -DMKL_ROOT=/opt/intel/compilers_and_libraries_2016.3.210/linux/mkl/ -DCUDNN_ROOT=/usr/local/cuda -DWITH_SWIG_PY=ON -DCMAKE_INSTALL_PREFIX=/ ..\n\npaddle version\nPaddlePaddle 0.8.0b0, compiled with\n    with_avx: ON\n    with_gpu: ON\n    with_double: OFF\n    with_python: ON\n    with_rdma: OFF\n    with_glog: ON\n    with_gflags: ON\n    with_metric_learning: \n    with_timer: OFF\n    with_predict_sdk: \n"
    },
    "satisfaction_conditions": [
      "Wheel package format matches the system platform configuration",
      "Package manager toolchain remains functional for Python 2.7"
    ],
    "created_at": "2016-09-19T15:22:45Z"
  }
]