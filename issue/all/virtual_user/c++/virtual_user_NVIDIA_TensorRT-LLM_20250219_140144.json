[
  {
    "id": "https://github.com/NVIDIA/TensorRT-LLM/issues/1778",
    "source": {
      "issue_number": 1778
    },
    "initial_question": {
      "title": "`Parameter transformer.layers.N.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method`",
      "body": "### System Info\r\n\r\nWhile trying to debug poor quality of outputs from TRT LLM for Llama3 70b tp=4 (compared to vLLM and HF), I ran into the following message when building bfloat16 engine.\r\n\r\n```\r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network\r\n```\r\n\r\n(repeated for each layer)\r\n\r\nIs this message harmless?\r\n\r\nThe commands I run:\r\n\r\n```sh\r\npython convert_checkpoint.py \\\r\n--model_dir /workspace/llama3-70b \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--dtype bfloat16 \\\r\n--tp_size 4\r\n\r\ntrtllm-build \\\r\n--checkpoint_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4-engine \\\r\n--gpt_attention_plugin bfloat16 \\\r\n--gemm_plugin bfloat16 \\\r\n--use_custom_all_reduce disable \\\r\n--max_num_tokens 32768 \\\r\n--max_batch_size 48 \\\r\n--max_input_len 8192 \\\r\n--max_output_len 4096\r\n```\r\n\r\nThe full logs:\r\n\r\n```\r\n[TensorRT-LLM] TensorRT-LLM version: 0.11.0.dev2024060400                                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set bert_attention_plugin to auto.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set nccl_plugin to auto.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lookup_plugin to None.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lora_plugin to None.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set moe_plugin to auto.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.                                                                                                                                                                                                                                   \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha to True.                                                                                                                                                                                                                                          \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_kv_cache to True.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set remove_input_padding to True.                                                                                                                                                                                                                                  \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multi_block_mode to False.                                                                                                                                                                                                                                     \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set enable_xqa to True.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set tokens_per_block to 64.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_paged_context_fmha to False.                                                                                                                                                                                                                               \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_fp8_context_fmha to False.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multiple_profiles to False.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_state to True.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set streamingllm to False.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Specifying a `max_num_tokens` larger than 16384 is usually not recommended, we do not expect perf gain with that and too large `max_num_tokens` could possibly exceed the TensorRT tensor volume, causing runtime errors. Got `max_num_tokens` = 32768             \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.mup_width_multiplier = 1.0                                                                                                                                                                                                          \r\n[06/13/2024-13:01:15] [TRT-LLM] [I] Set dtype to bfloat16.                                                                                                                                                                                                                                             \r\n[06/13/2024-13:01:15] [TRT] [I] [MemUsageChange] Init CUDA: CPU +17, GPU +0, now: CPU 160, GPU 528 (MiB)                                                                                                                                                                                               \r\n[06/13/2024-13:01:19] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 4607, GPU 1678 (MiB)                                                                                                                                                                      \r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.\r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                              [138/782]\r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.                                                                                                                                   \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set nccl_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                         \r\n```\r\n\r\n### Who can help?\r\n\r\n@byshiue \r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n4xH100 SXM\r\n\r\n### Expected behavior\r\n\r\nUnsure, maybe the message is harmless\r\n\r\n### actual behavior\r\n\r\nN/A\r\n\r\n### additional notes\r\n\r\nN/A"
    },
    "satisfaction_conditions": [
      "Model functionality remains unaffected despite warning messages",
      "Warning messages do not indicate a functional error"
    ],
    "created_at": "2024-06-13T13:06:39Z"
  },
  {
    "id": "https://github.com/NVIDIA/TensorRT-LLM/issues/1748",
    "source": {
      "issue_number": 1748
    },
    "initial_question": {
      "title": "\u2018cudaStream_t\u2019 has not been declared when building tensorrt_llm v0.10.0",
      "body": "### System Info\n\nCPU: INTEL\r\nGPU Name: A100-SXM4-80GB\r\nTensorRT-LLM: tag v0.10.0\r\nContainer Used: No\r\nDriver Version: 535.54.03\r\nCUDA Version: 12.1\r\nOS: Ubuntu 22.04\r\nOthers: tensorrt==10.0.1.16, pytorch==2.2.0+cu121, python==3.10.12\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\npython scripts/build_wheel.py --trt_root=/usr/local/tensorrt --clean --install --cuda_architectures=\"80-real\"\n\n### Expected behavior\n\nWorks fine\n\n### actual behavior\n\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:48:37: error: \u2018cudaStream_t\u2019 has not been declared\r\n   48 | void invokeRGLRU(lruParams& params, cudaStream_t stream);\r\n      |                                     ^~~~~~~~~~~~\r\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:51:43: error: \u2018cudaStream_t\u2019 has not been declared\r\n   51 | void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);\r\n      |                                           ^~~~~~~~~~~~\n\n### additional notes\n\nNo"
    },
    "satisfaction_conditions": [
      "Build environment has compatible CUDA development tools"
    ],
    "created_at": "2024-06-06T06:09:39Z"
  },
  {
    "id": "https://github.com/NVIDIA/TensorRT-LLM/issues/1523",
    "source": {
      "issue_number": 1523
    },
    "initial_question": {
      "title": "can trtllm-build process on cpu? ",
      "body": "### System Info\n\nNVIDIA A800 40G\n\n### Who can help?\n\n@byshiue \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\ncan trtllm-build process on cpu? like parameter load_model_on_cpu in convert_checkpoint.py\n\n### Expected behavior\n\nnone\n\n### actual behavior\n\nnone\n\n### additional notes\n\nnone"
    },
    "satisfaction_conditions": [
      "Hardware environment must include NVIDIA GPU",
      "Build process cannot execute on CPU-only systems",
      "User must be informed of GPU requirement"
    ],
    "created_at": "2024-04-29T09:02:41Z"
  },
  {
    "id": "https://github.com/NVIDIA/TensorRT-LLM/issues/1410",
    "source": {
      "issue_number": 1410
    },
    "initial_question": {
      "title": "What is the meaning for the benchmark output `tokens_per_sec` and `generation_tokens_per_second`? ",
      "body": "I run benchmark like this:\r\n```\r\nmpirun -n 2 --allow-run-as-root python benchmark.py \\\r\n    -m llama_13b \\\r\n    --mode plugin \\\r\n    --batch_size \"1;8;16\" \\\r\n    --input_output_len \"710,190\" \\\r\n    --max_input_len 750 --max_output_len 200\r\n```\r\nI got this:\r\n```\r\n[BENCHMARK] model_name llama_13b world_size 2 num_heads 40 num_kv_heads 40 num_layers 40 hidden_size 5120 vocab_size 32000 precision float16 batch_size 1 input_length 710 output_length 190 gpu_peak_mem(gb) 0.0 build_time(s) 116.39 tokens_per_sec 43.09 percentile95(ms) 5120.208 percentile99(ms) 5120.208 latency(ms) 4409.816 compute_cap sm80 quantization QuantMode.0 generation_time(ms) 3751.546 total_generated_tokens 189.0 generation_tokens_per_second 50.379\r\n```\r\n\r\nI see there are two token per sec numbers, which is correct? and what is the meaning for each of them?\r\n\r\nI can't find any documentation mentioning that."
    },
    "satisfaction_conditions": [
      "Correctly explains the distinction between total inference throughput and generation-only throughput metrics",
      "Demonstrates how the metrics are calculated from the available benchmark data",
      "Explains what time periods are included in each metric's calculation",
      "Values match the mathematical calculations shown in the benchmark output"
    ],
    "created_at": "2024-04-07T07:43:18Z"
  }
]