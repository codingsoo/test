[
  {
    "id": "https://github.com/ray-project/ray/issues/37677",
    "source": {
      "issue_number": 37677
    },
    "initial_question": {
      "title": "[<Ray component: Cluster>] KeyError: 'CPU' error in Linux",
      "body": "### What happened + What you expected to happen\r\n\r\n**What I will do:**\r\nI tried to get the total number of cpus provided by the cluster;\r\n\r\n**What I got wrong:**\r\nThe specific error information is as follows:\r\n{cluster_resources()['CPU']} CPU resources in total;\r\nKeyError: 'CPU'\r\n\r\n**Update:**\r\n_I seem to have found the reason, when there is no available cpu in the cluster, the 'CPU' key is no longer in the returned dict; This leads to errors;_\r\n\r\n### Versions / Dependencies\r\n\r\nray: 2.3.1\r\nos: debian 11\r\npython: 3.9.2\r\n\r\n### Reproduction script\r\n\r\nfrom ray import init, cluster_resources\r\ninit()\r\nprint(f\"{cluster_resources()['CPU']}\")\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task."
    },
    "satisfaction_conditions": [
      "Program handles missing CPU key gracefully",
      "Returns a valid numeric value for CPU resources",
      "Maintains compatibility with Ray cluster resource queries"
    ],
    "created_at": "2023-07-22T10:49:28Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/31151",
    "source": {
      "issue_number": 31151
    },
    "initial_question": {
      "title": "ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...>] ",
      "body": "### Description\n\nHellow,i'm sorry to bother you.I want to use ray 2.0.0.dev0.But I don't know where I can find it.There is only version ray 3.0.0.dev0 in the documentation.Can you tell me where i can get it,thanks!\n\n### Link\n\n_No response_"
    },
    "satisfaction_conditions": [
      "User must be able to access a working version of Ray 2.x",
      "Clear explanation of version numbering scheme must be provided",
      "Installation method must be accessible to the user",
      "Version options must include stable releases"
    ],
    "created_at": "2022-12-16T06:43:57Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/13576",
    "source": {
      "issue_number": 13576
    },
    "initial_question": {
      "title": "[tune] Errors when using points_to_evaluate argument for a few search algos",
      "body": "I am trying out a few search algos (run from scratch) with Ray version 1.1.0, and is running into a few issues when using `points_to_evaluate` argument:\r\n\r\nSearch Space Setup:\r\n```\r\neven_int_model_dim = [x for x in range(1, 12+1) if x % 2 == 0]\r\neven_int_batch_size = [x for x in range(1, 16+1) if x % 2 == 0]\r\nint_sequence = [x for x in range(20, 100+1)]\r\nint_local_context_len = [x for x in range(3, 15+1)]\r\nint_num_heads = [x for x in range(2, 6+1)]\r\n\r\nconfig={'seed': 0, 'train_start_date': 'None', 'valid_start_date': '2017-01-01', 'test_start_date': '2018-01-01',\r\n\t'data_path': 'path/to/data', 'num_epoch': 100, 'loss_fn': loss_fn, 'device': 'cuda:0', \r\n\t'sequence': tune.choice(int_sequence),\r\n\t'local_context_len': tune.choice(int_local_context_len), 'batch_size': tune.choice(even_int_batch_size), 'num_heads': tune.choice(int_num_heads),\r\n\t'model_dim': tune.choice(even_int_model_dim), 'num_layers': tune.choice([1, 2]), 'dropout': tune.uniform(0, 0.7),\r\n\t'allocator': 'numark', 'max_weight': 0.1, 'stochasticity': tune.choice([True, False]),\r\n\t'resample': False, 'n_draws': 100, 'n_portfolios': 5, 'feature_dims': 0, \r\n\t'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, \r\n\t'eps': 0.00000001, 'weight_decay': 0, 'amsgrad': True}\r\n\r\ncurrent_best = [{'sequence': 100, 'local_context_len': 5, 'batch_size': 16, \r\n\t\t 'num_heads': 6, 'model_dim': 12, 'num_layers': 1, 'dropout': 0.01, 'stochasticity': False}]\r\n```\r\n\r\nWhen using `HyperOpt`:\r\n`algo = HyperOptSearch(points_to_evaluate=current_best)`\r\nError:\r\n> File \"/home/user/anaconda3/envs/user/lib/python3.7/site-packages/hyperopt/pyll/base.py\", line 874, in rec_eval\r\n>     rval_var = node.pos_args[int(switch_i) + 1]\r\n> IndexError: list index out of range\r\n\r\nWhen using `Optuna`:\r\n`algo = OptunaSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'\r\n\r\nWhen using `Ax`:\r\n`algo = AxSearch(points_to_evaluate=current_best)`\r\nError:\r\n> TypeError: __init__() got an unexpected keyword argument 'points_to_evaluate'"
    },
    "satisfaction_conditions": [
      "Initial points (current_best) must be compatible with the search space definition",
      "All required hyperparameters must remain accessible for optimization"
    ],
    "created_at": "2021-01-20T05:28:07Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/13517",
    "source": {
      "issue_number": 13517
    },
    "initial_question": {
      "title": "[tune] how to enforce even integer number in the search space",
      "body": "I require my `batch_size` in the search space to be an **even number**, so I tried `tune.qrandint(4, 64, 2)`. However I am also using `HyperOpt` which gave this warning:\r\n\r\n> HyperOpt does not support quantization for integer values. Reverting back to 'randint'.\r\n\r\nMaking certain trials contain odd `batch_size` which produces error in my model. Is there another way to enforce even integer number in the search space?\r\n\r\nEDIT:\r\n\r\nI also tried `tune.sample_from(lambda spec: np.random.randint(2, 64) * 2)`, and HyperOpt gave error:\r\n\r\n> HyperOpt does not support parameters of type `Function` with samplers of type `NoneType`"
    },
    "satisfaction_conditions": [
      "Search space must only generate even integers within the specified range",
      "Solution must be compatible with HyperOpt tuning framework",
      "Generated values must maintain randomness within the constrained space",
      "Search range bounds must be accurately respected"
    ],
    "created_at": "2021-01-18T08:18:39Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/11971",
    "source": {
      "issue_number": 11971
    },
    "initial_question": {
      "title": "[rllib] PPO ICM learning rate",
      "body": "Hello, I know the default ppo learning rate is 5e-5, default curiosity learning rate is 0.001. \r\nI just want to know whether the two learning rate are same?   \r\n\r\nIf I use curiosity in ppotrainer, how do I set it?\r\nThank you!"
    },
    "satisfaction_conditions": [
      "Learning rates for PPO and curiosity module must be configurable independently",
      "Curiosity learning rate must be specifiable within exploration configuration",
      "Configuration structure must maintain proper hierarchy for exploration settings",
      "Exploration configuration must support all necessary curiosity parameters"
    ],
    "created_at": "2020-11-12T13:05:46Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/10312",
    "source": {
      "issue_number": 10312
    },
    "initial_question": {
      "title": "[ray] How to startup workers more than number of cores",
      "body": "How to set ray startup arguments to let 150 workers running on a 96 cores machine? I notice ray will auto-scale on the local machine, but how to set while running a cluster?"
    },
    "satisfaction_conditions": [
      "System successfully allows more workers than available CPU cores"
    ],
    "created_at": "2020-08-25T14:55:28Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/8622",
    "source": {
      "issue_number": 8622
    },
    "initial_question": {
      "title": "Restoring from checkpoint on different machine",
      "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI trained a model on one machine and I am able to load and restore properly from a saved checkpoint on that machine. When I copied over the checkpoint and try to restore and execute on a different machine, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_rllib_model.py\", line 77, in <module>\r\n    test_agent.restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/tune/trainable.py\", line 417, in restore\r\n    self._restore(checkpoint_path)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 639, in _restore\r\n    self.__setstate__(extra_data)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 192, in __setstate__\r\n    Trainer.__setstate__(self, state)\r\n  File \"/home/mlab/rl_dev/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 1070, in __setstate__\r\n    self.optimizer.restore(state[\"optimizer\"])\r\nAttributeError: 'NoneType' object has no attribute 'restore'\r\n```\r\nAm I missing some files or something? I've copied over all the files in the checkpoint directory and also the `params.pkl` and `params.json` file. \r\nI'm creating a trainer instance and restoring from the checkpoint. \r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\n"
    },
    "satisfaction_conditions": [
      "All required checkpoint files are present on target machine"
    ],
    "created_at": "2020-05-26T17:23:53Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/8545",
    "source": {
      "issue_number": 8545
    },
    "initial_question": {
      "title": "[ray] Is it bad practice to use sockets (pyzmq) to communicate between ray remote functions?",
      "body": "I have a `send()` function that generates random numpy arrays at every time step, and a `recv()` function that receives and prints those generated arrays. I am using `zmq` for sending/receiving the numpy arrays across the processes, and `pyarrow` to serialize and deserialize arrays. I wasn't able to find any examples using ray and zmq together, so I would like to know whether this is bad practice. If so, is there a recommended way to have the distributed-ly running processes communicate with each other using ray?\r\n\r\nThank you so much! \r\n\r\nPasted below is minimal working code (on Ubuntu 18.0.4, python=3.6.9, pyzmq=19.0.1, ray=0.8.5, pyarrow=0.17.1):\r\n\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport ray\r\nimport zmq\r\nray.init()\r\n\r\n\r\n@ray.remote\r\ndef send():\r\n    port = 5556\r\n    context = zmq.Context()\r\n    send_socket = context.socket(zmq.PUSH)\r\n    send_socket.bind(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        msg = np.random.rand(1, 3) # this could be larger, e.g. numpy-ed torch neural network weights\r\n        object_id = pa.serialize(msg).to_buffer()\r\n        send_socket.send(object_id)\r\n\r\n@ray.remote\r\ndef recv():        \r\n    port = 5556\r\n    context = zmq.Context()\r\n    recv_socket = context.socket(zmq.PULL)\r\n    recv_socket.connect(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        object_id = recv_socket.recv()\r\n        msg = pa.deserialize(object_id)\r\n        print(msg)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.wait([send.remote(), recv.remote()])\r\n```\r\n## Note:\r\nI had to use pyarrow for serialization since ray object id's (obtained via `ray.put()`) could not be passed through zmq sockets; doing so gives the error below:  \r\n```\r\nObjectID(45b95b1c8bd3a9c4ffffffff0100008801000000) does not provide a buffer interface.\r\n```"
    },
    "satisfaction_conditions": [
      "Reliable data transfer between distributed processes",
      "Proper serialization and deserialization of numpy arrays",
      "Concurrent execution of sender and receiver processes",
      "Thread-safe data handling in shared storage"
    ],
    "created_at": "2020-05-22T06:17:38Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/8413",
    "source": {
      "issue_number": 8413
    },
    "initial_question": {
      "title": "[sgd] Can TorchTrainer print out something every one or several iterations?",
      "body": "Seems by default TorchTrainer only returns stats after train() finishes? During the training, is there a way I get some information (for example loss values, or just something to indicate the training is happening in the background?) for each iteration or every several iterations?\r\nOtherwise if one epoch training takes a lot of time, then I probably don't know what's going on. I may doubt whether the program crashes indeed or the training is just long.\r\n\r\n@richardliaw \r\n"
    },
    "satisfaction_conditions": [
      "Training progress must be visually observable during execution",
      "Progress updates must occur at regular intervals during training",
      "Progress information must be accessible without interrupting the training process",
      "Training status indicators must be clear enough to distinguish between active training and program failure"
    ],
    "created_at": "2020-05-12T08:36:16Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/7912",
    "source": {
      "issue_number": 7912
    },
    "initial_question": {
      "title": "Details about the hyperparameter in PPO Algorithm?",
      "body": "Hi, so I want to tune my hyperparameter for the PPO Algorithm but I've found difficulties when reading the docs about the configs, so I guess I want to ask you guys in here about:\r\n1. What is the value of `lr_schedule` in the PPO Algorithm? Suppose that my starting learning_rate is `'lr': 1e-4` and I want to decay its value to 0 when I train.\r\n2. Is it possible to set the hidden layer size in the PPO algorithm? If yes, what is the corresponding config as I didn't find it in the documentation (I found this kind of config in the SAC algorithm documentation but not in PPO).\r\n\r\nThank you very much guys! I really appreciate your help \ud83d\ude04 "
    },
    "satisfaction_conditions": [
      "Learning rate schedule configuration must support decay from initial to final value over time",
      "Learning rate schedule must support specification of multiple time points with corresponding values",
      "Neural network architecture configuration must allow customization of hidden layer sizes",
      "Neural network activation function must be configurable",
      "Configuration parameters must be accessible through the algorithm's configuration interface"
    ],
    "created_at": "2020-04-06T14:27:08Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/7737",
    "source": {
      "issue_number": 7737
    },
    "initial_question": {
      "title": "Why actor methods cannot be called directly?",
      "body": "When calling a actor method, that is, call the `__call__` method of an `ActorMethod` object. And this method is implemented as raise an `Exception` directly\r\n```\r\nException: Actor methods cannot be called directly. Instead of running 'object.get()', try 'object.get.remote()'\r\n```\r\n\r\nBut why is it necessary? Why it can't be\r\n\r\n```python\r\nclass ActorMethod:\r\n    ...\r\n    def __call__(self, *args, **kwargs):\r\n        return ray.get(self._remote(args, kwargs))\r\n    ...\r\n```\r\n\r\nThen in some case, If do the following:\r\n```python\r\nclass Foo(object):\r\n    def foo(self):\r\n        return \"foo\"\r\n\r\nclass Bar(object):\r\n    def bar(self, foo_obj):\r\n        return foo_obj.foo()\r\n \r\nRayFoo = ray.remote(Foo)\r\nRayBar = ray.remote(Bar)\r\n\r\nif __name__ == \"__main__\":\r\n    f = Foo()\r\n    b = Bar()\r\n    print(b.bar(f))\r\n\r\n    ray.init(log_to_driver=False)\r\n    rf = RayFoo.remote()\r\n    rb = RayBar.remote()\r\n    print(rb.bar(rf))\r\n```\r\nwith the original `__call__` implementation, this is not possible, but with the proposed one, this works perfectly.\r\n\r\nIs there any design consideration?\r\n"
    },
    "satisfaction_conditions": [
      "API consistency is maintained across all Ray operations (tasks, methods, class invocations)",
      "Clear distinction exists between local and distributed method calls",
      "Error prevention mechanism exists for incorrect usage patterns",
      "Support for interoperability between local and Ray-decorated types is possible"
    ],
    "created_at": "2020-03-25T03:57:27Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/7467",
    "source": {
      "issue_number": 7467
    },
    "initial_question": {
      "title": "[tune][rllib] _InactiveRpcError Deadline Exceeded",
      "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have VirtualBox running on Centos 7 and I am having trouble initializing Ray. After I run ray.init(), I get an _InactiveRpcError due to a Deadline Exceeded exception. What info should I provide in order to troubleshoot this error?\r\n\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray 0.8.2\r\nredis 3.4.1\r\nPython 3.6\r\nCentos 7 on VirtualBox"
    },
    "satisfaction_conditions": [
      "Ray initialization completes without RPC errors",
      "Ray version compatibility issues are resolved"
    ],
    "created_at": "2020-03-05T16:32:58Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/7424",
    "source": {
      "issue_number": 7424
    },
    "initial_question": {
      "title": "Actor method arguments",
      "body": "Why do actor methods do not support passing arguments? There is an assertion that fails if the actor method function arguments are larger than 0.\r\n"
    },
    "satisfaction_conditions": [
      "Actor methods must successfully accept input parameters",
      "Actor class must be properly decorated for remote execution",
      "Multiple actor instances can process tasks in parallel",
      "Remote method calls return retrievable results",
      "Actor state persists between method calls"
    ],
    "created_at": "2020-03-03T19:27:19Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/7394",
    "source": {
      "issue_number": 7394
    },
    "initial_question": {
      "title": "Does DQN \"rollout.py\" have exploration turned off?",
      "body": "When I call \"rollout.py\" I am not sure if exploration is turned off or not. I've looked over the file and can't seem to find `explore=False` anywhere.\r\n\r\nSo, when we evaluate trained policy (e.g. DQN) with rollout script - does it actually turn off random actions or not?\r\n\r\nThanks."
    },
    "satisfaction_conditions": [
      "Exploration settings must be verifiable by the user",
      "Evaluation behavior must be consistent with training phase completion",
      "Exploration configuration must be controllable"
    ],
    "created_at": "2020-03-02T03:57:53Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/7194",
    "source": {
      "issue_number": 7194
    },
    "initial_question": {
      "title": "[rllib]PPO with action branching?",
      "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### PPO with action branching?\r\nRay version: 0.8.0\r\nTensorflow Version: 1.14.0\r\nOS: Ubuntu 18.04\r\n\r\nI'm currently working on training action branching agents with PPO. What else do I need to do besides set the action space to something like `gym.spaces.Tuple([gym.spaces.Discrete(3), gym.spaces.Discrete(5)])`, or I need to write a custom loss function? I was wondering if the gradients would be correct. "
    },
    "satisfaction_conditions": [
      "Action space configuration correctly handles multiple discrete actions",
      "PPO algorithm functions with the configured action space"
    ],
    "created_at": "2020-02-17T13:42:11Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/6986",
    "source": {
      "issue_number": 6986
    },
    "initial_question": {
      "title": "[Question][rllib] Stochastic Game tensorboard separate rewards",
      "body": "### What is your question?\r\n\r\nI am designing a simple stochastic game wherein I have two agents. The first agent (the good guy) is rewarded according to some task. The second agent (the adversary) is rewarded negative proportional to the first. This is to encourage the adversary to screw up the good guy.\r\n\r\nAs a first pass, I just set the reward of the adversary equal to negative the reward of the good guy. This seems to cause some issue with tensorboard, however, because it looks like the rewards are summed together, which results is a reward of 0 for each iteration.\r\n\r\nIt would be nice to be able to visualize the rewards of each agent individually. I imagine that this would be very useful for other MARL scenarios, not just SG. Is this something that is possible?\r\n\r\nThank you!\r\n\r\npython3.7\r\ntensorflow2.1\r\nray0.8.1\r\nmac10.14\r\n\r\n"
    },
    "satisfaction_conditions": [
      "Individual agent rewards must be separately observable in monitoring tools",
      "Reward tracking must work with multi-agent reinforcement learning (MARL) scenarios"
    ],
    "created_at": "2020-01-31T07:44:54Z"
  },
  {
    "id": "https://github.com/ray-project/ray/issues/3173",
    "source": {
      "issue_number": 3173
    },
    "initial_question": {
      "title": "Issue running train.py can not locate lz4 or GPU",
      "body": "When running the following (it does run succesfully but I get errors in setup on the gpu and lz4)\r\nRay does not find lz4 or my gpu\r\n.\r\n\r\n```\r\n===============================================================================================================================\r\n**********  stats  ************************\r\n\r\n\r\nUbuntu 16.04\r\n\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\n\r\n\r\n>>import ray; print(ray.__version__)\r\n0.5.3\r\n\r\n\r\n>>pip install lz4\r\nRequirement already satisfied: lz4 in /usr/lib/python2.7/dist-packages (0.7.0)\r\n\r\n\r\n>>lrwxrwxrwx  1 root root    10 Oct  1 22:50 cuda -> ./cuda-9.2\r\n\r\n>>nvidia-smi\r\n\r\nWed Oct 31 15:13:34 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1060    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   57C    P0    28W /  N/A |    365MiB /  6069MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1056      G   /usr/lib/xorg/Xorg                           262MiB |\r\n|    0      2508      G   compiz                                         7MiB |\r\n|    0      6924      G   ...uest-channel-token=14148272352303891226    92MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n=============================================================================================================\r\n\r\n\r\n\r\nsudo python /home/rjn/.local/lib/python2.7/site-packages/ray/rllib/train.py --env=Pong-ram-v4 --run=IMPALA \r\n\r\n\r\nProcess STDOUT and STDERR is being redirected to /tmp/raylogs/.\r\nWaiting for redis server at 127.0.0.1:27991 to respond...\r\nWaiting for redis server at 127.0.0.1:19620 to respond...\r\nStarting the Plasma object store with 6.00 GB memory.\r\nStarting local scheduler with the following resources: {'GPU': 1, 'CPU': 8}.\r\nFailed to start the UI, you may need to run 'pip install jupyter'.\r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\n\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nCreated LogSyncer for /home/rjn/ray_results/default/IMPALA_Pong-ram-v4_0_2018-10-31_15-27-354nWfrf -> \r\n== Status ==\r\nUsing FIFO scheduling algorithm.\r\nResources requested: 3/8 CPUs, 1/1 GPUs\r\nResult logdir: /home/rjn/ray_results/default\r\nRUNNING trials:\r\n - IMPALA_Pong-ram-v4_0:\tRUNNING\r\n\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:36.476907: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:36.551185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-31 15:27:36.551570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.50GiB\r\n2018-10-31 15:27:36.551584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-10-31 15:27:36.750449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-31 15:27:36.750495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-10-31 15:27:36.750501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-10-31 15:27:36.750697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5263 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\n2018-10-31 15:27:38.726462: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.728976: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.729023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729030: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.729057: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.729080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.729086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\n2018-10-31 15:27:38.747833: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-10-31 15:27:38.750367: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2018-10-31 15:27:38.750429: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750454: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: rjn-Oryx-Pro\r\n2018-10-31 15:27:38.750518: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.44.0\r\n2018-10-31 15:27:38.750584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.44.0\r\n2018-10-31 15:27:38.750590: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.44.0\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sample_batch.SampleBatch'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\r\nWARNING: Serializing objects of type <class 'ray.rllib.evaluation.sampler.RolloutMetrics'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\r\nResult for IMPALA_Pong-ram-v4_0:\r\n  date: 2018-10-31_15-27-47\r\n  done: false\r\n  episode_len_mean: 1290.9\r\n  episode_reward_max: -19.0\r\n  episode_reward_mean: -20.2\r\n  episode_reward_min: -21.0\r\n  episodes: 10\r\n  experiment_id: 16a7956cbdbf4bc78ef6f280ddc93142\r\n  hostname: rjn-Oryx-Pro\r\n  info:\r\n    learner:\r\n      cur_lr: 0.0005000000237487257\r\n      entropy: 862.072021484375\r\n      grad_gnorm: 40.0\r\n      policy_loss: -53.67061996459961\r\n      var_gnorm: 22.6600399017334\r\n      vf_explained_var: 0.07421219348907471\r\n      vf_loss: 30.673662185668945\r\n    num_steps_sampled: 14750\r\n    num_steps_trained: 14500\r\n    num_weight_syncs: 295\r\n    sample_throughput: 2048.296\r\n    train_throughput: 4096.592\r\n  iterations_since_restore: 1\r\n  node_ip: 192.168.1.100\r\n  pid: 7915\r\n  policy_reward_mean: {}\r\n  time_since_restore: 10.099857091903687\r\n  time_this_iter_s: 10.099857091903687\r\n  time_total_s: 10.099857091903687\r\n  timestamp: 1541014067\r\n  timesteps_since_restore: 14750\r\n  timesteps_this_iter: 14750\r\n  timesteps_total: 14750\r\n  training_iteration: 1\r\n\r\n\r\n\r\n```"
    },
    "satisfaction_conditions": [
      "The main training process must successfully utilize the GPU",
      "The training script must execute successfully despite lz4 warnings",
      "Worker processes must function properly even when running on CPU",
      "The training metrics must be generated and displayed"
    ],
    "created_at": "2018-10-31T19:32:10Z"
  }
]