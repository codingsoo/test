[
  {
    "number": 15673,
    "title": "PulsarAdmin Java library: sometimes it fails with NullPointerException",
    "created_at": "2022-05-19T11:57:35Z",
    "closed_at": "2024-02-25T08:35:10Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/admin",
      "good first issue"
    ],
    "url": "https://github.com/apache/pulsar/issues/15673",
    "body": "**Describe the bug**\r\nSometimes I see this error using the PulsarAdmin API, using Pulsar 2.10.x\r\n\r\n> java.lang.NullPointerException\r\n> 2022-05-19T11:19:42.2536607Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation.translate(JerseyInvocation.java:740)\r\n> 2022-05-19T11:19:42.2537320Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation.lambda$invoke$1(JerseyInvocation.java:675)\r\n> 2022-05-19T11:19:42.2538025Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation.call(JerseyInvocation.java:697)\r\n> 2022-05-19T11:19:42.2538746Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation.lambda$runInScope$3(JerseyInvocation.java:691)\r\n> 2022-05-19T11:19:42.2539410Z \tat org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\n> 2022-05-19T11:19:42.2540015Z \tat org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\n> 2022-05-19T11:19:42.2540997Z \tat org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:205)\r\n> 2022-05-19T11:19:42.2541706Z \tat org.apache.pulsar.shade.org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:390)\r\n> 2022-05-19T11:19:42.2542454Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation.runInScope(JerseyInvocation.java:691)\r\n> 2022-05-19T11:19:42.2543170Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation.invoke(JerseyInvocation.java:674)\r\n> 2022-05-19T11:19:42.2543870Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation$Builder.method(JerseyInvocation.java:450)\r\n> 2022-05-19T11:19:42.2544551Z \tat org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation$Builder.put(JerseyInvocation.java:334)\r\n\r\n**To Reproduce**\r\nI can't reproduce consistently, it happens more often is the server is unreachable\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/15673/comments",
    "author": "eolivelli",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-29T02:14:17Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "eolivelli",
        "created_at": "2022-06-30T13:07:50Z",
        "body": "@mattisonchao would you be interested in taking a look ?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-31T02:15:21Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "VinnuReddy18",
        "created_at": "2024-02-24T07:02:54Z",
        "body": "can i work on this issue? @eolivelli  @tisonkun "
      },
      {
        "user": "dao-jun",
        "created_at": "2024-02-24T09:39:30Z",
        "body": "> can i work on this issue? @eolivelli @tisonkun\r\n\r\nGreat!"
      },
      {
        "user": "VinnuReddy18",
        "created_at": "2024-02-24T14:01:56Z",
        "body": "How can i reproduce the error ? Can Anyone help me @dao-jun "
      },
      {
        "user": "eolivelli",
        "created_at": "2024-02-25T08:35:10Z",
        "body": "Hello,\r\nIt passed a lot of time, I don't know how to reproduce the error with the latest version.\r\n\r\nI am leaning towards closing this issue with 'Obsolete'"
      }
    ]
  },
  {
    "number": 14581,
    "title": "Provide the delayed message publish rate stats",
    "created_at": "2022-03-07T14:45:07Z",
    "closed_at": "2022-03-10T12:08:21Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/14581",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\nCurrently, the producer stats contains the following information\r\n\r\n```\r\n{\r\n    \"accessMode\" : \"Shared\",\r\n    \"msgRateIn\" : 0.016669007231704083,\r\n    \"msgThroughputIn\" : 11.384931939253889,\r\n    \"averageMsgSize\" : 683.0,\r\n    \"chunkedMessageRate\" : 0.0,\r\n    \"producerId\" : 2,\r\n    \"metadata\" : { },\r\n    \"address\" : \"/xxxx\",\r\n    \"connectedSince\" : \"2022-03-07T14:32:57.52849Z\",\r\n    \"producerName\" : \"xxx\"\r\n  }\r\n```\r\n\r\nFrom the stats, we are not able to know whether the problem\r\nis sending delayed messages or not, it will be useful for some cases\r\nto troubleshoot problems.\r\n\r\n**Describe the solution you'd like**\r\nAdd \"delayedMsgRateIn\" in the producer stats\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/14581/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "shibd",
        "created_at": "2022-03-07T15:03:05Z",
        "body": "@codelipenghui  I want to try it."
      },
      {
        "user": "shibd",
        "created_at": "2022-03-09T09:30:08Z",
        "body": "@codelipenghui \r\nThe delays is placed in the `MessageMetadata`. This feature will make the broker additional a deserialization `MessageMetaData` when receiving the producer message. Can we accept the performance impact?"
      },
      {
        "user": "shibd",
        "created_at": "2022-03-10T04:02:28Z",
        "body": "> @codelipenghui\r\nThe delays is placed in the MessageMetadata. This feature will make the broker additional a deserialization MessageMetaData when receiving the producer message. Can we accept the performance impact?\r\n\r\nOr, We need add new field `hasDeliver` to `CommandSend`, This can avoid  deserialization  the data of `MessageMetadata`. This is an API protocol change,  We need to discuss first.\r\n\r\n@codelipenghui @Technoboy-  PTAL"
      },
      {
        "user": "Technoboy-",
        "created_at": "2022-03-10T07:10:03Z",
        "body": "> > @codelipenghui\r\n> > The delays is placed in the MessageMetadata. This feature will make the broker additional a deserialization MessageMetaData when receiving the producer message. Can we accept the performance impact?\r\n> \r\n> Or, We need add new field `hasDeliver` to `CommandSend`, This can avoid deserialization the data of `MessageMetadata`. This is an API protocol change, We need to discuss first.\r\n> \r\n> @codelipenghui @Technoboy- PTAL\r\n\r\nI will help to check first and then decide to send out a discussion email."
      },
      {
        "user": "Technoboy-",
        "created_at": "2022-03-10T12:08:21Z",
        "body": "> > > @codelipenghui\r\n> > > The delays is placed in the MessageMetadata. This feature will make the broker additional a deserialization MessageMetaData when receiving the producer message. Can we accept the performance impact?\r\n> > \r\n> > \r\n> > Or, We need add new field `hasDeliver` to `CommandSend`, This can avoid deserialization the data of `MessageMetadata`. This is an API protocol change, We need to discuss first.\r\n> > @codelipenghui @Technoboy- PTAL\r\n> \r\n> I will help to check first and then decide to send out a discussion email.\r\n\r\nYes, you're right. If add this field in the `CommandSend`, it is duplicated with `MsgMetadata`, and deserialization is much heavier, so after discussion, we decide to close this issue now. "
      }
    ]
  },
  {
    "number": 14471,
    "title": "What is the advertage of InputStream for MultipartFile   ？",
    "created_at": "2022-02-25T10:23:45Z",
    "closed_at": "2022-12-09T14:41:02Z",
    "labels": [
      "help wanted",
      "deprecated/question",
      "lifecycle/stale",
      "Stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/14471",
    "body": "\r\n\r\nI do not know that why package upload param ‘uploadedInputStream' use InputStream as the type 。Usually we use MultipartFile  as type of upload file  param's type。I want to know the advantages of InputStream .\r\n\r\n\r\n`\r\n    @Consumes(MediaType.MULTIPART_FORM_DATA)\r\n    public void upload(\r\n        final @PathParam(\"type\") String type,\r\n        final @PathParam(\"tenant\") String tenant,\r\n        final @PathParam(\"namespace\") String namespace,\r\n        final @PathParam(\"packageName\") String packageName,\r\n        final @PathParam(\"version\") String version,\r\n        final @FormDataParam(\"metadata\") PackageMetadata packageMetadata,\r\n        final @FormDataParam(\"file\") InputStream uploadedInputStream,\r\n`\r\n`    final @FormDataParam(\"file\") InputStream uploadedInputStream,`\r\n\r\nIf this param use MultipartFile   as the type I can test success ,but use InputStream  I can not test success by postman.\r\n\r\nCan you tell me what can I do to test the rest api success？\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/14471/comments",
    "author": "JackrayWang",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-15T02:13:57Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-28T02:10:14Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      }
    ]
  },
  {
    "number": 14399,
    "title": "Flaky-test:  PulsarSourceE2ETest.setUp",
    "created_at": "2022-02-21T09:12:43Z",
    "closed_at": "2023-05-21T02:56:29Z",
    "labels": [
      "help wanted",
      "area/test",
      "type/flaky-tests",
      "lifecycle/stale",
      "Stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/14399",
    "body": "\r\nPulsarSourceE2ETest.setUp is flaky. It fails at code:\r\n```java\r\n        Optional<WorkerService> functionWorkerService = Optional.of(functionsWorkerService);\r\n        pulsar = new PulsarService(config, workerConfig, functionWorkerService, (exitCode) -> {});\r\n        pulsar.start(); // throw exception here\r\n```\r\ni guess that the failure may be is related to the machine environment, but i can not find the reason.\r\n\r\n```\r\n[INFO] Running org.apache.pulsar.io.PulsarBatchSourceE2ETest\r\n[ERROR] Tests run: 6, Failures: 3, Errors: 0, Skipped: 3, Time elapsed: 25.856 s <<< FAILURE! - in org.apache.pulsar.io.PulsarSourceE2ETest\r\n[ERROR] setup(org.apache.pulsar.io.PulsarSourceE2ETest)  Time elapsed: 9.864 s  <<< FAILURE!\r\norg.apache.pulsar.broker.PulsarServerException: java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException$ServerSideErrorException: HTTP 500 Internal Server Error\r\n    at org.apache.pulsar.broker.PulsarService.start(PulsarService.java:827)\r\n    at org.apache.pulsar.io.AbstractPulsarE2ETest.setup(AbstractPulsarE2ETest.java:168)\r\n...\r\n\r\nCaused by: java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException$ServerSideErrorException: HTTP 500 Internal Server Error\r\n\tat org.apache.pulsar.functions.worker.PulsarWorkerService.start(PulsarWorkerService.java:571)\r\n\tat org.apache.pulsar.broker.PulsarService.startWorkerService(PulsarService.java:1511)\r\n\tat org.apache.pulsar.broker.PulsarService.start(PulsarService.java:799)\r\n...\r\n\r\nCaused by: org.apache.pulsar.client.admin.PulsarAdminException$ServerSideErrorException: HTTP 500 Internal Server Error\r\n\tat org.apache.pulsar.client.admin.internal.BaseResource.getApiException(BaseResource.java:219)\r\n\tat org.apache.pulsar.client.admin.internal.BaseResource$1.failed(BaseResource.java:130)\r\n\tat org.glassfish.jersey.client.JerseyInvocation$1.failed(JerseyInvocation.java:882)\r\n\tat org.glassfish.jersey.client.JerseyInvocation$1.completed(JerseyInvocation.java:863)\r\n\tat org.glassfish.jersey.client.ClientRuntime.processResponse(ClientRuntime.java:229)\r\n\tat org.glassfish.jersey.client.ClientRuntime.access$200(ClientRuntime.java:62)\r\n\tat org.glassfish.jersey.client.ClientRuntime$2.lambda$response$0(ClientRuntime.java:173)\r\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\r\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\r\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:288)\r\n\tat org.glassfish.jersey.client.ClientRuntime$2.response(ClientRuntime.java:173)\r\n\tat org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$apply$1(AsyncHttpConnector.java:212)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\r\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)\r\n\tat org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:254)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\r\n\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)\r\n...\r\n```\r\n\r\n\r\n<details>\r\n<summary>Full exception stacktrace</summary>\r\n<code><pre>\r\n\r\n[INFO] Running org.apache.pulsar.io.PulsarBatchSourceE2ETest\r\n[ERROR] Tests run: 6, Failures: 3, Errors: 0, Skipped: 3, Time elapsed: 25.856 s <<< FAILURE! - in org.apache.pulsar.io.PulsarSourceE2ETest\r\n[ERROR] setup(org.apache.pulsar.io.PulsarSourceE2ETest)  Time elapsed: 9.864 s  <<< FAILURE!\r\norg.apache.pulsar.broker.PulsarServerException: java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException$ServerSideErrorException: HTTP 500 Internal Server Error\r\n    at org.apache.pulsar.broker.PulsarService.start(PulsarService.java:827)\r\n    at org.apache.pulsar.io.AbstractPulsarE2ETest.setup(AbstractPulsarE2ETest.java:168)\r\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n    at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:132)\r\n    at org.testng.internal.MethodInvocationHelper.invokeMethodConsideringTimeout(MethodInvocationHelper.java:61)\r\n    at org.testng.internal.ConfigInvoker.invokeConfigurationMethod(ConfigInvoker.java:366)\r\n    at org.testng.internal.ConfigInvoker.invokeConfigurations(ConfigInvoker.java:320)\r\n    at org.testng.internal.TestInvoker.runConfigMethods(TestInvoker.java:701)\r\n    at org.testng.internal.TestInvoker.invokeMethod(TestInvoker.java:527)\r\n    at org.testng.internal.TestInvoker.invokeTestMethod(TestInvoker.java:174)\r\n    at org.testng.internal.MethodRunner.runInSequence(MethodRunner.java:46)\r\n    at org.testng.internal.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:822)\r\n    at org.testng.internal.TestInvoker.invokeTestMethods(TestInvoker.java:147)\r\n    at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n    at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:128)\r\n    at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)\r\n    at org.testng.TestRunner.privateRun(TestRunner.java:764)\r\n    at org.testng.TestRunner.run(TestRunner.java:585)\r\n    at org.testng.SuiteRunner.runTest(SuiteRunner.java:384)\r\n    at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:378)\r\n    at org.testng.SuiteRunner.privateRun(SuiteRunner.java:337)\r\n    at org.testng.SuiteRunner.run(SuiteRunner.java:286)\r\n    at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:53)\r\n    at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:96)\r\n    at org.testng.TestNG.runSuitesSequentially(TestNG.java:1218)\r\n    at org.testng.TestNG.runSuitesLocally(TestNG.java:1140)\r\n    at org.testng.TestNG.runSuites(TestNG.java:1069)\r\n    at org.testng.TestNG.run(TestNG.java:1037)\r\n    at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:135)\r\n    at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeSingleClass(TestNGDirectoryTestSuite.java:112)\r\n    at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeLazy(TestNGDirectoryTestSuite.java:123)\r\n    at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:90)\r\n    at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:146)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\nCaused by: java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException$ServerSideErrorException: HTTP 500 Internal Server Error\r\n    at org.apache.pulsar.functions.worker.PulsarWorkerService.start(PulsarWorkerService.java:571)\r\n    at org.apache.pulsar.broker.PulsarService.startWorkerService(PulsarService.java:1511)\r\n    at org.apache.pulsar.broker.PulsarService.start(PulsarService.java:799)\r\n    ... 39 more\r\nCaused by: org.apache.pulsar.client.admin.PulsarAdminException$ServerSideErrorException: HTTP 500 Internal Server Error\r\n    at org.apache.pulsar.client.admin.internal.BaseResource.getApiException(BaseResource.java:219)\r\n    at org.apache.pulsar.client.admin.internal.BaseResource$1.failed(BaseResource.java:130)\r\n    at org.glassfish.jersey.client.JerseyInvocation$1.failed(JerseyInvocation.java:882)\r\n    at org.glassfish.jersey.client.JerseyInvocation$1.completed(JerseyInvocation.java:863)\r\n    at org.glassfish.jersey.client.ClientRuntime.processResponse(ClientRuntime.java:229)\r\n    at org.glassfish.jersey.client.ClientRuntime.access$200(ClientRuntime.java:62)\r\n    at org.glassfish.jersey.client.ClientRuntime$2.lambda$response$0(ClientRuntime.java:173)\r\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\r\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\r\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:244)\r\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:288)\r\n    at org.glassfish.jersey.client.ClientRuntime$2.response(ClientRuntime.java:173)\r\n    at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$apply$1(AsyncHttpConnector.java:212)\r\n    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\r\n    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\r\n    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\r\n    at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)\r\n    at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:254)\r\n    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\r\n    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\r\n    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\r\n    at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)\r\n    at org.asynchttpclient.netty.NettyResponseFuture.loadContent(NettyResponseFuture.java:222)\r\n    at org.asynchttpclient.netty.NettyResponseFuture.done(NettyResponseFuture.java:257)\r\n    at org.asynchttpclient.netty.handler.AsyncHttpClientHandler.finishUpdate(AsyncHttpClientHandler.java:241)\r\n    at org.asynchttpclient.netty.handler.HttpHandler.handleChunk(HttpHandler.java:114)\r\n    at org.asynchttpclient.netty.handler.HttpHandler.handleRead(HttpHandler.java:143)\r\n    at org.asynchttpclient.netty.handler.AsyncHttpClientHandler.channelRead(AsyncHttpClientHandler.java:78)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n    at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)\r\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\r\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\r\n    at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1504)\r\n    at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1253)\r\n    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1300)\r\n    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:508)\r\n    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:447)\r\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\r\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\r\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\r\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\r\n    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n    at java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: javax.ws.rs.InternalServerErrorException: HTTP 500 Internal Server Error\r\n    at org.glassfish.jersey.client.JerseyInvocation.convertToException(JerseyInvocation.java:960)\r\n    at org.glassfish.jersey.client.JerseyInvocation.access$700(JerseyInvocation.java:82)\r\n    ... 63 more\r\n\r\n</pre></code>\r\n</details>\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/14399/comments",
    "author": "liruixl",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-15T02:14:18Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-28T02:10:29Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-05-21T02:56:29Z",
        "body": "Closed as stale. If it comes back, let's open a new issue and refer here."
      }
    ]
  },
  {
    "number": 14324,
    "title": "[SECURITY] Hardcode the password in debezium-{}-config.yaml",
    "created_at": "2022-02-16T14:31:08Z",
    "closed_at": "2023-07-05T11:29:04Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale",
      "Stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/14324",
    "body": "Hi, Should use \"*****\" to replace the password in debezium-{}-config.yaml? It may have a security risk\r\n\r\npulsar/pulsar-io/debezium/src/main/resources/\r\n- debezium-mssql-source-config.yaml\r\n- debezium-oracle-source-config.yaml\r\n- debezium-postgres-source-config.yaml",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/14324/comments",
    "author": "AliIoT",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-15T02:14:35Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-28T02:10:44Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-10T05:37:49Z",
        "body": "I think they're placeholder only. But still confused about how they're in use.\r\n\r\n@dlg99 as you implemented #12256 include part of these files, do you have ideas on this question?"
      },
      {
        "user": "dlg99",
        "created_at": "2023-07-04T21:25:58Z",
        "body": "these aren't real (or default) passwords AFAIK. These are just examples."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-07-05T11:29:04Z",
        "body": "Close as not an issue.\r\n\r\nIf anyone has a concrete suggestion on \"improving\" here, they can open a new issue."
      }
    ]
  },
  {
    "number": 12991,
    "title": "Expose backlogSize for the topic stats and prometheus metrics",
    "created_at": "2021-11-26T12:30:21Z",
    "closed_at": "2021-12-02T14:08:54Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/12991",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\n\r\nCurrently, we are able to get the backlog size of the topic by using `bin/pulsar-admin topics stats`, but we are not exposing the backlog size to the Prometheus metrics. The backlog size is very important for users to monitor the data deletion, the large backlog size will lead to the topic data will not being deleted. So it will very helpful for exposing the backlog size through the Prometheus endpoint so that users can add an alert very conveniently.\r\n\r\nNow, we have `pulsar_storage_backlog_size` which is aggregated at the namespace level, we should expose it to the topic level if the topic level metrics is enabled, and if the topic level metrics is enabled, we should not expose the namespace level `pulsar_storage_backlog_size` again to keep consistent with the Prometheus format.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/12991/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "Jason918",
        "created_at": "2021-11-26T13:53:11Z",
        "body": "I can work on it."
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-11-29T11:07:54Z",
        "body": "Close the issue since we already have `pulsar_storage_backlog_size` for topics."
      }
    ]
  },
  {
    "number": 12757,
    "title": "Support enable or disable schema upload at the broker level",
    "created_at": "2021-11-11T14:42:55Z",
    "closed_at": "2021-11-18T12:37:38Z",
    "labels": [
      "help wanted",
      "type/feature",
      "good first issue"
    ],
    "url": "https://github.com/apache/pulsar/issues/12757",
    "body": "For users who want to avoid the client side upload schemas to the topic, we can only change the namespace policy to disable the schema auto upload, it's better to have a broker level control to avoid setting policies for every namespace.\r\n\r\n```\r\nisSchemaAutoUploadEnabled=true\r\n```",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/12757/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "Jason918",
        "created_at": "2021-11-11T14:48:37Z",
        "body": "I can work on it. :)"
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-11-11T15:12:41Z",
        "body": "Thanks, @Jason918 assigned to you."
      }
    ]
  },
  {
    "number": 12303,
    "title": "websocket connection authentication 401",
    "created_at": "2021-10-08T09:29:33Z",
    "closed_at": "2022-05-06T08:40:42Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/12303",
    "body": "websocket connection:\r\nws://127.0.0.1:8080/ws/v2/producer/persistent/public/default/aaa?token=Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ0ZXN0LXVzZXIifQ.NAZlgNYZkRF9xGXGasgS5pdqvPBv1IwAaykNoTd8ZiQ\r\nor \r\nws://127.0.0.1:8080/ws/v2/producer/persistent/public/default/aaa?token=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ0ZXN0LXVzZXIifQ.NAZlgNYZkRF9xGXGasgS5pdqvPBv1IwAaykNoTd8ZiQ\r\n\r\nerror:\r\n17:28:21.723 [pulsar-web-63-8] WARN  org.apache.pulsar.broker.web.AuthenticationFilter - [127.0.0.1] Failed to authenticate HTTP request: Authentication required\r\n17:28:21.724 [pulsar-web-63-8] INFO  org.eclipse.jetty.server.RequestLog - 127.0.0.1 - - [08/十月/2021:17:28:21 +0800] \"GET /ws/v2/producer/persistent/public/default/aaa??token=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ0ZXN0LXVzZXIifQ.NAZlgNYZkRF9xGXGasgS5pdqvPBv1IwAaykNoTd8ZiQ HTTP/1.1\" 401 584 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16_0) AppleWebKit/537.36 (KHTML, like Gecko) MQTTX/1.3.0 Chrome/73.0.3683.121 Electron/5.0.2 Safari/537.36\" 1\r\n\r\nconf/client.conf     \r\nauthPlugin=org.apache.pulsar.client.impl.auth.AuthenticationToken\r\nauthParams=token:eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ0ZXN0LXVzZXIifQ.NAZlgNYZkRF9xGXGasgS5pdqvPBv1IwAaykNoTd8ZiQ\r\n\r\nThe command line is successfully displayed\r\nbin/pulsar-admin tenants list\r\n\"public\"\r\n\"pulsar\"\r\n\"sample\"\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/12303/comments",
    "author": "beyondyinjl2",
    "comments": [
      {
        "user": "mattisonchao",
        "created_at": "2021-11-30T03:03:36Z",
        "body": "@codelipenghui \r\ni want to work on it."
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-12-02T13:23:58Z",
        "body": "Thanks @mattisonchao, assign the issue to you."
      },
      {
        "user": "mattisonchao",
        "created_at": "2021-12-02T14:30:57Z",
        "body": "@beyondyinjl2  Could you offer me more detail to reproduce it ?  \r\ni did't reproduce this issue. i think i was lost some configuration. "
      },
      {
        "user": "casuallc",
        "created_at": "2021-12-15T09:08:37Z",
        "body": "I think you should connect websocket server but not pulsar server.\r\n\r\nCheck the cofig in conf/websocket.conf\r\n```\r\n# Port to use to server HTTP request\r\nwebServicePort=9081\r\n# Port to use to server HTTPS request\r\nwebServicePortTls=\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-28T01:55:07Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      }
    ]
  },
  {
    "number": 12185,
    "title": "ratePeriodInSecond in DispatchRate should be non-positive",
    "created_at": "2021-09-25T01:14:00Z",
    "closed_at": "2023-05-08T07:47:35Z",
    "labels": [
      "type/enhancement",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/apache/pulsar/issues/12185",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\nIf set ratePeriodInSecond of DispatchRate as 0 when setting namespace policy, the admin api indicates success. However, successive operations like creating subscriptions under this namespace would fail and the caller doesn't know why due to improper settings. \r\n\r\n**Describe the solution you'd like**\r\nWhen calling admin api, check ratePeriodInSecond value if it's non-positive, failed the calling to let the caller try correct values.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/12185/comments",
    "author": "glcrazier",
    "comments": [
      {
        "user": "hellomyboy",
        "created_at": "2021-09-27T02:00:38Z",
        "body": "/assign"
      },
      {
        "user": "hellomyboy",
        "created_at": "2021-09-28T08:13:15Z",
        "body": "> **Is your enhancement request related to a problem? Please describe.**\r\n> If set ratePeriodInSecond of DispatchRate as 0 when setting namespace policy, the admin api indicates success. However, successive operations like creating subscriptions under this namespace would fail and the caller doesn't know why due to improper settings.\r\n> \r\n> **Describe the solution you'd like**\r\n> When calling admin api, check ratePeriodInSecond value if it's non-positive, failed the calling to let the caller try correct values.\r\n\r\nHow do you use it, can you tell me the steps?"
      },
      {
        "user": "glcrazier",
        "created_at": "2021-10-08T06:43:52Z",
        "body": "> > **Is your enhancement request related to a problem? Please describe.**\r\n> > If set ratePeriodInSecond of DispatchRate as 0 when setting namespace policy, the admin api indicates success. However, successive operations like creating subscriptions under this namespace would fail and the caller doesn't know why due to improper settings.\r\n> > **Describe the solution you'd like**\r\n> > When calling admin api, check ratePeriodInSecond value if it's non-positive, failed the calling to let the caller try correct values.\r\n> \r\n> How do you use it, can you tell me the steps?\r\n\r\nI called like the following:           \r\n```\r\n            adminClient.namespaces().setDispatchRate(\"namespace\",\r\n                    new DispatchRate(configuration.getDispatchThrottlingRateInMsg(),\r\n                            configuration.getDispatchThrottlingRateInByte(),\r\n                            0));\r\n\r\n```\r\nI accidentally created DispatchRate object with third parameters as 0. Then subsequent operations like creating subscription would fail.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-01T02:01:53Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-05-08T07:32:56Z",
        "body": "@glcrazier I guess you'd like to mention \"positive\" here? That is, `ratePeriodInSecond` should be always `> 0`?"
      },
      {
        "user": "tisonkun",
        "created_at": "2023-05-08T07:47:35Z",
        "body": "I try out:\r\n\r\n```\r\n./bin/pulsar-admin namespaces set-subscription-dispatch-rate public/default --dispatch-rate-period 0\r\n./bin/pulsar-client consume -s mysub -p Earliest persistent://public/default/topic \r\n```\r\n\r\nAnd it seems can proceed properly. Maybe it's fixed on master branch. Please open a new issue with repro if it's still relevant."
      }
    ]
  },
  {
    "number": 9757,
    "title": "Support to get which broker is the load coordinator",
    "created_at": "2021-03-01T06:34:28Z",
    "closed_at": "2021-03-10T23:36:03Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/9757",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently we only print logs if a broker becomes the load coordinator, so if users want to check who is the load coordinator, they must check the logs of the brokers. It's better to add a command to get who is the load coordinator, this is useful for the Pulsar maintainer.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9757/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "eolivelli",
        "created_at": "2021-03-01T07:13:57Z",
        "body": "+1 that would be super useful"
      },
      {
        "user": "linlinnn",
        "created_at": "2021-03-02T07:16:39Z",
        "body": "@codelipenghui  hi, is that means to add a command to get who is the leader broker?"
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-03T04:32:18Z",
        "body": "@linlinnn Yes, are you interested in push a PR to add this feature?"
      },
      {
        "user": "linlinnn",
        "created_at": "2021-03-03T05:37:38Z",
        "body": "@codelipenghui  ok"
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-03T07:38:55Z",
        "body": "Thanks, assigned."
      }
    ]
  },
  {
    "number": 9680,
    "title": "Backlog quota: allow time-based quota",
    "created_at": "2021-02-23T06:39:24Z",
    "closed_at": "2021-05-15T01:59:58Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/9680",
    "body": "Retention policies can be set based on size or time limit (or both), while backlog quotas can only be based on size. The recommendations I've gotten are to set backlog quota to be equal to or less than the retention policy, but if the retention policy is based solely on time, the backlog quota cannot be certain to be less than the retention policy.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9680/comments",
    "author": "flowchartsman",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2021-03-02T15:58:26Z",
        "body": "@flowchartsman You can use the message TTL to expire the backlogs of the subscription."
      },
      {
        "user": "flowchartsman",
        "created_at": "2021-03-02T16:49:58Z",
        "body": "There's no backpressure policy applied to TTL though, unless I missed something. Even if I'm combining them, there's no way for me to ensure that backlog is less than TTL."
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-03T04:10:11Z",
        "body": "Yes, it needs to combine them to achieve the purpose."
      },
      {
        "user": "flowchartsman",
        "created_at": "2021-03-04T19:05:12Z",
        "body": "It still feels inconsistent. With TTL there is no good backpressure strategy, and I am then still stuck with trying to guess whether or not backlog-quota fits into TTL time or not, since it is only measured in bytes.\r\n\r\nThis would also then require me to use retention, TTL _and_ backlog quota, and the docs seem to indicate that this is unusual.\r\n\r\nPut another way: if retention period can be set only with time, and if backlog quota is supposed to be equal to or less than retention, then backlog quota should also support being set with only time, otherwise there is no way to know that backlog quota is equal to or less than retention period."
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-05T00:12:18Z",
        "body": "Yes, the TTL can't achieve the backpressure strategy by the time duration. Thanks for the clarification, I think this should be a new feature, so I will change the label to `type/feature`."
      },
      {
        "user": "MarvinCai",
        "created_at": "2021-03-20T04:50:57Z",
        "body": "I'll try to add this feature."
      }
    ]
  },
  {
    "number": 9488,
    "title": "Check message backlog size using Reader interface",
    "created_at": "2021-02-05T00:35:18Z",
    "closed_at": "2021-09-28T11:43:42Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/9488",
    "body": "I am using reader interface to read topic starting from a messageId. \r\n\r\nWhen I check topicStats the backlogSize seems increasing, even after the reader completed. \r\n\r\nIs it possible  to calculate message back log size when reader interface is used ? Like comparing with current messageId and latest messageId ? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9488/comments",
    "author": "hanguyen6",
    "comments": [
      {
        "user": "MarvinCai",
        "created_at": "2021-02-06T01:05:26Z",
        "body": "I think we can probably add a admin API to calculate backlog size given a current position."
      },
      {
        "user": "gaozhangmin",
        "created_at": "2021-05-31T14:21:22Z",
        "body": "please assign to me, if no one working on this"
      }
    ]
  },
  {
    "number": 9466,
    "title": "Having record builder using JSON schema",
    "created_at": "2021-02-04T01:04:32Z",
    "closed_at": "2022-05-06T08:41:41Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/9466",
    "body": "I am trying to build a generic record from JSON schema dynamically using SchemaBuilder and RecordBuilder. \r\nBelow is the code snippet \r\n\r\n`\r\n    val recordBuilder = SchemaBuilder.record(\"testJson\")\r\n\r\n    recordBuilder.field(\"eventType\").type(SchemaType.STRING)\r\n    val si: SchemaInfo = recordBuilder.build(SchemaType.JSON)\r\n    val pulsarSchema = org.apache.pulsar.client.api.Schema.generic(si)\r\n`\r\n`  \r\n \r\n\r\n    val builder: GenericRecordBuilder = pulsarSchema.newRecordBuilder()\r\n    builder.set(\"eventType\", \"test\")\r\n    val pulsarGenericRecord: GenericRecord = builder.build()`\r\n\r\nAnd got this exeception (Pulsar version 2.6.1) \r\n\r\n`Json Schema doesn't support record builder yet\r\njava.lang.UnsupportedOperationException: Json Schema doesn't support record builder yet\r\n\tat org.apache.pulsar.client.impl.schema.generic.GenericJsonSchema.newRecordBuilder(GenericJsonSchema.java:78)`\r\n\r\nIs there other way to build this generic record from JSON schema other than using static POJO ? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9466/comments",
    "author": "hanguyen6",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2021-02-04T03:13:35Z",
        "body": "@hanguyen6 Currently, Pulsar only supports AVRO Generic Schema. I labeled as type/feature"
      },
      {
        "user": "hanguyen6",
        "created_at": "2021-02-04T03:16:23Z",
        "body": "@codelipenghui  thanks for confirmation. Is there other way I can build record dynamically using json schema ?"
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-02T14:36:45Z",
        "body": "@hanguyen6 Have you tried using `schema.JSON(Map.class)`? I'm not sure if this can work for your case."
      },
      {
        "user": "mattisonchao",
        "created_at": "2021-11-11T14:58:23Z",
        "body": "I can work on it. :)"
      },
      {
        "user": "mattisonchao",
        "created_at": "2021-11-12T09:28:36Z",
        "body": "@codelipenghui  This feature has been completed by @vroyer . see #10052\r\n I think we can close this issues."
      },
      {
        "user": "codelipenghui",
        "created_at": "2022-03-04T07:06:12Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      }
    ]
  },
  {
    "number": 9414,
    "title": "Pulsar can get stuck on a single unreadable entry",
    "created_at": "2021-02-02T01:38:35Z",
    "closed_at": "2022-12-11T05:21:05Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/broker",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/9414",
    "body": "**Describe the bug**\r\n\r\nPulsar can get stuck on a single unreadable entry in bookkeeper\r\n\r\n**To Reproduce**\r\n\r\nIncrease max message size from the default 5M to i.e 10M.\r\nWrite a ledger/stream with entries under 5M, entry < 5M, and then some that are less than 5M.\r\nreduce max message size back to 5M.\r\n\r\ntry to process the ledger\r\n\r\nPulsar gets stuck on the entry > 5M and `autoSkipNonRecoverableData` does not help\r\n\r\nPulsar logs\r\n```\r\norg.apache.bookkeeper.mledger.impl.OpReadEntry - ... read failed from ledger at position:X:Y : Bookie handle is not available\r\n```\r\n\r\n**Expected behavior**\r\n\r\n`autoSkipNonRecoverableData` to allow skipping such entry\r\n\r\n**Additional context**\r\n\r\nThis is not a problem right now (worked around this) and I will not spend more time on this, mostly an FYI in case anyone hits this.\r\n\r\nBelow is rather untested diff in case anyone needs it; to deal with this normally it would require unit tests with repro of such situations and/or similar tests in the bookkeeper (plus, possibly, better handlings of such entries on the bookie side)\r\n\r\n```\r\ndiff --git a/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/OpReadEntry.java b/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/OpReadEntry.java\r\nindex 91a6e26f567..fd0b0519280 100644\r\n--- a/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/OpReadEntry.java\r\n+++ b/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/OpReadEntry.java\r\n@@ -23,6 +23,8 @@\r\n import io.netty.util.Recycler;\r\n import io.netty.util.Recycler.Handle;\r\n import java.util.List;\r\n+\r\n+import org.apache.bookkeeper.client.BKException;\r\n import org.apache.bookkeeper.mledger.AsyncCallbacks.ReadEntriesCallback;\r\n import org.apache.bookkeeper.mledger.Entry;\r\n import org.apache.bookkeeper.mledger.ManagedLedgerException;\r\n@@ -97,6 +99,22 @@ public void readEntriesFailed(ManagedLedgerException exception, Object ctx) {\r\n                 callback.readEntriesComplete(entries, ctx);\r\n                 recycle();\r\n             }));\r\n+        } else if (cursor.config.isAutoSkipNonRecoverableData()\r\n+                && exception.getCause() instanceof BKException.BKBookieHandleNotAvailableException) {\r\n+            // It is possible to create situation when bookie client won't be able to read valid existing entry.\r\n+            // Specifically: write large entry and then reduce max message size\r\n+            // Bookie client will disconnect on attempt to deal with this\r\n+            // and throw the exception BKBookieHandleNotAvailableException.\r\n+            log.warn(\"[{}][{}] read failed from ledger at position:{} : {}; will skip the entry\",\r\n+                    cursor.ledger.getName(),\r\n+                    cursor.getName(),\r\n+                    readPosition,\r\n+                    exception.getMessage(),\r\n+                    exception);\r\n+            // Move to next valid position, skipping this one entry\r\n+            final Position nexReadPosition = readPosition.getNext();\r\n+            updateReadPosition(nexReadPosition);\r\n+            checkReadCompletion();\r\n         } else if (cursor.config.isAutoSkipNonRecoverableData() && exception instanceof NonRecoverableLedgerException) {\r\n             log.warn(\"[{}][{}] read failed from ledger at position:{} : {}\", cursor.ledger.getName(), cursor.getName(),\r\n                     readPosition, exception.getMessage());\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9414/comments",
    "author": "dlg99",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2021-02-02T03:24:03Z",
        "body": "@dlg99 Thanks for open this issue, Would you like to provide a fix for this? "
      },
      {
        "user": "merlimat",
        "created_at": "2021-02-02T04:51:47Z",
        "body": "```\r\n+        } else if (cursor.config.isAutoSkipNonRecoverableData()\r\n+                && exception.getCause() instanceof BKException.BKBookieHandleNotAvailableException) {\r\n```\r\nThe `BKBookieHandleNotAvailableException` is thrown when the client is not able to connect to a bookie, so this fix would result in a lot of data dropped :) \r\n"
      },
      {
        "user": "dlg99",
        "created_at": "2021-02-02T05:48:17Z",
        "body": "@merlimat hence this and not a pull request.\r\nThe decent fix would require change not the bookie side to handle this situation differently/return a different error,"
      },
      {
        "user": "codelipenghui",
        "created_at": "2022-03-04T07:06:37Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-11T05:21:05Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 9340,
    "title": "NullPointerException while stopping \"links localrun\" with Ctrl-C",
    "created_at": "2021-01-27T10:36:09Z",
    "closed_at": "2021-11-20T16:07:43Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/9340",
    "body": "**Describe the bug**\r\n\r\nI ofter see this error while stopping a \"localrun\" of a sink with Ctrl-C\r\n\r\n```\r\nException in thread \"main\" java.lang.NullPointerException\r\n\tat org.apache.pulsar.functions.LocalRunner.start(LocalRunner.java:360)\r\n\tat org.apache.pulsar.functions.LocalRunner.main(LocalRunner.java:168)\r\n\r\n```\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a sink with localrun and press ctrl-C\r\n\r\n**Expected behavior**\r\nNo exception to be logged, as it may look quite scary for the user\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9340/comments",
    "author": "eolivelli",
    "comments": [
      {
        "user": "mattisonchao",
        "created_at": "2021-11-11T15:06:19Z",
        "body": "I can work on it. :)"
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-11-11T15:16:57Z",
        "body": "Thanks @mattisonchao, I have assigned the issue to you."
      },
      {
        "user": "mattisonchao",
        "created_at": "2021-11-20T12:21:36Z",
        "body": "@eolivelli  I can not reproduce this issue. \r\n\r\nDo you have any idea to help me? thanks :)"
      },
      {
        "user": "eolivelli",
        "created_at": "2021-11-20T12:34:42Z",
        "body": "probably it has been fixed ?\r\n\r\nwhat happens in \"LocalRunner.java:360\" ? "
      },
      {
        "user": "mattisonchao",
        "created_at": "2021-11-20T13:01:27Z",
        "body": "@eolivelli  This bug has been fixed by #12278 \r\n\r\nI think we need to close this issue."
      },
      {
        "user": "eolivelli",
        "created_at": "2021-11-20T16:07:53Z",
        "body": "Thank you @mattisonchao "
      }
    ]
  },
  {
    "number": 9301,
    "title": "MultiTopicsReaderImpl – Start from separate MessageId for each topic/ partition",
    "created_at": "2021-01-24T15:24:36Z",
    "closed_at": "2021-05-15T02:00:45Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/9301",
    "body": "Currently in ReaderConfigurationData the API allow to ‘setStartMessageId’ only from single message ID and this apply to all consumers in the MultiTopicsReaderImpl.\r\n\r\nIs it possible to add start message per partition / topic.\r\n\r\nToday if I want to avoid loosing data I have to give MessageId.earliest and after that to execute the seek (as my code does).\r\nBut the annoying issue is that after I create the reader it start accept messages from Pulsar and till the seek is actually executed I consume events, I can avoid handling them because I keep tracking the cursor per topic / partition but I still need to consume them (to ensure exactly once). \r\nIt will be nicer if I could give the above map between topic and the start message ID to avoid it (and if I don’t have the last MessageId in my map it will be MessageId.earliest). \r\n\r\nThe best way is to give Function<String, MessageId> to reader configuration between Consumer.getTopic and the start message ID.\r\n\r\n- I am using key hash feature (pulsar broker / client version 2.7.0).\r\n- It will also be helpful add add support for multiple topics (by pattern / list) to the MultiTopicsReaderImpl\r\n\r\n`/**\r\n     * Create reader\r\n     */\r\n    private Reader<byte[]> createReader() throws PulsarClientException {\t\r\n        ReaderConfigurationData<byte[]> cloned = ...;\r\n        cloned.setStartMessageId(MessageId.earliest);\r\n        cloned.setReaderName(...);\r\n        cloned.setKeyHashRanges(Arrays.asList(new Range(myRange.getStart(), myRange.getEnd())));\t//Using key hash feature\r\n        Reader<byte[]> reader = client.getShared().createReaderAsync(cloned).join();\r\n        seek(reader);\r\n        return reader;\r\n    }\t\r\n\t\r\n\t/**\r\n     * Execute seek by partitions\r\n     */\r\n    private void seek(Reader reader) throws PulsarClientException {\r\n        if(reader instanceof MultiTopicsReaderImpl){\r\n            MultiTopicsReaderImpl multiTopicsReader = (MultiTopicsReaderImpl)reader;\r\n            List<ConsumerImpl> consumers = multiTopicsReader.getMultiTopicsConsumer().getConsumers();\r\n            for(Consumer consumer : consumers){\r\n                seek(consumer.getTopic(),  messageId -> consumer.seek(messageId));\r\n            }\r\n        }\r\n        else if(reader instanceof ReaderImpl){\r\n            seek(reader.getTopic(), messageId -> reader.seek(messageId));\r\n        }\r\n        else{\r\n            throw new IllegalArgumentException(\"Unknown reader type: \" + reader.getClass());\r\n        }\r\n    }\r\n\t\r\n    /**\r\n     * Find the relevant message ID to seek on the given consumer / reader\r\n     */\r\n    private void seek(String topic, ExceptionalConsumer<MessageId> seek){\r\n        PartitionedTopicCursor<MessageId, Message<T>> partitionCursor = cursors.get(topic);\t//Out internal cursor tracking\r\n        MessageId seekTo;\r\n        if(partitionCursor == null){\r\n            cursors.put(topic, new PartitionedTopicCursor(.../*Init new one to keep tracking on this partition*/));\r\n\t\t\t\r\n\t\t\treturn; //no need to seek\r\n        }\r\n\t\t\r\n\t\t\r\n\t\tMessageId seekTo seekTo = partitionCursor.getStartFrom();\r\n\t\tseek.accept(seekTo);\t\r\n    }`\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9301/comments",
    "author": "aryemazouz",
    "comments": [
      {
        "user": "315157973",
        "created_at": "2021-01-25T09:46:20Z",
        "body": "I will work on it"
      },
      {
        "user": "zymap",
        "created_at": "2021-03-02T02:50:38Z",
        "body": "Move this to the next release"
      },
      {
        "user": "eolivelli",
        "created_at": "2021-04-27T07:57:54Z",
        "body": "This is a new API, it is better to add it only on a new major release (2.8.0)"
      }
    ]
  },
  {
    "number": 9224,
    "title": "[Pulsar SQL] Some optimized points in PR 8422",
    "created_at": "2021-01-17T03:20:28Z",
    "closed_at": "2023-05-08T11:41:09Z",
    "labels": [
      "type/enhancement",
      "help wanted",
      "area/sql",
      "good first issue"
    ],
    "url": "https://github.com/apache/pulsar/issues/9224",
    "body": "# Motivation\r\n\r\nThe #8422 made a refactor for the Pulsar SQL, it's a major change. There are some points that could be considered to optimize.\r\n\r\n## 1. PulsarRowDecoderFactory\r\n\r\nThe various decoder factories could be initialized at the Pulsar SQL beginning, one time is enough and they could be reused.\r\n\r\nRefer to the method `private PulsarRowDecoderFactory createDecoderFactory(SchemaInfo schemaInfo)`  of the class `PulsarDispatchingRowDecoderFactory`.\r\n\r\n## 2. PulsarRowDecoderFactory\r\n\r\nIt seems that the multi-version schema decoder cache could be added and the decoders could be reused.\r\n\r\nRefer to the method `PulsarRowDecoder createRowDecoder(TopicName topicName, SchemaInfo schemaInfo,\r\n                                      Set<DecoderColumnHandle> columns)` in class `PulsarRowDecoderFactory`.\r\n\r\n```\r\n// PulsarRecordCursor.java\r\nPulsarRowDecoder keyDecoder = decoderFactory.createRowDecoder(topicName,\r\n                    schemaInfo,\r\n                    columnHandles.stream()\r\n                            .filter(col -> !col.isInternal())\r\n                            .filter(col -> PulsarColumnHandle.HandleKeyValueType.KEY\r\n                                    .equals(col.getHandleKeyValueType()))\r\n                            .collect(toImmutableSet()));\r\n```\r\n\r\n## 3. Internal Column decode optimize\r\n\r\nThe `switch-case` is more efficient than `if-else` and the PulsarInternalColumn could be changed to an enum.\r\n\r\n```\r\n// PulsarRecordCursor.java\r\nfor (DecoderColumnHandle columnHandle : columnHandles) {\r\n            if (columnHandle.isInternal()) {\r\n                if (PulsarInternalColumn.PARTITION.getName().equals(columnHandle.getName())) {\r\n                    currentRowValuesMap.put(columnHandle, longValueProvider(this.partition));\r\n                } else if (PulsarInternalColumn.EVENT_TIME.getName().equals(columnHandle.getName())) {\r\n                   ...\r\n            }\r\n        }\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9224/comments",
    "author": "gaoran10",
    "comments": [
      {
        "user": "kaori-seasons",
        "created_at": "2021-11-11T23:20:13Z",
        "body": "@codelipenghui I want to fix this issues."
      },
      {
        "user": "codelipenghui",
        "created_at": "2022-03-04T07:07:09Z",
        "body": "The issue had no activity for 30 days, mark with Stale label."
      },
      {
        "user": "ziang123",
        "created_at": "2022-12-18T01:50:29Z",
        "body": "Kindly to ask if there is somebody working for this issue? lf not, I'm glad to undertake it"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-18T02:15:18Z",
        "body": "@ziang123 I think no one is working on this issue yet. Go ahead!"
      },
      {
        "user": "tisonkun",
        "created_at": "2023-05-08T11:41:09Z",
        "body": "Closed by #19027."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-05-08T11:41:24Z",
        "body": "@ziang123 Thanks for your contribution!"
      }
    ]
  },
  {
    "number": 9128,
    "title": "admin-api-brokers list failed ",
    "created_at": "2021-01-05T07:55:37Z",
    "closed_at": "2021-01-13T10:27:50Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/cli"
    ],
    "url": "https://github.com/apache/pulsar/issues/9128",
    "body": " \r\n[dmq@host-10-33-50-221 bin]$ ./pulsar-admin --auth-plugin org.apache.pulsar.client.impl.auth.AuthenticationToken --auth-params token:CgB6e3x9PKWk714cEg4v8UPagxPZX7mCycV5OsUQle5SHOMO6yAXFeTd2Bjayy0Vj7Vxvj7/bbYr2r4lt+CUUrME brokers list pulsar-cluster-2\r\nFailed to validate Cluster configuration : cluster=pulsar-cluster-2  emsg=For input string: \"8080,10.33.50.220:8080,10.33.50.221:8080\"\r\n\r\nReason: Failed to validate Cluster configuration : cluster=pulsar-cluster-2  emsg=For input string: \"8080,10.33.50.220:8080,10.33.50.221:8080\"\r\n\r\n\r\nMaybe there have bug  in parses cluster service url",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/9128/comments",
    "author": "xiaotongwang1",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2021-01-08T06:51:14Z",
        "body": "@xiaotongwang1 How did you set cluster `pulsar-cluster-2`? Can you run `pulsar-admin clusters get pulsar-cluster-2`?"
      }
    ]
  },
  {
    "number": 8797,
    "title": "pulsar_ env.sh add different JVM options for different components",
    "created_at": "2020-12-03T01:29:09Z",
    "closed_at": "2022-12-06T13:47:43Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/build",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/8797",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nMultiple components of pulsar are deployed on a machine, and all components share a pulsar when they are PULSAR_MEM environment variable. If the JVM memory settings of each component are different, you can either export PULSAR_MEM when starting different components, or modify PULSAR_MEM in pulsar_env.sh, Each component can be provided with its own JVM variables, so in pulsar_env.sh you only need to modify all the required JVMs of all components once.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8797/comments",
    "author": "xuesongxs",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T13:47:43Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 8672,
    "title": "Complex Schema Deserialization Issues",
    "created_at": "2020-11-23T16:00:35Z",
    "closed_at": "2022-12-09T14:22:33Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/8672",
    "body": "**Describe the bug**\r\nmessage.value() deserialization failing with complex schema defined within the Python client when using Json and Avro schemas.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run the \"pulsar-all\" image in a Docker container\r\n2. Configure a nested object that uses a previously defined record as a field in another using the Python client\r\n3. Attempt to deserialize the messages from a topic with the nested object using message.value()\r\n4. See error (IndexError: list out of range with Avro, TypeError: '<dict>' found when ${INSERT_YOUR_DEFINED_SUBSCHEMA} expected\" with Json)\r\n\r\n**Expected behavior**\r\nObjects correctly deserialized from Json/Avro to the type specified in the schemas.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: CentOS 7, Docker version 19.03.13, build 4484c46d9d\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8672/comments",
    "author": "EliHarper",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-11-24T02:37:10Z",
        "body": "@EliHarper which version of the Pulsar image are you using?\r\n@gaoran10 @congbobo184  to take a look. Since is easy to reproduce, also mark this as help-wanted, anyone is interested in this issue could also to have a try. "
      },
      {
        "user": "EliHarper",
        "created_at": "2020-12-01T15:10:42Z",
        "body": "I'm sorry for the late response, and thanks for getting back to me.\r\n\r\nI'm pulled the latest on 11/9, which I believe is 2.6.2, according to the digest (4baf6e...)\r\n\r\nThanks again for your help, and please let me know if there's anything else I can get for you."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T14:22:33Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 8573,
    "title": "Export Prometheus metric for the number of messages that topic subscription skip through the TTL",
    "created_at": "2020-11-16T08:00:35Z",
    "closed_at": "2020-12-11T18:12:12Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/8573",
    "body": "Export Prometheus metric for the number of messages that topic subscription skip through the TTL。",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8573/comments",
    "author": "xuesongxs",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-11-16T08:46:48Z",
        "body": "Thanks, @xuesongxs made this feature request. I also met some users who want to know how many messages are expired at what time? Currently, these metrics are too few, so that TTL looks like a black box, unobservable. \r\n\r\nI think we can do the following things to improve the observability.\r\n\r\n1. Expose the last TTL task execute timestamp\r\n2. Recore the total expired message\r\n\r\nFurthermore, I think it's better to support write the expired messages to a specific topic so that users can get all expired messages. Or a server-side plugin to receive the message expired listener."
      }
    ]
  },
  {
    "number": 8532,
    "title": "Export Consumer/Producer connection status to user",
    "created_at": "2020-11-12T02:13:58Z",
    "closed_at": "2020-11-19T02:49:46Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/8532",
    "body": "Some times consumer/producer get re-connect to broker, but this is the internal connect retry, and user may not know the current connection status. It would be better to export the connection status.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8532/comments",
    "author": "jiazhai",
    "comments": [
      {
        "user": "315157973",
        "created_at": "2020-11-12T13:09:07Z",
        "body": "I will solve it"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-11-16T01:00:59Z",
        "body": "Thanks @315157973  for the help"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-11-16T01:27:18Z",
        "body": "@315157973 some times it will help a lot if we could also provide some connection information, such as the last success time or the first non-success re-connection time."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-11-16T05:40:37Z",
        "body": "related to #8532"
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-11-19T02:49:46Z",
        "body": "Currently, the consumer and the producer has a method to check the isConnected. So I think it's can help users to check the connection state, So I think we can close this issue first."
      }
    ]
  },
  {
    "number": 8408,
    "title": "Add test coverage for the Pulsar IO Batch connectors",
    "created_at": "2020-10-29T19:52:08Z",
    "closed_at": "2022-12-07T04:14:56Z",
    "labels": [
      "type/enhancement",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/8408",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\nThe Pulsar Batch IO classes do not have any unit tests for them\r\n\r\n**Describe the solution you'd like**\r\nI will be adding some unit tests for these classes\r\n\r\n**Describe alternatives you've considered**\r\nNot implementing unit tests.\r\n\r\n**Additional context**\r\nN/A\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8408/comments",
    "author": "david-streamlio",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-07T04:14:56Z",
        "body": "Closed as stale and no one worked on it. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 8298,
    "title": "Make logs more consistent",
    "created_at": "2020-10-19T11:29:27Z",
    "closed_at": "2021-09-10T06:27:04Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/8298",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\n Broker and bookkeeper logs do not include the date, only time\r\n\r\n**Describe the solution you'd like**\r\nI would like the logs from all services to include both date and time\r\n\r\n**Describe alternatives you've considered**\r\nA way to configure the logging format for kubernetes deployment of Pulsar. Or maybe there is a way but i missed it in docs?\r\n\r\n**Additional context**\r\nExample log missing date (the last entry here)\r\n`18:58:57.243 [pulsar-web-42-8] INFO  org.eclipse.jetty.server.RequestLog - 127.0.0.1 - - [08/Oct/2020:18:58:57 +0000] \"GET /admin/v2/persistent/perf-1/_system/_signals_ingest/stats HTTP/1.1\" 200 5656 \"-\" \"curl/7.64.0\" 4\r\n18:59:06.384 [pulsar-web-42-4] INFO  org.eclipse.jetty.server.RequestLog - 10.0.0.46 - - [08/Oct/2020:18:59:06 +0000] \"GET /status.html HTTP/1.1\" 200 2 \"-\" \"kube-probe/1.14+\" 1\r\n18:59:07.234 [pulsar-web-42-3] INFO  org.eclipse.jetty.server.RequestLog - 127.0.0.1 - - [08/Oct/2020:18:59:07 +0000] \"GET /admin/v2/persistent/perf-1/_system/_signals_ingest/stats HTTP/1.1\" 200 5656 \"-\" \"curl/7.64.0\" 2\r\n18:59:11.409 [pulsar-load-manager-3-1] INFO  org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl - Only 1 broker available: no load shedding will be performed `\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8298/comments",
    "author": "klys-equinix",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-10-22T19:00:44Z",
        "body": "@losiu97 We can change the default logging format. Are you interested in contributing a fix to this?"
      },
      {
        "user": "klys-equinix",
        "created_at": "2020-11-02T10:37:38Z",
        "body": "Hi, sorry i missed your reply. I found a way of fixing it for myself, but we could change the default format to \r\n%d{YYYY-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n, or any other format you find suitable"
      },
      {
        "user": "tomscut",
        "created_at": "2021-09-10T00:59:56Z",
        "body": "Hi @losiu97 @hangc0276 , this problem has been solved. Can we close the issue? Thanks."
      }
    ]
  },
  {
    "number": 8260,
    "title": "Support reset cursor to a batch index of the batching message",
    "created_at": "2020-10-14T09:33:13Z",
    "closed_at": "2020-10-21T11:16:30Z",
    "labels": [
      "help wanted",
      "type/feature",
      "release/2.6.2"
    ],
    "url": "https://github.com/apache/pulsar/issues/8260",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWe have support ack the batch message index since 2.6.0. But it's not supported by the reset cursor operation. So, if users reset the cursor to a batch index of the batch message then when consuming messages again, it will get these batch index again.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8260/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "315157973",
        "created_at": "2020-10-14T09:38:17Z",
        "body": "I will work on it\r\n\r\n"
      }
    ]
  },
  {
    "number": 8259,
    "title": "Support exclude the message when reset cursor by message ID",
    "created_at": "2020-10-14T09:27:48Z",
    "closed_at": "2020-10-21T16:57:58Z",
    "labels": [
      "help wanted",
      "type/feature",
      "release/2.6.2"
    ],
    "url": "https://github.com/apache/pulsar/issues/8259",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently, when reset the cursor to a position, the broker will set the mark delete position to the previous position of the reset position. For some usecase, we don't want to consume the reset position again, so it's better to provide a way to reset the cursor to a specific position and exclude this position. So that the consumers under the subscription can start consume messages from the next position of the reset position.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8259/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "315157973",
        "created_at": "2020-10-14T09:36:08Z",
        "body": "Please assign it to me"
      }
    ]
  },
  {
    "number": 8252,
    "title": "Support configure deduplicationEntriesInterval at the namespace/topic level",
    "created_at": "2020-10-14T00:57:03Z",
    "closed_at": "2022-08-11T11:45:46Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/8252",
    "body": "Currently we take de-duplication snapshots based on size and can only change the configuration at the broker level. It's better to allow to change it at the namespace/topic level.\r\n\r\nRelated to #8237",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8252/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "Jennifer88huang-zz",
        "created_at": "2020-10-14T03:15:37Z",
        "body": "Does it duplicate with #8237? If so, shall we close one? "
      },
      {
        "user": "hangc0276",
        "created_at": "2020-10-17T11:46:15Z",
        "body": "I will work for it."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-10-21T11:49:25Z",
        "body": "@Jennifer88huang No, it's not a dup, #8237 introduces a new mechanism for the deduplication check."
      }
    ]
  },
  {
    "number": 8135,
    "title": "[python-client] Support key based batching",
    "created_at": "2020-09-25T10:52:44Z",
    "closed_at": "2020-10-04T09:30:44Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/8135",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nNow the C++ client support key based batching from #7996, the `BatchingType` should be exposed to the Python wrapper to support key based batching for Python client.\r\n\r\n**Describe the solution you'd like**\r\nExpose the `ProducerConfiguration::BatchingType` and related setter/getter to Python client.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8135/comments",
    "author": "BewareMyPower",
    "comments": [
      {
        "user": "hangc0276",
        "created_at": "2020-09-29T14:59:13Z",
        "body": "I will try to fix it, Please assign to me."
      },
      {
        "user": "BewareMyPower",
        "created_at": "2020-09-29T15:08:09Z",
        "body": "@hangc0276 thx for your help. It seems that I don't have the permission to assign the issue, please assign this issue to hang, @jiazhai "
      }
    ]
  },
  {
    "number": 8121,
    "title": "Fix the flaky test testRemoveSubscribeRate",
    "created_at": "2020-09-24T03:11:32Z",
    "closed_at": "2020-11-17T02:45:29Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/8121",
    "body": "**Describe the bug**\r\n`testRemoveSubscribeRate` in the TopicPolicyTest is a flaky test when run all tests under the TopicPolicyTest. But only run `testRemoveSubscribeRate`.\r\n\r\nSo in #8114 ignore the `testRemoveSubscribeRate` to make sure other PRs does not block here.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8121/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "hangc0276",
        "created_at": "2020-09-30T02:32:21Z",
        "body": "I will fix it."
      },
      {
        "user": "wolfstudy",
        "created_at": "2020-10-31T03:03:11Z",
        "body": "> I will fix it.\r\n\r\n@hangc0276 What is the current status of this issue? Has pr fixed this problem?"
      },
      {
        "user": "wolfstudy",
        "created_at": "2020-10-31T03:06:23Z",
        "body": "Move this issue to 2.7.0."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-11-17T02:45:29Z",
        "body": "Seems this issue is fixed by #8557, So close it first."
      }
    ]
  },
  {
    "number": 8092,
    "title": "brokerServiceUrl do not support mutli hosts in ipv6 environment",
    "created_at": "2020-09-21T03:16:21Z",
    "closed_at": "2020-09-30T03:27:28Z",
    "labels": [
      "type/bug",
      "help wanted",
      "release/2.6.2"
    ],
    "url": "https://github.com/apache/pulsar/issues/8092",
    "body": "**Describe the bug**\r\nSet `brokerServiceUrl` in client.conf with mutli ipv6 host address like `pulsar://[fec0:0:0:ffff::1]:6650,[fec0:0:0:ffff::2]:6650`, when client connect to brokers it got such exception:\r\n`java.lang.IllegalArgumentException: Illegal character in port number at index 32: pulsar://[fec0:0:0:ffff::1]:6650,[fec0:0:0:ffff::2]:6650\r\nat java.net.URI.create(URI.java:852) ~[na:1.8.0_131]`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Start mutli brokers in ipv6 environment\r\n2. Set `brokerServiceUrl` in client.conf with mutli ipv6 host address and connect to brokers. \r\n\r\n**Expected behavior**\r\nClient can connect to brokers\r\n\r\n**Additional context**\r\nIt seems ipv6 URI do not support mutli hosts split by comma. Is there any workaround to specify mutli ipv6 hosts in `brokerServiceUrl` ? or some code need changed in pulsar-client?\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8092/comments",
    "author": "wangjialing218",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-09-22T02:07:47Z",
        "body": "Are you interested in pushing a PR for fixing this issue?"
      },
      {
        "user": "wangjialing218",
        "created_at": "2020-09-22T02:47:07Z",
        "body": "I will work on fixing this issue."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-09-22T02:48:14Z",
        "body": "Great, thanks @wangjialing218 "
      }
    ]
  },
  {
    "number": 8050,
    "title": "Replace map with set",
    "created_at": "2020-09-13T09:33:39Z",
    "closed_at": "2020-09-21T23:52:56Z",
    "labels": [
      "type/enhancement",
      "help wanted",
      "area/client"
    ],
    "url": "https://github.com/apache/pulsar/issues/8050",
    "body": "In PulsarClientImpl, we use IdentityHashMap to hold the reference of producers/consumers like below:\r\n```\r\nproducers.put(producer, Boolean.TRUE);\r\n```\r\nIt's better to use List, for value is never used but stored.\r\nBut Pulsar pursues performance, so looks good to use thread safe set -- Collections.newSetFromMap(new ConcurrentHashMap<>()) .",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/8050/comments",
    "author": "Technoboy-",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-09-14T02:21:12Z",
        "body": "Thanks @Technoboy- for the help"
      }
    ]
  },
  {
    "number": 7971,
    "title": "pulsar-client not usable in a jpms (java modules) setup",
    "created_at": "2020-09-03T15:55:45Z",
    "closed_at": "2023-01-08T01:56:53Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/client",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7971",
    "body": "- pulsar-client and pulsar-api both contain the packages org.apache.pulsar.common.schema which causes a split package problem when our application is ran on the module path. (upgrading from 2.5.2 to 2.6.1)\r\n\r\nError occurred during initialization of boot layer\r\njava.lang.module.ResolutionException: Modules pulsar.client.api and pulsar.client export package org.apache.pulsar.common.schema\r\n\r\n- aircompressor and pulsar-client both contain the packages io.airlift.compress.zstd\r\n\r\nError occurred during initialization of boot layer\r\njava.lang.module.ResolutionException: Modules aircompressor and pulsar.client export package io.airlift.compress.zstd\r\n\r\nThis was not a problem on the 2.5.x versions.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7971/comments",
    "author": "guyv",
    "comments": [
      {
        "user": "hollander-cegeka",
        "created_at": "2022-01-21T13:01:02Z",
        "body": "This is still an issue in version 2.9.1:\r\njava.lang.module.ResolutionException: Modules pulsar.client and pulsar.client.admin.api export package org.apache.pulsar.common.naming"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-10T04:34:19Z",
        "body": "@guyv @hollander-cegeka could you provide a demo repo or minimal file set to reproduce the issue?\r\n\r\nThen other community members can verify whether their fix is viable."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-01-08T01:56:53Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7959,
    "title": "Force namespace deletion even if not empty",
    "created_at": "2020-09-02T09:01:07Z",
    "closed_at": "2020-09-21T23:12:39Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7959",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\nCurrently namespace needs to be empty to be deleted.\r\n\r\n**Describe the solution you'd like**\r\nI think we should have a optional field to force the deletion of all stuffs related to namespace and delete them and finally delete the namespace.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7959/comments",
    "author": "KannarFr",
    "comments": [
      {
        "user": "murong00",
        "created_at": "2020-09-04T01:09:59Z",
        "body": "We have implemented this feature in our dev branch and I will finish this later when available."
      },
      {
        "user": "KannarFr",
        "created_at": "2020-09-07T01:13:22Z",
        "body": "@murong00 can you create a PR and maybe i can contribute to it?"
      }
    ]
  },
  {
    "number": 7878,
    "title": "NPE when acknowledge messages at the broker side",
    "created_at": "2020-08-22T13:02:15Z",
    "closed_at": "2020-09-04T07:52:25Z",
    "labels": [
      "type/bug",
      "help wanted",
      "release/2.6.2"
    ],
    "url": "https://github.com/apache/pulsar/issues/7878",
    "body": "**Describe the bug**\r\n```\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: java.lang.NullPointerException: null\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.pulsar.broker.service.persistent.PersistentSubscription.acknowledgeMessage(PersistentSubscription.java:386) ~[org.apache.pulsar-pulsar-broker-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.pulsar.broker.service.persistent.ReplicatedSubscriptionsController.receiveSubscriptionUpdated(ReplicatedSubscriptionsController.java:177) ~[org.apache.pulsar-pulsar-broker-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.pulsar.broker.service.persistent.ReplicatedSubscriptionsController.receivedReplicatedSubscriptionMarker(ReplicatedSubscriptionsController.java:92) ~[org.apache.pulsar-pulsar-broker-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.pulsar.broker.service.persistent.PersistentTopic.receivedReplicatedSubscriptionMarker(PersistentTopic.java:2155) ~[org.apache.pulsar-pulsar-broker-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.pulsar.broker.service.persistent.PersistentReplicator.checkReplicatedSubscriptionMarker(PersistentReplicator.java:745) ~[org.apache.pulsar-pulsar-broker-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.pulsar.broker.service.persistent.PersistentReplicator.readEntriesComplete(PersistentReplicator.java:351) ~[org.apache.pulsar-pulsar-broker-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.bookkeeper.mledger.impl.OpReadEntry.lambda$checkReadCompletion$2(OpReadEntry.java:152) ~[org.apache.pulsar-managed-ledger-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.bookkeeper.mledger.util.SafeRun$1.safeRun(SafeRun.java:32) [org.apache.pulsar-managed-ledger-2.6.0.jar:2.6.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.bookkeeper.common.util.SafeRunnable.run(SafeRunnable.java:36) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at org.apache.bookkeeper.common.util.OrderedExecutor$TimedRunnable.run(OrderedExecutor.java:203) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\nAug 22 15:59:37 dc3-pulsar-ec-beta-pro02 pulsar[13707]: at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]\r\n```\r\n**Additional context**\r\n2.6.0\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7878/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-08-24T01:41:24Z",
        "body": "👍 This should be an easy fix. "
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-09-04T07:52:25Z",
        "body": "close via #7937"
      }
    ]
  },
  {
    "number": 7804,
    "title": "ConcurrentModificationException occurs when dispatch message to consumers",
    "created_at": "2020-08-13T02:15:33Z",
    "closed_at": "2022-12-09T13:26:35Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7804",
    "body": "**Describe the bug**\r\n```\r\njava.util.ConcurrentModificationException: null\r\n\tat java.util.ArrayList$SubList.checkForComodification(ArrayList.java:1241) ~[?:1.8.0_265]\r\n\tat java.util.ArrayList$SubList.size(ArrayList.java:1050) ~[?:1.8.0_265]\r\n\tat org.apache.pulsar.broker.service.Consumer.lambda$sendMessages$0(Consumer.java:267) ~[org.apache.pulsar-pulsar-broker-2.6.0-sn-12.jar:2.6.0-sn-12]\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:387) [io.netty-netty-transport-native-epoll-4.1.48.Final-linux-x86_64.jar:4.1.48.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_265]\r\n```\r\n\r\n**To Reproduce**\r\nIt's hard to reproduce.\r\n\r\n**Expected behavior**\r\nKeep the list operation safely.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7804/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "wolfstudy",
        "created_at": "2020-10-31T03:06:46Z",
        "body": "Move this issue to 2.7.0."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-11-17T02:16:36Z",
        "body": "move to milestone 2.8.0"
      },
      {
        "user": "zymap",
        "created_at": "2021-03-02T02:39:03Z",
        "body": "Move this to the next release."
      },
      {
        "user": "congbobo184",
        "created_at": "2021-07-26T14:15:54Z",
        "body": "Move this to the next release."
      },
      {
        "user": "michaeljmarshall",
        "created_at": "2021-12-10T17:57:58Z",
        "body": "@codelipenghui - removing the 2.7.4 label. I'm not sure that we'll do a 2.7.5 release. We'll determine which branches to cherry pick to when this is completed/merged."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T13:26:35Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions.\r\n\r\nThe codebase has been changed a lot, and if we cannot reproduce the failure stably, leaving the issue open can starve."
      }
    ]
  },
  {
    "number": 7799,
    "title": "Pulsar metrics providing wrong information",
    "created_at": "2020-08-12T04:47:14Z",
    "closed_at": "2020-09-03T08:41:11Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7799",
    "body": "**Pulsar metrics endpoint provides irrelevant information**\r\nWanted to create pulsar grafana dashboard. we rely on */metrics* endpoint. In that *pulsar_topics_count* provides wrong information/ Correct me if the understanding is different.\r\n \r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nWe created two namespaces -> sample1, sample2\r\nIn sample1 we have 6 topics.\r\nIn sample2 we have 3 topics.\r\n\r\nthen pulsar_topics_count metrics as below\r\n\r\npulsar_topics_count{cluster=\"cluster1\",namespace=\"public/sample1\"} 6 1597206282645\r\npulsar_topics_count{cluster=\"cluster1\",namespace=\"public/sample2\"} 9 1597206282645\r\n\r\n**Expected behavior**\r\nBut the 2nd metric should provide information as below\r\npulsar_topics_count{cluster=\"cluster1\",namespace=\"public/sample2\"} 3 1597206282645\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7799/comments",
    "author": "Esakkimuthu991",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-08-12T14:58:44Z",
        "body": "Are the topics partitioned under the namespace `public/sample2`? Pulsar will count each partition into a topic."
      },
      {
        "user": "Esakkimuthu991",
        "created_at": "2020-08-12T15:32:51Z",
        "body": "> Are the topics partitioned under the namespace `public/sample2`? Pulsar will count each partition into a topic.\r\n\r\nNo, Its not a partitioned topic"
      }
    ]
  },
  {
    "number": 7787,
    "title": "[pulsar-client-cpp] Should throw std::exception (or a subtype), not const char*",
    "created_at": "2020-08-09T18:49:37Z",
    "closed_at": "2020-08-24T22:24:44Z",
    "labels": [
      "type/enhancement",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7787",
    "body": "**Is your enhancement request related to a problem? Please describe.**\r\nThe Pulsar C++ client library throws `const char*` in several locations instead of `std::exception` or a derived type.\r\n\r\n**Describe the solution you'd like**\r\nThrow `std::exception` or some derived type instead.\r\n\r\n**Describe alternatives you've considered**\r\nIf you're going to throw something, `std::exception` is the expected type in modern C++\r\n\r\n**Details**\r\n```c++\r\n/lib/MessageBuilder.cc:112:        throw \"sequenceId needs to be >= 0\";\r\n./lib/CompressionCodecSnappy.cc:59:    throw \"Snappy compression not supported\";\r\n./lib/CompressionCodecSnappy.cc:64:    throw \"Snappy compression not supported\";\r\n./lib/ProducerConfiguration.cc:70:        throw \"maxPendingMessages needs to be greater than 0\";\r\n./lib/ProducerConfiguration.cc:80:        throw \"maxPendingMessages needs to be greater than 0\";\r\n./lib/ProducerConfiguration.cc:134:        throw \"batchingMaxMessages needs to be greater than 1\";\r\n./lib/ConsumerConfiguration.cc:96:        throw \"Consumer Config Exception: Unacknowledged message timeout should be greater than 10 seconds.\";\r\n./lib/auth/AuthOauth2.cc:105:        throw \"ExpiresIn in Oauth2TokenResult invalid value: \" + expiredIn;\r\n./lib/auth/AuthToken.cc:55:        throw \"Failed to read environment variable \" + envVarName;\r\n./lib/auth/AuthToken.cc:77:        throw \"Invalid configuration for token provider\";\r\n./lib/MessageId.cc:79:        throw \"Failed to parse serialized message id\";\r\n./lib/CompressionCodecZstd.cc:61:SharedBuffer CompressionCodecZstd::encode(const SharedBuffer& raw) { throw \"ZStd compression not supported\"; }\r\n./lib/CompressionCodecZstd.cc:65:    throw \"ZStd compression not supported\";\r\n```\r\nInvalid arguments or bad config (i.e. `sequenceId needs to be >= 0`) should result in `std::invalid_argument`.  The `not supported` exceptions should perhaps be `runtime_error` or maybe just bare `exception`s.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7787/comments",
    "author": "Bklyn",
    "comments": [
      {
        "user": "BewareMyPower",
        "created_at": "2020-08-11T04:39:02Z",
        "body": "I agree with you. Throwing a `std::invalid_argument` is a better choice. The pulsar C++ client doesn't use exceptions except for fail-fast, but throwing a `const char*` won't print any message, while throwing an exception would print the message of `what()` method.\r\n\r\nWould you like to submit a PR?"
      },
      {
        "user": "Bklyn",
        "created_at": "2020-08-11T13:02:06Z",
        "body": "@BewareMyPower I will"
      }
    ]
  },
  {
    "number": 7763,
    "title": "Producers are Failing to Create when Connecting to a \"Non-Persistent Partitioned Replicated\" Topic",
    "created_at": "2020-08-05T20:24:15Z",
    "closed_at": "2022-12-09T13:27:52Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/broker",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7763",
    "body": "We have found a bug, where the producers are failing to create when connecting to a \"non-persistent partitioned replicated\" topic. Looks like the replicator fails to start based on the logs we see on the broker. The producer connects fine when it is a \"non-persistent partitioned\" topic but not replicated. The producer also connects fine when it is a \"persistent partitioned replicated\" topics. The issue occurs only when it is a \"non-persistent partitioned replicated\" topic. To make it more clear,\r\n\r\n\"non-persistent partitioned replicated\" >> fails to create producers.\r\n\"non-persistent partitioned non-replicated\" >> successfully creates producers.\r\n\"persistent partitioned replicated\" >>  successfully creates producers.\r\n\"persistent partitioned non-replicated\" >>  successfully creates producers.\r\n\r\n**Logs**\r\n18:19:56.844 [pulsar-io-23-3] WARN  org.apache.pulsar.broker.service.ServerCnx - Failed to get Partitioned Metadata [/127.0.0.1:54694] non-persistent://xxxx/yyyyy/zzzzz: non-persistent://xxxx/yyyyy/zzzzz failed to start replicator for us-west\r\norg.apache.pulsar.broker.service.BrokerServiceException$NamingException: non-persistent://xxxx/yyyyy/zzzzz failed to start replicator for us-west\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7763/comments",
    "author": "Saswatibhoi",
    "comments": [
      {
        "user": "congbobo184",
        "created_at": "2021-01-25T07:05:49Z",
        "body": "I haven't tested anything wrong with the current master branch. can you provide specific steps to reproduce?\r\n\r\nWhen the cluster 1 create non-persistent partitioned topic, one producer look up topic from cluster 2, and the topic haven't replicate from cluster 1 to cluster2, when cluster 2 create one non-partitioned non-persistent topic and found cluster 1 have create this topic by partitioned topic then it will throw this \r\n\r\n```\r\n18:19:56.844 [pulsar-io-23-3] WARN org.apache.pulsar.broker.service.ServerCnx - Failed to get Partitioned Metadata [/127.0.0.1:54694] non-persistent://xxxx/yyyyy/zzzzz: non-persistent://xxxx/yyyyy/zzzzz failed to start replicator for us-west\r\norg.apache.pulsar.broker.service.BrokerServiceException$NamingException: non-persistent://xxxx/yyyyy/zzzzz failed to start replicator for us-west\r\n```\r\n"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T13:27:52Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7742,
    "title": "Allow function instance configuring a different instance classpath",
    "created_at": "2020-08-04T09:30:16Z",
    "closed_at": "2022-12-09T13:29:49Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/function"
    ],
    "url": "https://github.com/apache/pulsar/issues/7742",
    "body": "*Motivation*\r\n\r\nCurrently, the function worker is using the function worker's classpath to configure the function instance (runner)'s classpath. So when the broker (function worker) is using an image that is different from the function instance (runner), the classpath will be wrong and the function instance is not able to load the instance classes.\r\n\r\nExample error messages:\r\n\r\n```\r\n[WARN] /pulsar/conf on functions instance classpath does not exist\r\n[WARN] /pulsar/lib on functions instance classpath does not exist\r\nUsing function root classloader: sun.misc.Launcher$AppClassLoader@7852e922\r\nUsing function instance classloader: java.net.URLClassLoader@42a57993\r\nException in thread \"main\" java.lang.RuntimeException: Class org.apache.pulsar.functions.runtime.JavaInstanceStarter must be in class path\r\n\tat org.apache.pulsar.functions.instance.JavaInstanceMain.createInstance(JavaInstanceMain.java:108)\r\n\tat org.apache.pulsar.functions.instance.JavaInstanceMain.main(JavaInstanceMain.java:93)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.pulsar.functions.runtime.JavaInstanceStarter\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat java.lang.Class.forName0(Native Method)\r\n\tat java.lang.Class.forName(Class.java:348)\r\n\tat org.apache.pulsar.functions.instance.JavaInstanceMain.createInstance(JavaInstanceMain.java:106)\r\n\t... 1 more\r\n```",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7742/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-11-04T10:10:57Z",
        "body": "@wolfstudy Are you interested in working on this issue?"
      },
      {
        "user": "wolfstudy",
        "created_at": "2020-11-05T01:14:34Z",
        "body": "> @wolfstudy Are you interested in working on this issue?\r\n\r\nSure, will try to process it."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-11-17T02:12:55Z",
        "body": "move to 2.8.0 first"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T13:29:49Z",
        "body": "Closed as stale and no one works on it. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7709,
    "title": "support namespace level fuzzy matching",
    "created_at": "2020-07-31T09:40:37Z",
    "closed_at": "2020-09-09T14:49:34Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/7709",
    "body": "We are using Pulsar as a logging service queue to address multiple product and business log output and consumption/alarm issues. We are now using the non-Partiton way to create multiple topics. We now the problem is to maintain a lot of the namespace (different business use different namespace to isolate) so if you create namespace is very much, we need a lot of backend flink pulsar topic data stream to consumption, because of the pulsar is not support namespace level fuzzy matching, the scale is likely to reach thousands of namespace, however, pulsar at the consumer end currently only supports the topic level of regular matching. This will result in a large number of tasks for Flink and high resource consumption. Why not raise the granularity of fuzzy matching to the namespace level?",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7709/comments",
    "author": "bin-albin",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-08-03T01:36:15Z",
        "body": "@bin-albin Is your requirement to support namespace matching in regex subscription?"
      },
      {
        "user": "bin-albin",
        "created_at": "2020-08-03T01:45:42Z",
        "body": "Yes, it does"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-09-09T12:38:45Z",
        "body": "This seems to be a dup to #7846 "
      }
    ]
  },
  {
    "number": 7692,
    "title": "Redolookup on REST API call",
    "created_at": "2020-07-29T13:58:15Z",
    "closed_at": "2021-03-16T11:31:54Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/proxy"
    ],
    "url": "https://github.com/apache/pulsar/issues/7692",
    "body": "**Describe the bug**\r\nWhen calling HTTP PUT on compaction endpoint I'm having:\r\n\r\n```\r\n...\r\nCaused by: org.apache.pulsar.broker.service.BrokerServiceException$ServiceUnitNotReadyException: Namespace bundle for topic (persistent://orga_89c3e460-5ea2-4729-9eed-9e871fa73afc/pulsar_f29f1ee2-99ad-45fb-b6ed-024809f91f70/webhook-wildmoka-7) not served by this instance. Please redo the lookup. Request is denied: namespace=orga_89c3e460-5ea2-4729-9eed-9e871fa73afc/pulsar_f29f1ee2-99ad-45fb-b6ed-024809f91f70\r\n        at org.apache.pulsar.broker.service.BrokerService.checkTopicNsOwnership(BrokerService.java:1372)\r\n        at org.apache.pulsar.broker.service.BrokerService.loadOrCreatePersistentTopic(BrokerService.java:955)\r\n        at org.apache.pulsar.broker.service.BrokerService.lambda$getTopic$13(BrokerService.java:732)\r\n        at org.apache.pulsar.common.util.collections.ConcurrentOpenHashMap$Section.put(ConcurrentOpenHashMap.java:277)\r\n        at org.apache.pulsar.common.util.collections.ConcurrentOpenHashMap.computeIfAbsent(ConcurrentOpenHashMap.java:130)\r\n        at org.apache.pulsar.broker.service.BrokerService.getTopic(BrokerService.java:731)\r\n        at org.apache.pulsar.broker.service.BrokerService.getTopicIfExists(BrokerService.java:711)\r\n        at org.apache.pulsar.broker.\r\n```\r\n\r\nIsn't the proxy that has to lookup for next broker by itself?\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7692/comments",
    "author": "KannarFr",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-07-30T01:47:59Z",
        "body": "@KannarFr Currently Proxy doesn't retry any requests. It only forwards requests to brokers."
      },
      {
        "user": "rudy2steiner",
        "created_at": "2020-08-02T15:57:47Z",
        "body": "Similar issue #7604"
      },
      {
        "user": "rudy2steiner",
        "created_at": "2020-08-12T02:23:47Z",
        "body": "@KannarFr  which service/method you call and how to reproduce, maybe I can help "
      }
    ]
  },
  {
    "number": 7660,
    "title": "Unable to correctly pass JSON object as inline message via CLI pulsar-client",
    "created_at": "2020-07-25T00:46:28Z",
    "closed_at": "2022-12-06T09:56:38Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/cli",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7660",
    "body": "**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Produce message via:\r\n`bin/pulsar-client produce persistent://public/default/testInput -m '{ \"test1\" : \"value1\", \"test2\": \"value2\" }'`\r\nAlso try it like this:\r\n`bin/pulsar-client produce persistent://public/default/testInput -m \"{ 'test1' : 'value1', 'test2': 'value2' }\"`\r\nand like this:\r\n`bin/pulsar-client produce persistent://public/default/testInput -m '\"test1\" : \"value1\", \"test2\": \"value2\"'`\r\n\r\n2. Try deserializing the message. In none of the above cases will the object deserialize correctly. \r\n3. \r\nSave the following text into a file named `test.json`:\r\n`{ \"test1\" : \"value1\", \"test2\": \"value2\" }`\r\n4. Produce message via:\r\n`bin/pulsar-client produce persistent://public/default/testInput -f test.json`\r\n5. Message deserializes correctly. \r\n\r\n**Expected behavior**\r\nPassing JSON via inline message should produce the same result as passing JSON via file. \r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7660/comments",
    "author": "devinbost",
    "comments": [
      {
        "user": "devinbost",
        "created_at": "2020-07-25T01:14:03Z",
        "body": "It appears to be splitting the string on the comma into multiple messages. "
      },
      {
        "user": "sijie",
        "created_at": "2020-07-27T01:41:08Z",
        "body": "@devinbost Are you interested in contributing a feature to produce JSON messages to pulsar-client CLI?"
      },
      {
        "user": "devinbost",
        "created_at": "2020-07-28T03:50:41Z",
        "body": "@sijie Yes, I can work on this after I complete my current project. "
      },
      {
        "user": "afedulov",
        "created_at": "2021-07-16T17:35:52Z",
        "body": "@devinbost Any progress on this issue? I see you already seem to have a corresponding fix on your fork?"
      },
      {
        "user": "eolivelli",
        "created_at": "2021-07-16T17:39:55Z",
        "body": "There is an open PR of mine that implements this feature"
      },
      {
        "user": "billysmt",
        "created_at": "2022-07-12T14:16:54Z",
        "body": "I'm also having issues with this.  Is there a planned fix for this in the future?"
      },
      {
        "user": "nicoloboschi",
        "created_at": "2022-07-12T14:25:53Z",
        "body": "You can create a file and use `-f` as workaround @billysmt "
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T09:56:38Z",
        "body": "It's about the separator (default to `,`).\r\n\r\nYou can workaround it with `-f` or `-s`:\r\n\r\n```\r\n./bin/pulsar-client produce persistent://public/default/testInput -m '{ \"test1\" : \"value1\", \"test2\": \"value2\" }' -s \"\\n\"\r\n```"
      }
    ]
  },
  {
    "number": 7657,
    "title": "AUTO_CONSUME with JSON serialized byte array produces string not byte array",
    "created_at": "2020-07-24T18:37:24Z",
    "closed_at": "2020-10-05T22:39:06Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7657",
    "body": "**Describe the bug**\r\nWhen subscribing to a topic with AUTO_CONSUME, where the schema is a JSON serialized class that contains a byte array, Pulsar de-serializes the message value into a string instead of a byte array for the byte array field.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Produce to a topic using the Java client with a JSON serialized Java class that contains a byte array (byte[]).\r\n2. Subscribe to that topic with AUTO_CONSUME\r\n3. Get the value of the message with getValue()\r\n4. Get the raw object of the byte array field with getField(\"name of byte arr field\")\r\n5. Check output of .getClass() on that raw object from step 4. This produces java.lang.String. instead of [B or byte[] (simple name).\r\n\r\n**Expected behavior**\r\nStep 5 should produce [B or byte[] if using simpleName. Pulsar client should not be encoding a byte array and not decode it to produce the same result.\r\n\r\n**Desktop (please complete the following information):**\r\n - Linux\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7657/comments",
    "author": "epsteina16",
    "comments": [
      {
        "user": "zymap",
        "created_at": "2020-09-24T02:30:06Z",
        "body": "@hangc0276 will handle this issue"
      }
    ]
  },
  {
    "number": 7571,
    "title": "Does puslar-admin support lookup partitioned topic?",
    "created_at": "2020-07-17T04:42:49Z",
    "closed_at": "2020-10-26T13:13:46Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/admin"
    ],
    "url": "https://github.com/apache/pulsar/issues/7571",
    "body": "Command  \r\n```shell \r\n$ pulsar-admin topics lookup topic\r\n```\r\ncan only search brokerUrl for non-partitioned topic. \r\nFor partition topic we have to use a loop to search each partition which is not convenient\r\n\r\n```shell\r\n$ for i in $(seq 0 8); do bin/pulsar-admin topics lookup public/default/test-partition-$i; done\r\n```\r\nIs there any way to search all brokerUrls for patitioned topic by pulsar-admin tool? \r\nIf not,  I think we should add support for it.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7571/comments",
    "author": "aloyszhang",
    "comments": [
      {
        "user": "aloyszhang",
        "created_at": "2020-07-17T23:52:30Z",
        "body": "@sijie Any suggestions about this?"
      },
      {
        "user": "sijie",
        "created_at": "2020-07-18T00:14:25Z",
        "body": "That is a great feature to add! +1"
      },
      {
        "user": "aloyszhang",
        "created_at": "2020-07-18T05:29:45Z",
        "body": "I'll work on it."
      }
    ]
  },
  {
    "number": 7556,
    "title": "Possibility to break a state in MultiTopicsConsumerImpl class during subscribe new topics",
    "created_at": "2020-07-16T12:04:41Z",
    "closed_at": "2020-08-06T21:17:02Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7556",
    "body": "**Describe the bug**\r\nIn MultiTopicsConsumerImpl class there is possibility to break an internal state during subscribe topics. This behavior may occur in ```doSubscribeTopicPartitions()``` method, which is invoking during subscribing, for example in ```subscribeAsync()``` method.\r\n\r\nThe MultiTopicsConsumerImpl class stores state about number of partitions for each subscribed topic. This state is keep in the following map: ```protected final ConcurrentHashMap<String, Integer> topics```. There is also another variable which keeps information about topic partitions (```AtomicInteger allTopicPartitionsNumber```), which stores sum of all topic partitions from all subscribed topics. There is also a map of consumers for particular topics.\r\n\r\nIf we are trying to subscribe a new topic, then it is added to topics' map, and then ```allTopicPartitionsNumber``` counter is incrementing:\r\n```\r\nthis.topics.putIfAbsent(topicName, numPartitions);\r\nallTopicPartitionsNumber.addAndGet(numPartitions);\r\n``` \r\nor\r\n```\r\nthis.topics.putIfAbsent(topicName, 1);\r\nallTopicPartitionsNumber.incrementAndGet();\r\n```\r\n\r\nThe new consumer will be also added to consumer's map.\r\n\r\nThe problem may occur, when two _same topics_ (topic names) will be subscribed at _the same time_. In this case in one of the subscribe operation, the topic will be not added to topics' map, because of ```putIfAbsent()``` method, but counter will be always incremented. If we assume that two separate subscription operations are running at the same time, for example due to asynchronous, then both of them may failed, because state will be incorrect:\r\n```\r\nint numTopics = this.topics.values().stream().mapToInt(Integer::intValue).sum();\r\ncheckState(allTopicPartitionsNumber.get() == numTopics,\r\n                    \"allTopicPartitionsNumber \" + allTopicPartitionsNumber.get()\r\n                        + \" not equals expected: \" + numTopics);\r\n```\r\n\r\nThis behavior cause closing internal consumer, which will be also removed from consumer's map and topic will be not subscribed.\r\n\r\nI think this is an unexpected behavior, because if we run such two operations sequentially, then everything would be fine - topic will be subscribed. To sum up, as ```doSubscribeTopicPartitions()``` method is invoked from ```subscribeAsync()``` method, which should be asynchronous, then such behavior is unexpected.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7556/comments",
    "author": "git-enzo",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-07-17T01:54:38Z",
        "body": "@git-enzo Are you interested in sending a pull request for this issue?"
      },
      {
        "user": "sandrzejczak",
        "created_at": "2020-07-29T09:15:35Z",
        "body": "Hi,\r\nI would like to prepare a pull request for this issue."
      }
    ]
  },
  {
    "number": 7551,
    "title": "Function instance/deps class loading mechanism is confused",
    "created_at": "2020-07-16T02:49:10Z",
    "closed_at": "2023-01-21T02:23:14Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7551",
    "body": "**Describe the bug**\r\nFunction if you use fat jar ${PULSAR_HOME}/instance/deps directory does not work. \r\nIf a non fat jar is used, it will cause many problems. \r\nFor example, \r\n1. Use a custom schema structure in the input and output types of the function. This schema structure is in another jar, and it must be put into ${PULSAR_HOME}/lib directory, if the schema modifies the member variables, you must restart functions worker;\r\n2. When creating function, function jar must also be placed in ${PULSAR_HOME}/instance/deps directory can it be used normally. In this way, from create function to run successful, the jar of fuction should have 3 copies: \r\na) function jar directory specified in bin/pulsar admin --jar ${FUNCTION_JAR_PATH}; \r\nb) ${PULSAR_HOME}/download directory;\r\nc) ${PULSAR_HOME}/instance/deps directory;\r\n\r\nwhich is too difficult to manage.\r\n\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7551/comments",
    "author": "xuesongxs",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2023-01-21T02:23:14Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7543,
    "title": "Dead code in MultiTopicsConsumerImpl class - occurs if topic name validation failed during subscribing",
    "created_at": "2020-07-15T11:50:24Z",
    "closed_at": "2020-07-27T04:26:52Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7543",
    "body": "**Describe the bug**\r\n\r\nThe MultiTopicsConsumerImpl class contains two methods named ```subscribeAsync()``` which return ```CompletableFuture<Void>``` as a result.\r\nThe first step in both methods is checking if given topic name is valid using ```topicNameValid()``` method as follow:\r\n\r\n```\r\nif (!topicNameValid(topicName)) {\r\n    return FutureUtil.failedFuture(\r\n        new PulsarClientException.AlreadyClosedException(\"Topic name not valid\"));\r\n}\r\n```\r\n\r\nThe problem is that ```topicNameValid()``` method always returns true or throws an exception. It never returns false, so code inside \"if\" block is unreachable.\r\n\r\n```\r\n   private boolean topicNameValid(String topicName) {\r\n        checkArgument(TopicName.isValid(topicName), \"Invalid topic name:\" + topicName); // throws Exception\r\n        checkArgument(!topics.containsKey(topicName), \"Topics already contains topic:\" + topicName); // throws Exception\r\n\r\n        return true;\r\n    }\r\n```\r\n\r\n**Expected behavior**\r\nMethod ```topicNameValid()``` should return false if topic validation failed.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7543/comments",
    "author": "git-enzo",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-07-17T01:33:11Z",
        "body": "@git-enzo Are you interested in contributing a bug fix?"
      },
      {
        "user": "315157973",
        "created_at": "2020-07-18T13:04:09Z",
        "body": "I'm free on the weekend, please assign it to me  :smile:"
      }
    ]
  },
  {
    "number": 7538,
    "title": "[Question] How to generate sequenceId for commit marker in partition topic",
    "created_at": "2020-07-15T07:28:27Z",
    "closed_at": "2023-01-21T02:24:00Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7538",
    "body": "### Context\r\n\r\nWhen publishing messages to a topic, the `sequenceId` param is used for topic message deduplication. \r\n\r\nIn the transaction, when the TC receives commit command, the TC needs to write a commit marker append to the topic partition, the commit marker needs a `sequenceId` to avoid duplication, if the commit marker is duplicated the topic partition cursor will send duplication committed messages to the consumer, or we need to find some other approaches to avoid duplication.\r\n\r\nRefer to #2664 ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7538/comments",
    "author": "gaoran10",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-07-17T01:23:48Z",
        "body": "Since the commit is initiated by TC and done by TB. TB first writes a `committing` record to the transaction buffer. You can use the message-id of the `committing` record to create a long number. This can be used as the sequence id to write the commit marker to the data partition."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-01-21T02:24:00Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7505,
    "title": "start/stop function fail",
    "created_at": "2020-07-10T15:15:33Z",
    "closed_at": "2023-01-21T02:24:46Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/function",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7505",
    "body": "**Describe the bug**\r\nAfter creating functions, the function runs normally. Stop function starts function again. The following error is reported in the function log. Update function is invalid. You must delete function and then create function to run normally. Function jar is a fat jar, pulsar version is 2.5.2.\r\n\r\n\r\n00:00:00.539 [public/default/streamPulsarBcn-80-SendThread(10.32.211.206:31180)] INFO  org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /xxx.xxx.xxx.xxx:53231, server: xxx.xxx.xxx.xxx/xxx.xxx.xxx:31180\r\n00:00:06.321 [public/default/streamPulsarBcn-80-SendThread(xxx.xxx.xxx.xxx:31180)] WARN  org.apache.zookeeper.ClientCnxn - Session 0x2730982e3a7024c for server xxx.xxx.xxx.xxx/xxx.xxx.xxx.xxx:31180, unexpected error, closing socket connection and attempting reconnect\r\njava.lang.NoClassDefFoundError: org/apache/zookeeper/proto/SetWatches\r\n\tat org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:906) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:352) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1118) [streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\r\n\r\norg.apache.dubbo.remoting.RemotingException: Failed connect to server /xxx.xxx.xxx.xxx:18005 from NettyClient xxx.xxx.xxx.xxx using dubbo version 1.0-SNAPSHOT, cause: io/netty/channel/AbstractChannelHandlerContext$13\r\n\tat org.apache.dubbo.remoting.transport.AbstractClient.connect(AbstractClient.java:211) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.remoting.transport.AbstractClient.reconnect(AbstractClient.java:246) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.remoting.exchange.support.header.HeaderExchangeClient.reconnect(HeaderExchangeClient.java:155) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.remoting.exchange.support.header.ReconnectTimerTask.doTask(ReconnectTimerTask.java:49) [streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.remoting.exchange.support.header.AbstractTimerTask.run(AbstractTimerTask.java:87) [streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.common.timer.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:648) [streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.common.timer.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:727) [streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.common.timer.HashedWheelTimer$Worker.run(HashedWheelTimer.java:449) [streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]\r\nCaused by: java.lang.NoClassDefFoundError: io/netty/channel/AbstractChannelHandlerContext$13\r\n\tat io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:610) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:465) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat io.netty.channel.DefaultChannelPipeline.close(DefaultChannelPipeline.java:1003) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat io.netty.channel.AbstractChannel.close(AbstractChannel.java:238) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.remoting.transport.netty4.NettyClient.doConnect(NettyClient.java:139) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.dubbo.remoting.transport.AbstractClient.connect(AbstractClient.java:190) ~[streamPulsarBcn-1.0-SNAPSHOT.jar:?]\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create function\r\n2. stop function\r\n3. start function\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7505/comments",
    "author": "xuesongxs",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2023-01-21T02:24:46Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7504,
    "title": "split-bundle for public/functions error",
    "created_at": "2020-07-10T14:37:44Z",
    "closed_at": "2023-05-18T03:20:29Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/admin",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7504",
    "body": "**Describe the bug**\r\nerror log in pulsar-broker-xx.log\r\n```\r\n22:16:07.855 [pulsar-web-40-5] ERROR org.apache.pulsar.broker.admin.impl.ClustersBase - [null] Failed to get clusters/pulsar-cluster/namespaceIsolationPolicies\r\norg.apache.pulsar.broker.web.RestException: NamespaceIsolationPolicies for cluster pulsar-cluster does not exist\r\n        at org.apache.pulsar.broker.admin.impl.ClustersBase.lambda$getNamespaceIsolationPolicies$3(ClustersBase.java:486) ~[org.apache.pulsar-pulsar-broker-2.5.2.jar:2.5.2]\r\n        at java.util.Optional.orElseThrow(Optional.java:290) ~[?:1.8.0_251]\r\n        at org.apache.pulsar.broker.admin.impl.ClustersBase.getNamespaceIsolationPolicies(ClustersBase.java:486) ~[org.apache.pulsar-pulsar-broker-2.5.2.jar:2.5.2]\r\n        at sun.reflect.GeneratedMethodAccessor123.invoke(Unknown Source) ~[?:?]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_251]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_251]\r\n        at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76) ~[org.glassfish.jersey.core-jersey-server-2.27.jar:?]\r\n        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148) [org.glassfish.jersey.core-jersey-server-2.27.jar:?]\r\n        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191) [org.glassfish.jersey.core-jersey-server-2.27.jar:?]\r\n        at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:243) [org.glassfish.jersey.core-jersey-server-2.27.jar:?]\r\n        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethoSocket error Event: 32 Error: 10053..jersey.core-jersey-server-2.27.jar:?]\r\n```\r\n\r\npulsar 2.5.2\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create a function\r\n2. bin/pulsar-admin namespaces split-bundle -b 0x40000000_0x80000000 -u public/functions\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7504/comments",
    "author": "xuesongxs",
    "comments": [
      {
        "user": "zymap",
        "created_at": "2020-09-23T11:35:16Z",
        "body": "which version of pulsar? I am not see any request for the NamespaceIsolationPolicies. Could you please provide more information?"
      },
      {
        "user": "xuesongxs",
        "created_at": "2020-09-24T00:52:13Z",
        "body": "> which version of pulsar? I am not see any request for the NamespaceIsolationPolicies. Could you please provide more information?\r\n\r\npulsar 2.5.2"
      },
      {
        "user": "tisonkun",
        "created_at": "2023-05-18T03:20:29Z",
        "body": "Closed as stale. Please open a new issue if it is still relevant to maintained versions."
      }
    ]
  },
  {
    "number": 7492,
    "title": "PulsarStandaloneBuilder does not use a custom configuration set with withConfig()",
    "created_at": "2020-07-09T15:39:49Z",
    "closed_at": "2022-12-10T04:59:21Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7492",
    "body": "**Describe the bug**\r\n`PulsarStandaloneBuilder` does not use a custom configuration set with `withConfig()`. It's clearly visible in the code:\r\n```java\r\n    public PulsarStandalone build() {\r\n        ServiceConfiguration config = new ServiceConfiguration();\r\n        config.setClusterName(\"standalone\");\r\n        pulsarStandalone.setConfig(config);\r\n        ...\r\n```\r\n At the beginning of `PulsarStandaloneBuilder.build()` a new `ServiceConfiguration` instance is created and set with `pulsarStandalone.setConfig()` overwriting any configuration instance set earlier with `PulsarStandaloneBuilder.withConfig()`.\r\n\r\nAdditionally unlike in `PulsarStandaloneStarter` the `PulsarStandaloneBuilder` does not evaluate the configuration file for the `ServiceConfiguration`, so only options for `LocalBookkeeperEnsemble` (via the `ServerConfiguration` class evaluation in `PulsarStandalone.start()`) will be read from the configuration file, all other (e.g. `managedLedgerDefaultEnsembleSize`) will be ignored.\r\n\r\n**Expected behavior**\r\n`PulsarStandaloneBuilder` uses a custom configuration if one is set with `withConfig()` and should create a new `ServiceConfiguration` instance only if no custom configuration was set.\r\n\r\nThe configuration file is evaluated when `PulsarStandalone` is initialized by `PulsarStandaloneBuilder`.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7492/comments",
    "author": "horsteff",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-07-10T01:58:18Z",
        "body": "@horsteff Are you interested in contributing a bug fix?"
      },
      {
        "user": "horsteff",
        "created_at": "2020-07-10T16:35:25Z",
        "body": "@sijie Yes, that's what I had in mind. I'm already working on it and will make a pull request then.\r\n\r\nOne question: After digging a little in the code and thinking about possible fixes I would rather like to drop the `withConfig` method from `PulsarStandaloneBuilder` and replace it with a `withConfigFile` method. I don't see any other way (without major changes), to apply configuration settings without unexpected overwriting and in an order a user would expect. A config file is required in any case and with a `withConfigFile` method the `PulsarStandaloneBuilder` is able to read it in it's `build` method like the `PulsarStandaloneStarter` constructor does, which would fix the second part of this issue. If a user wants to change some config settings by code, it's possible by modifying the `ServiceConfiguration` object of the created `PulsarStandalone` instance after calling `PulsarStandaloneBuilder.build()` and before calling `PulsarStandalone.start()` without the need to create an own `ServiceConfiguration` instance. This would also make clear to the user, that modifying the config object overwrites values set via configuration file (at least those not for the `LocalBookkeeperEnsemble`, but that's another story).\r\nDropping `PulsarStandaloneBuilder.withConfig` shouldn't be such a problem with existing code as currently it effectively does nothing. And it seems that no other user has noticed that. 8)\r\nIs this solution ok for you?\r\n"
      },
      {
        "user": "sijie",
        "created_at": "2020-07-14T02:05:39Z",
        "body": "@horsteff that sounds a good solution to me."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-10T04:59:21Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7478,
    "title": "pulsar-admin reset cursor hangs ",
    "created_at": "2020-07-08T08:50:34Z",
    "closed_at": "2022-12-06T09:48:19Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/cli",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7478",
    "body": "**Describe the bug**\r\nthe thread stack below is one of pulsar-web thread pool.\r\n```java\r\n\"pulsar-web-54-19\" #450 prio=5 os_prio=0 tid=0x00007fcbb40b7000 nid=0x1db1 waiting on condition [0x00007fc946584000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n        at sun.misc.Unsafe.park(Native Method)\r\n        - parking to wait for  <0x000000070021c320> (a java.util.concurrent.CompletableFuture$Signaller)\r\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n        at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)\r\n        at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)\r\n        at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)\r\n        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\r\n        at org.apache.pulsar.broker.admin.impl.PersistentTopicsBase.internalResetCursorOnPosition(PersistentTopicsBase.java:1802)\r\n        at org.apache.pulsar.broker.admin.v2.PersistentTopics.resetCursorOnPosition(PersistentTopics.java:890)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)\r\n        at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$$Lambda$300/1032656803.invoke(Unknown Source)\r\n        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)\r\n        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)\r\n        at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$VoidOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:183)\r\n        at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)\r\n        at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)\r\n        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)\r\n        at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)\r\n        at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)\r\n        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)\r\n        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)\r\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:316)\r\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:298)\r\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:268)\r\n        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)\r\n        at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)\r\n        at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)\r\n        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)\r\n        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)\r\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)\r\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)\r\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)\r\n        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:852)\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n        at org.apache.pulsar.broker.web.ResponseHandlerFilter.doFilter(ResponseHandlerFilter.java:53)\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1591)\r\n        at org.apache.pulsar.broker.intercept.BrokerInterceptor$BrokerInterceptorDisabled.onWebServiceRequest(BrokerInterceptor.java:73)\r\n        at org.apache.pulsar.broker.web.EventListenerFilter.doFilter(EventListenerFilter.java:46)\r\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1591)\r\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:542)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1581)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1307)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:482)\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1549)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1204)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:221)\r\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\r\n        at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:173)\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:494)\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:374)\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:268)\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\r\n        at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n   Locked ownable synchronizers:\r\n        - <0x000000070021c950> (a java.util.concurrent.ThreadPoolExecutor$Worker)\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7478/comments",
    "author": "ltamber",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-07-08T14:11:06Z",
        "body": "@ltamber Are you interested in fixing this problem? looks the problem is `sub.resetCursor(PositionImpl.get(messageId.getLedgerId(), messageId.getEntryId())).get();` can't complete and there are no timeout the the `future.get()`. It's better to change all logic of the reset cursor to async."
      },
      {
        "user": "jiawulin001",
        "created_at": "2020-07-14T03:47:46Z",
        "body": "@codelipenghui \r\nI'm thinking if we should introduce a timeout mechanism to avoid this problem instead of changing the logic to async? Since asynchronously resetting the cursor might return a response directly before it actually finish the work, and may influence the order of code execution on client side and lead to unexpected outcome."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T09:48:19Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7470,
    "title": "Broker url getting set to canonical host name rather than internal advertised listener",
    "created_at": "2020-07-07T13:55:36Z",
    "closed_at": "2020-08-24T02:33:57Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/7470",
    "body": "**Describe the bug**\r\nThe broker url for a broker will be set to whatever the advertisedAddress and brokerServicePort are set to in the config when I only use a single advertised listener. With pulsar 2.6.0 and trying to use the advertisedListeners list, the broker url is being set to the canonical host name instead of the internalListenerName or at least one of the advertisedListeners in the provided list like I would expect it to.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set the advertisedAddress and brokerServicePort to whatever is needed\r\n2. Start pulsar and see that the broker url matches these two properties combined\r\n3. Use the advertisedListeners and internalListenerName properties to set the advertised addresses\r\n4. Start pulsar and see that the broker url is not any of the values in the provided list but instead is set to the canonical host name\r\n\r\n**Expected behavior**\r\nI would think that the broker url with multiple advertised listeners would be set to whatever the internalListenerName or at least one of the entries in the listeners list instead of the canonical host name.\r\n\r\n**Desktop (please complete the following information):**\r\n - Windows 10 with WSL 1\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7470/comments",
    "author": "meleagle",
    "comments": [
      {
        "user": "zymap",
        "created_at": "2020-08-20T10:20:55Z",
        "body": "Hi, @meleagle. I can not reproduce this issue on my laptop. Could you please give me more information? Such as log or something else?"
      },
      {
        "user": "meleagle",
        "created_at": "2020-08-20T15:57:11Z",
        "body": "Hello @zymap, The logs weren't helpful for me to look at as there was no errors but I can give an example of the properties I set to get to the issue.\r\n\r\nIn broker.conf I would clear out the brokerServicePort and AdvertisedAddress fields.\r\n\r\nThen I would set the advertisedListeners and internalListenerName to something like below (with valid ip addressess in the real case).\r\n\r\nadvertisedListeners=internal:pulsar://0.0.0.0:6650,external:pulsar://1.1.1.1:6660\r\ninternalListenerName=internal\r\n\r\nSo when I would start pulsar up normally with just using the advertisedAddress and the brokerServicePort I would see a line like below in the logs where the broker url was the combination of the two which is what I expected.\r\n\r\n```\r\n15:46:33.806 [main] INFO  org.apache.pulsar.broker.PulsarService - messaging service is ready, bootstrap service port = 8080, broker url= pulsar://0.0.0.0:6650\r\n```\r\n\r\nBut when I used the advertisedListeners and internalListenerName fields, instead of the broker url being the ip:port combination that is expected from the advertisedListeners list it was getting resolved to the instances hostname instead.\r\n"
      },
      {
        "user": "zymap",
        "created_at": "2020-08-24T02:26:37Z",
        "body": "Hi @meleagle, I configure the advertisedListeners and internalListerName:\r\n```\r\nadvertisedListeners=internal:pulsar://192.168.3.2:6650\r\ninternalListenerName=internal\r\n```\r\nAfter starting the broker service, I can see the following log:\r\n\r\n```\r\n10:19:57.600 [main] INFO  org.apache.pulsar.broker.PulsarService - messaging service is ready, bootstrap service port = 8080, broker url= pulsar://192.168.3.2:6650,\r\n```\r\n\r\nSorry, I can't see the issue."
      },
      {
        "user": "meleagle",
        "created_at": "2020-08-24T02:33:57Z",
        "body": "@zymap No problem, thanks for looking into it."
      }
    ]
  },
  {
    "number": 7450,
    "title": "Why start script does not contain gc log ",
    "created_at": "2020-07-05T00:58:35Z",
    "closed_at": "2020-11-13T02:26:20Z",
    "labels": [
      "help wanted",
      "area/config"
    ],
    "url": "https://github.com/apache/pulsar/issues/7450",
    "body": "It is found that there is no gc log in the start script of Pulsar. Could you please tell me the trade-off?",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7450/comments",
    "author": "hezhangjian",
    "comments": [
      {
        "user": "wolfstudy",
        "created_at": "2020-07-05T04:16:28Z",
        "body": "> It is found that there is no gc log in the start script of Pulsar. Could you please tell me the trade-off?\r\n\r\nIt may just not expose the setting of JVM here, if you are interested, please send a pull request to fix this issue."
      }
    ]
  },
  {
    "number": 7441,
    "title": "Perf client should produce and consume AVRO objects",
    "created_at": "2020-07-03T05:04:18Z",
    "closed_at": "2023-01-21T02:25:16Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7441",
    "body": "*Motivation*\r\n\r\nWe should use perf tool to benchmark producing and consuming messages using Schema.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7441/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "zymap",
        "created_at": "2020-09-24T02:39:41Z",
        "body": "@hangc0276 will handle this issue."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-01-21T02:25:01Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      },
      {
        "user": "tisonkun",
        "created_at": "2023-01-21T02:25:16Z",
        "body": "IIRC it's implemented now."
      }
    ]
  },
  {
    "number": 7437,
    "title": "Pulsar Admin Java Client leaves hanging thread after close called",
    "created_at": "2020-07-02T19:07:49Z",
    "closed_at": "2020-07-29T06:34:11Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/client",
      "area/cli"
    ],
    "url": "https://github.com/apache/pulsar/issues/7437",
    "body": "**Describe the bug**\r\nAfter closing a Java pulsar admin instance, a delayer thread is left hanging in the background in a TIMED_WAITING state that can prevent the parent program from ending.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Create a pulsar admin instance.\r\n2. Add a new topic using the pulsar admin.\r\n3. Close the pulsar admin.\r\n4. Do a java thread dump to see the hanging delayer thread.\r\n\r\n**Expected behavior**\r\nCalling close should free and end all threads created by the PulsarAdmin. This includes all AsyncHttp related threads.\r\n\r\n**Desktop (please complete the following information):**\r\n- Linux",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7437/comments",
    "author": "epsteina16",
    "comments": [
      {
        "user": "rushsky518",
        "created_at": "2020-07-03T07:46:49Z",
        "body": "in `org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector`\r\nset daemon = true can solve this problem?\r\n```java\r\n    private final ScheduledExecutorService delayer = Executors.newScheduledThreadPool(1,\r\n            new DefaultThreadFactory(\"delayer\", true));\r\n````"
      },
      {
        "user": "BewareMyPower",
        "created_at": "2020-07-03T12:44:39Z",
        "body": "The reason is executors that were created by `delayer` to run timeout task were still running. Changing these threads to daemon is a way to solve the problem. Calling `delayer.shutdownNow()` in `AsyncHttpConnector.close()` method also works.\r\n\r\nWould you like to push a PR? @rushsky518 "
      },
      {
        "user": "rushsky518",
        "created_at": "2020-07-04T01:51:47Z",
        "body": "i'd like to make a contribution @BewareMyPower"
      }
    ]
  },
  {
    "number": 7419,
    "title": "Expose pulsar's metrics in topics",
    "created_at": "2020-07-01T15:16:09Z",
    "closed_at": "2021-03-16T11:31:30Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/7419",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nMetrics fetch is currently using prometheus endpoint that exposes metrics. External metrics collector should now poll this endpoint.\r\n\r\n**Describe the solution you'd like**\r\nWhat do you think about enable pulsar components to directly send their metrics to configuration defined topics?\r\n\r\nMaybe we can imagine `metricstenant/brokers/<broker-name>` pattern.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7419/comments",
    "author": "KannarFr",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-07-02T02:46:42Z",
        "body": "thanks @KannarFr for open this issue. this is a good feature. mark it as help wanted for anyone interest to contribute to it."
      },
      {
        "user": "KannarFr",
        "created_at": "2020-07-02T10:44:53Z",
        "body": "@jiazhai I'm working on, I will soon add a PR and we will discuss about it :)."
      }
    ]
  },
  {
    "number": 7398,
    "title": "possible inconsistencies en broker local metadata cache",
    "created_at": "2020-06-30T14:54:33Z",
    "closed_at": "2021-03-16T11:31:21Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7398",
    "body": "**Describe the bug**\r\nNot sure how to to describe it but:\r\n\r\nHad the following stack looks like inconsistencies on a broker local metada that prevent namespace deletion. The namespace was considered empty by pulsar-admin-cli, but when I tried to run deletion one of the broker logged the following stack.\r\n\r\nI just restarted the broker and everything worked well. I could run the deletion instantly.\r\n\r\n```\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: 14:42:29.710 [bookkeeper-ml-workers-OrderedExecutor-6-0] WARN  org.apache.pulsar.broker.service.persistent.PersistentTopic - [persistent://user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/test] Inactive topic deletion failed\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: java.util.concurrent.CompletionException: org.apache.pulsar.broker.service.BrokerServiceException$PersistenceException: org.apache.bookkeeper.mledger.ManagedLedgerException$MetaStoreException: java.util.concurrent.CompletionException: org.apache.pulsar.metadata.api.MetadataStoreException$NotFoundException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /managed-ledgers/user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/persistent/test\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:911) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:899) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.pulsar.broker.service.persistent.PersistentTopic$4.deleteLedgerFailed(PersistentTopic.java:914) ~[org.apache.pulsar-pulsar-broker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl$18.operationFailed(ManagedLedgerImpl.java:2333) ~[org.apache.pulsar-managed-ledger-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.bookkeeper.mledger.impl.MetaStoreImpl.lambda$null$21(MetaStoreImpl.java:218) ~[org.apache.pulsar-managed-ledger-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.bookkeeper.util.SafeRunnable$1.safeRun(SafeRunnable.java:43) [org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.bookkeeper.common.util.SafeRunnable.run(SafeRunnable.java:36) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.bookkeeper.common.util.OrderedExecutor$TimedRunnable.run(OrderedExecutor.java:203) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [io.netty-netty-common-4.1.48.Final.jar:4.1.48.Final]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.lang.Thread.run(Thread.java:748) [?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: Caused by: org.apache.pulsar.broker.service.BrokerServiceException$PersistenceException: org.apache.bookkeeper.mledger.ManagedLedgerException$MetaStoreException: java.util.concurrent.CompletionException: org.apache.pulsar.metadata.api.MetadataStoreException$NotFoundException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /managed-ledgers/user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/persistent/test\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  ... 10 more\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: Caused by: org.apache.bookkeeper.mledger.ManagedLedgerException$MetaStoreException: java.util.concurrent.CompletionException: org.apache.pulsar.metadata.api.MetadataStoreException$NotFoundException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /managed-ledgers/user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/persistent/test\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: Caused by: java.util.concurrent.CompletionException: org.apache.pulsar.metadata.api.MetadataStoreException$NotFoundException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /managed-ledgers/user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/persistent/test\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:647) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_192]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.pulsar.metadata.impl.zookeeper.ZKMetadataStore.lambda$null$16(ZKMetadataStore.java:238) ~[org.apache.pulsar-pulsar-metadata-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  ... 4 more\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: Caused by: org.apache.pulsar.metadata.api.MetadataStoreException$NotFoundException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /managed-ledgers/user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/persistent/test\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.pulsar.metadata.impl.zookeeper.ZKMetadataStore.getException(ZKMetadataStore.java:268) ~[org.apache.pulsar-pulsar-metadata-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.pulsar.metadata.impl.zookeeper.ZKMetadataStore.lambda$null$16(ZKMetadataStore.java:238) ~[org.apache.pulsar-pulsar-metadata-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  ... 4 more\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]: Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /managed-ledgers/user_7684cfc9-f54e-4e09-848c-1953af6e3e89/pulsar_c61f4274-4725-4e9f-8196-3cd7edcd77e5/persistent/test\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.zookeeper.KeeperException.create(KeeperException.java:118) ~[org.apache.pulsar-pulsar-zookeeper-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[org.apache.pulsar-pulsar-zookeeper-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.pulsar.metadata.impl.zookeeper.ZKMetadataStore.getException(ZKMetadataStore.java:262) ~[org.apache.pulsar-pulsar-metadata-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  at org.apache.pulsar.metadata.impl.zookeeper.ZKMetadataStore.lambda$null$16(ZKMetadataStore.java:238) ~[org.apache.pulsar-pulsar-metadata-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\r\nJun 30 14:42:29 yo-pulsar-c1-n3 pulsar-broker[10935]:  ... 4 more\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7398/comments",
    "author": "KannarFr",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-07-02T02:12:26Z",
        "body": "Thanks @KannarFr for open this issue. Are there steps to reproduce this issue? "
      },
      {
        "user": "KannarFr",
        "created_at": "2020-07-02T10:44:27Z",
        "body": "@jiazhai I have no reproductible case :/. It occurs on our production randomly on a namespace deletion."
      },
      {
        "user": "shulaoh",
        "created_at": "2021-09-10T02:02:47Z",
        "body": "Just wondering why this issue is closed, is it harmless or fixed indirectly?\r\nI hit this issue by using pulsar as flink source."
      }
    ]
  },
  {
    "number": 7366,
    "title": "Redundant exceptions in broker logs",
    "created_at": "2020-06-26T04:10:05Z",
    "closed_at": "2022-12-07T02:55:52Z",
    "labels": [
      "type/bug",
      "type/enhancement",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/7366",
    "body": "Our broker logs are filled with the exception below. That namespace indeed doesn't exists, and there is probably a \"bad\" consumer/publisher or a \"bad\" routing function. But still, those exceptions do not provide much useful info and just fill up the logs. If possible it would be great not to print them, or at least not to print the full exception stack.\r\n\r\n\r\n```\r\n2020-06-26 06:54:11 | 03:54:11.568 [ForkJoinPool.commonPool-worker-0] ERROR org.apache.pulsar.broker.web.PulsarWebResource - Policies not found for cve_updater_1592900096237_ABCD/risk namespace\r\n-- | --\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) [?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402) [?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at org.apache.pulsar.zookeeper.ZooKeeperCache.lambda$16(ZooKeeperCache.java:362) ~[org.apache.pulsar-pulsar-zookeeper-utils-2.5.2.jar:2.5.2]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at org.apache.pulsar.zookeeper.ZooKeeperCache.lambda$18(ZooKeeperCache.java:377) ~[org.apache.pulsar-pulsar-zookeeper-utils-2.5.2.jar:2.5.2]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at org.apache.pulsar.zookeeper.ZooKeeperDataCache.lambda$0(ZooKeeperDataCache.java:68) ~[org.apache.pulsar-pulsar-zookeeper-utils-2.5.2.jar:2.5.2]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_252]\r\n  |   | 2020-06-26 06:54:11 | at org.apache.pulsar.broker.web.PulsarWebResource.lambda$checkLocalOrGetPeerReplicationCluster$4(PulsarWebResource.java:697) ~[org.apache.pulsar-pulsar-broker-2.5.2.jar:2.5.2]\r\n  |   | 2020-06-26 06:54:11 | org.apache.pulsar.broker.web.RestException: Policies not found for cve_updater_1592900096232_ABCD/risk namespace\r\n  |   | 2020-06-26 06:54:11 | 03:54:11.567 [ForkJoinPool.commonPool-worker-1] WARN  org.apache.pulsar.broker.service.ServerCnx - Failed to get Partitioned Metadata [/10.128.22.46:52250] persistent://cve_updater_1592900096232_ABCD/risk/0: Policies not found for cve_updater_1592900096232_ABCD/risk namespace\r\n\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7366/comments",
    "author": "trexinc",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-07T02:55:52Z",
        "body": "Closed as stale and no one worked on it. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 7280,
    "title": "Support zero queue consumer for partitioned topic.",
    "created_at": "2020-06-15T16:17:59Z",
    "closed_at": "2022-12-09T13:00:10Z",
    "labels": [
      "help wanted",
      "area/client",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/7280",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently, the zero queue consumer just can subscribe to a non-partitioned topic. In some case, we need to use zero queue consumer to subscribe to a partitioned topic. ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7280/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "merlimat",
        "created_at": "2020-06-15T16:28:50Z",
        "body": "It’s not enabled because we cannot know which partition the next message will be coming from. Any suggestion on how to achieve that?\r\n\r\n"
      },
      {
        "user": "sundar-10",
        "created_at": "2020-06-17T13:52:12Z",
        "body": "Hello, I am planning to work on this issue. I am planning to read how zero queue consumer is subscribed to a nonpartitioned topic and proceed from there by getting some kind of input for which partition this consumer should be subscribed to and so on. Can you tell me where to find the code related to this?"
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-05T12:38:37Z",
        "body": "@sundar-10 You can check the `ZeroQueueConsumerImpl`."
      },
      {
        "user": "codelipenghui",
        "created_at": "2021-03-05T12:52:38Z",
        "body": "@merlimat How about syncing the backlog between the broker and client periodically. Treat topics with more backlogs as a high priority. Since we don't know if a topic will there be any messages written, to avoid a consumer block to one topic(no more data are written), we can reset the permits in a sync cycle if there are topics that have more backlog, looks like \r\n\r\n1. reset permits to 0 \r\n2. re-order the consumers \r\n3. trigger read data from the high priority consumer\r\n\r\nUsers can achieve the desired purpose by setting the frequency of synchronization.\r\n\r\nWe can optimize the sync mechanism, e.g. carry the backlog when dispatching messages. Only if the message is not dispatched for a period of time, perform a separate sync command."
      },
      {
        "user": "leizhiyuan",
        "created_at": "2022-04-06T01:37:01Z",
        "body": "what is the current status? "
      }
    ]
  },
  {
    "number": 7168,
    "title": "regex subscription not working for new topics in Python",
    "created_at": "2020-06-04T10:11:57Z",
    "closed_at": "2020-06-09T18:07:27Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7168",
    "body": "**Describe the bug**\r\nIf a new topic is added that matches a regex subscription then if the client is written in Java it detects it and adds a cursor but if it is written in Python it does not.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nRun the following as a Phython3 client:\r\n```\r\n#!/usr/bin/python3\r\nimport pulsar\r\nimport re\r\n\r\nclient = pulsar.Client(\"pulsar://localhost:6650\")\r\ninitial_position=pulsar.InitialPosition.Earliest )\r\nconsumer = client.subscribe(re.compile('.*'), subscription_name='my-sub' )\r\n\r\nwhile True:\r\n    msg = consumer.receive()\r\n    print(\"Received message '%s'\" % msg.data())\r\n    consumer.acknowledge(msg)\r\n\r\nclient.close()\r\n```\r\n\r\nIn another window run this command to get a Java client for comparison:\r\n```\r\n/opt/pulsar/bin/pulsar-client consume --regex '.*' -s all -n 0\r\n```\r\n\r\nIn another window send a message to a new topic:\r\n```\r\n/opt/pulsar//bin/pulsar-client produce addtopic -m 'm1'\r\n```\r\nWait a minute for the clients to detect the new topic.\r\nSend another message to the same topic.\r\n```\r\n/opt/pulsar//bin/pulsar-client produce addtopic -m 'm2'\r\n```\r\nThe Java client receives the message but the Python one does not.\r\n\r\nKill and restart the clients. Send a message to the same topic.\r\n```\r\n/opt/pulsar//bin/pulsar-client produce addtopic -m 'm3'\r\n```\r\nBoth clients receive the message.\r\n\r\n**Expected behavior**\r\nBoth clients should receive the second message.\r\n\r\n**Screenshots**\r\nNA\r\n\r\n**Desktop (please complete the following information):**\r\n Centos 7\r\nPulsar 2.5.1\r\nPython pulsar-client 2.5.2\r\n\r\n**Additional context**\r\n`initial_position=pulsar.InitialPosition.Earliest` does not help\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7168/comments",
    "author": "baynes",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-06-05T01:49:28Z",
        "body": "Thanks @baynes for reporting this issue. Since it is able to reproduce, mark it as help-wanted firstly. Any help on fix this issue is very appreciated. "
      },
      {
        "user": "BewareMyPower",
        "created_at": "2020-06-07T19:41:58Z",
        "body": "TL;DR It's because there's a deadlock in C++ client. I'll push a PR soon.\r\n\r\nJust because C++ stand library doesn't have something like `ConcurrentHashMap`, the current implementation used a `mutex_` to share a `std::map`.\r\nHowever, the `mutex_` is also acquired in `receive`, so if you called `receive`, the `mutex_` is acquired and held until a new message was pushed to the internal message queue. Then if the topic discovery timer found new topics, the callback to add new topics also needed to acquire the `mutex_`, which leads to a deadlock.\r\n\r\nThe deadlock problem is similar to my PR before, see the change of `PartitionedConsumerImpl::receive` in #6732. "
      }
    ]
  },
  {
    "number": 7049,
    "title": "Make WebSocketService.MaxTextFrameSize configurable",
    "created_at": "2020-05-26T16:44:00Z",
    "closed_at": "2020-06-04T02:33:26Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/websocket"
    ],
    "url": "https://github.com/apache/pulsar/issues/7049",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWe use the websocket interface to pulsar. Our current use case demands the transmission of big messages (10MB or even more), which is not possible due to the hardcoded limit in `WebSocketService`\r\n\r\n**Describe the solution you'd like**\r\nMake this value configurable, e.g. via websocket.conf\r\n\r\n**Describe alternatives you've considered**\r\nMaybe increase the hardcoded default.\r\n\r\n**Additional context**\r\nNone.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7049/comments",
    "author": "d-eggert",
    "comments": [
      {
        "user": "zhanghaou",
        "created_at": "2020-05-28T00:38:14Z",
        "body": "Thanks for your feedback, I will fix it."
      }
    ]
  },
  {
    "number": 7033,
    "title": "Ledger dir is redundant when using an existing bookkeeper cluster",
    "created_at": "2020-05-25T09:33:38Z",
    "closed_at": "2020-06-10T04:48:26Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/7033",
    "body": "**Describe the bug**\r\nCurrently, we can use an existing bookkeeper cluster by parameter `bookkeeperMetadataServiceUri` (based master branch), however, the ledger dir (`/ledgers`) is still created in pulsar metadata store when initialize cluster metadata. This may confuse some users when trying to locate bookie nodes.\r\n\r\n**Expected behavior**\r\nCreate the ledger dir when necessary.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/7033/comments",
    "author": "ayolivia",
    "comments": [
      {
        "user": "zhanghaou",
        "created_at": "2020-05-31T13:26:37Z",
        "body": "I will fix it. :)"
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-06-01T00:28:40Z",
        "body": "Thanks @zhanghaou "
      },
      {
        "user": "murong00",
        "created_at": "2020-06-01T01:17:59Z",
        "body": "@codelipenghui @zhanghaou  I think we should make a discussion, I hava already fixed this in my dev branch by adding a option `--bookkeeper-metadata-service-uri` some weeks ago, but I am not so sure it is appropriate. How about the proposal?"
      }
    ]
  },
  {
    "number": 6975,
    "title": "topic publish rate limit not take effect",
    "created_at": "2020-05-18T07:16:40Z",
    "closed_at": "2020-06-02T02:38:28Z",
    "labels": [
      "type/enhancement",
      "help wanted",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/6975",
    "body": "**Describe the bug**\r\n`set-publish-rate` does not take effect \r\n\r\n**To Reproduce**\r\n1. `set-publish-rate`\r\n2.  do publish\r\n\r\n**Expected behavior**\r\nproducer should not publish messages faster than the publish rate \r\n\r\n**Screenshots**\r\npublish rate limit information\r\n```\r\nbin/pulsar-admin namespaces get-publish-rate rate/rate \r\n{\r\n  \"publishThrottlingRateInMsg\" : 10,\r\n  \"publishThrottlingRateInByte\" : 10240\r\n}\r\n```\r\n\r\ndo publish\r\n```\r\nbin/pulsar-perf produce -threads 1 -u pulsar://100.76.43.216:6650 -o 10000 -n 1 -b 0 -bm 0 -s 1024 -r 100000 rate/rate/rate\r\n\r\n14:43:32.193 [main] INFO  org.apache.pulsar.testclient.PerformanceProducer - Throughput produced:    161.6  msg/s ---      1.4 Mbit/s --- failure      0.0 msg/s --- Latency: mean: 5314.773 ms - med: 5730.591 - 95pct: 8716.543 - 99pct: 8744.191 - 99.9pct: 8744.319 - 99.99pct: 8744.319 - Max: 8744.319\r\n14:43:42.262 [main] INFO  org.apache.pulsar.testclient.PerformanceProducer - Throughput produced:    214.2  msg/s ---      1.8 Mbit/s --- failure      0.0 msg/s --- Latency: mean: 14039.659 ms - med: 13728.319 - 95pct: 18667.391 - 99pct: 18700.927 - 99.9pct: 18701.055 - 99.99pct: 18701.055 - Max: 18701.055\r\n\r\n```\r\nAs shown, under the limit of 10 msgs and 10240 bytes, when send message with 1024B bytes, it should not be faster than 10msg/s. \r\nBut the tes result is much more than 10msg/s.\r\n**Additional context**\r\nAs my researching , pulsar do topic publish rate limit using two individual thread, \r\n```\r\n\r\n long topicTickTimeMs = pulsar().getConfiguration().getTopicPublisherThrottlingTickTimeMillis();\r\n        if (topicTickTimeMs > 0) {\r\n            if (this.topicPublishRateLimiterMonitor == null) {\r\n                this.topicPublishRateLimiterMonitor = Executors.newSingleThreadScheduledExecutor(\r\n                        new DefaultThreadFactory(\"pulsar-topic-publish-rate-limiter-monitor\"));\r\n                if (topicTickTimeMs > 0) {\r\n                    topicPublishRateLimiterMonitor.scheduleAtFixedRate(safeRun(() -> checkTopicPublishThrottlingRate()),\r\n                            topicTickTimeMs, topicTickTimeMs, TimeUnit.MILLISECONDS);\r\n                    \r\n                    topicPublishRateLimiterMonitor.scheduleAtFixedRate(safeRun(() -> refreshTopicPublishRate()), 1, 1,\r\n                            TimeUnit.SECONDS);\r\n                }\r\n            }\r\n        } \r\n```\r\n\r\none thread  schedule task that sums up publish-rate across all cnx on a topic and another schedule task that refreshes rate-limiting bucket.\r\nThis means we can only pause publish after send message for `topicPublisherThrottlingTickTimeMillis` long times, and messages send before `topicPublisherThrottlingTickTimeMillis` will never trigger rate limit.\r\n\r\nIn order to get efficient publish rate limit, I think, we should use something like `RateLimiter` instead of a period task.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6975/comments",
    "author": "aloyszhang",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-05-18T13:28:23Z",
        "body": "@aloyszhang Thanks for the feedback, I think this is an enhancement. I would like to change the tag to enhancement."
      },
      {
        "user": "aloyszhang",
        "created_at": "2020-05-19T03:33:16Z",
        "body": "@codelipenghui Thanks for your suggestion, LGTM.  "
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-05-19T15:14:43Z",
        "body": "@aloyszhang I think we can add options in the broker.conf. So that users can choose the precise rate control or not. If users don't care about the cost of the competition, they can choose the precise rate control."
      },
      {
        "user": "aloyszhang",
        "created_at": "2020-05-20T06:37:08Z",
        "body": "@codelipenghui  OK, I'll add the option."
      }
    ]
  },
  {
    "number": 6960,
    "title": "Duplicated messages are sent to dead letter topic",
    "created_at": "2020-05-14T10:06:38Z",
    "closed_at": "2020-06-05T01:42:53Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/client"
    ],
    "url": "https://github.com/apache/pulsar/issues/6960",
    "body": "**To Reproduce**\r\n1. Consumer config: SubscriptionMode=Shared, SubscriptionName=Test1, ack Timeout=10000ms, enable Dead Letter Policy and setMaxRedeliveryCount=3\r\n2. Run 3 Consumers with the above config in 3 parallel threads. The consumers will not send the ACK\r\n3. Send a message to the topic which the consumers are listening to\r\n4. Because the ACK will not be sent, the message will be sent to the dead letter topic after the redelivery count is exceeded.\r\n\r\n**Expected behavior**\r\nOnly one message is sent to dead-letters topic\r\n\r\n**Actual behavior**\r\nThe message is sent to the dead-letters topic twice\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6960/comments",
    "author": "bigbang489",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-05-18T01:46:45Z",
        "body": "@bigbang489 Thanks for reporting this issue, Could this be stable reproduced?   It would be appreciate if you could provide your code example to reproduce it. "
      },
      {
        "user": "bigbang489",
        "created_at": "2020-05-20T03:58:35Z",
        "body": "> @bigbang489 Thanks for reporting this issue, Could this be stable reproduced? It would be appreciate if you could provide your code example to reproduce it.\r\n\r\nThe issue is stable, just enable the deadletter policy and start 3 consumers in 3 parallel threads."
      },
      {
        "user": "bigbang489",
        "created_at": "2020-05-22T08:04:27Z",
        "body": " @jiazhai  This is my code\r\n\r\n`package com.test;\r\n\r\nimport java.util.concurrent.Executor;\r\nimport java.util.concurrent.Executors;\r\nimport java.util.concurrent.TimeUnit;\r\n\r\nimport org.apache.pulsar.client.api.AuthenticationFactory;\r\nimport org.apache.pulsar.client.api.ClientBuilder;\r\nimport org.apache.pulsar.client.api.Consumer;\r\nimport org.apache.pulsar.client.api.DeadLetterPolicy;\r\nimport org.apache.pulsar.client.api.Message;\r\nimport org.apache.pulsar.client.api.MessageListener;\r\nimport org.apache.pulsar.client.api.Producer;\r\nimport org.apache.pulsar.client.api.PulsarClient;\r\nimport org.apache.pulsar.client.api.PulsarClientException;\r\nimport org.apache.pulsar.client.api.SubscriptionType;\r\n\r\npublic class App {\r\n\tstatic Executor executor = Executors.newFixedThreadPool(3);\r\n\t\r\n\tpublic static void main(String[] args) throws InterruptedException, PulsarClientException {\r\n\t\tstartConsumer(\"my-topic\", false);\r\n\t\tstartConsumer(\"my-topic\", false);\r\n\t\tstartConsumer(\"my-topic\", false);\r\n\t\tstartConsumer(\"DLQ\", true); // This consumer is for dead letter\r\n\t\tThread.sleep(2000);\r\n\t\tpublish(String.valueOf(System.currentTimeMillis()), \"my-topic\");\r\n\t}\r\n\tstatic void startConsumer(String topic, boolean ack) {\r\n\t\texecutor.execute(()-> {\r\n\t\t\ttry {\r\n\t\t\t\tsubscribe(topic, ack);\r\n\t\t\t} catch (PulsarClientException e) {\r\n\t\t\t\te.printStackTrace();\r\n\t\t\t}\r\n\t\t});\r\n\t}\r\n\t\r\n\tstatic Consumer<byte[]> subscribe(String topic, boolean ack) throws PulsarClientException {\r\n\t\tClientBuilder cb = PulsarClient.builder();\r\n\t\tcb.serviceUrl(\"pulsar://pulsar:6650\");\r\n\t\tcb.authentication(AuthenticationFactory.token(\"*************\"));\r\n\t\tPulsarClient client = cb.build();\r\n\t\t@SuppressWarnings(\"serial\")\r\n\t\tConsumer<byte[]> consumer = client.newConsumer()\r\n\t\t\t.subscriptionName(\"test\")\r\n\t\t\t.subscriptionType(SubscriptionType.Shared)\r\n\t\t\t.topic(topic)\r\n\t\t\t.ackTimeout(12, TimeUnit.SECONDS)\r\n\t\t\t.messageListener(new MessageListener<byte[]>() {\r\n\r\n\t\t\t\t@Override\r\n\t\t\t\tpublic void received(Consumer<byte[]> consumer, Message<byte[]> msg) {\r\n\t\t\t\t\tSystem.out.println(\"\\r\\nReceive messaged: '\" + new String(msg.getData()) + \"' from topic: \" + msg.getTopicName());\r\n\t\t\t\t\ttry {\r\n\t\t\t\t\t\tif(ack) {\r\n\t\t\t\t\t\t\tconsumer.acknowledge(msg);\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t} catch (PulsarClientException e) {\r\n\t\t\t\t\t\te.printStackTrace();\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t})\r\n\t\t\t.deadLetterPolicy(DeadLetterPolicy.builder()\r\n\t\t\t\t\t.deadLetterTopic(\"DLQ\")\r\n\t\t\t\t\t.maxRedeliverCount(2)\r\n\t\t\t\t\t.build())\r\n\t\t\t.subscribe();\r\n\t\tSystem.out.println(\"\\r\\nStarted a consumer...\");\r\n\t\treturn consumer;\r\n\t}\r\n\t\r\n\tstatic void publish(String message, String topic) throws PulsarClientException {\r\n\t\tClientBuilder cb = PulsarClient.builder();\r\n\t\tcb.serviceUrl(\"pulsar://pulsar:6650\");\r\n\t\tcb.authentication(AuthenticationFactory.token(\"***************\"));\r\n\t\tPulsarClient client = cb.build();\r\n\t\tProducer<byte[]> producer = client.newProducer().topic(topic).create();\r\n\t\ttry {\r\n\t\t\tSystem.out.println(\"\\r\\nSent message: \" + producer.send(message.getBytes()));\r\n\t\t}\r\n\t\tfinally {\r\n\t\t\tproducer.close();\r\n\t\t}\r\n\t\t\r\n\t}\r\n}\r\n`\r\n\r\nPOM file:\r\n`<dependency>\r\n  \t\t<groupId>org.apache.pulsar</groupId>\r\n  \t\t<artifactId>pulsar-client</artifactId>\r\n  \t\t<version>2.5.1</version>\r\n  \t</dependency>\r\n  \t<dependency>\r\n  \t\t<groupId>org.apache.pulsar</groupId>\r\n  \t\t<artifactId>pulsar-client-api</artifactId>\r\n  \t\t<version>2.5.1</version>\r\n  \t</dependency>`\r\n\r\n**The result** \r\nReceive messaged: '1590133963111' from topic: persistent://public/default/my-topic\r\n\r\nReceive messaged: '1590133963111' from topic: persistent://public/default/my-topic\r\n\r\nReceive messaged: '1590133963111' from topic: persistent://public/default/my-topic\r\n\r\n**Receive messaged: '1590133963111' from topic: persistent://public/default/DLQ**\r\n\r\nReceive messaged: '1590133963111' from topic: persistent://public/default/my-topic\r\n\r\n**Receive messaged: '1590133963111' from topic: persistent://public/default/DLQ**\r\n\r\nAs you see, the message is sent to DLQ twice."
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-05-22T08:46:46Z",
        "body": "@bigbang489 Thanks, @315157973 already working on this issue."
      },
      {
        "user": "315157973",
        "created_at": "2020-05-23T05:58:02Z",
        "body": "I am fixing this bug"
      }
    ]
  },
  {
    "number": 6929,
    "title": "KeyValue schema in Function Error",
    "created_at": "2020-05-09T10:05:03Z",
    "closed_at": "2022-12-06T13:50:17Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/function"
    ],
    "url": "https://github.com/apache/pulsar/issues/6929",
    "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\nUse KeyValue schema in Producer\r\n\r\n            Producer<KeyValue<Student, Student>> producer = client.newProducer(\r\n                    KeyValueSchema.of(Schema.JSON(Student.class), Schema.JSON(Student.class)))\r\n                    .topic(\"persistent://public/default/test-kvin2\")\r\n                    .producerName(\"producerTest\").create();\r\n\r\nFunction code:\r\npublic class PulsarFunctionTest3 implements Function<KeyValue<Student, Student>, KeyValue<Student, Student>> {\r\n\r\n    @Override\r\n    public KeyValue<Student, Student> process(KeyValue<Student, Student> studentKeyValue, Context context) throws Exception {\r\n        context.getLogger().info(\"studentKeyValue key: {}\", studentKeyValue.getKey().toString());\r\n        context.getLogger().info(\"studentKeyValue: {}\", studentKeyValue.toString());\r\n        return studentKeyValue;\r\n    }\r\n}\r\n\r\nPulsar Function log shows:\r\njava.util.concurrent.CompletionException: org.apache.pulsar.client.api.PulsarClientException$IncompatibleSchemaException: Key schemas or Value schemas are different schema type, from key schema type is JSON and to key schema is BYTES, from value schema is JSON and to value schema is BYTES\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6929/comments",
    "author": "xuesongxs",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-11-04T02:49:07Z",
        "body": "@wolfstudy Could you please take a look at this issue?"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T13:50:17Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 6887,
    "title": "ttlDurationDefaultInSeconds is not applied",
    "created_at": "2020-05-06T06:59:25Z",
    "closed_at": "2020-05-12T10:53:07Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/6887",
    "body": "Pulsar 2.5.0\r\nbroker.conf has ttlDurationDefaultInSeconds=3600\r\nBut when creating a new namespace it is still created with ttl 0.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6887/comments",
    "author": "trexinc",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2020-05-07T01:36:34Z",
        "body": "> But when creating a new namespace it is still created with ttl 0.\r\n\r\nWhere did you check the TTL value?"
      },
      {
        "user": "alexku7",
        "created_at": "2020-05-07T11:46:16Z",
        "body": "Hi\r\nHere are the steps for reproduce the issue:\r\n1. Check that the default value is configured in the broker's config:\r\n\r\nroot@pulsar-bastion-5db9984cf8-w8stw:/pulsar/bin# ./pulsar-admin brokers get-runtime-config | grep ttlDurationDefaultInSeconds\r\n  \"ttlDurationDefaultInSeconds\" : \"3600\",\r\n\r\n2. create a new namescpace:\r\n\r\nroot@pulsar-bastion-5db9984cf8-w8stw:/pulsar/bin# ./pulsar-admin namespaces create internal/test44\r\n\r\n3. check the policy on the new created namespace:\r\n\r\nroot@pulsar-bastion-5db9984cf8-w8stw:/pulsar/bin# ./pulsar-admin namespaces get-message-ttl internal/test44\r\n0\r\nThe output shows the value of zero, e.g. endless ttl  but should be 3600\r\n\r\n"
      },
      {
        "user": "sijie",
        "created_at": "2020-05-08T01:54:25Z",
        "body": "@alexku7 Currently the admin restful api returns the value stored in namespace policies directly. It doesn't take the value from broker configuration. It should be simple to add a fix. Are you interested in contributing a fix for that?"
      },
      {
        "user": "trexinc",
        "created_at": "2020-05-08T07:49:19Z",
        "body": "I am interested if you can give some pointers."
      },
      {
        "user": "trexinc",
        "created_at": "2020-05-08T16:44:56Z",
        "body": "@sijie @Omega-Ariston - there is actually the following code in the broker. So it seems that it does take the default TTL into consideration at the actual expiry check. So the bug is kind of more \r\n cosmetic than anything else.\r\nDo you think that maybe get-namespace-ttl API should return the effective TTL?\r\n\r\n    @Override\r\n    public void checkMessageExpiry() {\r\n        TopicName name = TopicName.get(topic);\r\n        Policies policies;\r\n        try {\r\n            policies = brokerService.pulsar().getConfigurationCache().policiesCache()\r\n                    .get(AdminResource.path(POLICIES, name.getNamespace()))\r\n                    .orElseThrow(() -> new KeeperException.NoNodeException());\r\n            int defaultTTL = brokerService.pulsar().getConfiguration().getTtlDurationDefaultInSeconds();\r\n            int message_ttl_in_seconds = (policies.message_ttl_in_seconds <= 0 && defaultTTL > 0) ? defaultTTL\r\n                    : policies.message_ttl_in_seconds;\r\n            if (message_ttl_in_seconds != 0) {\r\n                subscriptions.forEach((subName, sub) -> sub.expireMessages(message_ttl_in_seconds));\r\n                replicators.forEach((region, replicator) -> ((PersistentReplicator)replicator).expireMessages(message_ttl_in_seconds));\r\n            }\r\n        } catch (Exception e) {\r\n            if (log.isDebugEnabled()) {\r\n                log.debug(\"[{}] Error getting policies\", topic);\r\n            }\r\n        }\r\n    }\r\n"
      },
      {
        "user": "trexinc",
        "created_at": "2020-05-08T17:07:40Z",
        "body": "Or maybe just the documentation should be updated?\r\n"
      },
      {
        "user": "trexinc",
        "created_at": "2020-05-08T17:16:14Z",
        "body": "As I see it - it is actually a nice feature. You can changes the global default TTL and it immediately applies to all namespaces with default 0 TTL."
      }
    ]
  },
  {
    "number": 6843,
    "title": "support setPassword to managedLedger from brokerconfg",
    "created_at": "2020-04-29T14:50:43Z",
    "closed_at": "2020-04-30T04:56:20Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/6843",
    "body": "## Motivation\r\n\r\nbroker supports ManagedLedger configuration\r\n\r\n## Proposed changes\r\n\r\n\r\norg.apache.pulsar.broker.ServiceConfiguration\r\n```java\r\n    @FieldContext(\r\n            category = CATEGORY_STORAGE_ML,\r\n            doc = \"Default  password to use when writing to BookKeeper. \\n\\nDefault is ``.\"\r\n        )\r\n    private String managedLedgerPassword = \"\";\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6843/comments",
    "author": "liudezhi2098",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-04-30T02:23:44Z",
        "body": "@liudezhi2098 Thanks for the help on it."
      }
    ]
  },
  {
    "number": 6748,
    "title": "`pulsar.broker-service-url` is misleading in presto configuration",
    "created_at": "2020-04-16T17:49:18Z",
    "closed_at": "2021-05-22T00:57:26Z",
    "labels": [
      "help wanted",
      "area/sql"
    ],
    "url": "https://github.com/apache/pulsar/issues/6748",
    "body": "*Motivation*\r\n\r\n`pulsar.broker-service-url` in presto configuration is misleading. The name seems to indicate that you should use a broker service URL. But actually it is expecting the admin/HTTP service URL. So it is better to add a new `pulsar.admin-service-url` to replace this `pulsar.broker-service-url` setting.\r\n\r\nAt least, we need to document this setting.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6748/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "eaba",
        "created_at": "2020-04-16T19:29:52Z",
        "body": "Yes, truly it is misleading!\r\nI had to look up several examples to understand how to set it!"
      },
      {
        "user": "sijie",
        "created_at": "2020-04-16T21:56:35Z",
        "body": "@eaba are you interested in sending out a pull request for fixing it?"
      },
      {
        "user": "eaba",
        "created_at": "2020-04-17T03:42:00Z",
        "body": "`pulsar.web-service-url` should be better? `admin-service-url` sounds like a new term.\r\nIt is documented that `broker-service-url` is `6650` binary and `web-service-url` is `8080` web.\r\n\r\n"
      },
      {
        "user": "ADHB",
        "created_at": "2020-04-17T06:27:42Z",
        "body": "pulsar.web-service-url will centainly fit better :)"
      }
    ]
  },
  {
    "number": 6709,
    "title": "unsubscribe option for pulsar-client cli",
    "created_at": "2020-04-10T09:17:34Z",
    "closed_at": "2020-09-23T07:24:52Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/6709",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nAfter running `pulsar-client consume` the subscription is kept active and it is not possible to unsubscribe without using the admin interface.\r\n\r\n**Describe the solution you'd like**\r\nThe option to unsubscribe after consuming. eg:\r\npulsar-client consume -s testsub --unsubscribe persistent://my/pulsar/topic\r\n\r\n**Describe alternatives you've considered**\r\nUsing pulsar-admin it's possible to unsubscribe but this requires the user has access to the admin interface.\r\n\r\n**Additional context**\r\nnone\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6709/comments",
    "author": "sbourkeostk",
    "comments": [
      {
        "user": "315157973",
        "created_at": "2020-04-11T07:25:33Z",
        "body": " you can assigne it to me :) @sijie "
      },
      {
        "user": "sijie",
        "created_at": "2020-04-11T17:48:22Z",
        "body": "@315157973 since you are not part of ASF org, I am not able to assign it to you in Github. In general, you can leave a message here when you start working on it."
      },
      {
        "user": "sijie",
        "created_at": "2020-04-11T17:49:59Z",
        "body": "@315157973 I don't think we need additional command to do that.\r\n\r\n@sbourkeostk  You can use `topics unsubscribe` to unsubscribe a subscription.\r\n\r\n```\r\nbin/pulsar-admin topics unsubscribe\r\nThe following option is required: -s, --subscription\r\n\r\nDelete a durable subscriber from a topic.\r\n\t\tThe subscription cannot be deleted if there are any active consumers attached to it\r\n\r\nUsage: unsubscribe [options] persistent://tenant/namespace/topic\r\n  Options:\r\n  * -s, --subscription\r\n       Subscription to be deleted\r\n```"
      },
      {
        "user": "sijie",
        "created_at": "2020-04-13T01:46:16Z",
        "body": "Closed this issue since we don't need to add `unsubscribe` command to pulsar-client."
      },
      {
        "user": "sbourkeostk",
        "created_at": "2020-04-15T09:28:48Z",
        "body": "@sijie Yes, you can unsubscribe using the admin interface but that is not the same thing. Allowing the CLI client to unsubscribe itself has two main advantages:\r\n\r\n1. Users without access to the admin interface can unsubscribe\r\n2. Convenience - I suspect that a lot of users use `pulsar-client consume` to check that messages are flowing on a topic and they do not intend the subscription to persist. On our Pulsar test cluster we often see backlogs on subscriptions that were started with the CLI and the user neglected to unsubscribe.\r\n\r\nGoing further, I think it would be good practice to the CLI unsubscribe be default with an option to persist the subscription.\r\n\r\nI know this would not be high priority but would you be opposed to a pull request for this feature if @315157973 or myself were willing to implement it?"
      },
      {
        "user": "fmiguelez",
        "created_at": "2020-07-28T13:42:22Z",
        "body": "This is a serious limitation. \r\n\r\nIf you use short-lived subscriptions clients can create new subscriptions which takes up resources. Those resources that are no longer necessary once the client is gone can only be taken back by an administrator. \r\n\r\nWe have not many options until NonDurable subscriptions work as expected (e.g. BUG #7619) "
      },
      {
        "user": "sijie",
        "created_at": "2020-07-28T19:06:07Z",
        "body": "@sbourkeostk @fmiguelez \r\n\r\nJust to be clear here, the \"unsubscribe\" operation is supported in Pulsar RESTful admin API. You can call the RESTful API directly or use `pulsar-admin` to unsubscribe a subscription. With that being said, both tools and API are available for you to use.\r\n\r\nMore specifically, `pulsar-admin` and `pulsar-client` are shipped together as part of the Pulsar binary release. You can get both of the tools out of the box. I don't see the case that users would have access to `pulsar-client` tool but not `pulsar-admin` tool. Step back, even we can add a new command in `pulsar-client`. The `unsubscribe` command will still be protected by access control. You need permissions to call this API no matter the command is in `pulsar-client` or `pulsar-admin`. Hope this clarifies your question on \"Users without access to the admin interface can unsubscribe\".\r\n\r\n> check that messages are flowing on a topic and they do not intend the subscription to persist. On our Pulsar test cluster we often see backlogs on subscriptions that were started with the CLI and the user neglected to unsubscribe.\r\n\r\nIf you are talking about adding a non-durable subscription option to `client consume` command, I think that's a good idea to me. \r\n"
      },
      {
        "user": "sbourkeostk",
        "created_at": "2020-07-28T21:33:25Z",
        "body": "Hi @sijie\r\nYeah, it's clear that the pulsar-admin can be used to unsubscribe - that fact was acknowledged in the original post. The issue is on the usability of pulsar-client. This is a nice simple useful tool but the fact that it is not capable of cleaning up after itself is very odd to me.\r\nI think the main use of this tool is for testing producing and consuming to a topic:\r\n\r\n1. Produce:\r\n `pulsar-client produce -m HiThere persistent://a/b/c `\r\n2. Consume:\r\n `pulsar-client consume -s s1 persistent://a/b/c`\r\n `pulsar-admin topics unsubscribe -s s1 persistent://a/b/c`\r\n\r\nWhy is use case 2 a two-step process and why is a second tool required to clean up?  I would suggest that the default behaviour be that the subscription does not persist but at the very least there should be a `-u`, `--unsubscribe` option so that the use of pulsar-admin is not required after **every** use of `pulsar-client consume`. \r\n\r\nOf course, setting up a subscription is also a valid use case but I would think it's probably much less common than the above.\r\n\r\nAnd sure, it's possible to do the job with the existing tools, it's just inconvenient and non-intuitive - the issue was put in as a feature request not a bug."
      },
      {
        "user": "sijie",
        "created_at": "2020-07-28T21:51:45Z",
        "body": "@sbourkeostk I was under the impression that you are suggesting introducing a \"unsubscribe\" command in pulsar-client CLI. \r\n\r\nIn my previous comment, I am fine with adding an option to `pulsar-client consume`. Although I don't suggest adding `--unsubscribe` option, instead we should just add an option to use a non-durable subscription. So the subscription will be automatically deleted after the command exits. "
      },
      {
        "user": "sbourkeostk",
        "created_at": "2020-07-28T22:02:06Z",
        "body": "@sijie sounds great - I was not aware of non-durable subscriptions until this discussion today.  "
      }
    ]
  },
  {
    "number": 6528,
    "title": "JSONSchema Deserializer does not work with kotlin data classes",
    "created_at": "2020-03-12T14:58:42Z",
    "closed_at": "2020-10-01T06:30:24Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/6528",
    "body": "* producer , consumer example*\r\n\r\nThe consumer fails with error: \r\n```\r\nException in thread \"main\" org.apache.pulsar.client.api.SchemaSerializationException: org.apache.pulsar.shade.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `example.simple.SensorReading` (no Creators, like default construct, exist): cannot deserialize from Object value (no delegate- or property-based Creator)\r\n at [Source: (byte[])\"{\"temperature\":1.5}\"; line: 1, column: 2]\r\n\tat org.apache.pulsar.client.impl.schema.reader.JsonReader.read(JsonReader.java:44)\r\n\tat org.apache.pulsar.client.api.schema.SchemaReader.read(SchemaReader.java:36)\r\n\tat org.apache.pulsar.client.impl.schema.StructSchema.decode(StructSchema.java:92)\r\n\tat org.apache.pulsar.client.impl.MessageImpl.getValue(MessageImpl.java:278)\r\n\tat example.simple.SimpleAppKt.main(SimpleApp.kt:52)\r\n\tat example.simple.SimpleAppKt.main(SimpleApp.kt)\r\nCaused by: org.apache.pulsar.shade.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `example.simple.SensorReading` (no Creators, like default construct, exist): cannot deserialize from Object value (no delegate- or property-based Creator)\r\n at [Source: (byte[])\"{\"temperature\":1.5}\"; line: 1, column: 2]\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1589)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1055)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1297)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:159)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4202)\r\n\tat org.apache.pulsar.shade.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3275)\r\n\tat org.apache.pulsar.client.impl.schema.reader.JsonReader.read(JsonReader.java:42)\r\n\r\n```\r\n\r\n\r\n```\r\npackage example.simple\r\n\r\nimport com.fasterxml.jackson.module.kotlin.jacksonObjectMapper\r\nimport com.fasterxml.jackson.module.kotlin.readValue\r\nimport org.apache.pulsar.client.api.*\r\nimport org.apache.pulsar.client.impl.schema.JSONSchema\r\n\r\n\r\nprivate const val SERVICE_URL = \"pulsar://localhost:6650\"\r\nprivate const val TOPIC_NAME = \"test-topic-jsonschema-002\"\r\nprivate const val SUBSCRIPTION_NAME = \"test-subscription-001\"\r\nprivate typealias PulsarProducer = Producer<SensorReading>\r\nprivate typealias PulsarConsumer = Consumer<SensorReading>\r\n\r\ndata class SensorReading(val temperature: Float)\r\n\r\nfun main() {\r\n    val schema = JSONSchema.of(SensorReading::class.java)\r\n    val client: PulsarClient by lazy {\r\n        PulsarClient.builder()\r\n                .serviceUrl(SERVICE_URL)\r\n                .build()\r\n    }\r\n    val producer: PulsarProducer = client.newProducer(schema)\r\n            .topic(TOPIC_NAME)\r\n            .compressionType(CompressionType.LZ4)\r\n            .create()\r\n\r\n    val consumer: PulsarConsumer = client.newConsumer(schema)\r\n            .topic(TOPIC_NAME)\r\n            .subscriptionType(SubscriptionType.Shared)\r\n            .subscriptionName(SUBSCRIPTION_NAME)\r\n            .subscribe()\r\n\r\n    // check json (stringify/parse) works\r\n    val JSON = jacksonObjectMapper()\r\n    val source = SensorReading(temperature = 2.5f)\r\n    val txt = JSON.writeValueAsString(source)\r\n    val sink: SensorReading = JSON.readValue(txt)\r\n\r\n    run {\r\n        val content = SensorReading(temperature = 1.5f)\r\n        val msg = producer.newMessage()\r\n                .value(content)\r\n        msg.send()\r\n    }\r\n\r\n    run {\r\n        while (true) {\r\n            val msg = consumer.receive()\r\n            println(\"received ${msg.messageId}\")\r\n            val content = msg.value\r\n            println(\"Message received: $content\")\r\n\r\n            // Acknowledge the message so that it can be deleted by the message broker\r\n            consumer.acknowledge(msg)\r\n            println(\"Ack sent.\")\r\n        }\r\n    }\r\n\r\n\r\n}\r\n\r\n\r\n```\r\n\r\n\r\n* side notes *\r\njackson-module-kotlin is not used by the build in objectmapper\r\n\r\nIs there any chance to get that supported?\r\nalternative: let's pass in the pre-configured objectmapper as argument of schema factory.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6528/comments",
    "author": "bastman",
    "comments": [
      {
        "user": "kindofwhat",
        "created_at": "2020-05-02T21:12:29Z",
        "body": "as a workaround: supply default values for all properties. this of course is not \"proper\" support for data classes,,,"
      },
      {
        "user": "tomas-c",
        "created_at": "2020-09-30T14:01:22Z",
        "body": "Since #6905 has been merged, it's now possible to create a schema that uses a custom ObjectMapper like so:\r\n\r\n```\r\nval objectMapper= ObjectMapper()\r\nobjectMapper.registerModule(KotlinModule())\r\nobjectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)\r\nobjectMapper.setSerializationInclusion(JsonInclude.Include.NON_NULL)\r\n\r\nval schemDefinition = SchemaDefinition.builder<YourClass>()\r\n            .withPojo(YourClass::class.java)\r\n            .withSchemaReader(JacksonJsonReader(objectMapper, YourClass::class.java))\r\n            .withSchemaWriter(JacksonJsonWriter(objectMapper))\r\n            .build()\r\n```\r\n\r\nNote that this only works with the unshaded version of the library (use org.apache.pulsar:pulsar-client-original instead of org.apache.pulsar:pulsar-client)"
      },
      {
        "user": "sijie",
        "created_at": "2020-10-01T06:30:24Z",
        "body": "@tomas-c Thank you for calling it now.\r\n\r\nClose the issue since it is fixed by #6905"
      }
    ]
  },
  {
    "number": 6414,
    "title": "High CPU load when PulsarClient (Java) is idle",
    "created_at": "2020-02-24T23:13:42Z",
    "closed_at": "2020-03-03T02:31:55Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/client"
    ],
    "url": "https://github.com/apache/pulsar/issues/6414",
    "body": "**Describe the bug**\r\nRunning a simple consumer on Windows using the `PulsarClient`, I notice that there is considerable CPU usage when the consumer is just idling. Profiling showed that the 'pulsar-timer-x-y' thread is heavy on the CPU. Digging into the source code, we can see that the `HashedWheelTimer `is set up with 1 ms tick time:\r\n\r\n`timer = new HashedWheelTimer(getThreadFactory(\"pulsar-timer\"), 1, TimeUnit.MILLISECONDS);`\r\n\r\nA simple test with similar values show that a running HashedWheelTimer can consume CPU equal to 100% of one hyperthread:\r\n\r\n```\r\n    @Test\r\n    public void testTimerWheel() throws Exception {\r\n        HashedWheelTimer timer = new HashedWheelTimer(1, TimeUnit.MILLISECONDS);\r\n\r\n        timer.newTimeout(new TimerTask() {\r\n\r\n            @Override\r\n            public void run(final Timeout timeout) throws Exception {\r\n                timer.newTimeout(this, 500, TimeUnit.MILLISECONDS);\r\n\r\n            }\r\n        }, 500, TimeUnit.MILLISECONDS);\r\n\r\n        Thread.sleep(30_000);\r\n        timer.stop();\r\n    }\r\n```\r\nMy understanding of the `HashedWheelTimer` is that it is indented to be used for a large number of approximated timeouts and not for millisecond precision scheduling, but I may be mistaken.\r\n\r\n**To Reproduce**\r\nRun a simple producer subscribing to a topic with no messages, or run the above simple JUnit test. Observe CPU usage.\r\n\r\n**Expected behavior**\r\nAn idle consumer should not incur high CPU load. For example, when running 8 consumers on a i7-8700 (4 cores 2 threads per core), TaskManager reported 100% CPU usage - when there was no traffic, no messages on the topics.\r\n\r\n**Discussion**\r\nHigh CPU load is a high price to pay for immediate batch dispatching (which may be the primary reason for setting the 1 ms tick time). It should be possible to lax on immediate batch dispatching in favor of more compute resources available to other tasks.\r\n\r\nAn alternative is to use a plain old `ScheduledExecutorService` for batch dispatching/reception, tests suggest it will also be lighter on the CPU when millisecond precision is required and the number of task is not too high. For example, I could schedule 2000 simple tasks every millisecond (`scheduleAtFixedRate`) with more compute available on a single thread. When scheduling 10k tasks every millisecond, the thread was working 100%, but scheduling 10k tasks every 10 millisecond was OK. So if a client is used to create a huge number of producers/consumers, it might be better to lax the batch timeout so the timeout thread has a chance to actually perform its work. Another observation is that the `HashedWheelTimer` used with 1 millisecond tick time is not able to run a single simple task every millisecond, party due to the `HashedWheelTimer` approximate nature, and maybe party because spinning the wheel competes with task expiration.\r\n\r\nNote that when using `ScheduledExecutorService` each task might not get exactly the same number of executions, for example when scheduling 1000 tasks run execute every millisecond for 30 seconds, task were executed between 29 986 and 30 012 times.\r\n\r\nSome suggestions:\r\n1) The tick time could be configurable, with default value = 1 millisecond to preserve current behavior.\r\n2) Use default tick time 100 milliseconds, then create a `ScheduledExecutorService `in `PulsarClientImpl` that is exposed the same way as the `Timer `object for other classes to use. The `ProducerImpl `and `ConsumerBase` could use the `ScheduledExecutorService `to schedule batch dispatching/reception.\r\n\r\n**Additional context**\r\nObserved under Windows 10 64-bit, running Java 11.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6414/comments",
    "author": "racorn",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-02-28T02:39:15Z",
        "body": "Thanks a lot for @racorn 's detailed analysis.  your suggestions sounds great. Would you like to provide a fix for this?"
      },
      {
        "user": "racorn",
        "created_at": "2020-02-28T10:42:37Z",
        "body": "Yes, I can try to implement what I suggested, by mid next week. Will that be OK with you?"
      },
      {
        "user": "racorn",
        "created_at": "2020-02-29T11:04:58Z",
        "body": "I wish I had seen this before: netty/netty#9710, and the fix netty/netty#9714\r\n\r\nAs the Netty version is now bumped to 4.1.45 on master, the CPU load is no longer an issue on Windows. \r\n\r\nAnyway, @jiazhai, I made the suggested implementation in my forked repository, you can have a look to see if it is of any value. I could be an improvement in the `ProducerImpl`, where `client.scheduledExecutorService().scheduleAtFixedRate` better reflects what you want to achieve than using a `Timeout`. Also, the `keyGeneratorTask` could use the `ScheduledExecutorService` instead of using the Netty eventLoopGroup. On the other hand, if you are producing messages for a huge number of topics (10k +), the `HashedWheelTimer` may perform better. So maybe it is best to leave it as it is."
      },
      {
        "user": "jiazhai",
        "created_at": "2020-03-03T02:31:55Z",
        "body": "Thanks @racorn for your great help. Agree with you, we could leave it as it is.  And would like to close this issue."
      }
    ]
  },
  {
    "number": 6248,
    "title": "Shutdown Standalone Gracefully",
    "created_at": "2020-02-06T19:52:05Z",
    "closed_at": "2022-12-14T11:00:40Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/6248",
    "body": "A standalone Pulsar daemon server often throws various Exceptions when a shutdown is requested.   \r\n\r\nThis does not inspire confidence in the product.  Can I trust this product in production?  If the developers couldn't get a clean shutdown on an idle server, what are the chances this beast works reliably with tons of messages going in and out?   And if the developers couldn't figure it out in the base case of a single server and no messages - how am I going to troubleshoot this huge amount of code that takes an hour to compile and apparently requires six servers to test under load?\r\n\r\nExample Exception 1:\r\n```\r\n14:17:48.392 [Curator-LeaderSelector-0] WARN  org.apache.bookkeeper.stream.storage.impl.cluster.ClusterControllerLeaderImpl - Controller leader is interrupted, giving up leadership\r\n14:17:48.396 [Curator-LeaderSelector-0] ERROR org.apache.curator.framework.recipes.leader.LeaderSelector - The leader threw an exception\r\njava.lang.InterruptedException: null\r\n        at java.lang.Object.wait(Native Method) ~[?:1.8.0_232]\r\n        at java.lang.Object.wait(Object.java:502) ~[?:1.8.0_232]\r\n        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1529) ~[org.apache.pulsar-pulsar-zookeeper-2.5.0.jar:2.5.0]\r\n        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1512) ~[org.apache.pulsar-pulsar-zookeeper-2.5.0.jar:2.5.0]\r\n        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:1791) ~[org.apache.pulsar-pulsar-zookeeper-2.5.0.jar:2.5.0]\r\n        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:274) ~[org.apache.curator-curator-framework-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:268) ~[org.apache.curator-curator-framework-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[org.apache.curator-curator-client-4.0.1.jar:?]\r\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[org.apache.curator-curator-client-4.0.1.jar:?]\r\n        at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:265) ~[org.apache.curator-curator-framework-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:249) ~[org.apache.curator-curator-framework-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:34) ~[org.apache.curator-curator-framework-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:347) ~[org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:124) ~[org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154) ~[org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:449) [org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:466) [org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:65) [org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:246) [org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:240) [org.apache.curator-curator-recipes-4.0.1.jar:4.0.1]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_232]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_232]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_232]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_232]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]\r\n```\r\n\r\nExample Exception 2:\r\n```\r\n14:17:48.511 [Thread-1] ERROR org.apache.bookkeeper.client.MetadataUpdateLoop - UpdateLoop(ledgerId=2243,loopId=6839c228) Error writing metadata to store\r\norg.apache.bookkeeper.client.BKException$BKClientClosedException: BookKeeper client is closed\r\n        at org.apache.bookkeeper.meta.CleanupLedgerManager.close(CleanupLedgerManager.java:245) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.client.BookKeeper.close(BookKeeper.java:1410) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.BookKeeperClient.close(BookKeeperClient.java:271) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.impl.BKNamespaceDriver.doClose(BKNamespaceDriver.java:404) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.impl.BKNamespaceDriver.close(BKNamespaceDriver.java:385) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at com.google.common.io.Closeables.close(Closeables.java:78) ~[com.google.guava-guava-25.1-jre.jar:?]\r\n        at org.apache.distributedlog.util.Utils.close(Utils.java:544) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.BKDistributedLogNamespace.close(BKDistributedLogNamespace.java:341) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.stream.server.service.DLNamespaceProviderService.doClose(DLNamespaceProviderService.java:135) ~[org.apache.bookkeeper-stream-storage-server-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.common.component.AbstractLifecycleComponent.close(AbstractLifecycleComponent.java:123) ~[org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.common.component.LifecycleComponentStack.lambda$close$4(LifecycleComponentStack.java:123) ~[org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at com.google.common.collect.ImmutableList.forEach(ImmutableList.java:407) [com.google.guava-guava-25.1-jre.jar:?]\r\n        at org.apache.bookkeeper.common.component.LifecycleComponentStack.close(LifecycleComponentStack.java:123) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.stream.server.StreamStorageLifecycleComponent.doClose(StreamStorageLifecycleComponent.java:61) [org.apache.bookkeeper-stream-storage-server-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.common.component.AbstractLifecycleComponent.close(AbstractLifecycleComponent.java:123) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at org.apache.pulsar.zookeeper.LocalBookkeeperEnsemble.stop(LocalBookkeeperEnsemble.java:443) [org.apache.pulsar-pulsar-zookeeper-utils-2.5.0.jar:2.5.0]\r\n        at org.apache.pulsar.PulsarStandaloneStarter$1.run(PulsarStandaloneStarter.java:102) [org.apache.pulsar-pulsar-broker-2.5.0.jar:2.5.0]\r\n14:17:48.512 [DLM-/stream/storage-OrderedScheduler-1-0-EventThread] INFO  org.apache.distributedlog.BKLogWriteHandler - Try storing max sequence number 4 in completing /stream/storage/streams_000000000000000001_000000000000000001_000000000000000000/<default>/ledgers/inprogress_000000000000000004.\r\n```\r\nExample Exception 3:\r\n```\r\n14:17:48.512 [Thread-1] ERROR org.apache.distributedlog.BKAbstractLogWriter - Completing Log segments encountered exception\r\njava.io.IOException: Failed to close ledger for streams_000000000000000000_000000000000000001_000000000000000000:<default>:inprogress_000000000000000004 : BookKeeper client is closed\r\n        at org.apache.distributedlog.BKLogSegmentWriter$6.closeComplete(BKLogSegmentWriter.java:660) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.client.LedgerHandle$5.lambda$safeRun$0(LedgerHandle.java:552) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_232]\r\n        at org.apache.bookkeeper.client.LedgerHandle$5.lambda$safeRun$3(LedgerHandle.java:614) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_232]\r\n        at org.apache.bookkeeper.client.MetadataUpdateLoop.lambda$writeLoop$1(MetadataUpdateLoop.java:146) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_232]\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_232]\r\n        at org.apache.bookkeeper.meta.CleanupLedgerManager.lambda$close$1(CleanupLedgerManager.java:246) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at java.util.concurrent.ConcurrentHashMap$KeySetView.forEach(ConcurrentHashMap.java:4649) ~[?:1.8.0_232]\r\n        at org.apache.bookkeeper.meta.CleanupLedgerManager.close(CleanupLedgerManager.java:246) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.client.BookKeeper.close(BookKeeper.java:1410) ~[org.apache.bookkeeper-bookkeeper-server-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.BookKeeperClient.close(BookKeeperClient.java:271) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.impl.BKNamespaceDriver.doClose(BKNamespaceDriver.java:404) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.impl.BKNamespaceDriver.close(BKNamespaceDriver.java:385) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at com.google.common.io.Closeables.close(Closeables.java:78) ~[com.google.guava-guava-25.1-jre.jar:?]\r\n        at org.apache.distributedlog.util.Utils.close(Utils.java:544) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.distributedlog.BKDistributedLogNamespace.close(BKDistributedLogNamespace.java:341) ~[org.apache.distributedlog-distributedlog-core-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.stream.server.service.DLNamespaceProviderService.doClose(DLNamespaceProviderService.java:135) ~[org.apache.bookkeeper-stream-storage-server-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.common.component.AbstractLifecycleComponent.close(AbstractLifecycleComponent.java:123) ~[org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.common.component.LifecycleComponentStack.lambda$close$4(LifecycleComponentStack.java:123) ~[org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at com.google.common.collect.ImmutableList.forEach(ImmutableList.java:407) [com.google.guava-guava-25.1-jre.jar:?]\r\n        at org.apache.bookkeeper.common.component.LifecycleComponentStack.close(LifecycleComponentStack.java:123) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.stream.server.StreamStorageLifecycleComponent.doClose(StreamStorageLifecycleComponent.java:61) [org.apache.bookkeeper-stream-storage-server-4.10.0.jar:4.10.0]\r\n        at org.apache.bookkeeper.common.component.AbstractLifecycleComponent.close(AbstractLifecycleComponent.java:123) [org.apache.bookkeeper-bookkeeper-common-4.10.0.jar:4.10.0]\r\n        at org.apache.pulsar.zookeeper.LocalBookkeeperEnsemble.stop(LocalBookkeeperEnsemble.java:443) [org.apache.pulsar-pulsar-zookeeper-utils-2.5.0.jar:2.5.0]\r\n        at org.apache.pulsar.PulsarStandaloneStarter$1.run(PulsarStandaloneStarter.java:102) [org.apache.pulsar-pulsar-broker-2.5.0.jar:2.5.0]\r\n```\r\n**To Reproduce**\r\n1. Download latest version of Pulsar (2.5.0 at this time)\r\n2. Execute \"bin/pulsar-daemon start standalone\"\r\n3. Monitor the log file to verify all is well so far\r\n4. Execute \"bin/pulsar-daemon stop standalone\"\r\n5. Observe disturbing number of Exceptions in log file\r\n\r\n**Expected behavior**\r\nI expect the software to shutdown cleanly (without Exceptions and stack traces).\r\n\r\n - OS: Red Hat Enterprise Linux 7\r\n\r\n**Additional context**\r\nServer is idle (no clients). ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6248/comments",
    "author": "slominskir",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-02-07T01:19:50Z",
        "body": "@slominskir Thanks for this issue.  Since it is reproduce able, would like to mark it as help wanted. "
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-14T11:00:40Z",
        "body": "Closed as stale. IIRC we have several commits for these cases. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 6228,
    "title": "zookeeper connection cause resource leak",
    "created_at": "2020-02-05T08:46:23Z",
    "closed_at": "2020-02-29T20:53:16Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/6228",
    "body": "in class `ZookeeperBkClientFactoryImpl`,  the `create` method\r\n\r\n```\r\nif (zk.getState() == States.CONNECTEDREADONLY && sessionType != SessionType.AllowReadOnly) {\r\n                    future.completeExceptionally(new IllegalStateException(\"Cannot use a read-only session\"));\r\n                }\r\n```\r\n\r\nalthough this code can not reach, but pulsar should handle 'zookeeper.close' for this code\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6228/comments",
    "author": "zplinuxlover",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-02-07T01:29:43Z",
        "body": "Thanks @zplinuxlover for this issue. Would you like to contribute to this?"
      }
    ]
  },
  {
    "number": 6205,
    "title": "[Go Function API] Need Prometheus support and statistics for Go functions",
    "created_at": "2020-02-04T02:09:20Z",
    "closed_at": "2022-12-09T13:21:16Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/6205",
    "body": "Go Functions API currently does not support Prometheus metrics. Because Pulsar is using Prometheus for providing statistics and related information, Go functions are also missing this crucial information. The missing statistics and Prometheus support are preventing Go functions from being production-ready. This feature must be added to make Go functions more production-ready and to provide visibility into operational health. ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6205/comments",
    "author": "devinbost",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-02-04T09:29:24Z",
        "body": "Thanks @devinbost for open this issue. This would be a good issue for beginner. mark it as help-wanted. Contributing to this issue is very welcome and very appreciated. "
      },
      {
        "user": "devinbost",
        "created_at": "2020-02-04T19:49:57Z",
        "body": "@jiazhai This issue is fixed with PR #6105 "
      },
      {
        "user": "devinbost",
        "created_at": "2020-02-04T22:32:02Z",
        "body": "BTW, @jiazhai the change turned out to be a lot more complex than anticipated because the Prometheus developers changed the way their framework can be used in the Go client library. \r\nSo, it wasn't a clean copy of the Python/Java approaches we took for adding statistics and Prometheus data. I needed to take an entirely different approach, based on the guidance from the Prometheus maintainers. \r\nThere's a link to a discussion with more detail in that PR if you're interested. "
      }
    ]
  },
  {
    "number": 6168,
    "title": "Unacked messages are not redelivered on time on C++",
    "created_at": "2020-01-30T07:12:49Z",
    "closed_at": "2020-03-02T19:55:34Z",
    "labels": [
      "help wanted",
      "area/client"
    ],
    "url": "https://github.com/apache/pulsar/issues/6168",
    "body": "#### Expected behavior\r\nUnacked messages are redelivered after about unAckedMessagesTimeout.\r\n\r\n#### Actual behavior\r\nAs the following log, unacked messages are redelivered after about 2 * unAckedMessagesTimeout.\r\n\r\n```\r\nmy-message-0: Wed Jan 29 13:52:39 2020\r\n2020-01-29 13:52:59.210 INFO  UnAckedMessageTrackerEnabled:47 | [persistent://k2la/test/topic0, sub1, 0] : 1 Messages were not acked within 10000 time\r\nmy-message-0: Wed Jan 29 13:52:59 2020\r\n2020-01-29 13:53:19.214 INFO  UnAckedMessageTrackerEnabled:47 | [persistent://k2la/test/topic0, sub1, 0] : 1 Messages were not acked within 10000 time\r\nmy-message-0: Wed Jan 29 13:53:19 2020\r\n```\r\n\r\n#### Steps to reproduce\r\nOn C++ lib:\r\n1. Set `unAckedMessagesTimeout`.\r\n2. Receive messages and not ack them.\r\n\r\n#### System configuration\r\n**Pulsar version**: 2.5.0\r\n**Client OS**: MacOS 10.15.2\r\n**Boost version**: 1.72.0\r\n**Python version**: 2.7.16\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6168/comments",
    "author": "k2la",
    "comments": [
      {
        "user": "codelipenghui",
        "created_at": "2020-01-30T08:49:14Z",
        "body": "@k2la Thanks for adding this issue. Feel free to discuss here if you have any idea. I think cpp client can use the same approach as java client. Here is the pull request in java client: #3118 \r\n"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-02-03T04:16:13Z",
        "body": "Thanks @k2la for reporting this issue. Thanks @codelipenghui for the analysis. \r\nThis is a cpp client feature catchup. The approach is as penghui's PR #3118. This is a direct fix, and also \r\n a good task for beginner,  marked this as help-wanted."
      },
      {
        "user": "k2la",
        "created_at": "2020-02-04T02:06:10Z",
        "body": "@codelipenghui I think your idea is good.\r\n@jiazhai I will fix this based on #3118.\r\n"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-02-07T02:13:37Z",
        "body": "Great @k2la  thanks for the help."
      }
    ]
  },
  {
    "number": 6019,
    "title": "Multiple topics consumer not able to receive the earliest message (python client)",
    "created_at": "2020-01-08T22:05:55Z",
    "closed_at": "2022-12-09T13:15:18Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/6019",
    "body": "**Describe the bug**\r\nWhen using multiple topics with the Python client and `initial_position=Earliest`, new subscriptions still begin with the *latest* message. Related: #3494, #3712.\r\n\r\n**To Reproduce**\r\nCode for consumer:\r\n```\r\nrx = re.compile('persistent://public/default/topic-.*')\r\nsub = pulsar_client.subscribe(rx, 'sub', pulsar.ConsumerType.Shared,\r\n        initial_position=pulsar.InitialPosition.Earliest)\r\n```\r\n\r\nWhen a new topic is created, it is auto-discovered, as expected. However, all messages sent from the creation of the topic, until the subscription is made, are lost.\r\n\r\n**Expected behavior**\r\nWhen using `initial_position=Earliest` with a topic pattern, all new topic subscriptions should begin with the earliest message.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/6019/comments",
    "author": "dhagrow",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-01-09T03:50:06Z",
        "body": "@dhagrow thanks for reporting this issue. Which version are you using?"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-01-09T03:54:24Z",
        "body": "This issue seems been not very hard, mark this as help wanted. \r\nlooks like the setting of `InitialPosition.Earliest` from the client, was lost when create new subscription, we need to check the reason.\r\nThe underneath implementation is in PR #3714, by @wolfstudy. and @wolfstudy could provide help on anyone who would like to contribute this fix. "
      },
      {
        "user": "dhagrow",
        "created_at": "2020-01-09T03:57:11Z",
        "body": "@jiazhai this is 2.4.2."
      },
      {
        "user": "EugenDueck",
        "created_at": "2020-01-24T22:13:07Z",
        "body": "@dhagrow @jiazhai `InitialPosition` is a feature of the **reader**, but pattern subscription is a feature of the **consumer**, so unless and until Pulsar starts offering pattern subscription for readers, this is not a bug. In fact, it is a feature I am thinking about requesting."
      },
      {
        "user": "dhagrow",
        "created_at": "2020-01-25T03:05:21Z",
        "body": "@EugenDueck #3494 seems to suggest otherwise. Unless I'm missing something?"
      },
      {
        "user": "EugenDueck",
        "created_at": "2020-01-25T05:54:45Z",
        "body": "@dhagrow You are absolutely right!\r\n\r\nSo I checked your link and the docs, the only thing that readers can do and consumers cannot in terms of initial position is their ability to start at a specific message, denoted by message id."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T13:15:18Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 5978,
    "title": "[offload] CLI/Rest endpoint to list and describe offloaded topics of namespace",
    "created_at": "2020-01-02T16:16:30Z",
    "closed_at": "2022-12-09T10:18:55Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/5978",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently, it is difficult to find which topics of a namespace are offloaded to secondary storage. It would be have such a command / REST end point. There should be other related information (like size, time info, link?) against this list.\r\n\r\nThe one alternative as of today is the prometheus info where a query of the following nature could throw some insights..\r\n\r\n> pulsar_storage_offloaded_size != 0",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5978/comments",
    "author": "shiv4289",
    "comments": [
      {
        "user": "ivan970101",
        "created_at": "2020-04-15T14:28:04Z",
        "body": "I'm interested in this issue. May I have a try?"
      },
      {
        "user": "shiv4289",
        "created_at": "2020-04-24T11:54:57Z",
        "body": "sure @ivan970101. Please go ahead. I would be happy to be work as a reviewer."
      }
    ]
  },
  {
    "number": 5934,
    "title": "Can support read/write properties from/to Message in flink pulsar consumer/producer",
    "created_at": "2019-12-25T05:14:20Z",
    "closed_at": "2020-03-26T11:00:26Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/5934",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently, flink pulsar consumer/producer can not read/write properties from/to Message, we hope to support it.\r\n\r\n**Describe the solution you'd like**\r\nA) For consumer, \r\nchange method `deserialize(Message message)’` access level from private to proteced in class 'PulsarConsumerSource', and we can overrided it in derived class.\r\nB) For producer,\r\n1. add a method like following code,  and we can overrided it in derived class.\r\n`protected Map<String, String> generateProperties(IN value) {\r\n        return new HashMap<>();\r\n }`\r\n2. invoke TypedMessageBuilder.properties() method addtional in `invoke(IN value, Context context)` method at class 'FlinkPulsarProducer'.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5934/comments",
    "author": "duli559",
    "comments": [
      {
        "user": "duli559",
        "created_at": "2019-12-25T08:27:39Z",
        "body": "@yjshen , hi, can you have a look, thanks!"
      },
      {
        "user": "yjshen",
        "created_at": "2019-12-25T11:36:23Z",
        "body": "Hi @duli559 , thanks for bringing this up.\r\n\r\nFor the consumer, I think the proposed way is possible, your derived class could return a Tuple (Map, T) for each message, and use the tuple afterward.\r\n\r\nFor the producer, the `invoke` method is invoked for each `value` in the stream, therefore,  you should use this `value` and tell which part goes into properties and which part went goes into Message body. I think you could extend the existing `FlinkPulsarProducer` directly and just overwrite the `invoke` method?"
      },
      {
        "user": "duli559",
        "created_at": "2019-12-25T12:23:37Z",
        "body": "Hi @yjshen , thanks for your reply.\r\n\r\nI think Message properties not only created by `value`, but also other ways, eg: some constants, values that are updated periodically, etc. so i think if we extract a method in super class and override in derived class, it is a better way, and we don't need care other details in derived class.\r\n\r\nThanks!"
      },
      {
        "user": "yjshen",
        "created_at": "2019-12-25T13:58:56Z",
        "body": "@duli559 that makes sense to me. Please create a PR for this :)"
      }
    ]
  },
  {
    "number": 5927,
    "title": "Change the time unit of `patternAutoDiscoveryPeriod` to SECONDS",
    "created_at": "2019-12-24T05:44:38Z",
    "closed_at": "2020-02-08T08:39:40Z",
    "labels": [
      "help wanted",
      "area/client",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/5927",
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently the time unit of `patternAutoDiscoveryPeriod` is MINUTE. People is not able to configure the time duration to be leass than 1 minute.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt would be good to add an overloaded method in the consumer builder to support passing TimeUnit in.\r\n\r\n```\r\npatternAutoDiscoveryPeriod(int interval, TimeUnit unit);\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5927/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "zhenglaizhang",
        "created_at": "2019-12-24T07:42:31Z",
        "body": "i think i could help add the overload this week."
      },
      {
        "user": "sijie",
        "created_at": "2019-12-24T12:37:05Z",
        "body": "@zhenglaizhang awesome!"
      }
    ]
  },
  {
    "number": 5828,
    "title": "Netty UDP connector doesn't work",
    "created_at": "2019-12-09T14:46:19Z",
    "closed_at": "2019-12-14T02:40:39Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/5828",
    "body": "NettyServerHandler doesn't work for UDP as it will receive DatagramPacket instead of byte[]\r\n\r\nI managed to make the UDP connector working using a new NettyUDPServerHandler :\r\n\r\npublic class NettyUDPServerHandler extends SimpleChannelInboundHandler<DatagramPacket> {\r\n\r\n    private static final Logger logger = LoggerFactory.getLogger(NettyUDPServerHandler.class);\r\n    private NettySource nettySource;\r\n\r\n    public NettyUDPServerHandler(NettySource nettySource) {\r\n        this.nettySource = nettySource;\r\n    }\r\n    \r\n    @Override\r\n    protected void channelRead0(ChannelHandlerContext channelHandlerContext, DatagramPacket packet) throws Exception {\r\n        ByteBuf buf = packet.content();\r\n        byte[] bytes = new byte[buf.readableBytes()];\r\n        buf.readBytes(bytes);\r\n        buf.retain();\r\n        nettySource.consume(new NettyRecord(Optional.ofNullable(\"\"), bytes));\r\n    }\r\n\r\n    @Override\r\n    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\r\n        logger.error(\"Error when processing incoming data\", cause);\r\n        ctx.close();\r\n    }\r\n\r\n    @Data\r\n    static private class NettyRecord implements Record<byte[]>, Serializable {\r\n        private final Optional<String> key;\r\n        private final byte[] value;\r\n    }\r\n\r\n}",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5828/comments",
    "author": "gbensa",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-12-10T03:01:15Z",
        "body": "Thanks @gbensa for the issue and PR"
      }
    ]
  },
  {
    "number": 5827,
    "title": "Netty UDP connector ClassCastException",
    "created_at": "2019-12-09T12:52:07Z",
    "closed_at": "2019-12-14T02:40:39Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/5827",
    "body": "When trying to create a Netty udp connector, a ClassCastException is thrown\r\n\r\nconfigs:\r\n    type: \"udp\"\r\n    host: \"127.0.0.1\"\r\n    port: 10999\r\n    numberOfThreads: 1\r\n\r\nException : \r\n12:50:43.259 [nioEventLoopGroup-6-1] WARN  io.netty.channel.ChannelInitializer - Failed to initialize a channel. Closing: [id: 0xbeba8942]\r\njava.lang.ClassCastException: io.netty.channel.socket.nio.NioDatagramChannel cannot be cast to io.netty.channel.socket.SocketChannel\r\n\tat org.apache.pulsar.io.netty.server.NettyChannelInitializer.initChannel(NettyChannelInitializer.java:29) ~[pulsar-io-netty-2.4.2.nar-unpacked/:?]\r\n\tat io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:115) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:107) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:637) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.access$000(DefaultChannelPipeline.java:46) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline$PendingHandlerAddedTask.execute(DefaultChannelPipeline.java:1487) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.callHandlerAddedForAllHandlers(DefaultChannelPipeline.java:1161) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.invokeHandlerAddedIfNeeded(DefaultChannelPipeline.java:686) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:514) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:427) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:474) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_232]\r\n\r\n\r\nProposed solution :\r\nIn NettyChannelInitializer.java SocketChannel chould be replaced by Channel\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5827/comments",
    "author": "gbensa",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-12-10T03:06:37Z",
        "body": "👍 Thanks for the issue and PR @gbensa "
      }
    ]
  },
  {
    "number": 5763,
    "title": "Broker graceful shutdown when znode of loadbalance disappeared",
    "created_at": "2019-11-28T07:04:41Z",
    "closed_at": "2019-12-07T15:01:53Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/5763",
    "body": "**Describe the bug**\r\n```\r\n00:03:35.345 [pulsar-load-manager-4-1] WARN  org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl - Error writing broker data on ZooKeeper: {}\r\norg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /loadbalance/brokers/172.30.92.32:8080\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:114) ~[org.apache.pulsar-pulsar-zookeeper-2.5.0-SNAPSHOT.jar:2.5.0-SNAPSHOT]\r\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[org.apache.pulsar-pulsar-zookeeper-2.5.0-SNAPSHOT.jar:2.5.0-SNAPSHOT]\r\n\tat org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1336) ~[org.apache.pulsar-pulsar-zookeeper-2.5.0-SNAPSHOT.jar:2.5.0-SNAPSHOT]\r\n\tat org.apache.bookkeeper.zookeeper.ZooKeeperClient.access$3101(ZooKeeperClient.java:70) ~[org.apache.bookkeeper-bookkeeper-server-4.9.2.jar:4.9.2]\r\n\tat org.apache.bookkeeper.zookeeper.ZooKeeperClient$21.call(ZooKeeperClient.java:1065) ~[org.apache.bookkeeper-bookkeeper-server-4.9.2.jar:4.9.2]\r\n\tat org.apache.bookkeeper.zookeeper.ZooKeeperClient$21.call(ZooKeeperClient.java:1059) ~[org.apache.bookkeeper-bookkeeper-server-4.9.2.jar:4.9.2]\r\n\tat org.apache.bookkeeper.zookeeper.ZooWorker.syncCallWithRetries(ZooWorker.java:140) ~[org.apache.bookkeeper-bookkeeper-server-4.9.2.jar:4.9.2]\r\n\tat org.apache.bookkeeper.zookeeper.ZooKeeperClient.setData(ZooKeeperClient.java:1059) ~[org.apache.bookkeeper-bookkeeper-server-4.9.2.jar:4.9.2]\r\n\tat org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl.writeBrokerDataOnZooKeeper(ModularLoadManagerImpl.java:873) [org.apache.pulsar-pulsar-broker-2.5.0-SNAPSHOT.jar:2.5.0-SNAPSHOT]\r\n\tat org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerWrapper.writeLoadReportOnZookeeper(ModularLoadManagerWrapper.java:120) [org.apache.pulsar-pulsar-broker-2.5.0-SNAPSHOT.jar:2.5.0-SNAPSHOT]\r\n\tat org.apache.pulsar.broker.loadbalance.LoadReportUpdaterTask.run(LoadReportUpdaterTask.java:41) [org.apache.pulsar-pulsar-broker-2.5.0-SNAPSHOT.jar:2.5.0-SNAPSHOT]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181]\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_181]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [io.netty-netty-all-4.1.32.Final.jar:4.1.32.Final]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]\r\n00:03:35.752 [pulsar-io-22-37] WARN  org.apache.pulsar.broker.MessagingServiceShutdownHook - Graceful shutdown timeout expired. Closing nowzh\r\n```\r\n\r\n**Expected behavior**\r\nCatch the no node exception and create the disappeared znode\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5763/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-11-28T15:43:50Z",
        "body": "Hi @codelipenghui Thanks for the reporting. Seems the fix is direct,  need to \"Catch the no node exception and create the disappeared znode\" , right?"
      },
      {
        "user": "codelipenghui",
        "created_at": "2019-11-29T07:46:19Z",
        "body": "Yes, i think so, will fixed soon"
      }
    ]
  },
  {
    "number": 5752,
    "title": "Expose key shared consumer selector range in API",
    "created_at": "2019-11-26T13:04:32Z",
    "closed_at": "2020-11-17T00:40:14Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/5752",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen using Key-Shared subscription, there's currently no way to know which consumer will read a given message based on its key.\r\n\r\n**Describe the solution you'd like**\r\nIt would be nice if the selector range handled by a consumer is exposed in the relevant subscription stats API/endpoints.\r\nFor instance in /{topic}/stats.\r\n\r\nRelated to #4077 #4079 #4169 \r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5752/comments",
    "author": "cbornet",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-11-27T03:54:00Z",
        "body": "Thanks @cbornet for watching on this.  will put this in our backlog. It would be great if you or any other would like to contribute to it. "
      }
    ]
  },
  {
    "number": 5739,
    "title": "Use message bytes instead of message number for producer pending messages",
    "created_at": "2019-11-25T12:34:18Z",
    "closed_at": "2022-11-15T16:54:09Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/5739",
    "body": "In PR #5045 we introduce use bytes instead of message number to limit a batch message.  It would be good to following the same way to use bytes instead of message number for  the `maxPendingMessages` and `maxPendingMessagesAcrossPartitions`",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5739/comments",
    "author": "jiazhai",
    "comments": [
      {
        "user": "zhaohaidao",
        "created_at": "2019-11-30T02:06:02Z",
        "body": "Is there someone working on it?\r\nIf not, I want to have a try."
      },
      {
        "user": "codelipenghui",
        "created_at": "2019-12-01T01:03:47Z",
        "body": "welcome @zhaohaidao and looking forward to your contribution."
      },
      {
        "user": "Renkai",
        "created_at": "2020-08-19T15:53:07Z",
        "body": "I will try to work on this."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-15T16:54:09Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 5732,
    "title": "Automatic configure max publish rate limit based on NIC bandwidth",
    "created_at": "2019-11-25T01:26:35Z",
    "closed_at": "2022-11-15T16:54:22Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/5732",
    "body": "In PR #5710 , we provide a broker publish rate limiter, which provide a dynamic system config to set the rate limit. \r\nwe should consider automatically configuring the max bytes based on the NIC bandwidth that broker detects from the system. Pulsar already has this mechanism detecting the NIC bandwidth for load report. We should piggyback this functionality to re-use that feature.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5732/comments",
    "author": "jiazhai",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-11-15T16:54:22Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 5709,
    "title": "Support multiple topic subscriptions across multiple namespace",
    "created_at": "2019-11-20T12:57:00Z",
    "closed_at": "2020-06-04T06:23:13Z",
    "labels": [
      "help wanted",
      "area/client",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/5709",
    "body": "Currently, multiple topic subscriptions are supported in the same namespace. Some users want to support multi-topic subscriptions across multiple namespaces. The main consideration is authentication and authorization under multiple namespaces and multiple topics.\r\n\r\nFirst:\r\n\r\nMultiTopicsConsumerImpl.java => topicNamesValid\r\nRemove check logic\r\n\r\nSecond:\r\nCheck the content related to permissions, mainly for verification logic under multi-namespace and multi-topic, such as creating consumer, calling schema, authorizing, canceling permissions, etc\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5709/comments",
    "author": "tuteng",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-11-21T02:38:22Z",
        "body": "Seems this not need to much change, maybe remove the namespace checking, and check if all the the tests passed."
      }
    ]
  },
  {
    "number": 5704,
    "title": "revert terminate topic ",
    "created_at": "2019-11-20T01:00:24Z",
    "closed_at": "2022-12-09T04:36:14Z",
    "labels": [
      "help wanted",
      "deprecated/question"
    ],
    "url": "https://github.com/apache/pulsar/issues/5704",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nsometimes need restrict publish message，when use terminate function，the topic will can not be reverted as a normal topic\r\n\r\n**Describe the solution you'd like**\r\nSupport un-terminate function\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5704/comments",
    "author": "laxpio",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-11-21T02:57:52Z",
        "body": "@laxpio Thanks for open this issue. Do you mind share a little of your use case? why a topic is terminated but need to revert? "
      },
      {
        "user": "laxpio",
        "created_at": "2019-11-21T07:41:31Z",
        "body": "case 1:\r\nupdate/modify route function，before update function，need restrict message write to the topic，prevent dirty data from being written to other topics\r\ncase 2:\r\nMisoperation"
      },
      {
        "user": "Srar",
        "created_at": "2021-09-17T16:12:50Z",
        "body": "Same feature request."
      },
      {
        "user": "truong-hua",
        "created_at": "2022-10-28T02:27:42Z",
        "body": "Same expectation too"
      },
      {
        "user": "codelipenghui",
        "created_at": "2022-10-28T03:51:09Z",
        "body": "Looks like we can just delete the terminated topic and create a new one with the same topic name? After the topic is terminated, the consumer can continue to consume messages. We can safely delete the topic after the backlogs have been cleaned up."
      },
      {
        "user": "truong-hua",
        "created_at": "2022-10-30T16:18:20Z",
        "body": "Thank @codelipenghui for a workaround solution but I think in reality, what will happen to us is that we have to force delete the topic and make sure that every consumer can re-subscribe properly to the new topic. Besides, creating a topic of our system is not as easy as just creating but it's a provisioning process with many configurations/policies has to be applied like retention, message rate, subscription policies and etc. So that why we expect a un-terminate/resume method which will help to prevent the re-provisioning of the topic with a lot of verifications later. To me, it's likely why we need the terminate feature while we can block publishers by restricting their IPs. Another point worth talking is that we don't expect to lose all topic history after the termination which is very important to our business requirements."
      },
      {
        "user": "codelipenghui",
        "created_at": "2022-10-31T04:00:26Z",
        "body": "> To me, it's likely why we need the terminate feature while we can block publishers by restricting their IPs. Another point worth talking is that we don't expect to lose all topic history after the termination which is very important to our business requirements.\r\n\r\nI think it should not be a case that topic termination wants to resolve. We should only terminate the topic if we know the topic will not be used (producer first. consumes tries to drain the backlog then closes eventually.)\r\n\r\nIf the requirement is how to prevent the producers to publish new messages. I think we should use the publish rate limiter, backlog quota policy to block the producer or just a proxy to prevent producer connections."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T04:36:14Z",
        "body": "Closed as answered."
      }
    ]
  },
  {
    "number": 5694,
    "title": "PositionImpl init throws NullPointerException",
    "created_at": "2019-11-19T03:19:46Z",
    "closed_at": "2022-11-14T09:04:02Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/5694",
    "body": "**Describe the bug**\r\nversion 2.4.1\r\n\r\ncode path：\r\nmanaged-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/PositionImpl.java\r\n\r\ninit function \r\n```\r\npublic static PositionImpl get(PositionImpl other) {\r\n        return new PositionImpl(other);\r\n    }\r\npublic PositionImpl(PositionImpl other) {\r\n        this.ledgerId = other.ledgerId;\r\n        this.entryId = other.entryId;\r\n    }\r\n```\r\nwhen the param \"other\" be Null Object, the process will throw NullPointerException.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5694/comments",
    "author": "laxpio",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-11-21T03:37:27Z",
        "body": "@laxpio Thanks for reporting this. How about return a NULL when passed in NULL?"
      },
      {
        "user": "laxpio",
        "created_at": "2019-11-21T07:33:09Z",
        "body": "the readPosition will be  null when ledger list is null or can not find the nextValidPosition \r\nthe code as follow:\r\nmanaged-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedCursorImpl.java\r\n```\r\npublic void asyncReadEntries(final int numberOfEntriesToRead, final ReadEntriesCallback callback,\r\n            final Object ctx) {\r\n        checkArgument(numberOfEntriesToRead > 0);\r\n        if (isClosed()) {\r\n            callback.readEntriesFailed(new ManagedLedgerException(\"Cursor was already closed\"), ctx);\r\n            return;\r\n        }\r\n\r\n        PENDING_READ_OPS_UPDATER.incrementAndGet(this);\r\n        OpReadEntry op = OpReadEntry.create(this, PositionImpl.get(readPosition), numberOfEntriesToRead, callback, ctx);\r\n        ledger.asyncReadEntries(op);\r\n    }\r\n```\r\nmanaged-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/OpReadEntry.java\r\n```\r\npublic static OpReadEntry create(ManagedCursorImpl cursor, PositionImpl readPositionRef, int count,\r\n            ReadEntriesCallback callback, Object ctx) {\r\n        OpReadEntry op = RECYCLER.get();\r\n        op.readPosition = cursor.ledger.startReadOperationOnLedger(readPositionRef);\r\n        op.cursor = cursor;\r\n        op.count = count;\r\n        op.callback = callback;\r\n        op.entries = Lists.newArrayList();\r\n        op.ctx = ctx;\r\n        op.nextReadPosition = PositionImpl.get(op.readPosition);\r\n        return op;\r\n    }\r\n```\r\n"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-14T09:04:01Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant in maintained versions.\r\n\r\nIf we don't have a real-world issue on this NPE, I tend to close this kind of issue if there's no volunteer to improve."
      }
    ]
  },
  {
    "number": 5612,
    "title": "  Update Deprecated classes for MongoSink ",
    "created_at": "2019-11-11T09:04:33Z",
    "closed_at": "2019-12-20T05:37:44Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/connector"
    ],
    "url": "https://github.com/apache/pulsar/issues/5612",
    "body": "Some Classes have Deprecated in ”com.mongodb.async.client” package ，LIke MongoClient、MongoClients。\r\n\r\nshould we replace ”com.mongodb.async.client” package with ”com.mongodb.reactivestreams.client.”",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5612/comments",
    "author": "huangdx0726",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2019-11-12T09:13:25Z",
        "body": "@huangdx0726 Would you like to contribute to it?"
      },
      {
        "user": "huangdx0726",
        "created_at": "2019-11-12T09:42:01Z",
        "body": "OK，I’ll&nbsp; try\r\n\r\n\r\n\r\n---Original---\r\nFrom: \"Jia Zhai\"<notifications@github.com&gt;\r\nDate: Tue, Nov 12, 2019 17:13 PM\r\nTo: \"apache/pulsar\"<pulsar@noreply.github.com&gt;;\r\nCc: \"Mention\"<mention@noreply.github.com&gt;;\"huangdx0726\"<localhost80@foxmail.com&gt;;\r\nSubject: Re: [apache/pulsar]   Update Deprecated classes for MongoSink  (#5612)\r\n\r\n\r\n\r\n@huangdx0726 Would you like to contribute to it?\r\n \r\n—\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe."
      }
    ]
  },
  {
    "number": 5538,
    "title": "Pulsar function to read compacted topics",
    "created_at": "2019-11-02T08:10:24Z",
    "closed_at": "2020-07-02T07:14:31Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/5538",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nPulsar functions cannot currently be configured to read the compacted version of a topic.\r\n\r\n**Describe the solution you'd like**\r\nAn option to `pulsar-admin functions create/update`, and/or setting in `--function-config-file`, to request subscribing to the compacted version of a topic.\r\n\r\nNote: a function can have multiple topics as inputs.  There are already other options which are on a per-input basis (e.g. `custom-schema-inputs`, `custom-serde-inputs`) so it ought to work like those.\r\n\r\n**Describe alternatives you've considered**\r\nIt occurred to me that subscription options could be part of the topic URI, e.g.\r\n\r\n```\r\npersistent://public/default/my-topic?compacted=yes\r\n```\r\n\r\n... but that would be a major change if deployed everywhere that subscriptions are used.\r\n\r\n**Additional context**\r\nA similar option might be required for sinks.\r\n\r\nPR #5532 adds a similar feature to start a sink from \"earliest\" rather than \"latest\" message.  If that's just for sinks then it would be useful for functions too.\r\n\r\nHence there seems to be a more general case use around setting initial subscription options for both functions and sinks.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5538/comments",
    "author": "candlerb",
    "comments": [
      {
        "user": "315157973",
        "created_at": "2020-05-31T15:58:13Z",
        "body": "Please assign it to me @codelipenghui "
      }
    ]
  },
  {
    "number": 5450,
    "title": "Some functional requirements about pulsar sql",
    "created_at": "2019-10-23T13:40:55Z",
    "closed_at": "2022-12-06T10:45:20Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/sql",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/5450",
    "body": "**Is your feature request related to a problem? Please describe.**\r\n       There is currently a need to manage and display pulsar's unconsumed metadata. But currently pulsar SQL will display the consumed data.\r\n\r\n**Describe the solution you'd like**\r\n       According to the cursor to get the largest markDeletePosition, which is the location of the last spent cursor, and then reassemble the messageId, like this __message_id__ > '(850,0,0)'. Data pagination can then be paged according to __publish_time__, like this: __publish_time__ > timestamp'2019-10-23 21:32:00' LIMIT 100.If it is a partition theme, filter the corresponding data according to the partition.\r\n\r\n**Describe alternatives you've considered**\r\n       #Use the consumer to get the data directly, but not ack, do this to the data display.\r\n       # If pulsar is logically deleted, it should have a logo, which can consume the consumed data according to that identifier.\r\n\r\n Finally thank all the staff of pulsar.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5450/comments",
    "author": "apacheBright",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T10:45:20Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 5326,
    "title": "pulsar released package missing configuraion for ranges from bookkie",
    "created_at": "2019-10-07T18:24:19Z",
    "closed_at": "2020-01-25T03:21:01Z",
    "labels": [
      "type/bug",
      "help wanted"
    ],
    "url": "https://github.com/apache/pulsar/issues/5326",
    "body": "**Describe the bug**\r\nStandalone pulsar's bookie creates data/ranges folder even bookkeeper's data path is changed to another folder. This data/ranges folder is used by data.bookkeeper.ranges, which can be configured in the bookkeeper package downloaded from apache bookkeeper but not pulsar.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to the conf folder and change all data paths of the bookie. Don't use any path containing `data/`\r\n2. Execute standalone with `--bookkeeper-dir` pointing to other locations outside PULSAR_HOME for some time.\r\n\r\n**Expected behavior**\r\nNo data/ranges should exist because bookkeeper's data folder is changed to another location, but I can still see it.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5326/comments",
    "author": "hanbo1990",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2019-10-27T16:47:54Z",
        "body": "@hanbo1990 yeah, this should be simple fix. do you want to try to make a contribution to this?"
      },
      {
        "user": "hanbo1990",
        "created_at": "2019-10-27T20:07:31Z",
        "body": "Sure, I will do it. "
      },
      {
        "user": "hanbo1990",
        "created_at": "2020-01-16T14:27:26Z",
        "body": "I am going to create a config interface in pulsar's bookie.conf  \r\n`\r\nstorage.range.store.dirs=data/bookkeeper/ranges   \r\n\r\nstorage.serve.readonly.tables=false\r\n\r\nstorage.cluster.controller.schedule.interval.ms=30000`  \r\n\r\nIs that fine? Or they are not important at all from pulsar view and I will hard code them in the code and put the ranges folder under bookie data dir"
      }
    ]
  },
  {
    "number": 5285,
    "title": "Schema support for Flatbuffers??",
    "created_at": "2019-09-26T19:07:49Z",
    "closed_at": "2022-12-09T04:58:49Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/5285",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nOrganizationally we have been looking at a variety of serialization solutions, there has been significant interest around flatbuffers as a messaging model.   This has a lot of benefits for IOT style devices and communication between services on device.   This also has potential benefits for rpc and/or rest style binary payloads.\r\n\r\n**Describe the solution you'd like**\r\nI am interested in understanding how the pulsar project views flatbuffers vis-a-vis the currently supported schema models.   Is this a yeah that seems interesting maybe someday?  Or a yeah, we are definitely going to be working on that?  Or a yeah, no, thats not part of our future strategy.\r\n\r\n**Describe alternatives you've considered**\r\nObviously there are strategies that could be employed to using protobuf and simply convert etc, or revert to JSON.  So these are all on the table for us, but the perceived performance gains resulting from a non-copy access seem to be something that could quite beneficial to pulsar.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5285/comments",
    "author": "cavanaug",
    "comments": [
      {
        "user": "aahmed-se",
        "created_at": "2019-09-28T08:44:09Z",
        "body": "It's possible to add flatbuffers schema support just like protobuf. But it's not clear what the advantages would be. Flatbuffers is generally used in the context of low latency ipc not really suitable for persisting messages in log based pub sub systems."
      },
      {
        "user": "vicaya",
        "created_at": "2019-10-03T17:01:16Z",
        "body": "What about people want to use non-persistent topics for lower latency pubsub for certain topics? Flatbuffer also uses a lot less CPU and memory allocations (esp. vs protobuf). We'd welcome things that would reduce the CPU/memory usage of pulsar broker."
      },
      {
        "user": "merlimat",
        "created_at": "2019-10-03T17:06:46Z",
        "body": "> We'd welcome things that would reduce the CPU/memory usage of pulsar broker.\r\n\r\nKeep in mind that serialization/deserialization are done only by producers/consumer. Brokers will not do these at any point. The only thing that broker does is to validate the schema definition."
      },
      {
        "user": "sijie",
        "created_at": "2019-10-27T16:33:00Z",
        "body": "@cavanaug \r\n\r\nAs what Matteo pointed out, the serialization and deserialization are only done at the client side. Brokers only deal with the schema definition and validation. So adding a new type is trivial if we are able to get the schema definition. I am not familiar with flatbuffer. but if there is a way to get schema definition, we can easily add it."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T04:58:49Z",
        "body": "Closed as stale and no one works on it. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 5264,
    "title": "It takes too long for the client to reconnect to the broker server",
    "created_at": "2019-09-24T07:49:52Z",
    "closed_at": "2022-12-09T04:59:03Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/5264",
    "body": "**Description error**\r\n\r\nproducer.serviceUrl(\"server1:6650,server2:6650,server3:6650\")\r\nIf the producer connection to server1, kill -9 server1, will cause the producer to reconnect continuously within 2-5 minutes.\r\n\r\nThis time too long.\r\n\r\n**Reproduced**\r\nSteps to reproduce the behavior:\r\n1. producer.serviceUrl(\"server1:6650,server2:6650,server3:6650\")\r\n2. If the producer is connected to server1\r\n3. kill -9 server1\r\n4. will cause the producer to reconnect continuously within 2-5 minutes.\r\n\r\n**Expected behavior**\r\nI hope that pulsar will provide the number of retries and retry time configurations to the user.\r\n\r\n**Desktop (please fill in the information below):**\r\n- system: linux, pulsar-2.4",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/5264/comments",
    "author": "JoyJava",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T04:59:03Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 4940,
    "title": "Serialization and Deserialization for MultiMessageIdImpl",
    "created_at": "2019-08-12T15:19:18Z",
    "closed_at": "2023-04-11T02:26:00Z",
    "labels": [
      "help wanted",
      "area/client",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4940",
    "body": "In PR #4911, we added MultiMessageIdImpl, which contains a Map of <topic, MessageId>, that could be used for partition/multi-topics/pattern consumer methods, e.g. seek(), ackCumulative(), getLastMessageId().\r\n\r\nIn streaming applications, they tends to store messageId along with the state. so that the streaming applications can restore the state back. we need to provide a consistent behavior for how to serialize and deserialize different message id implementations.\r\n\r\nThis issue is to track and discuss uniforming the serialization and deserialization of message id implementations (basically we need the ability to deserialize the message ids from the byte array in a consistent way).",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4940/comments",
    "author": "jiazhai",
    "comments": [
      {
        "user": "gvolpe",
        "created_at": "2022-04-15T10:59:17Z",
        "body": "Hi folks, has this issue been discussed somewhere? I just came across this limitation when trying out `ackCumulativeAsync` in my application, and I would like to help to get it fixed."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-14T08:54:33Z",
        "body": "@BewareMyPower IIRC you're working on the message ID related stuff now. Do you have this issue in mind?"
      },
      {
        "user": "BewareMyPower",
        "created_at": "2022-11-14T09:14:09Z",
        "body": "Yeah. I think we should remove the `MultiMessageIdImpl` class and disable `getLastMessageId` API for the multi-topics consumer. Instead, we should add a new API. But I'm still trying to figure out a good way and preparing a proposal."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-14T09:46:15Z",
        "body": "@BewareMyPower Make sense. Then you can take over this issue and close it with a fix or superseding proposal :)"
      }
    ]
  },
  {
    "number": 4816,
    "title": "Pulsar functions context potential OOM ",
    "created_at": "2019-07-25T21:54:07Z",
    "closed_at": "2022-12-09T05:30:58Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/function",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4816",
    "body": "Pulsar functions ContextImpl getProducer method caches created producers in a private hash map publishProducers and never voids any of them. Once producer goes to the map, it stays there forever.\r\n\r\nIf you start a function and it will publish to unbounded set of random topics, you'll finally get OOM.\r\n\r\nIt would be great to clean up the map based on the last producer access timestamp. Like 1 or 10 minutes after the last access. (I cannot offer a reasonable timeout, cause I don't know how expensive a producer and its creation in the context of function)\r\n\r\nApplicable to Pulsar 2.4.0 and earlier.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4816/comments",
    "author": "zubchenok",
    "comments": [
      {
        "user": "jerrypeng",
        "created_at": "2019-07-25T22:41:14Z",
        "body": "@zubchenok is there a reason why you are publishing to an unbounded set of random topics from a function?  That is not a typical use case.  We can perhaps add the capability to set a maximum number of topics any function publish to."
      },
      {
        "user": "zubchenok",
        "created_at": "2019-07-28T10:25:41Z",
        "body": "We use hundred thousands topics per day (topic per user session). I've already explained in detail our case in Pulsar community earlier.\r\n\r\nI don't think this is reasonable to set such limit, because you can implement a smart cache or improve the functions API to publish messages without explicit creation a publisher per topic. "
      },
      {
        "user": "jerrypeng",
        "created_at": "2019-07-28T17:41:11Z",
        "body": "> you can implement a smart cache\r\n\r\nWe can implement a cache for producers.  That closes \"old\" producers automatically\r\n\r\n> improve the functions API to publish messages without explicit creation a publisher per topic.\r\n\r\nThis cannot be done in Pulsar in general.  With the current producer API, it one producer per topic"
      },
      {
        "user": "jiazhai",
        "created_at": "2020-07-20T06:27:28Z",
        "body": "Add label `help wanted` for the contributor that want to contribute to this feature.\r\nas above comments: \"We can implement a cache for producers. That closes \"old\" producers automatically\""
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T05:30:58Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 4807,
    "title": "Pulsar schema service should retry on concurrent schema updates",
    "created_at": "2019-07-25T09:21:20Z",
    "closed_at": "2022-12-09T14:13:59Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4807",
    "body": "**Describe the bug**\r\n\r\nCurrently Pulsar doesn't retry on exceptions thrown when concurrently updating same schema. So the exception is propagated to Pulsar client directly. This is not a good experience for clients.\r\n\r\nWe should consider handling errors and retry it again.\r\n\r\nExample exceptions are:\r\n\r\n```\r\norg.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion\r\n\tat org.apache.pulsar.client.api.PulsarClientException.unwrap(PulsarClientException.java:297) ~[pulsar-client-api-2.5.0-2cc34afc0.jar:2.5.0-2cc34afc0]\r\n\tat org.apache.pulsar.client.impl.ProducerBuilderImpl.create(ProducerBuilderImpl.java:88) ~[pulsar-client-all-2.5.0-2cc34afc0.jar:2.5.0-2cc34afc0]\r\n```",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4807/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "areebahmed04",
        "created_at": "2019-09-11T20:17:22Z",
        "body": "@sijie I'd like to work on this. How should I proceed? I'm new to this project."
      },
      {
        "user": "TC-oKozlov",
        "created_at": "2019-10-17T20:01:45Z",
        "body": "Hi, what's the recommended work around ? I'm getting this intermittently while trying to a 16-thread producer on 16-core machine, 1 time out of 2-3 runs"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T14:13:59Z",
        "body": "Closed as stale. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 4777,
    "title": "Can't limits consumer's rate using command \"set-subscription-dispatch-rate\"",
    "created_at": "2019-07-22T11:57:55Z",
    "closed_at": "2021-05-23T02:24:56Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/cli",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/4777",
    "body": "**Describe the bug**\r\nCan't limits consumer's rate using command \"set-subscription-dispatch-rate\"\r\n\r\n**To Reproduce**\r\n\r\n1.  Set subscription message-dispatch-rate\r\n\r\n```\r\n# bin/pulsar-admin namespaces set-subscription-dispatch-rate public/default --msg-dispatch-rate 10 --dispatch-rate-period 1\r\n\r\n{\r\n  \"dispatchThrottlingRateInMsg\" : 10,\r\n  \"dispatchThrottlingRateInByte\" : -1,\r\n  \"ratePeriodInSecond\" : 1\r\n}\r\n```\r\n\r\n2. case 1:  Set Batching=true in pulsar.ProducerOptions, but the result doesn't I want\r\n\r\n```\r\nproducer, err := client.CreateProducer(pulsar.ProducerOptions{\r\n\tTopic:              test-topic-1,\r\n\tBatching:           true,\r\n\tBlockIfQueueFull:   true,\r\n\tSendTimeout:        10,\r\n\tMaxPendingMessages: 10000,\r\n})\r\n```\r\n\r\n```\r\n# ./pulsar-admin  topics stats persistent://public/default/test-topic-1\r\n\r\n{\r\n  \"msgRateIn\" : 0.0,\r\n  \"msgThroughputIn\" : 0.0,\r\n  \"msgRateOut\" : 583.2003004842348,\r\n  \"msgThroughputOut\" : 616088.1840958354,\r\n  \"averageMsgSize\" : 0.0,\r\n  \"storageSize\" : 1849634,\r\n  \"publishers\" : [ ],\r\n  \"subscriptions\" : {\r\n    \"test-1\" : {\r\n      \"msgRateOut\" : 583.2003004842348,\r\n      \"msgThroughputOut\" : 616088.1840958354,\r\n      \"msgRateRedeliver\" : 0.0,\r\n      \"msgBacklog\" : 14,\r\n      \"blockedSubscriptionOnUnackedMsgs\" : false,\r\n      \"msgDelayed\" : 0,\r\n      \"unackedMessages\" : 0,\r\n      \"type\" : \"Exclusive\",\r\n      \"activeConsumerName\" : \"145283\",\r\n      \"msgRateExpired\" : 0.0,\r\n      \"consumers\" : [ {\r\n        \"msgRateOut\" : 583.2003004842348,\r\n        \"msgThroughputOut\" : 616088.1840958354,\r\n        \"msgRateRedeliver\" : 0.0,\r\n        \"consumerName\" : \"145283\",\r\n        \"availablePermits\" : -955,\r\n        \"unackedMessages\" : 0,\r\n        \"blockedConsumerOnUnackedMsgs\" : false,\r\n        \"metadata\" : { },\r\n        \"address\" : \"/10.111.4.158:29127\",\r\n        \"connectedSince\" : \"2019-07-22T19:32:24.877+08:00\",\r\n        \"clientVersion\" : \"2.3.1\"\r\n      } ],\r\n      \"isReplicated\" : false\r\n    }\r\n  },\r\n  \"replication\" : { },\r\n  \"deduplicationStatus\" : \"Disabled\"\r\n}\r\n```\r\n\r\n3. case 2:  Set Batching=false in pulsar.ProducerOptions, but the result doesn't I want\r\n\r\n```\r\nproducer, err := client.CreateProducer(pulsar.ProducerOptions{\r\n\tTopic:              test-topic-1,\r\n\tBatching:           false,\r\n\tBlockIfQueueFull:   true,\r\n\tSendTimeout:        10,\r\n\tMaxPendingMessages: 10000,\r\n})\r\n```\r\n\r\n```\r\n./pulsar-admin  topics stats persistent://public/default/test-topic-1\r\n\r\n{\r\n  \"msgRateIn\" : 0.0,\r\n  \"msgThroughputIn\" : 0.0,\r\n  \"msgRateOut\" : 50.14999267057858,\r\n  \"msgThroughputOut\" : 52904.95893460692,\r\n  \"averageMsgSize\" : 0.0,\r\n  \"storageSize\" : 7004225,\r\n  \"publishers\" : [ ],\r\n  \"subscriptions\" : {\r\n    \"test-1\" : {\r\n      \"msgRateOut\" : 50.14999267057858,\r\n      \"msgThroughputOut\" : 52904.95893460692,\r\n      \"msgRateRedeliver\" : 0.0,\r\n      \"msgBacklog\" : 53,\r\n      \"blockedSubscriptionOnUnackedMsgs\" : false,\r\n      \"msgDelayed\" : 0,\r\n      \"unackedMessages\" : 0,\r\n      \"type\" : \"Exclusive\",\r\n      \"activeConsumerName\" : \"145283\",\r\n      \"msgRateExpired\" : 0.0,\r\n      \"consumers\" : [ {\r\n        \"msgRateOut\" : 50.14999267057858,\r\n        \"msgThroughputOut\" : 52904.95893460692,\r\n        \"msgRateRedeliver\" : 0.0,\r\n        \"consumerName\" : \"145283\",\r\n        \"availablePermits\" : -917,\r\n        \"unackedMessages\" : 0,\r\n        \"blockedConsumerOnUnackedMsgs\" : false,\r\n        \"metadata\" : { },\r\n        \"address\" : \"/10.111.4.158:29127\",\r\n        \"connectedSince\" : \"2019-07-22T19:32:24.877+08:00\",\r\n        \"clientVersion\" : \"2.3.1\"\r\n      } ],\r\n      \"isReplicated\" : false\r\n    }\r\n  },\r\n  \"replication\" : { },\r\n  \"deduplicationStatus\" : \"Disabled\"\r\n}\r\n```\r\n\r\n4. case 3:  Set Batching=true and BatchingMaxMessages=2 in pulsar.ProducerOptions, but the result doesn't I want\r\n\r\n```\r\nproducer, err := client.CreateProducer(pulsar.ProducerOptions{\r\n\tTopic:              test-topic-1,\r\n\tBatching:           true,\r\n\tBlockIfQueueFull:   true,\r\n\tSendTimeout:        10,\r\n\tMaxPendingMessages: 10000,\r\n        BatchingMaxMessages: 2,\r\n})\r\n```\r\n\r\n```\r\n# ./pulsar-admin  topics stats persistent://public/default/test-topic-1\r\n\r\n{\r\n  \"msgRateIn\" : 0.0,\r\n  \"msgThroughputIn\" : 0.0,\r\n  \"msgRateOut\" : 19.983333794948344,\r\n  \"msgThroughputOut\" : 21562.417164758503,\r\n  \"averageMsgSize\" : 0.0,\r\n  \"storageSize\" : 51715374,\r\n  \"publishers\" : [ ],\r\n  \"subscriptions\" : {\r\n    \"test-1\" : {\r\n      \"msgRateOut\" : 19.983333794948344,\r\n      \"msgThroughputOut\" : 21562.417164758503,\r\n      \"msgRateRedeliver\" : 0.0,\r\n      \"msgBacklog\" : 1351,\r\n      \"blockedSubscriptionOnUnackedMsgs\" : false,\r\n      \"msgDelayed\" : 0,\r\n      \"unackedMessages\" : 0,\r\n      \"type\" : \"Exclusive\",\r\n      \"activeConsumerName\" : \"145283\",\r\n      \"msgRateExpired\" : 0.0,\r\n      \"consumers\" : [ {\r\n        \"msgRateOut\" : 19.983333794948344,\r\n        \"msgThroughputOut\" : 21562.417164758503,\r\n        \"msgRateRedeliver\" : 0.0,\r\n        \"consumerName\" : \"145283\",\r\n        \"availablePermits\" : 708,\r\n        \"unackedMessages\" : 0,\r\n        \"blockedConsumerOnUnackedMsgs\" : false,\r\n        \"metadata\" : { },\r\n        \"address\" : \"/10.111.4.158:29127\",\r\n        \"connectedSince\" : \"2019-07-22T19:32:24.877+08:00\",\r\n        \"clientVersion\" : \"2.3.1\"\r\n      } ],\r\n      \"isReplicated\" : false\r\n    }\r\n  },\r\n  \"replication\" : { },\r\n  \"deduplicationStatus\" : \"Disabled\"\r\n}\r\n```\r\n\r\n**Expected behavior**\r\n\r\n\"msgRateOut\"  is  about 10.0\r\n\r\n**Desktop (please complete the following information):**\r\n - OS:  Linux x86_64 GNU/Linux\r\n - Producer: pulsar-2.4.0\r\n - Consumer: pulsar-2.3.1",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4777/comments",
    "author": "zhangbitao",
    "comments": [
      {
        "user": "jiazhai",
        "created_at": "2020-07-20T01:54:49Z",
        "body": "@zhangbitao If batch is true, it maybe not accurate, but if batch == false also not work, there should be some issue. According to your description,  this is easy to reproduce, would also mark this as 'help wanted'. so other contributor could help on this issue."
      }
    ]
  },
  {
    "number": 4772,
    "title": "Rename the --destination-topic-name switch to --output for the Pulsar Admin CLI create source command",
    "created_at": "2019-07-21T19:07:18Z",
    "closed_at": "2022-12-06T09:59:52Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/function",
      "area/cli",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4772",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nThe sink create command has a --input switch, so this feature would bring consistency between the two commands in terms of specifying the topics.\r\n\r\n**Describe the solution you'd like**\r\nRename the --destination-topic-name switch to --output switch for the Pulsar Admin CLI create source command \r\n\r\n**Describe alternatives you've considered**\r\nN/A\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4772/comments",
    "author": "david-streamlio",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T09:59:52Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 4651,
    "title": "HDFS sink with different schema",
    "created_at": "2019-07-01T13:41:13Z",
    "closed_at": "2022-11-10T01:27:39Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/connector",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4651",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nIt seems that the hdfs sink connector can only parse string messages, this is a problem when working with schemas or already encoded messages that can't be parse to string (let's say encrypted messages for example) and want to send those messages to a data lake in hdfs for cold storage.\r\n \r\n**Describe the solution you'd like**\r\nI would like that the hdfs connector supports schemas different than StringSchema, or at least the raw schema (bytes[]).\r\n\r\n**Describe alternatives you've considered**\r\nImplement tiered storage for hdfs as well, S3 wont be a solution if working on-premises.\r\n\r\n**Additional context**\r\nIf the topic's schema is different to StringSchema you can see the pulsar console printing a IncompatibleSchemaException when you try to run the HDFS sink connector. If you check the connector's status, you will see a IO exception error and multiple retries.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4651/comments",
    "author": "avatart93",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-11-10T01:27:39Z",
        "body": "Closed as no response. It seems the community has little interest on this topic.\r\n\r\nTo attract more attention, @avatart93 you may create a one-more-step concrete proposal and post it on the dev@ mailing list ([subscribe](mailto:dev@pulsar.apache.org))."
      }
    ]
  },
  {
    "number": 4641,
    "title": "The project does not build with JDK 12. ",
    "created_at": "2019-06-29T17:19:06Z",
    "closed_at": "2020-11-04T01:15:37Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/build"
    ],
    "url": "https://github.com/apache/pulsar/issues/4641",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nPulsar does not build with JDK 12.    It requires JDK 1.8.\r\nJDK 1.8 will end of life in Dec 2020.  That is a year and a half away, but conversion may take a while.\r\n\r\n**Describe the solution you'd like**\r\nPlease start an effort (branch) to support JDK 12\r\n\r\n**Describe alternatives you've considered**\r\nOpenJDK\r\n\r\n**Additional context**\r\nMany many many systems still use 1.8 and will for potentially yrs.  It can be very difficult to support a mission critical system that uses several different jre versions.  You may need to start to support 12 soon and continue to support 8 and 12 for several years.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4641/comments",
    "author": "annubiz",
    "comments": [
      {
        "user": "merlimat",
        "created_at": "2019-07-02T17:55:01Z",
        "body": "@annubiz We do support JDK 11, (even though the unit tests are not completely passing, mostly due to Mockito and Powermock issues), though we haven't started to check with JDK 12. \r\n\r\nIt would be good to have a Jenkins pipeline with JDK 12 setup for 2.5 release"
      },
      {
        "user": "leonidv",
        "created_at": "2019-10-14T04:42:25Z",
        "body": "I've run Apache Pulsar 2.4.1 in standalone mode on open-jdk-13:\r\n\r\n`\r\n 07:28:28.461 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:host.name=lv-linux\r\n07:28:28.461 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.version=13\r\n07:28:28.461 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.vendor=N/A\r\n07:28:28.461 [main] INFO  org.apache.zookeeper.server.ZooKeeperServer - Server environment:java.home=/usr/lib/jvm/java-13-openjdk\r\n`\r\n\r\nI've remove -XX:+AggressiveOpts from PULSAR_GC"
      },
      {
        "user": "codelipenghui",
        "created_at": "2020-05-19T05:55:09Z",
        "body": "@annubiz Is @leonidv's comment can help? Move it to 2.7.0 first, feel free to move it back if it needs to onboard 2.6.0 release."
      }
    ]
  },
  {
    "number": 4618,
    "title": "Return empty object of Namespace Isolation Policies instead of PulsarAdminException$NotFoundException",
    "created_at": "2019-06-27T03:02:17Z",
    "closed_at": "2022-12-06T10:00:29Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/cli",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4618",
    "body": "**Describe the bug**\r\n\r\nAPI: _**/admin/v2/clusters/{cluster}/namespaceIsolationPolicies**_ returns _**PulsarAdminException$NotFoundException**_ after creating new cluster\r\nThis is not an exception while cluster is just created.\r\n\r\n**Expected behavior**\r\n\r\nReturn `{}` if not exist.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4618/comments",
    "author": "hungndiam",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T10:00:29Z",
        "body": "Closed as stale. Please open a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 4506,
    "title": "Compressed batch payload size reported incorrectly in Java client",
    "created_at": "2019-06-10T21:34:30Z",
    "closed_at": "2022-12-06T23:08:38Z",
    "labels": [
      "type/bug",
      "type/enhancement",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4506",
    "body": "**Describe the bug**\r\nIn a batch and compression enabled producer, the uncompressed size of each message remains in the OpSendMsg, (`op.setBatchSizeByte(batchMessageContainer.currentBatchSizeBytes);`)\r\ninstead of the compressed size, which is correctly set for non-batch messages (`op.setBatchSizeByte(encryptedPayload.readableBytes());`). These are inside `ProducerImpl.java`.\r\n\r\nConsumer stats also reports the uncompressed size, but I'm less concerned about that.\r\n\r\nAlso the max batch size is split according to the uncompressed size - that is intentional, correct?\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Send batch message with and without compression\r\n2. Print `org.apache.pulsar.client.api.ProducerStats#getTotalBytesSent`\r\n3. Message size is always the same though the compressed one should be different\r\n\r\n**Expected behavior**\r\nMessage size in producer stats should reflect the compressed size\r\n\r\n**Additional context**\r\nLow impact since I expect the broker stats to reflect the compressed size.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4506/comments",
    "author": "sleungtoast",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2019-06-13T08:38:53Z",
        "body": "@sleungtoast maybe we can have two different metrics, one for compressed, the other for the raw bytes?"
      },
      {
        "user": "sleungtoast",
        "created_at": "2019-06-13T13:28:42Z",
        "body": "That sounds reasonable. "
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T23:08:37Z",
        "body": "We're using `op.setBatchSizeByte(encryptedPayload.readableBytes());` for batch payload now.\r\n\r\nClosed. Please open a new issue if it's still relevant."
      }
    ]
  },
  {
    "number": 4498,
    "title": "Seperate name of namespaces, topics from its parent",
    "created_at": "2019-06-09T04:07:34Z",
    "closed_at": "2022-12-09T05:55:23Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/4498",
    "body": "I'm on a frond-end project to manage Pulsar by Angular.\r\nWhen I call API, received Namespace is: _**public/default**_ : _**public**_ is tenant, but the name actually is **_default_**. Or with Topic _**persistent://public/functions/assignments**_: topic is _**assignments**_\r\n\r\n#### Expected behavior \r\n\r\nI want to have APIs to get topics, namespaces with their properties like\r\n\r\n> [\r\n  {\r\n    \"name\": \"metadata\",\r\n    \"tenant\": \"public\",\r\n    \"namespace\": \"functions\",\r\n    \"type\": \"persistent\",\r\n    \"link\": \"persistent://public/functions/metadata\"\r\n  },\r\n  {\r\n    \"name\": \"assignments\",\r\n    \"tenant\": \"public\",\r\n    \"namespace\": \"functions\",\r\n    \"type\": \"persistent\",\r\n    \"link\": \"persistent://public/functions/assignments\"\r\n  }\r\n]\r\n\r\n#### Actual behavior\r\n\r\nNow what I get is:\r\n\r\n> [\r\n\"persistent://public/functions/metadata\", \r\n\"persistent://public/functions/assignments\", \r\n\"persistent://public/functions/coordinate\"\r\n]\r\n\r\n#### Steps to reproduce\r\n\r\nAdd new APIs.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4498/comments",
    "author": "hungndiam",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-09T05:55:23Z",
        "body": "Closed as stale and no one works on it. Please create a new issue if it's still relevant to the maintained versions.\r\n\r\nAPI changes should be go through a PIP process and I suggest you to throw the proposal on the dev@ mailing list."
      }
    ]
  },
  {
    "number": 4481,
    "title": "a little change needed of pulsar cli",
    "created_at": "2019-06-05T11:17:39Z",
    "closed_at": "2019-06-27T02:50:05Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/4481",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen I tried to use pulsar cli to test my code, send a json string to a topic, like this:\r\n```\r\n./pulsar-client produce my-topic --messages '{\"appId\":\"test\",\"userId\":38,\"phone\":\"13800138000\",\"score\":664}'\r\n```\r\nBut the default behavior of pulsar-client produce is use comma as separator to divide messages, so my json divided into several messages which I didn't want to, like this:\r\n```\r\n19:06:03.742 [main] INFO  org.apache.pulsar.client.cli.PulsarClientTool - 4 messages successfully produced\r\n```\r\nAnd I couldn't find any way to escape the comma(backslash / double backslash did not work).\r\nAnd after that I try to put the json into a file and use -f option, like:\r\n```\r\n./pulsar-client produce my-topic -f ./test.json\r\n```\r\nAnd I got \r\n```\r\njava.nio.file.NoSuchFileException: ./test.json\r\n```\r\nSo I had to use the full path.\r\n\r\n**Describe the solution you'd like**\r\n* Do not use comma as default separator in cli, or at least tell users how to escape it in the command line help.\r\n* Support relative path in cli as other clis.\r\n\r\n**Describe alternatives you've considered**\r\nI can't imagine.\r\n\r\n**Additional context**\r\nnone.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4481/comments",
    "author": "sxcooler",
    "comments": [
      {
        "user": "liketic",
        "created_at": "2019-06-09T14:15:20Z",
        "body": "I think there are two things we can do for this:\r\n1. split messages with another separator line \\n.\r\n2. add support for loading messages from file.\r\n\r\n@jiazhai Which one do your prefer? Or both."
      },
      {
        "user": "sijie",
        "created_at": "2019-06-13T08:31:33Z",
        "body": "2 sounds a good option to start. because 1 will change some default behaviors which can involve backward compatibility discussion."
      },
      {
        "user": "liketic",
        "created_at": "2019-06-23T14:36:27Z",
        "body": "@sxcooler Which version are you using? I didn't reproduce this bug on master branch."
      },
      {
        "user": "sxcooler",
        "created_at": "2019-06-24T02:40:57Z",
        "body": "> @sxcooler Which version are you using? I didn't reproduce this bug on master branch.\r\n\r\nI was using 2.3.2 on Mac."
      },
      {
        "user": "liketic",
        "created_at": "2019-06-27T01:40:32Z",
        "body": "@sxcooler 2.4.0 is on the way. I would like to suggest you try 2.4.0 once it's released. "
      },
      {
        "user": "sxcooler",
        "created_at": "2019-06-27T02:50:05Z",
        "body": "> @sxcooler 2.4.0 is on the way. I would like to suggest you try 2.4.0 once it's released.\r\n\r\nThanks a lot!"
      },
      {
        "user": "dmi-kov",
        "created_at": "2020-01-24T15:43:02Z",
        "body": "hey @liketic, I've got the same issue on the latest (2.5.0) version of Pulsar."
      },
      {
        "user": "eladchen",
        "created_at": "2020-03-03T12:24:27Z",
        "body": "Hi, \r\n\r\nThis still appears to be broken (using version 2.5.0)"
      }
    ]
  },
  {
    "number": 4279,
    "title": "some ZK path for Pulsar metadata is not configurable",
    "created_at": "2019-05-15T01:49:55Z",
    "closed_at": "2019-05-19T13:06:34Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/4279",
    "body": "some ZK path for Pulsar metadata is hard coded and not configurable, It would be great to make them configurable, so user could leverage their existing zk cluster, and put all pulsar related data in user configured place. ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4279/comments",
    "author": "jiazhai",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2019-05-19T13:06:34Z",
        "body": "@jiazhai you can use zookeeper chroot to configure the root path for pulsar metadata. \r\n\r\ne.g. \"zk:2181/newroot\"\r\n\r\nSo I don't think we need a code change for this."
      }
    ]
  },
  {
    "number": 4180,
    "title": "Delete Inactive Topic should also clean up the schema",
    "created_at": "2019-04-30T12:27:22Z",
    "closed_at": "2019-05-26T02:17:10Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/cli"
    ],
    "url": "https://github.com/apache/pulsar/issues/4180",
    "body": "Similar as #3941, we should also clean up schema when deleting inactive topic. Otherwise the schema information would be just left there :(",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/4180/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "hellozepp",
        "created_at": "2019-05-01T09:46:36Z",
        "body": "Hi~ @sijie \r\nI have reviewed CmdTopics.Java, is not the feature you mentioned the following code?\r\n```\r\n @Parameters(commandDescription = \"Delete a topic. \\n\"\r\n            + \"\\t\\tThe topic cannot be deleted if there's any active subscription or producers connected to it.\")\r\n...\r\n  if (deleteSchema) {\r\n                admin.schemas().deleteSchema(topic);\r\n            }\r\n```"
      },
      {
        "user": "sijie",
        "created_at": "2019-05-05T08:13:13Z",
        "body": "@hellozepp \r\n\r\nSorry for late response. the code snippet you pasted is the feature I mentioned in #3941 \r\n\r\nHowever the feature here is about doing the same thing when pulsar broker deleteInactiveTopic. In #3941 , users delete topic explicitly using pulsar-admin. In this issue, it is broker itself deleting the topics when they are inactive. Hope this give you the idea about what this issue for. "
      },
      {
        "user": "liketic",
        "created_at": "2019-05-11T08:18:15Z",
        "body": "hi @sijie , I opened #4262 , happy to hear your thoughts. :)"
      }
    ]
  },
  {
    "number": 3805,
    "title": "Pulsar should use bookkeeper's default values",
    "created_at": "2019-03-12T09:43:50Z",
    "closed_at": "2022-12-07T00:32:43Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/3805",
    "body": "*Motivation*\r\n\r\nCurrently Pulsar overrides some bookkeeper default values. For example, `maxPendingReadRequestsPerThread`. I would suggest making them using the default values provided by bookkeeper, rather than overriding the values in pulsar. This would make things much clearer and easier to maintain.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3805/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-12-07T00:32:43Z",
        "body": "Closed as stale and no one worked on it. Please open a new issue if it's still relevant to the maintained versions.\r\n\r\nFYI @hangc0276 @zymap "
      }
    ]
  },
  {
    "number": 3643,
    "title": "[client] Support seek at partitioned topic",
    "created_at": "2019-02-21T08:03:29Z",
    "closed_at": "2020-11-17T02:11:01Z",
    "labels": [
      "help wanted",
      "area/client",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/3643",
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently seeking is not supported on partitioned topics.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt should be doable to support seek on partitioned topics.\r\n\r\nBecause:\r\n\r\n1) seeking by earliest / latest can just seek on all partitions.\r\n2) seeking by messageId is also doable. because messageId provides partition id.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNo other alternative. We can just decide not to support it.\r\n\r\n**Additional context**\r\n\r\nIt would be much easier for people to use if we support `seek` at partitioned topics.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3643/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "merlimat",
        "created_at": "2019-02-21T08:15:16Z",
        "body": "> * seeking by earliest / latest can just seek on all partitions. \r\n\r\nAgree. It doesn't make sense not to support earliest/latest... \r\n\r\n> 2\\. seeking by messageId is also doable. because messageId provides partition id.\r\n\r\nIf the seek operation only affects the cursor in one partition, it might become confusing to a user, since it's a different behavior from the above. \r\n\r\nIn general, I'm not a big fan of seek by message id on the consumer interface, since the purpose of `Consumer` should be that the reading position is \"managed\" by the system. If an application wants to position to a particular message id, it's more of manual handling and maybe `Reader` is a better option there."
      },
      {
        "user": "ltamber",
        "created_at": "2019-12-19T09:00:38Z",
        "body": "@sijie Is this feature expected to have time to do?"
      }
    ]
  },
  {
    "number": 3614,
    "title": "Provide a NAR way to load customized implemented of class in broker config",
    "created_at": "2019-02-16T05:31:12Z",
    "closed_at": "2022-11-11T08:17:54Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/broker",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/3614",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nSome user is asking Provide a NAR way to load customized implemented of class in broker config.\r\ne.g. \r\nloadManagerClassName\r\nauthorizationProvider\r\nschemaRegistryStorageClassName",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3614/comments",
    "author": "jiazhai",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2022-11-11T08:17:54Z",
        "body": "Closed as stale.\r\n\r\nThe development of this initiative seems stalled and this thread doesn't contain too much information. If one wants to revive the proposal, please open a brand new thread.\r\n\r\nIIRC we release connectors in NAR format, not sure whether it's supported already."
      }
    ]
  },
  {
    "number": 3600,
    "title": "Intermittent test failure in C++ BasicEndToEndTest.testSyncFlushBatchMessages ",
    "created_at": "2019-02-14T17:53:05Z",
    "closed_at": "2019-02-23T17:39:05Z",
    "labels": [
      "help wanted",
      "area/test"
    ],
    "url": "https://github.com/apache/pulsar/issues/3600",
    "body": "\r\n\r\n```\r\n[86/148] BasicEndToEndTest.testSyncFlushBatchMessages (2428 ms)\r\n\u001b[0;33mNote: Google Test filter = BasicEndToEndTest.testSyncFlushBatchMessages\r\n\u001b[m\u001b[0;32m[==========] \u001b[mRunning 1 test from 1 test case.\r\n\u001b[0;32m[----------] \u001b[mGlobal test environment set-up.\r\n\u001b[0;32m[----------] \u001b[m1 test from BasicEndToEndTest\r\n\u001b[0;32m[ RUN      ] \u001b[mBasicEndToEndTest.testSyncFlushBatchMessages\r\n2019-02-14 17:21:32.335 INFO  ConnectionPool:72 | Created connection for pulsar://localhost:6650\r\n2019-02-14 17:21:32.337 INFO  ClientConnection:300 | [127.0.0.1:34920 -> 127.0.0.1:6650] Connected to broker\r\n2019-02-14 17:21:32.343 INFO  BatchMessageContainer:43 | { BatchContainer [size = 0] [batchSizeInBytes_ = 0] [maxAllowedMessageBatchSizeInBytes_ = 131072] [maxAllowedNumMessagesInBatch_ = 10] [topicName = persistent://public/default/test-flush-batch-messages-1550164892] [producerName_ = ] [batchSizeInBytes_ = 0] [numberOfBatchesSent = 0] [averageBatchSize = 0]} BatchMessageContainer constructed\r\n2019-02-14 17:21:32.344 INFO  HandlerBase:52 | [persistent://public/default/test-flush-batch-messages-1550164892, ] Getting connection from pool\r\n2019-02-14 17:21:32.546 INFO  ProducerImpl:155 | [persistent://public/default/test-flush-batch-messages-1550164892, ] Created producer on broker [127.0.0.1:34920 -> 127.0.0.1:6650]\r\n2019-02-14 17:21:32.547 INFO  Client:88 | Subscribing on Topic :test-flush-batch-messages-1550164892\r\n2019-02-14 17:21:32.551 INFO  HandlerBase:52 | [persistent://public/default/test-flush-batch-messages-1550164892, subscription-name, 0] Getting connection from pool\r\n2019-02-14 17:21:32.717 INFO  ConsumerImpl:169 | [persistent://public/default/test-flush-batch-messages-1550164892, subscription-name, 0] Created consumer on broker [127.0.0.1:34920 -> 127.0.0.1:6650]\r\n2019-02-14 17:21:32.718 INFO  BasicEndToEndTest:2132 | sending first half messages in async, should timeout to receive\r\n2019-02-14 17:21:33.720 INFO  BasicEndToEndTest:2147 | sending the other half messages in async, should able to receive\r\n/pulsar/pulsar-client-cpp/tests/BasicEndToEndTest.cc:2149: Failure\r\nValue of: consumer.receive(receivedMsg, 1000)\r\n  Actual: TimeOut\r\nExpected: ResultOk\r\nWhich is: Ok\r\n2019-02-14 17:21:34.721 WARN  ConsumerImpl:97 | [persistent://public/default/test-flush-batch-messages-1550164892, subscription-name, 0] Destroyed consumer which was not properly closed\r\n2019-02-14 17:21:34.721 INFO  ConsumerImpl:823 | [persistent://public/default/test-flush-batch-messages-1550164892, subscription-name, 0] Closing consumer for topic persistent://public/default/test-flush-batch-messages-1550164892\r\n\u001b[0;31m[  FAILED  ] \u001b[mBasicEndToEndTest.testSyncFlushBatchMessages (2389 ms)\r\n\u001b[0;32m[----------] \u001b[m1 test from BasicEndToEndTest (2389 ms total)\r\n\r\n\u001b[0;32m[----------] \u001b[mGlobal test environment tear-down\r\n\u001b[0;32m[==========] \u001b[m1 test from 1 test case ran. (2390 ms total)\r\n\u001b[0;32m[  PASSED  ] \u001b[m0 tests.\r\n\u001b[0;31m[  FAILED  ] \u001b[m1 test, listed below:\r\n\u001b[0;31m[  FAILED  ] \u001b[mBasicEndToEndTest.testSyncFlushBatchMessages\r\n\r\n 1 FAILED TEST\r\n```",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3600/comments",
    "author": "merlimat",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2019-02-21T09:19:55Z",
        "body": "it seems we can just increase the timeout."
      }
    ]
  },
  {
    "number": 3463,
    "title": "Unable to delete a namespace using pulsar-admin CLI",
    "created_at": "2019-01-28T21:24:49Z",
    "closed_at": "2019-08-18T12:21:57Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/cli"
    ],
    "url": "https://github.com/apache/pulsar/issues/3463",
    "body": "**Expected behavior**\r\nDelete a namespace created in tenant/namespace format after running namespaces command using pulsar-admin CLI. \r\n\r\n**Actual behavior**\r\nI can only delete namespace created in tenant/cluster/namespace format. \r\n\r\nReturns below error when deleting namespace created with tenant/namespace format.\r\n\r\n```\r\nnull\r\nReason: javax.ws.rs.ProcessingException: java.net.ProtocolException: Server redirected too many  times (20)\r\ncommand terminated with exit code 1\r\n```\r\n\r\n**Steps to reproduce** \r\n1. pulsar-admin tenants create sample --admin-roles admin\r\n2. pulsar-admin namespaces create sample/ns\r\n3. pulsar-admin namespaces set-clusters sample/ns --clusters us-central\r\n4. pulsar-admin namespaces delete sample/ns\r\n\r\n**System configuration**\r\napache-pulsar-2.2.1 on google kubernetes cluster. Pulsar cluster name us-central. ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3463/comments",
    "author": "shrikant-pa",
    "comments": [
      {
        "user": "liketic",
        "created_at": "2019-01-29T15:39:39Z",
        "body": "I can't reproduce this bug on master branch. @jiazhai , do you have any suggestions ? "
      },
      {
        "user": "shrikant-pa",
        "created_at": "2019-01-29T15:49:50Z",
        "body": "So this feels like GKE deployment issue. The source code YAML files have pulsar-gke as local cluster name but metadata has us-central as cluster name. While creating namespaces it was causing issue and had to set the replication cluster for each namespace and cluster names were different. \r\n\r\nI redeployed GKE cluster and changed cluster name in metadata to pulsar-gke as this value is provided in other yaml files. After this i did not encounter this issue. "
      },
      {
        "user": "sijie",
        "created_at": "2019-08-18T12:21:57Z",
        "body": "Closed this issue since it seems to be related GKE issue."
      }
    ]
  },
  {
    "number": 3436,
    "title": "Add configuration properties to limit the automatic creation of topics.",
    "created_at": "2019-01-25T13:33:29Z",
    "closed_at": "2019-08-18T12:21:09Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/broker"
    ],
    "url": "https://github.com/apache/pulsar/issues/3436",
    "body": "**Describe**\r\nWhen Pulsar client creates a producer or consumer, it first gets partition-metadata.\r\nIf partition-metadata is not queried, and the topic does not exist, the broker will automatically create a non-partition topic.\r\n\r\n**Describe the solution you'd like**\r\n Add configuration parameters which can limit the broker to automatically create a non-partition topic.\r\n\r\n#issue-number",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3436/comments",
    "author": "bilahepan",
    "comments": [
      {
        "user": "bilahepan",
        "created_at": "2019-01-25T13:33:45Z",
        "body": "#3430  is related"
      },
      {
        "user": "sijie",
        "created_at": "2019-01-26T10:08:33Z",
        "body": "+1 on providing a configuration setting to disable topic auto-creation."
      },
      {
        "user": "ConcurrencyPractitioner",
        "created_at": "2019-01-27T00:47:23Z",
        "body": "Ok, I have a pretty good idea on how to add these configuration parameters. Will start working on this one."
      },
      {
        "user": "sijie",
        "created_at": "2019-08-18T12:21:09Z",
        "body": "This is fixed by #3450 "
      }
    ]
  },
  {
    "number": 3361,
    "title": "Kafka configure properties are not included subscription type and subscription mode",
    "created_at": "2019-01-11T17:00:48Z",
    "closed_at": "2019-01-21T15:14:42Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/3361",
    "body": "I am trying to use the latest pulsar release to migrate current kafka client to pulsar and try to use the kafka pulsar wrapper.\r\n\r\nI am using non-persistent topic,\r\nin the kafka pulsar client config, Following is the default\r\n\"subscriptionTopicsMode\" : \"PERSISTENT\", \r\n \"subscriptionType\" : \"Failover\",\r\n\r\nIs there anyway to set subscriptionTopicsMode and subscriptionType through the kafka configuration properties map?",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3361/comments",
    "author": "sautran",
    "comments": [
      {
        "user": "merlimat",
        "created_at": "2019-01-11T17:08:20Z",
        "body": "I agree that `subscriptionTopicsMode` should be configurable. Though I believe `subscriptionType` can only be `Failover`, since Kafka API has no option to do individual acknowledge of messages."
      },
      {
        "user": "sautran",
        "created_at": "2019-01-11T17:15:54Z",
        "body": "For the subscriptionType, right now our kafka application have a few consumers using the same group.id on a partitioned topic.  Each consumer reads some partitions. My question is in Pulsar,  if one consumer die, are other consumers taking those partitions over?"
      },
      {
        "user": "merlimat",
        "created_at": "2019-01-11T18:24:43Z",
        "body": "@sautran That's correct. A \"failover\" subscription over a partitioned topic will work in a similar way as of a Kafka consumer group. That's the reason we hardcode the the subscription type to \"failover\" in the Kafka client wrapper."
      },
      {
        "user": "sautran",
        "created_at": "2019-01-11T18:26:18Z",
        "body": "@merlimat Great! Thanks. "
      },
      {
        "user": "sautran",
        "created_at": "2019-01-21T17:43:13Z",
        "body": "Thanks, Guys."
      }
    ]
  },
  {
    "number": 3343,
    "title": "Proxy shouldn't require both brokerWebServiceURL & brokerServiceURL if http is being used for lookup",
    "created_at": "2019-01-09T15:23:54Z",
    "closed_at": "2023-01-11T14:30:34Z",
    "labels": [
      "help wanted",
      "type/feature",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/3343",
    "body": "This is in the context of configuring a proxy to not use ZK. Rather than requiring that the use specify the brokerServiceURL, if there is a brokerWebServiceURL specified, we should fallback to that and do http lookup.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3343/comments",
    "author": "ivankelly",
    "comments": [
      {
        "user": "tisonkun",
        "created_at": "2023-01-11T14:30:34Z",
        "body": "Closed as stale and no one seems to work on it. Please create a new issue if it's still relevant to the maintained versions."
      }
    ]
  },
  {
    "number": 3253,
    "title": "Can  Producer and Consumer run with jdk1.7?",
    "created_at": "2018-12-26T02:50:48Z",
    "closed_at": "2022-11-09T17:03:57Z",
    "labels": [
      "help wanted",
      "deprecated/question",
      "area/client",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/3253",
    "body": "In my env, there are some 'old-system' which running with JDK1.7 and Kafka,\r\nbut now I want to use Pulsar.\r\nSo, can Pulsar's Producer and Consumer run with jdk1.7? \r\nHow to?",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3253/comments",
    "author": "cumtwwei",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2018-12-26T19:54:44Z",
        "body": "@cumtwwei currently the java client requires java 8+, since it uses a lot of features like CompletableFuture, Java8 Functions. \r\n\r\none thing we can do is to add a java 6 or java 7 client that wraps existing C++ client using JNI."
      },
      {
        "user": "vngantk",
        "created_at": "2019-01-07T04:09:04Z",
        "body": "I have been doing a lot of Kotlin programming recently. One of the advantages of Kotlin is that it still supports JDK 1.6 because Kotlin is a very popular language for developing Android apps and the support for JDK 1.6 is a must. One approach to provide a pre-JDK1.8 client library is to rewrite or create an additional client library which is written in pure Kotlin. With proper annotations added, the Kotlin version of the client library can also be accessed by Java code without the need to program in Kotlin. This change also addresses the need for supporting Android as client."
      },
      {
        "user": "vngantk",
        "created_at": "2019-01-07T04:18:12Z",
        "body": "To create a Kotlin client library, we do not need to do it completely from scratch. Jetbrains' Intellij provides a tool to convert Java code to Kotlin. Although the conversion is not perfect and still requires additional manual fixes and porting work, it can be a very good initial version for making a workable Kotlin client library."
      },
      {
        "user": "sijie",
        "created_at": "2019-01-07T05:30:36Z",
        "body": "Writing a client is probably easy. As what you said, there are tools on doing the initial work. However I think the main concern is maintainability (e.g. is the client production battle tested, are there people dedicated on maintaining it, fixing bugs and etc). What we have done in pulsar is trying to maintain two battle tested versions of clients, one is java and the other is c++. All other non-java clients are just simple wrappers of C++ client. This approach is kind of working well and ensure having high quality pulsar clients of different languages. Although I am not familiar with kotlin, I can’t speak what would be the best approach for kotlin."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-09T17:03:57Z",
        "body": "The client requires 1.8 now.\r\n\r\nClosed as information delivered."
      }
    ]
  },
  {
    "number": 3074,
    "title": "pulsar-client produce topic -f file in cluster error if you use relative path",
    "created_at": "2018-11-28T04:11:58Z",
    "closed_at": "2022-11-10T06:50:13Z",
    "labels": [
      "type/bug",
      "help wanted",
      "area/client",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/3074",
    "body": "pulsar-client produce topic -f file in cluster error if you use relative path\r\nSolution：\r\n\r\nyou should use Absolute path",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/3074/comments",
    "author": "xluckly",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2018-11-28T21:37:12Z",
        "body": "@Hanqingkuo are you interested in contributing a bug fix for this?"
      },
      {
        "user": "xluckly",
        "created_at": "2018-11-30T07:59:13Z",
        "body": "yes\n\nAt 2018-11-29 05:37:24, \"Sijie Guo\" <notifications@github.com> wrote:\n\n\n@Hanqingkuo are you interested in contributing a bug fix for this?\n\n—\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread."
      },
      {
        "user": "tisonkun",
        "created_at": "2022-11-10T06:50:13Z",
        "body": "Closed as stale. Feel free to open a new issue if it's still relevant for maintained versions.\r\n\r\nI noticed that current codebase uses `Paths.get` to resolve paths so it may not be an issue anymore."
      }
    ]
  },
  {
    "number": 2939,
    "title": "Config maxUnackMessagesPerConsumer and maxUnackMessagesPerSubscription on a namespace",
    "created_at": "2018-11-06T04:06:46Z",
    "closed_at": "2020-02-04T04:06:36Z",
    "labels": [
      "help wanted",
      "type/feature"
    ],
    "url": "https://github.com/apache/pulsar/issues/2939",
    "body": "For now, we can config maxUnackMessagesPerConsumer and maxUnackMessagesPerSubscription in broker.conf.\r\n\r\nIt's useful to support config it on namespace level to overwrite global broker config.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/2939/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "wolfstudy",
        "created_at": "2020-02-04T04:06:36Z",
        "body": "The issue has been fixed in #5936 "
      }
    ]
  },
  {
    "number": 2912,
    "title": "Get message by messageId",
    "created_at": "2018-11-02T08:03:47Z",
    "closed_at": "2020-04-22T09:13:21Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/cli"
    ],
    "url": "https://github.com/apache/pulsar/issues/2912",
    "body": "Is it possible to support a rest api to get message by messageId? For now we can peek the head message but can't query message by messageId.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/2912/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "foreversunyao",
        "created_at": "2018-11-15T07:47:05Z",
        "body": "rest api could have bad performance to do this ?"
      },
      {
        "user": "codelipenghui",
        "created_at": "2018-11-15T08:00:26Z",
        "body": "@foreversunyao I'm not sure ! Maybe better read from bookie straightforward."
      },
      {
        "user": "sijie",
        "created_at": "2020-01-14T16:45:04Z",
        "body": "I think reading a message by a message-id is the same as peeking messages. "
      }
    ]
  },
  {
    "number": 2758,
    "title": "Provide example yaml files for pulsar builtin connectors",
    "created_at": "2018-10-10T01:14:16Z",
    "closed_at": "2022-12-06T10:31:50Z",
    "labels": [
      "help wanted",
      "doc",
      "area/connector",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/2758",
    "body": "We should provide example yaml files for pulsar builtin connectors. ",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/2758/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2019-02-21T09:22:59Z",
        "body": "I think we already have the generator to automatically generate example yaml files from the code. so should just integration the generator as part of website build."
      },
      {
        "user": "Anonymitaet",
        "created_at": "2019-08-02T03:11:56Z",
        "body": "@tuteng as discussed offline, could you please help generate the yaml files automatically?"
      },
      {
        "user": "Anonymitaet",
        "created_at": "2019-08-26T02:44:30Z",
        "body": "@tuteng any progress on this issue?"
      },
      {
        "user": "tisonkun",
        "created_at": "2022-12-06T10:31:50Z",
        "body": "Closed as stale and vague. Please specify what information is missing on which pages."
      }
    ]
  },
  {
    "number": 2323,
    "title": "Dataset by pulsar functions",
    "created_at": "2018-08-07T07:24:17Z",
    "closed_at": "2022-12-06T23:09:21Z",
    "labels": [
      "help wanted",
      "type/feature",
      "area/function"
    ],
    "url": "https://github.com/apache/pulsar/issues/2323",
    "body": "When use pulsar functions, The ending strategy of consume messages is useful in dataset scenes.\r\n\r\nSuch as:\r\n\r\n1. Ending by message produce time.\r\n2. Ending by message id.\r\n3. Ending by user(user support the ending strategy).\r\n\r\nIs this feature suitable for pulsar function? \r\nThe reason for this feature is i think pulsar function can support offline not only stream.\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/2323/comments",
    "author": "codelipenghui",
    "comments": [
      {
        "user": "srkukarni",
        "created_at": "2018-08-07T23:50:40Z",
        "body": "This feature indeed make sense. Is it possible for you to to volunteer to add this functionality?"
      }
    ]
  },
  {
    "number": 2122,
    "title": "Allow configuring backoff strategy on clients",
    "created_at": "2018-07-09T22:23:00Z",
    "closed_at": "2019-04-02T16:41:04Z",
    "labels": [
      "type/enhancement",
      "help wanted",
      "area/client"
    ],
    "url": "https://github.com/apache/pulsar/issues/2122",
    "body": "*Problem*\r\n\r\nCurrently the backoff strategy on pulsar clients are hardcoded with 100ms backoff. The backoff settings are too aggressive when you have a lot of topics, it would be good to make these settings configurable.",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/2122/comments",
    "author": "sijie",
    "comments": [
      {
        "user": "zubchenok",
        "created_at": "2018-07-09T22:25:59Z",
        "body": "When it happens during bundles movement to another broker, it will be much better to wait until the correspondent bundles are moved until the next retry.\r\n\r\nClients could be probably notified by brokers about bundles status."
      },
      {
        "user": "ConcurrencyPractitioner",
        "created_at": "2019-03-10T02:51:22Z",
        "body": "@sijie I could take on this issue. :)"
      },
      {
        "user": "sijie",
        "created_at": "2019-03-10T02:59:18Z",
        "body": "@ConcurrencyPractitioner ok.\r\n\r\nAssigned to @ConcurrencyPractitioner "
      },
      {
        "user": "ConcurrencyPractitioner",
        "created_at": "2019-03-16T16:39:03Z",
        "body": "Hi, Just a meta-comment.\r\nThe backoff strategy in Pulsar is implemented by a class literally named ```Backoff```. I noticed that this particular class is used in both pulsar-broker and pulsar-client. So I'm curious:  should we only make the backoff in pulsar-client configurable or should the backoff strategy in pulsar-broker also be configurable? Thought it might be related."
      }
    ]
  },
  {
    "number": 509,
    "title": "C++ Client - Producer is never destructed",
    "created_at": "2017-06-21T01:02:39Z",
    "closed_at": "2022-03-23T06:35:39Z",
    "labels": [
      "type/bug",
      "help wanted",
      "lifecycle/stale"
    ],
    "url": "https://github.com/apache/pulsar/issues/509",
    "body": "#### Expected behavior\r\n\r\nWhen producer goes out of scope the producer should be destructed.\r\n\r\n#### Actual behavior\r\n\r\nConsumer and Client are destructed but the producer never gets destructed.\r\n\r\n#### Steps to reproduce\r\n\r\na. Enable Debug logging\r\nb. Add inner function  ({..}) to any unit test and put 5 seconds sleep after closing braces.\r\nc. Observe the behavior in the log files - you will never see ~Producer\r\n\r\n#### System configuration\r\n",
    "comments_url": "https://api.github.com/repos/apache/pulsar/issues/509/comments",
    "author": "jai1",
    "comments": [
      {
        "user": "sijie",
        "created_at": "2018-11-28T21:52:01Z",
        "body": "@jai1 you are working on this? "
      },
      {
        "user": "jiazhai",
        "created_at": "2020-01-03T04:58:21Z",
        "body": "From the description, This seems to be easy to reproduce, mark as help-wanted"
      },
      {
        "user": "BewareMyPower",
        "created_at": "2022-03-22T05:26:19Z",
        "body": "Here is the reproduce code.\r\n\r\n```c++\r\n#include <pulsar/Client.h>\r\n#include <chrono>\r\n#include <thread>\r\n\r\nusing namespace pulsar;\r\n\r\nint main() {\r\n    ClientConfiguration conf;\r\n    conf.setLogger(new ConsoleLoggerFactory(Logger::LEVEL_DEBUG));\r\n    Client client(\"pulsar://localhost:6650\", conf);\r\n    {\r\n        Producer producer;\r\n        Result result = client.createProducer(\"my-topic\", producer);\r\n        if (result != ResultOk) {\r\n            return -1;\r\n        }\r\n    }\r\n\r\n    std::this_thread::sleep_for(std::chrono::seconds(5));\r\n    client.close();\r\n}\r\n```\r\n\r\nThe destruction happens after the `client.close()` is called. (see the 3rd line)\r\n\r\n```\r\n2022-03-22 13:11:10.064 DEBUG [0x70000debe000] ClientImpl:586 | listenerExecutorProvider_ is closed\r\n2022-03-22 13:11:10.064 DEBUG [0x70000debe000] ClientImpl:588 | partitionListenerExecutorProvider_ is closed\r\n2022-03-22 13:11:10.064 DEBUG [0x70000debe000] ProducerImpl:110 | [persistent://public/default/my-topic, standalone-5-1] ~ProducerImpl\r\n2022-03-22 13:11:10.064 INFO  [0x70000debe000] ProducerImpl:653 | Producer - [persistent://public/default/my-topic, standalone-5-1] , [batchMessageContainer = { BatchMessageContainer [size = 0] [bytes = 0] [maxSize = 1000] [maxBytes = 131072] [topicName = persistent://public/default/my-topic] [numberOfBatchesSent_ = 1] [averageBatchSize_ = 0] }]\r\n```"
      }
    ]
  }
]