[
  {
    "number": 8997,
    "title": "Per-sample weights using masks",
    "created_at": "2020-06-10T04:03:53Z",
    "closed_at": "2021-04-15T05:50:34Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/8997",
    "body": "Hi DL4J community.\r\n\r\nIs it currently possible to specify per-sample weights by using mask values other than 0 or 1? (ie. 0.5 for half the weight of a normal sample, 2 for twice the weight). Alternatively could samples be weighted by applying a non-integer to a label value? I haven't been able to find any DL4J documentation on per-sample weights.",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/8997/comments",
    "author": "cacophany53",
    "comments": [
      {
        "user": "cacophany53",
        "created_at": "2020-07-09T13:53:38Z",
        "body": "Does anyone have any insights into this?"
      },
      {
        "user": "agibsonccc",
        "created_at": "2021-04-15T05:50:34Z",
        "body": "Closing this. For future readers, please use our forums for these kinds of questions. The issue tracker is for  bugs. As of now, only per label weights are supported."
      }
    ]
  },
  {
    "number": 8587,
    "title": "Object Detection using Darknet19",
    "created_at": "2020-01-01T18:08:27Z",
    "closed_at": "2020-01-02T06:33:06Z",
    "labels": [
      "Question",
      "DL4J"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/8587",
    "body": "I used DL4J Yolo and TinyYolo examples for object detection using opencv it works fine\r\nBut When I first used Darknet19 I saw that there is no way to get the detected object boundaries\r\nYolo has Yolo2OutputLayer but Darknet19 has loss output layer and there is no way to get boundaries.\r\nI checked documentation/examples/tests but couldn't find a way.\r\nIs there any guide or documentation for such an issue?",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/8587/comments",
    "author": "Paranormaly",
    "comments": [
      {
        "user": "saudet",
        "created_at": "2020-01-02T06:32:34Z",
        "body": "Darknet doesn't do object detection on its own. That's what the Yolo layers do.\r\n"
      }
    ]
  },
  {
    "number": 8274,
    "title": "Platform \"android-x86\" not supported by class org.bytedeco.hdf5.global.hdf5",
    "created_at": "2019-10-08T05:55:34Z",
    "closed_at": "2019-10-08T06:17:15Z",
    "labels": [
      "Question",
      "DL4J Keras",
      "Duplicate"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/8274",
    "body": "#### Platform \"android-x86\" not supported by class org.bytedeco.hdf5.global.hdf5\r\n\r\nI'm trying to import hdf5 keras model in android application and geting the\r\nPlatform \"android-x86\" not supported by class org.bytedeco.hdf5.global.hdf5 runtime error.\r\n\r\n#### Version Information\r\n* Deeplearning4j version\r\n\r\nThat is my android dependencies\r\n\r\ncompile (group: 'org.deeplearning4j', name: 'deeplearning4j-core', version: '1.0.0-beta4') {\r\n            exclude group: 'org.bytedeco', module: 'opencv-platform'\r\n            exclude group: 'org.bytedeco', module: 'leptonica-platform'\r\n            /*exclude group: 'org.bytedeco', module: 'hdf5-platform'*/\r\n            exclude group: 'org.nd4j', module: 'nd4j-base64'\r\n        }\r\n\r\n\r\nI'm trying to run on android.\r\n\r\n#### Additional Information\r\n The following is my stack trace.\r\nio.reactivex.exceptions.UndeliverableException: java.lang.UnsatisfiedLinkError: Platform \"android-x86\" not supported by class org.bytedeco.hdf5.global.hdf5\r\n        at io.reactivex.plugins.RxJavaPlugins.onError(RxJavaPlugins.java:367)\r\n        at io.reactivex.internal.schedulers.ScheduledRunnable.run(ScheduledRunnable.java:69)\r\n        at io.reactivex.internal.schedulers.ScheduledRunnable.call(ScheduledRunnable.java:57)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:301)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n        at java.lang.Thread.run(Thread.java:919)\r\n     Caused by: java.lang.UnsatisfiedLinkError: Platform \"android-x86\" not supported by class org.bytedeco.hdf5.global.hdf5\r\n        at org.bytedeco.javacpp.Loader.load(Loader.java:984)\r\n        at org.bytedeco.javacpp.Loader.load(Loader.java:963)\r\n        at org.deeplearning4j.nn.modelimport.keras.Hdf5Archive.<clinit>(Hdf5Archive.java:59)\r\n        at org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.modelHdf5Filename(KerasModelBuilder.java:225)\r\n        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:222)\r\n        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:113)\r\n        at com.geek.vois.service.KerasHelper$1.call(KerasHelper.java:38)\r\n        at com.geek.vois.service.KerasHelper$1.call(KerasHelper.java:31)\r\n        at io.reactivex.internal.operators.observable.ObservableFromCallable.subscribeActual(ObservableFromCallable.java:42)\r\n        at io.reactivex.Observable.subscribe(Observable.java:12030)\r\n        at io.reactivex.internal.operators.observable.ObservableSubscribeOn$SubscribeTask.run(ObservableSubscribeOn.java:96)\r\n        at io.reactivex.Scheduler$DisposeTask.run(Scheduler.java:579)\r\n        at io.reactivex.internal.schedulers.ScheduledRunnable.run(ScheduledRunnable.java:66)\r\n        at io.reactivex.internal.schedulers.ScheduledRunnable.call(ScheduledRunnable.java:57) \r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) \r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:301) \r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) \r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641) \r\n        at java.lang.Thread.run(Thread.java:919) \r\n\r\n\r\n#### Contributing\r\n\r\nIf you'd like to help us fix the issue by contributing some code, but would\r\nlike guidance or help in doing so, please mention it!\r\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/8274/comments",
    "author": "aungpyaekyaw",
    "comments": [
      {
        "user": "saudet",
        "created_at": "2019-10-08T06:17:15Z",
        "body": "The message says it all: HDF5 doesn't support Android!"
      }
    ]
  },
  {
    "number": 8201,
    "title": "DataVec: Add NHWC support for ImageRecordReader / NativeImageLoader",
    "created_at": "2019-09-05T03:42:19Z",
    "closed_at": "2020-05-04T07:17:58Z",
    "labels": [
      "Enhancement",
      "Question",
      "DataVec / ETL"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/8201",
    "body": "",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/8201/comments",
    "author": "blackhu",
    "comments": [
      {
        "user": "saudet",
        "created_at": "2019-09-07T02:50:07Z",
        "body": "At the moment, this is hard coded, but we can use NDArray.permute() on the arrays returned by NativeImageLoader as a post-processing step."
      },
      {
        "user": "blackhu",
        "created_at": "2019-09-09T01:38:56Z",
        "body": "> At the moment, this is hard coded, but we can use NDArray.reshape() on the arrays returned by NativeImageLoader as a post-processing step.\r\n\r\nBut reshape cannot reshape result is (height,weight,channel)"
      },
      {
        "user": "AlexDBlack",
        "created_at": "2019-09-09T02:13:10Z",
        "body": "You want permute, not reshape.\r\n\r\n```\r\nINDArray nhwc = imgNchw.permute(0,2,3,1);\r\n```"
      },
      {
        "user": "blackhu",
        "created_at": "2019-09-09T02:19:11Z",
        "body": "> ```\r\n> INDArray nhwc = imgNchw.permute(0,2,3,1);\r\n> ```\r\n\r\nThat's ok. Thank you very much"
      }
    ]
  },
  {
    "number": 8158,
    "title": "Build error: ld undefined reference to GOMP",
    "created_at": "2019-08-28T00:52:40Z",
    "closed_at": "2019-08-28T04:14:31Z",
    "labels": [
      "Question",
      "LIBND4J"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/8158",
    "body": "Trying to build for the first time. Fedora 30, fresh install/VM:\r\n\r\n> sudo yum install maven make cmake g++\r\n\r\n\r\n> [ 98%] Building CXX object tests_cpu/layers_tests/CMakeFiles/runtests.dir/VariableSpaceTests.cpp.o\r\n> [ 98%] Building CXX object tests_cpu/layers_tests/CMakeFiles/runtests.dir/VariableTests.cpp.o\r\n> [ 98%] Building CXX object tests_cpu/layers_tests/CMakeFiles/runtests.dir/WorkspaceTests.cpp.o\r\n> [100%] Linking CXX executable runtests\r\n> /usr/bin/ld: CMakeFiles/runtests.dir/PlaygroundTests.cpp.o: in function `PlaygroundTests_loopThroughArrs_test1_Test::TestBody() [clone ._omp_fn.1]':\r\n> PlaygroundTests.cpp:(.text+0x13cb): undefined reference to `GOMP_loop_nonmonotonic_guided_start'\r\n> /usr/bin/ld: PlaygroundTests.cpp:(.text+0x1496): undefined reference to `GOMP_loop_nonmonotonic_guided_next'\r\n> /usr/bin/ld: CMakeFiles/runtests.dir/PlaygroundTests.cpp.o: in function `PlaygroundTests_loopThroughArrs_test1_Test::TestBody() [clone ._omp_fn.0]':\r\n> \r\n> ...\r\n> \r\n> collect2: error: ld returned 1 exit status\r\n> make[2]: *** [tests_cpu/layers_tests/CMakeFiles/runtests.dir/build.make:1378: tests_cpu/layers_tests/runtests] Error 1\r\n> make[1]: *** [CMakeFiles/Makefile2:687: tests_cpu/layers_tests/CMakeFiles/runtests.dir/all] Error 2\r\n> make: *** [Makefile:152: all] Error 2\r\n> [INFO] ------------------------------------------------------------------------\r\n> [INFO] Reactor Summary:\r\n> [INFO] \r\n> [INFO] deeplearning4j 1.0.0-SNAPSHOT ...................... SUCCESS [  4.426 s]\r\n> [INFO] libnd4j ............................................ FAILURE [  01:39 h]\r\n> [INFO] nd4j ............................................... SKIPPED\r\n> ",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/8158/comments",
    "author": "michaelmalak",
    "comments": [
      {
        "user": "saudet",
        "created_at": "2019-08-28T04:12:05Z",
        "body": "You'll need to build all dependencies with GCC 9.x as well for this to work.\r\n"
      }
    ]
  },
  {
    "number": 6836,
    "title": "import keras model, start application throw .javacpp/...jar/... no such file.",
    "created_at": "2018-12-11T08:46:53Z",
    "closed_at": "2018-12-11T08:49:04Z",
    "labels": [
      "Question",
      "Duplicate"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/6836",
    "body": "hi,\r\n   my project dependency dl4j 1.0.0.beta3,  start application on linux, throw exception info:\r\n   javacpp/cache/openblas-0.3.0-1.4.2-linux-x86_64.jar/org/bytedeco/javacpp/linux-x86_64/libjniopenblas_nolapack.so: libopenblas_nolapack.so.0: cannot open shared object file: No such file or directory\r\ni will try 1.0.0.beat and 1.0.0.beat2, throw like same.\r\nbut mac is well for all dl4j version.\r\n    thanks.",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/6836/comments",
    "author": "levou",
    "comments": [
      {
        "user": "levou",
        "created_at": "2018-12-11T08:50:14Z",
        "body": "just 1.0.0.beta2 and 3 have problem."
      },
      {
        "user": "saudet",
        "created_at": "2018-12-12T03:38:34Z",
        "body": "Duplicate of #6132.\r\n\r\nThat's a known issue with old versions of Spring Boot."
      },
      {
        "user": "levou",
        "created_at": "2018-12-14T09:07:36Z",
        "body": "add softlink: ln -s libjniopenblas_nolapack.so libopenblas_nolapack.so.0 ,then resolve this problem."
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-13T16:47:24Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"
      }
    ]
  },
  {
    "number": 6815,
    "title": "Try to find a way to build DQN",
    "created_at": "2018-12-07T13:32:25Z",
    "closed_at": "2019-03-18T08:44:57Z",
    "labels": [
      "Question",
      "RL4J"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/6815",
    "body": "I want to build a DQN structure in my project use deeplearning4j, but i dont know the  \"minimize\" method in python also cloud implemented in  deeplearning4j? Thank you!",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/6815/comments",
    "author": "chloechj",
    "comments": [
      {
        "user": "saudet",
        "created_at": "2018-12-08T05:03:55Z",
        "body": "You could refer to RL4J's implementation and start from there. Sounds good?\n"
      },
      {
        "user": "agibsonccc",
        "created_at": "2019-03-18T08:44:57Z",
        "body": "Closing due to inactivity."
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-04-17T09:02:41Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 6693,
    "title": "NativeOpExecutioner exception restoring model/ initialising Nd4J",
    "created_at": "2018-11-09T22:48:44Z",
    "closed_at": "2018-11-12T16:47:11Z",
    "labels": [
      "Question",
      "ND4J"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/6693",
    "body": "Hi there,\r\n\r\nI'm on 1.0.0-beta.\r\n\r\nI'm getting the below exception when running:\r\n      val net: ComputationGraph = ModelSerializer.restoreComputationGraph(netFile)\r\n\r\nI trained and saved the model on windows, and the exception occurs when executing above line on a linux box. Running this on windows does not result in this exception, the model loads/ performs fine. \r\n\r\nCPU info of linux box\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                64\r\nOn-line CPU(s) list:   0-63\r\nThread(s) per core:    2\r\nCore(s) per socket:    16\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 79\r\nModel name:            Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz\r\nStepping:              1\r\nCPU MHz:               2101.000\r\nBogoMIPS:              4199.42\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              40960K\r\nNUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62\r\nNUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63\r\n\r\n\r\nAny chance of some pointers for explaining below please ?\r\n\r\njava.lang.ExceptionInInitializerError at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.<init>(NativeOpExecutioner.java:56) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at java.lang.Class.newInstance(Class.java:442) at org.nd4j.linalg.factory.Nd4j.initWithBackend(Nd4j.java:6405) at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:6300) at org.nd4j.linalg.factory.Nd4j.<clinit>(Nd4j.java:210) at org.deeplearning4j.util.ModelSerializer.restoreComputationGraph(ModelSerializer.java:564) at org.deeplearning4j.util.ModelSerializer.restoreComputationGraph(ModelSerializer.java:476) at rice.genesis.ir.workflow.quotation.model.tradeparser.model.RnnModel$.dl4jModelFromDisk(RnnModel.scala:55) at rice.genesis.ir.workflow.quotation.model.tradeparser.model.RnnModel$.$anonfun$getModel$2(RnnModel.scala:34) at scala.Option.flatMap(Option.scala:171) at rice.genesis.ir.workflow.quotation.model.tradeparser.model.RnnModel$.getModel(RnnModel.scala:34) at rice.genesis.ir.workflow.quotation.model.tradeparser.TradeParserModels$.$anonfun$getAll$1(TradeParser.scala:113) at \r\n\r\n\r\nregards\r\nRichard\r\n\r\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/6693/comments",
    "author": "rsantoso",
    "comments": [
      {
        "user": "AlexDBlack",
        "created_at": "2018-11-10T01:12:13Z",
        "body": "Can you share your pom.xml (or dependency management config for other tools)?\r\nPlus we'll need the full output/stack trace here, not just the last few lines of the stack trace."
      },
      {
        "user": "rsantoso",
        "created_at": "2018-11-12T16:47:11Z",
        "body": "Thanks for the hint.... there were missing linux dependencies for nd4j & openblas which has fixed the issue."
      },
      {
        "user": "lock[bot]",
        "created_at": "2018-12-13T03:58:33Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"
      }
    ]
  },
  {
    "number": 5556,
    "title": "need help about running YOLO2",
    "created_at": "2018-06-11T12:24:23Z",
    "closed_at": "2018-06-12T02:52:43Z",
    "labels": [
      "Question",
      "DL4J"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/5556",
    "body": "code ：\r\n```\r\npublic static void main(String[] args) throws Exception {\r\n        // parameters for model;\r\n        int width = 480;\r\n        int height = 300;\r\n        int nChannels = 3;\r\n        int gridWidth = 15;\r\n        int gridHeight = 10;\r\n\r\n        //class\r\n        int nClasses = 5;\r\n\r\n        // parameters for the Yolo2OutputLayer\r\n        int nBoxes = 5;\r\n        double lambdaNoObj = 0.5;\r\n        double lambdaCoord = 1.0;\r\n        double[][] priorBoxes = {{2, 5}, {2.5, 6}, {3, 7}, {3.5, 8}, {4, 9}};\r\n        double detectionThreshold= 0.8;\r\n\r\n        // parameters for the training phase\r\n        int batchSize = 20;\r\n        int nEpochs = 500;\r\n        double learningRate = 1e-4;\r\n        double lrMomentum = 0.9;\r\n\r\n        log.info(\"Load data...\");\r\n\r\n        String dataDir = \"F:\\\\java projects\\\\DLPro\\\\src\\\\main\\\\resources\\\\surface_data\\\\train\";\r\n        File imageDir = new File(dataDir, \"JPEGImages\");\r\n\r\n        int seed = 123;\r\n        Random rng = new Random(seed);\r\n        RandomPathFilter pathFilter = new RandomPathFilter(rng) {\r\n            protected boolean accept(String name) {\r\n                name = name.replace(\"/JPEGImages/\", \"/Annotations/\").replace(\".jpg\", \".xml\");\r\n                try {\r\n                    return new File(new URI(name)).exists();\r\n                } catch (URISyntaxException ex) {\r\n                    throw new RuntimeException(ex);\r\n                }\r\n            }\r\n        };\r\n\r\n        InputSplit[] data = new FileSplit(imageDir, NativeImageLoader.ALLOWED_FORMATS, rng).sample(pathFilter, 0.8, 0.2);\r\n        InputSplit trainData = data[0];\r\n        InputSplit testData = data[1];\r\n\r\n\r\n        ObjectDetectionRecordReader recordReaderTrain = new ObjectDetectionRecordReader(height, width, nChannels,\r\n                gridHeight, gridWidth, new VocLabelProvider(dataDir));\r\n        recordReaderTrain.initialize(trainData);\r\n\r\n\r\n        ObjectDetectionRecordReader recordReaderTest = new ObjectDetectionRecordReader(height, width, nChannels,\r\n                gridHeight, gridWidth, new VocLabelProvider(dataDir));\r\n        recordReaderTest.initialize(testData);\r\n\r\n\r\n        // ObjectDetectionRecordReader performs regression, so we need to specify it here\r\n        RecordReaderDataSetIterator train = new RecordReaderDataSetIterator(recordReaderTrain, batchSize, 1, 1, true);\r\n        train.setPreProcessor(new ImagePreProcessingScaler(0, 1));\r\n\r\n        RecordReaderDataSetIterator test = new RecordReaderDataSetIterator(recordReaderTest, 1, 1, 1, true);\r\n        test.setPreProcessor(new ImagePreProcessingScaler(0, 1));\r\n\r\n        ComputationGraph model;\r\n        String modelFilename = \"model_surface_YOLO2.zip\";\r\n\r\n\r\n        if (new File(modelFilename).exists()) {\r\n            log.info(\"Load model...\");\r\n            model = ModelSerializer.restoreComputationGraph(modelFilename);\r\n        } else {\r\n            log.info(\"Build model...\");\r\n            ComputationGraph graph =  (ComputationGraph) YOLO2.builder().build().initPretrained();\r\n            INDArray priors = Nd4j.create(priorBoxes);\r\n\r\n            FineTuneConfiguration fineTuneConf = new FineTuneConfiguration.Builder()\r\n                    .seed(seed)\r\n                    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\r\n                    .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)\r\n                    .gradientNormalizationThreshold(1.0)\r\n                    .updater(new Adam.Builder().learningRate(1e-3).build())\r\n                    .l2(0.00001)\r\n                    .activation(Activation.IDENTITY)\r\n                    .trainingWorkspaceMode(WorkspaceMode.SEPARATE)\r\n                    .inferenceWorkspaceMode(WorkspaceMode.SEPARATE)\r\n                    .build();\r\n\r\n            model = new TransferLearning.GraphBuilder(graph)\r\n                    .fineTuneConfiguration(fineTuneConf)\r\n                    .addLayer(\"outputs\", new Yolo2OutputLayer.Builder()\r\n                                    .boundingBoxPriors(priors)\r\n                                    .build(),\r\n                            \"conv2d_23\")\r\n                    .setOutputs(\"outputs\")\r\n                    .build();\r\n\r\n           System.out.println(model.summary(InputType.convolutional(height, width, nChannels)));\r\n\r\n            log.info(\"Train model...\");\r\n            model.setListeners(new ScoreIterationListener(1));\r\n            for (int i = 0; i < nEpochs; i++) {\r\n                train.reset();\r\n                while (train.hasNext()) {\r\n                    model.fit(train.next());\r\n                }\r\n                log.info(\"*** Completed epoch {} ***\", i);\r\n            }\r\n            ModelSerializer.writeModel(model, modelFilename, true);\r\n        } }}\r\n```\r\n\r\n\r\nthe exception like this：\r\n```\r\nException in thread \"main\" org.deeplearning4j.nn.conf.inputs.InvalidInputTypeException: Invalid input: MergeVertex cannot merge CNN activations of different width/heights:first [channels,width,height] = [256,15,9], input 1 = [1024,15,10]\r\n\tat org.deeplearning4j.nn.conf.graph.MergeVertex.getOutputType(MergeVertex.java:165)\r\n\tat org.deeplearning4j.nn.graph.ComputationGraph.summary(ComputationGraph.java:3943)\r\n\tat SurfaceObjectDetectionByYOLO2.main(SurfaceObjectDetectionByYOLO2.java:148)\r\n\r\n```\r\nThe Exception occurs in this line：\r\n“System.out.println(model.summary(InputType.convolutional(height, width, nChannels)));”\r\n\r\nHow to fix it or any suggestion？\r\nthanks！\r\n\r\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/5556/comments",
    "author": "muxiangwei",
    "comments": [
      {
        "user": "AlexDBlack",
        "created_at": "2018-06-12T02:52:43Z",
        "body": "Looks like this is simply due to using invalid input sizes.\r\nBasically, your input dimensions need to be a multiple of 32 - because the YOLO2 net does down-sampling by a factor of 32x between the input and the output.\r\nIf I switch the input dimension to [480,320] from [480,300] I can run the model using your code, without issue."
      },
      {
        "user": "lock[bot]",
        "created_at": "2018-09-21T19:59:28Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"
      }
    ]
  },
  {
    "number": 4754,
    "title": "failed to load the googlenews mode in deeplearning4j",
    "created_at": "2018-03-03T21:12:24Z",
    "closed_at": "2018-03-04T09:10:56Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/4754",
    "body": " public static void main(String[] args) throws Exception {\r\n\r\n        File gModel = new File(\"D:/tutorials/mechine learning/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin\");\r\n        Word2Vec word2Vec1=WordVectorSerializer.loadGoogleModel(gModel,true);\r\n        //String filePath1 = new ClassPathResource(\"googlenews.txt\").getFile().getAbsolutePath();\r\n        //Word2Vec word2Vec = WordVectorSerializer.readWord2VecModel(filePath1);\r\n\r\n        Collection<String> lst = word2Vec1.wordsNearestSum(\"day\", 10);\r\n        System.out.println(lst);\r\n        \r\n\r\n    }\r\n\r\n\r\ni am facing the issue of \r\n----------------------------------------------------------------------------------------------------\r\nException in thread \"main\" java.lang.OutOfMemoryError: Cannot allocate new FloatPointer(900000000): totalBytes = 1, physicalBytes = 144M\r\n\tat org.bytedeco.javacpp.FloatPointer.<init>(FloatPointer.java:76)\r\n\tat org.nd4j.linalg.api.buffer.BaseDataBuffer.<init>(BaseDataBuffer.java:541)\r\n\tat org.nd4j.linalg.api.buffer.FloatBuffer.<init>(FloatBuffer.java:61)\r\n\tat org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.createFloat(DefaultDataBufferFactory.java:255)\r\n\tat org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1468)\r\n\tat org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1442)\r\n\tat org.nd4j.linalg.api.ndarray.BaseNDArray.<init>(BaseNDArray.java:247)\r\n\tat org.nd4j.linalg.cpu.nativecpu.NDArray.<init>(NDArray.java:109)\r\n\tat org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.create(CpuNDArrayFactory.java:262)\r\n\tat org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:5014)\r\n\tat org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4965)\r\n\tat org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4093)\r\n\tat org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readBinaryModel(WordVectorSerializer.java:247)\r\n\tat org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.loadGoogleModel(WordVectorSerializer.java:135)\r\n\tat org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.loadGoogleModel(WordVectorSerializer.java:113)\r\n\tat Word2VecRawTextExample.main(Word2VecRawTextExample.java:35)\r\nCaused by: java.lang.OutOfMemoryError: Failed to allocate memory within limits: totalBytes = 1 + 3G > maxBytes = 1G\r\n\tat org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:572)\r\n\tat org.bytedeco.javacpp.Pointer.init(Pointer.java:121)\r\n\tat org.bytedeco.javacpp.FloatPointer.allocateArray(Native Method)\r\n\tat org.bytedeco.javacpp.FloatPointer.<init>(FloatPointer.java:68)\r\n\t... 15 more\r\n------------------------------------------------------------------------------------------------------------------",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/4754/comments",
    "author": "jituumakanta",
    "comments": [
      {
        "user": "saudet",
        "created_at": "2018-03-04T09:09:57Z",
        "body": "Increase memory available to Java either with the -Xmx command line option\nor any other means.\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2018-09-23T05:28:08Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"
      }
    ]
  },
  {
    "number": 2104,
    "title": "Poor GPU performance relative to CPU",
    "created_at": "2016-09-14T22:35:58Z",
    "closed_at": "2016-10-03T21:00:34Z",
    "labels": [
      "Bug",
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/2104",
    "body": "This issue is similar to #2053, with a few key differences:\n- I'm using a recurrent neural network.\n- Performance is consistently and significantly _worse_ with the GPU backend (not merely comparable).\n- I have a small dataset (16x9x308), which is probably at least partly to blame. However, the nsight profiling results with a larger dataset may suggest that DL4J can close the gap somewhat (see below).\n\nI'm using the latest release of DeepLearning4J (0.5.0).\n\n**Minimized Test Case**\n\n``` java\npackage com.stottlerhenke.illuminate;\n\nimport java.util.List;\n\nimport com.stottlerhenke.illuminate.training.PerformanceListener;\n\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm;\nimport org.deeplearning4j.nn.conf.*;\nimport org.deeplearning4j.nn.conf.layers.GravesLSTM;\nimport org.deeplearning4j.nn.conf.layers.RnnOutputLayer;\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\nimport org.deeplearning4j.nn.weights.WeightInit;\nimport org.nd4j.linalg.api.ndarray.INDArray;\nimport org.nd4j.linalg.dataset.DataSet;\nimport org.nd4j.linalg.dataset.api.DataSetPreProcessor;\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\nimport org.nd4j.linalg.factory.Nd4j;\nimport org.nd4j.linalg.lossfunctions.LossFunctions;\n\npublic class GpuProblem {\n\n    public static int timeSeriesCount = 16;\n    public static int inputCount = 9;\n    public static int timeSteps = 308;\n    public static int numHiddenNodes = 20;\n    public static int truncatedBPTTLength = 100;\n    public static int epochCount = 3;\n\n    public static class MyDatasetIterator implements DataSetIterator {\n\n        private DataSetPreProcessor preProcessor;\n\n        int cursor = 0;\n\n        @Override\n        public boolean hasNext() {\n            return cursor < epochCount;\n        }\n\n        @Override\n        public DataSet next() {\n            return next(1);\n        }\n\n        @Override\n        public DataSet next(int num) {\n\n            DataSet ds = createDataset();\n            if (preProcessor != null)\n                preProcessor.preProcess(ds);\n            cursor += num;\n            System.out.println(\"NEW CURSOR \" + cursor);\n            return ds;\n        }\n\n        private DataSet createDataset() {\n            INDArray createDataSetinput = Nd4j.zeros(timeSeriesCount, inputCount, timeSteps);\n            INDArray createDataSetlabels = Nd4j.zeros(timeSeriesCount, 2, timeSteps);\n            return new DataSet(createDataSetinput, createDataSetlabels);\n        }\n\n        @Override\n        public int totalExamples() {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public int inputColumns() {\n            return inputCount;\n        }\n\n        @Override\n        public int totalOutcomes() {\n            return 2;\n        }\n\n        @Override\n        public boolean resetSupported() {\n            return true;\n        }\n\n        @Override\n        public void reset() {\n            cursor = 0;\n            System.out.println(\"RESET NEW CURSOR \" + cursor);\n        }\n\n        @Override\n        public int batch() {\n            return timeSeriesCount;\n        }\n\n        @Override\n        public int cursor() {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public int numExamples() {\n            throw new UnsupportedOperationException();\n        }\n\n        @Override\n        public void setPreProcessor(DataSetPreProcessor preProcessor) {\n            this.preProcessor = preProcessor;\n\n        }\n\n        @Override\n        public DataSetPreProcessor getPreProcessor() {\n            return this.preProcessor;\n        }\n\n        @Override\n        public List<String> getLabels() {\n            return null;\n        }\n\n    }\n\n    public static void main(String[] args) {\n        MultiLayerConfiguration.Builder builder =\n                new NeuralNetConfiguration.Builder()\n                        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)\n                        .updater(Updater.ADAM).adamMeanDecay(0.9).adamVarDecay(0.999)\n                        .regularization(true).l1(1e-4).dropOut(0.5)\n                        .weightInit(WeightInit.XAVIER)\n                        .learningRate(5e-4)\n                        .list()\n                        .layer(0, new GravesLSTM.Builder().nIn(inputCount).nOut(numHiddenNodes)\n                                .activation(\"tanh\").build())\n                        .layer(1, new RnnOutputLayer.Builder().nIn(numHiddenNodes)\n                                .activation(\"softmax\")\n                                .lossFunction(LossFunctions.LossFunction.MCXENT)\n                                .nIn(numHiddenNodes).nOut(2).build())\n                        .pretrain(false)\n                        .backprop(true)\n                        .backpropType(BackpropType.TruncatedBPTT)\n                   .tBPTTBackwardLength(truncatedBPTTLength).tBPTTForwardLength(truncatedBPTTLength);\n\n        MultiLayerNetwork net = new MultiLayerNetwork(builder.build());\n        net.init();\n\n        net.setListeners(new PerformanceListener(1));\n\n        net.fit(new MyDatasetIterator());\n    }\n}\n```\n\n**Performance Results**\nCPU\n\n```\niteration 1; iteration time: 341 ms; samples/sec: 46.921; batches/sec: 2.933;\niteration 2; iteration time: 170 ms; samples/sec: 94.118; batches/sec: 5.882;\niteration 3; iteration time: 158 ms; samples/sec: 101.266; batches/sec: 6.329;\niteration 4; iteration time: 181 ms; samples/sec: 88.398; batches/sec: 5.525;\niteration 5; iteration time: 127 ms; samples/sec: 125.984; batches/sec: 7.874;\niteration 6; iteration time: 122 ms; samples/sec: 131.148; batches/sec: 8.197;\niteration 7; iteration time: 122 ms; samples/sec: 131.148; batches/sec: 8.197;\niteration 8; iteration time: 119 ms; samples/sec: 134.454; batches/sec: 8.403;\niteration 9; iteration time: 119 ms; samples/sec: 134.454; batches/sec: 8.403;\n```\n\nGPU\n\n```\niteration 1; iteration time: 1667 ms; samples/sec: 9.598; batches/sec: 0.600;\niteration 2; iteration time: 1266 ms; samples/sec: 12.638; batches/sec: 0.790;\niteration 3; iteration time: 1241 ms; samples/sec: 12.893; batches/sec: 0.806;\niteration 4; iteration time: 1192 ms; samples/sec: 13.423; batches/sec: 0.839;\niteration 5; iteration time: 1204 ms; samples/sec: 13.289; batches/sec: 0.831;\niteration 6; iteration time: 1178 ms; samples/sec: 13.582; batches/sec: 0.849;\niteration 7; iteration time: 1137 ms; samples/sec: 14.072; batches/sec: 0.880;\niteration 8; iteration time: 1141 ms; samples/sec: 14.023; batches/sec: 0.876;\niteration 9; iteration time: 1183 ms; samples/sec: 13.525; batches/sec: 0.845;\n```\n\nI also tested with more data by increasing the number of time series from 16 to 16,000. With this change, the GPU does outperform the CPU (as expected); however, profiling with nsight shows only 9.1% GPU utilization (although I was only able to profile one iteration due to an issue with nsight that caused the test application to terminate early with an access violation, and the utilization percentage is probably unduly influenced by the time period before network training begins).\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/2104/comments",
    "author": "anovstrup",
    "comments": [
      {
        "user": "anovstrup",
        "created_at": "2016-09-14T22:50:15Z",
        "body": "Performance listener output for 16k time series count:\nCPU\n\n```\niteration 1; iteration time: 7332 ms; samples/sec: 2182.215; batches/sec: 0.136;\niteration 2; iteration time: 7488 ms; samples/sec: 2136.752; batches/sec: 0.134;\niteration 3; iteration time: 7410 ms; samples/sec: 2159.244; batches/sec: 0.135;\niteration 4; iteration time: 7426 ms; samples/sec: 2154.592; batches/sec: 0.135;\niteration 5; iteration time: 7472 ms; samples/sec: 2141.328; batches/sec: 0.134;\niteration 6; iteration time: 9813 ms; samples/sec: 1630.490; batches/sec: 0.102;\n```\n\nGPU\n\n```\niteration 1; iteration time: 7090 ms; samples/sec: 2256.699; batches/sec: 0.141;\niteration 2; iteration time: 4913 ms; samples/sec: 3256.666; batches/sec: 0.204;\niteration 3; iteration time: 4931 ms; samples/sec: 3244.778; batches/sec: 0.203;\niteration 4; iteration time: 5129 ms; samples/sec: 3119.516; batches/sec: 0.195;\niteration 5; iteration time: 4910 ms; samples/sec: 3258.656; batches/sec: 0.204;\niteration 6; iteration time: 4995 ms; samples/sec: 3203.203; batches/sec: 0.200;\n```\n"
      },
      {
        "user": "AlexDBlack",
        "created_at": "2016-09-15T00:09:43Z",
        "body": "So you have 2 layers, one of size 20, and one of size 2... with minibatch size 16... of course it's going to be slow on GPU.\nGPUs need a sufficient amount of parallelism for good performance; your network and data is way too small to get anywhere close to full efficiency.\n"
      },
      {
        "user": "anovstrup",
        "created_at": "2016-09-15T00:19:31Z",
        "body": "Important hardware specs: I ran these tests on a machine with 2 6-core Xeon E5-2620V2 processors. The GPU is a GeForce GLX 980 Ti.\n"
      },
      {
        "user": "raver119",
        "created_at": "2016-10-03T21:00:34Z",
        "body": "So, looks like @AlexDBlack is totally right here, and increasing performance with higher amounts of input data confirms that.\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-20T17:57:08Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 2103,
    "title": "VPTree search differs from exhaustive search",
    "created_at": "2016-09-14T17:01:59Z",
    "closed_at": "2017-07-01T23:38:40Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/2103",
    "body": "I'm getting strange results from the VPTree search, so I wrote a comparison to exhaustive search.\n\n``` java\npackage com.ccri;\n\nimport org.deeplearning4j.clustering.vptree.VPTree;\nimport org.nd4j.linalg.cpu.nativecpu.NDArray;\nimport org.deeplearning4j.clustering.sptree.DataPoint;\n\nimport java.util.*;\n\n/**\n * Hello VPTree!\n *\n */\npublic class App \n{\n    // make some random points\n    private static NDArray generate_data( int m, int n )\n    {\n        int[] shape = new int[]{m, n};\n        NDArray arr = new NDArray( shape );\n        Random rng = new Random();\n        for (int i = 0; i < m; ++i)\n        {\n            //System.out.print( \"point = \");\n            for (int j = 0; j < n; ++j)\n            {\n                double xij = rng.nextGaussian();\n                arr.putScalar(i, j, xij);\n                //System.out.print( xij  * 200 + 500 +\" \");\n            }\n            //System.out.print(\"\\n\");\n        }\n        return arr;\n    }\n\n    private static class dist_index implements Comparable<dist_index> {\n        public double dist;\n        public int index;\n        public int compareTo( dist_index r ){ return Double.compare( dist,  r.dist ); }\n    }\n\n    private static void check_knn( NDArray arr, VPTree t, int k )\n    {\n        int m = arr.rows();\n        for ( int target_index = 0;  target_index< m; ++target_index)\n        {\n            // Do an exhaustive search\n            TreeSet<Integer> s = new TreeSet<>();\n            PriorityQueue<dist_index> pq = new PriorityQueue<>();\n            for ( int j = 0;  j < m; ++j)\n            {\n                double d = arr.getRow(target_index).distance2(arr.getRow(j));\n                dist_index di = new dist_index();\n                di.dist = d;\n                di.index = j;\n                pq.add( di );\n            }\n\n            // keep closest k\n            for ( int i = 0; i < k; ++ i )\n            {\n                dist_index di = pq.poll();\n                System.out.println( \"exsaustive d=\" + di.dist);\n                s.add( di.index );\n            }\n\n            // Check what VPTree gives for results\n            ArrayList<DataPoint> results = new ArrayList<>();\n            ArrayList<Double> distances = new ArrayList<>();\n            DataPoint p = new DataPoint( target_index, arr.getRow(target_index));\n            t.search( p, k, results, distances );\n            //List<DataPoint> items = t.getItems();\n            TreeSet<Integer> result_set = new TreeSet<>();\n\n            // keep k in a set\n            for ( int i = 0; i < k; ++ i ) {\n                DataPoint result = results.get(i);\n                int r = result.getIndex();\n                result_set.add(r);\n            }\n            // check\n            for ( int r : result_set)\n            {\n\n                //int r = items.get(result.getIndex()).getIndex();\n                if ( s.contains(r)) {\n                    System.out.println(\"yippy\");\n                }\n                else {\n                    System.out.println( \"VPTree result \" + r +\n                            \" is not in the closest \" + k +\n                            \" from the exhaustive search.\"\n                    );\n                }\n           }\n           for ( int r : s )\n           {\n               if ( result_set.contains(r))\n               {\n                   System.out.println(\"yippy\");\n               }\n               else\n               {\n                   System.out.println( \"The exhaustive search for the closest \" + k +\n                           \" includes \" + r +\n                           \" which is not in theVPTree results.\"\n                   );\n               }\n           }\n        }\n    }\n\n    public static void main( String[] args )\n    {\n        int m = Integer.parseInt( args[0] );\n        int n = 2;\n\n        // Generate data\n        NDArray arr = generate_data( m, n );\n\n        // Make VPTree\n        VPTree t = new VPTree( arr, \"euclidean\", false );\n\n        // Check kNN\n        check_knn( arr, t, Integer.parseInt(args[1]) );\n\n    }\n}\n```\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/2103/comments",
    "author": "khobbs-ccri",
    "comments": [
      {
        "user": "agibsonccc",
        "created_at": "2017-07-01T23:39:39Z",
        "body": "For any future readers, I added this as a test and didn't actually find any issues here. VPTree, kmeans and co are now their own modules. We will be supporting these modules a bit more now."
      },
      {
        "user": "lock[bot]",
        "created_at": "2018-09-26T12:57:35Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"
      }
    ]
  },
  {
    "number": 2044,
    "title": "ScoreIterationListener is under-reporting when using truncated BPTT",
    "created_at": "2016-08-29T17:28:26Z",
    "closed_at": "2018-04-26T02:58:18Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/2044",
    "body": "While training an RNN on time series data with 450 time steps and a truncation length of 100, `ScoreIterationListener` is reporting the score 4 times per epoch. I expected 5 parameter updates and therefore 5 score reports per epoch. \n\nMy main worry is that the under-reporting could indicate that no parameter update is being performed for the last 50 time steps of my data.\n\n**Dataset Details**\nI have a training dataset with 8 multivariate time series consisting of 450 time steps, 65 input features, and 2 one-hot output labels (i.e., the feature tensor's shape is 8x65x450 and the label tensor's shape is 8x2x450). I am treating the full dataset as a single minibatch.\n\n**Network Configuration**\n\n``` scala\n    val truncatedBackPropLength = 100\n    val numInputs = 65\n    val numHiddenNodes = 20\n    val conf = new Builder()\n      .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)\n      .updater(Updater.ADAM).adamMeanDecay(0.9).adamVarDecay(0.999)\n      .regularization(true).l1(1e-4).dropOut(0.5)\n      .weightInit(WeightInit.XAVIER)\n      .learningRate(1e-4)\n      .list()\n      .layer(0, new GravesLSTM.Builder().nIn(numInputs).nOut(numHiddenNodes)\n        .activation(\"tanh\").build())\n      .layer(1, new RnnOutputLayer.Builder().nIn(numHiddenNodes)\n        .activation(\"softmax\")\n        .lossFunction(LossFunctions.LossFunction.MCXENT)\n        .nIn(numHiddenNodes).nOut(2).build())\n      .pretrain(false)\n      .backprop(true)\n      .backpropType(BackpropType.TruncatedBPTT)\n      .tBPTTBackwardLength(truncatedBackPropLength).tBPTTForwardLength(truncatedBackPropLength)\n      .build()\n```\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/2044/comments",
    "author": "anovstrup",
    "comments": [
      {
        "user": "raver119",
        "created_at": "2016-08-29T17:46:57Z",
        "body": "How exactly ScoreIterationListener configured? Show constructor line please\n"
      },
      {
        "user": "anovstrup",
        "created_at": "2016-08-29T17:48:18Z",
        "body": "``` scala\nval net = new MultiLayerNetwork(conf)\nnet.init()\nnet.setListeners(new ScoreIterationListener(1))\n```\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2018-09-22T16:13:54Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n"
      }
    ]
  },
  {
    "number": 1775,
    "title": "Getting continuous output from a network",
    "created_at": "2016-07-02T15:26:55Z",
    "closed_at": "2016-07-12T00:20:50Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/1775",
    "body": "I am having some trouble in getting a single continuous variable out of a network. I am using the following code:\n\n```\n    public NeuralNetModel generate(List<LabeledPoint> labeledPoints) {\n        if (labeledPoints.isEmpty()) {\n            throw new IllegalArgumentException(\"Cannot pass no points into generator\");\n        }\n        LabeledPoint point = labeledPoints.get(0);\n        int width = point.features().size();\n\n        JavaRDD<LabeledPoint> data = localSparkContextContainer.getJavaSparkContext().parallelize(labeledPoints);\n\n        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n                .seed(12345)\n                .iterations(5)\n                .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT)\n                .learningRate(1e-1)\n                .weightInit(WeightInit.XAVIER)\n                .updater(Updater.NESTEROVS).momentum(0.9)\n                .list()\n                .layer(0, new DenseLayer.Builder().nIn(width).nOut(width / 2)\n                        .activation(\"tanh\")\n                        .build()\n                )\n                .layer(1, new DenseLayer.Builder().nIn(width / 2).nOut(width / 4)\n                        .activation(\"tanh\")\n                        .build()\n                )\n                .layer(2, new OutputLayer.Builder(LossFunction.MSE)\n                        .activation(\"identity\")\n                        .nIn(width / 4).nOut(1).build()\n                )\n                .backprop(true).pretrain(false)\n                .build();\n\n        MultiLayerNetwork net = new MultiLayerNetwork(conf);\n        net.init();\n\n        //Create Spark network\n        SparkDl4jMultiLayer sparkNetwork = new SparkDl4jMultiLayer(localSparkContextContainer.getJavaSparkContext(), net);\n\n        sparkNetwork.fit(localSparkContextContainer.getJavaSparkContext(), data);\n\n        return new NeuralNetModel(sparkNetwork);\n    }\n```\n\nThe labelled points contain several features and a label which are all continuous variables which may be negative.\n\nI am trying to train the network to take these features and produce a prediction which is a continuous double variable which may be negative.\n\nI am finding that I am being hit by:\n\n```\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -1\n    at org.nd4j.linalg.util.FeatureUtil.toOutcomeVector(FeatureUtil.java:38)\n    at org.deeplearning4j.spark.util.MLLibUtil.fromLabeledPoint(MLLibUtil.java:311)\n    at org.deeplearning4j.spark.util.MLLibUtil.fromLabeledPoint(MLLibUtil.java:297)\n    at org.deeplearning4j.spark.util.MLLibUtil.fromLabeledPoint(MLLibUtil.java:242)\n    at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:261)\n```\n\nbecause the call to `sparkNetwork.fit(localSparkContextContainer.getJavaSparkContext(), data);` takes the labelled point and the first thing it tries to do is make a create an array where the value I specified as nOut in the output layer is the number of possible values that the output can take. which is just not what I want because the output is continuous.\n\nI am assuming that I have fundamentally misunderstood something about neural networks or how to configure them in this framework and a bit of help would be much appreciated.\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/1775/comments",
    "author": "doo-gl",
    "comments": [
      {
        "user": "agibsonccc",
        "created_at": "2016-07-12T00:20:50Z",
        "body": "Please come in to gitter if you have questions. Or link us in gitter to a stack overflow. That's more appropriate. Thanks!\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-20T22:53:20Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 1587,
    "title": "lstm example ",
    "created_at": "2016-05-23T12:03:51Z",
    "closed_at": "2016-05-25T10:30:54Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/1587",
    "body": "I have data set which is given below\n3 3 373 27 9 615 9 16 10 34 0 8 0 199 65917 1319 122 402 319 183\n3 3 77 12 4 66 4 3 0 5 0 14 3 50 106 139 38 164 53 109\n3 3 86 6 2 6 2 0 0 1 0 25 0 4 284 77888 19 66 11 25\n3 3 469 21 7 291 7 43 15 82 0 207 0 181 115646 59073 294 928 112 675\n3 3 2090 21 7 4035 7 17 8 40 0 317 10 717 1033 25661 142 2054 1795 1023\n3 3 691 18 6 597 6 30 16 61 0 245 18 273 719 2352305 213 1106 324 719\n6 6 229 0 8 526 0 11 1 13 0 6 5 101 7246 2082 120 141 288 1570\n3 3 1158 9 3 649 3 16 6 17 1 247 38 477 592 987626 82 1305 653 707\n4 4 211 0 10 429 0 16 9 20 0 3 0 106 42725 27302 4280 133 477 1567\n\nThe first column is the target which has 9 classes and around 1803 features. I am trying to apply lstm for prediction. I would like to implement lstm for my dataset to test with different network topologies as one memory block with one cell, two memory block with two cell,three memory block with three cell and four memory block with four cell. could you please provides the example for the above dataset.\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/1587/comments",
    "author": "vinayakumarr",
    "comments": [
      {
        "user": "AlexDBlack",
        "created_at": "2016-05-25T10:30:54Z",
        "body": "DL4J's LSTM implementation is exactly the same as every other LSTM implementation out there - i.e., there is no independent control over \"memory block\" and \"cell\" counts (fwiw nobody really uses that terminology either). To control the number of LSTM units, it's just .nOut(...) in the configuration.\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T01:53:28Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 1497,
    "title": "IllegalStateException when installing custom loss function",
    "created_at": "2016-05-03T15:56:13Z",
    "closed_at": "2016-06-03T19:58:55Z",
    "labels": [
      "Question",
      "Documentation"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/1497",
    "body": "Hello, I am trying to install a custom loss function in DL4J v0.4-rc3.8 on the GravesLSTMCharModellingExample. I used:\n\n```\npublic class CustomLoss extends BaseLossFunction {\n        @Override\n        public String name() {\n            return \"CustomLoss\";\n        }\n… (all function throw UnsupportedOperationException at this time)\n}\n```\n\n```\nString customLossFunction= \"CustomLoss\";\nOptimizationAlgorithm optimizationAlgo = OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT;\n...\n.layer(2,\nnew RnnOutputLayer.Builder(LossFunctions.LossFunction.CUSTOM).customLossFunction(customLossFunction).activation(\"softmax\")\n```\n\nI expected an exception in the loss function implementation. Instead, I get:\n\n> Exception in thread \"main\" java.lang.IllegalStateException: Invalid loss function: CUSTOM\n>     at org.deeplearning4j.nn.layers.BaseOutputLayer.getGradientsAndDelta(BaseOutputLayer.java:210)\n>     at org.deeplearning4j.nn.layers.BaseOutputLayer.backpropGradient(BaseOutputLayer.java:133)\n>     at org.deeplearning4j.nn.layers.recurrent.RnnOutputLayer.backpropGradient(RnnOutputLayer.java:74)\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/1497/comments",
    "author": "fac2003",
    "comments": [
      {
        "user": "fac2003",
        "created_at": "2016-05-03T16:05:07Z",
        "body": "(I tried testing this against 0.4-rc3.9-SNAPSHOT but got a different issue.)\n\n> Exception in thread \"main\" java.lang.AbstractMethodError: org.nd4j.linalg.cpu.CpuNDArrayFactory.toFlattened(CLjava/util/Collection;)Lorg/nd4j/linalg/api/ndarray/INDArray;\n>   at org.nd4j.linalg.factory.Nd4j.toFlattened(Nd4j.java:1577)\n>   at org.deeplearning4j.nn.layers.BaseLayer.params(BaseLayer.java:260)\n>   at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.params(MultiLayerNetwork.java:846)\n>   at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.pack(MultiLayerNetwork.java:905)\n>   at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.initMask(MultiLayerNetwork.java:1769)\n>   at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.init(MultiLayerNetwork.java:364)\n"
      },
      {
        "user": "treo",
        "created_at": "2016-05-03T16:16:40Z",
        "body": "At the moment you can't use rc3.9 from the snapshot repository, you have to build it from source yourself.\n"
      },
      {
        "user": "fac2003",
        "created_at": "2016-05-03T22:39:12Z",
        "body": "I have not found instructions for building from master on GitHub and it looks like I am missing some dependencies so I will wait for some more stable snapshot. Looks like the issue was addressed from what I could tell with the source. Since the code was reorganized, is there any reason why you would not use polymorphism on LossFunctions and still need to rely on enums and a special custom case where you use a class?\n"
      },
      {
        "user": "eraly",
        "created_at": "2016-06-03T19:58:55Z",
        "body": "Closing\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T01:52:58Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 1487,
    "title": "Random range in Word2Vec's skipGram method",
    "created_at": "2016-05-02T09:44:59Z",
    "closed_at": "2017-02-17T05:44:33Z",
    "labels": [
      "Question",
      "Documentation"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/1487",
    "body": "Hi,\nI would like better understand the skip-gram model implementation in this project. I do not understand the meaning of the b value, which seems to change randomly the dimension of the context to analyze (i.e. the \"end\" variable in class deeplearning4j/deeplearning4j-scaleout/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/word2vec/Word2Vec.java).\nCan someone explain why it is needed to loop on a random range rather than on a range having always dimension  n = window \\* 2 + 1 ?\n\nI report in the following the code that I'm referring to:\n\n`\nprivate void skipGram(int i, List<T> sentence, int b, AtomicLong nextRandom, double alpha) {\n\n```\n    ...\n\n   int end = window * 2 + 1 - b;\n\n    for(int a = b; a < end; a++) {\n\n        ...\n\n    }\n}\n```\n\n` \n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/1487/comments",
    "author": "fabiana001",
    "comments": [
      {
        "user": "lock[bot]",
        "created_at": "2019-01-19T14:07:12Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 1377,
    "title": "I use the DBN train the data but i always get the  same result",
    "created_at": "2016-04-07T14:54:06Z",
    "closed_at": "2016-04-19T23:35:23Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/1377",
    "body": "`public class DBNEntity {\n\n```\nprivate static Logger log = LoggerFactory.getLogger(DBNEntity.class);\n\npublic static void main(String[] args) throws Exception {\n    String filePath = new ClassPathResource(\"testresult.txt\").getFile().getAbsolutePath();\n    RecordReader recordReader = new CSVRecordReader(0, \" \");\n    recordReader.initialize(new FileSplit(new File(filePath)));\n    //reader,label index,number of possible labels\n    org.nd4j.linalg.dataset.api.iterator.DataSetIterator iter = new RecordReaderDataSetIterator(recordReader, 400, 2);\n\n    // Customizing params\n    Nd4j.MAX_SLICES_TO_PRINT = 10;\n    Nd4j.MAX_ELEMENTS_PER_SLICE = 10;\n\n    final int numInputs = 400;\n    int outputNum = 2;\n    int iterations = 5;\n    int seed = 12;\n    int listenerFreq = iterations;\n\n    final int numRows = 400;//\n    final int numColumns = 2;\n\n    int numSamples = 60000;\n    int batchSize = 100;\n\n\n    log.info(\"Load data....\");\n    //DataSetIterator iter = new MnistDataSetIterator(batchSize, numSamples, true);\n    DataSet next = iter.next();\n    next.normalizeZeroMeanZeroUnitVariance();\n\n\n    log.info(\"Build model....\");\n    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n            .seed(seed)\n            .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)\n            .gradientNormalizationThreshold(1.0)\n            .iterations(iterations)\n            .momentum(0.5)\n            .momentumAfter(Collections.singletonMap(3, 0.9))\n            .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)\n            .list(5)\n            .layer(0, new RBM.Builder().nIn(400).nOut(3)\n                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunction.RMSE_XENT)\n                    .visibleUnit(RBM.VisibleUnit.BINARY)\n                    .hiddenUnit(RBM.HiddenUnit.BINARY)\n                    .build())\n            .layer(1, new RBM.Builder().nIn(3).nOut(2)\n                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunction.RMSE_XENT)\n                    .visibleUnit(RBM.VisibleUnit.BINARY)\n                    .hiddenUnit(RBM.HiddenUnit.BINARY)\n                    .build())\n            .layer(2, new RBM.Builder().nIn(2).nOut(2)\n                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunction.RMSE_XENT)\n                    .visibleUnit(RBM.VisibleUnit.BINARY)\n                    .hiddenUnit(RBM.HiddenUnit.BINARY)\n                    .build())\n            .layer(3, new RBM.Builder().nIn(2).nOut(2)\n                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunction.RMSE_XENT)\n                    .visibleUnit(RBM.VisibleUnit.BINARY)\n                    .hiddenUnit(RBM.HiddenUnit.BINARY)\n                    .build())\n            .layer(4, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD).activation(\"softmax\")\n                    .nIn(2).nOut(outputNum).build())\n            .pretrain(true).backprop(false)\n            .build();\n  //run the model\n    MultiLayerNetwork model = new MultiLayerNetwork(conf);\n    model.init();\n    model.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));\n   //model.setListeners(new HistogramIterationListener(1));\n\n    next.normalizeZeroMeanZeroUnitVariance();\n    next.shuffle();\n    //split test and train\n    SplitTestAndTrain testAndTrain = next.splitTestAndTrain(0.7);\n    model.fit(testAndTrain.getTrain());\n    //evaluate the model\n    Evaluation eval = new Evaluation(5);\n    DataSet test = testAndTrain.getTest();\n    INDArray output = model.output(test.getFeatureMatrix());\n    eval.eval(test.getLabels(), output);\n\n    log.info(eval.stats());\n    //save the model\n    //DefaultModelSaver saveModel1 = new DefaultModelSaver();\n    //saveModel1.save(model);\n  //Write the network parameter`\n```\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/1377/comments",
    "author": "abner2015",
    "comments": [
      {
        "user": "nyghtowl",
        "created_at": "2016-04-19T23:35:23Z",
        "body": "@abner2015 you are using a seed so the same result is expected\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T06:53:23Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 1274,
    "title": "Scores depending on multiple consecutive samples in time-based data",
    "created_at": "2016-03-14T13:18:42Z",
    "closed_at": "2016-05-01T12:24:27Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/1274",
    "body": "A quite common situation in a neural network that feeds into behaviour (eg motor actuation in a robot) is that a cost is a function of different derivatives of a target. The simplest case is where there is a single output p_t depending on timestep t and the cost is some error function:\nE(p_t, p_t-1)\nrather than just E(p_t)\nIn general p may be a vector of outputs and also E may depend on k consecutive time steps (which can be viewed as being dependence on derivatives up to the (k-1)th).  In the simplest case, the gradient of E with respect to p_t has two components coming from adjacent time steps.\n\nIn training, what would do the job is to take snippets of the time series of length L (L could be as small as k, but sometimes might be more efficient to be larger) and to do a form of minibatch gradient descent using L-k-1 examples from each snippet (those where E can be calculated).\n\nHope I have expressed that clearly enough. If not, please tell me!\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/1274/comments",
    "author": "Elroch",
    "comments": [
      {
        "user": "agibsonccc",
        "created_at": "2016-05-01T12:24:27Z",
        "body": "Come in to gitter for another explanation or ask for clarifying docs. Thanks!\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T05:52:44Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 741,
    "title": "Loading takes unusually long time for a vector file trained with small size vocabulary",
    "created_at": "2015-09-25T19:14:39Z",
    "closed_at": "2016-05-01T12:28:55Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/741",
    "body": "I sent @agibsonccc the sample vector file I tried to test with. The program that loads the file and returns a list of neighbors takes forever, which makes it all suspicious that there is something wrong with it inside. \n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/741/comments",
    "author": "mhjang",
    "comments": [
      {
        "user": "agibsonccc",
        "created_at": "2015-09-26T19:06:09Z",
        "body": "Will wait for the file on this one. Once you get it to me I'll look closer at it.\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T04:53:31Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 635,
    "title": "Word2Vec: vec.wordsNearest just returns \"</s>\" not any other words",
    "created_at": "2015-08-27T14:06:43Z",
    "closed_at": "2016-05-06T07:59:14Z",
    "labels": [
      "Bug",
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/635",
    "body": "Not sure whether it is a bug or i am missing something.\nFile model = new File(\"/data/../w2v_models/GoogleNews-vectors-negative300.bin.gz\");\nWord2Vec vec = WordVectorSerializer.loadGoogleModel(model, true);\nCollection<String> wordNearest = vec.wordsNearest(\"day\", 20);\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/635/comments",
    "author": "firojalam",
    "comments": [
      {
        "user": "agibsonccc",
        "created_at": "2016-05-06T07:59:14Z",
        "body": "Guessing not a problem anymore.\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T03:53:06Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  },
  {
    "number": 297,
    "title": "Serializing Paragraph Vectors (Doc2Vec)",
    "created_at": "2015-05-28T01:30:03Z",
    "closed_at": "2016-04-22T16:10:42Z",
    "labels": [
      "Question"
    ],
    "url": "https://github.com/deeplearning4j/deeplearning4j/issues/297",
    "body": "I am trying to generate paragraph embedding for my dataset using ParagraphVectors class.\nWorking with Word2Vec there's a WordVectorSerializer making possible to store the generated vectors in a file using the WordVectorSerializer.writeWordVectors method.\nWhat's the equivalent for ParagraphVectors ?\n",
    "comments_url": "https://api.github.com/repos/deeplearning4j/deeplearning4j/issues/297/comments",
    "author": "ml-tn",
    "comments": [
      {
        "user": "agibsonccc",
        "created_at": "2015-06-02T21:24:59Z",
        "body": "The way paragraphvectors works is the labels are just appended to the end of the lookup table for words. You can just use WordVectorSerializer all the same.\n"
      },
      {
        "user": "lock[bot]",
        "created_at": "2019-01-21T05:53:22Z",
        "body": "This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs."
      }
    ]
  }
]