[
  {
    "number": 621,
    "title": "error:python3 quantize.py 7B ",
    "created_at": "2023-03-30T09:44:35Z",
    "closed_at": "2023-03-30T23:22:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/issues/621",
    "body": "When I tried the llama model and run :**python3 quantize.py 7B**    for operation, ```\r\nthe \"quantize\" script was not found in the current location appeared\r\nIf you want to use it from another location, set the -- quantify script path argument from the command line\r\n```\r\nIt's still this error. I have also made other attempts: `python3/Users/sunxiaotong/Desktop/llama/llama.cpp/quantize.py - q/ quantize.py 7B`\r\n```\r\n\r\n```\r\nusage: python3 quantize.py [-h] [-r] [-m MODELS_PATH]\r\n[-q QUANTIZE_SCRIPT_PATH]\r\n{7B,13B,30B,65B} [{7B,13B,30B,65B} ...]\r\npython3 quantize.py: error: argument models: invalid choice: '/Users/sunxiaotong/Desktop/llama/llama.cpp/models/7B/ggml-model-f16.bin' (choose from '7B', '13B', '30B', '65B')\r\n```\r\nMay I ask where I was wrong? Can you give me some suggestions",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/621/comments",
    "author": "sun-rabbit",
    "comments": [
      {
        "user": "besnardjb",
        "created_at": "2023-03-30T11:23:40Z",
        "body": "Initially had the same issue on Linux, you need to point using the \"-q\" argument to the \"quantize\" binary generated when compiling llama (quantize.py doing a subprocess call to it and it generates this error when calling itself). \r\n\r\n```\r\npython3 quantize.py -q ./BUILD/bin/quantize  7B\r\n```"
      },
      {
        "user": "prusnak",
        "created_at": "2023-03-30T23:22:05Z",
        "body": "The `quantize.py` script is not needed anymore. Just fetch the latest code and do this as a quantization step:\r\n\r\n```\r\n./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n```"
      },
      {
        "user": "huangzhimin4read",
        "created_at": "2023-03-31T00:34:49Z",
        "body": "> The `quantize.py` script is not needed anymore. Just fetch the latest code and do this as a quantization step:\r\n> \r\n> ```\r\n> ./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n> ```\r\n\r\nI tried this method in Colab, but it still reports an error:\r\n/bin/bash: ./quantize: No such file or directory"
      },
      {
        "user": "archiwed",
        "created_at": "2023-03-31T01:21:59Z",
        "body": "@besnardjb, i was having the same error and this worked for me.\r\n\r\nOS; Linux"
      },
      {
        "user": "sun-rabbit",
        "created_at": "2023-03-31T02:16:43Z",
        "body": "> The `quantize.py` script is not needed anymore. Just fetch the latest code and do this as a quantization step:\r\n> \r\n> ```\r\n> ./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n> ```\r\n\r\nThank you very much， this worked for me.\r\n\r\nOS： macBookPro"
      },
      {
        "user": "sania96",
        "created_at": "2024-07-19T05:49:46Z",
        "body": "> > The `quantize.py` script is not needed anymore. Just fetch the latest code and do this as a quantization step:\r\n> > ```\r\n> > ./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin 2\r\n> > ```\r\n> \r\n> I tried this method in Colab, but it still reports an error: /bin/bash: ./quantize: No such file or directory\r\n\r\nHi, Did you find any solution for colab?\r\nI am having the same issue.\r\nRegards."
      },
      {
        "user": "rickb-lb",
        "created_at": "2024-08-27T12:16:26Z",
        "body": "zsh: no such file or directory: ./quantize\r\n\r\nThere is no file called quantize in the llama.cpp folder structure"
      }
    ]
  },
  {
    "number": 614,
    "title": "Which tokenizer.model is needed for GPT4ALL? ",
    "created_at": "2023-03-30T00:44:00Z",
    "closed_at": "2023-03-31T02:59:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/issues/614",
    "body": "Which tokenizer.model is needed for GPT4ALL for use with convert-gpt4all-to-ggml.py? Is it the one for LLaMA 7B? It is unclear from the current README and gpt4all-lora-quantized.bin seems to be typically distributed without the tokenizer.model file.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/614/comments",
    "author": "cannin",
    "comments": [
      {
        "user": "BenjiKCF",
        "created_at": "2023-03-30T06:25:27Z",
        "body": "I wanna kwo this question too. "
      },
      {
        "user": "KASR",
        "created_at": "2023-03-30T07:05:47Z",
        "body": "I've used the same tokenizer as the one for the llama models. "
      },
      {
        "user": "MeinDeutschkurs",
        "created_at": "2023-04-30T09:39:25Z",
        "body": "Yeah, it's interesting. Want to use the model of freedomGPT inside of it. :D But it does not work, if I simply copy the file to GPT4ALLs path."
      }
    ]
  },
  {
    "number": 503,
    "title": "Is it possible to run 65B with 32Gb of Ram ?",
    "created_at": "2023-03-25T17:17:10Z",
    "closed_at": "2023-03-26T10:18:47Z",
    "labels": [
      "question",
      "hardware",
      "model"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/issues/503",
    "body": "I already quantized my files with this command ./quantize ./ggml-model-f16.bin.X E:\\GPThome\\LLaMA\\llama.cpp-master-31572d9\\models\\65B\\ggml-model-q4_0.bin.X 2 , the first time it reduced my files size from 15.9 to 4.9Gb and when i tried to do it again nothing changed. After i executed this command \"./main -m ./models/65B/ggml-model-q4_0.bin -n 128 --interactive-first\" and when everything is loaded i enter my prompt, my memory usage goes to 98% (25Gb by main.exe) and i just wait dozens of minutes with nothing that appears heres an example:\r\n\r\n**PS E:\\GPThome\\LLaMA\\llama.cpp-master-31572d9> ./main -m ./models/65B/ggml-model-q4_0.bin -n 128 --interactive-first\r\nmain: seed = 1679761762\r\nllama_model_load: loading model from './models/65B/ggml-model-q4_0.bin' - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load: n_ctx   = 512\r\nllama_model_load: n_embd  = 8192\r\nllama_model_load: n_mult  = 256\r\nllama_model_load: n_head  = 64\r\nllama_model_load: n_layer = 80\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 2\r\nllama_model_load: n_ff    = 22016\r\nllama_model_load: n_parts = 8\r\nllama_model_load: ggml ctx size = 41477.73 MB\r\nllama_model_load: memory_size =  2560.00 MB, n_mem = 40960\r\nllama_model_load: loading model part 1/8 from './models/65B/ggml-model-q4_0.bin'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 2/8 from './models/65B/ggml-model-q4_0.bin.1'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 3/8 from './models/65B/ggml-model-q4_0.bin.2'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 4/8 from './models/65B/ggml-model-q4_0.bin.3'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 5/8 from './models/65B/ggml-model-q4_0.bin.4'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 6/8 from './models/65B/ggml-model-q4_0.bin.5'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 7/8 from './models/65B/ggml-model-q4_0.bin.6'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\nllama_model_load: loading model part 8/8 from './models/65B/ggml-model-q4_0.bin.7'\r\nllama_model_load: .......................................................................................... done\r\nllama_model_load: model size =  4869.09 MB / num tensors = 723\r\n\r\nsystem_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 |\r\n\r\nmain: prompt: ' '\r\nmain: number of tokens in prompt = 2\r\n     1 -> ''\r\n 29871 -> ' '\r\n\r\nmain: interactive mode on.\r\nsampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\r\n\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - If you want to submit another line, end your input in '\\'.\r\n\r\n how to become rich**",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/503/comments",
    "author": "TerraTR",
    "comments": [
      {
        "user": "sussyboiiii",
        "created_at": "2023-03-25T18:32:54Z",
        "body": "I ran it and it took about 45GB of RAM and was pretty slow."
      },
      {
        "user": "SpeedyCraftah",
        "created_at": "2023-03-25T20:26:41Z",
        "body": "I tried to recently and it's a terrible experience. My computer kept freezing and it made it even hard to move the mouse because of the disk swap (I have a very fast SSD) - not to mention the 1 minute per word or so of performance.\r\n\r\nStick with 30B, it's still good and very capable, and you can actually use your computer while running the model, otherwise upgrade to 64GB."
      },
      {
        "user": "anzz1",
        "created_at": "2023-03-25T22:32:01Z",
        "body": "Yes 4-bit quantized max is 30B for 32GB of RAM. Couldn't use 65B at all. With 3-bit or 2-bit quantization the 65B could fit, however from the data published it seems that for both RTN/GPTQ , 4-bit quantized 30B outperforms 3b/2b 65B.\r\n\r\n"
      },
      {
        "user": "gjmulder",
        "created_at": "2023-03-26T09:05:41Z",
        "body": "The memory access patterns are very random so trying to use swap is just performance testing the OS paging system and your swap drive."
      },
      {
        "user": "prusnak",
        "created_at": "2023-03-26T10:18:57Z",
        "body": "I think this has been answered, let's close"
      },
      {
        "user": "slipperybeluga",
        "created_at": "2023-05-09T01:42:17Z",
        "body": "FWIW, I was unable to use quantized 65B even with 64 gb of ram. Still way too slow on ubuntu with an i9-10850k & rtx 3060 and binary compiled w/ BLAS. Giving up and switching to 30B. "
      }
    ]
  },
  {
    "number": 228,
    "title": "7B/13B: Inability to write certain words/names with smaller models",
    "created_at": "2023-03-17T07:46:08Z",
    "closed_at": "2023-03-17T08:31:37Z",
    "labels": [
      "question",
      "model"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/issues/228",
    "body": "Hey!\r\n\r\nWhen I attempted to tell the bot in a chat-like prompt that my name is \"Nils\", I ran into an issue where the bot kept interpreting my name as \"Nil\" instead. I then noticed further issues with the word \"guild\" and some other words too.\r\nIs this a bug or to be expected? It does not happen on 30B, I couldn't give 65B a try.\r\n\r\nThanks\r\nNiansa",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/228/comments",
    "author": "niansa",
    "comments": [
      {
        "user": "gjmulder",
        "created_at": "2023-03-17T08:17:19Z",
        "body": "Larger models are going to perform better. They \"understand\" a wider range of concepts such as the difference between \"niL\" and \"nils\"."
      },
      {
        "user": "niansa",
        "created_at": "2023-03-17T08:31:37Z",
        "body": "Alright, that explains it. Thanks!"
      }
    ]
  },
  {
    "number": 10246,
    "title": "web UI : support syntax highlighting",
    "created_at": "2024-11-10T13:49:17Z",
    "closed_at": "2024-12-15T11:55:56Z",
    "labels": [
      "enhancement",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/issues/10246",
    "body": "Add support for syntax highlighting in the code fragments of the model output.",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/10246/comments",
    "author": "slaren",
    "comments": [
      {
        "user": "ChineseHamberger",
        "created_at": "2024-11-13T09:05:29Z",
        "body": "I would like to try this issue."
      },
      {
        "user": "ChineseHamberger",
        "created_at": "2024-11-20T08:53:31Z",
        "body": "I am new to this area and would appreciate any guidance you can provide on how to get started. Could you please offer some suggestions or point me towards resources that might be helpful?\r\n\r\nThank you,"
      },
      {
        "user": "slaren",
        "created_at": "2024-11-21T17:25:22Z",
        "body": "The web UI is made using vuejs and daisyui, so I would suggest looking into them and see if they have any components that may be useful for this purpose. I don't have any experience with these frameworks, so I can't offer you much more guidance that."
      }
    ]
  },
  {
    "number": 5745,
    "title": "build(python): Package scripts with pep-0517 compliance",
    "created_at": "2024-02-27T06:34:38Z",
    "closed_at": "2024-07-04T15:39:13Z",
    "labels": [
      "help wanted",
      "build",
      "script",
      "refactoring",
      "examples",
      "Review Complexity : Low",
      "python",
      "devops"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/pull/5745",
    "body": "## Gist\n\nPackages the python scripts in this repo in a PEP 517-compliant way.\nOther changes:\n- Script names had to be converted to snake_case so they could be used as modules via\n  entrypoints declared in `pyproject.toml`.\n\nRef: #5664\n",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/5745/comments",
    "author": "ditsuke",
    "comments": [
      {
        "user": "ditsuke",
        "created_at": "2024-03-10T16:31:10Z",
        "body": "Would appreciate a review here. I can rebase to resolve conflicts once a review begins."
      },
      {
        "user": "ggerganov",
        "created_at": "2024-03-10T16:53:22Z",
        "body": "Update script names in `ci/run.sh` to use underscores and check other places that might need updates too"
      },
      {
        "user": "ditsuke",
        "created_at": "2024-07-02T10:36:24Z",
        "body": "Rebased, resolved conflicts and addressed a minor review point. Requesting review again"
      },
      {
        "user": "ditsuke",
        "created_at": "2024-07-03T20:00:35Z",
        "body": "There is again a trivial conflict here. I'll resolve it once the PR is otherwise ready to merge, looking forward to an approval."
      },
      {
        "user": "ditsuke",
        "created_at": "2024-07-04T15:27:17Z",
        "body": "Rebased"
      },
      {
        "user": "ditsuke",
        "created_at": "2024-07-04T16:51:31Z",
        "body": "Thanks for merging"
      },
      {
        "user": "ggerganov",
        "created_at": "2024-07-05T04:48:21Z",
        "body": "@SomeoneSerge We prefer to squash-merge all PRs, even when the individual commits are well organized"
      },
      {
        "user": "SomeoneSerge",
        "created_at": "2024-07-05T04:59:10Z",
        "body": "@ggerganov Acknowledged. I rushed a bit to sneak in before new conflicts"
      }
    ]
  },
  {
    "number": 3730,
    "title": "Missing tokenizer tests",
    "created_at": "2023-10-22T20:00:20Z",
    "closed_at": "2023-10-24T07:17:18Z",
    "labels": [
      "help wanted",
      "testing"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/issues/3730",
    "body": "AFAIU we are missing tokenizer tests for supported models like\r\n\r\n* Baichuan\r\n* Bloom\r\n* GptNeoX\r\n* Persimmon\r\n* Refact\r\n* Starcoder\r\n\r\nIt would be great if anyone would be helping out.",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/3730/comments",
    "author": "goerch",
    "comments": [
      {
        "user": "Galunid",
        "created_at": "2023-10-22T21:01:43Z",
        "body": "If it's the same deal as what I did in the stablelm PR, I can do that tomorrow"
      },
      {
        "user": "Galunid",
        "created_at": "2023-10-22T21:47:29Z",
        "body": "Should we apply this fix to conversion scripts of other `gpt2`-tokenizer based models before generating the vocab files?\r\n\r\n```diff\r\nfor i in range(vocab_size):\r\n-    tokens.append(reverse_vocab[i] if i in reverse_vocab else f\"[PAD{i}]\")\r\n-    scores.append(0.0) # dummy\r\n-    toktypes.append(gguf.TokenType.NORMAL)\r\n+    if i not in reverse_vocab:\r\n+        tokens.append(f\"[PAD{i}]\")\r\n+        toktypes.append(gguf.TokenType.USER_DEFINED)\r\n+    elif reverse_vocab[i] in added_vocab:\r\n+        # NOTE: wouldn't we like to distinguish CONTROL tokens here?\r\n+        tokens.append(reverse_vocab[i])\r\n+        toktypes.append(gguf.TokenType.USER_DEFINED)\r\n+    else:\r\n+        tokens.append(reverse_vocab[i])\r\n+        toktypes.append(gguf.TokenType.NORMAL)\r\n```"
      },
      {
        "user": "goerch",
        "created_at": "2023-10-22T22:01:09Z",
        "body": "> Should we apply this fix to conversion scripts of other `gpt2`-tokenizer based models before generating the vocab files?\r\n\r\nGood idea, but no: let us see and fix the issues accordingly."
      },
      {
        "user": "goerch",
        "created_at": "2023-10-23T13:24:13Z",
        "body": "@Galunid : I retested conversion of `mpt` and tested conversion of `gpt-neox` with the following code\r\n\r\n```\r\nadded_vocab = tokenizer.get_added_vocab()\r\nreverse_vocab = {id: encoded_tok for encoded_tok, id in tokenizer.vocab.items()}\r\n\r\nfor i in range(vocab_size):\r\n    if i not in reverse_vocab:\r\n        tokens.append(f\"[PAD{i}]\")\r\n        toktypes.append(gguf.TokenType.USER_DEFINED)\r\n    elif reverse_vocab[i] in added_vocab:\r\n        tokens.append(reverse_vocab[i])\r\n        if tokenizer.added_tokens_decoder[i].special:\r\n            toktypes.append(gguf.TokenType.CONTROL)\r\n        else:\r\n            toktypes.append(gguf.TokenType.USER_DEFINED)\r\n    else:\r\n        tokens.append(reverse_vocab[i])\r\n        toktypes.append(gguf.TokenType.NORMAL)\r\n\r\ngguf_writer.add_token_list(tokens)\r\ngguf_writer.add_token_types(toktypes)\r\n```\r\n\r\nincoorperating @jploski 's explanation of how to detect special tokens. Tests work still (`mpt`) and now(`gpt-neox`). So I think it is fine for me if you go ahead and fix the conversion scripts of the gpt2-tokenizer this way. Thanks for your help!"
      },
      {
        "user": "Galunid",
        "created_at": "2023-10-23T14:00:21Z",
        "body": "Sure, to clarify, you mean to rework all the gpt2 conversion scripts and update the vocabs in the test PR?"
      },
      {
        "user": "goerch",
        "created_at": "2023-10-23T14:11:08Z",
        "body": "> Sure, to clarify, you mean to rework all the gpt2 conversion scripts and update the vocabs in the test PR?\r\n\r\nAs you like and your time permits. I hope someone will object if this is the wrong way to go."
      }
    ]
  },
  {
    "number": 1181,
    "title": "Update SHA256SUMS after quantization change",
    "created_at": "2023-04-25T19:10:29Z",
    "closed_at": "2023-04-25T21:41:56Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/ggml-org/llama.cpp/pull/1181",
    "body": "#729 changed the quantization output for Q4_0 and Q4_2. We should update SHA256SUMS so as not to confuse people.\r\n\r\nI don't have the 65B model lying around, would be great if someone could provide that and check my changes.",
    "comments_url": "https://api.github.com/repos/ggml-org/llama.cpp/issues/1181/comments",
    "author": "sw",
    "comments": [
      {
        "user": "prusnak",
        "created_at": "2023-04-25T21:23:25Z",
        "body": "I'll do the 65B variant ... don't merge yet."
      },
      {
        "user": "prusnak",
        "created_at": "2023-04-25T21:42:29Z",
        "body": "> I'll do the 65B variant ... don't merge yet.\r\n\r\n65B added and merged"
      }
    ]
  }
]