[
  {
    "number": 4347,
    "title": "Where is TensorRT-10.8.0.43.l4t.aarch64-gnu.cuda-12.8.tar.gz ?",
    "created_at": "2025-02-05T18:55:33Z",
    "closed_at": "2025-02-05T21:25:27Z",
    "labels": [
      "question",
      "Module:Embedded",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/4347",
    "body": "Not exists TensorRT 10.8 for jetpack6? ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/4347/comments",
    "author": "johnnynunez",
    "comments": [
      {
        "user": "kevinch-nv",
        "created_at": "2025-02-05T21:25:26Z",
        "body": "There is unfortunately no TensorRT 10.8 Jetpack build due to a few known issues. The recommendation is to use 10.7 until the next release."
      }
    ]
  },
  {
    "number": 3996,
    "title": "Colcon build failure of TensorRT using python3.8 and tensorrt-10.1.0.27",
    "created_at": "2024-07-10T09:01:42Z",
    "closed_at": "2025-02-12T00:26:00Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/3996",
    "body": "## Description\r\nRunning colcon build on **autoware** package of ROS2 humble into the **docker** environment gives me this errors:\r\n\r\n```\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp: In member function ‘bool lidar_transfusion::TransfusionTRT::preprocess(const PointCloud2&, const tf2_ros::Buffer&)’:\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:158:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setTensorAddress’\r\n  158 |   network_trt_ptr_->context->setTensorAddress(\r\n      |                              ^~~~~~~~~~~~~~~~\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:160:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setInputShape’; did you mean ‘setInputShapeBinding’?\r\n  160 |   network_trt_ptr_->context->setInputShape(\r\n      |                              ^~~~~~~~~~~~~\r\n      |                              setInputShapeBinding\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:165:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setTensorAddress’\r\n  165 |   network_trt_ptr_->context->setTensorAddress(\r\n      |                              ^~~~~~~~~~~~~~~~\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:167:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setInputShape’; did you mean ‘setInputShapeBinding’?\r\n  167 |   network_trt_ptr_->context->setInputShape(\r\n      |                              ^~~~~~~~~~~~~\r\n      |                              setInputShapeBinding\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:170:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setTensorAddress’\r\n  170 |   network_trt_ptr_->context->setTensorAddress(\r\n      |                              ^~~~~~~~~~~~~~~~\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:172:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setInputShape’; did you mean ‘setInputShapeBinding’?\r\n  172 |   network_trt_ptr_->context->setInputShape(\r\n      |                              ^~~~~~~~~~~~~\r\n      |                              setInputShapeBinding\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:176:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setTensorAddress’\r\n  176 |   network_trt_ptr_->context->setTensorAddress(\r\n      |                              ^~~~~~~~~~~~~~~~\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:178:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setTensorAddress’\r\n  178 |   network_trt_ptr_->context->setTensorAddress(\r\n      |                              ^~~~~~~~~~~~~~~~\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:180:30: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘setTensorAddress’\r\n  180 |   network_trt_ptr_->context->setTensorAddress(\r\n      |                              ^~~~~~~~~~~~~~~~\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp: In member function ‘bool lidar_transfusion::TransfusionTRT::inference()’:\r\n/home/user/autoware/src/universe/autoware.universe/perception/lidar_transfusion/lib/transfusion_trt.cpp:187:44: error: ‘class nvinfer1::IExecutionContext’ has no member named ‘enqueueV3’; did you mean ‘enqueueV2’?\r\n  187 |   auto status = network_trt_ptr_->context->enqueueV3(stream_);\r\n      |                                            ^~~~~~~~~\r\n      |                                            enqueueV2\r\ngmake[2]: *** [CMakeFiles/transfusion_lib.dir/build.make:160: CMakeFiles/transfusion_lib.dir/lib/transfusion_trt.cpp.o] Error 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:143: CMakeFiles/transfusion_lib.dir/all] Error 2\r\ngmake: *** [Makefile:146: all] Error 2\r\n---\r\nFailed   <<< lidar_transfusion [1min 0s, exited with code 2]\r\n```\r\n\r\n\r\n\r\n## Environment\r\ndocker environment: autoware ROS2 package\r\n\r\n**TensorRT Version**:\r\n10.1.0.27\r\n\r\n**NVIDIA GPU**:\r\nNVIDIA RTX A2000 Laptop GPU\r\n\r\n**NVIDIA Driver Version**:\r\n555.42.06\r\n\r\n**CUDA Version**:\r\n12.5\r\n\r\n**CUDNN Version**:\r\ncuda_12.2.r12.2\r\n\r\nOperating System:\r\nLinux - Ubuntu 20.04 LTS\r\n\r\nPython Version (if applicable):\r\n3.8\r\n\r\n\r\n\r\n## Question ##\r\nHow can I solve this problem?\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/3996/comments",
    "author": "annb3",
    "comments": [
      {
        "user": "lix19937",
        "created_at": "2024-07-10T15:21:27Z",
        "body": "First you should check your env has multi tensorrt lib so ?  And check your makefile the nvinfer related include path and library path . "
      },
      {
        "user": "LeoZDong",
        "created_at": "2025-02-12T00:26:00Z",
        "body": "Closing as stale issue."
      }
    ]
  },
  {
    "number": 2448,
    "title": "Are src/tensor_quant.cu and .cpp files still used? or deprecated? ",
    "created_at": "2022-11-02T10:07:37Z",
    "closed_at": "2023-02-15T09:39:43Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/2448",
    "body": "Hello,\r\n\r\nlike as what I mentioned in the title, I am wondering whether these files are still used or not.\r\n\r\nIn the `pytorch-quantization/pytorch-quantization/tensor_quant.py`  file, all the functionality of `fake_quantization` are implemented and it seems that `src/tensor_quant.cu` and `src/tensor_quant.cu` are not used anywhere.\r\n\r\nAre these files going to be deprecated in the near future? and if so, for the case of custom number format (e.g., exp_bits=6 and mantissa=1 based FP8), I hope to know your plan (if planned to support).\r\n\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/2448/comments",
    "author": "chaeunl",
    "comments": [
      {
        "user": "zerollzeng",
        "created_at": "2022-11-02T15:55:04Z",
        "body": "@ttyio ^ ^"
      },
      {
        "user": "ttyio",
        "created_at": "2022-11-28T05:26:33Z",
        "body": "@chaeunl , the `*.cu` are cuda extension for fake quant, we will use these to replace current python implementation in the long term.\r\nFor the FP8, we are evaluation it but might not commit in the next release."
      },
      {
        "user": "chaeunl",
        "created_at": "2022-12-05T07:33:54Z",
        "body": "@zerollzeng @ttyio  thanks to your reply.\r\n\r\nSo far as I understand, `*.cu` files are supposed to accelerate some operations like type-casting from float to integer or floating point to block point (or maybe customized number format) with CUDA kernel, whose approach is similar to `QPyTorch`.\r\n\r\nRelated with this issue, is there any way to accumulate the multiplication of two scalar values whose number format is customized? For instance, I am trying to make a custom format FP10 which has 1 bit assigned for sign, 2 bits for mantissa and 7bit for exponent, and multiply them with FP10 multiplier. The multiplication might be quite simple, but dealing with the accumulation of FP10 (i.e., accumulation using custom accumulator), it doesn't look simple task. Could you let me know if there is any good reference for this issue? "
      },
      {
        "user": "ttyio",
        "created_at": "2023-02-15T09:39:43Z",
        "body": "@chaeunl sorry I have no recommandation for the acceleration on customized quantization bits.\r\n\r\nclosing and thanks!"
      }
    ]
  },
  {
    "number": 2183,
    "title": "i just want to known when to use trt api and use onnx if all of them can be work…",
    "created_at": "2022-07-26T11:36:35Z",
    "closed_at": "2022-07-27T06:30:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/2183",
    "body": "Could someone tell me what a difference in build with tensort api and build with onnx? When i test in 2d net work, the speed of them seem to be same and the speed in 3d point clound network may be some different. I like to build net work with tensorRT api but i don’t know whether it has some advantage than onnx.Thanks!.i just want to known when to use trt api and use onnx if all of them can be work…",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/2183/comments",
    "author": "blazeCCC",
    "comments": [
      {
        "user": "zhenhuaw-me",
        "created_at": "2022-07-27T06:30:22Z",
        "body": "In general, if ONNX parser works for you, you don't need to build with TensorRT API since that might take you some time to finish as you need to understand the semantic of the APIs.\r\n\r\nSometimes, API programming might have better performance and engineering benefits.\r\n* For performance, some ONNX models contains subgraph that can be optimized out while TensorRT might not cover yet. For example, consider there is `Mul(1, tensor)` in ONNX model which equals to `tensor` semantically, you can skip the Mul with API). This problem is due to the ONNX exporters generate some unneeded ops.\r\n* For engineering benefits, API gives you fine grain control over the network while the parser doesn't. For example, it would be easier to dump the output of an activation (mark it as output), also easier to setup precision per layer.\r\n\r\nClosing as this is a QA. Feel free to reopen if any further question."
      },
      {
        "user": "blazeCCC",
        "created_at": "2022-07-27T06:37:58Z",
        "body": "> \r\n\r\nthank you! I basically get it."
      }
    ]
  },
  {
    "number": 1948,
    "title": "The volume of a tensor cannot exceed 2^31-1  (tensorRT 8.0.16)",
    "created_at": "2022-04-23T14:28:51Z",
    "closed_at": "2022-04-25T12:12:49Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1948",
    "body": "[Matrix Multiply]_output: tensor volume exceeds (2^31)-1, dimensions are [4,65536,65536])\r\n\r\nwhat should I do?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1948/comments",
    "author": "773041642",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-04-25T08:34:05Z",
        "body": "@773041642 , tensor size has 2G limitation in TensorRT, could you make your network smaller? Thanks"
      },
      {
        "user": "773041642",
        "created_at": "2022-04-25T08:40:24Z",
        "body": "Whether there is a way to modify this limit, changing the size will affect the accuracy of the model ."
      },
      {
        "user": "ttyio",
        "created_at": "2022-04-25T11:40:56Z",
        "body": "@773041642 , sorry this is not configurable , because many kernel implementations using 32bit when addressing the offset in tensors."
      },
      {
        "user": "773041642",
        "created_at": "2022-04-25T12:12:46Z",
        "body": "@ttyio, thanks for your help, I will try to reduce my tensor."
      },
      {
        "user": "gesanqiu",
        "created_at": "2024-10-09T08:06:39Z",
        "body": "@ttyio Any update about this limitation? For stable-diffusion model, it's tensor size can exceed this limitation easily because of the high resolution output."
      }
    ]
  },
  {
    "number": 1924,
    "title": "How to use *.engine file in VS2017?",
    "created_at": "2022-04-14T08:59:07Z",
    "closed_at": "2022-06-15T07:17:28Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1924",
    "body": "I have exported *.onnx to *.engine file, but I cannot find an approach to call it wherein demos.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1924/comments",
    "author": "June1124",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-04-15T01:46:45Z",
        "body": "@June1124 , you can use `trtexec --loadEngine` to use the file, the source code is also open sourced, thanks!"
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-06-15T07:17:28Z",
        "body": "closing this issue for now due to >14 days with no response. Please feel free to reopen if the issue still exists. Thanks"
      }
    ]
  },
  {
    "number": 1911,
    "title": "cross Operating System platform compatibility with the same GPU platform",
    "created_at": "2022-04-09T02:41:20Z",
    "closed_at": "2022-04-12T12:28:30Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1911",
    "body": "A quick question:\r\nI have compiled a TensorRT inference engine on Linux x86-64 Ubuntu 18.04.2 LTS and GPU 3070.\r\n1. Is it ok to reuse it on Windows-7 x86-64 and the same GPU 3070, plus the same (or as similar as possible) TensorRT and CUDA environment?\r\n2. If not, any issue or problem?\r\n3. And what can I do about it?\r\n\r\nRegards\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1911/comments",
    "author": "leimeng86",
    "comments": [
      {
        "user": "zerollzeng",
        "created_at": "2022-04-12T03:20:37Z",
        "body": "no, we don't support that"
      },
      {
        "user": "ttyio",
        "created_at": "2022-04-12T08:13:20Z",
        "body": "Hello @leimeng86 , this is not recommend workflow and not tested by QA, Could you elaborate the use scenario here? thanks"
      },
      {
        "user": "leimeng86",
        "created_at": "2022-04-12T12:28:23Z",
        "body": "> Hello @leimeng86 , this is not recommend workflow and not tested by QA, Could you elaborate the use scenario here? thanks\r\n\r\nWe are developing models and doing TensorRT optimizations on Linux OS, while the actual end product would be running on Windows platforms.\r\nAnd as you say, it would not be an appropriate workflow and we are trying to work around it.\r\nThanks!"
      }
    ]
  },
  {
    "number": 1908,
    "title": "platform_has_fast_fp16平台支持问题",
    "created_at": "2022-04-07T09:37:31Z",
    "closed_at": "2022-06-15T07:38:27Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1908",
    "body": "您好，我想问一下笔记本mx系列显卡（mx150、mx250、mx350、mx330）支持fp16 的engine么？",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1908/comments",
    "author": "ggyybb",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-04-12T08:10:38Z",
        "body": "Hello @ggyybb , they are pascal devices with capability sm61, fp16 is supported but not enabled in TRT due to they are not high performance. Thanks"
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-06-15T07:38:27Z",
        "body": "closing this for now due to >14 days with no response. Please feel free to reopen if the issue still exists. Thanks"
      }
    ]
  },
  {
    "number": 1844,
    "title": "how to use the trt API to implement torch.roll ?tks",
    "created_at": "2022-03-09T06:01:12Z",
    "closed_at": "2022-04-26T00:11:25Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1844",
    "body": null,
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1844/comments",
    "author": "henbucuoshanghai",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-03-15T14:00:31Z",
        "body": "@henbucuoshanghai , have you tried export it to onnx? thanks!"
      },
      {
        "user": "ttyio",
        "created_at": "2022-04-26T00:11:25Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have issue, thanks!"
      }
    ]
  },
  {
    "number": 1810,
    "title": "Creating a AGX Xavier trt file from x86 ",
    "created_at": "2022-02-21T15:19:27Z",
    "closed_at": "2022-03-15T08:30:55Z",
    "labels": [
      "question",
      "wontfix",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1810",
    "body": "For our workflow it would be very convenient to not have to use a Xavier to create each trt on an actual Xavier device to test. Is there any way to do this? Can the Xavier environment be emulated in any way with docker and qemu?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1810/comments",
    "author": "Emmanuel-Messulam",
    "comments": [
      {
        "user": "zerollzeng",
        "created_at": "2022-02-23T04:30:17Z",
        "body": "we don't support this feature now :-) building a trt engine require running all kernel on a device and choosing the fastest kernel. so it needs to be built on the actual device. "
      },
      {
        "user": "ttyio",
        "created_at": "2022-03-15T08:30:55Z",
        "body": "Closing since no activatity for more than 3 weeks, thanks!"
      }
    ]
  },
  {
    "number": 1776,
    "title": "Is it normal that the first call to executeV2 takes long?",
    "created_at": "2022-02-03T08:11:06Z",
    "closed_at": "2022-06-15T08:03:54Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1776",
    "body": "The first call to executeV2 after engine and execution context initialization takes much longer (1 s) than all subsequent calls to executeV2 (10 ms). Is this normal? Is there anything that can be done to reduce the duration of the first executeV2 call?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1776/comments",
    "author": "katrasnikj",
    "comments": [
      {
        "user": "spivakoa",
        "created_at": "2022-02-03T09:44:45Z",
        "body": "Yes, totally. The first call to the driver API takes time."
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-06-15T08:03:54Z",
        "body": "closing for now due to >14 days with no response. Please feel free to reopen if the issue still exists. Thanks"
      }
    ]
  },
  {
    "number": 1740,
    "title": "Memory leak when creating and deleting a new IRuntime",
    "created_at": "2022-01-18T10:19:00Z",
    "closed_at": "2022-04-26T00:18:16Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1740",
    "body": "# Description\r\n\r\nI noticed that every time I new and delete an IRuntime, it will cause the GPU memory to increase. \r\nAlthough I have deleted the IRuntime absolutely, but the GPU memory is not clear.\r\n\r\n# Environment\r\n\r\nTensorRT Version: 8.2.2.1\r\nGPU Type: NVIDIA Quadro T1000\r\nNvidia Driver Version: 460.89\r\nCUDA Version: 11.3\r\nCUDNN Version: 8.2.1\r\nOperating System + Version: win 10\r\n\r\n# Relevant code\r\n\r\n```cpp\r\n#include \"NvInferRuntimeCommon.h\"\r\n#include \"NvInferRuntime.h\"\r\n#include \"cuda_runtime_api.h\"\r\n#include <iostream>\r\n\r\nclass Logger : public nvinfer1::ILogger\r\n{\r\n    public:\r\n    void log(Severity severity, const char *msg) noexcept\r\n    {\r\n        using namespace std;\r\n        string s;\r\n        switch (severity) {\r\n            case Severity::kINTERNAL_ERROR:\r\n                s = \"INTERNAL_ERROR\";\r\n                break;\r\n            case Severity::kERROR:\r\n                s = \"ERROR\";\r\n                break;\r\n            case Severity::kWARNING:\r\n                s = \"WARNING\";\r\n                break;\r\n            case Severity::kINFO:\r\n                s = \"INFO\";\r\n                break;\r\n            case Severity::kVERBOSE:\r\n                s = \"VERBOSE\";\r\n                break;\r\n        }\r\n        std::cerr << s << \": \" << msg << std::endl;\r\n    }\r\n\r\n};\r\n\r\nvoid main(){\r\n    {\r\n        size_t avail;\r\n        size_t total;\r\n        cudaMemGetInfo(&avail, &total);\r\n        size_t used = total - avail;\r\n        std::cout << \"===================\" << std::endl;\r\n        std::cout << \"Device memory used: \" << (float)used/(1024*1024*1024) << \" GB\" << std::endl;\r\n        std::cout << \"Total memory used: \" << (float)total/(1024*1024*1024) << \" GB\" << std::endl;\r\n        std::cout << \"===================\" << std::endl;\r\n    }\r\n    \r\n    Logger* logger = new Logger;\r\n    IRuntime* runtime = createInferRuntime(*logger);\r\n    delete logger;\r\n    delete runtime;\r\n    \r\n    while (1) {\r\n        size_t avail;\r\n        size_t total;\r\n        cudaMemGetInfo(&avail, &total);\r\n        size_t used = total - avail;\r\n        std::cout << \"===================\" << std::endl;\r\n        std::cout << \"Device memory used: \" << (float)used/(1024*1024*1024) << \" GB\" << std::endl;\r\n        std::cout << \"Total memory used: \" << (float)total/(1024*1024*1024) << \" GB\" << std::endl;\r\n        std::cout << \"===================\" << std::endl;\r\n\r\n        Sleep(1000);\r\n    } \r\n    \r\n}\r\n```\r\n\r\n\r\n\r\n```bash\r\n===================\r\nDevice memory used: 0.748047 GB\r\nTotal memory used: 4 GB\r\n===================\r\nINFO: [MemUsageChange] Init CUDA: CPU +329, GPU +0, now: CPU 14350, GPU 891 (MiB)\r\n===================\r\nDevice memory used: 0.870117 GB\r\nTotal memory used: 4 GB\r\n===================\r\n===================\r\nDevice memory used: 0.870117 GB\r\nTotal memory used: 4 GB\r\n===================\r\n===================\r\nDevice memory used: 0.870117 GB\r\nTotal memory used: 4 GB\r\n===================\r\n===================\r\nDevice memory used: 0.870117 GB\r\nTotal memory used: 4 GB\r\n===================\r\n```\r\n\r\n\r\n\r\nthe GPU memory will increase about 0.12207 GB, and  `delete`  doesn't work! \r\n\r\nWhy does this happen, shouldn't the memory be freed after delete?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1740/comments",
    "author": "LeonJinC",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-02-22T07:54:46Z",
        "body": "@LeonJinC these memory are used by library like cublas, cudnn... they are not free unless you unload the library. "
      },
      {
        "user": "ttyio",
        "created_at": "2022-04-26T00:18:16Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have issue, thanks!"
      },
      {
        "user": "ghgggg",
        "created_at": "2022-12-08T07:47:23Z",
        "body": "hi, @LeonJinC  ,do    you  have solved  this issue ? if so, could you give me some advice ,I met this also. Thanks a lot"
      },
      {
        "user": "ReverseSystem001",
        "created_at": "2023-10-30T01:55:24Z",
        "body": "> these memory are used by library like cublas, cudnn... they are not free unless you unload the library.\r\n\r\nI meet this problem too. Have you solved it, and how"
      },
      {
        "user": "OPyshkin",
        "created_at": "2024-01-15T20:08:27Z",
        "body": "Same problem"
      }
    ]
  },
  {
    "number": 1715,
    "title": "Deprecation of destroy() and buildEngineWithConfig()",
    "created_at": "2022-01-12T15:21:11Z",
    "closed_at": "2022-04-26T00:16:20Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1715",
    "body": "Hello, could someone briefly outline how to handle the deprecation of destroy() and buildEngineWithConfig() in TRT 8 plugins?\r\n\r\nAs I understand it from the release notes, calling destroy() is not necessary anymore and is handled by smart pointers / destructor call under the hood?\r\nWhich fucntion / pattern replaces buildEngineWithConfig()?\r\n\r\nThanks in advance for clarifying this.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1715/comments",
    "author": "philipp-schmidt",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-02-22T07:31:58Z",
        "body": "@philipp-schmidt , \r\nThe `destroy()` is not deprecated for plugin, we just deprecated for builder/runtime, etc.\r\n\r\nNow we use `buildSerializedNetwork` as replacement for `buildEngineWithConfig`"
      },
      {
        "user": "ttyio",
        "created_at": "2022-04-26T00:16:20Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have issue, thanks!"
      }
    ]
  },
  {
    "number": 1713,
    "title": "the softmax result is differnet with pytorch nn.softmax(-1)",
    "created_at": "2022-01-12T02:27:44Z",
    "closed_at": "2022-04-26T00:16:09Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1713",
    "body": "\r\nMy input dim is 1 16 65 65,then I use the \t\r\nnvinfer1::ISoftMaxLayer *softmax = m_network->addSoftMax(*Layers[inputName]);\r\nsoftmax->setAxes(3);\r\n\r\nbut whatever the axe I use(0,1,2,3),I can not got the same result with pytorc",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1713/comments",
    "author": "YLongJin",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2022-02-22T07:33:24Z",
        "body": "@YLongJin , have you tried expose your pytorch model that contains only softmax only to onnx, and load the onnx in trt, is the result different? "
      },
      {
        "user": "ttyio",
        "created_at": "2022-04-26T00:16:09Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have issue, thanks!"
      },
      {
        "user": "YLongJin",
        "created_at": "2022-04-26T00:18:08Z",
        "body": "Tanks  i  solove it by antoher "
      }
    ]
  },
  {
    "number": 1684,
    "title": "TensorRt Engines For SampleMaskRCNN",
    "created_at": "2021-12-24T19:01:48Z",
    "closed_at": "2022-01-20T05:11:30Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1684",
    "body": "Hi have been using the sampleUffMaskRCNN in the repository for building a segmentation application. For which i have made and saved the engine files locally. I have mofdified the sample code to suit my requirements.\r\n\r\nI have two different engines both built for the image segmentation based on MaskRCNN but slight different. I want to deserilize and load both engine to memory so that my application can choose which engien to run inference on as per the inpus given to the application. Is this possible i.e loading two tensorrt engines simulatenously to jetsoon Xavier Nx's memory?\r\n\r\nJetpack version 4.6",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1684/comments",
    "author": "hakunaMatataHub",
    "comments": [
      {
        "user": "zerollzeng",
        "created_at": "2021-12-31T14:50:23Z",
        "body": "Yes you can do that"
      },
      {
        "user": "ttyio",
        "created_at": "2022-01-20T05:11:30Z",
        "body": "Closing since the question is answered, thanks"
      }
    ]
  },
  {
    "number": 1678,
    "title": "How to use the ONNX model with int8 calibration? ",
    "created_at": "2021-12-21T09:59:19Z",
    "closed_at": "2022-03-16T01:29:57Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1678",
    "body": "Hi,\r\n\r\nI am trying to following the example in PyTorch-Quantization Toolkit to do the int8 Quantization.\r\nHowever, after I manager to get the generated onnx file. I am not sure how to use this onnx file and use trtexec to complete the inference of python tensorrt.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1678/comments",
    "author": "liuzhuang1024",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-12-22T04:35:06Z",
        "body": "@lzmisscc , you can use \r\n\r\n        trtexec --onnx your_onnx_file --int8\r\n\r\nthanks"
      },
      {
        "user": "liuzhuang1024",
        "created_at": "2021-12-23T07:28:45Z",
        "body": "> @lzmisscc , you can use\r\n> \r\n> ```\r\n>     trtexec --onnx your_onnx_file --int8\r\n> ```\r\n> \r\n> thanks\r\n\r\nBut after I converted to int8, I used tensorrt for reasoning,  video memory did not decrease, only the speed decreased. What's the matter?\r\n\r\nthanks"
      },
      {
        "user": "Water2style",
        "created_at": "2021-12-26T20:37:26Z",
        "body": "> Hi,\r\n> \r\n> I am trying to following the example in PyTorch-Quantization Toolkit to do the int8 Quantization. However, after I manager to get the generated onnx file. I am not sure how to use this onnx file and use trtexec to complete the inference of python tensorrt.\r\n\r\n兄弟你可以成功转onnx吗？我们直接失败了.还在debug\r\n要不要合作一波啊 一起搞."
      },
      {
        "user": "liuzhuang1024",
        "created_at": "2021-12-27T02:56:55Z",
        "body": "> > Hi,\r\n> > I am trying to following the example in PyTorch-Quantization Toolkit to do the int8 Quantization. However, after I manager to get the generated onnx file. I am not sure how to use this onnx file and use trtexec to complete the inference of python tensorrt.\r\n> \r\n> 兄弟你可以成功转onnx吗？我们直接失败了.还在debug 要不要合作一波啊 一起搞.\r\n\r\nemail: 1028741371@qq.com"
      },
      {
        "user": "ttyio",
        "created_at": "2021-12-28T02:54:27Z",
        "body": "> > @lzmisscc , you can use\r\n> > ```\r\n> >     trtexec --onnx your_onnx_file --int8\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > thanks\r\n> \r\n> But after I converted to int8, I used tensorrt for reasoning, video memory did not decrease, only the speed decreased. What's the matter?\r\n> \r\n> thanks\r\n\r\nFrom our previous experience, the most use of gpu memory come from the load of cudnn, cublas library. The weights and activations not contribute to the major of the memory consumption. "
      },
      {
        "user": "liuzhuang1024",
        "created_at": "2021-12-28T08:30:24Z",
        "body": "> > > @lzmisscc , you can use\r\n> > > ```\r\n> > >     trtexec --onnx your_onnx_file --int8\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > thanks\r\n> > \r\n> > \r\n> > But after I converted to int8, I used tensorrt for reasoning, video memory did not decrease, only the speed decreased. What's the matter?\r\n> > thanks\r\n> \r\n> From our previous experience, the most use of gpu memory come from the load of cudnn, cublas library. The weights and activations not contribute to the major of the memory consumption.\r\n\r\nok，I see. Thanks!"
      },
      {
        "user": "ttyio",
        "created_at": "2022-03-16T01:29:57Z",
        "body": "Closing and please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1632,
    "title": "Error Code 1: Cuda Runtime (context is destroyed)",
    "created_at": "2021-11-22T03:44:29Z",
    "closed_at": "2022-07-01T06:17:43Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1632",
    "body": "## Description\r\n\r\nI've had the strangest cleanup error. In the snippet below there is a simple setup of loading a trt engine, then create a context. I noticed if I didn't do `cuda_mem = cuda.mem_alloc(1)`, or the three `del` statements, TensorRT would complain Seg Fault. Baffled! I'm manually cleaning up the objects now, but wonder why this happens.\r\n\r\nI tried two different engine files, from two distinct sources.\r\n\r\n```\r\n[TensorRT] ERROR: 1: [hardwareContext.cpp::terminateCommonContext::141] Error Code 1: Cuda Runtime (invalid device context)\r\n[TensorRT] INTERNAL ERROR: [defaultAllocator.cpp::free::85] Error Code 1: Cuda Runtime (invalid argument\r\n```\r\n\r\n```\r\nimport pycuda.driver as cuda\r\nimport tensorrt as trt\r\nimport pycuda.autoinit\r\n\r\nif __name__ == '__main__':\r\n    model_path = \"engine.trt\"\r\n\r\n    with open(model_path, \"rb\") as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:\r\n        engine = runtime.deserialize_cuda_engine(f.read())\r\n    context = engine.create_execution_context()\r\n    # cuda_mem = cuda.mem_alloc(1)\r\n    del context\r\n    del engine\r\n    del runtime\r\n```\r\n\r\n## Environment\r\n\r\n**TensorRT Version**: 8.0.1 \r\n**NVIDIA GPU**: T4\r\n**NVIDIA Driver Version**: 450.119.03\r\n**CUDA Version**: 11.4\r\n**CUDNN Version**: 8.2.2\r\n**Operating System**: Linux\r\n**Python Version (if applicable)**: 3.8.10\r\n\r\n**Baremetal or Container (if so, version)**: NVIDIA Release 21.08 (build 25497161)",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1632/comments",
    "author": "cjenkins5614",
    "comments": [
      {
        "user": "svobora",
        "created_at": "2021-11-30T12:40:52Z",
        "body": "Double delete of runtime (first is auto deleted because of \"with\" block)\r\n\r\nTry this:\r\n```\r\nimport pycuda.driver as cuda\r\nimport tensorrt as trt\r\nimport pycuda.autoinit\r\n\r\nif __name__ == '__main__':\r\n    model_path = \"engine.trt\"\r\n    runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\r\n\r\n    with open(model_path, \"rb\") as f:\r\n        engine = runtime.deserialize_cuda_engine(f.read())\r\n\r\n    context = engine.create_execution_context()\r\n\r\n    # cuda_mem = cuda.mem_alloc(1)\r\n\r\n    del context\r\n    del engine\r\n    del runtime\r\n```"
      },
      {
        "user": "leoluopy",
        "created_at": "2022-03-16T05:35:07Z",
        "body": "in my case , this error happens when engine is not del . so only del engine works fine as well."
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-07-01T06:17:43Z",
        "body": "Closing due to >14 days without activity. Please feel free to reopen if the issue still exists. Thanks\r\n"
      }
    ]
  },
  {
    "number": 1630,
    "title": "How to save the calibration.bin files?",
    "created_at": "2021-11-20T09:29:05Z",
    "closed_at": "2022-01-25T01:41:56Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1630",
    "body": "## Description\r\nCurrently, I'm trying to generate INT8 TRT engine with calibrations, like that\r\n`calibrator = Calibrator(data_loader=calib_data(), cache=\"identity-calib.cache\")\r\n    build_engine = EngineFromNetwork(\r\n        NetworkFromOnnxPath(\"identity.onnx\"), config=CreateConfig(int8=True, calibrator=calibrator)\r\n    )`\r\nBut I was really confused about the mechanisms:\r\n1. when was calibration performed? within the 'EngineFromNetwork' process? I tried to set break-point at calibration::get_batch(), but It did not works;\r\n2. How to get the calibration.bin files? I have tried to call the function of \"write_calibration_cache(self, cache)\",  But I don't know which 'cache' to pass in.\r\nPlease help me with these two problems, Thanks a lot.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1630/comments",
    "author": "xingyueye",
    "comments": [
      {
        "user": "pranavm-nvidia",
        "created_at": "2021-11-22T14:59:46Z",
        "body": "1. Yes, calibration happens when the engine is being built. `EngineFromNetwork` is lazily evaluated, so you need to call it to build the engine: `engine = build_engine()`. \r\nAlternatively, you can use the immediately evaluated variants (`snake_case` instead of `PascalCase`):\r\n```py\r\nengine = engine_from_network(NetworkFromOnnxPath(\"identity.onnx\"), \r\n                             config=CreateConfig(int8=True, calibrator=calibrator) )\r\n```\r\n\r\n2. The calibrator is an interface that's implemented by the user and called by TRT. So `write_calibration_cache` is not intended to be called by you; instead it will be called once TRT finishes calibrating. "
      },
      {
        "user": "xingyueye",
        "created_at": "2021-11-23T06:44:29Z",
        "body": "Thanks for your kindly reply. I have build quantized engine correctly and found the generated \"calibration.cache\".\r\nBTW, Is there any superior ways to select a better calibration datasets? \r\n"
      },
      {
        "user": "pranavm-nvidia",
        "created_at": "2021-11-23T14:17:14Z",
        "body": "It should ideally be representative of your input data; e.g. a subset of the training data may work well"
      },
      {
        "user": "ttyio",
        "created_at": "2022-01-25T01:41:56Z",
        "body": "close since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1599,
    "title": "7.2.3 tag?",
    "created_at": "2021-11-08T22:42:04Z",
    "closed_at": "2022-01-25T01:43:17Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1599",
    "body": "I see tags for most other versions of the TensorRT binary distribution, but not one for 7.2.3 - is there another version of this repo which should be used with the 7.2.3 binary instead?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1599/comments",
    "author": "markdjwilliams",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-12-10T06:36:10Z",
        "body": "Hello @markdjwilliams , could you use `release/7.2`? thanks!"
      },
      {
        "user": "ttyio",
        "created_at": "2022-01-25T01:43:17Z",
        "body": "close since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1546,
    "title": "tensorrt.IBuilderConfig",
    "created_at": "2021-10-11T02:18:45Z",
    "closed_at": "2022-01-25T01:46:19Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1546",
    "body": "AttributeError: 'tensorrt.tensorrt.IBuilderConfig' object has no attribute 'set_calibration_profile'",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1546/comments",
    "author": "qybing",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-12-10T02:01:49Z",
        "body": "Hello @qybing , which TRT version are you using? this API should be available in 7.2 and later. thanks!"
      },
      {
        "user": "ttyio",
        "created_at": "2022-01-25T01:46:19Z",
        "body": "close since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1499,
    "title": "How can I use int8 mode?",
    "created_at": "2021-09-19T14:28:33Z",
    "closed_at": "2022-01-25T01:46:59Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1499",
    "body": "when A.onnx is 1000MB\r\nI use  './trtexec --onnx=A.onnx --workspace=1000 --saveEngine=B.trt --fp16' to generate B.trt\r\nB.trt is ~500M\r\nThen I use './trtexec --onnx=A.onnx --workspace=1000 --saveEngine=B.trt --int8' to generate C.trt\r\nC.trt is more than 1000MB\r\n\r\nIs  this correct?\r\nhow can i generate a 1/4 size model using int8?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1499/comments",
    "author": "franksilke",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-09-28T06:04:02Z",
        "body": "Hello @franksilke , some int8 implementation require persistent memory in the algorithm, and these persistent memory is also serialized in the engine file, so you observe the file size increase.\r\nBTW, what's the trt version and cuda version in your test? thanks"
      },
      {
        "user": "ttyio",
        "created_at": "2022-01-25T01:46:59Z",
        "body": "close since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1489,
    "title": "Plugin use constant node as input (reformatted node is added automatically)",
    "created_at": "2021-09-14T08:30:03Z",
    "closed_at": "2021-09-28T06:24:37Z",
    "labels": [
      "question",
      "Module:Plugins",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1489",
    "body": "## Description\r\n\r\nI extended the IPluginV2IOExt to implement a matmul plugin. When using a constant layer as input, a reformatted layer is automatically inserted by TensorRT (reformatted from shape [1, 64] to [-2, 64]). I want to know is there any solutions to avoid this since it hurts the performance much. Thanks for any advice! \r\n\r\n\r\n## Environment\r\n\r\n**TensorRT Version**: 8.0\r\n**CUDA Version**: 11\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1489/comments",
    "author": "haoxuhao",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-09-28T06:21:42Z",
        "body": "@haoxuhao\r\nwhen the output tensor layout (`TensorFormat`) from previous layer not supported by your plugin's `supportsFormatCombination`, we will insert reformat to make the layout match, then call your plugin.\r\nso you can add breakpoint to check the missing layout, and implement the layout in your kernel to remove this reformat.\r\nThanks!"
      },
      {
        "user": "haoxuhao",
        "created_at": "2021-09-28T06:24:18Z",
        "body": "Thanks!!!, I've already solved it through setting \"canBroadcastInputAcrossBatch\". "
      },
      {
        "user": "ttyio",
        "created_at": "2021-09-28T06:37:37Z",
        "body": "> Thanks!!!, I've already solved it through setting \"canBroadcastInputAcrossBatch\".\r\n\r\nGreat, thanks for let me know. You are right, just checked the code:\r\n\r\n    For IPlugin, when the input is broadcast and kernel did not use N stride = 0, the plugin crashed;\r\n    For IPluginV2, broadcast tensors are replicated before feed plugin, plugin kernel can access it as non-broadcast tensor;\r\n    For IPluginV2Ext, when the input is broadcast and the canBroadcastInputAcrossBatch() return true, the input is a broadcast tensor like IPlugin;\r\n\r\nSo you remember to set N stride = 0 in your code."
      },
      {
        "user": "duduscript",
        "created_at": "2022-10-09T03:28:43Z",
        "body": "Hi @ttyio , I extended IPluginV2DynamicExt and I can not override \"canBroadcastInputAcrossBatch\", how to solve this problem?"
      }
    ]
  },
  {
    "number": 1482,
    "title": "Question relate to format of input and output",
    "created_at": "2021-09-08T14:33:40Z",
    "closed_at": "2021-09-10T02:40:10Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1482",
    "body": "I have a question !\r\n\r\nI know 2 variables to set data types of input and output: --inputIOFormats and --outputIOFormats. 2 variables are set for all input and output. (These inputs and outputs have the same data types)\r\n\r\nAnd my question is if my model have multi input (or multi output) and these inputs have different data types, , how to adjust properly? \r\n\r\nFor example: if I have 3 inputs, input1: int32, input2: float32, input3: float64, and 2 outputs, output1: float16, output2: float32. What should I do ?\r\n\r\nThanks for reading and hope to get reply soon !",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1482/comments",
    "author": "taquanghung1705199",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-09-10T02:29:14Z",
        "body": "Hello @taquanghung1705199 , we can use comma to separated them, e.g,\r\n\r\n       --inputIOFormats=int32:chw,float32:chw"
      }
    ]
  },
  {
    "number": 1439,
    "title": "Inconsistent inference results between tensorrt 7 and tensorrt 8",
    "created_at": "2021-08-06T08:24:56Z",
    "closed_at": "2021-12-13T10:26:09Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1439",
    "body": "When I use the same onnx converted to trt file, the inference results in tensorrt7 and tensorrt8 are not consistent, where tensorrt 8 has the correct result and tensorrt 7 appears to have more bbox, some of which have some position shift.\r\nThe model I am using is retinanet,what is the problem and why tensorrt7 and tensorrt8 behave differently on this model?\r\nWhen I use the ssd model, the results of tensorrt7 and tensorrt8 are basically the same and correct.\r\n\r\nEnvironment\r\nTensorRT Version: 8.0.1.6\r\nNVIDIA GPU: GTX1650TI\r\nNVIDIA Driver Version: 470\r\nCUDA Version: 10.2\r\nCUDNN Version: 8.2.0\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1439/comments",
    "author": "mesenlol",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-09-09T14:34:34Z",
        "body": "Hello @mesenlol , we have bug fix in each release. Do you have accuracy result for both on whole public benchmark/dataset? thanks"
      },
      {
        "user": "ttyio",
        "created_at": "2021-12-13T10:26:09Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1421,
    "title": "What's the default quantization mode for TensorRT PTQ.",
    "created_at": "2021-07-31T03:39:38Z",
    "closed_at": "2021-12-13T10:25:34Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1421",
    "body": "According to TensorRT's document, TensorRT only supports **symmetric and uniform** type quantization, which means quantization zero-point should always be 0.\r\n\r\nBut when I set the dynamic range(e.g. (0, 5.6845)) for network layers manually, I find TensorRT calculates a scale and a non-zero zero-point through the verbose logs. So does TensorRT support non-symmetric uniform type quantization which is in conflict with the document?\r\n\r\nAnd are the weights quantized per channel by default in PTQ? Can the user configure it to be per tensor?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1421/comments",
    "author": "un-knight",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-09-09T10:36:54Z",
        "body": "@un-knight , no we only support symmetric and uniform so far. could you paste the log line here?\r\n\r\nYes the weights is per channel and cannot configure by user for PTQ."
      },
      {
        "user": "ttyio",
        "created_at": "2021-12-13T10:25:34Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1378,
    "title": "generate calibration Environment",
    "created_at": "2021-07-16T04:02:03Z",
    "closed_at": "2021-09-10T05:53:49Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1378",
    "body": "## Description\r\nI generate a calibration file on Environment (tensorrt 7.2.1).\r\nCan I directly use the calibration file on  Environment (tensorrt 7.2.3) or I have to generate  the a calibration file on Environment(tensorrt 7.2.3) again?\r\nDoes it has (something) to do with TensorRT Version, NVIDIA GPU, NVIDIA Driver Version, CUDA Version？So, I have to re-generate a calibration file on New Environment\r\n？\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1378/comments",
    "author": "alicera",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-08-02T07:36:27Z",
        "body": "Hello @alicera , you can try direct use the calibration file using the calibrator's `read_calibration_cache` interface, trt will skip the file if it is incompatible, thanks!"
      },
      {
        "user": "ttyio",
        "created_at": "2021-09-10T05:53:49Z",
        "body": "Closing since no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1351,
    "title": "Does TensorRT 8.0.1 OSS support windows build?",
    "created_at": "2021-07-07T07:54:44Z",
    "closed_at": "2021-07-22T03:17:19Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1351",
    "body": "I'm very excited about TensorRT8.0.1 new features.\r\nEven though there is prebuild version, I still want to know that TensorRT8.0.1 OOS native build will be official supported?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1351/comments",
    "author": "p890040",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-07-08T02:01:57Z",
        "body": "Sorry @p890040 no official windows build for OSS yet."
      },
      {
        "user": "pustar",
        "created_at": "2021-07-21T00:50:34Z",
        "body": "What's the meaning of OSS?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-22T03:16:50Z",
        "body": "@pustar \r\nOpen-source software"
      }
    ]
  },
  {
    "number": 1349,
    "title": "Do I have to create and run the engine on the same Nvidia Graphics Card?",
    "created_at": "2021-07-07T03:51:21Z",
    "closed_at": "2021-07-08T02:38:11Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1349",
    "body": "I generate tensorrt model (.trt) from pytorch model in machine A, and want to run the .trt file in machine B, (A and B have different  Graphics Cards). Is it allowed?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1349/comments",
    "author": "BChunlei",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-07-08T02:08:36Z",
        "body": "Hello @BChunlei , it is not allowed, some generated kernel is device dependent in the mode."
      },
      {
        "user": "BChunlei",
        "created_at": "2021-07-08T02:36:34Z",
        "body": "ok, thanks. @ttyio"
      },
      {
        "user": "BChunlei",
        "created_at": "2021-07-08T02:37:24Z",
        "body": "ok, thanks. @ttyio"
      }
    ]
  },
  {
    "number": 1344,
    "title": "dynamic input",
    "created_at": "2021-07-03T12:26:25Z",
    "closed_at": "2022-11-29T05:13:50Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1344",
    "body": "## Description\r\n\r\n<!-- A clear and concise description of the bug or issue. -->\r\n\r\n\r\n## Environment\r\n\r\n**TensorRT Version**: 8.0\r\n**NVIDIA GPU**: 2060\r\n**NVIDIA Driver Version**: 460.56\r\n**CUDA Version**: 10.2\r\n**CUDNN Version**: 8.1\r\n**Operating System**: ubantu\r\n**Python Version (if applicable)**: 3.6\r\n\r\n\r\n\r\n## Relevant Files\r\n\r\nsfl->getOutput(0)->getDimensions() is 1*512*-1,why the shape becomes 0*0*0 by the operation as follows\r\n sfl->setFirstTranspose(Permutation{1, 2, 0});\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1344/comments",
    "author": "cqray1990",
    "comments": [
      {
        "user": "cqray1990",
        "created_at": "2021-07-03T15:01:00Z",
        "body": "lstm input shape: 3 [1 -1 512]\r\n[07/03/2021-22:58:45] [E] [TRT] 3: [network.cpp::addRNNv2::774] Error Code 3: Internal Error (Parameter check failed at: optimizer/api/network.cpp::addRNNv2::774, condition: maxSeqLen > 0\r\n)\r\n\r\ndoes RNN support dynamic input?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-08T02:37:27Z",
        "body": "@cqray1990 yes, the error here is not related to dynamic shape, it is `maxSeqLen`, this is fix value and must larger than 0\r\nThanks"
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-07-01T06:03:09Z",
        "body": "@cqray1990 Could you try TRT 8.4 and see if the issue still exists? Thanks"
      },
      {
        "user": "ttyio",
        "created_at": "2022-11-29T05:13:50Z",
        "body": "closing due to no activity for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1330,
    "title": "How to use maskrcnn in python? Also, can we take live frame from realsense and use instead of ppm ",
    "created_at": "2021-06-30T03:14:14Z",
    "closed_at": "2023-05-30T18:49:11Z",
    "labels": [
      "question",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1330",
    "body": "How to use this repo in python? the program is in C++ and I would like to add realsense data to this uisng python. Can I do that? I am trying to use maskrcnn on jetson xavier. Is there any other source? The samepleMaskrcnn is in c++ and it takes only ppm format. I would like to add realsense data and perform segmentation on a frame.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1330/comments",
    "author": "akshayacharya97",
    "comments": [
      {
        "user": "pra-dan",
        "created_at": "2021-07-05T07:52:05Z",
        "body": "Did you get your model trained on mrcnn and converted it to UFF ?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-06T11:11:48Z",
        "body": "Adding @Tyler-D , thanks ^^"
      },
      {
        "user": "ttyio",
        "created_at": "2023-05-30T18:49:10Z",
        "body": "closing since maskrcnn sample is deprecated in latest release. thanks!"
      }
    ]
  },
  {
    "number": 1325,
    "title": "Onnx -> TensorRT. No speed difference between models of different sizes.",
    "created_at": "2021-06-24T17:15:50Z",
    "closed_at": "2021-08-02T08:33:41Z",
    "labels": [
      "question",
      "Module:ONNX",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1325",
    "body": "I have two yolov5 models of different sizes. One has 35.9m parameters, the other 12.7m.\r\nWhen I convert the models to TensorRT with `trtexec --onnx=model.onnx --batch=5 --fp16` the resulting models have roughly the same inference speed (21 fps) even though the speed should be vastly different.\r\nWhat am I doing wrong?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1325/comments",
    "author": "Fschoeller",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-07-06T09:27:25Z",
        "body": "@Fschoeller , could you check if the batch dimension in the the onnx input has dynamic shape (-1)?\r\nAlso trtexec need `--minShapes, --optShapes, --maxShapes` to build engine with dynamic shapes, and need `--shapes` to inference engine with dynamic shapes.\r\nThanks"
      },
      {
        "user": "Fschoeller",
        "created_at": "2021-07-06T10:24:50Z",
        "body": "The engine was built with a static input size of `(5,3,1280,1280)`\r\nThanks"
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-06T11:06:51Z",
        "body": "@Fschoeller , could you share the 2 models? thanks!"
      },
      {
        "user": "ttyio",
        "created_at": "2021-08-02T08:33:41Z",
        "body": "Closing since no activities for more than 3 weeks, please reopen if you still have question. Thanks!"
      }
    ]
  },
  {
    "number": 1323,
    "title": "What is the distribution when using trtexec to randomly generate data/weight?",
    "created_at": "2021-06-24T09:42:28Z",
    "closed_at": "2021-08-02T08:38:01Z",
    "labels": [
      "question",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1323",
    "body": "For example:\r\n`trtexec --deploy=model.prototxt --iterations=100 --device=0 --avgRuns=1 --warmUp=0 --duration=0 --loadEngine=myengine.eg --output=output --batch=1 --fp16 `\r\n\r\nThen the inputs and weights are both randomly generated. What is the distribution of them? Are they uniform distribution?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1323/comments",
    "author": "xyyaman",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-07-06T09:21:51Z",
        "body": "@xyyaman , the weights come from `myengine.eg` when `--loadEngine` is used, else it is random. \r\nThe input is random when `--loadInputs` is not used.\r\n\r\nAnd yes it is uniform distribution, the range is [-128, 127] for int type and [-1, 1] for floating type."
      },
      {
        "user": "ttyio",
        "created_at": "2021-08-02T08:38:01Z",
        "body": "Closing since no activities for more than 3 weeks, please reopen if you still have question. Thanks!"
      }
    ]
  },
  {
    "number": 1318,
    "title": "Does RTX3090 generated trt engine can run on A100? They all Ampare arch",
    "created_at": "2021-06-22T02:49:21Z",
    "closed_at": "2021-08-02T08:33:02Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1318",
    "body": "Does RTX3090 generated trt engine can run on A100? They all Ampare arch",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1318/comments",
    "author": "lucasjinreal",
    "comments": [
      {
        "user": "lucasjinreal",
        "created_at": "2021-06-22T02:49:35Z",
        "body": "Suppose on same machine, 2 types cards."
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-06T09:09:00Z",
        "body": "@jinfagang , no, their compute capability is different though sass is compatible for them. Why do you want to reuse the engine cross different gpu? thanks"
      },
      {
        "user": "ttyio",
        "created_at": "2021-08-02T08:33:02Z",
        "body": "Closing since no activities for more than 3 weeks, please reopen if you still have question. Thanks!"
      }
    ]
  },
  {
    "number": 1283,
    "title": "How to build 3D tensor with c++ in Tensorrt",
    "created_at": "2021-06-01T08:23:37Z",
    "closed_at": "2021-07-08T04:19:53Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1283",
    "body": "I try to inferent 3dcnn model with tensorrt，but i have no idea adout how to build 3d tensor in tensorrt with c++, could you provide some example to it? ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1283/comments",
    "author": "wangzhenlin123",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-07-02T05:25:38Z",
        "body": "Hello @wangzhenlin123 , tensor support 8d tensor at most.  The n-d tensor are the same, and are you asking for 3d conv? You can use `addConvolutionNd` to add it. Sorry there is no existing sample that I am aware."
      },
      {
        "user": "wangzhenlin123",
        "created_at": "2021-07-02T05:43:19Z",
        "body": "Forgive me for my unclear explanation，for example：“cudaMemcpyAsync(buffers[inputIndex], input, batchSize *INPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice, stream)” We can use OpenCV to read a 2D Mat graph and put it in the Buffer, but， In the case of 3D, is it possible to build the 3D Tensorr input by reading multiple 2D images in succession and then placing them in the Buffer one by one？3D is very important to me because I work with medical 3D images ."
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-02T05:48:41Z",
        "body": "Hello @wangzhenlin123 , yes it is the same, we can first check the input layout of your network, allocate GPU buffer has the same size, and finally use cudaMemcpyAsync to initialize the content of the GPU buffer. Did you hit any issue in doing so? thanks"
      },
      {
        "user": "wangzhenlin123",
        "created_at": "2021-07-08T02:50:30Z",
        "body": "I haven't tried yet，just Do some technical advice first。"
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-08T04:19:50Z",
        "body": "Thanks @wangzhenlin123 , closing, please reopen if you have more issues, thanks"
      }
    ]
  },
  {
    "number": 1255,
    "title": "Saving a context",
    "created_at": "2021-05-18T08:20:16Z",
    "closed_at": "2021-05-21T07:04:55Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1255",
    "body": "Hi,\r\n\r\nI am using your inference.py algorithm in the demo/BERT folder. To run inference, you need to load the engine to build the context as follows:\r\n\r\n  with open(args.engine, \"rb\") as f, \\\r\n      trt.Runtime(TRT_LOGGER) as runtime, \\\r\n      runtime.deserialize_cuda_engine(f.read()) as engine, \\\r\n      engine.create_execution_context() as context:\r\n\r\nIs it possible to save the context so that we don't have to load it every time we want to run inference? \r\nThank you for your answer!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1255/comments",
    "author": "fdlci",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-05-21T04:52:59Z",
        "body": "Hello @fdlci , you can reuse the context, no need to create it for every inference. Did you hit any error? thanks!"
      },
      {
        "user": "fdlci",
        "created_at": "2021-05-21T07:03:29Z",
        "body": "Yes I can run several inferences without loading the context again. I thought I couldn't do that as I was running inference.py on one example only and everytime I tried a new example I had to run the entire code and reload the context again.\r\nThank you for your answer!\r\n "
      },
      {
        "user": "ttyio",
        "created_at": "2021-05-21T07:04:55Z",
        "body": "@fdlci Thanks for confirm, closing"
      }
    ]
  },
  {
    "number": 1247,
    "title": "which version introduced `setOptimizationProfileAsync` ?",
    "created_at": "2021-05-11T08:20:17Z",
    "closed_at": "2021-07-02T08:54:27Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1247",
    "body": "I used TensorRT 7.1.3, the compiler reported an error \"‘class nvinfer1::IExecutionContext’ has no member named setOptimizationProfileAsync \"  ?\r\nDid I miss something or I have to upgrade to version > 7.2.x ?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1247/comments",
    "author": "EmilioZhao",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-05-21T02:47:42Z",
        "body": "Hello @EmilioZhao , yes it is introduced in 7.2."
      }
    ]
  },
  {
    "number": 1245,
    "title": " implemented in c++ CUDA vs TensorRT",
    "created_at": "2021-05-11T05:42:33Z",
    "closed_at": "2021-05-11T07:11:41Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1245",
    "body": "@ttyio\r\nRecently, the object detection model has been successfully converted and the inference rate has also been reduced.\r\nTensorRT uses C++ CUDA more efficiently than PyTorch. For this reason, I am wondering which is the fastest, which is the fastest model implemented in c++ cuda or the model converted to TensorRT.\r\nIf the model is implemented in c++ CUDA and the speed is better than TensorRT, the model will be directly implemented in c++ CUDA.\r\nCan i get a public indicator of this?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1245/comments",
    "author": "DonggeunYu",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-05-11T05:53:24Z",
        "body": "Hello @DonggeunYu , TRT has various optimization for your network, like graph fusion, memory optimization and optimized kernels for different precisions.  It is a lot of effort if you implement all of them. And TRT keep evolution on new GPU arch. There are a lot of adapter work if you direct implement in C++ CUDA."
      },
      {
        "user": "DonggeunYu",
        "created_at": "2021-05-11T07:11:39Z",
        "body": "Thank you :)\r\nIt would be nice to continue using TensorRT."
      }
    ]
  },
  {
    "number": 1237,
    "title": "No heterogeneous precision implementation(FP16,FP32) of Fully Connected Layer in Tensorrt",
    "created_at": "2021-05-07T09:10:26Z",
    "closed_at": "2021-06-07T12:31:18Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1237",
    "body": "## Description\r\nCuda FMA instruction( D = C + A\\*B ) has some implementation that do A\\*B in FP16 and accumelate the result to FP32. This produce more accurate result than that do accumelation in FP16. I tried to achieve this in tensorrt by setting precision mode of \r\nFully Connected Layer to FP16 and Output type to FP32. But this seem not work. Output type of Fully Connected Layer is always same as its precision mode.\r\n\r\n## Code \r\n// layer is an instance of nvinfer1::IFullyConnectedLayer\r\n  layer->setPrecision(nvinfer1::DataType::kHALF);\r\n  layer->getOutput(0)->setType(nvinfer1::DataType::kFLOAT);\r\n  layer->setOutputType(0, nvinfer1::DataType::kFLOAT);\r\n\r\n## Log\r\n// The  output type is still half\r\n2021-05-07 16:40:52.478711: 140697332041472 I tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:32] Layer(FullyConnected): (Unnamed Layer* 7) [Fully Connected], Tactic: 7, (Unnamed Layer* 6) [Shuffle]_output[Half(170,1,1)] -> (Unnamed Layer* 7) [Fully Connected]_output[Half(1024,1,1)]\r\n\r\n## Environment\r\n**TensorRT Version**: 7.0.0.11\r\n**NVIDIA GPU**:  Tesla T4\r\n**NVIDIA Driver Version**: 440.33.01\r\n**CUDA Version**: 10.2\r\n**CUDNN Version**: 7.6.5\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1237/comments",
    "author": "ccjjs",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-05-12T09:20:07Z",
        "body": "Hello @ccjjs , there are 3 precisions in each kernel: input, accumulator and output. \r\nAnd the kernels in TRT do not always have accumulator precision equal to output precision.  \r\nAlso not all layers support kernel that input and output precision different. \r\nThanks"
      },
      {
        "user": "ccjjs",
        "created_at": "2021-05-12T09:30:08Z",
        "body": "hi @ttyio , thanks for reply.Is this mean that a Fully Connected Layer with precision mode FP16 may do accumulation in FP32. "
      },
      {
        "user": "ttyio",
        "created_at": "2021-05-17T02:16:30Z",
        "body": "Hello @ccjjs , it depends, most run on FP16 accumulation, with some fusion pattern might run on FP32.   "
      },
      {
        "user": "ttyio",
        "created_at": "2021-06-07T12:31:18Z",
        "body": "closing since no activity for 3 weeks, please reopen if you still have question, thanks"
      }
    ]
  },
  {
    "number": 1202,
    "title": "Support for refittable networks with dynamic shapes",
    "created_at": "2021-04-16T10:52:50Z",
    "closed_at": "2021-04-27T07:01:14Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1202",
    "body": "Hello,\r\n\r\nI would like to ask whether support for refittable networks with dynamic shapes is planned in near future. Or if it is not a priority.\r\nUse case: U-net like networks where we need to change all weights (without having to rebuild whole network).\r\nI am using TensorRT 7.2.3.4 and Win10.\r\nCurrently the network won't build with this error message:\r\nRefittable networks with dynamic shapes is not supported.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1202/comments",
    "author": "m-ky",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-04-26T10:17:17Z",
        "body": "Sorry @m-ky , there is no support for refit on dynamic shapes. It is not in the high priority queue. \r\nDo you want N to be the dynamic shape? If so you can WAR using the implicit batch in TRT."
      },
      {
        "user": "m-ky",
        "created_at": "2021-04-27T07:01:14Z",
        "body": "Thanks for answer.\r\nI am aware N can be dynamic with implicit batch, but I need to have dynamic size X and Y as well."
      },
      {
        "user": "pmixer",
        "created_at": "2021-05-28T03:38:23Z",
        "body": "> Sorry @m-ky , there is no support for refit on dynamic shapes. It is not in the high priority queue.\r\n> Do you want N to be the dynamic shape? If so you can WAR using the implicit batch in TRT.\r\n\r\nHi Vincent,\r\n\r\nSame question and similar needs here, but for recommendation scenario.\r\n\r\nI could understand the engineering burden to support refit feature for dynamic batch input, as it concerns the new runtime used for dynamic shape input networks introduced in TrT6.\r\n\r\nJust want to double check that:\r\n\r\n0) refit for dynamic shape input networks is doable, just not implemented yet?\r\n1) for NCHW input, if we get NC fixed, only HW accept dynamic shape input, would it be easier to support refit feature?\r\n\r\nI still think dynamic shape input feature is of great value for encouraging wider use of TrT with more flexibility. People used to fail using it as the feature was kind of incomplete(like N, C need to be fixed etc.) before which makes it seems of low priority.\r\n\r\ncc @joohoon FYI.\r\n\r\nRegards,\r\nPeter"
      }
    ]
  },
  {
    "number": 1184,
    "title": "A question about TensorRT cancel point and IExecutionContext",
    "created_at": "2021-04-12T16:14:55Z",
    "closed_at": "2021-04-14T03:20:46Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1184",
    "body": "Hello there, I am a developer of inference task serving system. We use TensorRT 6/TensorRT 7 as our inference execute framework. Due to soft realtime limitation, we sometimes need to cancel current context->execute() / context->executeV2() for next inference task running safely.\r\nI didn't find any solution on TensorRT documentation, can TensorRT development team gives me some advice of cancel context->execute()? My context->execute() is running on a single POSIX thread, can I cancel it safely? Or can you give me more information about TensorRT cancellation point? Thanks a lot!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1184/comments",
    "author": "KarKLi",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-04-14T02:08:06Z",
        "body": "Sorry @KarKLi , it is CUDA limitation that we cannot cancel the kernels that already enqueued. even for cudaDeviceReset, it will first flush the work that pending in the queue and wait for GPU idle first."
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T02:29:41Z",
        "body": "> Sorry @KarKLi , it is CUDA limitation that we cannot cancel the kernels that already enqueued. even for cudaDeviceReset, it will first flush the work that pending in the queue and wait for GPU idle first.\r\n\r\nThx. And I have another question that the IExecutionContext created by engine->CreateExecutionContext() / engine->CreateExecutionContextWithoutDeviceMemory() can be reused? The \"reused\" means I don't call ctx->destroy(), save the pointer and use it again for later inference with CUDA stream or just CUDA. Will the inference execute properly?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T02:36:58Z",
        "body": "Hello @KarKLi , yes the `IExecutionContext` can be reused. But do not call `IExecutionContext::enqueue()`  with 2 different cuda stream simultaneously.  This is because intermediate tensor is resource of `IExecutionContext`,  behavior of execute the same context simultaneously on 2 different stream is undefined."
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T02:40:58Z",
        "body": "> Hello @KarKLi , yes the `IExecutionContext` can be reused. But do not call `IExecutionContext::enqueue()` with 2 different cuda stream simultaneously. This is because intermediate tensor is resource of `IExecutionContext`, behavior of execute the same context simultaneously on 2 different stream is undefined.\r\n\r\nthanks for your reply! What if I create two ```IExecutionContext``` pointer by the same engine or different engines and call ```IExecutionContext::enqueue()``` / ```IExecutionContext::enqueueV2()``` with a same cuda stream, will it cause undefined behaviour?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T02:53:26Z",
        "body": "Hello @KarKLi , \r\ncases are valid:\r\n- ctx A and ctx B run on cuda stream A \r\n- ctx A run on cuda stream A and ctx B run on cuda stream B\r\n- ctx A run on cuda stream A, then run on stream B after waiting stream A finished\r\n\r\nonly invalid case:\r\n- ctx A run on cuda stream A, and run on stream B without event sync/wait"
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T03:00:23Z",
        "body": "> Hello @KarKLi ,\r\n> cases are valid:\r\n> \r\n> * ctx A and ctx B run on cuda stream A\r\n> * ctx A run on cuda stream A and ctx B run on cuda stream B\r\n> * ctx A run on cuda stream A, then run on stream B after waiting stream A finished\r\n> \r\n> only invalid case:\r\n> \r\n> * ctx A run on cuda stream A, and run on stream B without event sync/wait\r\n\r\nThanks! I have last question that can the ctx's execution memory be exposed to user by some kind of TensorRT API? If not, forget to record the device memory address when I call ```ctx->setDeviceMemory()``` will cause GPU memory leak?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T03:19:12Z",
        "body": "Helo @KarKLi , \r\ndo you mean activations when you say `execution memory`? activations are shared between contexts for the same engine.\r\ncurrently only the device memory is exposed and you can use `createExecutionContextWithoutDeviceMemory`/`setDeviceMemory` to set them, or use `createExecutionContext` to ask TRT to manage this part of memory. and yes there will be memory leak if you manage it but not proper released."
      },
      {
        "user": "KarKLi",
        "created_at": "2021-04-14T03:20:46Z",
        "body": "> Helo @KarKLi ,\r\n> do you mean activations when you say `execution memory`? activations are shared between contexts for the same engine.\r\n> currently only the device memory is exposed and you can use `createExecutionContextWithoutDeviceMemory`/`setDeviceMemory` to set them, or use `createExecutionContext` to ask TRT to manage this part of memory. and yes there will be memory leak if you manage it but not proper released.\r\n\r\nGot it. Thanks!"
      }
    ]
  },
  {
    "number": 1142,
    "title": "Low FPS on tensorRT YoloV3 Jetson Nano",
    "created_at": "2021-03-20T06:57:09Z",
    "closed_at": "2022-06-15T10:34:04Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1142",
    "body": "I converted my custom yolov3 model to onnx to tesnsorrt model on Jetson nano, it is taking 0.2 sec to predict on images i tried it on video and it is giving only 3 fps is there any way to increase this. ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1142/comments",
    "author": "anantgupta129",
    "comments": [
      {
        "user": "galagam",
        "created_at": "2022-05-12T07:18:03Z",
        "body": "@anantgupta129 by \"custom\" - do you mean custom weights, or custom layers? \r\nSee below a partial list of directions to help you debug this further: \r\n1. Which power mode are you using? \r\n2. What other processes are you running on the Jetson Nano? (0.2s per image should result in 5 fps, not 3fps) \r\n3. Which JetPack/TensorRT versions? Try upgrading\r\n4. Profile the network's runtime layer by layer, using `trtexec --exportProfile=<file>`. Which layers are most time consuming?"
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-06-15T10:34:04Z",
        "body": "\r\nClosing for now due to >14 days with no response. Please feel free to reopen if the issue still exists. Thanks\r\n\r\n"
      }
    ]
  },
  {
    "number": 1101,
    "title": "When will the Ubuntu 20.04 support be released?",
    "created_at": "2021-03-06T02:31:10Z",
    "closed_at": "2021-04-14T05:50:52Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1101",
    "body": "We are in  2021 now.\r\nPlease be harry, we need to use TensorRT 5.1.5.0 on Ubuntu 20.04.\r\nThank you for your contribution in the past, and please release Ubuntu 20.04 support as soon as possible.\r\nGL !",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1101/comments",
    "author": "xhding1997",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-03-08T01:02:11Z",
        "body": "Adding @rajeevsrao for the roadmap of platform support, thanks!"
      },
      {
        "user": "rajeevsrao",
        "created_at": "2021-03-09T17:54:52Z",
        "body": "@xhding1997 Ubuntu 20.04 is tentatively scheduled for the 21.05 release timeframe."
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T05:50:52Z",
        "body": "Closing since no activity in this thread for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 1050,
    "title": "How to set cuda device with tensorRT python API?",
    "created_at": "2021-02-08T05:39:54Z",
    "closed_at": "2022-06-15T10:39:40Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1050",
    "body": "The engine built with python api is always running on GPU:0. Is there any python api to set the device that the engine run?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1050/comments",
    "author": "XianglongTan",
    "comments": [
      {
        "user": "JosephChenHub",
        "created_at": "2021-02-08T09:53:19Z",
        "body": "> The engine built with python api is always running on GPU:0. Is there any python api to set the device that the engine run?\r\n\r\nFor pycuda, you can set the environment `CUDA_DEVICE` before \r\n```\r\nimport pycuda.driver as cuda\r\nimport pycuda.autoinit\r\n```\r\ne.g.,  to set `os.environ['CUDA_DEVICE'] = '1'` will make GPU:1 as the default device.  Another method provided in `onnx-tensorrt` is \r\n```\r\nfrom ctypes import cdll, c_char_p\r\nlibcudart = cdll.LoadLibrary('libcudart.so')\r\nlibcudart.cudaGetErrorString.restype = c_char_p\r\ndef cudaSetDevice(device_idx):\r\n    ret = libcudart.cudaSetDevice(device_idx)\r\n    if ret != 0:\r\n        error_string = libcudart.cudaGetErrorString(ret)\r\n        raise RuntimeError(\"cudaSetDevice: \" + error_string)\r\n\r\n```"
      },
      {
        "user": "nvpohanh",
        "created_at": "2022-06-15T10:39:40Z",
        "body": "\r\nClosing for now due to >14 days with no response. Please feel free to reopen if the issue still exists. Thanks\r\n\r\n"
      },
      {
        "user": "RunningLeon",
        "created_at": "2023-04-12T06:38:59Z",
        "body": "@nvpohanh Hi, is it normal that after set device to 1, GPU memory(around 1G) on device 0 is also used when do inference? When I run the engine with `trtexec` with `--device=1`,  no GPU memory from device 0 is used. "
      },
      {
        "user": "nvpohanh",
        "created_at": "2023-04-13T00:59:22Z",
        "body": "That may be a bug in trtexec code..."
      },
      {
        "user": "RunningLeon",
        "created_at": "2023-04-13T07:23:35Z",
        "body": "> That may be a bug in trtexec code...\r\n\r\n\r\n@nvpohanh \r\nWhy is that? When `--device=1`, it's reasonable to use GPU memory only from device 1."
      },
      {
        "user": "nvpohanh",
        "created_at": "2023-04-13T07:27:08Z",
        "body": "You are correct. Could you open a new issue for this and provide repro steps? After that, @zerollzeng can try to repro and open an internal tracker for this issue. Thanks"
      }
    ]
  },
  {
    "number": 1042,
    "title": "nvinfer1::ICudaEngine::getNbBindings output Tensor order",
    "created_at": "2021-02-03T09:33:44Z",
    "closed_at": "2021-04-14T05:46:07Z",
    "labels": [
      "question",
      "Module:ONNX",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/1042",
    "body": "I use onnx interface to deploy my network by onnxruntime and tensorrt. But I found the output tensor order of onnxruntime  matched with torch.export.onnx order and the output tensor order of tensorrt mismatched with it.\r\n\r\nfor example torch.export.onnx  as \r\n``` features = self.backbone(images)\r\n        output = features\r\n        if self.model_config.get('neck', False):\r\n            x = self.neck(features)\r\n        if self.model_config.get('bbox_head', False):\r\n            box_cls, box_regression, centerness = self.bbox_head(x)\r\n            output = [box_clses, box_regressions, centernesses]\r\n        return output\r\n```\r\ntensorrt output order: [box_cls, box_regression, centerness] * fpn_level\r\nonnxruntime output order:  [box_clses, box_regressions, centernesses]\r\nhow can I change the tensorrt config to match torch.export.onnx order?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/1042/comments",
    "author": "CharelBIT",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2021-02-06T00:27:35Z",
        "body": "Sorry @CharelBIT , could you elaborate more on what the `fpn_level`? I donot see them in the code.  "
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-14T05:46:07Z",
        "body": "Closing since no activity in this thread for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 864,
    "title": "TensorRT-7.1.3.4/samples/common/buffers.h:447: undefined reference to `sample::gLogError",
    "created_at": "2020-10-29T13:04:25Z",
    "closed_at": "2020-11-04T00:52:20Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/864",
    "body": "hi, i am  use  tensorrt to accerate a simple classification model, liek resnet,i learned a similar project(tensorrt's googlenet),after i write my code and run it, it occurs such problem, can anybody help me?\r\nmy envir:ubuntu16.04+cuda10.2+tensorrt7.1.3.4+cudnn8.0.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/864/comments",
    "author": "chegnyanjun",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-29T13:56:43Z",
        "body": "`gLogError` is defined in `${SAMPLES_DIR}/common/logger.cpp`. You need to make sure to link your sample to this file. See `CMakeLists.txt` for any sample, it has the line `include(../../CMakeSamplesTemplate.txt)`. `CMakeSamplesTemplate.txt` adds logger.cpp.\r\nDoes this fix your problem?"
      },
      {
        "user": "chegnyanjun",
        "created_at": "2020-11-04T00:49:11Z",
        "body": "＠mk-nvidia,thanks for your reply, i have fixed my problem.thank you so much."
      },
      {
        "user": "iambyd",
        "created_at": "2022-08-03T01:10:05Z",
        "body": "i got a sample error ,my CMakeLists.txt have include header file like this \r\n`include_directories(/usr/local/TensorRT-8.4.1.5/samples/common)`\r\n\r\nmy env:\r\n - system: ubuntu 20.4\r\n - cuda:11.1 \r\n - tensort:8.4.1.5\r\n - cudnn:8.4.1.50\r\n"
      },
      {
        "user": "FengZhiheng",
        "created_at": "2023-09-19T03:27:24Z",
        "body": "I also have the same problem, my CMakeLists.txt  have \r\n```\r\nINCLUDE_DIRECTORIES(/usr/local/TensorRT-8.6.1.6/include/) \r\nINCLUDE_DIRECTORIES(/usr/local/TensorRT-8.6.1.6/samples/common/)\r\n```\r\nmy env:\r\nsystem: linux centos\r\ncuda:10.2\r\ntensort:8.6.1.6\r\n"
      },
      {
        "user": "BrandonVeldman",
        "created_at": "2023-10-15T20:05:14Z",
        "body": "Hi \r\nafter i have done coding my program of simple_shell i experience these right after adding my gcc compiler command line what should i do to make it right ?\r\n\r\n\r\n/usr/bin/ld: /tmp/ccia5GLt.o: in function `exit_cmd':\r\ncmd_exit.c:(.text+0x28): undefined reference to `free_buffers'\r\n/usr/bin/ld: /tmp/ccEShV1t.o: in function `execution':\r\nexecu.c:(.text+0x85): undefined reference to `free_buffers'\r\n/usr/bin/ld: /tmp/ccJamUGt.o: in function `main':\r\nshell.c:(.text+0x8a): undefined reference to `free_buffers'\r\n/usr/bin/ld: shell.c:(.text+0x96): undefined reference to `free_buffers'\r\ncollect2: error: ld returned 1 exit status"
      }
    ]
  },
  {
    "number": 858,
    "title": "How to pass two input images to the network for the interface in the TensorRT-7.1.3.4/samples/python/yolov3_onnx",
    "created_at": "2020-10-28T11:43:08Z",
    "closed_at": "2021-02-08T02:19:22Z",
    "labels": [
      "question",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/858",
    "body": "Hi,  I am using a Yolov3-Tensorflow model (This model basically changed for the two inputs for training (visual and infrared image) at the same time, then applied two-darknet 53 models for features extraction, then features fusion, and finally object detection). \r\n\r\nI have trained, test and used this model for demo in my computer with TensorFlow. Now I want to accelerate it using TensorRT. \r\n\r\nI have converted my model from  .pb > .onnx > .trt engine sucessfully.  \r\n\r\nNow How i can utilise this converted .trt engine for interface using  `TensorRT-7.1.3.4/samples/python/yolov3_onnx` example? \r\nThe only difference is two inputs so I do not understand how to change this example for my model. \r\n1- How I can pass two input images? \r\n2- What changes I need to make in this example?  \r\n3- Or is there any other example for using .trt model for the interface? ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/858/comments",
    "author": "MuhammadAsadJaved",
    "comments": [
      {
        "user": "rajeevsrao",
        "created_at": "2020-11-17T10:11:31Z",
        "body": "Specify the input shapes before build_cuda_engine\r\n```\r\nnetwork.get_input(0).shape = [1, 3, 608, 608]\r\nnetwork.get_input(1).shape = [1, 3, 608, 608]\r\n```\r\n\r\nAllocate buffers and update input bindings before do_inference_v2\r\n```\r\ninputs, outputs, bindings, stream = common.allocate_buffers(engine)\r\ninputs[0].host = image1\r\ninputs[1].host = image2\r\n```"
      },
      {
        "user": "ttyio",
        "created_at": "2021-02-08T02:19:22Z",
        "body": "Closing since no activity in this thread for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 853,
    "title": "make trtexec error  fatal error: NvOnnxParserTypedefs.h: No such file or directory",
    "created_at": "2020-10-27T09:11:28Z",
    "closed_at": "2020-10-30T03:53:57Z",
    "labels": [
      "question",
      "Module:Samples",
      "Module:OSS Build",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/853",
    "body": "## Description\r\n\r\nI have installed TensorRT-7.1 using deb package. with cuda-10.2 and cuDNN 8.0.  I am able to import tensorrt as trt in python3 as well as `dpkg -l | grep TensorRT` show the installed packages. \r\n\r\n1- Now I am trying to make `trtexec` which are availabe in `/usr/src/tensorrt/samples/trtexec` folder. \r\n\r\nWhen i run the `make` or `sudo make` in the trtexec directory i got the follwing errors. \r\n\r\n2- I am confused that i have installed the tensorrt now do i still need to install TensorRT OSS   to run samples and use trtexec etc?  or the tensorrt installed in the `/usr/src/tensorrt/samples/trtexec` is enough? \r\n\r\nI have attached the error which i found during make in the trtexec. \r\n\r\n\r\n\r\n\r\n## Environment\r\n\r\n**TensorRT Version**:  7.1.3-4+cuda10.2\r\n**GPU Type**:  GTX 1080 Ti\r\n**Nvidia Driver Version**:   \r\n**CUDA Version**:  10-2\r\n**CUDNN Version**: 8.0.1\r\n**Operating System + Version**: Ubuntu 16.04 \r\n**Python Version (if applicable)**:  3.5\r\n**TensorFlow Version (if applicable)**:  \r\n**PyTorch Version (if applicable)**: \r\n**Baremetal or Container (if container which image + tag)**: \r\n\r\n\r\n\r\n## Steps To Reproduce\r\n\r\n\r\n`cd /usr/src/tensorrt/samples/trtexec/`\r\n`sudo make`\r\n\r\nerror \r\n\r\n`\r\n../Makefile.config:10: CUDA_INSTALL_DIR variable is not specified, using /usr/local/cuda by default, use CUDA_INSTALL_DIR=<cuda_directory> to change.\r\n../Makefile.config:15: CUDNN_INSTALL_DIR variable is not specified, using /usr/local/cuda by default, use CUDNN_INSTALL_DIR=<cudnn_directory> to change.\r\n../Makefile.config:28: TRT_LIB_DIR is not specified, searching ../../lib, ../../lib, ../lib by default, use TRT_LIB_DIR=<trt_lib_directory> to change.\r\nif [ ! -d ../../bin/chobj/../common ]; then mkdir -p ../../bin/dchobj/../common; fi; :\r\nCompiling: trtexec.cpp\r\nIn file included from ../common/sampleEngines.h:24:0,\r\n                 from trtexec.cpp:41:\r\n/usr/local/include/NvOnnxParser.h:27:34: fatal error: NvOnnxParserTypedefs.h: No such file or directory\r\ncompilation terminated.\r\n../Makefile.config:312: recipe for target '../../bin/dchobj/trtexec.o' failed\r\nmake: *** [../../bin/dchobj/trtexec.o] Error 1\r\n` \r\n  Please include:\r\n  * Exact steps/commands to build your repro\r\n  * Exact steps/commands to run your repro\r\n  * Full traceback of errors encountered \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/853/comments",
    "author": "MuhammadAsadJaved",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-29T23:06:53Z",
        "body": "@MuhammadAsadJaved I just tried this with the latest 7.2 release of TensorRT and it worked as expected. Can you upgrade to this version? If not, I'll have to dig some more. The file `NvOnnxParserTypedefs.h` does not exist in 7.2 and is not referenced."
      },
      {
        "user": "MuhammadAsadJaved",
        "created_at": "2020-10-30T03:53:57Z",
        "body": "@mk-nvidia  I have installed 7.1 using the .tar file and it's more convenient. Now It can convert without any error.  Thanks. "
      }
    ]
  },
  {
    "number": 797,
    "title": "sampleSSD TensorRT 5 up to TensorRT7.1.3",
    "created_at": "2020-09-25T09:53:41Z",
    "closed_at": "2021-02-08T01:40:28Z",
    "labels": [
      "question",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/797",
    "body": "I used the sampleSSD code in TensorRT5,\r\nit can get right detections in trt5,\r\nbut get wrong detections in trt7.1.3 using the same code, why?\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/797/comments",
    "author": "HappyKerry",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-11-10T23:49:43Z",
        "body": "@HappyKerry can you please post details of the OS / platform / GPU you're using? Please also post details / logs of the wrong detections you've noticed."
      },
      {
        "user": "ttyio",
        "created_at": "2021-02-08T01:40:28Z",
        "body": "Closing since no activity for more than 3 months, please reopen if you still have questions, thanks!"
      }
    ]
  },
  {
    "number": 748,
    "title": "Jetson Xavier NX build",
    "created_at": "2020-08-17T08:56:57Z",
    "closed_at": "2020-10-13T19:55:04Z",
    "labels": [
      "question",
      "Module:Embedded",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/748",
    "body": "Did anyone manage to build the project on Xavier NX with Jetpack 4.4?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/748/comments",
    "author": "m-kzein",
    "comments": [
      {
        "user": "thunder95",
        "created_at": "2020-09-07T04:00:15Z",
        "body": "simillar question, i am using tensorrt 6,  so need to build the project  on xavier nx with jetpack4.3."
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-09-17T02:14:07Z",
        "body": "@MohammadKassemZein / @thunder95 are you hitting errors while building, or are you looking for directions to build on this platform? Are you cross compiling or building natively?"
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-13T19:55:04Z",
        "body": "Closing due to no response from user."
      }
    ]
  },
  {
    "number": 734,
    "title": "How does TensorRT optimize memory usage？",
    "created_at": "2020-08-03T10:29:14Z",
    "closed_at": "2021-04-19T00:41:02Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/734",
    "body": "Hi all\r\nI am doing a large model's inference using TenorRT. \r\nThe problem is that this model uses too much  GPU memory. When inference with batch_size=1, the gpu memory usage is 1.0+G...\r\nI think most of the memory space is used to store intermediate results for each layer. I did my best to fuse the layers, but there are still 110+layers.\r\nIs there any other ways to optimize memory usage?  \r\n\r\n1. Platform: Centos 7\r\n2. TensorRT: 7.0.0.11\r\n3. CUDA 9.0\r\n4. CUDNN: 7.6\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/734/comments",
    "author": "LitLeo",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-09-17T04:07:56Z",
        "body": "CC @kevinch-nv @rmccorm4 "
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-13T19:59:55Z",
        "body": "FYI we'll soon be publishing additional documentation on memory usage in TensorRT"
      },
      {
        "user": "piotyrus",
        "created_at": "2020-11-03T00:03:58Z",
        "body": "Hi LitLeo.\r\n\r\nYou can query the size of memory required for intermediate tensor computation using ICudaEngine::getDeviceMemorySize API. This memory is unique and private for each IExecutionContext, as opposed to ICudaEngine's Weights memory, which is constant and shared between IExecutionContextes.\r\n\r\nTensorRT tries to minimize the Activation memory by re-purposing the intermediate Activation memory that does not contribute to the final Network Output tensors. \r\n\r\nYou can build an engine trimmed to maxBatchSize == 1 in case of IMPLICIT_BATCH, or kOPT=kMIN=kMAX=1 in case of Dynamic Shapes, to minimize memory usage. \r\n\r\nI hope that helps. \r\n\r\nBest regards,\r\nPiotr Wojciechowski"
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-11-03T00:43:37Z",
        "body": "Thanks @piotyrus. @LitLeo any more questions?"
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-19T00:41:02Z",
        "body": "close since no activity for more than 3 weeks, please reopen if you still have question, thanks! "
      }
    ]
  },
  {
    "number": 714,
    "title": "找不到zlib库",
    "created_at": "2020-07-28T03:19:52Z",
    "closed_at": "2020-10-26T23:41:42Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/714",
    "body": "你好，我在windows10环境下通过cmake编译TensorRT5.1，提示找不到zlib库，但是我已经在thirdparty文件夹下添加了zlib库并且已经编译，请问这个问题如何解决呢？谢谢",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/714/comments",
    "author": "MrLee12138",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-13T20:19:32Z",
        "body": "@MrLee12138 TensorRT5.1 is pretty old now, please retry with the latest TensorRT version to see if this error still exists. Please also paste logs of all relevant error messages."
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-26T23:41:42Z",
        "body": "Closed due to no response."
      }
    ]
  },
  {
    "number": 713,
    "title": "[TensorRT] ERROR: ../rtSafe/cuda/cudaConvolutionRunner.cpp (303) - Cudnn Error in execute: 8 (CUDNN_STATUS_ EXECUTION_FAILED)",
    "created_at": "2020-07-28T02:10:00Z",
    "closed_at": "2021-07-07T02:45:09Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/713",
    "body": "## Description\r\n\r\nI tried to convert my docker container to a new GPU(RTX 2070 driver version:440.83) environment .When I tried to rebuild my engine for my new environment,I got this error.Same environment only different GPU,so what's wrong with me？\r\n\r\n[TensorRT] ERROR: ../rtSafe/cuda/cudaConvolutionRunner.cpp (303) - Cudnn Error in execute: 8 (CUDNN_STATUS_ EXECUTION_FAILED)\r\n[TensorRT] ERROR: ../rtSafe/cuda/cudaConvolutionRunner.cpp (303) - Cudnn Error in execute: 8 (CUDNN_STATUS_ EXECUTION_FAILED)\r\n## Environment\r\n\r\n**TensorRT Version**: 6\r\n**GPU Type**: Titan xp\r\n**Nvidia Driver Version**: \r\n**CUDA Version**: 9.0\r\n**CUDNN Version**: 7\r\n**Operating System + Version**: Ubuntu 16\r\n**Python Version (if applicable)**: 3.5\r\n**Baremetal or Container (if container which image + tag)**: docker：ismconnectiris/tensorrt4-cuda9_0-cudnn7-pycuda:latest\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/713/comments",
    "author": "ChaunceySun7",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-27T14:12:50Z",
        "body": "@ChaunceySun7 Please elaborate on what you did when you \"converted your docker container to a new GPU\". Did you copy the image to a new machine? Are the two GPUs installed in the same machine? Please elaborate."
      },
      {
        "user": "qianwangn",
        "created_at": "2020-11-10T03:13:43Z",
        "body": "same machine. different image, both build in same machine with Tesla T4.\r\none image is build with TensorRT6. Other is build with TensorRT7.\r\nI meet the same problem."
      },
      {
        "user": "ttyio",
        "created_at": "2021-05-27T06:08:38Z",
        "body": "@ChaunceySun7  @qianwangn  could you try latest 8.0? thanks"
      },
      {
        "user": "ttyio",
        "created_at": "2021-07-07T02:45:09Z",
        "body": "Close since no activity for more than 3 weeks, please reopen if you still have question, thanks"
      }
    ]
  },
  {
    "number": 693,
    "title": "How to build TensorRT parsers?",
    "created_at": "2020-07-17T02:44:31Z",
    "closed_at": "2020-08-17T17:02:16Z",
    "labels": [
      "question",
      "Module:OSS Build"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/693",
    "body": "## Description\r\n\r\nFailed to configure **TensorRT parsers** with **BUILD_PARSERS ON**, with the following **ERROR** messages:\r\n\r\n```\r\n Building for TensorRT version: 7.1.3, library version: 7\r\n\r\n Generated: ....../TensorRT/build/parsers/onnx/third_party/onnx/onnx/onnx_onnx2trt_onnx-ml.proto\r\n\r\n Generated: ....../TensorRT/build/parsers/onnx/third_party/onnx/onnx/onnx-operators_onnx2trt_onnx-ml.proto\r\n\r\n ERRORCannot find TensorRT library.\r\n\r\n CMake Error: The following variables are used in this project, but they are set to NOTFOUND.\r\n Please set them or make sure they are set and tested correctly in the CMake files:\r\n TENSORRT_LIBRARY_INFER\r\n     linked by target \"nvonnxparser_static\" in directory ....../TensorRT/parsers/onnx\r\n     linked by target \"nvonnxparser\" in directory ....../TensorRT/parsers/onnx\r\n TENSORRT_LIBRARY_INFER_PLUGIN\r\n     linked by target \"nvonnxparser_static\" in directory ....../TensorRT/parsers/onnx\r\n     linked by target \"nvonnxparser\" in directory ....../TensorRT/parsers/onnx\r\n TENSORRT_LIBRARY_MYELIN\r\n     linked by target \"nvonnxparser_static\" in directory ....../TensorRT/parsers/onnx\r\n     linked by target \"nvonnxparser\" in directory ....../TensorRT/parsers/onnx\r\n```\r\n\r\n\r\n## Environment\r\n\r\n**TensorRT Version**: 7.1.3.4 (current git)\r\n**GPU Type**: Geforce 1050 Ti\r\n**Nvidia Driver Version**: 450.57\r\n**CUDA Version**: 11.0\r\n**CUDNN Version**: 8.0.1 (beta)\r\n**Operating System + Version**: Ubuntu 20.04\r\n**Python Version (if applicable)**: 3.8.2\r\n**TensorFlow Version (if applicable)**: 2.2.0\r\n**PyTorch Version (if applicable)**: NOT Installed yet\r\n**Baremetal or Container (if container which image + tag)**:  N/A\r\n\r\n\r\n\r\nI can proceed with **BUILD_PARSERS OFF**. However, we prefer building out the **TensorRT Parsers**. Has anybody done that before?\r\n\r\nCheers\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/693/comments",
    "author": "peijason",
    "comments": [
      {
        "user": "rajeevsrao",
        "created_at": "2020-07-20T17:36:55Z",
        "body": "Do you have `libnvinfer.so.7` in your LD_LIBRARY_PATH?"
      },
      {
        "user": "kotzir",
        "created_at": "2020-08-16T01:58:35Z",
        "body": "I solved this error using these commands prior the build:\r\n\r\n```bash\r\nexport TRT_SOURCE=`pwd`\r\n\r\nexport TRT_RELEASE=`pwd`/TensorRT-7.1.3.4\r\n\r\nexport TENSORRT_LIBRARY_INFER=$TRT_RELEASE/targets/x86_64-linux-gnu/lib/libnvinfer.so.7\r\n\r\nexport TENSORRT_LIBRARY_INFER_PLUGIN=$TRT_RELEASE/targets/x86_64-linux-gnu/lib/libnvinfer_plugin.so.7\r\n\r\nexport TENSORRT_LIBRARY_MYELIN=$TRT_RELEASE/targets/x86_64-linux-gnu/lib/libmyelin.so\r\n\r\n# Edit: You will need to install python3.7, and python2 because currently TensorRT does not generate .whl files for python3.8, while some submodules require python2\r\n\r\nsudo add-apt-repository ppa:deadsnakes/ppa\r\n\r\nsudo apt-get update\r\n\r\nsudo apt-get install python3.7 python-dev\r\n\r\n# Also had issues with GCC/G++ version. GCC-7 and G++7 worked.\r\n\r\nsudo apt install cmake gcc-7 g++-7\r\n\r\n# In case Cuda host compiler reports unknown\r\n# Modify paths according to your cuda installation and versions\r\n\r\nsudo ln -s /usr/bin/gcc-7 /usr/lib/cuda-10.2/bin/gcc\r\n\r\nsudo ln -s /usr/bin/g++-7 /usr/lib/cuda-10.2/bin/g++\r\n\r\n# The command used for cmake\r\n# Modify according versions and GPU ARCH\r\ncmake .. -DTRT_LIB_DIR=$TRT_RELEASE/lib -DTRT_OUT_DIR=`pwd`/out -DCMAKE_CXX_COMPILER=/usr/bin/g++-7 -DCUDA_VERSION=\"10.2\" -DCUDNN_VERSION=\"7.6.5\" -DGPU_ARCHS=\"75\""
      },
      {
        "user": "peijason",
        "created_at": "2020-08-17T17:02:16Z",
        "body": "\r\nProblem solved.... Thank you ..."
      },
      {
        "user": "tugbakara",
        "created_at": "2023-08-22T11:37:54Z",
        "body": "I have the same issue and tried above solution but nothing changed, is there anyone who faced with this issue and solved it?"
      }
    ]
  },
  {
    "number": 678,
    "title": "protobuf version 3.0.0?",
    "created_at": "2020-07-10T04:17:58Z",
    "closed_at": "2021-02-08T01:48:15Z",
    "labels": [
      "question",
      "Module:OSS Build",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/678",
    "body": "plz check 3rd party version.\n\nI cant download protobuf 3.0.0 while cmake build",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/678/comments",
    "author": "y30n9ju1v",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-07-14T08:36:29Z",
        "body": "CC @rajeevsrao @kevinch-nv "
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-11-03T01:21:53Z",
        "body": "@iamyeongjunkim Can you please try the latest 7.2 release? I couldn't reproduce the problem at my end."
      },
      {
        "user": "ttyio",
        "created_at": "2021-02-08T01:48:15Z",
        "body": "Closing since no activity in this thread for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 662,
    "title": "No speedup on batch size larger than 1",
    "created_at": "2020-07-03T20:05:40Z",
    "closed_at": "2020-11-28T12:56:21Z",
    "labels": [
      "question",
      "Module:Performance",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/662",
    "body": "I've setup tensorRT to work on my yolov3 model where I'm running inference on each frame of a video stream. When I run with a single video stream and process each frame one at at time, I notice that the tensorRT version of the model gets a solid speedup over the regular model (going from 43 fps to 57 fps). However, when I try to process frames from larger batch sizes, like 5 different videos (and batch together 1 frame from each video into a batch size of 5), I don't see any speedup with tensorRT.\r\n\r\nI'm trying to understand why I see a speedup with batch size of 1 vs a batch size of 5. Any ideas why this might be happening or what I can look into for improving batch performance? I'm running with float 32 but would still expect a speedup for larger batch sizes for the tensorRT model.\r\n\r\nHere is an outline of my steps for creating and running the tensorRT engine:\r\n\r\n1. Export yolo model to onnx using `torch.onnx.export` with the dynamic batches param\r\n2. Convert onnx to tensorRT engine\r\n     - parse onnx model\r\n     - create a single optimization profile for a specific batch size: `profile.set_shape(inp.name, min=(batch_size, *shape), opt=(batch_size, *shape), max=(batch_size, *shape))`\r\n     - build engine\r\n3. Load the tensorRT engine + context\r\n     - select the right tensorRT engine based on input batch size to inference function\r\n     - Set the binding shape: `context.set_binding_shape(0, (BATCH_SIZE, 3, IMAGE_SIZE)) `\r\n     - Set the optimization profile: `context.active_optimization_profile = 0`\r\n\r\nNot sure if there's anything else I should be doing but these steps seem to be fine for handling inference with larger batch sizes. I'm running this on the latest TensorRT version. \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/662/comments",
    "author": "prathik-naidu",
    "comments": [
      {
        "user": "ttyio",
        "created_at": "2020-09-28T03:20:54Z",
        "body": "Hello @prathik-naidu , Thanks for reporting. \r\n\r\nWe will have larger workload when use larger batch size, the increased workload will potentially increase occupancy on each SM if the GPU is \"hungry\" in small batch size. However, if there is already enough workload to fully occupy the device for batch size one, then there is no more perf gain when increate the batch size.\r\n\r\nCould you provide nsightCompute dump for both batch size 1 and batch size 5 for further triage? thanks."
      },
      {
        "user": "zhangkui669",
        "created_at": "2020-11-13T07:26:09Z",
        "body": "please use command 'nvidia-smi' to check GPU-Util when increasing batch size, if gpu-util is already close to 100%, there is no speedup when use larger batch size.\r\nif gpu is not busy, less than 90%, maybe you should check your preprocess pipeline"
      },
      {
        "user": "ttyio",
        "created_at": "2020-11-28T12:56:21Z",
        "body": "closing since no response for a long time, please reopen if you still have question. thanks!"
      },
      {
        "user": "archwolf118",
        "created_at": "2021-04-02T02:12:50Z",
        "body": "context.set_binding_shape(0, (BATCH_SIZE, 3, IMAGE_SIZE)) is lead to the slow, I found If I remove the function with fix input shape, the speed is fast. But I don't know how to improve the speed with dynamic shape input."
      },
      {
        "user": "ttyio",
        "created_at": "2021-04-02T06:26:09Z",
        "body": "Hello @archwolf118 , the `set_binding_shape` is required for dynamic shape, because some backend runner require some setup when the shape in profile changed. Have you tried latest release?"
      },
      {
        "user": "kostiantynbrandbrigade",
        "created_at": "2023-04-20T03:14:29Z",
        "body": "@prathik-naidu have you found the reason why batch bigger than 1 did not give speedup?"
      }
    ]
  },
  {
    "number": 639,
    "title": "Refitting An Engine: weights count error",
    "created_at": "2020-06-24T09:24:18Z",
    "closed_at": "2021-01-15T08:20:11Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/639",
    "body": "I want to change weights and bias of all the convolution layer in the engine, but the weights count is reported wrong.\r\n\r\n```\r\n[F] [TRT] Assertion failed: newWeights.count == original.count\r\nC:\\source\\builder\\refit.cpp:596\r\nAborting...\r\n```\r\n\r\nIf only one layer's weights and bias are changed, the error message becomes as follows:\r\n\r\n```\r\n[05/24/2020-16:46:40] [F] [TRT] Assertion failed: tDims.nbDims > 3\r\nC:\\source\\rtSafe\\tensorLayout.cpp:19\r\nAborting...\r\n```\r\n\r\nThe 'refitter->getMissing' returns 0.\r\n\r\nThe code to change one layer's weights and bias(weights_shape=[64, 3, 3, 3], bias_shape=[64]) is:\r\n```\r\nWeights newWeights_tmp;\r\nnewWeights_tmp.count = 1728;\r\nfloat* newWeightsLocal = new float[1728];\r\nfor (int i = 0; i < 1728; i++)\r\n{\r\n\tnewWeightsLocal[i] = 0.0001 * i;\r\n}\r\nnewWeights_tmp.values = newWeightsLocal;\r\nnewWeights_tmp.type = DataType::kFLOAT;\r\n\r\nWeights newBias_tmp;\r\nnewBias_tmp.count = 64;\r\nfloat* newBiasLocal = new float[64];\r\nfor (int i = 0; i < 64; i++)\r\n{\r\n\tnewBiasLocal[i] = 0.0001 * i;\r\n}\r\n\r\nnewBias_tmp.values = newBiasLocal;\r\nnewBias_tmp.type = DataType::kFLOAT;\r\n\r\nIRefitter* refitter = createInferRefitter(*mEngine, gLogger);\r\nrefitter->setWeights(\"conv0_fwd\", WeightsRole::kKERNEL, newWeights_tmp);\r\nrefitter->setWeights(\"conv0_fwd\", WeightsRole::kBIAS, newBias_tmp);\r\n```\r\n\r\nWhat is the cause of the error?  How to define the new weights correctly? Anyone knows? ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/639/comments",
    "author": "ningningG",
    "comments": [
      {
        "user": "ys0232",
        "created_at": "2020-10-12T09:04:57Z",
        "body": "have same problem. Can solve this problem when update Tensorrt-7.0 to Tensorrt-7.1?"
      },
      {
        "user": "BowenFu",
        "created_at": "2020-10-19T05:59:33Z",
        "body": "> have same problem. Can solve this problem when update Tensorrt-7.0 to Tensorrt-7.1?\r\n\r\nThis should have been fixed in Tensorrt-7.1."
      },
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-20T00:25:47Z",
        "body": "@ningningG @ys0232 Can you please check if you still hit this problem with TensorRT-7.2 or TensorRT-7.1? Thanks"
      },
      {
        "user": "ttyio",
        "created_at": "2021-01-15T08:20:11Z",
        "body": "I will close since no response for more than 3 weeks, please reopen if you still have question, thanks!"
      }
    ]
  },
  {
    "number": 617,
    "title": "trtexec usage about --loadInputs",
    "created_at": "2020-06-15T07:16:27Z",
    "closed_at": "2020-06-23T22:06:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/617",
    "body": "Hi everyone,\r\nAbout trtexec, can any one show a sample usage about the param --loadInputs?\r\nI found someone use the command like:\r\ntrtexec --onnx=test.onnx --dumpOutput --batch=1 --safe --loadInputs=Input:input.txt\r\nbut he didn't explain where the input.txt is and its data format.\r\nThank you.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/617/comments",
    "author": "WangXuanBT",
    "comments": [
      {
        "user": "CallmeZhangChenchen",
        "created_at": "2020-06-16T05:15:52Z",
        "body": "This should be the value of the incoming network，\r\n\r\nLoad input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\r\n                              Input values spec ::= Ival[\",\"spec]\r\n                                           Ival ::= name\":\"file"
      },
      {
        "user": "rmccorm4",
        "created_at": "2020-06-23T22:05:51Z",
        "body": "Hi @WangXuanBT ,\r\n\r\nI believe `input.txt` is just a binary file of the raw data. Since `trtexec` is generally used for benchmarking, people generally just use random data. If you wanted to test on real data, I suggest writing a short script using the TensorRT C++/Python APIs directly - this way you can handle all of the necessary pre-processing and post-processing as need be."
      },
      {
        "user": "14asaf",
        "created_at": "2020-12-01T11:45:20Z",
        "body": "Is there new?"
      }
    ]
  },
  {
    "number": 590,
    "title": "Compile Error while make -j in CentOS?",
    "created_at": "2020-06-04T09:15:50Z",
    "closed_at": "2020-11-12T08:11:01Z",
    "labels": [
      "question",
      "Module:OSS Build",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/590",
    "body": "## Description\r\n`third_party/onnx/libonnx_proto.a(onnx_onnx2trt_onnx-ml.pb.cc.o): In function `onnx2trt_onnx::TypeProto::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0x3bb): undefined reference to `google_private::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'\r\nthird_party/onnx/libonnx_proto.a(onnx_onnx2trt_onnx-ml.pb.cc.o): In function `onnx2trt_onnx::GraphProto::InternalSerializeWithCachedSizesToArray(bool, unsigned char*) const':\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0x7bc): undefined reference to `google_private::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0x7d4): undefined reference to `google_private::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned char*)'\r\nthird_party/onnx/libonnx_proto.a(onnx_onnx2trt_onnx-ml.pb.cc.o): In function `onnx2trt_onnx::StringStringEntryProto::SerializeWithCachedSizes(google_private::protobuf::io::CodedOutputStream*) const':\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0xaf5): undefined reference to `google_private::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google_private::protobuf::io::CodedOutputStream*)'\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0xb0d): undefined reference to `google_private::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google_private::protobuf::io::CodedOutputStream*)'\r\nthird_party/onnx/libonnx_proto.a(onnx_onnx2trt_onnx-ml.pb.cc.o): In function `onnx2trt_onnx::TypeProto_Opaque::SerializeWithCachedSizes(google_private::protobuf::io::CodedOutputStream*) const':\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0xbf5): undefined reference to `google_private::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google_private::protobuf::io::CodedOutputStream*)'\r\nonnx_onnx2trt_onnx-ml.pb.cc:(.text+0xc0d): undefined reference to `google_private::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google_private::protobuf::io::CodedOutputStream*)'\r\n\r\n...\r\n\r\n`\r\nModelImporter.cpp:(.text+0x8ceb): undefined reference to `google_private::protobuf::TextFormat::PrintToString(google_private::protobuf::Message const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [parsers/onnx/getSupportedAPITest] Error 1\r\nmake[1]: *** [parsers/onnx/CMakeFiles/getSupportedAPITest.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n`\r\nI can not find the error reason.\r\n`",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/590/comments",
    "author": "warren-lei",
    "comments": [
      {
        "user": "mk-nvidia",
        "created_at": "2020-10-20T02:10:40Z",
        "body": "@warren-lei please add details of the version of TensorRT you used, CUDA versions, and whether you tried a native or container build."
      },
      {
        "user": "warren-lei",
        "created_at": "2020-10-28T02:53:38Z",
        "body": "> @warren-lei please add details of the version of TensorRT you used, CUDA versions, and whether you tried a native or container build.\r\n\r\nAfter commenting out onnx, the compilation is successful.\r\nThanks for your reply."
      },
      {
        "user": "ttyio",
        "created_at": "2020-11-12T08:11:01Z",
        "body": "Closing since the issue solved, thanks!"
      },
      {
        "user": "redlibo",
        "created_at": "2020-12-30T08:47:44Z",
        "body": "@ttyio  Same thing happens to me. \r\ncentos7.8 cuda10.2 cudnn8.0 tensorrt7.2.1\r\n\r\n[ 98%] Linking CXX executable getSupportedAPITest\r\n\r\ncd /usr/src/TensorRT/build/parsers/onnx && /usr/local/cmake/bin/cmake -E cmake_link_script CMakeFiles/getSupportedAPITest.dir/link.txt --verbose=1\r\n/usr/local/bin/g++  -Wno-deprecated-declarations  -DBUILD_SYSTEM=cmake_oss -Wall -Wno-deprecated-declarations -Wno-unused-function -O3 -DNDEBUG   CMakeFiles/getSupportedAPITest.dir/getSupportedAPITest.cpp.o CMakeFiles/getSupportedAPITest.dir/ModelImporter.cpp.o  -o getSupportedAPITest ../../third_party.protobuf/lib64/libprotobuf.a libnvonnxparser_static.a -lpthread -ldl third_party/onnx/libonnx_proto.a ../../third_party.protobuf/lib64/libprotobuf.a /usr/src/TensorRT-7.2.1.6/lib/libnvinfer.so /usr/src/TensorRT-7.2.1.6/lib/libnvinfer_plugin.so /usr/src/TensorRT-7.2.1.6/lib/libmyelin.so\r\n\r\nlibnvonnxparser_static.a(builtin_op_importers.cpp.o)：‘onnx2trt::(anonymous namespace)::importFallbackPluginImporter(onnx2trt::IImporterContext*, onnx2trt_onnx::NodeProto const&, std::vector<onnx2trt::TensorOrWeights, std::allocator<onnx2trt::TensorOrWeights> >&)’：\r\nbuiltin_op_importers.cpp:(.text+0x3e7c0)：undefined reference to ‘google_private::protobuf::internal::NameOfEnum[abi:cxx11](google_private::protobuf::EnumDescriptor const*, int)’\r\n\r\n...\r\n\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [parsers/onnx/CMakeFiles/getSupportedAPITest.dir/build.make:106：parsers/onnx/getSupportedAPITest] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:1223：parsers/onnx/CMakeFiles/getSupportedAPITest.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n"
      },
      {
        "user": "ttyio",
        "created_at": "2020-12-30T08:54:17Z",
        "body": "@redlibo Have you tried docker?"
      }
    ]
  },
  {
    "number": 547,
    "title": "Engine file created with Tensorrt7 is much bigger than Tensorrt4",
    "created_at": "2020-05-13T11:35:39Z",
    "closed_at": "2020-05-14T15:50:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/547",
    "body": "## Description\r\n\r\nI am planning to upgrade to tensorrt7, when I convert my caffe model to trt engine file, I found that is much bigger than before, about 340M  vs 800M, and curiously, when I set the max batchsize to 10, it is bigger than set the max batchsize to 50\r\n\r\n\r\n## Environment\r\n\r\n**TensorRT Version**: 7.0.1.1\r\n**GPU Type**: 1080TI\r\n**Nvidia Driver Version**: 390.77 \r\n**CUDA Version**: CUDA9.0\r\n**CUDNN Version**: 7.6.5\r\n**Operating System + Version**:  ubuntu 16.04\r\n**Python Version (if applicable)**: \r\n**TensorFlow Version (if applicable)**: \r\n**PyTorch Version (if applicable)**: \r\n**Baremetal or Container (if container which image + tag)**: \r\n\r\n\r\n## Relevant Files\r\n\r\n\r\n\r\n## Steps To Reproduce\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/547/comments",
    "author": "HaoLiuHust",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-05-14T15:50:22Z",
        "body": "Hi @HaoLiuHust, \r\n\r\nThe Caffe parser is deprecated as of TensorRT 7, please convert your model to ONNX and try the ONNX parser."
      },
      {
        "user": "HaoLiuHust",
        "created_at": "2020-05-18T11:57:29Z",
        "body": "I have tried using onnx to convert to trt model, it's much bigger than tensorr4"
      },
      {
        "user": "HaoLiuHust",
        "created_at": "2020-05-18T11:57:46Z",
        "body": "@rmccorm4 "
      },
      {
        "user": "HaoLiuHust",
        "created_at": "2020-05-18T12:59:44Z",
        "body": "and it's much slower when using trt model than caffe model in tensorrt7, nearly 8 times slower"
      },
      {
        "user": "HaoLiuHust",
        "created_at": "2020-05-18T13:09:49Z",
        "body": "and using onnx model directly also much faster than convert it to trt model"
      }
    ]
  },
  {
    "number": 515,
    "title": "How to protect the model file model.engine when deploying? How to encode the model or pack the model into .so or .bin?",
    "created_at": "2020-04-27T09:45:32Z",
    "closed_at": "2020-04-28T17:15:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/515",
    "body": "I want to use tensorRT to deploy a model for a company, but I want to protect my model, so how can I do that? Is is possible to encode the .engine file or just pack it into .bin?\r\nThanks in advance.\r\nBest, \r\nEdward",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/515/comments",
    "author": "Edwardmark",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-04-28T17:14:41Z",
        "body": "Hi @Edwardmark ,\r\n\r\n1. The serialized engine files are already expected to be serialized and stored in binary, not plaintext. \r\n2. When deserializing an engine, I don't believe there is currently an API that exposes how to examine the underlying model used in the engine.\r\n3. Even if you could examine the underlying model from the engine somehow, it is likely that various layer fusions and optimizations were made when building the engine that would make the resulting engine different from the original model."
      }
    ]
  },
  {
    "number": 463,
    "title": "TensorRT verison support torch verision list?",
    "created_at": "2020-03-31T06:48:32Z",
    "closed_at": "2020-04-06T00:29:18Z",
    "labels": [
      "question",
      "Module:ONNX"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/463",
    "body": "anyone can give a list about what verison of TensorRT support what version of torch\r\n\r\nI only know TRT6 did not support torch1.3 up",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/463/comments",
    "author": "Stephenfang51",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-04-06T00:29:18Z",
        "body": "Aside from the issue with TensorRT 6 + PyTorch 1.3 - I don't think there is an official support matrix. It's mostly a matter of the ONNX graphs generated by PyTorch being supported by TensorRT. This can depend on the graph/ops used in your PyTorch model as well.\r\n\r\nIf you find any versions that don't seem to work, please share here to help others as well."
      }
    ]
  },
  {
    "number": 449,
    "title": "Strange time cost for pytorch and tensorrt.",
    "created_at": "2020-03-25T14:50:43Z",
    "closed_at": "2021-05-26T09:52:18Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/449",
    "body": "Now I am working on CRNN with pytorch and want to use tensorrt7 for inference.\r\nI found one interesting thing. In pytroch, when I use cnn part only, the time cost is for example 37ms. But when I add one layer Bi-lstm to the network, the cnn time cost will be 13ms, and rnn part is about 30ms. So the total time cost is about 43ms. When I use tensorrt7, the cnn part is about 23ms, which is accelerated compared to only cnn part(37ms), and lstm part is about 16ms. So total time cost for tensorrt7 is 39ms. From the final result, we feel that tensorrt does not accelerate the inference. \r\nIt seems that pytorch has done some special accelerations in CRNN(cnn+lstm). So can anyone explain the reason for such phenomenon for me? Or anyone else has met the same problem?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/449/comments",
    "author": "ZimingLu",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-03-26T00:23:56Z",
        "body": "Hi @ZimingLu, have you tried to speed up the inference with TensorRT's FP16 builder flag?"
      },
      {
        "user": "Hubert2102",
        "created_at": "2020-03-26T02:05:43Z",
        "body": "@rmccorm4 [TensorRT] WARNING: Half2 support requested on hardware without native FP16 support, performance will be negatively affected.\r\nit costs the same time."
      },
      {
        "user": "ZimingLu",
        "created_at": "2020-03-26T03:05:21Z",
        "body": "> Hi @ZimingLu, have you tried to speed up the inference with TensorRT's FP16 builder flag?\r\n\r\nMy GPU can not support FP16. My question is, why is the accelereation ratio of tenssort smaller than pytorch. Maybe the pytorch's optimization of lstm is better than tensorrt?"
      },
      {
        "user": "ll490187880",
        "created_at": "2020-07-31T03:29:27Z",
        "body": "Have you solved the problem? I have also encountered this problem for crnn models. There is no acceleration for lstm module."
      },
      {
        "user": "shen865069799",
        "created_at": "2020-08-11T13:37:00Z",
        "body": "> Have you solved the problem? I have also encountered this problem for crnn models. There is no acceleration for lstm module.\r\n\r\nhi, i trained a crnn model with pytorch,and torch.onnx.export to onnx,when i use onnx model to infer with onnxruntime,the result is right,but when i use tensorrt parse onnx model, the result is wrong, and the output of tensorrt parsed is [-1,-1,-1], i think that this is due to lstm，should i modify crnn's lstm layer? i want to know how you can convert model and run with tensorr ,thx very much"
      },
      {
        "user": "kevinch-nv",
        "created_at": "2020-08-26T07:21:10Z",
        "body": "@ZimingLu @ll490187880 can you provide the models you are running?"
      },
      {
        "user": "quietsmile",
        "created_at": "2020-09-10T13:05:37Z",
        "body": "> > Have you solved the problem? I have also encountered this problem for crnn models. There is no acceleration for lstm module.\r\n> \r\n> hi, i trained a crnn model with pytorch,and torch.onnx.export to onnx,when i use onnx model to infer with onnxruntime,the result is right,but when i use tensorrt parse onnx model, the result is wrong, and the output of tensorrt parsed is [-1,-1,-1], i think that this is due to lstm，should i modify crnn's lstm layer? i want to know how you can convert model and run with tensorr ,thx very much\r\n\r\n请问你这个问题解决了么？最后加速明显么？谢谢"
      },
      {
        "user": "ttyio",
        "created_at": "2021-05-26T09:52:18Z",
        "body": "Closing since there is no repro provide for debug, please reopen if you still have question"
      }
    ]
  },
  {
    "number": 357,
    "title": "Is there any smart way to make one-hot encoding with python api?",
    "created_at": "2020-01-24T09:26:28Z",
    "closed_at": "2020-01-30T02:15:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/357",
    "body": "## Description\r\nI defined a seq2seq model using python api. My attention decoder needs to perform one-hot encoding every 1 timestep for next time-step input.\r\n\r\nInitially, I created an identity matrix tensor and used gather layer. The shape tensor was obtained using the topk layer (k = 1). However, since there are more than 7000 classes, memory error is occured when building the ICduaEngine. The workspace was set to 14GB. And the decoder's max timestep is 25.\r\n\r\nIs there any smart way to make one-hot encoding with tensorrt python api?\r\n[Please support assign/scatter operation... T^T]\r\n\r\nMy dumb code is following ...\r\n```python\r\ndef one_hot(net, prev_logit, i_mat):\r\n    # prev_logit => (-1, 7021, 1, 1)\r\n    topk_layer = net.add_topk(input=prev_logit,\r\n                              op=trt.TopKOperation.MAX,\r\n                              k=1,\r\n                              axes=(1 << 1))\r\n\r\n    reshape_pred_idx = net.add_shuffle(topk_layer.get_output(1))\r\n    reshape_pred_idx.reshape_dims = trt.Dims([-1])\r\n    one_hot = net.add_gather(input=i_mat,\r\n                             indices=reshape_pred_idx.get_output(0),\r\n                             axis=0)\r\n```\r\n\r\nP.S. I don't know why am I have to use axes like above. I thought \"axes=(1<<0)\" is right, but \"axes=(1<<1)\" works with a channel axis. (python api documentation said axes=(1<<0) is right)\r\n\r\nP.S.2 This is just my guess. So please let me know if I'm wrong.\r\n**Memory consumption for i_mat (my dumb one-hot encoding, num_class => 7021)**\r\n7021 x 7021 x 4 = 197177764 (byte) (== 188 MB)\r\n\r\nSince my decoder max_length==25, so memory consumption for i_mat is\r\n188 * 25 = 4700 MB\r\n\r\n\r\n\r\n## Environment\r\n\r\n**TensorRT Version**: 7.0.0.11\r\n**GPU Type**: T4\r\n**Nvidia Driver Version**: 440.33\r\n**CUDA Version**: 10.2\r\n**CUDNN Version**: 7.6.5\r\n**Operating System + Version**: ubuntu 18.04\r\n**Python Version (if applicable)**: 3.6\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/357/comments",
    "author": "dhkim0225",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-01-25T22:25:42Z",
        "body": "@pranavm-nvidia any idea on this?"
      },
      {
        "user": "dhkim0225",
        "created_at": "2020-01-26T01:32:04Z",
        "body": "Well, I'm currently try to make it with iloop layer + islicelayer +i concat layer.\r\nPlease give me opinions whether this is good or not.\r\n\r\nPseudocode\r\n```\r\nzeros_src = trt zeros constant (7021 length)\r\none_src = trt 1 constant (1 length)\r\nidx = one-hot idx\r\n\r\nres = list()\r\nFor i in batch:  # (iloop layer)\r\n    a = zeros_src[:idx-1]\r\n    b = one_src\r\n    c = zeros_src[idx:]\r\n    res.append(Concat([a, b, c]))\r\n```"
      },
      {
        "user": "dhkim0225",
        "created_at": "2020-01-26T07:35:11Z",
        "body": "Above idea doesn't work for me since I use dynamic batch size...  :(\r\n\r\nIloopLayer is for static loop similar to tf.while_loop as you know."
      },
      {
        "user": "pranavm-nvidia",
        "created_at": "2020-01-27T18:49:39Z",
        "body": "Might be able to use the `EQUAL` elementwise op for this. \r\nGenerate a sequence tensor:\r\n`x = [0, 1, 2, 3, 4, ..., N]`\r\nand then:\r\n`x == index` gives you a one-hot vector. \r\n\r\n\r\nAlso, out of curiosity, in your original implementation, why do you need one identity matrix for each timestep? Are they different?"
      },
      {
        "user": "dhkim0225",
        "created_at": "2020-01-28T01:38:33Z",
        "body": "@rmccorm4 Long time no see :)\r\n@pranavm-nvidia Thank you for your reply, and I'll try it now. \r\n\r\nAbout your question, actually I don't know the exact reason why it is used, since I didn't train this network, but I can guess about the reason.\r\n\r\nOne-hot vector goes into embedding layer after produce.\r\nMaybe, it's because the embedding layer is not well-trained in training logic. I think that performing one-hot encoding before embedding layer will have a better result if embedding layer makes 'dumb' manifold.\r\n\r\nIt's just my guess :/ . I'll ask to model-maker(?) about it after a week later (next meeting)"
      },
      {
        "user": "dhkim0225",
        "created_at": "2020-01-30T01:57:35Z",
        "body": "Sorry. This was a stupid question. \r\n\r\nThe embedding layer originally collects vectors via gather() function, which can be implemented simply using IGatherLayer in tensorrt. \r\n\r\nThe model I am switching to tensorrt implements the embedding layer using fullyconnected layer.\r\nSince one-hot encoding is done before forward fully_connected layer, I could solve the problem by adding IGatherLayer to the fully_connected weight constant.\r\n\r\nPseudocode\r\n```\r\nsrc = net.add_constant(weight=fc_weight)\r\nembedded = net.add_gather(fc_weight, indices)\r\n```\r\n\r\nSo, one-hot encoding was just for embedding layer."
      },
      {
        "user": "rmccorm4",
        "created_at": "2020-01-30T02:12:38Z",
        "body": "Glad it worked out @dhkim0225, please close if your issue is resolved :slightly_smiling_face: "
      }
    ]
  },
  {
    "number": 326,
    "title": "fcPlugin.h TensorRT Open Source Release/6.0 ：too old to let me know how to use in trt version7.0",
    "created_at": "2020-01-10T07:38:13Z",
    "closed_at": "2020-02-14T00:37:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/326",
    "body": "## fcPlugin.h, the file in sample/samplePlugin is still version6.0, while now trt is release version 7.0, could you please update that as latest in handbook?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/326/comments",
    "author": "sumagic",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2020-01-25T23:33:24Z",
        "body": "Hi @sumagic,\r\n\r\nDo you have a specific error that you can reproduce? Just because the file wasn't updated since TensorRT 6.0 doesn't necessarily mean that it won't work in TensorRT 7.0."
      },
      {
        "user": "rmccorm4",
        "created_at": "2020-02-14T00:37:19Z",
        "body": "Closing since no response."
      }
    ]
  },
  {
    "number": 287,
    "title": "How to use IOptimizationProfile::setDimensions in dynamic shape tensors?",
    "created_at": "2019-12-20T06:43:43Z",
    "closed_at": "2019-12-24T22:31:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/287",
    "body": "The docmeantation of `IOptimizationProfile::setDimensions` :\r\n> Set the minimum / optimum / maximum dimensions for a dynamic **input tensor**. This function must be called three times (for the minimum, optimum, and maximum) for any **network input tensor** ...\r\n\r\nMy questions are:\r\n1. it's for `network input tensor`, but is it also available for other network layer (not the input layer)?\r\n2. how to set a dynamic `output tensor`? \r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/287/comments",
    "author": "zeng-hello-world",
    "comments": [
      {
        "user": "gcp",
        "created_at": "2019-12-20T11:43:54Z",
        "body": "As for 2, if the output has a dynamic size, it is automatically computed from the input size and you don't need to set it manually. You can't call setDimensions on outputs (it will assert)."
      },
      {
        "user": "rmccorm4",
        "created_at": "2019-12-22T03:05:51Z",
        "body": "Hi @nan0755,\r\n\r\nFor (1), I'm not sure if it makes much sense to set an optimization profile for an intermediate layer of the network. After the input layer, the shapes are calculated by the intermediate layers/ops in your network. If for some reason you need/expect a certain shape at some point in the middle of the network, I would think you could calculate what the input shape has to be to achieve that.\r\n\r\nFor (2) it's basically the same answer as (1) and like @gcp said."
      }
    ]
  },
  {
    "number": 251,
    "title": "uff genererated for mask rcnn not working with trtexec",
    "created_at": "2019-12-04T13:30:53Z",
    "closed_at": "2019-12-29T15:30:13Z",
    "labels": [
      "question",
      "Module:Samples"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/251",
    "body": "I followed the steps mentioned in sampleUffMaskRCNN to generate the uff. The uff file was generated successfully with the following log.\r\n`The output names of tensorflow graph nodes: ['mrcnn_mask/Reshape_1']\r\nWARNING:tensorflow:From mrcnn_to_trt_single.py:137: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nWARNING:tensorflow:From /usr/lib/python3.6/dist-packages/uff/converters/tensorflow/conversion_helpers.py:231: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nNOTE: UFF has been tested with TensorFlow 1.12.0. Other versions are not guaranteed to work\r\nWARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.\r\nUFF Version 0.6.3\r\n[name: \"input_image\"\r\nop: \"Placeholder\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"shape\"\r\n  value {\r\n    shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n      dim {\r\n        size: 3\r\n      }\r\n      dim {\r\n        size: 1024\r\n      }\r\n      dim {\r\n        size: 1024\r\n      }\r\n    }\r\n  }\r\n}\r\n]\r\n\r\n\r\nUsing output node mrcnn_detection\r\nUsing output node mrcnn_mask/Sigmoid\r\nConverting to UFF graph\r\nWarning: No conversion function registered for layer: PyramidROIAlign_TRT yet.\r\nConverting roi_align_mask_trt as custom op: PyramidROIAlign_TRT\r\nWarning: No conversion function registered for layer: ResizeNearest_TRT yet.\r\nConverting fpn_p5upsampled as custom op: ResizeNearest_TRT\r\nWarning: No conversion function registered for layer: ResizeNearest_TRT yet.\r\nConverting fpn_p4upsampled as custom op: ResizeNearest_TRT\r\nWarning: No conversion function registered for layer: ResizeNearest_TRT yet.\r\nConverting fpn_p3upsampled as custom op: ResizeNearest_TRT\r\nWarning: No conversion function registered for layer: SpecialSlice_TRT yet.\r\nConverting mrcnn_detection_bboxes as custom op: SpecialSlice_TRT\r\nWarning: No conversion function registered for layer: DetectionLayer_TRT yet.\r\nConverting mrcnn_detection as custom op: DetectionLayer_TRT\r\nWarning: No conversion function registered for layer: ProposalLayer_TRT yet.\r\nConverting ROI as custom op: ProposalLayer_TRT\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: keepdims is ignored by the UFF Parser and defaults to True\r\nWarning: No conversion function registered for layer: PyramidROIAlign_TRT yet.\r\nConverting roi_align_classifier as custom op: PyramidROIAlign_TRT\r\nNo. nodes: 3044\r\nUFF Output written to ../mrcnn_nchw.uff\r\nUFF Text Output written to ../mrcnn_nchw.pbtxt\r\n`\r\n**But running this file using trtexec throws the following error. Is there any way to check if the file is converted to uff correctly? Any inputs to fix the following error?**\r\n\r\n`sudo /usr/src/tensorrt/bin/trtexec --avgRuns=100 --uffInput=input_image,3,1024,1024 --output=mrcnn_detection --fp16 --batch=1 --iterations=60 --output=cls_prob --useSpinWait --useDLACore=1 --allowGPUFallback --uff=/usr/src/tensorrt/samples/sampleUffMaskRCNN/converted/mrcnn_nchw.uff\r\n&&&& RUNNING TensorRT.trtexec # /usr/src/tensorrt/bin/trtexec --avgRuns=100 --uffInput=input_image,3,1024,1024 --output=mrcnn_detection --fp16 --batch=1 --iterations=60 --output=cls_prob --useSpinWait --useDLACore=1 --allowGPUFallback --uff=/usr/src/tensorrt/samples/sampleUffMaskRCNN/converted/mrcnn_nchw.uff\r\n[I] avgRuns: 100\r\n[I] uffInput: input_image,3,1024,1024\r\n[I] output: mrcnn_detection\r\n[I] fp16\r\n[I] batch: 1\r\n[I] iterations: 60\r\n[I] output: cls_prob\r\n[I] useSpinWait\r\n[I] useDLACore: 1\r\n[I] allowGPUFallback\r\n[I] uff: /usr/src/tensorrt/samples/sampleUffMaskRCNN/converted/mrcnn_nchw.uff\r\n[E] [TRT] UffParser: Unsupported number of graph 0\r\n[E] Engine could not be created\r\n[E] Engine could not be created\r\n&&&& FAILED TensorRT.trtexec # /usr/src/tensorrt/bin/trtexec --avgRuns=100 --uffInput=input_image,3,1024,1024 --output=mrcnn_detection --fp16 --batch=1 --iterations=60 --output=cls_prob --useSpinWait --useDLACore=1 --allowGPUFallback --uff=/usr/src/tensorrt/samples/sampleUffMaskRCNN/converted/mrcnn_nchw.uff\r\n`\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/251/comments",
    "author": "manogna-s",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2019-12-09T17:37:10Z",
        "body": "Hi @manga22,\r\n\r\nJust a guess, but you might need to specify the necessary plugins for the model with `--plugins <path/to/plugin.so file>`\r\n\r\n```\r\n$ trtexec --help\r\n...\r\n  --plugins                   Plugin library (.so) to load (can be specified multiple times)\r\n```"
      },
      {
        "user": "rmccorm4",
        "created_at": "2019-12-29T15:30:13Z",
        "body": "Closing, I'll reopen if you're still having issues"
      }
    ]
  },
  {
    "number": 236,
    "title": "The difference of plugin method between sample \"faster_rcnn\" and \"sample-ssd\"?",
    "created_at": "2019-11-22T07:02:33Z",
    "closed_at": "2020-11-03T23:13:41Z",
    "labels": [
      "question",
      "Module:Plugins",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/236",
    "body": "I find that tensorRT has already support the following layers to plugin: \r\n    RPROI_TRT\r\n    Normalize_TRT\r\n    PriorBox_TRT\r\n    GridAnchor_TRT\r\n    NMS_TRT\r\n    LReLU_TRT\r\n    Reorg_TRT\r\n    Region_TRT\r\n    Clip_TRT\r\n\r\nsample \"**faster-rcnn**\" have a RPROI layer to plugin, and sample \"**ssd**\" have three layers to plugin (e.g. Normalize). They both use _**initLibNvInferPlugins**_ to link the _libnvinfer_plugin.so_.\r\nHowever, why does \"**faster-rcnn**\" need a IPluginFactoryV2 class while \"**ssd**\" doesn't?\r\n\r\n(I also tried to cancel the IPluginFactorV2 in  \"**faster-rcnn**\" and modify the the layer type from \"IPlugin\" to \"RPROI\", but it doesn't work.)\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/236/comments",
    "author": "SSSSER1994",
    "comments": [
      {
        "user": "SSSSER1994",
        "created_at": "2019-11-22T09:00:26Z",
        "body": "Besides, I also find that in \"_**sampleuffssd**_\" , things is different again, it write a IPluginV2 class and  IPluginCreator class, then use the REGISTER_TENSORRT_PLUGIN function. So where is the _IPluginFactoryV2_ ? \r\nI think the official developer guide is not detailed enough, I can't make it clear."
      },
      {
        "user": "rajeevsrao",
        "created_at": "2020-11-03T23:13:09Z",
        "body": "@SSSSER1994 going forward, we will only support plugins that implement the IPluginV2Ext and derivative interfaces (IPluginV2DynamicExt, IPluginV2IOExt). With these interfaces, the implementation is only required to provide the Plugin and Creator class implementations. The REGISTER macro registers the plugin creator with the global plugin registry from with the corresponding plugin can be queried. Please see sampleUffPluginV2Ext for example and let me know if you have more questions."
      }
    ]
  },
  {
    "number": 121,
    "title": "Where is the MaskRCNN sample",
    "created_at": "2019-09-18T02:56:38Z",
    "closed_at": "2019-09-18T08:43:18Z",
    "labels": [
      "question",
      "Module:Samples"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/121",
    "body": "After downloaded tensorrt 6.0, I can not found any maskrcnn sample in tar file.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/121/comments",
    "author": "lucasjinreal",
    "comments": [
      {
        "user": "seanyuner",
        "created_at": "2019-09-18T08:24:25Z",
        "body": "TensorRT/samples/opensource/sampleUffMaskRCNN/"
      }
    ]
  },
  {
    "number": 120,
    "title": "Support TensorFlow Object Detection API",
    "created_at": "2019-09-17T19:26:29Z",
    "closed_at": "2020-11-03T22:12:27Z",
    "labels": [
      "Feature Request",
      "question",
      "Module:Plugins",
      "Module:Samples",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/120",
    "body": "Hello,\r\n\r\nI see TensorRT now supports Tensorflow Faster RCNN models. Do the Tensorflow object detection api models work with this?\r\n\r\nIf so, what would be a sample config file in order for the uff processor to work properly? \r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/120/comments",
    "author": "marvision-ai",
    "comments": [
      {
        "user": "niccoloraspa",
        "created_at": "2019-09-19T14:09:34Z",
        "body": "up"
      },
      {
        "user": "rajeevsrao",
        "created_at": "2019-10-15T15:39:07Z",
        "body": "We have a UFF model used for sampleFasterRCNN but that model is trained with NVidia TransferLearningTootlkit, and not with TensorFlow object detection APIs. The implementation of TLT and Tensorflow objection detection APIs are different according to the author of those plugins/sample, so I believe the plugins (with exception of CropAndResize) may not be useful in present form for TF models"
      },
      {
        "user": "rmccorm4",
        "created_at": "2019-10-31T06:25:32Z",
        "body": "@mbufi, If you find anything helpful or implement plugins to work with the Tensorflow Object Detection API, please feel free to share for those who run into a similar issue.\r\n\r\nClosing, but feel free to re-open if you have any more issues."
      },
      {
        "user": "marvision-ai",
        "created_at": "2019-11-21T21:31:38Z",
        "body": "> We have a UFF model used for sampleFasterRCNN but that model is trained with NVidia TransferLearningTootlkit, and not with TensorFlow object detection APIs. The implementation of TLT and Tensorflow objection detection APIs are different according to the author of those plugins/sample, so I believe the plugins (with exception of CropAndResize) may not be useful in present form for TF models\r\n\r\nI would like to reopen this issue now. I used TLT to train a Resnet50 Faster RCNN model. I now have both .etlt and .engine files for this model. **How would I go about using TensorRT python API to utilize this model?** \r\n\r\nMy thinking is that this should be no problem since both of the libraries are internal Nvidia libraries which will play nicely together. \r\n\r\nPlease let me know how to do this.\r\n\r\nThank you so much for the effort!"
      },
      {
        "user": "rmccorm4",
        "created_at": "2019-11-22T03:07:20Z",
        "body": "Hi @mbufi,\r\n\r\nI believe the `.etlt` file is meant to be used for Deepstream, and the `.engine` file can be used like any other TensorRT engine.\r\n\r\nYou can checkout the python samples that come with the TRT release from devzone/NGC container, and look at how they deserialize the engine file and perform inference on it. \r\n\r\nI think `/use/src/tensorrt/samples/python/common.py` has most of what you're looking for."
      },
      {
        "user": "marvision-ai",
        "created_at": "2019-11-22T14:21:08Z",
        "body": "Are there specific plugins I need to use? I ask this because the TLT documentation tells me I need to use specific pligins and patches to run the conversion and to run on deepstream. \r\nMaybe @rajeevsrao can comment on this to see if there is python api code that supports the FasterRCNN model from TLT.\r\n\r\nThanks!"
      },
      {
        "user": "WildChlamydia",
        "created_at": "2020-03-03T12:26:26Z",
        "body": "Is there any chance to convert TF OD API Rcnn to TRT engine?\r\nDid someone find a way?"
      },
      {
        "user": "marvision-ai",
        "created_at": "2020-03-03T14:16:37Z",
        "body": "No, still no way. It is best to us TLT instead to train your model."
      },
      {
        "user": "WildChlamydia",
        "created_at": "2020-03-03T14:51:34Z",
        "body": "@mbufi Thanks for answer.\r\nHeh, we all understand that goal of NVIDIA is to push us use their frameworks only, but it can not be even compared to Tensorflow code base =\\"
      },
      {
        "user": "rajeevsrao",
        "created_at": "2020-11-03T22:12:27Z",
        "body": "The goal for TensorRT is to support the TF-OD models through the ONNX workflow. We are in the process of improving our ONNX operator coverage and TF2ONNX converter support to support this. "
      }
    ]
  },
  {
    "number": 119,
    "title": "What is the best CPU acceleration solution for BERT now?",
    "created_at": "2019-09-17T08:58:42Z",
    "closed_at": "2019-10-29T06:45:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/119",
    "body": "Thank you very much.\r\nThank you very much.\r\nThank you very much.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/119/comments",
    "author": "guotong1988",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2019-10-29T04:45:25Z",
        "body": "Hi @guotong1988,\r\n\r\nTensorRT is used specifically for GPU acceleration at the moment, sorry I can't help further."
      }
    ]
  },
  {
    "number": 116,
    "title": "tf2.0 support for bert python demo",
    "created_at": "2019-09-13T01:12:32Z",
    "closed_at": "2019-10-31T06:28:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/116",
    "body": "As tensorflow 2.0 is the coming to shape, (rc1 just arrived), is it possible to have a version that work with tf2.0?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/116/comments",
    "author": "xiaoyunwu",
    "comments": [
      {
        "user": "rmccorm4",
        "created_at": "2019-10-31T06:28:59Z",
        "body": "Hi @xiaoyunwu,\r\n\r\nWe don't have support for TF2.0 in that demo right now, but hopefully in a future release.\r\n\r\nClosing for now."
      },
      {
        "user": "xiaoyunwu",
        "created_at": "2019-12-17T03:28:30Z",
        "body": "Now that tensorflow 2.0 is stable and 2.1 is also coming, do we have the plans to support it from tensorrt side?"
      }
    ]
  },
  {
    "number": 110,
    "title": "coule ICaffeParser parse a network without any parameters?",
    "created_at": "2019-09-07T07:43:24Z",
    "closed_at": "2019-09-09T03:21:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/110",
    "body": "Hi, I'm writing a bilinear sampler plugin for tensorrt. To test if it works properly, I create a prototxt with only the input layer and a bilinear sampler layer, so the created network has no parameters. \r\n\r\nI have tried to feed another caffemodel file together with this prototxt when parsing the network and get segmentation fault. May I ask whether the ICafferParser support parsing a network without any parameters?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/110/comments",
    "author": "yaoqi-zd",
    "comments": [
      {
        "user": "nuanxinqing",
        "created_at": "2019-09-25T07:38:00Z",
        "body": "hi I have the same question,have you solved the problem?"
      },
      {
        "user": "yaoqi-zd",
        "created_at": "2019-09-28T13:07:41Z",
        "body": "@nuanxinqing Yes, it can support parsing a network without any parameters. The segmentation fault problem encountered comes from the bug in my code."
      },
      {
        "user": "nuanxinqing",
        "created_at": "2019-09-28T13:30:20Z",
        "body": "me too------------------ 原始邮件 ------------------\r\n发件人: \"yaoqi_isee\"<notifications@github.com>\r\n发送时间: 2019年9月28日(星期六) 晚上9:07\r\n收件人: \"NVIDIA/TensorRT\"<TensorRT@noreply.github.com>;\r\n抄送: \"Shi Yating\"<824265060@qq.com>;\"Mention\"<mention@noreply.github.com>;\r\n主题: Re: [NVIDIA/TensorRT] coule ICaffeParser parse a network without anyparameters? (#110)\r\n\r\n\r\n\r\n@nuanxinqing Yes, it can support parsing a network without any parameters. The segmentation fault problem encountered comes from the bug in my code.\r\n \r\n&mdash;\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or mute the thread."
      }
    ]
  },
  {
    "number": 97,
    "title": "why no effect when I speed up backbone in faster-rcnn model and ssd model？",
    "created_at": "2019-08-24T04:20:31Z",
    "closed_at": "2019-10-31T20:38:03Z",
    "labels": [
      "question",
      "Module:Samples"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/97",
    "body": "I used tensorRT to speed up backbone in faster-rcnn model and ssd model.  but it just speed up a little.  10%\r\n\r\nhardware:1080Ti\r\ndetection framework: mmdetection\r\nmodel:ssd and faster-rcnn\r\ntensorRTconvert library : trt2pytorch",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/97/comments",
    "author": "linssswww",
    "comments": [
      {
        "user": "DilipSequeira",
        "created_at": "2019-09-05T01:49:45Z",
        "body": "How much time is being spent running the backbone in the original implementation?"
      },
      {
        "user": "rmccorm4",
        "created_at": "2019-10-31T20:38:03Z",
        "body": "Closing due to no response - Feel free to ask for it to be reopened if you have more details."
      }
    ]
  },
  {
    "number": 93,
    "title": "demo/BERT don't work on a Titan Xp Nvidia GPU with an error \"cuda failure 209“",
    "created_at": "2019-08-22T05:33:08Z",
    "closed_at": "2019-10-30T05:37:19Z",
    "labels": [
      "question",
      "Module:Samples"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/93",
    "body": "demo/BERT works well on a GTX 2080 GPU，but crush on Titan Xp。backtrace is as following:\r\n```\r\n(gdb) bt full\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n        set = {__val = {0, 1649481172316, 51539607584, 1, 4294967680, 1024, 1024, 140030812475818, 2097314, 4, 1, 8592, 5, 34817, 0, 1024}}\r\n        pid = <optimized out>\r\n        tid = <optimized out>\r\n        ret = <optimized out>\r\n#1  0x00007f5b76d1f801 in __GI_abort () at abort.c:79\r\n        save_stage = 1\r\n        act = {__sigaction_handler = {sa_handler = 0x7f5976d80000, sa_sigaction = 0x7f5976d80000}, sa_mask = {__val = {1432254016, 0, \r\n              140030812537859, 140030816073568, 140737474456976, 10, 140030812517195, 140030618316384, 140030615750539, 1, 140030618316384, 0, \r\n              140030816073568, 140030816055968, 140030812477565, 140027740815360}}, sa_flags = 1799564896, \r\n          sa_restorer = 0x7f5b6b432e60 <std::cout>}\r\n        sigs = {__val = {32, 0 <repeats 15 times>}}\r\n        __cnt = <optimized out>\r\n        __set = <optimized out>\r\n        __cnt = <optimized out>\r\n        __set = <optimized out>\r\n#2  0x00007f5b0fc1e22f in bert::launchTransQkv(CUstream_st*, int, int, int, int, float const*, float*) ()\r\n   from /data/nfsdata/TensorRT/demo/BERT/build/libbert_plugins.so\r\nNo symbol table info available.\r\n#3  0x00007f5b0fc23c2b in int bert::qkvToCtx<float>(cublasContext*&, int, int, int, int, float, float const*, float*, float*, float*, float*, CUstream_st*, int const*) () from /data/nfsdata/TensorRT/demo/BERT/build/libbert_plugins.so\r\nNo symbol table info available.\r\n#4  0x00007f5b0fc1ff04 in bert::QKVToContextPlugin::enqueue(int, void const* const*, void**, void*, CUstream_st*) ()\r\n   from /data/nfsdata/TensorRT/demo/BERT/build/libbert_plugins.so\r\nNo symbol table info available.\r\n#5  0x00007f5b6ceb1e80 in nvinfer1::cudnn::selectTactic(nvinfer1::rt::EngineBuildContext const&, nvinfer1::rt::Layer&, nvinfer1::builder::Node*)\r\n    () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#6  0x00007f5b6ce63df9 in nvinfer1::builder::buildSingleLayer(nvinfer1::rt::EngineBuildContext&, nvinfer1::builder::Node&, std::unordered_map<std::string, std::unique_ptr<nvinfer1::rt::Region, std::default_delete<nvinfer1::rt::Region> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<nvinfer1::rt::Region, std::default_delete<nvinfer1::rt::Region> > > > > const&, std::unordered_map<std::string, std::vector<float, std::allocator<float> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<float, std::allocator<float> > > > >*, bool) () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#7  0x00007f5b6ce66980 in nvinfer1::builder::EngineTacticSupply::getBestTactic(nvinfer1::builder::Node&, nvinfer1::query::Ports<nvinfer1::RegionFormatL> const&, bool) () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#8  0x00007f5b6ce3ba36 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#9  0x00007f5b6ce4018a in nvinfer1::builder::chooseFormatsAndTactics(nvinfer1::builder::Graph&, nvinfer1::builder::TacticSupply&, std::unordered_map<std::string, std::vector<float, std::allocator<float> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::vector<float, std::allocator<float> > > > >*, bool) () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#10 0x00007f5b6ce68834 in ?? () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#11 0x00007f5b6ce6d303 in nvinfer1::builder::buildEngine(nvinfer1::CudaEngineBuildConfig&, nvinfer1::rt::HardwareContext const&, nvinfer1::Network const&) () from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#12 0x00007f5b6ce577cd in nvinfer1::builder::Builder::buildCudaEngine(nvinfer1::INetworkDefinition&) ()\r\n   from /usr/lib/x86_64-linux-gnu/libnvinfer.so.5\r\nNo symbol table info available.\r\n#13 0x00007f5b75980023 in void pybind11::cpp_function::initialize<pybind11::cpp_function::initialize<nvinfer1::ICudaEngine*, nvinfer1::IBuilder, nvinfer1::INetworkDefinition&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, char const*>(nvinfer1::ICudaEngine* (nvinfer1::IBuilder::*)(nvinfer1::INetworkDefinition&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, char const* const&)::{lambda(nvinfer1::IBuilder*, nvinfer1::INetworkDefinition&)#1}, nvinfer1::ICudaEngine*, nvinfer1::IBuilder*, nvinfer1::INetworkDefinition&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, char const*>(pybind11::cpp_function::initialize<nvinfer1::ICudaEngine*, nvinfer1::IBuilder, nvinfer1::INetworkDefinition&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, char const*>(nvinfer1::ICudaEngine* (nvinfer1::IBuilder::*)(nvinfer1::INetworkDefinition&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, char const* const&)::{lambda(nvinfer1::IBuilder*, nvinfer1::INetworkDefinition&)#1}&&, nvinfer1::ICudaEngine* (*)(nvinfer1::IBuilder*, nvinfer1::INetworkDefinition&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, char const* const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) ()\r\n   from /usr/lib/python3.6/dist-packages/tensorrt/tensorrt.so\r\nNo symbol table info available.\r\n#14 0x00007f5b758cc08a in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()\r\n   from /usr/lib/python3.6/dist-packages/tensorrt/tensorrt.so\r\n```",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/93/comments",
    "author": "taomiao",
    "comments": [
      {
        "user": "DilipSequeira",
        "created_at": "2019-09-05T01:47:36Z",
        "body": "It looks like the BERT sample is set up to compile only for SM 7.0 and 7.5. I suggest adjusting CMAKE_CUDA_FLAGS in CMakeLists.txt.\r\n\r\nUpdate: the sample uses fp16 instrinsics, and thus will not work on a Titan Xp."
      }
    ]
  },
  {
    "number": 88,
    "title": "Will Fusion of Conv + LeakyReLU be supported?",
    "created_at": "2019-08-20T02:25:00Z",
    "closed_at": "2019-08-27T15:03:08Z",
    "labels": [
      "question",
      "Module:Performance"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/88",
    "body": "I notice LeakyReLU has been natively supported in lastest TensorRT. And my experiment showed a large performance gap between fused Conv+ReLU and currently separated Conv+LeakyReLU. So will **fusion of Conv + LeakyReLU** be supported in your timetable？",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/88/comments",
    "author": "yangfly",
    "comments": [
      {
        "user": "CMagWheels",
        "created_at": "2019-08-27T00:35:03Z",
        "body": "More fusions with GEMM's/Conv is being considered in future versions of TensorRT."
      },
      {
        "user": "yangfly",
        "created_at": "2019-08-27T15:03:01Z",
        "body": "Look forward to your good news. Thank you!"
      }
    ]
  },
  {
    "number": 84,
    "title": "does trt support the ceil_mode for maxpool2d?",
    "created_at": "2019-08-17T17:04:29Z",
    "closed_at": "2019-11-02T01:56:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/84",
    "body": "i convert the pytorch model which use `ceil_mode` in `maxpool2d` to onnx and it works. and when i convert onnx model to trt model, it still works. but when i run the trt model, it fails to get the right shape because the `ceil_mode` does not work correctly.\r\nany help is appreciated!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/84/comments",
    "author": "zimenglan-sysu-512",
    "comments": [
      {
        "user": "DilipSequeira",
        "created_at": "2019-09-05T01:41:26Z",
        "body": "It should correspond to TensorRT's PaddingMode::kEXPLICIT_ROUND_UP mode. \r\n\r\nSounds like a bug. It would be helpful if you can provide the input size you're using and the output size you're expecting - or better yet, the ONNX model."
      },
      {
        "user": "rmccorm4",
        "created_at": "2019-11-02T01:56:02Z",
        "body": "Closing since no response. Feel free to open a new issue or ask to have this one re-opened."
      }
    ]
  },
  {
    "number": 66,
    "title": "How to build it without downloading binary release tensorrt?",
    "created_at": "2019-08-05T15:48:11Z",
    "closed_at": "2019-08-30T11:42:57Z",
    "labels": [
      "question",
      "Module:OSS Build"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/66",
    "body": "Why cmake .. and make got this:\r\n\r\n\r\n```\r\nCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\r\nPlease set them or make sure they are set and tested correctly in the CMake files:\r\nTENSORRT_LIBRARY_INFER\r\n    linked by target \"nvonnxparser\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_plugin\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_runtime\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_static\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_runtime_static\" in directory /home/jintian/TensorRT/parsers/onnx\r\nTENSORRT_LIBRARY_INFER_PLUGIN\r\n    linked by target \"nvonnxparser\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_plugin\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_runtime\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_static\" in directory /home/x/TensorRT/parsers/onnx\r\n    linked by target \"nvonnxparser_runtime_static\" in directory /home/x/TensorRT/parsers/onnx\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/66/comments",
    "author": "lucasjinreal",
    "comments": [
      {
        "user": "joshuafc",
        "created_at": "2019-08-06T10:27:19Z",
        "body": "This repository is part of whole TensorRT!\r\nreadme said:\r\n\r\n> This repository contains the Open Source Software (OSS) components of NVIDIA TensorRT. Included are the sources for TensorRT plugins and parsers (Caffe and ONNX), as well as sample applications demonstrating usage and capabilities of the TensorRT platform.\r\n"
      },
      {
        "user": "narendasan",
        "created_at": "2019-08-06T20:14:22Z",
        "body": "You need at minimum `libnvinfer.so` from the binary release to build components in this repo. "
      },
      {
        "user": "qiuyang163",
        "created_at": "2019-08-30T11:42:26Z",
        "body": "UffParser hasn't open source!Oh my god!"
      }
    ]
  },
  {
    "number": 45,
    "title": "Could IBuilder create multi INetworkDefinition?",
    "created_at": "2019-07-23T07:53:26Z",
    "closed_at": "2019-11-02T22:52:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/45",
    "body": "Could IBuilder create multi INetworkDefinition? And could IBuilder build an ICudaEngine used an INetworkDefinition which createt by another IBuilder?\r\n\r\nIn the samples, it is one IBuilder corresponding one INetworkDefinition and one ICudaEngine,\r\nbut i used one IBuilder to create multi INetworkDefinition and deliver the networks to other builders, it seems to work fine.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/45/comments",
    "author": "lcxywfe",
    "comments": [
      {
        "user": "rajeevsrao",
        "created_at": "2019-07-24T18:29:50Z",
        "body": "Yes, you can generate multiple networks (and engines thereof) from a single builder object. Sharing a network across builders is not recommended."
      }
    ]
  },
  {
    "number": 11,
    "title": "do I have to implement IPluginFactoryV2 in plugin?",
    "created_at": "2019-06-21T02:27:36Z",
    "closed_at": "2019-10-31T16:59:47Z",
    "labels": [
      "question",
      "Module:Plugins"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/11",
    "body": "I have seen the how to register plugin in the code, but If I have to implement IPluginFactoryV2 ? I haven't seen in the code, and just call the function 'initLibNvInferPlugins()' ? It can auto find plugin which I have registered or I have to set parser->setPluginFactoryV2(&pluginFactoryV2);\r\nIt confused me.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/11/comments",
    "author": "clancylian",
    "comments": [
      {
        "user": "nfeng0105",
        "created_at": "2019-06-21T06:36:46Z",
        "body": "@clancylian PluginFactory and PluginRegistry are two different manners to implement plugin layer. PlugnFactory is more friend to caffe plugin layer, while PluginRegistry is new introduced type for Uff and ONNX. "
      }
    ]
  },
  {
    "number": 5,
    "title": "where createUffParser implement",
    "created_at": "2019-06-18T10:00:21Z",
    "closed_at": "2019-06-25T05:13:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/5",
    "body": "i  cant find where is createUffParser implement",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/5/comments",
    "author": "xuansan915",
    "comments": [
      {
        "user": "xuansan915",
        "created_at": "2019-06-18T10:01:19Z",
        "body": "is all code there ?"
      },
      {
        "user": "ArmageddonKnight",
        "created_at": "2019-06-18T21:05:57Z",
        "body": "Probably related: #3 "
      },
      {
        "user": "rajeevsrao",
        "created_at": "2019-06-20T01:21:22Z",
        "body": "We will not be releasing the UFF parser sources in this release. Please use the UFF parser from the TensorRT binary installation."
      }
    ]
  },
  {
    "number": 3,
    "title": "NO KERNEL? NO FUSED OPTIMIZATION?",
    "created_at": "2019-06-18T02:57:40Z",
    "closed_at": "2019-06-18T05:08:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT/issues/3",
    "body": "",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT/issues/3/comments",
    "author": "lcxywfe",
    "comments": [
      {
        "user": "BowieHsu",
        "created_at": "2019-06-18T04:48:55Z",
        "body": "\"Included are the sources for TensorRT plugins and parsers (Caffe and ONNX)\", yep, you are right, no kernel, no fused optimization."
      },
      {
        "user": "lcxywfe",
        "created_at": "2019-06-18T05:08:47Z",
        "body": "> \"Included are the sources for TensorRT plugins and parsers (Caffe and ONNX)\", yep, you are right, no kernel, no fused optimization.\r\n\r\nT_T"
      }
    ]
  }
]