[
  {
    "number": 1919,
    "title": "Add time frames",
    "created_at": "2024-03-04T13:30:38Z",
    "closed_at": "2024-03-10T14:01:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1919",
    "body": "Tell me, in the example it is indicated\r\n./main -m models/ggml-large-v1.bin -l ru --no-timestamps -f ~/output.wav -of output -otxt\r\nThis is to remove the time frames, but how can we make sure they still exist? If I delete --no-timestamps, then they are still not there? How can I make them exist?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1919/comments",
    "author": "DittmerOk",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2024-03-05T16:04:24Z",
        "body": "In the function located in main.cpp, you'll notice that when we direct the output to a text file, we only include the textual content, excluding any special tokens. If you wish to include special tokens in the text file output, please make use of the additional option `--print-special`"
      },
      {
        "user": "DittmerOk",
        "created_at": "2024-03-10T14:01:53Z",
        "body": "yes, thanks. it's work"
      }
    ]
  },
  {
    "number": 1900,
    "title": "Add support for 1.5, 2 and 3 bit quantization",
    "created_at": "2024-02-24T13:14:46Z",
    "closed_at": "2024-02-25T14:23:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1900",
    "body": "Ggml and llama.cpp has recently added support for 1.5-, 2- and 3-bit quantization. So, @ggerganov please add these quantization support to whisper.cpp as well. Will be useful for running the whisper-large models with limited RAM.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1900/comments",
    "author": "bil-ash",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2024-02-24T16:52:28Z",
        "body": "> Ggml and llama.cpp has recently added support for 1.5-, 2- and 3-bit quantization.\r\n\r\nWhisper.cpp already supports `Q2_K` and `Q3_K`. If you give it a try, you'll notice that the quality of the results has significantly deteriorated. I can not see any reasons we need to implement `1.5bit` quantization."
      },
      {
        "user": "bil-ash",
        "created_at": "2024-02-24T20:06:33Z",
        "body": "Would be nice if this is added to the docs"
      },
      {
        "user": "bil-ash",
        "created_at": "2024-02-25T14:23:44Z",
        "body": "Yeah, ran the inference on q2_k and realized that inference is horrible. So closing the issue"
      }
    ]
  },
  {
    "number": 1844,
    "title": "Improvement: Wanting to change 16khz requirement to 8khz (or Xkhz ideally)",
    "created_at": "2024-02-08T05:40:57Z",
    "closed_at": "2024-02-10T04:51:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1844",
    "body": "\r\nCurrently main program requires\r\nRIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 16000 Hz\r\n\r\nbut all my audios are created as \r\nRIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 8000 Hz\r\n\r\nSure I can convert them using `sox` or `ffmpeg` but would be best if it I can just recompile `main` program to use 8khz. Can anyone provide any guidance and what modifications are required? I'm thinking this should be easy am I wrong? \r\n\r\nLooking at examples/common.cpp  I see \r\n#define COMMON_SAMPLE_RATE 16000\r\n\r\ntried changing to \r\n#define COMMON_SAMPLE_RATE 8000\r\nbut then I get random words in output. \r\n\r\nTried searching for  COMMON_SAMPLE_RATE din't find anything. Kinda of weird. Is the 16khz hardcoded and that define is just a check? or how does it work? Any guidance would be appreciated. ",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1844/comments",
    "author": "lfmunoz",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2024-02-09T18:40:16Z",
        "body": "It's not feasible because the model was trained using data at a 16KHz sampling rate. When you input audio at 8KHz, it doesn't match the training data's distribution, leading to considerable distortion in the output. AI learns by identifying statistically significant information within the provided dataset. "
      },
      {
        "user": "lfmunoz",
        "created_at": "2024-02-10T04:51:26Z",
        "body": "Thanks "
      }
    ]
  },
  {
    "number": 1684,
    "title": "Can't compile with cuBLAS with non-standard CUDA setup on linux",
    "created_at": "2023-12-23T15:24:00Z",
    "closed_at": "2023-12-27T20:36:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1684",
    "body": "I've installed CUDA packages from the Negativo17 repo on a Fedora 38 system (because I'm using their nvidia-driver on this Lenovo P50 laptop with its discrete Nvidia GPU, which used to be tricky to setup when I first got this machine, and this was the driver that worked), and I'm having problems compiling whisper.cpp with cuBLAS enabled. I get a barrage of weird errors like \r\n\r\n```\r\n/usr/include/cuda/std/detail/libcxx/include/__cuda/atomic_prelude.h(29): error: identifier \"ATOMIC_LLONG_LOCK_FREE\" is undefined\r\n      static_assert(ATOMIC_LLONG_LOCK_FREE == 2, \"\");\r\n                    ^\r\n\r\n/usr/include/cuda/std/detail/libcxx/include/__cuda/atomic_prelude.h(30): error: identifier \"ATOMIC_POINTER_LOCK_FREE\" is undefined\r\n      static_assert(ATOMIC_POINTER_LOCK_FREE == 2, \"\");\r\n                    ^\r\n\r\nggml-cuda.cu(6519): error: namespace \"std\" has no member \"atomic_flag\"\r\n      std::atomic_flag& lock;\r\n           ^\r\n\r\nggml-cuda.cu(6520): error: namespace \"std\" has no member \"atomic_flag\"\r\n      scoped_spin_lock(std::atomic_flag& lock) : lock(lock) {\r\n```\r\n\r\nThe problem seems to be that these packages install the CUDA headers and libraries under /usr/ instead of /usr/local/cuda (like the official cuda-toolkit from Nvidia does). Apart from whisper.cpp (and llama.cpp for that matter) the setup seems to be functional, e.g. I could compile and run the examples from cuda-samples.\r\n\r\nDoes anyone have any ideas or experience on how to fix this?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1684/comments",
    "author": "pjuhasz",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-12-23T16:14:31Z",
        "body": "> The problem seems to be that these packages install the CUDA headers and libraries under /usr/ instead of /usr/local/cuda (like the official cuda-toolkit from Nvidia does). \r\n\r\nTry `CMake` and see if that helps.\r\n\r\n`cmake -S [source] -B [build] -DWHISPER_CUBLAS=1`\r\n\r\n`cmake --build [build] --config release`\r\n"
      },
      {
        "user": "pjuhasz",
        "created_at": "2023-12-23T17:49:28Z",
        "body": "It didn't help:\r\n```\r\nCMake Error at /usr/share/cmake/Modules/CMakeDetermineCUDACompiler.cmake:227 (message):\r\n  Couldn't find CUDA library root.\r\n```\r\n"
      },
      {
        "user": "pjuhasz",
        "created_at": "2023-12-27T20:36:32Z",
        "body": "After reinstalling the machine with the official Nvidia driver packages, the errors went away and I could compile the program. So, PEBKAC, I guess."
      },
      {
        "user": "stevestorey",
        "created_at": "2024-01-15T15:46:20Z",
        "body": "Just for anyone else who gets here prior to something being changed in the negativo17 packaging - the current workaround is to comment the line `INCLUDES += \"-I/usr/include/cuda\"` from the `/usr/bin/nvcc.profile` file after which compilation works."
      }
    ]
  },
  {
    "number": 1663,
    "title": "cannot make talk.llama on mac",
    "created_at": "2023-12-20T18:22:59Z",
    "closed_at": "2023-12-29T15:43:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1663",
    "body": "```\r\n➜  talk-llama git:(master) brew install sdl2\r\nWarning: sdl2 2.28.5 is already installed and up-to-date.\r\nTo reinstall 2.28.5, run:\r\n  brew reinstall sdl2\r\n➜  talk-llama git:(master) make talk-llama\r\nc++     talk-llama.cpp   -o talk-llama\r\ntalk-llama.cpp:4:10: fatal error: 'common-sdl.h' file not found\r\n#include \"common-sdl.h\"\r\n         ^~~~~~~~~~~~~~\r\n1 error generated.\r\nmake: *** [talk-llama] Error 1\r\n```",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1663/comments",
    "author": "dssjon",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-12-20T23:54:24Z",
        "body": "Ensure you are at the root of the repo when running `make talk-llama`"
      }
    ]
  },
  {
    "number": 1657,
    "title": "Timestamp accuracy needs to be improved ",
    "created_at": "2023-12-19T19:42:01Z",
    "closed_at": "2024-03-20T16:25:27Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1657",
    "body": "Timestamp accuracy is very problematic. Often subtitles start showing too early. When one subtitle ends, the next subtitle is shown immediately instead of disappearing and waiting for the speech. This is because there is no time gap between the two subtitles. The other whisper modifications (whisperX, faster-whisper, whisper-timestamped) do not have this problem. Only whisper.ccp has this problem. This needs to be fixed.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1657/comments",
    "author": "erturkdotgg",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-12-20T13:43:30Z",
        "body": "See #1485 "
      },
      {
        "user": "erturkdotgg",
        "created_at": "2023-12-20T20:11:54Z",
        "body": "> See #1485\r\n\r\nIs it possible to use the changes in that topic? Like beta? Or is it just too early?"
      },
      {
        "user": "bmurray",
        "created_at": "2023-12-21T08:05:41Z",
        "body": "It's still too early to use it in production IMHO. It's close but theres some issues that need to get sorted."
      },
      {
        "user": "YeDaxia",
        "created_at": "2024-03-20T16:25:59Z",
        "body": "感谢您的来信，我会尽快阅读的。\nThanks, I would read it as soon as possible~"
      }
    ]
  },
  {
    "number": 1629,
    "title": "can't compile linux with cuda installed / working",
    "created_at": "2023-12-12T14:43:19Z",
    "closed_at": "2024-01-15T12:48:17Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1629",
    "body": "```\r\n nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n```\r\n\r\n```\r\n whisper.cpp git:(master) dpkg -l | grep cuda\r\nii  libcudart11.0:amd64                        11.5.117~11.5.1-1ubuntu1                amd64        NVIDIA CUDA Runtime Library\r\nii  nvidia-cuda-dev:amd64                      11.5.1-1ubuntu1                         amd64        NVIDIA CUDA development files\r\nii  nvidia-cuda-gdb                            11.5.114~11.5.1-1ubuntu1                amd64        NVIDIA CUDA Debugger (GDB)\r\nii  nvidia-cuda-toolkit                        11.5.1-1ubuntu1                         amd64        NVIDIA CUDA development toolkit\r\nii  nvidia-cuda-toolkit-doc                    11.5.1-1ubuntu1                         all          NVIDIA CUDA and OpenCL documentation\r\nii  nvidia-cuda-toolkit-gcc                    11.5.1-1ubuntu1                         amd64        NVIDIA CUDA development toolkit (GCC compatibility)\r\n```\r\n\r\n```\r\n whisper.cpp git:(master) nvidia-smi\r\nTue Dec 12 22:42:40 2023\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   37C    P8              12W / 320W |    261MiB / 16376MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|    0   N/A  N/A      1871      G   /usr/lib/xorg/Xorg                          239MiB |\r\n|    0   N/A  N/A      2008      G   /usr/bin/gnome-shell                         12MiB |\r\n+---------------------------------------------------------------------------------------+\r\n\r\n```\r\n\r\n```\r\n(base) ➜  whisper.cpp git:(master) make\r\nI whisper.cpp build info:\r\nI UNAME_S:  Linux\r\nI UNAME_P:  x86_64\r\nI UNAME_M:  x86_64\r\nI CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\r\nI CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\r\nI LDFLAGS:\r\nI CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nI CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n\r\ng++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/main/main.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o main\r\n/usr/bin/ld: ggml.o: in function `ggml_init':\r\nggml.c:(.text+0x1916a): undefined reference to `ggml_init_cublas'\r\n/usr/bin/ld: ggml.o: in function `ggml_get_n_tasks':\r\nggml.c:(.text+0x1b1ea): undefined reference to `ggml_cuda_can_mul_mat'\r\n/usr/bin/ld: ggml.o: in function `ggml_graph_compute_thread':\r\nggml.c:(.text+0x2c9f4): undefined reference to `ggml_cuda_compute_forward'\r\n/usr/bin/ld: ggml.c:(.text+0x2ca47): undefined reference to `ggml_cuda_compute_forward'\r\n/usr/bin/ld: ggml.c:(.text+0x2ca8b): undefined reference to `ggml_cuda_compute_forward'\r\n/usr/bin/ld: ggml.c:(.text+0x2cb41): undefined reference to `ggml_cuda_compute_forward'\r\n/usr/bin/ld: ggml.c:(.text+0x2cb8a): undefined reference to `ggml_cuda_compute_forward'\r\n/usr/bin/ld: ggml.o: in function `ggml_graph_plan':\r\nggml.c:(.text+0x30a0c): undefined reference to `ggml_cuda_can_mul_mat'\r\n/usr/bin/ld: whisper.o: in function `whisper_init_state':\r\nwhisper.cpp:(.text+0xc8ad): undefined reference to `ggml_cublas_loaded'\r\n/usr/bin/ld: whisper.cpp:(.text+0xd1d5): undefined reference to `ggml_backend_cuda_init'\r\n/usr/bin/ld: whisper.o: in function `whisper_model_load(whisper_model_loader*, whisper_context&)':\r\nwhisper.cpp:(.text+0x19dff): undefined reference to `ggml_cublas_loaded'\r\n/usr/bin/ld: whisper.cpp:(.text+0x19e2b): undefined reference to `ggml_backend_cuda_init'\r\n/usr/bin/ld: ggml-backend.o: in function `ggml_backend_reg_get_count':\r\nggml-backend.c:(.text+0x1051): undefined reference to `ggml_backend_cuda_reg_devices'\r\n/usr/bin/ld: ggml-backend.o: in function `ggml_backend_reg_find_by_name':\r\nggml-backend.c:(.text+0x10af): undefined reference to `ggml_backend_cuda_reg_devices'\r\n/usr/bin/ld: ggml-backend.o: in function `ggml_backend_reg_get_name':\r\nggml-backend.c:(.text+0x1156): undefined reference to `ggml_backend_cuda_reg_devices'\r\n/usr/bin/ld: ggml-backend.o: in function `ggml_backend_reg_get_default_buffer_type':\r\nggml-backend.c:(.text+0x1206): undefined reference to `ggml_backend_cuda_reg_devices'\r\n/usr/bin/ld: ggml-backend.o: in function `ggml_backend_reg_alloc_buffer':\r\nggml-backend.c:(.text+0x12be): undefined reference to `ggml_backend_cuda_reg_devices'\r\n/usr/bin/ld: ggml-backend.o:ggml-backend.c:(.text+0x137e): more undefined references to `ggml_backend_cuda_reg_devices' follow\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [Makefile:353: main] Error 1\r\n(base) ➜  whisper.cpp git:(master)\r\n```\r\n\r\nAnything I am missing? I have no issue compiling / running llama.cpp",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1629/comments",
    "author": "abretonc7s",
    "comments": [
      {
        "user": "JJones780",
        "created_at": "2023-12-15T10:43:09Z",
        "body": "You need to run \r\n`WHISPER_CUBLAS=1 make stream `\r\n\r\nthis will modify the gcc command so that it includes` ggml-cuda.o `and also  `-lcudart -lcublas  `\r\n"
      }
    ]
  },
  {
    "number": 1624,
    "title": "cURL with PHP results different than command line",
    "created_at": "2023-12-11T19:38:33Z",
    "closed_at": "2023-12-12T18:41:56Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1624",
    "body": "I don t know if it is php or some other issue, and hope someone helps me,\r\n\r\nI have server with following command\r\n\r\n```./server --host 0.0.0.0 --port 7863 -t 24 -l auto -m models/ggml-medium.bin --convert```\r\n\r\nif i call it with  command line, \r\n```curl xxx.xxx.xxx.xxx:7863/inference -H \"Content-Type: multipart/form-data\" -F file=\"@./uploads/out.wav\" -F temperature=\"0.2\" -F response-format=\"json\"```\r\n\r\nif works perfectly.\r\n\r\nBut when I call from php with the following code\r\n```\r\n   $url = \"xxx.xxx.xxx.xxx:7863/inference\";\r\n   $cFile = \"@./uploads/out.wav\";\t\t         \r\n   $postData = array(\r\n\t'file'=> $cFile,\r\n\t'temperature'=>'0.2',\r\n\t'response-format' => 'json'\r\n   );\r\n\r\n   ch = curl_init();\r\n   curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\r\n   curl_setopt($ch, CURLOPT_URL, $url);\r\n   curl_setopt($ch, CURLOPT_POST, true);\r\n   curl_setopt($ch, CURLOPT_POSTFIELDS, $postData);\t\r\n   $response = curl_exec($ch);\r\n```\r\n\r\nthen I get {\"error\":\"FFmpeg conversion failed.\"}\r\n\r\nAny help appreciated.\r\n\r\n\r\nAdditional Note : I got following on server side when calling from php\r\n```\r\nwhisper_server_temp_file.wav: Invalid data found when processing input\r\n```",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1624/comments",
    "author": "erturkkadir",
    "comments": [
      {
        "user": "felrock",
        "created_at": "2023-12-12T08:15:41Z",
        "body": "Could the issue be that you have --convert and still sending a .wav file? **{\"error\":\"FFmpeg conversion failed.\"}** means ffmpeg failed with the convert command."
      },
      {
        "user": "erturkkadir",
        "created_at": "2023-12-12T16:48:43Z",
        "body": "Thanks @felrock  for your reply\r\n\r\nI send same file with cURL command line, it converts and generates result."
      },
      {
        "user": "erturkkadir",
        "created_at": "2023-12-12T18:41:56Z",
        "body": "Finally I got it working with following (the issue with PHP 5.5+ and cUrl)\r\n\r\n```\r\n$data = array(\r\n    'file'=> curl_file_create(realpath('uploads/out.wav')),\r\n    'temperature'=>'0.2',\r\n    'response-format' => 'json'\r\n);\r\n\t\t\r\n\t\t\r\n$ch = curl_init($url);\r\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\r\ncurl_setopt($ch, CURLOPT_POST, true);\t\t\r\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $data);\t\r\ncurl_setopt($ch, CURLOPT_USERAGENT, 'Mozilla/4.0 (compatible; MSIE 5.01; Windows NT 5.0'); \r\n$response = curl_exec($ch);\r\n```\r\n\r\nI will close it."
      }
    ]
  },
  {
    "number": 1611,
    "title": "cannot build talk-llama ",
    "created_at": "2023-12-09T15:26:19Z",
    "closed_at": "2023-12-16T16:47:34Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1611",
    "body": "i built whisper.cpp via cmake and tried to run\r\nE > Repos > whisper.cpp :  cmake -DWHISPER_CUBLAS=1 examples/talk-llama \r\ni got the following error:\r\nCMake Error at CMakeLists.txt:30 (include):\r\n  include could not find requested file\r\nDefaultTargetOptions\r\n\r\n\r\nCMake Error at main/CMakeLists.txt:4 (include):\r\ninclude could not find requested file:\r\nDefaultTargetOptions\r\n\r\n\r\nCMake Error at server/CMakeLists.txt:4 (include):\r\ninclude could not find requested file:\r\nDefaultTargetOptions\r\n\r\n\r\nCMake Error at bench/CMakeLists.txt:4 (include):\r\n  include could not find requested file:\r\n\r\nDefaultTargetOptions\r\n\r\n\r\nCMake Error at quantize/CMakeLists.txt:4 (include):\r\n  include could not find requested file:\r\n\r\nDefaultTargetOptions\r\n\r\n\r\n-- Configuring incomplete, errors occurred!",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1611/comments",
    "author": "rohithbojja",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-12-09T19:28:09Z",
        "body": "To construct Talk-Llama, you can follow these straightforward steps:\r\n\r\nStart by downloading the source code for whisper.cpp from its main repository, commonly known as `whisper.cpp-master`. After acquiring the code, you'll need to prepare the build for Talk-Llama and any related examples that necessitate SDL2. This is achieved by executing the command:\r\n```\r\ncmake -S whisper.cpp-master -B [desired_build_directory] -DWHISPER_CUBLAS=1 -DWHISPER_SDL2=1\r\n```\r\nReplace `[desired_build_directory]` with the path where you want the build files to be placed. This command ensures all necessary configurations are set up. Following this, you can build the binary with this command:\r\n```\r\ncmake --build [desired_build_directory] --config release\r\n```\r\nHere, again, replace `[desired_build_directory]` with the path you specified earlier. This will compile and create the Talk-Llama application for you."
      },
      {
        "user": "rohithbojja",
        "created_at": "2023-12-12T15:35:56Z",
        "body": "SDL-2 isnt installing in windows! Help me\r\n"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-12-16T16:47:34Z",
        "body": "#1648 "
      }
    ]
  },
  {
    "number": 1556,
    "title": "error when building for windows (SDL error )",
    "created_at": "2023-11-26T15:39:40Z",
    "closed_at": "2024-01-15T12:48:59Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1556",
    "body": "Hello,\r\n\r\nthanks for great asset!. I am trying to run command.cpp but i get error of linker for SDL; first i downloaded SDL 2.28.5, but I got these errors:\r\n\r\n\r\n```\r\nError\tLNK2019\tunresolved external symbol \"public: __cdecl audio_async::audio_async(int)\" (??0audio_async@@QEAA@H@Z) referenced in function SDL_main\t\t\t\r\n************\r\nError\tLNK2019\tunresolved external symbol \"public: __cdecl audio_async::~audio_async(void)\" (??1audio_async@@QEAA@XZ) referenced in function SDL_main\t\r\n************\r\nError\tLNK2019\tunresolved external symbol \"public: bool __cdecl audio_async::init(int,int)\" (?init@audio_async@@QEAA_NHH@Z) referenced in function SDL_main\r\n************\r\nLNK2019\tunresolved external symbol \"public: bool __cdecl audio_async::resume(void)\" (?resume@audio_async@@QEAA_NXZ) referenced in function SDL_main\r\n************\r\nunresolved external symbol \"public: bool __cdecl audio_async::pause(void)\" (?pause@audio_async@@QEAA_NXZ) referenced in function SDL_main\t\r\n************\r\n```\r\n\t\r\n\t\t\r\n\r\n\t\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1556/comments",
    "author": "Ehsanwww",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-11-26T17:48:27Z",
        "body": "The errors you're encountering, such as `LNK2019`, are linking errors, where `LNK` signifies a problem with linking. The issue might stem from the fact that your compiler is unable to locate the SDL, even though it's been downloaded. To resolve this, you need to direct your compiler to the SDL files using CMake configurations."
      },
      {
        "user": "BrianTheBrain11",
        "created_at": "2024-08-21T05:28:28Z",
        "body": "I aswell have this error but do not have any issues linking against any other SDL2 functions. just the audio ones seem to be unresolved external symbols. Using the precompiled binaries and includes of the latest SDL2 VC release on windows. "
      }
    ]
  },
  {
    "number": 1531,
    "title": "Why does whisper.cpp not support Chinese recognition",
    "created_at": "2023-11-21T00:44:06Z",
    "closed_at": "2023-11-26T03:43:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1531",
    "body": "Whisper supports Chinese recognition. Whisper is an advanced speech recognition system developed by OpenAI, capable of recognizing and transcribing multiple languages, including Chinese. This means Whisper can recognize Chinese speech and convert it into Chinese text. Thanks to its advanced deep learning model, Whisper excels in handling a variety of languages and dialects. Why does whisper.cpp not support Chinese recognition?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1531/comments",
    "author": "litongjava",
    "comments": [
      {
        "user": "flameddd",
        "created_at": "2023-11-21T05:58:46Z",
        "body": "what's your issue ? you can indicate language with `-l` flag\r\n\r\n```\r\n -l zh\r\n```"
      },
      {
        "user": "litongjava",
        "created_at": "2023-11-21T06:04:04Z",
        "body": "I tested it, it supports Chinese recognition, but the result is very bad.But Whipser recognizes it better than that.Why?"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-11-21T10:05:34Z",
        "body": "> I tested it, it supports Chinese recognition, but the result is very bad.But Whipser recognizes it better than that.Why?\r\n\r\nCan you check if beam search is enabled? Please make sure that the `-bs` parameter is set to `5`."
      },
      {
        "user": "yexiangyu",
        "created_at": "2023-11-26T01:51:31Z",
        "body": "> I tested it, it supports Chinese recognition, but the result is very bad.But Whipser recognizes it better than that.Why?\r\n\r\n- please consider to use prompt like, `--prompt \"请用简体中文输出\"`\r\n- use `-l zh`\r\n- use larger model if tiny is used."
      },
      {
        "user": "litongjava",
        "created_at": "2023-11-26T03:43:57Z",
        "body": "thanks."
      },
      {
        "user": "sheng-di",
        "created_at": "2023-12-16T03:53:14Z",
        "body": "How to enable multiple language output?"
      }
    ]
  },
  {
    "number": 1517,
    "title": "Full GPU Support for Release.",
    "created_at": "2023-11-18T04:02:51Z",
    "closed_at": "2023-11-19T10:43:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1517",
    "body": "I am sorry, I don't much about coding. But Whisper.cpp helps me a lot in my life and work. The release versions seem not to support CUDA gpu by default, while users have to build on their own. However, I know few about stuff like \"Make\" or \"CMake\" and thus don't know how to build it on my Windows PC. Could you release a version with full GPU support? Thank you.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1517/comments",
    "author": "AeneasZhu",
    "comments": [
      {
        "user": "Ajaja",
        "created_at": "2023-11-18T11:12:48Z",
        "body": "Just use artifact ```whisper-cublas-bin-x64```  from actions. But you also have to install CUDA Toolkit 12.1 from nvidia.com  or just extract  ```cublas64_12.dll```, ```cublasLt64_12.dll``` and  ```cudart64_12.dll``` libraries from the toolkit and put them into whisper.cpp folder."
      },
      {
        "user": "AeneasZhu",
        "created_at": "2023-11-18T12:06:11Z",
        "body": "> Just use artifact `whisper-cublas-bin-x64` from actions. But you also have to install CUDA Toolkit 12.1 from nvidia.com or just extract `cublas64_12.dll`, `cublasLt64_12.dll` and `cudart64_12.dll` libraries from the toolkit and put them into whisper.cpp folder.\r\n\r\nI have installed the CUDA Toolkit. But I input `main` in terminal and nothing happens."
      },
      {
        "user": "Ajaja",
        "created_at": "2023-11-18T15:01:33Z",
        "body": "It's strange. Maybe it needs ```cublas64_12.dll```, ```cublasLt64_12.dll``` and ```cudart64_12.dll``` in the folder with ```main.exe```. I have RTX 2060 and everything works well with ```whisper-cublas-bin-x64``` artifacts from ```master``` tree and those 3 dlls from CUDA toolkit."
      },
      {
        "user": "bobqianic",
        "created_at": "2023-11-18T22:00:08Z",
        "body": "> Maybe it needs cublas64_12.dll, cublasLt64_12.dll and cudart64_12.dll in the folder with main.exe\r\n\r\nYes.\r\n\r\nWe should add these `.dll` files to the release, for added convenience."
      }
    ]
  },
  {
    "number": 1465,
    "title": "talk-llama: say command not found",
    "created_at": "2023-11-09T11:21:18Z",
    "closed_at": "2023-11-12T20:47:22Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1465",
    "body": "Thanks for this work, highly appreciated. I'm getting quite good results on an orangepi5, using wizardLM-7B.ggmlv3.q4_0.bin that I've converted to gguf.\r\n\r\nWhen I test talk-llama, I get this in the exchanges:\r\n```\r\nGeorgi:./examples/talk-llama/speak: line 13: say: command not found\r\nmain: failed to speak\r\n```\r\nThis isn't a big deal, but a little annoying.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1465/comments",
    "author": "hbarnard",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2023-11-09T11:28:42Z",
        "body": "You can edit the script `./examples/talk-llama/speak` to use any TTS you'd like. You can make it empty to disable speaking completely"
      },
      {
        "user": "hbarnard",
        "created_at": "2023-11-09T12:43:30Z",
        "body": "Thanks, my bad, should have peeked into the script first. But hopefully will help someone else too."
      }
    ]
  },
  {
    "number": 1450,
    "title": "Chinese Traditional to Simplified",
    "created_at": "2023-11-08T07:12:55Z",
    "closed_at": "2023-11-08T10:26:29Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1450",
    "body": "  -dl,       --detect-language   [false  ] exit after automatically detecting language\r\n             --prompt PROMPT     [以下是普通話的句子。] initial prompt\r\n\r\n./main -m models/ggml-base.bin -f  demo.wav \r\n\r\nsystem_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | METAL = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 0 | \r\n\r\nmain: processing '/Users/yaoyajie/Downloads/1699425251144.wav' (180800 samples, 11.3 sec), 4 threads, 1 processors, lang = zh, task = transcribe, timestamps = 1 ...\r\n\r\n[00:00:00.000 --> 00:00:11.000]  公譽善其事,必先立其棄。進行開發工作時,利用並熟練使用恰當的工具可以讓工作效率得到大幅度提高。下邊。\r\n\r\nHow can i convert Traditional Chinese to Simplified Chinese?  Thanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1450/comments",
    "author": "summerHearts",
    "comments": [
      {
        "user": "Glowin",
        "created_at": "2023-11-08T09:05:12Z",
        "body": "`./main -m models/ggml-base.bin -f demo.wav --prompt \"以下是普通话的句子\"`"
      }
    ]
  },
  {
    "number": 1427,
    "title": "Extra newlines and text?",
    "created_at": "2023-11-04T14:08:57Z",
    "closed_at": "2023-11-12T20:48:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1427",
    "body": "Is there significance to the seemingly random newlines and extra text that Whisper inserts into its transcriptions?\r\n\r\nIt doesn't happen every time, but for certain audio clips, Whisper will break the transcription into multiple lines and sometimes even add extra text that is either redundant or not in the audio. Does this signify an \"alternate\" transcription or is it just an error?\r\n\r\nFor example, I have an audio file of a man saying, \"Typhoid, did I tell you?\" and Whispher's raw output is:\r\n\r\n    Typhoid, did I tell you?\r\n    tell you\r\n\r\nSo, pretty close, but it's adding that extra line in for some reason. Any idea why? I have a few other examples and their files to share if anyone's interested in debugging this.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1427/comments",
    "author": "chrisspen",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-11-07T00:07:29Z",
        "body": "This is simply an error, stemming from the model's hallucination. It's a widely discussed issue in the field of LLM."
      }
    ]
  },
  {
    "number": 1416,
    "title": "Does it only support wav files, not mp3?  ",
    "created_at": "2023-11-02T04:56:22Z",
    "closed_at": "2023-11-12T20:51:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1416",
    "body": "Does it only support wav files, not mp3?\r\n ",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1416/comments",
    "author": "fniks",
    "comments": [
      {
        "user": "jmalfara",
        "created_at": "2023-11-02T13:41:08Z",
        "body": "From my experience whisper in general needs specific encoding. You can use FFMPEG to process the audio before pushing it to transcription. I use the Node Addon so here's what I do.\r\n\r\n```\r\n  async encodeForWhisper(\r\n    inputFile: string\r\n  ): Promise<string> {\r\n    const newFile = `${this.tempPathDir}/${randomUUID()}.wav`;\r\n    await new Promise((resolve, reject) => {\r\n      ffmpegCommand()\r\n        .addInput(inputFile)\r\n        .audioFrequency(16000)\r\n        .audioBitrate(16000)\r\n        .audioFilters([\r\n          'lowpass=3000',\r\n          'highpass=200',\r\n          'afftdn=nf=-80',\r\n          'silenceremove=stop_periods=-1:stop_duration=2:stop_threshold=0.02',\r\n        ])\r\n        .on('error', (err) => {\r\n          this.logger.error(err);\r\n          reject(err);\r\n        })\r\n        .on('end', () => {\r\n          resolve(newFile);\r\n        })\r\n        .save(newFile);\r\n    });\r\n    return newFile;\r\n  }\r\n```\r\n\r\nUsing FFMPEG this changes bitrate and frequency to 16000 khz and removes any \"whitespace\" in the audio file.\r\nHope that helps"
      },
      {
        "user": "bjnortier",
        "created_at": "2023-11-06T08:40:49Z",
        "body": "Yes you have to convert to 16bit 16kHz PCM:\r\n```$ ffmpeg -i <input gile> -acodec pcm\\_s16le -ac 1 -ar 16000 <output file>```"
      },
      {
        "user": "mikkovedru",
        "created_at": "2023-11-28T02:03:49Z",
        "body": "@bjnortier \r\n> Yes you have to convert to 16bit 16kHz PCM: `$ ffmpeg -i <input gile> -acodec pcm\\_s16le -ac 1 -ar 16000 <output file>`\r\n\r\nDo you know why is this trivial conversion not done automatically, like it is done in the original Whisper?\r\n\r\n"
      },
      {
        "user": "bjnortier",
        "created_at": "2023-11-28T07:31:48Z",
        "body": "The original Whisper also uses ffmpeg, and requires it as an external dependency. It just runs it automatically. Not requiring ffmpeg in Whisper.cpp is the right decision, because not all platforms can use it. For example in my Swift apps I use other libraries or standard libraries. \r\n"
      },
      {
        "user": "mikkovedru",
        "created_at": "2023-11-28T22:29:36Z",
        "body": "Thank you for answering. \r\n\r\nI understand not requiring any external dependency to get the normal functionality. In fact, I also support it.\r\n\r\nBut I don't understand why whisper.cpp can't have a highly sought extra functionality **on top** of the normal abilities:\r\n1. if you give an acceptable input format, whisper.cpp can just use it\r\n2. if the format is unacceptable, and ffmpeg exists, whisper.cpp can inform/warn about it, but still convert it automatically\r\n3. if the format is unacceptable, but ffmpeg doesn't exist, whisper.cpp can inform/warn about it and quit.\r\n\r\nNow, in order to use whisper.cpp from command line I have to spend time trying to program, test and debug some kind of helpful shell script as a wrapper for whisper.cpp, which would convert the files, run whisper.cpp and then delete extra audio files. This is just wasteful for who knows how many other people apart from me (let alone many people who don't know how to do it).\r\n\r\nDoes that sound reasonable to you?"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-11-28T22:38:11Z",
        "body": "> Does that sound reasonable to you?\r\n\r\nYes, absolutely. However, we're not full-time developers, and there are some more urgent tasks that need our attention first. But if anyone has spare time and would like to contribute their code, please don't hesitate to open a pull request."
      },
      {
        "user": "jasonkaplan79",
        "created_at": "2024-12-01T19:14:40Z",
        "body": "> Thank you for answering.\r\n> \r\n> I understand not requiring any external dependency to get the normal functionality. In fact, I also support it.\r\n> \r\n> But I don't understand why whisper.cpp can't have a highly sought extra functionality **on top** of the normal abilities:\r\n> \r\n> 1. if you give an acceptable input format, whisper.cpp can just use it\r\n> 2. if the format is unacceptable, and ffmpeg exists, whisper.cpp can inform/warn about it, but still convert it automatically\r\n> 3. if the format is unacceptable, but ffmpeg doesn't exist, whisper.cpp can inform/warn about it and quit.\r\n> \r\n> Now, in order to use whisper.cpp from command line I have to spend time trying to program, test and debug some kind of helpful shell script as a wrapper for whisper.cpp, which would convert the files, run whisper.cpp and then delete extra audio files. This is just wasteful for who knows how many other people apart from me (let alone many people who don't know how to do it).\r\n> \r\n> Does that sound reasonable to you?\r\n\r\n+1 for this request - sounds very reasonable to me."
      }
    ]
  },
  {
    "number": 1385,
    "title": "Does this do mp4 videos?",
    "created_at": "2023-10-21T18:53:18Z",
    "closed_at": "2023-10-22T10:44:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1385",
    "body": "I am new to this sorry, I have some beginner questions.\r\n\r\nDoes this only suport Wav files with 16khz? I have videos that are mp4 files. Can I do them as well with this? I tired importing my mp4 videos but it gave the error, failed to read wav file.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1385/comments",
    "author": "dazzng",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-10-21T19:16:38Z",
        "body": "> Does this only suport Wav files with 16khz?\r\n\r\nYes, whisper models only support 16KHz `.wav` PCM audio.\r\n\r\n> Can I do them as well with this?\r\n\r\nAbsolutely. Whisper.cpp itself cannot extract audio from `.mp4` videos and convert it to 16KHz `.wav` PCM audio, but you can achieve that by using `ffmpeg`."
      },
      {
        "user": "dazzng",
        "created_at": "2023-10-21T22:26:58Z",
        "body": "hey thanks for the answer. so I should use ffmpeg to convert mp4 video files to wav 16 khz audio files and then transcribe them?"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-10-21T22:28:10Z",
        "body": "> hey thanks for the answer. so I should use ffmpeg to convert mp4 video files to wav 16 khz audio files and then transcribe them?\r\n\r\nExactly."
      }
    ]
  },
  {
    "number": 1366,
    "title": "much worst transcription than official openai-whisper",
    "created_at": "2023-10-14T14:18:11Z",
    "closed_at": "2023-10-22T10:44:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1366",
    "body": "First thank for good work! Performance on my macbook is amazing!\r\n\r\nBut I have problem, maybe with some settings.\r\nI try to use large model and on whisper.cpp i got much worst result than on official openai-whisper app.\r\nnow I only found that `-mc 0` give me little better result but still not same as openai-whisper.\r\n\r\nwhich setting I should change?\r\n\r\nBest regards!",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1366/comments",
    "author": "piotr-sikora-v",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-10-14T14:31:12Z",
        "body": "whisper.cpp does not use beam search by default. If you want more accurate results, please adjust the `-bs` and `-bo` parameters."
      },
      {
        "user": "polvoazul",
        "created_at": "2023-10-18T02:25:10Z",
        "body": "Out of curiosity, why isn't that the default?"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-10-19T21:34:37Z",
        "body": "> Out of curiosity, why isn't that the default?\r\n\r\nIt's a balance between speed and accuracy. \r\nFor most applications, greedy mode tends to be sufficient."
      }
    ]
  },
  {
    "number": 1340,
    "title": "Can it run with a large model with GPU memory of 8GB?",
    "created_at": "2023-10-02T04:46:10Z",
    "closed_at": "2023-10-09T13:54:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1340",
    "body": "I'm going to buy a video card to run whisper.cpp with large model in GPU mode, How much vram does it need?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1340/comments",
    "author": "playgithub",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-10-02T08:26:27Z",
        "body": "Please provide some descriptions."
      },
      {
        "user": "playgithub",
        "created_at": "2023-10-03T03:14:18Z",
        "body": "> Please provide some descriptions.\r\n\r\ndescription updated"
      },
      {
        "user": "mirek190",
        "created_at": "2023-10-08T00:34:58Z",
        "body": "I'm using the biggest one ggml-large-v1.bin ( 2.88 ) GB not quantized because even 8bit quantized version is much worse than full fp16 from my tests. \r\n\r\nCurrent version for GPU ( cuda ) takes 0.9 GB of my VRAM.  "
      },
      {
        "user": "playgithub",
        "created_at": "2023-10-09T13:54:29Z",
        "body": "It only takes 332MB of GPU memory for me when working with the large model, thank godness, no need to buy a Graphics card recently."
      }
    ]
  },
  {
    "number": 1336,
    "title": "'common-sdl.h' file not found — MacOS Sonoma",
    "created_at": "2023-09-30T16:52:51Z",
    "closed_at": "2023-10-29T17:11:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1336",
    "body": "When trying to `make` any of the examples requiring `common-sdl.h` I run into\r\n\r\n```\r\ncommand.cpp:9:10: fatal error: 'common-sdl.h' file not found\r\n```\r\n\r\nI've installed `sdl2` using brew, and I've even tried building from source, but no joy.\r\n\r\nAny suggestions appreciated, thanks",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1336/comments",
    "author": "joshfarrant",
    "comments": [
      {
        "user": "DissonanceTK",
        "created_at": "2023-10-05T18:17:51Z",
        "body": "Same issue - MacOS Sonoma. Macbook Air M1.\r\n`make stream`\r\n`c++     stream.cpp   -o stream`\r\n`stream.cpp:5:10: fatal error: 'common-sdl.h' file not found`\r\n`#include \"common-sdl.h\"`\r\n`         ^~~~~~~~~~~~~~`\r\n`1 error generated.`\r\n`make: *** [stream] Error 1`"
      },
      {
        "user": "ai-at-home",
        "created_at": "2023-10-29T02:46:43Z",
        "body": "It seems likely that this error comes from not compiling in the root directory.  I am on Linux however if you attempt to compile from within the directory this is the error you get:\r\n\r\n```\r\nwhisper.cpp/examples/stream$ make stream\r\ng++     stream.cpp   -o stream\r\nstream.cpp:6:10: fatal error: common/sdl.h: No such file or directory\r\n    6 | #include \"common/sdl.h\"\r\n      |          ^~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake: *** [<builtin>: stream] Error 1\r\n```\r\n\r\nThe solution is simply to `cd ../../` to go to the root dir then compile.\r\n\r\nAs such I don't think its a bug so much as perhaps a clarification needed in the docs."
      },
      {
        "user": "joyyang1215",
        "created_at": "2024-04-22T08:08:32Z",
        "body": "\"The solution is simply to cd ../../ to go to the root dir then compile.\" --- it works\r\n"
      },
      {
        "user": "gabriel-Oak",
        "created_at": "2024-12-16T12:07:01Z",
        "body": "Wich command should I run from the root directory to compile? I've never worked with cmake before, I'm trying to run `make command`, `make command -C examples/command`, but it isn't working."
      }
    ]
  },
  {
    "number": 1301,
    "title": "still low performances with CoreML model (using ANE) compared to Vosk (Kaldi) using CPU",
    "created_at": "2023-09-17T09:19:15Z",
    "closed_at": "2023-09-18T12:22:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1301",
    "body": "Hi,\r\n\r\nusing the large CoreML encoder  provided by huggingface I still have performances very low compared to Vosk with Kaldi and I don't get why.\r\n\r\nwhen I run it:\r\n\r\nwhisper_init_state: Core ML model loaded\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | COREML = 1 | OPENVINO = 0 | \r\n\r\nso everything is finely set and in theory I should use ANE that should be really performant.\r\n\r\nfor converting 3h audio (16khz and 1channel) it took 1h, while with Vosk using Kaldi (I use only the CPU), same quality but it took 11min, how is it possible? Am I missing something?\r\n\r\nThank you\r\nLuca",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1301/comments",
    "author": "xcottos",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-09-17T10:08:53Z",
        "body": ">  I still have performances very low compared to Vosk with Kaldi and I don't get why.\r\n\r\nCompared to the Whisper large model, which is 2.9 GiB, the Vosk and Kaldi models are relatively small. In addition, the `KAIDI GigaSpeech ASR` models were trained on datasets that are an order of magnitude smaller than `Whisper` (10,000 hours vs 680,000 hours). If you're aiming for faster transcription speeds, you might want to consider using `Metal` instead of `CoreML` or other smaller models (eg. small, medium).\r\n\r\n> In this work we close that gap, scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper."
      },
      {
        "user": "xcottos",
        "created_at": "2023-09-17T11:30:31Z",
        "body": "Thank you bobqianic,\r\n\r\nI can see, indeed, that the transcription with whisper is much accurate (writes numbers as numbers, the punctuation is much better etc) so I assume the better accuracy is also the result of a larger training set/model.\r\n\r\nI am studying the options I have and I would like to push whisper at its max performances on my M1 Max (64GB Ram and 32 cores GPU). Now I started studying the acceleration with my Mac and as far as I understood ANE should be even more performant compared to Metal but it's still only theory for me that's why I'm experimenting.\r\n\r\nFor using ANE the only way (as far as I could get) is using CoreML and apparently I am using it now (so in theory I am using the NPU)\r\n\r\nIs there already the support for Metal with whisper to test its performances on my GPU?\r\n\r\nThank you\r\nLuca"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-09-17T11:41:19Z",
        "body": "> Is there already the support for Metal with whisper to test its performances on my GPU?\r\n\r\nYes. See #1270"
      },
      {
        "user": "xcottos",
        "created_at": "2023-09-18T12:20:52Z",
        "body": "well... great improvement it took 20 min now with Metal  :)\r\n\r\nlarge model and 3h audio in Italian\r\n\r\nI still don't get why ANE is less performant...\r\n\r\nThanks for your suggestion bobqianic, really appreciate it\r\n\r\nLuca"
      }
    ]
  },
  {
    "number": 1288,
    "title": "full ggml-large.bin vs ggml-large_q8.bin   <- even 8bit is much worse than full model ",
    "created_at": "2023-09-14T21:57:02Z",
    "closed_at": "2023-09-19T02:48:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1288",
    "body": "As my tests show full ggml-large.bin vs ggml-large_q8.bin   <- even 8bit is much worse than full model \r\n\r\nTestes with the newest source whisper.cpp built for CPU only - windows 11\r\n\r\n.\\main.exe -m ggml-large.bin -f OUTPUT.WAV -t 28 -pc --prompt music                      <- proper  in 99%  grabbed text, almost perfect - audio is very bad quality from old vinyl\r\n\r\n````\r\n[00:00:00.000 --> 00:00:03.480]   [MUSIC PLAYING]\r\n[00:00:03.480 --> 00:00:06.960]   [MUSIC PLAYING]\r\n[00:00:06.960 --> 00:00:10.440]   [MUSIC PLAYING]\r\n[00:00:11.400 --> 00:00:14.880]   [MUSIC PLAYING]\r\n[00:00:14.880 --> 00:00:18.360]   [MUSIC PLAYING]\r\n[00:00:18.360 --> 00:00:21.840]   [MUSIC PLAYING]\r\n[00:00:21.840 --> 00:00:25.320]   [MUSIC PLAYING]\r\n[00:00:25.320 --> 00:00:27.600]   ÔÖ¬ whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:00:27.600 --> 00:00:31.320]   ÔÖ¬ Dum, dum, dum, dum, dum ÔÖ¬\r\n[00:00:31.320 --> 00:00:33.220]   ÔÖ¬ Dum bidoo be dum ÔÖ¬\r\n[00:00:33.220 --> 00:00:35.660]   ÔÖ¬ Dum, dum, dum, dum ÔÖ¬\r\n[00:00:35.660 --> 00:00:39.860]   ÔÖ¬ Dum bidoo be dum dum, dum, dum ÔÖ¬\r\n[00:00:39.860 --> 00:00:42.260]   ÔÖ¬ Dum bidoo be dum ÔÖ¬\r\n[00:00:42.260 --> 00:00:44.420]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:00:44.420 --> 00:00:48.320]   ÔÖ¬ Dum, dum, dum, dum, dum ÔÖ¬\r\n[00:00:48.320 --> 00:00:52.400]   ÔÖ¬ Dum bidoo be dum dum, dum, dum ÔÖ¬\r\n[00:00:52.900 --> 00:00:56.900]   ÔÖ¬ Dum bidoo be dum dum, dum, dum ÔÖ¬\r\n[00:00:56.900 --> 00:00:59.160]   ÔÖ¬ Dum bidoo be dum ÔÖ¬\r\n[00:00:59.160 --> 00:01:01.320]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:01:01.320 --> 00:01:04.660]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:04.660 --> 00:01:06.820]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:01:06.820 --> 00:01:09.000]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:09.000 --> 00:01:11.160]   ÔÖ¬ Way beyond the sea ÔÖ¬\r\n[00:01:11.160 --> 00:01:13.320]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:13.320 --> 00:01:16.500]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:01:16.500 --> 00:01:18.660]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:01:18.660 --> 00:01:21.620]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:21.800 --> 00:01:23.800]   ÔÖ¬ Come into my heart ÔÖ¬\r\n[00:01:23.800 --> 00:01:25.960]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:25.960 --> 00:01:28.120]   ÔÖ¬ We will never part ÔÖ¬\r\n[00:01:28.120 --> 00:01:30.300]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:30.300 --> 00:01:33.460]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:01:33.460 --> 00:01:35.620]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:01:35.620 --> 00:01:38.300]   ÔÖ¬ Yes, I love you ÔÖ¬\r\n[00:01:38.300 --> 00:01:40.460]   ÔÖ¬ Yes, I really love you ÔÖ¬\r\n[00:01:40.460 --> 00:01:42.620]   ÔÖ¬ Please stay now ÔÖ¬\r\n[00:01:42.620 --> 00:01:44.800]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:01:44.800 --> 00:01:46.960]   ÔÖ¬ Yes, I love you ÔÖ¬\r\n[00:01:46.960 --> 00:01:49.120]   ÔÖ¬ No, I'll never leave you ÔÖ¬\r\n[00:01:49.120 --> 00:01:51.300]   ÔÖ¬ You're never gonna be a- ÔÖ¬\r\n[00:01:51.460 --> 00:01:53.460]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:53.460 --> 00:01:55.620]   ÔÖ¬ Come into my heart ÔÖ¬\r\n[00:01:55.620 --> 00:01:57.800]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:01:57.800 --> 00:01:59.960]   ÔÖ¬ We will never part ÔÖ¬\r\n[00:01:59.960 --> 00:02:02.120]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:02:02.120 --> 00:02:04.300]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:02:04.300 --> 00:02:06.460]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:02:06.460 --> 00:02:08.620]   ÔÖ¬\r\n[00:02:08.620 --> 00:02:10.620]   [music]\r\n[00:02:10.620 --> 00:02:12.780]   [clapping]\r\n[00:02:12.780 --> 00:02:14.960]   [music]\r\n[00:02:14.960 --> 00:02:17.120]   [clapping]\r\n[00:02:17.120 --> 00:02:19.280]   [music]\r\n[00:02:19.280 --> 00:02:21.460]   [clapping]\r\n[00:02:21.460 --> 00:02:23.620]   [music]\r\n[00:02:23.620 --> 00:02:25.780]   [clapping]\r\n[00:02:25.780 --> 00:02:27.960]   ÔÖ¬ Yes, I love you ÔÖ¬\r\n[00:02:27.960 --> 00:02:30.120]   ÔÖ¬ Yes, I really love you ÔÖ¬\r\n[00:02:30.120 --> 00:02:32.280]   ÔÖ¬ Please stay now ÔÖ¬\r\n[00:02:32.280 --> 00:02:34.460]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:02:34.460 --> 00:02:36.620]   ÔÖ¬ Yes, I love you ÔÖ¬\r\n[00:02:36.780 --> 00:02:38.780]   ÔÖ¬ No, I'll never leave you ÔÖ¬\r\n[00:02:38.780 --> 00:02:40.960]   ÔÖ¬ You're never gonna be a- ÔÖ¬\r\n[00:02:40.960 --> 00:02:43.120]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:02:43.120 --> 00:02:45.280]   ÔÖ¬ Come into my heart ÔÖ¬\r\n[00:02:45.280 --> 00:02:47.460]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:02:47.460 --> 00:02:49.620]   ÔÖ¬ We will never part ÔÖ¬\r\n[00:02:49.620 --> 00:02:51.780]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:02:51.780 --> 00:02:53.960]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:02:53.960 --> 00:02:56.120]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:02:56.120 --> 00:02:58.280]   ÔÖ¬ Whoa, whoa, whoa, whoa ÔÖ¬\r\n[00:02:58.280 --> 00:03:00.460]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:03:00.460 --> 00:03:02.620]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:03:02.620 --> 00:03:04.780]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:03:04.960 --> 00:03:06.960]   ÔÖ¬ Dum, dum, dum, dum ÔÖ¬\r\n[00:03:06.960 --> 00:03:09.120]   ÔÖ¬ Dum, little darling ÔÖ¬\r\n[00:03:09.120 --> 00:03:10.720]   (fire crackling)\r\n````\r\n\r\n\r\n.\\main.exe -m ggml-large_q8.bin -f OUTPUT.WAV -t 28 -pc --prompt music        <-- q_8 version of that model ... much worse output ... why so bad? q_8 should be very close to fp16 quality ?? ...\r\n wtf.\r\n\r\n\r\n````\r\n[00:00:00.000 --> 00:00:03.480]   [MUSIC PLAYING]\r\n[00:00:03.480 --> 00:00:06.960]   [MUSIC PLAYING]\r\n[00:00:06.960 --> 00:00:10.440]   [MUSIC PLAYING]\r\n[00:00:11.320 --> 00:00:14.800]   [MUSIC PLAYING]\r\n[00:00:14.800 --> 00:00:18.280]   [MUSIC PLAYING]\r\n[00:00:18.280 --> 00:00:21.760]   [MUSIC PLAYING]\r\n[00:00:21.760 --> 00:00:25.240]   [MUSIC PLAYING]\r\n[00:00:25.680 --> 00:00:29.160]   [MUSIC PLAYING]\r\n[00:00:29.160 --> 00:00:32.640]   [MUSIC PLAYING]\r\n[00:00:32.640 --> 00:00:35.640]   [MUSIC PLAYING]\r\n[00:00:35.640 --> 00:00:39.120]   [MUSIC PLAYING]\r\n[00:00:39.120 --> 00:00:42.880]   ÔÖ¬ Dum dum dum dum-de-do de dum ÔÖ¬\r\n[00:00:42.880 --> 00:00:46.080]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:00:46.080 --> 00:00:50.960]   ÔÖ¬ Dum dum dum dum-de-do de dum ÔÖ¬\r\n[00:00:50.960 --> 00:00:55.240]   ÔÖ¬ Dum dum dum dum-de-do de dum ÔÖ¬\r\n[00:00:55.240 --> 00:01:00.080]   ÔÖ¬ Dum dum dum dum-de-do de dum ÔÖ¬\r\n[00:01:00.080 --> 00:01:03.400]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:01:03.400 --> 00:01:05.860]   ÔÖ¬ Come here darlin' ÔÖ¬\r\n[00:01:05.860 --> 00:01:08.060]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:01:08.060 --> 00:01:10.000]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:01:10.000 --> 00:01:12.360]   ÔÖ¬ Way beyond the sea ÔÖ¬\r\n[00:01:12.360 --> 00:01:14.280]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:01:14.280 --> 00:01:17.280]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:01:17.280 --> 00:01:20.240]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:01:20.240 --> 00:01:22.720]   ÔÖ¬ Come here darlin' ÔÖ¬\r\n[00:01:22.720 --> 00:01:25.040]   ÔÖ¬ Come into my heart ÔÖ¬\r\n[00:01:25.040 --> 00:01:27.040]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:01:27.040 --> 00:01:29.320]   ÔÖ¬ We will never part ÔÖ¬\r\n[00:01:29.320 --> 00:01:31.240]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:01:31.240 --> 00:01:34.080]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:01:34.080 --> 00:01:37.000]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:01:37.000 --> 00:01:39.040]   ÔÖ¬ Yes I love you ÔÖ¬\r\n[00:01:39.040 --> 00:01:41.120]   ÔÖ¬ Yes I really love you ÔÖ¬\r\n[00:01:41.120 --> 00:01:42.720]   ÔÖ¬ Please stay now ÔÖ¬\r\n[00:01:42.720 --> 00:01:45.160]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:01:45.160 --> 00:01:47.160]   ÔÖ¬ Yes I love you ÔÖ¬\r\n[00:01:47.160 --> 00:01:49.840]   ÔÖ¬ No I'll never leave you ÔÖ¬\r\n[00:01:49.840 --> 00:01:53.520]   ÔÖ¬ You never gonna be a tear ÔÖ¬\r\n[00:01:53.520 --> 00:01:55.920]   ÔÖ¬ Come here darlin' ÔÖ¬\r\n[00:01:55.920 --> 00:01:58.240]   ÔÖ¬ Come into my heart ÔÖ¬\r\n[00:01:58.240 --> 00:02:00.160]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:02:00.160 --> 00:02:02.440]   ÔÖ¬ We will never part ÔÖ¬\r\n[00:02:02.440 --> 00:02:04.400]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:02:04.400 --> 00:02:07.320]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:02:07.320 --> 00:02:10.160]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:02:10.160 --> 00:02:28.760]   ÔÖ¬ Yes I love you ÔÖ¬\r\n[00:02:28.760 --> 00:02:30.880]   ÔÖ¬ Yes I really love you ÔÖ¬\r\n[00:02:30.880 --> 00:02:32.480]   ÔÖ¬ Please stay now ÔÖ¬\r\n[00:02:32.480 --> 00:02:35.040]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:02:35.040 --> 00:02:37.040]   ÔÖ¬ Yes I love you ÔÖ¬\r\n[00:02:37.040 --> 00:02:39.720]   ÔÖ¬ No I'll never leave you ÔÖ¬\r\n[00:02:39.720 --> 00:02:43.440]   ÔÖ¬ You never gonna be a tear ÔÖ¬\r\n[00:02:43.440 --> 00:02:45.960]   ÔÖ¬ Come here darlin' ÔÖ¬\r\n[00:02:45.960 --> 00:02:48.200]   ÔÖ¬ Come into my heart ÔÖ¬\r\n[00:02:48.200 --> 00:02:50.160]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:02:50.160 --> 00:02:52.400]   ÔÖ¬ We will never part ÔÖ¬\r\n[00:02:52.400 --> 00:02:54.320]   ÔÖ¬ Oh darlin' ÔÖ¬\r\n[00:02:54.320 --> 00:02:57.040]   ÔÖ¬ Come and go with me ÔÖ¬\r\n[00:02:57.040 --> 00:03:00.160]   ÔÖ¬ Wah wah wah wah ÔÖ¬\r\n[00:03:00.160 --> 00:03:03.280]   ÔÖ¬ Come, come, come, come, come ÔÖ¬\r\n[00:03:03.280 --> 00:03:07.800]   ÔÖ¬ Come, little bit, come, come, come, come ÔÖ¬\r\n[00:03:07.800 --> 00:03:10.540]   (fire crackling)\r\n````\r\n\r\n\r\nAny ideas?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1288/comments",
    "author": "mirek190",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-09-19T02:48:02Z",
        "body": "The answer can be found in post #1302"
      }
    ]
  },
  {
    "number": 1284,
    "title": "Significantly Less Accurate Results when Using Multiple Threads",
    "created_at": "2023-09-13T13:25:19Z",
    "closed_at": "2023-09-13T15:14:43Z",
    "labels": [
      "question",
      "solution"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1284",
    "body": "I'm aware that there is a warning for transcription accuracy when using multiple threads, but in my testing, even using something like 4 threads on a 30-40 second clip results in wildly different results from using a single thread. Is this expected? Is there any way to improve this with what `whisper.cpp` offers out of the box? \r\n\r\nI assume the issue is that the audio is split into even chunks which could be right in the middle of a sentence. One idea would be to try to auto detect \"silence\" in audio clips and split it into chunks based on that and then transcribe those chunks using multiple threads.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1284/comments",
    "author": "DeveloperPaul123",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-09-13T13:52:49Z",
        "body": "Which flag did you use to set the number of threads? `-p` or `-t`?"
      },
      {
        "user": "DeveloperPaul123",
        "created_at": "2023-09-13T14:22:49Z",
        "body": "I'm currently setting this in code, not from a command line. Something like:\r\n```cpp\r\nwhisper_full_params whisper_params =\r\n                whisper_full_default_params(WHISPER_SAMPLING_BEAM_SEARCH);\r\nwhisper_params.n_threads = 4;\r\n```\r\n\r\nEdit: Just realized I may have been doing this wrong. When calling `whisper_full_parallel`, I was passing the `n_threads` parameter as the final argument instead of the the processor count as is done in the `main` example. "
      },
      {
        "user": "bobqianic",
        "created_at": "2023-09-13T14:35:55Z",
        "body": "> I'm currently setting this in code, not from a command line. Something like:\r\n> \r\n> ```c++\r\n> whisper_full_params whisper_params =\r\n>                 whisper_full_default_params(WHISPER_SAMPLING_BEAM_SEARCH);\r\n> whisper_params.n_threads = 4;\r\n> ```\r\n\r\nCould you let me know the value of `whisper_params.n_processors`? If this value is set to `1`, even when you invoke `whisper_full_parallel`, it should default to `whisper_full`. In this case, the audio shouldn't be split."
      },
      {
        "user": "DeveloperPaul123",
        "created_at": "2023-09-13T14:38:40Z",
        "body": "Yes you are correct. I did some more testing and looks like with a max thread count on 1 processor, there are some performance benefits. Sorry for the oversight on my part! And thanks for your help."
      },
      {
        "user": "DeveloperPaul123",
        "created_at": "2023-09-13T14:39:42Z",
        "body": "@bobqianic Do you know if the CLBlast backed is more effective than OpenBlas? It seems like there is no tensor offloading for whisper like in llama.cpp."
      },
      {
        "user": "bobqianic",
        "created_at": "2023-09-13T14:50:25Z",
        "body": "> @bobqianic Do you know if the CLBlast backed is more effective than OpenBlas? It seems like there is no tensor offloading for whisper like in llama.cpp.\r\n\r\nI‘m not sure, but you can give it a try. Based on my experience with cuBLAS, there is a performance improvement, but it's not significant : )"
      }
    ]
  },
  {
    "number": 1276,
    "title": "Any ways to generate japanese readings?",
    "created_at": "2023-09-12T12:18:23Z",
    "closed_at": "2023-10-24T23:39:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1276",
    "body": "In Japanese, there are chinese characters (Kanji, e.g. 日本, \"nihon, nippon\") and there's an alphabet (Hiragana, e.g. にほん, \"ni-ho-n\"). Kanji has more than one pronunciation and depends on context. When it's not clear how to read a particular Kanji, they put Hiragana near the word like that: 日本(にほん). \r\n\r\nWhisper.cpp generates the kanji version, but it has all pronunciation information to generate hiragana version. In fact, it should be easier to generate hiragana because it's purely pronunciation, whereas there's multiple kanjis for a given pronunciation.\r\n\r\nIs it possible to implement the hiragana mode? Maybe it's possible to suppress certain tokens? ",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1276/comments",
    "author": "ganqqwerty",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-10-24T23:39:14Z",
        "body": "> Is it possible to implement the hiragana mode?\r\n\r\nNo, you might have to fine-tune the model to make that happen."
      }
    ]
  },
  {
    "number": 1188,
    "title": "After coverted to CoreML model, can the bin model file no longer be needed?",
    "created_at": "2023-08-18T03:39:42Z",
    "closed_at": "2023-10-24T23:51:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1188",
    "body": "Ideally, no bin file should be needed after convert model to CoreML model.\r\n\r\nCurrently this not only increases the file size of the app, but also increases memory consumption.\r\n\r\nI know that CoreML for encoder is currently working fine,  but decode seems to still have problems.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1188/comments",
    "author": "haozes",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2023-08-25T14:59:13Z",
        "body": "The `.bin` file is still necessary in order to run the Decoder.\r\nIf we implement CoreML for the Decoder, then we can remove the .bin file"
      }
    ]
  },
  {
    "number": 1107,
    "title": "Automatic translation even when -tr false specified",
    "created_at": "2023-07-14T10:57:41Z",
    "closed_at": "2023-10-24T23:41:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1107",
    "body": "```\r\n(py310-whisper) ➜  whisper.cpp git:(master) ✗ ./main -m models/ggml-large.bin -f samples/output.wav --print-colors -tr false \r\nwhisper_init_from_file_no_state: loading model from 'models/ggml-large.bin'\r\nwhisper_model_load: loading model\r\nwhisper_model_load: n_vocab       = 51865\r\nwhisper_model_load: n_audio_ctx   = 1500\r\nwhisper_model_load: n_audio_state = 1280\r\nwhisper_model_load: n_audio_head  = 20\r\nwhisper_model_load: n_audio_layer = 32\r\nwhisper_model_load: n_text_ctx    = 448\r\nwhisper_model_load: n_text_state  = 1280\r\nwhisper_model_load: n_text_head   = 20\r\nwhisper_model_load: n_text_layer  = 32\r\nwhisper_model_load: n_mels        = 80\r\nwhisper_model_load: ftype         = 1\r\nwhisper_model_load: qntvr         = 0\r\nwhisper_model_load: type          = 5\r\nwhisper_model_load: mem required  = 3557.00 MB (+   71.00 MB per decoder)\r\nwhisper_model_load: adding 1608 extra tokens\r\nwhisper_model_load: model ctx     = 2951.27 MB\r\nwhisper_model_load: model size    = 2950.66 MB\r\nwhisper_init_state: kv self size  =   70.00 MB\r\nwhisper_init_state: kv cross size =  234.38 MB\r\nwhisper_init_state: loading Core ML model from 'models/ggml-large-encoder.mlmodelc'\r\nwhisper_init_state: first run on a device may take a while ...\r\nwhisper_init_state: Core ML model loaded\r\n\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | COREML = 1 | OPENVINO = 0 | \r\n\r\nmain: processing 'samples/output.wav' (148268 samples, 9.3 sec), 4 threads, 1 processors, lang = en, task = translate, timestamps = 1 ...\r\n\r\n\r\n[00:00:00.000 --> 00:00:09.000]   To record the audio natively in macOS I have to put the command 16.\r\n```\r\n\r\nI made an spanish audio recording, it is translating weather I left it without -tr false or with -tr false.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1107/comments",
    "author": "adriangalilea",
    "comments": [
      {
        "user": "alonfaraj",
        "created_at": "2023-07-16T08:09:04Z",
        "body": "Hi @adriangalilea,\r\nYou're trying to use true/false flags in a certain way, but they don't function like that.   \r\nIf you want to enable a flag, simply include `--my_flag` in the command. The value of true/false is ignored for this flag, similar to how you passed `--print-colors`. \r\nIf you want to disable the flag, just omit it from the command.\r\nIn your specific case, you can achieve the desired behavior by removing the `-tr`."
      },
      {
        "user": "adriangalilea",
        "created_at": "2023-07-16T10:54:01Z",
        "body": "Hi @alonfaraj,\r\nThat's the first thing I tried, it wasn't working, I fixed it with `--language auto`\r\n\r\nI tested again today from scratch and it worked just fine without the language auto, so no idea :P\r\n\r\nThank you."
      },
      {
        "user": "bobqianic",
        "created_at": "2023-10-24T23:41:31Z",
        "body": "> I tested again today from scratch and it worked just fine without the language auto, so no idea :P\r\n\r\nWelcome to the world of transformers : )\r\n"
      }
    ]
  },
  {
    "number": 936,
    "title": "100x slowdown on Windows vs macOS",
    "created_at": "2023-05-19T16:38:38Z",
    "closed_at": "2023-11-12T19:27:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/936",
    "body": "Hey - just wanted to check if I was doing this right, as I've tried `whisper.cpp` on an M2 Macbook Pro, and also on a Windows Surface 5 laptop - and I've found close to a 100x slowdown on the JFK sample.\r\n\r\nHere are the traces:\r\nWindows:\r\n`./main.exe -f ..\\..\\..\\samples\\jfk.wav -m ..\\..\\..\\models\\ggml-base.en.bin`\r\n\r\nwhisper_print_timings:     load time =   555.54 ms\r\nwhisper_print_timings:     fallbacks =   0 p /   0 h\r\nwhisper_print_timings:      mel time =  1046.80 ms\r\nwhisper_print_timings:   sample time =   124.91 ms /    27 runs (    4.63 ms per run)\r\nwhisper_print_timings:   encode time = 35476.98 ms /     1 runs (35476.98 ms per run)\r\nwhisper_print_timings:   decode time =  2374.36 ms /    27 runs (   87.94 ms per run)\r\nwhisper_print_timings:    total time = 39730.49 ms\r\n\r\nmacOS:\r\n`./main samples/jfk.wav -m models/ggml-base.en.bin`\r\nwhisper_print_timings:     load time =    94.64 ms\r\nwhisper_print_timings:     fallbacks =   0 p /   0 h\r\nwhisper_print_timings:      mel time =    51.58 ms\r\nwhisper_print_timings:   sample time =    11.33 ms /    27 runs (    0.42 ms per run)\r\nwhisper_print_timings:   encode time =    65.68 ms /     1 runs (   65.68 ms per run)\r\nwhisper_print_timings:   decode time =    71.34 ms /    27 runs (    2.64 ms per run)\r\nwhisper_print_timings:    total time =   402.46 ms\r\n\r\nOn Windows, I ran the following steps:\r\n- `./models/download-ggml-model.sh base.en`\r\n- `cmake -S . -B ./build`\r\n- Open `build/whisper.cpp.sln` in Visual Studio 2022 and run `Build Solution`\r\n- Run `./main.exe -f ..\\..\\..\\samples\\jfk.wav -m ..\\..\\..\\models\\ggml-base.en.bin`\r\n\r\nIs there anything different I could do to get better performance on Windows?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/936/comments",
    "author": "venkr",
    "comments": [
      {
        "user": "venkr",
        "created_at": "2023-05-19T19:40:15Z",
        "body": "Looks like there's some automatic throttling at play, it's 2x faster (only ~50x slower than macOS!) when plugged in:\r\n\r\nwhisper_print_timings:     load time =   416.59 ms\r\nwhisper_print_timings:     fallbacks =   0 p /   0 h\r\nwhisper_print_timings:      mel time =   821.72 ms\r\nwhisper_print_timings:   sample time =    59.23 ms /    27 runs (    2.19 ms per run)\r\nwhisper_print_timings:   encode time = 16046.74 ms /     1 runs (16046.74 ms per run)\r\nwhisper_print_timings:   decode time =   658.34 ms /    27 runs (   24.38 ms per run)\r\nwhisper_print_timings:    total time = 18098.11 ms"
      },
      {
        "user": "nalbion",
        "created_at": "2023-06-01T11:21:41Z",
        "body": "The bottleneck seems to be `ggml_compute_forward_mul_mat_f16_f32()` for `GGML_TASK_COMPUTE` which frequently takes longer than 100ms to execute (particularly nodes #75, 159 and 243), but only the 1st time `ggml_graph_compute()` is called. The first time this is called it processes **268 nodes** and takes **over 3 seconds** to run on my laptop, but then I see it being called again with 66 nodes (200ms) and then 27 more times with 368 nodes, but each of those calls only takes about 10ms.\r\n\r\nOn my Dell laptop I get a total time of 4000 to 9000 ms for `jfk.wav` and `ggml-base.en.bin`\r\nIf I build with CUBLAS that comes down to 3000 to 5000 ms. \r\n\r\nNoob question - if I have an Nvidia GPU, are cuBLAS, CLBlast, OpenBLAS"
      }
    ]
  },
  {
    "number": 578,
    "title": "What is the audio length in the stream?",
    "created_at": "2023-03-07T00:34:03Z",
    "closed_at": "2023-03-07T21:53:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/578",
    "body": "```\r\n./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\r\n```\r\nin the --help it says\r\n>            --length N      [10000  ] audio length in milliseconds\r\n\r\nI thought is the max length of a segment?  but it is not that\r\nthe length to run the stream? (I know it isn't that)\r\nis it the time allowed to process it?\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/578/comments",
    "author": "G2G2G2G",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2023-03-07T19:35:02Z",
        "body": "It's the length of each segment.\r\nEach transcribed line by the `stream` example is a segment and it corresponds to `--length` ms of audio."
      },
      {
        "user": "G2G2G2G",
        "created_at": "2023-03-07T21:53:23Z",
        "body": "ohh ok the segment of voice that turns into 1 line of text?"
      }
    ]
  },
  {
    "number": 565,
    "title": "Audio Buffer Format",
    "created_at": "2023-03-04T23:52:27Z",
    "closed_at": "2023-03-07T04:05:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/565",
    "body": "Does whisper.cpp do any downsampling of audio or do we have to provide it 16000hz audio as Floats? I'm trying to get things working on iOS, but the input sample rate doesn't match 16k hz and I'm not sure if that's the reason it's not recognizing in words.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/565/comments",
    "author": "cerupcat",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2023-03-06T18:48:06Z",
        "body": "You have to provide 16 kHz PCM. `whisper.cpp` does not perform resampling. If the audio is not 16 kHz, it will not be transcribed correctly"
      },
      {
        "user": "cristalwen",
        "created_at": "2023-08-09T09:23:56Z",
        "body": "> You have to provide 16 kHz PCM. `whisper.cpp` does not perform resampling. If the audio is not 16 kHz, it will not be transcribed correctly\r\n\r\nIf I want to run a file with any sampling rate, how should I modify it?"
      }
    ]
  },
  {
    "number": 510,
    "title": "Spanish model",
    "created_at": "2023-02-17T02:00:50Z",
    "closed_at": "2023-02-19T06:43:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/510",
    "body": "Hi, I see the only available models are:\r\n\r\ntiny.en tiny base.en base small.en small medium.en medium large-v1 large\r\n\r\nCan anyone help me to generate the ggml-base multilingual or spanish model?\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/510/comments",
    "author": "leonardorame",
    "comments": [
      {
        "user": "leonardorame",
        "created_at": "2023-02-17T02:05:56Z",
        "body": "Easy, just do `./download-ggml-model.sh small` (to download the small multilingual model)."
      },
      {
        "user": "ManuXD32",
        "created_at": "2023-05-27T13:08:01Z",
        "body": "> Easy, just do `./download-ggml-model.sh small` (to download the small multilingual model).\r\n\r\nBut It still translates the audio to english, is there a way so the text is just transcribed and written in spanish and not translates?"
      },
      {
        "user": "panagiotidi",
        "created_at": "2023-06-01T07:28:04Z",
        "body": "Try to use `--language es` when invoking"
      },
      {
        "user": "George-P-1",
        "created_at": "2024-04-09T18:26:45Z",
        "body": "In addition to `--language es` which indicates the input audio language, also use `--task transcribe` which instructs whisper to convert Spanish audio to Spanish text. If you wanna translate to English text then you can specify `--task translate` instead."
      }
    ]
  },
  {
    "number": 489,
    "title": "--diarize labels everything (speaker ?)",
    "created_at": "2023-02-09T23:39:52Z",
    "closed_at": "2023-02-15T18:15:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/489",
    "body": "\r\nThe following command yields the results snippet below and I was wondering if anyone could provide insight as to why?\r\n(using Windows x64 executable)\r\n\r\n`\r\n$ ./main -m models/ggml-tiny.bin -f audio/The-Big-Tech-Show_Dropbox-CEO.wav --diarize\r\n`\r\n\r\nAlso why does it repeat the one sentence for a minute and a half?\r\n\r\n```\r\n[00:09:00.000 --> 00:09:10.000]  (speaker ?) Do you say you're more or less likely or there's no difference in advancing the career giving a promotion to somebody who you don't meet and see regularly?\r\n[00:09:10.000 --> 00:09:23.000]  (speaker ?) I'd say, so to the extent the promotions are based on FaceTime, then clearly folks that have any imbalances there are harmful to equity.\r\n[00:09:23.000 --> 00:09:29.000]  (speaker ?) But that said, you know, FaceTime probably shouldn't be using some other process.\r\n[00:09:29.000 --> 00:09:33.000]  (speaker ?) Then just how much have you been physically together with someone.\r\n[00:09:33.000 --> 00:09:43.000]  (speaker ?) I think one of the great things about the distributed world is some of the ways that it does level the playing field is, you know, I'm the CEO of the company.\r\n[00:09:43.000 --> 00:09:47.000]  (speaker ?) But my tile is not any bigger than anyone else's.\r\n[00:09:47.000 --> 00:09:55.000]  (speaker ?) I think we were all learned a lot before the pandemic about how little subtle biases or where you set it a table.\r\n[00:09:55.000 --> 00:10:02.000]  (speaker ?) You know, what you interrupt people and all these little kind of micro patterns actually have a big effect on how you're perceived.\r\n[00:10:02.000 --> 00:10:05.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:05.000 --> 00:10:08.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:08.000 --> 00:10:11.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:11.000 --> 00:10:14.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:14.000 --> 00:10:17.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:17.000 --> 00:10:20.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:20.000 --> 00:10:23.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:23.000 --> 00:10:26.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:26.000 --> 00:10:29.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:29.000 --> 00:10:32.000]  (speaker ?) I think that's a great way to do that.\r\n[00:10:32.000 --> 00:10:35.000]  (speaker ?) I think that's a great way to do that.\r\n\r\n```\r\n\r\nBelow is the ffmpeg command I use when converting an MP3 file to a WAV Stereo 16khz file\r\n\r\n```\r\ncommand = [\r\n    \"ffmpeg\",\r\n    \"-i\", input_file,\r\n    \"-ac\", \"2\", # stereo\r\n    \"-ar\", \"16000\", # 16kHz\r\n    \"-acodec\", \"pcm_s16le\",\r\n    output_file\r\n]\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/489/comments",
    "author": "SLong97",
    "comments": [
      {
        "user": "strangelearning",
        "created_at": "2023-02-14T15:58:04Z",
        "body": "As a complete novice, I wonder if one can simply \"convert\" a mono (1 channel) into stereo audio file. \r\n\r\nI just ran into the same issue with `speaker ?` for each speaker. Going to try and download a \"natively\" stereo audio file and then try that. "
      },
      {
        "user": "strangelearning",
        "created_at": "2023-02-14T18:27:40Z",
        "body": "I'm aware of that ffmpeg command, but it would seem weird if that actually\nworked, and in fact, does not work for me.\n\nAfter running it, I get (? speaker) for each line of transcription\nunfortunately.\n\n> Message ID: ***@***.***>\n>\n"
      },
      {
        "user": "ggerganov",
        "created_at": "2023-02-15T18:15:13Z",
        "body": "The existing `--diarize` option is designed only for stereo recordings where one speaker speaks in channel 1 and the other in channel 2. This is very basic strategy and will not work in the general case.\r\n\r\nConverting mono to stereo will not work.\r\nBetter diarization might be available in the future."
      },
      {
        "user": "strangelearning",
        "created_at": "2023-02-23T21:40:16Z",
        "body": "> The existing `--diarize` option is designed only for stereo recordings where one speaker speaks in channel 1 and the other in channel 2. This is very basic strategy and will not work in the general case.\r\n> \r\n> Converting mono to stereo will not work. Better diarization might be available in the future.\r\n\r\nThanks for addressing our concerns and for all of your work on this. It is truly appreciated 🙏"
      }
    ]
  },
  {
    "number": 272,
    "title": "Total Memory Use",
    "created_at": "2022-12-14T16:22:08Z",
    "closed_at": "2022-12-15T22:23:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/272",
    "body": "First of all, this is a wonderful project and lots of fun to play with — thanks for all the hard work that went into it!\r\n\r\nI’m wondering about what decides total memory use, particularly on iPhones. At the moment this implementation works well on iOS as seen in the Objective-C example. But the memory use (> 500MB for the base model) is obviously on the high side for anything but professional apps (ie. apps like Photoshop, more likely to be running on iPad anyway, where users will use them for a long period for specific pieces of work, and be more forgiving if all their other apps get terminated by the system).\r\n\r\nOn a high level, what are the constraints on total memory usage? Is it basically a fixed quantity relating to the work the encoder has to do? Is there any prospect of it coming down much in future, using quantisation or other techniques? Would future use of GPUs (or perhaps even the Apple Neural Engine) reduce the memory requirement, or would that only relate to a speedup in processing time? I’m really just trying to get a rough idea of what levers exist to be pulled, if any.\r\n\r\nThanks again!",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/272/comments",
    "author": "danhalliday",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2022-12-15T21:37:21Z",
        "body": "It's actually possible to drastically reduce the runtime memory compared to what is currently being used.\r\nFor example, in the following `base.en` case:\r\n\r\n```java\r\n$  ./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav\r\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\r\nwhisper_model_load: n_vocab       = 51864\r\nwhisper_model_load: n_audio_ctx   = 1500\r\nwhisper_model_load: n_audio_state = 512\r\nwhisper_model_load: n_audio_head  = 8\r\nwhisper_model_load: n_audio_layer = 6\r\nwhisper_model_load: n_text_ctx    = 448\r\nwhisper_model_load: n_text_state  = 512\r\nwhisper_model_load: n_text_head   = 8\r\nwhisper_model_load: n_text_layer  = 6\r\nwhisper_model_load: n_mels        = 80\r\nwhisper_model_load: f16           = 1\r\nwhisper_model_load: type          = 2\r\nwhisper_model_load: adding 1607 extra tokens\r\nwhisper_model_load: mem_required  =  506.00 MB\r\nwhisper_model_load: ggml ctx size =  140.60 MB\r\nwhisper_model_load: memory size   =   22.83 MB\r\nwhisper_model_load: model size    =  140.54 MB\r\n```\r\n\r\nWe currently use a total of `506 MB`, but we really need `140 MB` to store the model and `~23 MB` to store the KV cache (i.e. memory). The rest of the memory usage currently goes to store the intermediate tensors that are created by `ggml` during the inference. This is because we maintain the entire computation graph. But technically, we don't need it.\r\n\r\nIt will take some modifications in `ggml` to support this. Probably not an easy task at the moment for anyone else other than me, due to lack of good documentation of how the library works.\r\n\r\nBut yes - in theory, the mem usage can be reduced."
      },
      {
        "user": "danhalliday",
        "created_at": "2022-12-15T22:23:05Z",
        "body": "This is great to have in mind. Thanks for the detailed information!"
      }
    ]
  },
  {
    "number": 221,
    "title": "After converting to ggml model, there will be additional output at the end compared with whisper",
    "created_at": "2022-12-06T06:13:13Z",
    "closed_at": "2022-12-16T17:50:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/221",
    "body": "whisper output\r\n\r\n[00:00.000 --> 00:04.000] 贵州省依靠新进的信息技术和严格的制度\r\n[00:04.000 --> 00:06.400] 为生态文明建设保驾护航\r\n[00:06.400 --> 00:10.000] 让山更绿 天更蓝 水更清\r\n[00:12.000 --> 00:17.000] 这段时间 林业调查员正在对贵阳市新造林进行中期检查\r\n[00:17.000 --> 00:21.000] 过去上山核查要带上罗盘仪和一大堆图纸\r\n[00:21.000 --> 00:23.000] 现在通过智慧林业云平台\r\n[00:23.000 --> 00:25.000] 只需一台平板电脑\r\n[00:25.000 --> 00:27.000] 诠释的林业信息一目了然\r\n[00:27.000 --> 00:32.000] 我们可以对比某一个区域这个林地的资源的变化的情况\r\n[00:32.000 --> 00:37.000] 有没有被大量占用 破坏 改变林地用途的这种现象存在\r\n[00:37.000 --> 00:40.000] 这个智慧林业云平台汇聚了近7年来\r\n[00:40.000 --> 00:46.000] 贵阳540多万亩林地的地块分布 全数 林木蓄积量等20多项数据\r\n[00:46.000 --> 00:50.000] 信息技术不仅守护着森林 也在保护着环境\r\n[00:50.000 --> 00:52.000] 贵安新区引入的环保云平台\r\n[00:52.000 --> 00:58.000] 可实时获取5个大气监测站 1个水检测站和污水处理厂的数据\r\n[00:58.000 --> 01:01.000] 一旦有超标排放 系统就会自动报警\r\n[01:01.000 --> 01:03.000] 以前找到一个污染源\r\n[01:03.000 --> 01:07.000] 通常要对这个片区的企业进行全面的摸排\r\n[01:07.000 --> 01:09.000] 而现在有了这套大数据系统以后\r\n[01:09.000 --> 01:13.000] 的确给我们节约了很多人力和物力的一个成本\r\n[01:13.000 --> 01:16.000] 先进的技术让环保更智慧高效\r\n[01:16.000 --> 01:20.000] 严格的制度让生态建设更持久长效\r\n[01:20.000 --> 01:24.000] 贵州在国内率先出台了生态文明建设促进条例\r\n[01:24.000 --> 01:28.000] 明确了生态红线区域 生态环境补偿机制\r\n[01:32.000 --> 01:36.000] 覆盖了省内河道 湖泊 水库等各类水域\r\n[01:36.000 --> 01:51.000] 进一步用制度保护好绿水青山\r\n\r\nggml output\r\n\r\n\r\n[00:00:00.000 --> 00:00:04.000]  贵州省依靠先进的信息技术和严格的制度\r\n[00:00:04.000 --> 00:00:06.400]  为生态文明建设保驾护航\r\n[00:00:06.400 --> 00:00:10.000]  让山更绿 天更蓝 水更清\r\n[00:00:10.000 --> 00:00:17.000]  这段时间 林业调查员正在对贵阳市新造林进行中期检查\r\n[00:00:17.000 --> 00:00:21.000]  过去上山核查要带上罗盘仪和一大堆图纸\r\n[00:00:21.000 --> 00:00:23.500]  现在通过智慧林业云平台\r\n[00:00:23.500 --> 00:00:25.000]  只需一台平板电脑\r\n[00:00:25.000 --> 00:00:27.000]  全市的林业信息一目了然\r\n[00:00:27.000 --> 00:00:32.000]  我们可以对比某一个区域林地资源的变化的情况\r\n[00:00:32.000 --> 00:00:37.000]  有没有被大量占用 破坏 改变林地用途的现象存在\r\n[00:00:37.000 --> 00:00:40.000]  这个智慧林业云平台汇聚了近七年来\r\n[00:00:40.000 --> 00:00:46.000]  贵阳540多万亩林地的地块分布 全数 林木蓄积量等20多项数据\r\n[00:00:46.000 --> 00:00:50.000]  信息技术不仅守护着森林 也在保护着环境\r\n[00:00:50.000 --> 00:00:52.000]  贵安新区引入的环保云平台\r\n[00:00:52.000 --> 00:00:58.000]  可实时获取5个大气监测站 1个水检测站和污水处理厂的数据\r\n[00:00:58.000 --> 00:01:01.000]  一旦有超标排放 系统就会自动报警\r\n[00:01:01.000 --> 00:01:03.000]  以前找到一个污染源\r\n[00:01:03.000 --> 00:01:07.000]  通常要对这个片区的企业进行全面的摸排\r\n[00:01:07.000 --> 00:01:09.000]  而现在有了这套大数据系统以后\r\n[00:01:09.000 --> 00:01:13.000]  的确给我们节约了很多人力和物力的一个成本\r\n[00:01:13.000 --> 00:01:16.000]  先进的技术让环保更智慧高效\r\n[00:01:16.000 --> 00:01:20.000]  严格的制度让生态建设更持久长效\r\n[00:01:20.000 --> 00:01:24.000]  贵州在国内率先出台了生态文明建设促进条例\r\n[00:01:24.000 --> 00:01:28.000]  明确了生态红线区域 生态环境补偿机制\r\n[00:01:28.000 --> 00:01:32.000]  今年3月 贵州全面推行省市县乡村五级合掌制度\r\n[00:01:32.000 --> 00:01:36.000]  覆盖了省内河道 湖泊 水库等各类水域\r\n[00:01:36.000 --> 00:01:39.000]  进一步用制度保护好绿水青山\r\n[00:01:39.000 --> 00:01:41.000]  本集完\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/221/comments",
    "author": "wuhongsheng",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2022-12-06T07:32:58Z",
        "body": "The decoding strategy in `whisper.cpp` is not 100% the same as the original Whisper, so this kind of differences can be expected."
      },
      {
        "user": "ggerganov",
        "created_at": "2022-12-16T17:50:58Z",
        "body": "This might or might not be fixed via 6a7c82501e3794724ba80bfb9a983810af036803\r\n\r\nIf not, will be resolved in the future with better decoding"
      }
    ]
  },
  {
    "number": 215,
    "title": "Trying to understand what Random Seed does",
    "created_at": "2022-12-02T05:48:42Z",
    "closed_at": "2022-12-06T12:18:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/215",
    "body": "Could anyone explain what it does? \r\n\r\nI know about random seed in general, but I don't understand what purpose it could serve in the context of a Whisper user. \r\n\r\nMy main issue is the occasional loops that Whisper tends to get stuck in when there are periods of  silence in the audio. Was wondering if Random Seed could help avoid this. Right now I manually break the transcription and then restart it with a time offset.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/215/comments",
    "author": "regstuff",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2022-12-02T18:21:17Z",
        "body": "I think you have some older version of `whisper.cpp`, because I remove the random seed at some point.\r\nIt wasn't used for anything.\r\n\r\nThe idea was to be used for the temperature fallback strategy which helps to mitigate exactly the problem that you experience, but it hasn't been implemented yet."
      }
    ]
  },
  {
    "number": 118,
    "title": "Accelerate on Intel Macs",
    "created_at": "2022-10-31T12:18:24Z",
    "closed_at": "2022-11-02T16:00:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/118",
    "body": "The accelerate framework should be available on intel macs too I believe",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/118/comments",
    "author": "jafri",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2022-10-31T17:40:56Z",
        "body": "Yes - it is available. I need to update the `Makefile` to support it.\r\nBut I think using `CMake` will already support it if you want to give it a try."
      }
    ]
  },
  {
    "number": 61,
    "title": "This code but with CUDA",
    "created_at": "2022-10-18T02:27:45Z",
    "closed_at": "2022-10-26T20:44:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/61",
    "body": "Does anyone have any ideas of how to use this code but with CUDA libs? I want to move away from the Python version but keep PyTorch CUDA.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/61/comments",
    "author": "ejkitchen",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2022-10-18T15:25:53Z",
        "body": "Currently this implementation does not support any GPU framework and there are no plans to do so for the near future."
      },
      {
        "user": "ejkitchen",
        "created_at": "2022-10-18T19:13:29Z",
        "body": "@ggerganov thanks for the quick response! Appreciate it. "
      }
    ]
  },
  {
    "number": 14,
    "title": "Comparison with torch jit",
    "created_at": "2022-10-03T08:55:46Z",
    "closed_at": "2022-10-29T16:58:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/14",
    "body": "Great work! I find especially the implementation of ggml interesting. It looks like you implement all the basic neural network building blocks with ggml. How do you compare it with the torch jit approach of using a pytorch model in c++?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/14/comments",
    "author": "aichr",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2022-10-03T10:50:08Z",
        "body": "Hi, thanks!\r\n\r\nI'm not sure how it compares - I've never used torch jit. In general, I have very little experience with the python frameworks out there. This is one of the reasons to implement `ggml` - to not have to use python and the various extra dependencies that usually come with the framework, and to avoid their overhead. Also, it is a good learning experience - it helps to understand how these NN approaches work on a lower level."
      },
      {
        "user": "skaiware",
        "created_at": "2022-10-03T16:08:05Z",
        "body": "Hi @ggerganov \r\ntorch.jit does nt require any python after exporting the model to a jit file. It s then quite easy to run inference just using the torch Cpp API and linking with torch_cpu.so/dll. But quite invasive (up to 2GB of libraries), specially compared to onnxrt (20MB). And usually slower than onnxrt so not that a priority to try.\r\n"
      },
      {
        "user": "aichr",
        "created_at": "2022-10-04T09:03:07Z",
        "body": "@ggerganov thanks for the reply. I am astonished by your c implementation of these neural network building blocks. Like @skaiware said, the downside of jit could be that it comes with a big linking burden. I think your solution has a great potential to make big models run faster on any device!"
      },
      {
        "user": "aichr",
        "created_at": "2022-10-04T09:07:14Z",
        "body": "how did you make sure that the c implementation renders the same results as pytorch functions? Any tests to guard this?"
      },
      {
        "user": "ggerganov",
        "created_at": "2022-10-04T12:22:15Z",
        "body": "> how did you make sure that the c implementation renders the same results as pytorch functions? Any tests to guard this?\r\n\r\nThe results are note the same - it's hard to make them exactly the same due to round-off errors when using floating point numbers. Instead, I just verified manually that the numbers that I get after each layer are similar to the one from the original python implementation.\r\n\r\nIt would be great to add some tests in the future to make sure the results match the reference implementation within some tolerance."
      },
      {
        "user": "ejkitchen",
        "created_at": "2022-10-18T02:25:07Z",
        "body": "@skaiware, how would you convert this C++ code to use CUDA? any ideas?\r\n\r\n"
      },
      {
        "user": "ggerganov",
        "created_at": "2022-10-29T16:58:24Z",
        "body": "IMHO it does not make sense to port this code to CUDA. If you want to use CUDA or other GPU framework, you are better off using some of the well-established python frameworks (PyTorch, Tensforflow, etc.).\r\n\r\nWith `whisper.cpp`, the main idea is to avoid the overhead that comes from using a high-level programming language (python) and also avoid moving data back and forth across the PCI. We do lose a lot of GPU performance, but we try to compensate by optimizing the CPU operations and memory bandwidth as much as possible.\r\n\r\nAnd here also comes the Apple Silicon hardware which gives you the AMX matrix coprocessor offering great performance boost (at least according to my experiments). It is so easy to integrate it in your project (simply add `-framework Accelerate` to your compile flags) that there is no reason not to use it.\r\n\r\nMy understanding is that python frameworks at the moment do not fully support the Accelerate framework (I might be wrong), but soon they will do and probably you won't see much of a performance improvement when using `whisper.cpp`. But for the moment, I think this project offers better performance - at least this is what I see on my MacBook when comparing to PyTorch. And again, take this with a grain of salt, because I have almost no experience with python, so I could be doing something wrong."
      },
      {
        "user": "ejkitchen",
        "created_at": "2022-10-29T18:14:28Z",
        "body": "@ggerganov Sorry, I should have been clearer. I meant C++ with Pytorch CUDA-enabled drivers. I thought a lot of the performance issues were with Python, but it wasn't as simple as that after looking into it with a profiler called scalene which looks at Python code, C/C++ etc library code, system calls and various GPU metrics.\r\n\r\nWe're now converting Whisper to use TorchScript with TensorRT and some other CUDA optimizations like pinning memory etc. But again this is all for CUDA-enabled devices. We think we're going to get about a 3x-5x, but until completed this is just a theory with quick back-of-the-napkin calculations.\r\n\r\nIn any case, thank you so much for this C++ version; it was quite an inspiration and great work. I might come back to it if I feel we can squeeze more performance gains by remaining entirely native but with Pytorch libs and the other tricks mentioned above."
      }
    ]
  },
  {
    "number": 387,
    "title": "Add process-specific timings",
    "created_at": "2023-01-08T11:10:37Z",
    "closed_at": "2023-04-15T13:31:52Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/pull/387",
    "body": "Adds functions that use `CLOCK_PROCESS_CPUTIME_ID` instead of `CLOCK_MONOTONIC` for timings, and are therefore not affected by other processes on the system.\r\n\r\nOne thing to check before we merge: since the time is per-process, the time reported is `n_threads` times more than realtime since each active thread adds to the timings - should we adjust for this?\r\n\r\nAlso updated bench-all to include information about the measured parameters, better formatting and better configuration detection.\r\n\r\nCloses #382.",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/387/comments",
    "author": "abitofevrything",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2023-01-15T10:32:44Z",
        "body": "@abitofevrything\r\nCan you verify that everything is good on Windows?\r\nAfter that I will merge it"
      },
      {
        "user": "abitofevrything",
        "created_at": "2023-01-15T12:48:27Z",
        "body": "Tested on my Windows machine and there seems to be some issues with using `clock()`. Apparently its implementation is OS-specific and on Windows it returns wall clock time instead of process time.\r\n\r\nApparently there is a `GetProcessTimes` to get process time, but I'll need to look into that later."
      },
      {
        "user": "abitofevrything",
        "created_at": "2023-01-15T14:29:03Z",
        "body": "Well, I changed the implementation to use GetProcessTimes but the issue with the timing still persists: process time is sometimes counted as negative.\r\n\r\nI'm pretty sure this is an underflow/overflow issue, or an issue with the wrong data type being used somewhere, but I don't have a setup to debug this. I'll see what I could do, but if anyone used to windows development has any idea what is happening here I'd be glad if you could let me know."
      }
    ]
  }
]