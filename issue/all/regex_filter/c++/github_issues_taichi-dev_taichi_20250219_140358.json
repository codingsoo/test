[
  {
    "number": 8536,
    "title": "Error when attempting to use GGUI",
    "created_at": "2024-05-31T16:42:47Z",
    "closed_at": "2024-06-09T16:31:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/8536",
    "body": "When I try to run the simplest ggui program using any backend including cuda, vulkan, cpu, opengl:\r\n\r\n```python\r\nimport taichi as ti\r\nti.init(arch=ti.cpu)\r\nwindow = ti.ui.Window(name='Title', res=(640, 360))\r\ncanvas = window.get_canvas()\r\n\r\nwhile window.running:\r\n    window.show()\r\n```\r\nThe window will open and immediately close. I get the error:\r\n```shell\r\ntest.py\r\n[Taichi] version 1.7.1, llvm 15.0.4, commit 0f143b2f, linux, python 3.10.14\r\n[Taichi] Starting on arch=x64\r\nRHI Error: (-1000001004) vkAcquireNextImageKHR failed\r\npython: /home/dev/taichi/taichi/rhi/vulkan/vulkan_device.cpp:2808: virtual taichi::lang::StreamSemaphore taichi::lang::vulkan::VulkanSurface::acquire_next_image(): Assertion `false && \"Error without return code\"' failed.\r\nzsh: IOT instruction (core dumped)  /home/matthew/Code/jvenv/bin/python \r\n```\r\n\r\nWith debug=True:\r\n```shell\r\nRHI Error: (-1000001004) vkAcquireNextImageKHR failed\r\npython: /home/dev/taichi/taichi/rhi/vulkan/vulkan_device.cpp:2808: virtual taichi::lang::StreamSemaphore taichi::lang::vulkan::VulkanSurface::acquire_next_image(): Assertion `false && \"Error without return code\"' failed.\r\n[E 05/31/24 17:18:12.556 15543] Received signal 6 (Aborted)\r\n\r\n\r\n***********************************\r\n* Taichi Compiler Stack Traceback *\r\n***********************************\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x4805994) [0x7fe338005994]\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x1b35d8e) [0x7fe335335d8e]\r\n/usr/lib/libc.so.6(+0x3cae0) [0x7fe3623a8ae0]\r\n/usr/lib/libc.so.6(+0x94e44) [0x7fe362400e44]\r\n/usr/lib/libc.so.6: gsignal\r\n/usr/lib/libc.so.6: abort\r\n/usr/lib/libc.so.6(+0x243df) [0x7fe3623903df]\r\n/usr/lib/libc.so.6(+0x34c67) [0x7fe3623a0c67]\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x4421afe) [0x7fe337c21afe]\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x1dda7b1) [0x7fe3355da7b1]\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x1de5ac0) [0x7fe3355e5ac0]\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x1c2917d) [0x7fe33542917d]\r\n/home/matthew/Code/jvenv/lib/python3.10/site-packages/taichi/_lib/core/taichi_python.cpython-310-x86_64-linux-gnu.so(+0x1c0e1cf) [0x7fe33540e1cf]\r\n/home/matthew/Code/jvenv/bin/python(+0x1445a6) [0x5653ef8995a6]\r\n/home/matthew/Code/jvenv/bin/python: _PyObject_MakeTpCall\r\n/home/matthew/Code/jvenv/bin/python(+0x150866) [0x5653ef8a5866]\r\n/home/matthew/Code/jvenv/bin/python: _PyEval_EvalFrameDefault\r\n/home/matthew/Code/jvenv/bin/python: _PyFunction_Vectorcall\r\n/home/matthew/Code/jvenv/bin/python: _PyEval_EvalFrameDefault\r\n/home/matthew/Code/jvenv/bin/python: _PyFunction_Vectorcall\r\n/home/matthew/Code/jvenv/bin/python: _PyEval_EvalFrameDefault\r\n/home/matthew/Code/jvenv/bin/python(+0x1d7c60) [0x5653ef92cc60]\r\n/home/matthew/Code/jvenv/bin/python: PyEval_EvalCode\r\n/home/matthew/Code/jvenv/bin/python(+0x20812a) [0x5653ef95d12a]\r\n/home/matthew/Code/jvenv/bin/python(+0x203523) [0x5653ef958523]\r\n/home/matthew/Code/jvenv/bin/python(+0x9a6f5) [0x5653ef7ef6f5]\r\n/home/matthew/Code/jvenv/bin/python: _PyRun_SimpleFileObject\r\n/home/matthew/Code/jvenv/bin/python: _PyRun_AnyFileObject\r\n/home/matthew/Code/jvenv/bin/python: Py_RunMain\r\n/home/matthew/Code/jvenv/bin/python: Py_BytesMain\r\n/usr/lib/libc.so.6(+0x25c88) [0x7fe362391c88]\r\n/usr/lib/libc.so.6: __libc_start_main\r\n/home/matthew/Code/jvenv/bin/python(+0x1cb0f1) [0x5653ef9200f1]\r\n```\r\n\r\nOperating System: Manjaro Linux \r\nKDE Plasma Version: 6.0.5\r\nKDE Frameworks Version: 6.2.0\r\nQt Version: 6.7.1\r\nKernel Version: 6.6.32-1-MANJARO (64-bit)\r\nGraphics Platform: Wayland\r\nProcessors: 16 × 11th Gen Intel® Core™ i7-11700K @ 3.60GHz\r\nMemory: 125.6 GiB of RAM\r\nGraphics Processor: Mesa Intel® Graphics\r\nManufacturer: Gigabyte Technology Co., Ltd.\r\nProduct Name: B560M DS3H\r\n\r\nPython version 3.10.14\r\n\r\nMy laptop runs the same operating system, has no gpu, and is able to run ggui without issues. \r\n\r\nI have tried creating a new virtual environment and reinstalling vulkan-tools. taichi diagnose doesn't identify any issues when testing the ggui program. \r\n\r\nSince this is working on my other computer, I am hesitant to assume it is a bug. If anyone has had a similar experience and can suggest a solution, I would be grateful ☯️",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/8536/comments",
    "author": "grindstm",
    "comments": [
      {
        "user": "grindstm",
        "created_at": "2024-06-09T16:31:53Z",
        "body": "The problem is Wayland + GPU. I do not have these issues when using X11 or when using Wayland on a machine without a dedicated GPU. "
      },
      {
        "user": "ArvinSKushwaha",
        "created_at": "2024-12-24T21:43:17Z",
        "body": "Can we reopen this? This is an issue I am also experiencing on my Wayland + GPU configuration."
      }
    ]
  },
  {
    "number": 8491,
    "title": "fp128 and fp256",
    "created_at": "2024-03-19T13:57:02Z",
    "closed_at": "2024-03-19T19:41:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/8491",
    "body": "Is there a way to set the taichi compute precision to a different precision than only 8,16,32,64bit?\r\nFor example 128bit, 256bit, 512bit or 1024bit?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/8491/comments",
    "author": "snapo",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2024-03-19T19:41:44Z",
        "body": "No, there's no hardware support for them and I'm not sure whether there's even a specification for more than 128bit floating point numbers "
      }
    ]
  },
  {
    "number": 8426,
    "title": "关于FPS的一些问题",
    "created_at": "2023-12-04T11:36:30Z",
    "closed_at": "2023-12-08T02:03:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/8426",
    "body": "注意到使用ti.gui和ti.windows的时候都会默认显示FPS，如何把这个显示关了",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/8426/comments",
    "author": "WeiMin-Y",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2023-12-08T02:03:06Z",
        "body": "Sorry that isn't possible"
      }
    ]
  },
  {
    "number": 8233,
    "title": "fill() not working after upgrading to 1.6.0",
    "created_at": "2023-06-27T18:56:26Z",
    "closed_at": "2023-07-13T19:48:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/8233",
    "body": "The following code snippet doesn't work after i upgraded to 1.6.0:\r\n```python\r\nimport taichi as ti\r\nti.init(ti.gpu)\r\nparticle_info = ti.types.struct(\r\n    v_in  = ti.types.vector(3, ti.f32), # input momentum/velocity\r\n    mu      = ti.f32,\r\n    lam     = ti.f32,\r\n)\r\n\r\nparticles = particle_info.field(shape=(10, 100, 100), layout=ti.Layout.SOA)\r\n\r\n\r\n@ti.kernel\r\ndef foo(f: ti.i32):\r\n    for I in ti.grouped(ti.ndrange(100, 100)):\r\n        particles[f, I].fill(0)\r\n\r\nfoo(5)\r\n```\r\n\r\n```bash\r\nAttributeError: '_IntermediateStruct0' object has no attribute 'fill'\r\n```\r\nIt used to work before 1.6.0. What should i do to set elements in a struct field to zero? I can iterate over all elements in the struct, but it's too clumsy.\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/8233/comments",
    "author": "zhouxian",
    "comments": [
      {
        "user": "lin-hitonami",
        "created_at": "2023-07-10T05:02:57Z",
        "body": "It turns out that we deprecated fuction `fill` of Struct in v1.5.0 and removed it in v1.6.0, and we forgot to mention it on the release note. We are sorry for the inconvenience. "
      },
      {
        "user": "zhouxian",
        "created_at": "2023-07-10T05:09:42Z",
        "body": "@lin-hitonami Is there any clean way to fill all elements in a struct to be 0?\r\n"
      },
      {
        "user": "lin-hitonami",
        "created_at": "2023-07-10T08:51:11Z",
        "body": "Sadly no. You have to set each element to zero now. "
      }
    ]
  },
  {
    "number": 8214,
    "title": "How to release gpu memory when deleting field",
    "created_at": "2023-06-23T16:21:17Z",
    "closed_at": "2023-06-27T20:08:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/8214",
    "body": "The following snippet throws out of cuda memory error:\r\n```python\r\nimport taichi as ti\r\nti.init(ti.gpu, device_memory_GB=1)\r\na = ti.field(int, (20000, 10000))\r\na.fill(1)\r\ndel a\r\na = ti.field(int, (20000, 10000))\r\na.fill(1)\r\n```\r\nI understand we can use FieldsBuilder to manually manage memory, but in the doc it says \"Generally, Taichi manages memory allocation and destruction without disturbing the users.\" I wonder: how does memory release work? when will memory associated with deleted field be release?",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/8214/comments",
    "author": "zhouxian",
    "comments": [
      {
        "user": "lin-hitonami",
        "created_at": "2023-06-25T09:14:47Z",
        "body": "Taichi does not release the memory of the field if you delete it because we manage the memory of the fields using something called SNode tree. A SNode tree can contain multiple fields, and the memory of these fields are managed together, which means that sometimes we cannot release the memory of a field without releasing the memory of other fields in the SNode tree. \r\n\r\nBy using FieldsBuilder, you can create a SNode tree. When you destroy a SNode tree, the memory of all fields it contains can be freed at once."
      }
    ]
  },
  {
    "number": 7976,
    "title": "Generate aot file failed when use both rw_texture and ndarray in ti.kernel",
    "created_at": "2023-05-10T12:04:12Z",
    "closed_at": "2023-05-15T06:25:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/7976",
    "body": "Hi, when I use both rw_texture and ndarray in ti.kernel, an error is reported when generating the aot file：\r\n\r\n\r\n\r\n\r\n`[W 05/10/23 19:38:18.222 1784794] [spirv_codegen.cpp:spriv_message_consumer@2278] input\r\n[441:0:0] Id is 0\r\n[E 05/10/23 19:38:18.222 1784794] [spirv_codegen.cpp:run@2371] SPIRV optimization failed\r\nTraceback (most recent call last):\r\n  File \"/Users/xxx/test_taichi_tex_buf.py\", line 43, in <module>\r\n    test_texture_buf(ti.vulkan)\r\n  File \"/Users/xxx/test_taichi_tex_buf.py\", line 39, in test_texture_buf\r\n    m.add_graph(\"taichi_kernel_test\", g)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/taichi/aot/module.py\", line 173, in add_graph\r\n    self._aot_builder.add_graph(name, graph._compiled_graph)\r\nRuntimeError: [spirv_codegen.cpp:run@2371] SPIRV optimization failed`\r\n\r\nIs it because the code I wrote is wrong ？ Or ti.kernel does not support simultaneous use rw_texture and ndarray？\r\n\r\nDemo Code:\r\n\r\n```\r\n        import numpy as np\r\n            import taichi as ti\r\n\r\n    def test_texture_buf(arch):\r\n        ti.init(arch=arch)\r\n        width  = 1080\r\n        height = 1920\r\n        @ti.kernel\r\n        def taichi_kernel_test(\r\n        in_ten:ti.types.rw_texture(num_dimensions=2, fmt=ti.Format.rgba8, lod=0),\r\n        out_ten: ti.types.rw_texture(num_dimensions=2, fmt=ti.Format.rgba8, lod=0),\r\n        in_ten_buf: ti.types.ndarray(dtype=ti.u8, ndim=2),\r\n        height: ti.i32,\r\n        width: ti.i32,\r\n        ):\r\n        for i, j in ti.ndrange(height, width):\r\n            in_num = in_ten.load(ti.Vector([j, i]))\r\n            in_num.x = in_num.x + in_ten_buf[i, j]\r\n            out_ten.store(ti.Vector([j, i]), in_num)\r\n\r\n    arg_0 = ti.graph.Arg(tag=ti.graph.ArgKind.RWTEXTURE,\r\n                         name=\"in_ten\",\r\n                         fmt=ti.Format.rgba8,\r\n                         ndim=2)\r\n    arg_1 = ti.graph.Arg(tag=ti.graph.ArgKind.RWTEXTURE,\r\n                         name=\"out_ten\",\r\n                         fmt=ti.Format.rgba8,\r\n                         ndim=2)\r\n    arg_2  = ti.graph.Arg(ti.graph.ArgKind.NDARRAY, \"in_ten_buf\",  dtype=ti.u8, ndim=2)\r\n    arg_3 = ti.graph.Arg(ti.graph.ArgKind.SCALAR, \"height\", ti.i32)\r\n    arg_4 = ti.graph.Arg(ti.graph.ArgKind.SCALAR, \"width\", ti.i32)\r\n\r\n    g = ti.graph.GraphBuilder()\r\n    g.dispatch(taichi_kernel_test, arg_0, arg_1, arg_2, arg_3, arg_4)\r\n    g = g.compile()\r\n\r\n    # save\r\n    m = ti.aot.Module(arch)\r\n    m.add_graph(\"taichi_kernel_test\", g)\r\n    m.save(\"models_taichi_kernel_test\")\r\n\r\n    if __name__ == \"__main__\":\r\n        test_texture_buf(ti.vulkan)\r\n```\r\n\r\nTaichi Version: v1.5.0\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/7976/comments",
    "author": "helloworldstone",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2023-05-10T20:23:38Z",
        "body": "This is likely a bug"
      },
      {
        "user": "ailzhang",
        "created_at": "2023-05-15T06:25:10Z",
        "body": "@helloworldstone FYI I've run this on master and it works as expected. Here's the script I used: \r\n\r\n```\r\nimport numpy as np\r\nimport taichi as ti\r\n\r\ndef test_texture_buf(arch):\r\n    ti.init(arch=arch)\r\n    width  = 1080\r\n    height = 1920\r\n\r\n    @ti.kernel\r\n    def taichi_kernel_test(\r\n    in_ten:ti.types.rw_texture(num_dimensions=2, fmt=ti.Format.rgba8, lod=0),\r\n    out_ten: ti.types.rw_texture(num_dimensions=2, fmt=ti.Format.rgba8, lod=0),\r\n    in_ten_buf: ti.types.ndarray(dtype=ti.u8, ndim=2),\r\n    height: ti.i32,\r\n    width: ti.i32,\r\n    ):\r\n        for i, j in ti.ndrange(height, width):\r\n            in_num = in_ten.load(ti.Vector([j, i]))\r\n            in_num.x = in_num.x + in_ten_buf[i, j]\r\n            out_ten.store(ti.Vector([j, i]), in_num)\r\n\r\n    arg_0 = ti.graph.Arg(tag=ti.graph.ArgKind.RWTEXTURE,\r\n                         name=\"in_ten\",\r\n                         fmt=ti.Format.rgba8,\r\n                         ndim=2)\r\n    arg_1 = ti.graph.Arg(tag=ti.graph.ArgKind.RWTEXTURE,\r\n                         name=\"out_ten\",\r\n                         fmt=ti.Format.rgba8,\r\n                         ndim=2)\r\n    arg_2  = ti.graph.Arg(ti.graph.ArgKind.NDARRAY, \"in_ten_buf\",  dtype=ti.u8, ndim=2)\r\n    arg_3 = ti.graph.Arg(ti.graph.ArgKind.SCALAR, \"height\", ti.i32)\r\n    arg_4 = ti.graph.Arg(ti.graph.ArgKind.SCALAR, \"width\", ti.i32)\r\n\r\n    g = ti.graph.GraphBuilder()\r\n    g.dispatch(taichi_kernel_test, arg_0, arg_1, arg_2, arg_3, arg_4)\r\n    g = g.compile()\r\n\r\n    # save\r\n    m = ti.aot.Module(arch, caps=[ti.DeviceCapability.spirv_has_int8])\r\n    m.add_graph(\"taichi_kernel_test\", g)\r\n    m.save(\"models_taichi_kernel_test\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_texture_buf(ti.vulkan)\r\n```\r\nClosing as it's resolved but feel free to reopen if you have any followup questions!"
      }
    ]
  },
  {
    "number": 7516,
    "title": "Passing structures to and from kernels",
    "created_at": "2023-03-08T22:48:37Z",
    "closed_at": "2023-03-09T15:16:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/7516",
    "body": "I'm struggling with gettings structs into and out of a Taichi kernel. For example, I need a kernel that scans a large vector field and finds the limits (xmin/xmax, ...). \r\n\r\nI've tried all the permutations I can think of for defining the data structure with ti.dataclass, ti.type.struct, and Python dicts, and for passing it in and out with class names or ti.template(); I get type errors for all of them.\r\n\r\nWhat's the recommended way to pass a data structure with named fields into and out of a kernel? Can that be done by reference so that it can be edited? Or is it not possible? My current work-around is to just use an array, and define global variables that are names of the array locations (xmin=0, xmax=1, ...).\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/7516/comments",
    "author": "neilger",
    "comments": [
      {
        "user": "yuanming-hu",
        "created_at": "2023-03-09T15:08:47Z",
        "body": "Good point... IIRC, we don't support writing out struct arguments (i.e., pass by reference) in a kernel yet. It's true that `ti.ndarray` is by reference.\r\n\r\nOne potential solution for your case though: let the kernel return a `ti.Vector` with your max/min:\r\n\r\n```python\r\nvec2 = ti.math.vec2\r\n\r\ndef minmax() -> vec2:\r\n    ...\r\n    return vec2(min, max)\r\n```"
      },
      {
        "user": "neilger",
        "created_at": "2023-03-09T15:16:01Z",
        "body": "@yuanming-hu glad to hear that you are working on this. Returning a vector is my current work-around, but quickly becomes unwieldy when there are many named attributes to refer to."
      },
      {
        "user": "Datamance",
        "created_at": "2023-11-10T16:58:38Z",
        "body": "@yuanming-hu is there an issue where we can follow the development of this feature? @neilger's idea is a nice workaround, but it would be great to be able to explicitly pass struct fields into kernels."
      }
    ]
  },
  {
    "number": 7464,
    "title": "Texture load in @ti.func",
    "created_at": "2023-03-01T00:42:23Z",
    "closed_at": "2023-03-09T18:13:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/7464",
    "body": "Hi,\r\nI have a question related with texture load in Taichi.\r\nIn example `python/taichi/examples/rendering/simple_texture.py`, the texture object is declared out of taichi scope, and passed as `ti.types.texture` type to @ti.kernel function. It seemed that the texture will be converted to `taichi.lang._texture.TextureSampler` object type. \r\nMy question is that how can use texture method like `sample_lod` or `fetch` in @ti.func?\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/7464/comments",
    "author": "afiretony",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2023-03-01T01:12:30Z",
        "body": "You need to pass the texture in `@ti.template` (pass-by-reference)"
      },
      {
        "user": "afiretony",
        "created_at": "2023-03-01T01:26:05Z",
        "body": "Hi @bobcao3 , thanks for answering!\r\nCould you elaborate what do you meant to pass in `ti.template`?\r\nThis is what I tried to do but not works:\r\n\r\n```\r\ntexture = ti.Texture(ti.Format.r32f, (n, n, n))\r\n\r\n@ti.kernel\r\ndef paint(t: ti.f32, n: ti.i32):\r\n    for i, j in pixels:\r\n        uv = ti.Vector([i / res[0], j / res[1], t])\r\n        c = ti.math.vec4(0.0)\r\n        c = sample_tex(texture, uv)\r\n        pixels[i, j] = [c.r, c.r, c.r]\r\n\r\n@ti.func\r\ndef sample_tex(tex:ti.template(), uv):\r\n    tex.sample_lod(uv, 0.0)\r\n```\r\n\r\nAttributeError: 'Texture' object has no attribute 'sample_lod'"
      },
      {
        "user": "bobcao3",
        "created_at": "2023-03-01T01:52:59Z",
        "body": "You would also need to pass the texture into the kernel. Taichi textures can not be captured through global varaibles"
      },
      {
        "user": "afiretony",
        "created_at": "2023-03-01T02:26:10Z",
        "body": "Hey @bobcao3 thanks, do you mean\r\n```\r\n@ti.kernel\r\ndef paint(t: ti.f32, tex: ti.types.texture(num_dimensions=3), n: ti.i32):\r\n    for i, j in pixels:\r\n        uv = ti.Vector([i / res[0], j / res[1], t])\r\n        c = ti.math.vec4(0.0)\r\n        c = sample_tex(tex, uv)\r\n        pixels[i, j] = [c.r, c.r, c.r]\r\n\r\n@ti.func\r\ndef sample_tex(tex:ti.template(), uv):\r\n    tex.sample_lod(uv, 0.0)\r\n```\r\nBut still gets error:\r\n\r\nTraceback (most recent call last):\r\n  File \"playground.py\", line 93, in <module>\r\n    main()\r\n  File \"playground.py\", line 86, in main\r\n    paint(t,texture, n)\r\n  File \"C:\\Users\\\\Anaconda3\\envs\\taichi\\lib\\site-packages\\taichi\\lang\\kernel_impl.py\", line 974, in wrapped\r\n    return primal(*args, **kwargs)\r\n  File \"C:\\Users\\\\Anaconda3\\envs\\taichi\\lib\\site-packages\\taichi\\lang\\kernel_impl.py\", line 901, in __call__\r\n    return self.runtime.compiled_functions[key](*args)\r\n  File \"C:\\Users\\\\Anaconda3\\envs\\taichi\\lib\\site-packages\\taichi\\lang\\kernel_impl.py\", line 826, in func__\r\n    raise e from None\r\n  File \"C:\\Users\\\\Anaconda3\\envs\\taichi\\lib\\site-packages\\taichi\\lang\\kernel_impl.py\", line 823, in func__\r\n    t_kernel(launch_ctx)\r\nRuntimeError: [type_factory.cpp:taichi::lang::promoted_type@222] Assertion failure: a->is<TensorType>() && b->is<TensorType>()\r\n"
      },
      {
        "user": "bobcao3",
        "created_at": "2023-03-01T02:28:16Z",
        "body": "Hmmmmm. That seems like a bug? Let me check "
      },
      {
        "user": "ailzhang",
        "created_at": "2023-03-09T08:23:47Z",
        "body": "@afiretony sorry for the late reply, this is not a bug since your `ti.func` is missing a return but we didn't do well in error reporting lol. \r\n\r\nThis works for me: \r\n```\r\n@ti.func\r\ndef sample_tex(tex: ti.template(), uv):\r\n    return tex.sample_lod(uv, 0.0)\r\n\r\n@ti.kernel\r\ndef paint(t: ti.f32, pixels: ti.types.ndarray(ndim=2),\r\n          tex: ti.types.texture(num_dimensions=2)):\r\n    for i, j in pixels:\r\n        uv = ti.Vector([i / res[0], j / res[1]])\r\n        warp_uv = uv + ti.Vector(\r\n            [ti.cos(t + uv.x * 5.0),\r\n             ti.sin(t + uv.y * 5.0)]) * 0.1\r\n        c = ti.math.vec4(0.0)\r\n        if uv.x > 0.5:\r\n            # c = tex.sample_lod(warp_uv, 0.0)\r\n            c = sample_tex(tex, warp_uv)\r\n        else:\r\n            c = tex.fetch(ti.cast(warp_uv * 128, ti.i32), 0)\r\n        pixels[i, j] = [c.r, c.r, c.r, 1.0]\r\n```\r\nLet us know if you have followup questions! We'll try to improve the error message here. \r\n"
      },
      {
        "user": "afiretony",
        "created_at": "2023-03-09T18:13:58Z",
        "body": "Thank you @ailzhang for pointing that out! Didn't notice such an obvious mistake here hahaha. "
      }
    ]
  },
  {
    "number": 7255,
    "title": "dataclass in the for loop not showing the right value",
    "created_at": "2023-01-30T04:54:49Z",
    "closed_at": "2023-02-03T02:26:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/7255",
    "body": "Hi all,\r\n\r\nThis might be a silly question but I think I don't understand the multiple for-loops here.\r\n\r\nThe data class called inside the third for loop is not showing the right value as I expect. I learned from the tutorial that the outermost loop will be parallelized, and others are the same with python. But that did not happen either.\r\n\r\nI am wondering why the inside value is not changing but keeps its initial value.\r\nThank you! The code is as follows:\r\n\r\n\r\n```python\r\nimport taichi as ti\r\n\r\nti.init()\r\nNp = 3\r\nNw = 2\r\nNt = 5\r\n\r\n@ti.dataclass\r\nclass Particle:\r\n    r: ti.types.vector(3, ti.f64)\r\n    t: ti.f64\r\n    @ti.func\r\n    def initParticles(self,x,y,z):\r\n        self.r = ti.Vector([x,y,z])\r\n        self.t = 0\r\n\r\n\r\n\r\nparticles = Particle.field(shape = (Np,))\r\n\r\ndr = 0.1\r\ndw = 0.3\r\n\r\n@ti.kernel\r\ndef init():\r\n    for n in range(Np):\r\n        particles[n].initParticles(0,0,dr * n)\r\n        \r\n@ti.kernel\r\ndef simulate():\r\n    for n in range(Np):\r\n        for t in range(Nt):\r\n            #rr = particles[n].r\r\n            print('time***',t )\r\n            print('r outside',particles[n].r)\r\n            for m in range(Nw):\r\n                print('r inside',particles[n].r) # why particles [n].r here showing particles' init value\r\n\r\n            particles[n].r +=ti.Vector([0,0,1])\r\n                \r\ninit()\r\nsimulate()\r\n```\r\n\r\n\r\nThe result is \r\n\r\n> [Taichi] Starting on arch=arm64\r\ntime*** 0\r\nr outside [0.000000000000, 0.000000000000, 0.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\ntime*** 1\r\nr outside [0.000000000000, 0.000000000000, 1.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\ntime*** 2\r\nr outside [0.000000000000, 0.000000000000, 2.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\ntime*** 3\r\nr outside [0.000000000000, 0.000000000000, 3.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\ntime*** 4\r\nr outside [0.000000000000, 0.000000000000, 4.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\nr inside [0.000000000000, 0.000000000000, 0.000000000000]\r\ntime*** 0\r\nr outside [0.000000000000, 0.000000000000, 0.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\ntime*** 1\r\nr outside [0.000000000000, 0.000000000000, 1.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\ntime*** 2\r\nr outside [0.000000000000, 0.000000000000, 2.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\ntime*** 3\r\nr outside [0.000000000000, 0.000000000000, 3.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\ntime*** 4\r\nr outside [0.000000000000, 0.000000000000, 4.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\nr inside [0.000000000000, 0.000000000000, 0.100000001490]\r\ntime*** 0\r\nr outside [0.000000000000, 0.000000000000, 0.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\ntime*** 1\r\nr outside [0.000000000000, 0.000000000000, 1.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\ntime*** 2\r\nr outside [0.000000000000, 0.000000000000, 2.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\ntime*** 3\r\nr outside [0.000000000000, 0.000000000000, 3.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\ntime*** 4\r\nr outside [0.000000000000, 0.000000000000, 4.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]\r\nr inside [0.000000000000, 0.000000000000, 0.200000002980]",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/7255/comments",
    "author": "donglai96",
    "comments": [
      {
        "user": "AmesingFlank",
        "created_at": "2023-01-30T17:40:20Z",
        "body": "This looks like a bug in TLS optimization. I wrote more about it here: #7263. For now, you can work around this issue by doing `advanced_optimization = False` in `ti.init()`"
      },
      {
        "user": "donglai96",
        "created_at": "2023-01-30T20:32:06Z",
        "body": "Thank you so much!"
      },
      {
        "user": "lin-hitonami",
        "created_at": "2023-02-03T02:26:21Z",
        "body": "We can follow-up this on #7263. Closing this."
      }
    ]
  },
  {
    "number": 7185,
    "title": "Performance implications of dynamic indexing",
    "created_at": "2023-01-17T02:19:33Z",
    "closed_at": "2023-01-17T03:05:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/7185",
    "body": "I have a rather large taichi program, which I wrote before `dynamic_indexing` is enabled by default, it contained a lot of loops unrolled using `for i in ti.static(range(..))`.  Now that  `dynamic_indexing` is the default, I decided to try rewriting my program to always use dynamic indices. I noticed that after dynamic indices are used, the performance of the program dropped by around 15%. I mostly use the CUDA backend.\r\n\r\nSo my questions are:\r\n* Is the performance drop expected?\r\n* Are there any general guidelines of when should loop-unrolling should be preferred over dynamic indexing in terms of performance? What are the rules of thumb?\r\n\r\n@strongoier \r\nThanks!",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/7185/comments",
    "author": "AmesingFlank",
    "comments": [
      {
        "user": "strongoier",
        "created_at": "2023-01-17T02:48:59Z",
        "body": "Thanks for your feedback on trying the feature!\r\n\r\n> Is the performance drop expected?\r\n\r\nYes. The reason is that for a local matrix, the internal representation goes from multiple scalars to an array.\r\n\r\n> Are there any general guidelines of when should loop-unrolling should be preferred over dynamic indexing in terms of performance? What are the rules of thumb?\r\n\r\nWe don't expect users to rewrite all their previous programs at this moment. For those matrices with only constant indices required, using loop-unrolling can optimize the array representation to scalars so as to achieve best performance. Of course, automatic loop-unrolling in certain cases can be an optimization opportunity in Taichi in the future.\r\n\r\nFor now, just as the release note says,\r\n\r\n> Before v1.4.0, when you wanted to access a vector/matrix with a runtime variable instead of a compile-time constant, you had to set ti.init(dynamic_index=True). However, that option only works for LLVM-based backends (CPU & CUDA) and may slow down runtime performance because all matrices are affected. Starting from v1.4.0, that option is no longer needed. You can use variable indices whenever necessary on all backends without affecting the performance of those matrices with only constant indices.\r\n\r\nFor previous users, the removal of the option is most beneficial for cases where the dynamic index feature is really needed (i.e., loop-unrolling cannot solve their problem). But for new users, this feature makes it easier to learn Taichi. Loop-unrolling is now only treated as an optimization strategy rather than a required concept to write normal programs."
      },
      {
        "user": "AmesingFlank",
        "created_at": "2023-01-17T03:05:02Z",
        "body": "I see. This makes sense. Many thanks for your explanation! "
      }
    ]
  },
  {
    "number": 7067,
    "title": "Is there any plan to support compilation optimization based on polyhedron model ?",
    "created_at": "2023-01-05T12:25:46Z",
    "closed_at": "2023-01-13T01:43:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/7067",
    "body": null,
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/7067/comments",
    "author": "zh0ngtian",
    "comments": [
      {
        "user": "ailzhang",
        "created_at": "2023-01-13T01:43:59Z",
        "body": "Synced offline that this request is more about general runtime performance improvement, closing for now. Thanks! "
      }
    ]
  },
  {
    "number": 6902,
    "title": "What's the best way to extract a vector from a matrix?",
    "created_at": "2022-12-14T10:01:42Z",
    "closed_at": "2023-01-06T05:50:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/6902",
    "body": "I tried slicing the matrix:\r\n```py\r\n@ti.kernel\r\ndef k():\r\n    m = ti.Matrix([[0,1], [2,3]])\r\n    print(m[0, :])\r\n\r\nk() \r\n```\r\nBut this gives me a matrix instead of a vector, that is, it prints `[[0, 1]]` instead of `[0, 1]`.",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/6902/comments",
    "author": "AmesingFlank",
    "comments": [
      {
        "user": "strongoier",
        "created_at": "2022-12-22T08:46:59Z",
        "body": "In the past vectors and matrices in taichi were mixed together so the semantics of slicing was not well designed. We'd better make the result `[0, 1]` (numpy also does so). WDYT @jim19930609 "
      },
      {
        "user": "jim19930609",
        "created_at": "2022-12-22T10:10:08Z",
        "body": "Yep, sounds good to me"
      }
    ]
  },
  {
    "number": 6034,
    "title": "Can not Taichi interact with other domain-specific python packages?",
    "created_at": "2022-09-11T08:39:11Z",
    "closed_at": "2022-09-16T07:36:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/6034",
    "body": "Hi.\r\nI would like to accelerate my codes by using Taichi.\r\nBut I meet a error when I add Taici to my codes. It seems that there is a conflict between the Taichi and my other domain-specific python packages. Like this:\r\n`Traceback (most recent call last):\r\n\r\n  File ~/plexe-pyapi/examples/brakedemo.py:162 in <module>\r\n    main()\r\n\r\n  File ~/anaconda3/lib/python3.8/site-packages/taichi/lang/kernel_impl.py:920 in wrapped\r\n    raise type(e)('\\n' + str(e)) from None\r\n\r\nTaichiTypeError: \r\nOn line 103 of file \"/home/vcdc/plexe-pyapi/examples/brakedemo.py\", in main:\r\n    plexe = Plexe()\r\n    ^^^^^^^^^^^^^^^\r\nInvalid constant scalar data type: <class 'plexe.plexe.Plexe'>`\r\n\r\nThe Plexe() comes from a python package :\r\n`from plexe import Plexe`\r\n\r\nSo, I want to make sure that Can not Taichi interact with other domain-specific python packages?\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/6034/comments",
    "author": "17150934",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2022-09-12T17:34:41Z",
        "body": "If you are trying to use this within a the Taichi scope (ti.func and ti.kernel) then no it will not work. You can use them normally outside the taichi scope"
      }
    ]
  },
  {
    "number": 5970,
    "title": "print has no output inside taichi kernel",
    "created_at": "2022-09-04T02:53:43Z",
    "closed_at": "2022-09-04T14:07:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/5970",
    "body": "The code:\r\n```python\r\nimport taichi as ti\r\n\r\nti.init(arch=ti.cpu, debug=True)\r\n\r\nx = ti.field(ti.f32)\r\nblock = ti.root.pointer(ti.ij, (4,4))\r\npixel = block.bitmasked(ti.ij, (2,2))\r\npixel.place(x)\r\n\r\n@ti.kernel\r\ndef activate():\r\n    x[2,3] = 1.0\r\n    x[2,4] = 2.0\r\n\r\n@ti.kernel\r\ndef print_active():\r\n    for i, j in block:\r\n        print(\"Active block\", i, j)\r\n    for i, j in x:\r\n        print('field x[{}, {}] = {}'.format(i, j, x[i, j]))\r\n\r\nif __name__ == '__main__':\r\n    print_active()\r\n```\r\n\r\noutput:\r\n```\r\n[Taichi] version 1.1.2, llvm 10.0.0, commit f25cf4a2, linux, python 3.7.4\r\n[Taichi] Starting on arch=x64\r\n```\r\n\r\nThe `print` in taichi kernel has no output.\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/5970/comments",
    "author": "hiyyg",
    "comments": [
      {
        "user": "yuanming-hu",
        "created_at": "2022-09-04T12:32:33Z",
        "body": "I believe you forgot to call `activate()` before `print_active()`. The following code would work as expected:\r\n\r\n```python\r\nimport taichi as ti\r\n\r\nti.init(arch=ti.cpu, debug=True)\r\n\r\nx = ti.field(ti.f32)\r\nblock = ti.root.pointer(ti.ij, (4,4))\r\npixel = block.bitmasked(ti.ij, (2,2))\r\npixel.place(x)\r\n\r\n@ti.kernel\r\ndef activate():\r\n    x[2,3] = 1.0\r\n    x[2,4] = 2.0\r\n\r\n@ti.kernel\r\ndef print_active():\r\n    for i, j in block:\r\n        print(\"Active block\", i, j)\r\n    for i, j in x:\r\n        print('field x[{}, {}] = {}'.format(i, j, x[i, j]))\r\n\r\nif __name__ == '__main__':\r\n    activate()\r\n    print_active()\r\n```"
      },
      {
        "user": "hiyyg",
        "created_at": "2022-09-04T14:07:51Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 5743,
    "title": "Lock metadata.lock failed",
    "created_at": "2022-08-12T03:06:36Z",
    "closed_at": "2022-08-12T07:23:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/5743",
    "body": "import taichi as ti\r\nti.init(arch=ti.cpu)\r\n\r\n提示：Lock C:/taichi_cache/ticache/metadata.lock failed\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/5743/comments",
    "author": "sunjinlongsir",
    "comments": [
      {
        "user": "qiao-bo",
        "created_at": "2022-08-12T03:55:38Z",
        "body": "Hi, can you please provide the full error log? and perhaps the output of `ti.diagnose`? thanks"
      },
      {
        "user": "lin-hitonami",
        "created_at": "2022-08-12T06:20:40Z",
        "body": "This is only a false warning, and it does not affect the execution of the Taichi program. I've removed the warning in #5747."
      }
    ]
  },
  {
    "number": 5474,
    "title": "Is it possible to merge fields?",
    "created_at": "2022-07-19T11:49:15Z",
    "closed_at": "2022-07-19T14:21:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/5474",
    "body": "Hello!\r\nI need to combine the fields so that later in `@ti.kernel` I can list all their elements, is this possible?\r\nThe number of fields is dynamic",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/5474/comments",
    "author": "compik710",
    "comments": [
      {
        "user": "neozhaoliang",
        "created_at": "2022-07-19T12:55:27Z",
        "body": "Please give more information, for example some code snippet?"
      },
      {
        "user": "compik710",
        "created_at": "2022-07-19T14:21:10Z",
        "body": "Sorry, while I was writing an example, I came up with a solution :sweat_smile: \r\nThank you for your attention)\r\n\r\n```\r\nimport taichi as ti\r\nimport numpy as np\r\n\r\nti.init(arch=ti.vulkan)\r\n\r\nlist_of_fields = [\r\n\tti.field(ti.f32, (10,7)),\r\n\tti.field(ti.f32, (40,7)),\r\n\tti.field(ti.f32, (50,7))\r\n] # dynamic list\r\n\r\n\r\n# ... # actions with some of fields in list_of_fields # ... #\r\nfor fild in list_of_fields:\r\n\tfild.from_numpy(np.random.random(fild.shape).astype(np.float32))\r\n####################\r\n\r\n\r\n### Combining fields into one big_field ###\r\n@ti.kernel\r\ndef write_data(fromm:ti.template(), too:ti.template(), from_num:ti.i32, to_num:ti.i32):\r\n\tfor i in range(to_num-from_num):\r\n\t\ttoo[from_num+i,0]=fromm[i,0]\r\n\t\ttoo[from_num+i,1]=fromm[i,1]\r\n\t\ttoo[from_num+i,2]=fromm[i,2]\r\n\t\ttoo[from_num+i,3]=fromm[i,3]\r\n\t\ttoo[from_num+i,4]=fromm[i,4]\r\n\t\ttoo[from_num+i,5]=fromm[i,5]\r\n\t\ttoo[from_num+i,6]=fromm[i,6]\r\n\t\ttoo[from_num+i,7]=fromm[i,7]\r\nbig_field = ti.field(ti.f32, (sum([fild.shape[0] for fild in list_of_fields]),7))\r\nlast = 0\r\n\r\nfor fild in list_of_fields:\r\n\twrite_data(fild,big_field,last,last+fild.shape[0])\r\n\tlast += fild.shape[0]\r\n######################\r\n\r\n\r\n### Do magic with combined data ###\r\nfor i in range(big_field.shape[0]):\r\n\tprint([big_field[i,i2] for i2 in range(7)])\r\n\t# it works !\r\n\r\n#rendering(big_field)\r\n##############################\r\n```"
      }
    ]
  },
  {
    "number": 5156,
    "title": "Inconsistent execution speed of to_numpy() operation",
    "created_at": "2022-06-13T18:03:00Z",
    "closed_at": "2022-06-13T18:56:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/5156",
    "body": "Hi,\r\n\r\nI am new to taichi and is a bit confused about the execution speed of the to_numpy() operation. What I am doing in the following code snippet is assigning random value to a taichi field and convert it to numpy. What's weird is if more assignment happens before the to_numpy conversion, the slower the conversion is.\r\n\r\nTo reproduce this behavior: \r\n```\r\nimport taichi as ti\r\nimport numpy as np\r\nti.init(arch=ti.gpu)\r\n\r\nfield = ti.Vector.field(3, dtype=ti.f32, shape=(500, 500))\r\n\r\n@ti.kernel\r\ndef render():\r\n    for i, j in field:\r\n        field[i, j] = ti.Vector([0.30, 1.4, 1.0]) * ti.random()\r\n\r\n\r\nfrom time import time\r\nwhile True:\r\n    T = 1\r\n    for i in range(T):\r\n        render()\r\n    t = time()\r\n    field_np = field.to_numpy()\r\n    print(time()-t)\r\n```\r\n\r\nWhen T = 1, the printed time is 0.02s. With T = 20, the time bumps up to 0.45.\r\nI am not expecting this because the field is a constant-size variable. \r\n\r\nCould you please provide any insight and potentially any fix regarding this?\r\nThanks!\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/5156/comments",
    "author": "zhouxian",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2022-06-13T18:56:27Z",
        "body": "The backend scheduling may not and actually do not execute all the `render` call immediately and it does not wait until previous calls to finish unless there is an dependency. This is normal behavior caused by more advanced scheduling. If you put a `ti.sync()` after the loop and forces the backend to wait until all work has finished, you will see a more consistent time (however as an result lose a bit of performance)"
      },
      {
        "user": "bobcao3",
        "created_at": "2022-06-13T18:56:40Z",
        "body": "If you have further questions feel free to re-open the issue"
      },
      {
        "user": "zhouxian",
        "created_at": "2022-06-13T19:11:26Z",
        "body": "@bobcao3 Thanks for the quick response. However, I believe it's not a behavior caused by unsynced render() call in the background.\r\nI changed the code to the following (by manually enforcing a dependency between each render() calls)\r\n```\r\nimport taichi as ti\r\nimport numpy as np\r\nti.init(arch=ti.gpu)\r\n\r\nfield = ti.Vector.field(3, dtype=ti.f32, shape=(500, 500))\r\n\r\n@ti.kernel\r\ndef render():\r\n    for i, j in field:\r\n        field[i, j] += ti.Vector([1.0, 1.4, 1.0]) + ti.Vector([0.00, 1.4, 1.0]) * ti.random()\r\n\r\n\r\nfrom time import time\r\nwhile True:\r\n    T = 1\r\n    for i in range(T):\r\n        render()\r\n    t = time()\r\n    field_np = field.to_numpy()\r\n    print(time()-t, field_np[0][0])\r\n```\r\nThe inconsistent behavior still persists. I also tried ti.sync() and found no difference.\r\n\r\nTo be more succinct, it seems the problem is simply that when a taichi field gets updated more times before being converted to numpy, the conversion takes longer.\r\n\r\n--------------------------------------------\r\nUpdate: I realized that indeed it's caused by the asynchronous execution of render() in the background. And calling to_numpy() looks like would call ti.sync() under the hood, thus its execution time would be dependent on the number of times render() is called."
      }
    ]
  },
  {
    "number": 4934,
    "title": "unexpected behavior with Hilles-Steele scan ",
    "created_at": "2022-05-09T03:13:59Z",
    "closed_at": "2022-05-15T16:15:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/4934",
    "body": "I'm not sure if this is a bug of some kind, or just user error/misunderstanding of how Taichi is supposed to work, but I was surprised by the behavior in the code example below.\r\n\r\nThis snippet implements a simple Hilles-Steele scan and uses it to attempt get the cumulative sum of a 100000 element field filled with 1s.\r\n\r\nThe output is fine up until the 30720th entry in the array, after which the entries are incorrect, and non-derministic. For example, the last element of the output array should of course be 100000, but I've been seeing values from 210000 to 290000 ish.\r\n\r\nThe number 30720 looked suspicious to me, and indeed it's 16*1920, the latter of which is the number of CUDA cores available in my Nvidia RTX 2060.\r\n\r\nI don't really know what is going wrong -- my best guess is that it seems to be some kind of race situation, in particular because of the appearance of that magic number 30720... maybe the hardware is saturated, and <something something> data race? But I'm not really sure why there would be a race condition -- I've certainly run parallel `for` loops over fields with many more elements than this without any issues.\r\n\r\nThis scan implementation is inspired by the `parallel_sort` function in `python/taichi/_kernels.py`, and I have noticed that that function includes a `sync()` call. I've tried putting `sync()` in a many different places with no change in the values I'm seeing, but I wonder if I need to use `sync()` somehow, or some other low-level runtime method like that?\r\n\r\nFinally, in case it might be helpful for writing docs that guide other users away from whatever I am misunderstanding, I'll add a bit of background about why I found this behavior surprising (and, I guess relatedly, why I was surprised that a `sync()` function even exists, and was surprised it would be needed in `parallel_sort`): In my mental model of Taichi, and based on the examples I've looked at, I have been assuming that from the perspective of the python code calling the kernels, kernel invocations are inherently serial and synchronous. That is: if you have python code like...\r\n```python\r\nti_kernel_1()\r\nti_kernel_2()\r\nti_kernel_3()\r\n```\r\n...then all of the GPU work associated with `ti_kernel_1` will finish before any of `ti_kernel_2`'s work starts, and all of `ti_kernel_2`'s work will finish before ti_kernel_3` starts, and it should not be possible to have data races between these kernels. The existence of the `sync()` function seem like a hint to me my mental model is not correct, because it seems like it would not be needed if all kernel invocations were serial and synchronous.\r\n\r\nSo I guess perhaps this is an area of the docs that might benefit from some clarification:\r\n- when are kernels launched async/parallel vs sync/serial? and how is this controlled?\r\n- when can kernel invocations result in race conditions between subsequent invocations? how can these race conditions be prevented?\r\n- what does the `sync()` function do, and when is it needed?\r\n\r\nThanks as always for the great work on Taichi! I'm always learning something fun when I work with it :-) (And I'm sure I will learn a lot from whatever I'm misunderstanding in this example!)\r\n\r\n### My non-working scan code\r\n\r\n```python\r\nimport taichi as ti\r\nimport numpy as np\r\n\r\nti.init(arch=ti.gpu)\r\n\r\n@ti.kernel\r\ndef fill_int(field: ti.template(), x: int):\r\n    for i in field:\r\n        field[i] = x\r\n\r\n\r\ndef prepare_HS_scan(in_field, out_field, ti_func):\r\n    @ti.kernel\r\n    def scan_stage(last: ti.template(), step: int):\r\n        for i in last:\r\n            out_field[i] = last[i] if i < step else ti_func(last[i - step], last[i])\r\n\r\n    def scan():\r\n        step = 1\r\n        while step < N:\r\n            last = in_field if step == 1 else out_field\r\n            scan_stage(last, step)\r\n            step = step * 2\r\n\r\n    return scan\r\n\r\n\r\n@ti.func\r\ndef ti_sum(a, b):\r\n    return a + b\r\n\r\n\r\nN = int(100000)\r\nfield_in = ti.field(ti.int32, shape=N)\r\nfill_int(field_in, 1)\r\n\r\nfield_out = ti.field(ti.int32, shape=N)\r\n\r\nscan1 = prepare_HS_scan(in_field=field_in, out_field=field_out, ti_func=ti_sum)\r\n\r\nscan1()\r\n\r\nfield_out_np = field_out.to_numpy()\r\n\r\n# last value in the array\r\n# should be 100000, non-deterministic output from 220000 to 280000 ish\r\nprint(field_out[N - 1])\r\n\r\n# first element of the arrays that doesn't match\r\n# always 30720 on my machine, which is 1920*16 -- RTX2060 has 1920 cuda cores\r\nprint(np.argmin(np.equal(field_out_np, np.arange(1, N + 1))))\r\n```\r\n\r\nExample output from the above (including version and system info)\r\n```\r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=cuda\r\n290112\r\n30720\r\n```",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/4934/comments",
    "author": "bcolloran",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2022-05-09T03:34:08Z",
        "body": "That sync is for dealing with a previous bug in Vulkan. Speaking of Vulkan, can you try this on Vulkan as well so that we can narrow down the problem a bit. (If Vulkan also shows a wrong number we might have a CHI-IR level problem, if not then we have a CUDA runtime / codegen problem) Thanks!"
      },
      {
        "user": "bcolloran",
        "created_at": "2022-05-09T04:20:41Z",
        "body": "Thanks for the fast reply @bobcao3!\r\n\r\nIndeed, I get similar behavior with `ti.init(arch=ti.vulkan)`. Example output:\r\n```\r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=vulkan\r\n[I 05/08/22 21:11:43.047 140724] [vulkan_device_creator.cpp:pick_physical_device@363] Found Vulkan Device 0 (NVIDIA GeForce RTX 2060)\r\n[I 05/08/22 21:11:43.047 140724] [vulkan_device_creator.cpp:pick_physical_device@363] Found Vulkan Device 1 (llvmpipe (LLVM 12.0.0, 256 bits))\r\n[I 05/08/22 21:11:43.048 140724] [vulkan_device_creator.cpp:create_logical_device@431] Vulkan Device \"NVIDIA GeForce RTX 2060\" supports Vulkan 0 version 1.2.175\r\n305088\r\n30720\r\n```\r\n\r\nJust to see what would happen, I also decided to try it out with `ti.cpu`... in this case I got very different (incorrect) results. In case it will be helpful, here are some example outputs:\r\n```\r\n$ python scan_error.py \r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=x64\r\n41257076\r\n4\r\n\r\n\r\n$ python scan_error.py \r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=x64\r\n-1087188976\r\n4\r\n\r\n\r\n$ python scan_error.py \r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=x64\r\n919975044\r\n4\r\n\r\n\r\n$ python scan_error.py \r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=x64\r\n-1183775544\r\n4\r\n\r\n\r\n$ python scan_error.py \r\n[Taichi] version 1.0.1, llvm 10.0.0, commit 1c3619d9, linux, python 3.9.7\r\n[Taichi] Starting on arch=x64\r\n-14705324\r\n4\r\n```\r\n(that is with AMD Ryzen 5 2600 Six-Core Processor)\r\n\r\nPlease let me know if I can supply any more information! :-)\r\n"
      },
      {
        "user": "bcolloran",
        "created_at": "2022-05-11T01:34:32Z",
        "body": "(Oh, one more quick question @bobcao3: just for my understanding, does your comment indicate that this behavior _is_ a bug, and not user error on my part in the code above? Thanks! :-) )"
      },
      {
        "user": "bobcao3",
        "created_at": "2022-05-14T16:27:23Z",
        "body": "Just checked your code, it seems out_field might be used as both input and output in some cases? That might be why this produces non deterministic outputs "
      },
      {
        "user": "bcolloran",
        "created_at": "2022-05-15T16:15:13Z",
        "body": "@bobcao3 you are of course entirely correct... well I feel foolish for taking your time! Thank you for your patience and help, and apologies for the noise!"
      }
    ]
  },
  {
    "number": 4854,
    "title": "IndexError: Field with dim 2 accessed with indices of dim 1",
    "created_at": "2022-04-24T13:23:57Z",
    "closed_at": "2022-04-25T05:18:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/4854",
    "body": "I started learning Taichi and wrote the following code:\r\n```\r\nimport taichi as ti\r\nimport numpy\r\nti.init(arch=ti.opengl)\r\n\r\n\r\ndic = numpy.array([[10,9,8,7], [10,9,8,2]])\r\n@ti.kernel\r\ndef func(dic:ti.types.ndarray()):\r\n\tdic[0][0]=11\r\n\t\r\nfunc(dic)\r\nprint(dic)\r\n```\r\n\r\nAnd get error:\r\n`IndexError: Field with dim 2 accessed with indices of dim 1`\r\n\r\nWhy it not works? ",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/4854/comments",
    "author": "compik710",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2022-04-24T23:27:21Z",
        "body": "use ```dic[0, 0]```"
      },
      {
        "user": "compik710",
        "created_at": "2022-04-25T05:18:25Z",
        "body": "@bobcao3 Thank you!"
      }
    ]
  },
  {
    "number": 4613,
    "title": "Why are there precision warnings when no type changes (target = u8, value = u8)?",
    "created_at": "2022-03-24T05:28:34Z",
    "closed_at": "2022-04-15T05:21:41Z",
    "labels": [
      "question",
      "welcome contribution"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/4613",
    "body": "Hi, I jsut tried doing bit operations and got some losing precision warnings that I could not understand since the data type wasn't changed. Is this a bug?\r\nBesides, why there was another warning (`Atomic bit_or (i32 to u8)`) when I had explicitly cast the type before the bit operation in `set_bit()`?\r\n```\r\n@ti.func\r\ndef set_bit(switches, i, dt):\r\n    switches  |= (ti.cast(1, dt) << i)\r\n    return switches\r\n\r\n@ti.kernel\r\ndef test():\r\n    for _ in range(1):\r\n        for i in range(8):\r\n            eight_flags = ti.cast(0, ti.u8)\r\n            eight_flags = set_bit(eight_flags, i, ti.u8)\r\n            print(ti.cast(eight_flags, ti.i32))\r\ntest()\r\n```\r\n\r\n\r\n```\r\n[Taichi] version 0.9.1, llvm 10.0.0, commit e2e0e669, linux, python 3.7.4\r\n[Taichi] Starting on arch=x64\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2592] Local store may lose precision (target = u8, value = u8), at\r\n2592\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2595] Local store may lose precision (target = u8, value = u8), at\r\n2595\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@67] [$2603] Atomic bit_or (i32 to u8) may lose precision, at\r\n\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2604] Local store may lose precision (target = u8, value = u8), at\r\n2604\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2606] Local store may lose precision (target = u8, value = u8), at\r\n2606\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2592] Local store may lose precision (target = u8, value = u8), at\r\n2592\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2595] Local store may lose precision (target = u8, value = u8), at\r\n2595\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2604] Local store may lose precision (target = u8, value = u8), at\r\n2604\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2606] Local store may lose precision (target = u8, value = u8), at\r\n2606\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2592] Local store may lose precision (target = u8, value = u8), at\r\n2592\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2595] Local store may lose precision (target = u8, value = u8), at\r\n2595\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2604] Local store may lose precision (target = u8, value = u8), at\r\n2604\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2606] Local store may lose precision (target = u8, value = u8), at\r\n2606\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2592] Local store may lose precision (target = u8, value = u8), at\r\n2592\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2595] Local store may lose precision (target = u8, value = u8), at\r\n2595\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2604] Local store may lose precision (target = u8, value = u8), at\r\n2604\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2606] Local store may lose precision (target = u8, value = u8), at\r\n2606\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$22] Local store may lose precision (target = u8, value = u8), at\r\n22\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$2621] Local store may lose precision (target = u8, value = u8), at\r\n2621\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.275 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at\r\n27\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$19] Local store may lose precision (target = u8, value = u8), at\r\n19\r\n[W 03/24/22 13:19:51.276 148735] [type_check.cpp:visit@142] [$27] Local store may lose precision (target = u8, value = u8), at 27\r\n```",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/4613/comments",
    "author": "GCChen97",
    "comments": [
      {
        "user": "eliotlouet",
        "created_at": "2022-03-28T16:28:39Z",
        "body": "I got the same problem with the given code below :\r\n\r\n```\r\nimport taichi as ti\r\nti.init()\r\n\r\nstructs = ti.field(shape=(1), dtype=ti.int16)\r\n\r\n@ti.kernel\r\ndef Init():\r\n    struct = structs[0]\r\n\r\nInit()\r\n\r\n```\r\nIt happens when I try to access a field value of type int16, int8, uint16, uint8, for other types it won't be a problem.\r\n"
      },
      {
        "user": "strongoier",
        "created_at": "2022-03-29T06:45:12Z",
        "body": "Thanks for reporting this! This is indeed a bug in Taichi and we are looking into a proper way to fix it."
      },
      {
        "user": "yuanming-hu",
        "created_at": "2022-04-13T14:42:03Z",
        "body": "Running into the same problem... Does anyone have an idea what's happening here? Many thanks."
      },
      {
        "user": "yuanming-hu",
        "created_at": "2022-04-14T15:02:25Z",
        "body": "Let me fix this one. Will open a PR soon."
      }
    ]
  },
  {
    "number": 4399,
    "title": "Difference between `FrontendSNodeOpStmt` and `SNodeOpExpression`",
    "created_at": "2022-02-27T08:50:37Z",
    "closed_at": "2022-03-11T06:53:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/4399",
    "body": "These two constructs have identical structures, so it would be ideal to merge them into one `SNodeOpExpression`; do they have any functional difference?\r\n\r\nWhat I've noticed is that `SNodeOpExpression::flatten` does not account for all cases of `SNodeOpType`. Are those operations only expected to be used with `FrontendSNodeOpStmt`? Is this also true vice versa?",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/4399/comments",
    "author": "re-xyr",
    "comments": [
      {
        "user": "qiao-bo",
        "created_at": "2022-02-28T01:31:28Z",
        "body": "cc @strongoier "
      },
      {
        "user": "strongoier",
        "created_at": "2022-02-28T02:51:10Z",
        "body": "IIUC `SNodeOpExpression` is for snode ops with return values, while `FrontendSNodeOpStmt` is for snode ops with no return values. Currently only `activate` and `deactivate` are handled by  `FrontendSNodeOpStmt`."
      }
    ]
  },
  {
    "number": 4385,
    "title": "How to import pictures",
    "created_at": "2022-02-25T03:26:25Z",
    "closed_at": "2022-02-25T09:27:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/4385",
    "body": "A = ti.field.from_numpy(ti.imread('296363.jpg'))\r\n\r\nreport：\r\nAttributeError: 'function' object has no attribute 'from_numpy'",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/4385/comments",
    "author": "chuiliuyiyi",
    "comments": [
      {
        "user": "bobcao3",
        "created_at": "2022-02-25T05:57:28Z",
        "body": "The `from_numpy` function only works on fields. You should declare a field first and then load the image:\r\n\r\n```python\r\nA = ti.Vector.field(n=3, dtype=ti.f32, shape=(width, height))\r\nA.from_numpy(ti.imread('296363.jpg'))\r\n```\r\n\r\nThe other option is to use `ti.ext_arr` and directly use the numpy array as input field to kernel. (This will be slower if you need to use the image multiple times)"
      },
      {
        "user": "chuiliuyiyi",
        "created_at": "2022-02-25T06:29:13Z",
        "body": "think you very much"
      }
    ]
  },
  {
    "number": 4036,
    "title": "Passing data to kernel",
    "created_at": "2022-01-16T00:56:48Z",
    "closed_at": "2022-01-17T17:41:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/4036",
    "body": "One thing that is not entirely clear from the docs (or maybe I missed this).  What is the most efficient way to pass data to kernels?  Will some ways allow kernels to be re-used vs re-compiled for each change of data fields? \r\n\r\nExamples:\r\n\r\nKernel accesses global field\r\n```\r\nglobal_field = ti.field(....)\r\n\r\n@ti.kernel\r\ndef my_kernel() -> ti.i32:\r\n    return ... # do something with global_field\r\n\r\nmy_kernel()\r\nglobal_field[x] = # change some data\r\nmy_kernel()\r\n```\r\n\r\npass field to kernel\r\n```\r\n\r\n@ti.kernel\r\ndef my_kernel(field_to_process) -> ti.i32:\r\n    return ... # do something with field_to_process\r\n\r\ndef main():\r\n    field = ti.field(....)\r\n    my_kernel(field)\r\n    field[x] = # change some data\r\n    my_kernel(field)\r\n```\r\n\r\ndata_oriented\r\n```\r\n\r\n@ti.data_oriented\r\nclass MyClass:\r\n    def __init__(self):\r\n        self.field = ti.field(....)\r\n\r\n    @ti.kernel\r\n    def my_kernel(self) -> ti.i32:\r\n           return ... # do something with self.field\r\n\r\ninstance = MyClass()\r\ninstance.my_kernel()\r\ninstance.field[x] = # change some data\r\ninstance.my_kernel()\r\n```\r\n\r\nWill one of these allow me to skip kernel recompilation if I change the field data?  Or be better?  Or is it all the same.",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/4036/comments",
    "author": "bsavery",
    "comments": [
      {
        "user": "AmesingFlank",
        "created_at": "2022-01-16T15:07:29Z",
        "body": "I don't think any of these will trigger re-compilation."
      },
      {
        "user": "bsavery",
        "created_at": "2022-01-17T17:41:03Z",
        "body": "Yes, I should have tested this more myself. The performance seems nearly the same after testing a few runs."
      },
      {
        "user": "bsavery",
        "created_at": "2022-01-17T17:44:01Z",
        "body": "On metal I get this:  \r\n\r\nglobal 11.295220851898193\r\npassed 11.354559898376465\r\ndata oriented 11.506935119628906 (seems a tiny overhead with the data class)\r\n\r\n```\r\nimport taichi as ti\r\nimport time as time\r\nimport numpy as np\r\n\r\n\r\nti.init(arch=ti.gpu)\r\n\r\n\r\nglobal_field = ti.field(dtype=ti.f32, shape=(100000,))\r\nglobal_field.from_numpy(np.random.rand(100000).astype(np.float32))\r\n\r\n\r\n@ti.kernel\r\ndef my_kernel() -> ti.i32:\r\n    sum = 0.0\r\n    for i in global_field:\r\n        sum += global_field[i]\r\n    return sum\r\n\r\n\r\nt = time.time()\r\nfor _ in range(10000):\r\n    my_kernel()\r\nprint(\"global\", time.time() - t)\r\n\r\n\r\n@ti.kernel\r\ndef my_kernel_passed(some_field: ti.template()) -> ti.i32:\r\n    sum = 0.0\r\n    for i in some_field:\r\n        sum += some_field[i]\r\n    return sum\r\n\r\n\r\nt = time.time()\r\nfor _ in range(10000):\r\n    my_kernel_passed(global_field)\r\nprint(\"passed\", time.time() - t)\r\n\r\n\r\n@ti.data_oriented\r\nclass MyClass:\r\n    def __init__(self):\r\n        self.field = ti.field(dtype=ti.f32, shape=(100000,))\r\n        self.field.from_numpy(np.random.rand(100000).astype(np.float32))\r\n\r\n    @ti.kernel\r\n    def my_kernel(self) -> ti.i32:\r\n        sum = 0.0\r\n        for i in self.field:\r\n            sum += self.field[i]\r\n        return sum\r\n\r\ninstance = MyClass()\r\nt = time.time()\r\nfor _ in range(10000):\r\n    instance.my_kernel()\r\nprint(\"data oriented\", time.time() - t)"
      }
    ]
  },
  {
    "number": 3872,
    "title": "Building from source failed",
    "created_at": "2021-12-24T08:59:23Z",
    "closed_at": "2022-04-14T23:11:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/3872",
    "body": "I failed to build taichi and here's the output from the console. My machine is Macbook Pro with Apple M1 Pro, and I use a ubuntu virtual machine with Ubuntu 20.04.2 LTS. I'd like to provide more detailed runtime environment information if needed.\r\n\r\n```\r\n/Ubuntu Shared Folders/taichi$ python3 setup.py develop --user\r\n[]\r\n['taichi', 'taichi.ui', 'taichi.tools', 'taichi.types', 'taichi.linalg', 'taichi.snode', 'taichi._lib', 'taichi.lang', 'taichi.profiler', 'taichi.aot', 'taichi._lib.core', 'taichi.lang.ast']\r\nrunning develop\r\nrunning egg_info\r\nwriting python/taichi.egg-info/PKG-INFO\r\nwriting dependency_links to python/taichi.egg-info/dependency_links.txt\r\nwriting entry points to python/taichi.egg-info/entry_points.txt\r\nwriting requirements to python/taichi.egg-info/requires.txt\r\nwriting top-level names to python/taichi.egg-info/top_level.txt\r\nreading manifest file 'python/taichi.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nwarning: no files found matching 'python/taichi/*.md'\r\nwarning: no files found matching 'python/taichi/_lib/core/*.so'\r\nwarning: no files found matching 'python/taichi/_lib/core/*.pyd'\r\nwarning: no files found matching 'python/taichi/_lib/runtime/*.bc'\r\nwarning: no files found matching 'python/taichi/_lib/runtime/*.dylib'\r\nwarning: no previously-included files matching '*.pyc' found anywhere in distribution\r\nwarning: no previously-included files matching '*.pyo' found anywhere in distribution\r\nwarning: no previously-included files matching 'ffmpeg' found anywhere in distribution\r\nadding license file 'LICENSE'\r\nwriting manifest file 'python/taichi.egg-info/SOURCES.txt'\r\nrunning build_ext\r\ncmake version 3.22.1\r\n\r\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\r\n---------- Running CMake prepare ----------------------------------------\r\nTaichi Version 0.8.9\r\n       Commit 65060be5a7a55e274df05ddd8a14fa00b240703c\r\nUsing /home/parallels/anaconda3/envs/taichi/bin/python3 as python executable.\r\nPython 3.9.7\r\n    version: 3.9\r\n    include: /home/parallels/anaconda3/envs/taichi/include/python3.9\r\n    library: /home/parallels/anaconda3/envs/taichi/lib/libpython3.9.so\r\n    numpy include: /home/parallels/anaconda3/envs/taichi/lib/python3.9/site-packages/numpy/core/include\r\n    pybind11 include: /home/parallels/.local/lib/python3.9/site-packages/pybind11/include;/home/parallels/.local/lib/python3.9/site-packages/pybind11/include\r\nUsing C++ compiler: /usr/bin/c++\r\nGNU compiler detected. Using std=c++17.\r\nCMake Warning at cmake/TaichiCXXFlags.cmake:42 (message):\r\n  It is detected that you are using gcc as the compiler.  This is an\r\n  experimental feature.  Consider adding -DCMAKE_CXX_COMPILER=clang argument\r\n  to CMake to switch to clang (or MSVC on Windows).\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:63 (include)\r\n\r\n\r\nBuilding for processor aarch64\r\nUsing float32 (single) precision as real\r\nBuilding with GLFW\r\n-- Including X11 support\r\n-- Found LLVM 10.0.0\r\n-- Using LLVMConfig.cmake in: /home/parallels/Desktop/taichi-llvm-10.0.0-linux/lib/cmake/llvm\r\nLLVM include dirs /home/parallels/Desktop/taichi-llvm-10.0.0-linux/include\r\nLLVM library dirs /home/parallels/Desktop/taichi-llvm-10.0.0-linux/lib\r\n-- TI_WITH_CUDA_TOOLKIT = OFF\r\n-- SPIRV-Cross: Finding Git version for SPIRV-Cross.\r\n-- SPIRV-Cross: Git hash: 97a438d2\r\n-- Found PythonInterp: /home/parallels/anaconda3/envs/taichi/bin/python3 (found version \"3.9.7\") \r\nSPIRV-Cross:  Testing will be disabled for SPIRV-Cross. Could not find glslang or SPIRV-Tools build under external/. To enable testing, run ./checkout_glslang_spirv_tools.sh and ./build_glslang_spirv_tools.sh first.\r\n-- Found PythonInterp: /home/parallels/anaconda3/envs/taichi/bin/python3 (found suitable version \"3.9.7\", minimum required is \"3\") \r\nPYTHON_LIBRARIES: /home/parallels/anaconda3/envs/taichi/lib/libpython3.9.so\r\nC++ Flags:  -DTI_ISE_NONE -std=c++17 -fsized-deallocation -Wno-class-memaccess -Wno-comment -Wno-sign-compare -Wall  -DTI_ARCH_ARM -DTI_PASS_EXCEPTION_TO_PYTHON -DTI_INCLUDED -DTI_WITH_LLVM -DTI_WITH_CUDA -DTI_WITH_METAL -DTI_WITH_OPENGL -DTI_WITH_CC\r\nBuild type: Release\r\nClang executable /usr/bin/clang-10\r\nclang --version: 10.0.0\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /media/psf/Ubuntu Shared Folders/taichi/build\r\n---------- Building extensions ----------------------------------------\r\n[  0%] Built target spirv-tools-build-version\r\n[  0%] Built target core_tables\r\n[  1%] Built target enum_string_mapping\r\n[  1%] Built target spirv-tools-header-NonSemanticShaderDebugInfo100\r\n[  1%] Built target spv-tools-spv-amd-sevp\r\n[  1%] Built target spv-tools-spv-amd-stm\r\n[  1%] Built target spv-tools-spv-amd-gs\r\n[  2%] Built target spv-tools-spv-amd-sb\r\n[  2%] Built target spv-tools-debuginfo\r\n[  2%] Built target spv-tools-cldi100\r\n[  2%] Built target spv-tools-shdi100\r\n[  2%] Built target spv-tools-clspvreflection\r\n[  2%] Built target spirv-tools-header-DebugInfo\r\n[  2%] Built target spirv-tools-header-OpenCLDebugInfo100\r\nruntime.cpp:1043:18: error: invalid input constraint 'l' in asm\r\n               : \"l\"(value), \"r\"(mask));\r\n                 ^\r\nruntime.cpp:1053:44: warning: value size does not match register size specified by the constraint and modifier [-Wasm-operand-widths]\r\n  asm volatile(\"activemask.b32 %0;\" : \"=r\"(mask));\r\n                                           ^\r\nruntime.cpp:1053:32: note: use constraint modifier \"w\"\r\n  asm volatile(\"activemask.b32 %0;\" : \"=r\"(mask));\r\n                               ^~\r\n                               %w0\r\n1 warning and 1 error generated.\r\nmake[2]: *** [CMakeFiles/generate_llvm_runtime_cuda.dir/build.make:70: CMakeFiles/generate_llvm_runtime_cuda] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:757: CMakeFiles/generate_llvm_runtime_cuda.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[  2%] Built target generate_llvm_runtime_arm64\r\nmake: *** [Makefile:136: all] Error 2\r\nTraceback (most recent call last):\r\n  File \"/media/psf/Ubuntu Shared Folders/taichi/setup.py\", line 245, in <module>\r\n    setup(name=project_name,\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/site-packages/setuptools/__init__.py\", line 153, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/site-packages/setuptools/command/develop.py\", line 34, in run\r\n    self.install_for_development()\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/site-packages/setuptools/command/develop.py\", line 114, in install_for_development\r\n    self.run_command('build_ext')\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/distutils/cmd.py\", line 313, in run_command\r\n    self.distribution.run_command(command)\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/media/psf/Ubuntu Shared Folders/taichi/setup.py\", line 166, in run\r\n    subprocess.check_call(cmake_cmd, cwd=self.build_temp)\r\n  File \"/home/parallels/anaconda3/envs/taichi/lib/python3.9/subprocess.py\", line 373, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--config', 'Release', '--', '-j2']' returned non-zero exit status 2.\r\n```",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/3872/comments",
    "author": "DDDOH",
    "comments": [
      {
        "user": "AmesingFlank",
        "created_at": "2021-12-24T11:54:47Z",
        "body": "This looks like the clang version is to high. Perhaps try using clang <= 12"
      }
    ]
  },
  {
    "number": 3739,
    "title": "Why not use mpi as one parallelization option when using cpu backend?",
    "created_at": "2021-12-08T01:59:17Z",
    "closed_at": "2022-02-15T06:40:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/3739",
    "body": "Hello,\r\n\r\nI'm an algorithm developer from CAE industry. We usually use mpi as the default parallelizaiton method, especially when two or more computing nodes are needed. In general, MPI+X is the common scheme., and X can be openmp, pthread(rarely used), GPU, etc.\r\n\r\nIs there any problem to implement mpi codes in Taichi, or other concerns that i don't know.\r\n\r\nThanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/3739/comments",
    "author": "GumpXiaoli",
    "comments": [
      {
        "user": "turbo0628",
        "created_at": "2021-12-10T16:09:45Z",
        "body": "X can be Taichi in the MPI + X common scheme. In the current version, you can temporarily solve this problem by combining MPI4py + Taichi. This solution should work well with coarse-grained parallelism (many processes, each process can be solved with an independent smaller Taichi kernel),  but shall not tackle with fine-grained parallelism (grand taichi field distributed over a large compute cluster). The latter parallelization pattern is obviously complex for Taichi: Taichi would have to automatically analyze and parallelize with the right multi-level parallel scheme (MPI + CPU + GPU, for example). \r\n\r\nI guess you might want to manually embedding MPI code into Taichi scope? It is neither a reasonable implementation: the compiler cannot override hand-written grids, so it's a great challenge to keep it consistent between the compiler's parallelization strategy and the user specified grids.\r\n\r\nTherefore, I would suggest that you try out small taichi kernels and use some forms of MPI for cluster parallelization. I believe that the fine-grained scale-out implementation is something on the distant roadmap :) "
      },
      {
        "user": "qiao-bo",
        "created_at": "2022-02-15T06:40:03Z",
        "body": "We close this for now, please reopen this if you have further questions."
      }
    ]
  },
  {
    "number": 3411,
    "title": "Do the test time out from time to time?",
    "created_at": "2021-11-06T18:42:12Z",
    "closed_at": "2021-11-07T06:51:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/3411",
    "body": "Hey,\r\n\r\nI've made a pull request who passes all tests except the Macos Testing (Intel).\r\nHowever, I have checked the \"cancelled\" test. It has (seemingly) nothing to do with my commit and furthermore there is no reason this test should have failed on Macos but passed on all the other platforms.\r\nTherefore I am wondering if the Test maybe just took so long and timed out?\r\n\r\nThanks!\r\nNiclas \r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/3411/comments",
    "author": "NiclasSchwalbe",
    "comments": [
      {
        "user": "NiclasSchwalbe",
        "created_at": "2021-11-06T22:02:19Z",
        "body": "I just pushed again. Works now..."
      },
      {
        "user": "k-ye",
        "created_at": "2021-11-07T06:51:50Z",
        "body": "Sorry, sometimes the CI tests can get flaky 😂 "
      }
    ]
  },
  {
    "number": 1142,
    "title": "[Lang] Do we have a C/C++ Frontend?",
    "created_at": "2020-06-04T14:18:37Z",
    "closed_at": "2021-12-13T01:28:18Z",
    "labels": [
      "question",
      "wontfix",
      "feature request"
    ],
    "url": "https://github.com/taichi-dev/taichi/issues/1142",
    "body": "**Concisely describe the proposed feature**\r\nI understand that AST transforms take place mainly in Python, but is there an incentive to develop a C/C++ frontend for those unfamiliar with Python/prefers a C++ environment? \r\n\r\n**Describe the solution you'd like (if any)**\r\nMaintaining a cpp api that closely maps the Python life-cycle, and are kept only for advanced usage, while the recommended environment is still Python.\r\n\r\n**Additional comments**\r\n- cpp syntax may allow more possibilities for taichi\r\n- but I'm not sure if this is actually doable\r\n",
    "comments_url": "https://api.github.com/repos/taichi-dev/taichi/issues/1142/comments",
    "author": "liaopeiyuan",
    "comments": [
      {
        "user": "rexwangcc",
        "created_at": "2020-06-04T15:02:57Z",
        "body": "related discussion: #439 "
      },
      {
        "user": "archibate",
        "created_at": "2020-06-04T15:33:15Z",
        "body": "We used to have a legacy C++ frontend, called `taichi.h`, now its code is out-of-date, but still lying in this repo, not sure it's still working, ask @yuanming-hu for more info."
      }
    ]
  }
]